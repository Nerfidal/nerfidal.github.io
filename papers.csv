id;link;pub_date;title;abstract;authors;pdf_link;category;comments;tags
2412.18584v1;http://arxiv.org/abs/2412.18584v1;2024-12-24;Resolution-Robust 3D MRI Reconstruction with 2D Diffusion Priors:  Diverse-Resolution Training Outperforms Interpolation;"Deep learning-based 3D imaging, in particular magnetic resonance imaging
(MRI), is challenging because of limited availability of 3D training data.
Therefore, 2D diffusion models trained on 2D slices are starting to be
leveraged for 3D MRI reconstruction. However, as we show in this paper,
existing methods pertain to a fixed voxel size, and performance degrades when
the voxel size is varied, as it is often the case in clinical practice. In this
paper, we propose and study several approaches for resolution-robust 3D MRI
reconstruction with 2D diffusion priors. As a result of this investigation, we
obtain a simple resolution-robust variational 3D reconstruction approach based
on diffusion-guided regularization of randomly sampled 2D slices. This method
provides competitive reconstruction quality compared to posterior sampling
baselines. Towards resolving the sensitivity to resolution-shifts, we
investigate state-of-the-art model-based approaches including Gaussian
splatting, neural representations, and infinite-dimensional diffusion models,
as well as a simple data-centric approach of training the diffusion model on
several resolutions. Our experiments demonstrate that the model-based
approaches fail to close the performance gap in 3D MRI. In contrast, the
data-centric approach of training the diffusion model on various resolutions
effectively provides a resolution-robust method without compromising accuracy.";Anselm Krainovic<author:sep>Stefan Ruschke<author:sep>Reinhard Heckel;http://arxiv.org/pdf/2412.18584v1;cs.CV;;
2412.18380v1;http://arxiv.org/abs/2412.18380v1;2024-12-24;RSGaussian:3D Gaussian Splatting with LiDAR for Aerial Remote Sensing  Novel View Synthesis;"This study presents RSGaussian, an innovative novel view synthesis (NVS)
method for aerial remote sensing scenes that incorporate LiDAR point cloud as
constraints into the 3D Gaussian Splatting method, which ensures that Gaussians
grow and split along geometric benchmarks, addressing the overgrowth and
floaters issues occurs. Additionally, the approach introduces coordinate
transformations with distortion parameters for camera models to achieve
pixel-level alignment between LiDAR point clouds and 2D images, facilitating
heterogeneous data fusion and achieving the high-precision geo-alignment
required in aerial remote sensing. Depth and plane consistency losses are
incorporated into the loss function to guide Gaussians towards real depth and
plane representations, significantly improving depth estimation accuracy.
Experimental results indicate that our approach has achieved novel view
synthesis that balances photo-realistic visual quality and high-precision
geometric estimation under aerial remote sensing datasets. Finally, we have
also established and open-sourced a dense LiDAR point cloud dataset along with
its corresponding aerial multi-view images, AIR-LONGYAN.";Yiling Yao<author:sep>Wenjuan Zhang<author:sep>Bing Zhang<author:sep>Bocheng Li<author:sep>Yaning Wang<author:sep>Bowen Wang;http://arxiv.org/pdf/2412.18380v1;cs.CV;;gaussian splatting
2412.17635v2;http://arxiv.org/abs/2412.17635v2;2024-12-23;LangSurf: Language-Embedded Surface Gaussians for 3D Scene Understanding;"Applying Gaussian Splatting to perception tasks for 3D scene understanding is
becoming increasingly popular. Most existing works primarily focus on rendering
2D feature maps from novel viewpoints, which leads to an imprecise 3D language
field with outlier languages, ultimately failing to align objects in 3D space.
By utilizing masked images for feature extraction, these approaches also lack
essential contextual information, leading to inaccurate feature representation.
To this end, we propose a Language-Embedded Surface Field (LangSurf), which
accurately aligns the 3D language fields with the surface of objects,
facilitating precise 2D and 3D segmentation with text query, widely expanding
the downstream tasks such as removal and editing. The core of LangSurf is a
joint training strategy that flattens the language Gaussian on the object
surfaces using geometry supervision and contrastive losses to assign accurate
language features to the Gaussians of objects. In addition, we also introduce
the Hierarchical-Context Awareness Module to extract features at the image
level for contextual information then perform hierarchical mask pooling using
masks segmented by SAM to obtain fine-grained language features in different
hierarchies. Extensive experiments on open-vocabulary 2D and 3D semantic
segmentation demonstrate that LangSurf outperforms the previous
state-of-the-art method LangSplat by a large margin. As shown in Fig. 1, our
method is capable of segmenting objects in 3D space, thus boosting the
effectiveness of our approach in instance recognition, removal, and editing,
which is also supported by comprehensive experiments.
\url{https://langsurf.github.io}.";Hao Li<author:sep>Roy Qin<author:sep>Zhengyu Zou<author:sep>Diqi He<author:sep>Bohan Li<author:sep>Bingquan Dai<author:sep>Dingewn Zhang<author:sep>Junwei Han;http://arxiv.org/pdf/2412.17635v2;cs.CV;\url{https://langsurf.github.io};gaussian splatting
2412.17532v1;http://arxiv.org/abs/2412.17532v1;2024-12-23;Exploring Dynamic Novel View Synthesis Technologies for Cinematography;"Novel view synthesis (NVS) has shown significant promise for applications in
cinematographic production, particularly through the exploitation of Neural
Radiance Fields (NeRF) and Gaussian Splatting (GS). These methods model real 3D
scenes, enabling the creation of new shots that are challenging to capture in
the real world due to set topology or expensive equipment requirement. This
innovation also offers cinematographic advantages such as smooth camera
movements, virtual re-shoots, slow-motion effects, etc. This paper explores
dynamic NVS with the aim of facilitating the model selection process. We
showcase its potential through a short montage filmed using various NVS models.";Adrian Azzarelli<author:sep>Nantheera Anantrasirichai<author:sep>David R Bull;http://arxiv.org/pdf/2412.17532v1;cs.CV;;gaussian splatting<tag:sep>nerf
2412.17769v1;http://arxiv.org/abs/2412.17769v1;2024-12-23;ActiveGS: Active Scene Reconstruction using Gaussian Splatting;"Robotics applications often rely on scene reconstructions to enable
downstream tasks. In this work, we tackle the challenge of actively building an
accurate map of an unknown scene using an on-board RGB-D camera. We propose a
hybrid map representation that combines a Gaussian splatting map with a coarse
voxel map, leveraging the strengths of both representations: the high-fidelity
scene reconstruction capabilities of Gaussian splatting and the spatial
modelling strengths of the voxel map. The core of our framework is an effective
confidence modelling technique for the Gaussian splatting map to identify
under-reconstructed areas, while utilising spatial information from the voxel
map to target unexplored areas and assist in collision-free path planning. By
actively collecting scene information in under-reconstructed and unexplored
areas for map updates, our approach achieves superior Gaussian splatting
reconstruction results compared to state-of-the-art approaches. Additionally,
we demonstrate the applicability of our active scene reconstruction framework
in the real world using an unmanned aerial vehicle.";Liren Jin<author:sep>Xingguang Zhong<author:sep>Yue Pan<author:sep>Jens Behley<author:sep>Cyrill Stachniss<author:sep>Marija Popović;http://arxiv.org/pdf/2412.17769v1;cs.RO;;gaussian splatting
2412.17378v1;http://arxiv.org/abs/2412.17378v1;2024-12-23;Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained  Tiling;"3D Gaussian Splatting (3DGS) is increasingly attracting attention in both
academia and industry owing to its superior visual quality and rendering speed.
However, training a 3DGS model remains a time-intensive task, especially in
load imbalance scenarios where workload diversity among pixels and Gaussian
spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS,
a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS
training process, perfectly solving load-imbalance issues. First, we
innovatively introduce the inter-block dynamic workload distribution technique
to map workloads to Streaming Multiprocessor(SM) resources within a single GPU
dynamically, which constitutes the foundation of load balancing. Second, we are
the first to propose the Gaussian-wise parallel rendering technique to
significantly reduce workload divergence inside a warp, which serves as a
critical component in addressing load imbalance. Based on the above two
methods, we further creatively put forward the fine-grained combined load
balancing technique to uniformly distribute workload across all SMs, which
boosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we
present a self-adaptive render kernel selection strategy during the 3DGS
training process based on different load-balance situations, which effectively
improves training efficiency.";Hao Gui<author:sep>Lin Hu<author:sep>Rui Chen<author:sep>Mingxiao Huang<author:sep>Yuxin Yin<author:sep>Jin Yang<author:sep>Yong Wu;http://arxiv.org/pdf/2412.17378v1;cs.CV;;gaussian splatting
2412.17715v1;http://arxiv.org/abs/2412.17715v1;2024-12-23;GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal  Guidance;"In this paper, we present GaussianPainter, the first method to paint a point
cloud into 3D Gaussians given a reference image. GaussianPainter introduces an
innovative feed-forward approach to overcome the limitations of time-consuming
test-time optimization in 3D Gaussian splatting. Our method addresses a
critical challenge in the field: the non-uniqueness problem inherent in the
large parameter space of 3D Gaussian splatting. This space, encompassing
rotation, anisotropic scales, and spherical harmonic coefficients, introduces
the challenge of rendering similar images from substantially different Gaussian
fields. As a result, feed-forward networks face instability when attempting to
directly predict high-quality Gaussian fields, struggling to converge on
consistent parameters for a given output. To address this issue, we propose to
estimate a surface normal for each point to determine its Gaussian rotation.
This strategy enables the network to effectively predict the remaining Gaussian
parameters in the constrained space. We further enhance our approach with an
appearance injection module, incorporating reference image appearance into
Gaussian fields via a multiscale triplane representation. Our method
successfully balances efficiency and fidelity in 3D Gaussian generation,
achieving high-quality, diverse, and robust 3D content creation from point
clouds in a single forward pass.";Jingqiu Zhou<author:sep>Lue Fan<author:sep>Xuesong Chen<author:sep>Linjiang Huang<author:sep>Si Liu<author:sep>Hongsheng Li;http://arxiv.org/pdf/2412.17715v1;cs.CV;To appear in AAAI 2025;gaussian splatting
2412.17612v1;http://arxiv.org/abs/2412.17612v1;2024-12-23;CoSurfGS:Collaborative 3D Surface Gaussian Splatting with Distributed  Learning for Large Scene Reconstruction;"3D Gaussian Splatting (3DGS) has demonstrated impressive performance in scene
reconstruction. However, most existing GS-based surface reconstruction methods
focus on 3D objects or limited scenes. Directly applying these methods to
large-scale scene reconstruction will pose challenges such as high memory
costs, excessive time consumption, and lack of geometric detail, which makes it
difficult to implement in practical applications. To address these issues, we
propose a multi-agent collaborative fast 3DGS surface reconstruction framework
based on distributed learning for large-scale surface reconstruction.
Specifically, we develop local model compression (LMC) and model aggregation
schemes (MAS) to achieve high-quality surface representation of large scenes
while reducing GPU memory consumption. Extensive experiments on Urban3d,
MegaNeRF, and BlendedMVS demonstrate that our proposed method can achieve fast
and scalable high-fidelity surface reconstruction and photorealistic rendering.
Our project page is available at \url{https://gyy456.github.io/CoSurfGS}.";Yuanyuan Gao<author:sep>Yalun Dai<author:sep>Hao Li<author:sep>Weicai Ye<author:sep>Junyi Chen<author:sep>Danpeng Chen<author:sep>Dingwen Zhang<author:sep>Tong He<author:sep>Guofeng Zhang<author:sep>Junwei Han;http://arxiv.org/pdf/2412.17612v1;cs.CV;"Our project page is available at
  \url{https://gyy456.github.io/CoSurfGS}";gaussian splatting<tag:sep>nerf
2412.17628v1;http://arxiv.org/abs/2412.17628v1;2024-12-23;Editing Implicit and Explicit Representations of Radiance Fields: A  Survey;"Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent
years by offering a new volumetric representation, which is compact and
provides high-quality image rendering. However, the methods to edit those
radiance fields developed slower than the many improvements to other aspects of
NeRF. With the recent development of alternative radiance field-based
representations inspired by NeRF as well as the worldwide rise in popularity of
text-to-image models, many new opportunities and strategies have emerged to
provide radiance field editing. In this paper, we deliver a comprehensive
survey of the different editing methods present in the literature for NeRF and
other similar radiance field representations. We propose a new taxonomy for
classifying existing works based on their editing methodologies, review
pioneering models, reflect on current and potential new applications of
radiance field editing, and compare state-of-the-art approaches in terms of
editing options and performance.";Arthur Hubert<author:sep>Gamal Elghazaly<author:sep>Raphael Frank;http://arxiv.org/pdf/2412.17628v1;cs.CV;;nerf
2412.17812v1;http://arxiv.org/abs/2412.17812v1;2024-12-23;FaceLift: Single Image to 3D Head with View Generation and GS-LRM;"We present FaceLift, a feed-forward approach for rapid, high-quality,
360-degree head reconstruction from a single image. Our pipeline begins by
employing a multi-view latent diffusion model that generates consistent side
and back views of the head from a single facial input. These generated views
then serve as input to a GS-LRM reconstructor, which produces a comprehensive
3D representation using Gaussian splats. To train our system, we develop a
dataset of multi-view renderings using synthetic 3D human head as-sets. The
diffusion-based multi-view generator is trained exclusively on synthetic head
images, while the GS-LRM reconstructor undergoes initial training on Objaverse
followed by fine-tuning on synthetic head data. FaceLift excels at preserving
identity and maintaining view consistency across views. Despite being trained
solely on synthetic data, FaceLift demonstrates remarkable generalization to
real-world images. Through extensive qualitative and quantitative evaluations,
we show that FaceLift outperforms state-of-the-art methods in 3D head
reconstruction, highlighting its practical applicability and robust performance
on real-world images. In addition to single image reconstruction, FaceLift
supports video inputs for 4D novel view synthesis and seamlessly integrates
with 2D reanimation techniques to enable 3D facial animation. Project page:
https://weijielyu.github.io/FaceLift.";Weijie Lyu<author:sep>Yi Zhou<author:sep>Ming-Hsuan Yang<author:sep>Zhixin Shu;http://arxiv.org/pdf/2412.17812v1;cs.CV;Project page: https://weijielyu.github.io/FaceLift;
2412.16809v1;http://arxiv.org/abs/2412.16809v1;2024-12-22;GeoTexDensifier: Geometry-Texture-Aware Densification for High-Quality  Photorealistic 3D Gaussian Splatting;"3D Gaussian Splatting (3DGS) has recently attracted wide attentions in
various areas such as 3D navigation, Virtual Reality (VR) and 3D simulation,
due to its photorealistic and efficient rendering performance. High-quality
reconstrution of 3DGS relies on sufficient splats and a reasonable distribution
of these splats to fit real geometric surface and texture details, which turns
out to be a challenging problem. We present GeoTexDensifier, a novel
geometry-texture-aware densification strategy to reconstruct high-quality
Gaussian splats which better comply with the geometric structure and texture
richness of the scene. Specifically, our GeoTexDensifier framework carries out
an auxiliary texture-aware densification method to produce a denser
distribution of splats in fully textured areas, while keeping sparsity in
low-texture regions to maintain the quality of Gaussian point cloud. Meanwhile,
a geometry-aware splitting strategy takes depth and normal priors to guide the
splitting sampling and filter out the noisy splats whose initial positions are
far from the actual geometric surfaces they aim to fit, under a Validation of
Depth Ratio Change checking. With the help of relative monocular depth prior,
such geometry-aware validation can effectively reduce the influence of
scattered Gaussians to the final rendering quality, especially in regions with
weak textures or without sufficient training views. The texture-aware
densification and geometry-aware splitting strategies are fully combined to
obtain a set of high-quality Gaussian splats. We experiment our GeoTexDensifier
framework on various datasets and compare our Novel View Synthesis results to
other state-of-the-art 3DGS approaches, with detailed quantitative and
qualitative evaluations to demonstrate the effectiveness of our method in
producing more photorealistic 3DGS models.";Hanqing Jiang<author:sep>Xiaojun Xiang<author:sep>Han Sun<author:sep>Hongjie Li<author:sep>Liyang Zhou<author:sep>Xiaoyu Zhang<author:sep>Guofeng Zhang;http://arxiv.org/pdf/2412.16809v1;cs.CV;12 pages, 8 figures, 1 table;gaussian splatting
2412.16932v1;http://arxiv.org/abs/2412.16932v1;2024-12-22;GSemSplat: Generalizable Semantic 3D Gaussian Splatting from  Uncalibrated Image Pairs;"Modeling and understanding the 3D world is crucial for various applications,
from augmented reality to robotic navigation. Recent advancements based on 3D
Gaussian Splatting have integrated semantic information from multi-view images
into Gaussian primitives. However, these methods typically require costly
per-scene optimization from dense calibrated images, limiting their
practicality. In this paper, we consider the new task of generalizable 3D
semantic field modeling from sparse, uncalibrated image pairs. Building upon
the Splatt3R architecture, we introduce GSemSplat, a framework that learns
open-vocabulary semantic representations linked to 3D Gaussians without the
need for per-scene optimization, dense image collections or calibration. To
ensure effective and reliable learning of semantic features in 3D space, we
employ a dual-feature approach that leverages both region-specific and
context-aware semantic features as supervision in the 2D space. This allows us
to capitalize on their complementary strengths. Experimental results on the
ScanNet++ dataset demonstrate the effectiveness and superiority of our approach
compared to the traditional scene-specific method. We hope our work will
inspire more research into generalizable 3D understanding.";Xingrui Wang<author:sep>Cuiling Lan<author:sep>Hanxin Zhu<author:sep>Zhibo Chen<author:sep>Yan Lu;http://arxiv.org/pdf/2412.16932v1;cs.CV;;gaussian splatting
2412.16737v1;http://arxiv.org/abs/2412.16737v1;2024-12-21;LUCES-MV: A Multi-View Dataset for Near-Field Point Light Source  Photometric Stereo;"The biggest improvements in Photometric Stereo (PS) field has recently come
from adoption of differentiable volumetric rendering techniques such as NeRF or
Neural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV
benchmark. However, while there are sizeable datasets for environment lit
objects such as Digital Twin Catalogue (DTS), there are only several small
Photometric Stereo datasets which often lack challenging objects (simple,
smooth, untextured) and practical, small form factor (near-field) light setup.
  To address this, we propose LUCES-MV, the first real-world, multi-view
dataset designed for near-field point light source photometric stereo. Our
dataset includes 15 objects with diverse materials, each imaged under varying
light conditions from an array of 15 LEDs positioned 30 to 40 centimeters from
the camera center. To facilitate transparent end-to-end evaluation, our dataset
provides not only ground truth normals and ground truth object meshes and poses
but also light and camera calibration images.
  We evaluate state-of-the-art near-field photometric stereo algorithms,
highlighting their strengths and limitations across different material and
shape complexities. LUCES-MV dataset offers an important benchmark for
developing more robust, accurate and scalable real-world Photometric Stereo
based 3D reconstruction methods.";Fotios Logothetis<author:sep>Ignas Budvytis<author:sep>Stephan Liwicki<author:sep>Roberto Cipolla;http://arxiv.org/pdf/2412.16737v1;cs.CV;;nerf
2412.16619v1;http://arxiv.org/abs/2412.16619v1;2024-12-21;Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for  Optimized Structural Integrity;"Gaussian Splatting (GS) has emerged as a crucial technique for representing
discrete volumetric radiance fields. It leverages unique parametrization to
mitigate computational demands in scene optimization. This work introduces
Topology-Aware 3D Gaussian Splatting (Topology-GS), which addresses two key
limitations in current approaches: compromised pixel-level structural integrity
due to incomplete initial geometric coverage, and inadequate feature-level
integrity from insufficient topological constraints during optimization. To
overcome these limitations, Topology-GS incorporates a novel interpolation
strategy, Local Persistent Voronoi Interpolation (LPVI), and a topology-focused
regularization term based on persistent barcodes, named PersLoss. LPVI utilizes
persistent homology to guide adaptive interpolation, enhancing point coverage
in low-curvature areas while preserving topological structure. PersLoss aligns
the visual perceptual similarity of rendered images with ground truth by
constraining distances between their topological features. Comprehensive
experiments on three novel-view synthesis benchmarks demonstrate that
Topology-GS outperforms existing methods in terms of PSNR, SSIM, and LPIPS
metrics, while maintaining efficient memory usage. This study pioneers the
integration of topology with 3D-GS, laying the groundwork for future research
in this area.";Tianqi Shen<author:sep>Shaohua Liu<author:sep>Jiaqi Feng<author:sep>Ziye Ma<author:sep>Ning An;http://arxiv.org/pdf/2412.16619v1;cs.CV;;gaussian splatting
2412.16604v1;http://arxiv.org/abs/2412.16604v1;2024-12-21;OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional  Images with Editable Capabilities;"Feed-forward 3D Gaussian Splatting (3DGS) models have gained significant
popularity due to their ability to generate scenes immediately without needing
per-scene optimization. Although omnidirectional images are getting more
popular since they reduce the computation for image stitching to composite a
holistic scene, existing feed-forward models are only designed for perspective
images. The unique optical properties of omnidirectional images make it
difficult for feature encoders to correctly understand the context of the image
and make the Gaussian non-uniform in space, which hinders the image quality
synthesized from novel views. We propose OmniSplat, a pioneering work for fast
feed-forward 3DGS generation from a few omnidirectional images. We introduce
Yin-Yang grid and decompose images based on it to reduce the domain gap between
omnidirectional and perspective images. The Yin-Yang grid can use the existing
CNN structure as it is, but its quasi-uniform characteristic allows the
decomposed image to be similar to a perspective image, so it can exploit the
strong prior knowledge of the learned feed-forward network. OmniSplat
demonstrates higher reconstruction accuracy than existing feed-forward networks
trained on perspective images. Furthermore, we enhance the segmentation
consistency between omnidirectional images by leveraging attention from the
encoder of OmniSplat, providing fast and clean 3DGS editing results.";Suyoung Lee<author:sep>Jaeyoung Chung<author:sep>Kihoon Kim<author:sep>Jaeyoo Huh<author:sep>Gunhee Lee<author:sep>Minsoo Lee<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2412.16604v1;cs.CV;;gaussian splatting
2412.16028v1;http://arxiv.org/abs/2412.16028v1;2024-12-20;CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from  Defocused Images;"3D Gaussian Splatting (3DGS) has attracted significant attention for its
high-quality novel view rendering, inspiring research to address real-world
challenges. While conventional methods depend on sharp images for accurate
scene reconstruction, real-world scenarios are often affected by defocus blur
due to finite depth of field, making it essential to account for realistic 3D
scene representation. In this study, we propose CoCoGaussian, a Circle of
Confusion-aware Gaussian Splatting that enables precise 3D scene representation
using only defocused images. CoCoGaussian addresses the challenge of defocus
blur by modeling the Circle of Confusion (CoC) through a physically grounded
approach based on the principles of photographic defocus. Exploiting 3D
Gaussians, we compute the CoC diameter from depth and learnable aperture
information, generating multiple Gaussians to precisely capture the CoC shape.
Furthermore, we introduce a learnable scaling factor to enhance robustness and
provide more flexibility in handling unreliable depth in scenes with reflective
or refractive surfaces. Experiments on both synthetic and real-world datasets
demonstrate that CoCoGaussian achieves state-of-the-art performance across
multiple benchmarks.";Jungho Lee<author:sep>Suhwan Cho<author:sep>Taeoh Kim<author:sep>Ho-Deok Jang<author:sep>Minhyeok Lee<author:sep>Geonho Cha<author:sep>Dongyoon Wee<author:sep>Dogyoon Lee<author:sep>Sangyoun Lee;http://arxiv.org/pdf/2412.16028v1;cs.CV;Project Page: https://Jho-Yonsei.github.io/CoCoGaussian/;gaussian splatting
2412.15867v1;http://arxiv.org/abs/2412.15867v1;2024-12-20;IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing;"In inverse rendering, accurately modeling visibility and indirect radiance
for incident light is essential for capturing secondary effects. Due to the
absence of a powerful Gaussian ray tracer, previous 3DGS-based methods have
either adopted a simplified rendering equation or used learnable parameters to
approximate incident light, resulting in inaccurate material and lighting
estimations. To this end, we introduce inter-reflective Gaussian splatting
(IRGS) for inverse rendering. To capture inter-reflection, we apply the full
rendering equation without simplification and compute incident radiance on the
fly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we
present an efficient optimization scheme to handle the computational demands of
Monte Carlo sampling for rendering equation evaluation. Furthermore, we
introduce a novel strategy for querying the indirect radiance of incident light
when relighting the optimized scenes. Extensive experiments on multiple
standard benchmarks validate the effectiveness of IRGS, demonstrating its
capability to accurately model complex inter-reflection effects.";Chun Gu<author:sep>Xiaofei Wei<author:sep>Zixuan Zeng<author:sep>Yuxuan Yao<author:sep>Li Zhang;http://arxiv.org/pdf/2412.15867v1;cs.CV;Project page: https://fudan-zvg.github.io/IRGS;gaussian splatting
2412.15890v1;http://arxiv.org/abs/2412.15890v1;2024-12-20;NeuroPump: Simultaneous Geometric and Color Rectification for Underwater  Images;"Underwater image restoration aims to remove geometric and color distortions
due to water refraction, absorption and scattering. Previous studies focus on
restoring either color or the geometry, but to our best knowledge, not both.
However, in practice it may be cumbersome to address the two rectifications
one-by-one. In this paper, we propose NeuroPump, a self-supervised method to
simultaneously optimize and rectify underwater geometry and color as if water
were pumped out. The key idea is to explicitly model refraction, absorption and
scattering in Neural Radiance Field (NeRF) pipeline, such that it not only
performs simultaneous geometric and color rectification, but also enables to
synthesize novel views and optical effects by controlling the decoupled
parameters. In addition, to address issue of lack of real paired ground truth
images, we propose an underwater 360 benchmark dataset that has real paired
(i.e., with and without water) images. Our method clearly outperforms other
baselines both quantitatively and qualitatively.";Yue Guo<author:sep>Haoxiang Liao<author:sep>Haibin Ling<author:sep>Bingyao Huang;http://arxiv.org/pdf/2412.15890v1;cs.CV;;nerf
2412.15609v1;http://arxiv.org/abs/2412.15609v1;2024-12-20;AvatarPerfect: User-Assisted 3D Gaussian Splatting Avatar Refinement  with Automatic Pose Suggestion;"Creating high-quality 3D avatars using 3D Gaussian Splatting (3DGS) from a
monocular video benefits virtual reality and telecommunication applications.
However, existing automatic methods exhibit artifacts under novel poses due to
limited information in the input video. We propose AvatarPerfect, a novel
system that allows users to iteratively refine 3DGS avatars by manually editing
the rendered avatar images. In each iteration, our system suggests a new body
and camera pose to help users identify and correct artifacts. The edited images
are then used to update the current avatar, and our system suggests the next
body and camera pose for further refinement. To investigate the effectiveness
of AvatarPerfect, we conducted a user study comparing our method to an existing
3DGS editor SuperSplat, which allows direct manipulation of Gaussians without
automatic pose suggestions. The results indicate that our system enables users
to obtain higher quality refined 3DGS avatars than the existing 3DGS editor.";Jotaro Sakamiya<author:sep>I-Chao Shen<author:sep>Jinsong Zhang<author:sep>Mustafa Doga Dogan<author:sep>Takeo Igarashi;http://arxiv.org/pdf/2412.15609v1;cs.HC;13 pages;gaussian splatting
2412.15550v1;http://arxiv.org/abs/2412.15550v1;2024-12-20;EGSRAL: An Enhanced 3D Gaussian Splatting based Renderer with Automated  Labeling for Large-Scale Driving Scene;"3D Gaussian Splatting (3D GS) has gained popularity due to its faster
rendering speed and high-quality novel view synthesis. Some researchers have
explored using 3D GS for reconstructing driving scenes. However, these methods
often rely on various data types, such as depth maps, 3D boxes, and
trajectories of moving objects. Additionally, the lack of annotations for
synthesized images limits their direct application in downstream tasks. To
address these issues, we propose EGSRAL, a 3D GS-based method that relies
solely on training images without extra annotations. EGSRAL enhances 3D GS's
capability to model both dynamic objects and static backgrounds and introduces
a novel adaptor for auto labeling, generating corresponding annotations based
on existing annotations. We also propose a grouping strategy for vanilla 3D GS
to address perspective issues in rendering large-scale, complex scenes. Our
method achieves state-of-the-art performance on multiple datasets without any
extra annotation. For example, the PSNR metric reaches 29.04 on the nuScenes
dataset. Moreover, our automated labeling can significantly improve the
performance of 2D/3D detection tasks. Code is available at
https://github.com/jiangxb98/EGSRAL.";Yixiong Huo<author:sep>Guangfeng Jiang<author:sep>Hongyang Wei<author:sep>Ji Liu<author:sep>Song Zhang<author:sep>Han Liu<author:sep>Xingliang Huang<author:sep>Mingjie Lu<author:sep>Jinzhang Peng<author:sep>Dong Li<author:sep>Lu Tian<author:sep>Emad Barsoum;http://arxiv.org/pdf/2412.15550v1;cs.CV;AAAI2025;gaussian splatting
2412.16253v1;http://arxiv.org/abs/2412.16253v1;2024-12-20;Interactive Scene Authoring with Specialized Generative Primitives;"Generating high-quality 3D digital assets often requires expert knowledge of
complex design tools. We introduce Specialized Generative Primitives, a
generative framework that allows non-expert users to author high-quality 3D
scenes in a seamless, lightweight, and controllable manner. Each primitive is
an efficient generative model that captures the distribution of a single
exemplar from the real world. With our framework, users capture a video of an
environment, which we turn into a high-quality and explicit appearance model
thanks to 3D Gaussian Splatting. Users then select regions of interest guided
by semantically-aware features. To create a generative primitive, we adapt
Generative Cellular Automata to single-exemplar training and controllable
generation. We decouple the generative task from the appearance model by
operating on sparse voxels and we recover a high-quality output with a
subsequent sparse patch consistency step. Each primitive can be trained within
10 minutes and used to author new scenes interactively in a fully compositional
manner. We showcase interactive sessions where various primitives are extracted
from real-world scenes and controlled to create 3D assets and scenes in a few
minutes. We also demonstrate additional capabilities of our primitives:
handling various 3D representations to control generation, transferring
appearances, and editing geometries.";Clément Jambon<author:sep>Changwoon Choi<author:sep>Dongsu Zhang<author:sep>Olga Sorkine-Hornung<author:sep>Young Min Kim;http://arxiv.org/pdf/2412.16253v1;cs.CV;;gaussian splatting
2412.16346v1;http://arxiv.org/abs/2412.16346v1;2024-12-20;SOUS VIDE: Cooking Visual Drone Navigation Policies in a Gaussian  Splatting Vacuum;"We propose a new simulator, training approach, and policy architecture,
collectively called SOUS VIDE, for end-to-end visual drone navigation. Our
trained policies exhibit zero-shot sim-to-real transfer with robust real-world
performance using only on-board perception and computation. Our simulator,
called FiGS, couples a computationally simple drone dynamics model with a high
visual fidelity Gaussian Splatting scene reconstruction. FiGS can quickly
simulate drone flights producing photorealistic images at up to 130 fps. We use
FiGS to collect 100k-300k observation-action pairs from an expert MPC with
privileged state and dynamics information, randomized over dynamics parameters
and spatial disturbances. We then distill this expert MPC into an end-to-end
visuomotor policy with a lightweight neural architecture, called SV-Net. SV-Net
processes color image, optical flow and IMU data streams into low-level body
rate and thrust commands at 20Hz onboard a drone. Crucially, SV-Net includes a
Rapid Motor Adaptation (RMA) module that adapts at runtime to variations in
drone dynamics. In a campaign of 105 hardware experiments, we show SOUS VIDE
policies to be robust to 30% mass variations, 40 m/s wind gusts, 60% changes in
ambient brightness, shifting or removing objects from the scene, and people
moving aggressively through the drone's visual field. Code, data, and
experiment videos can be found on our project page:
https://stanfordmsl.github.io/SousVide/.";JunEn Low<author:sep>Maximilian Adang<author:sep>Javier Yu<author:sep>Keiko Nagami<author:sep>Mac Schwager;http://arxiv.org/pdf/2412.16346v1;cs.RO;;gaussian splatting
2412.16141v1;http://arxiv.org/abs/2412.16141v1;2024-12-20;NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for  Vision of Autonomous Systems;"Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.";Laura Weihl<author:sep>Bilal Wehbe<author:sep>Andrzej Wąsowski;http://arxiv.org/pdf/2412.16141v1;cs.CV;;nerf
2412.15447v1;http://arxiv.org/abs/2412.15447v1;2024-12-19;LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene  Reconstruction;"Photorealistic 3D scene reconstruction plays an important role in autonomous
driving, enabling the generation of novel data from existing datasets to
simulate safety-critical scenarios and expand training data without additional
acquisition costs. Gaussian Splatting (GS) facilitates real-time,
photorealistic rendering with an explicit 3D Gaussian representation of the
scene, providing faster processing and more intuitive scene editing than the
implicit Neural Radiance Fields (NeRFs). While extensive GS research has
yielded promising advancements in autonomous driving applications, they
overlook two critical aspects: First, existing methods mainly focus on
low-speed and feature-rich urban scenes and ignore the fact that highway
scenarios play a significant role in autonomous driving. Second, while LiDARs
are commonplace in autonomous driving platforms, existing methods learn
primarily from images and use LiDAR only for initial estimates or without
precise sensor modeling, thus missing out on leveraging the rich depth
information LiDAR offers and limiting the ability to synthesize LiDAR data. In
this paper, we propose a novel GS method for dynamic scene synthesis and
editing with improved scene reconstruction through LiDAR supervision and
support for LiDAR rendering. Unlike prior works that are tested mostly on urban
datasets, to the best of our knowledge, we are the first to focus on the more
challenging and highly relevant highway scenes for autonomous driving, with
sparse sensor views and monotone backgrounds.";Pou-Chun Kung<author:sep>Xianling Zhang<author:sep>Katherine A. Skinner<author:sep>Nikita Jaipuria;http://arxiv.org/pdf/2412.15447v1;cs.CV;;gaussian splatting<tag:sep>nerf
2412.15171v2;http://arxiv.org/abs/2412.15171v2;2024-12-19;SqueezeMe: Efficient Gaussian Avatars for VR;"Gaussian Splatting has enabled real-time 3D human avatars with unprecedented
levels of visual quality. While previous methods require a desktop GPU for
real-time inference of a single avatar, we aim to squeeze multiple Gaussian
avatars onto a portable virtual reality headset with real-time drivable
inference. We begin by training a previous work, Animatable Gaussians, on a
high quality dataset captured with 512 cameras. The Gaussians are animated by
controlling base set of Gaussians with linear blend skinning (LBS) motion and
then further adjusting the Gaussians with a neural network decoder to correct
their appearance. When deploying the model on a Meta Quest 3 VR headset, we
find two major computational bottlenecks: the decoder and the rendering. To
accelerate the decoder, we train the Gaussians in UV-space instead of
pixel-space, and we distill the decoder to a single neural network layer.
Further, we discover that neighborhoods of Gaussians can share a single
corrective from the decoder, which provides an additional speedup. To
accelerate the rendering, we develop a custom pipeline in Vulkan that runs on
the mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently
at 72 FPS on a VR headset. Demo videos are at
https://forresti.github.io/squeezeme.";Shunsuke Saito<author:sep>Stanislav Pidhorskyi<author:sep>Igor Santesteban<author:sep>Forrest Iandola<author:sep>Divam Gupta<author:sep>Anuj Pahuja<author:sep>Nemanja Bartolovic<author:sep>Frank Yu<author:sep>Emanuel Garbin<author:sep>Tomas Simon;http://arxiv.org/pdf/2412.15171v2;cs.CV;v2;gaussian splatting
2412.14579v1;http://arxiv.org/abs/2412.14579v1;2024-12-19;GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D  Gaussian Splatting;"3D occupancy perception is gaining increasing attention due to its capability
to offer detailed and precise environment representations. Previous
weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU
varying by 5-10 points due to sampling count along camera rays. Recently,
real-time Gaussian splatting has gained widespread popularity in 3D
reconstruction, and the occupancy prediction task can also be viewed as a
reconstruction task. Consequently, we propose GSRender, which naturally employs
3D Gaussian Splatting for occupancy prediction, simplifying the sampling
process. In addition, the limitations of 2D supervision result in duplicate
predictions along the same camera ray. We implemented the Ray Compensation (RC)
module, which mitigates this issue by compensating for features from adjacent
frames. Finally, we redesigned the loss to eliminate the impact of dynamic
objects from adjacent frames. Extensive experiments demonstrate that our
approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while
narrowing the gap with 3D supervision methods. Our code will be released soon.";Qianpu Sun<author:sep>Changyong Shu<author:sep>Sifan Zhou<author:sep>Zichen Yu<author:sep>Yan Chen<author:sep>Dawei Yang<author:sep>Yuan Chun;http://arxiv.org/pdf/2412.14579v1;cs.CV;;gaussian splatting<tag:sep>nerf
2412.14547v1;http://arxiv.org/abs/2412.14547v1;2024-12-19;Bright-NeRF:Brightening Neural Radiance Field with Color Restoration  from Low-light Raw Images;"Neural Radiance Fields (NeRFs) have demonstrated prominent performance in
novel view synthesis. However, their input heavily relies on image acquisition
under normal light conditions, making it challenging to learn accurate scene
representation in low-light environments where images typically exhibit
significant noise and severe color distortion. To address these challenges, we
propose a novel approach, Bright-NeRF, which learns enhanced and high-quality
radiance fields from multi-view low-light raw images in an unsupervised manner.
Our method simultaneously achieves color restoration, denoising, and enhanced
novel view synthesis. Specifically, we leverage a physically-inspired model of
the sensor's response to illumination and introduce a chromatic adaptation loss
to constrain the learning of response, enabling consistent color perception of
objects regardless of lighting conditions. We further utilize the raw data's
properties to expose the scene's intensity automatically. Additionally, we have
collected a multi-view low-light raw image dataset to advance research in this
field. Experimental results demonstrate that our proposed method significantly
outperforms existing 2D and 3D approaches. Our code and dataset will be made
publicly available.";Min Wang<author:sep>Xin Huang<author:sep>Guoqing Zhou<author:sep>Qifeng Guo<author:sep>Qing Wang;http://arxiv.org/pdf/2412.14547v1;cs.CV;Accepted by AAAI2025;nerf
2412.14568v1;http://arxiv.org/abs/2412.14568v1;2024-12-19;Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF  Separation;"Recent learning-based Multi-View Stereo models have demonstrated
state-of-the-art performance in sparse-view 3D reconstruction. However,
directly applying 3D Gaussian Splatting (3DGS) as a refinement step following
these models presents challenges. We hypothesize that the excessive positional
degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting
color patterns at the cost of structural fidelity. To address this, we propose
reprojection-based DoF separation, a method distinguishing positional DoFs in
terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To
independently manage each DoF, we introduce a reprojection process along with
tailored constraints for each DoF. Through experiments across various datasets,
we confirm that separating the positional DoFs of Gaussians and applying
targeted constraints effectively suppresses geometric artifacts, producing
reconstruction results that are both visually and geometrically plausible.";Yongsung Kim<author:sep>Minjun Park<author:sep>Jooyoung Choi<author:sep>Sungroh Yoon;http://arxiv.org/pdf/2412.14568v1;cs.CV;11 pages;gaussian splatting
2412.15400v1;http://arxiv.org/abs/2412.15400v1;2024-12-19;SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface  Reconstruction;"Gaussian splatting has achieved impressive improvements for both novel-view
synthesis and surface reconstruction from multi-view images. However, current
methods still struggle to reconstruct high-quality surfaces from only sparse
view input images using Gaussian splatting. In this paper, we propose a novel
method called SolidGS to address this problem. We observed that the
reconstructed geometry can be severely inconsistent across multi-views, due to
the property of Gaussian function in geometry rendering. This motivates us to
consolidate all Gaussians by adopting a more solid kernel function, which
effectively improves the surface reconstruction quality. With the additional
help of geometrical regularization and monocular normal estimation, our method
achieves superior performance on the sparse view surface reconstruction than
all the Gaussian splatting methods and neural field methods on the widely used
DTU, Tanks-and-Temples, and LLFF datasets.";Zhuowen Shen<author:sep>Yuan Liu<author:sep>Zhang Chen<author:sep>Zhong Li<author:sep>Jiepeng Wang<author:sep>Yongqing Liang<author:sep>Zhengming Yu<author:sep>Jingdong Zhang<author:sep>Yi Xu<author:sep>Scott Schaefer<author:sep>Xin Li<author:sep>Wenping Wang;http://arxiv.org/pdf/2412.15400v1;cs.CV;Project page: https://mickshen7558.github.io/projects/SolidGS/;gaussian splatting
2412.14957v1;http://arxiv.org/abs/2412.14957v1;2024-12-19;Dream to Manipulate: Compositional World Models Empowering Robot  Imitation Learning with Imagination;"A world model provides an agent with a representation of its environment,
enabling it to predict the causal consequences of its actions. Current world
models typically cannot directly and explicitly imitate the actual environment
in front of a robot, often resulting in unrealistic behaviors and
hallucinations that make them unsuitable for real-world applications. In this
paper, we introduce a new paradigm for constructing world models that are
explicit representations of the real world and its dynamics. By integrating
cutting-edge advances in real-time photorealism with Gaussian Splatting and
physics simulators, we propose the first compositional manipulation world
model, which we call DreMa. DreMa replicates the observed world and its
dynamics, allowing it to imagine novel configurations of objects and predict
the future consequences of robot actions. We leverage this capability to
generate new data for imitation learning by applying equivariant
transformations to a small set of demonstrations. Our evaluations across
various settings demonstrate significant improvements in both accuracy and
robustness by incrementing actions and object distributions, reducing the data
needed to learn a policy and improving the generalization of the agents. As a
highlight, we show that a real Franka Emika Panda robot, powered by DreMa's
imagination, can successfully learn novel physical tasks from just a single
example per task variation (one-shot policy learning). Our project page and
source code can be found in https://leobarcellona.github.io/DreamToManipulate/";Leonardo Barcellona<author:sep>Andrii Zadaianchuk<author:sep>Davide Allegro<author:sep>Samuele Papa<author:sep>Stefano Ghidoni<author:sep>Efstratios Gavves;http://arxiv.org/pdf/2412.14957v1;cs.RO;;gaussian splatting
2412.15278v1;http://arxiv.org/abs/2412.15278v1;2024-12-18;DreaMark: Rooting Watermark in Score Distillation Sampling Generated  Neural Radiance Fields;"Recent advancements in text-to-3D generation can generate neural radiance
fields (NeRFs) with score distillation sampling, enabling 3D asset creation
without real-world data capture. With the rapid advancement in NeRF generation
quality, protecting the copyright of the generated NeRF has become increasingly
important. While prior works can watermark NeRFs in a post-generation way, they
suffer from two vulnerabilities. First, a delay lies between NeRF generation
and watermarking because the secret message is embedded into the NeRF model
post-generation through fine-tuning. Second, generating a non-watermarked NeRF
as an intermediate creates a potential vulnerability for theft. To address both
issues, we propose Dreamark to embed a secret message by backdooring the NeRF
during NeRF generation. In detail, we first pre-train a watermark decoder.
Then, the Dreamark generates backdoored NeRFs in a way that the target secret
message can be verified by the pre-trained watermark decoder on an arbitrary
trigger viewport. We evaluate the generation quality and watermark robustness
against image- and model-level attacks. Extensive experiments show that the
watermarking process will not degrade the generation quality, and the watermark
achieves 90+% accuracy among both image-level attacks (e.g., Gaussian noise)
and model-level attacks (e.g., pruning attack).";Xingyu Zhu<author:sep>Xiapu Luo<author:sep>Xuetao Wei;http://arxiv.org/pdf/2412.15278v1;cs.GR;;nerf
2412.13547v1;http://arxiv.org/abs/2412.13547v1;2024-12-18;Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance  Fields;"Novel-view synthesis is an important problem in computer vision with
applications in 3D reconstruction, mixed reality, and robotics. Recent methods
like 3D Gaussian Splatting (3DGS) have become the preferred method for this
task, providing high-quality novel views in real time. However, the training
time of a 3DGS model is slow, often taking 30 minutes for a scene with 200
views. In contrast, our goal is to reduce the optimization time by training for
fewer steps while maintaining high rendering quality. Specifically, we combine
the guidance from both the position error and the appearance error to achieve a
more effective densification. To balance the rate between adding new Gaussians
and fitting old Gaussians, we develop a convergence-aware budget control
mechanism. Moreover, to make the densification process more reliable, we
selectively add new Gaussians from mostly visited regions. With these designs,
we reduce the Gaussian optimization steps to one-third of the previous approach
while achieving a comparable or even better novel view rendering quality. To
further facilitate the rapid fitting of 4K resolution images, we introduce a
dilation-based rendering technique. Our method, Turbo-GS, speeds up
optimization for typical scenes and scales well to high-resolution (4K)
scenarios on standard datasets. Through extensive experiments, we show that our
method is significantly faster in optimization than other methods while
retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.";Tao Lu<author:sep>Ankit Dhiman<author:sep>R Srinath<author:sep>Emre Arslan<author:sep>Angela Xing<author:sep>Yuanbo Xiangli<author:sep>R Venkatesh Babu<author:sep>Srinath Sridhar;http://arxiv.org/pdf/2412.13547v1;cs.CV;Project page: https://ivl.cs.brown.edu/research/turbo-gs;gaussian splatting
2412.13509v1;http://arxiv.org/abs/2412.13509v1;2024-12-18;Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data  Presentation;"Understanding sensor data can be challenging for non-experts because of the
complexity and unique semantic meanings of sensor modalities. This calls for
intuitive and effective methods to present sensor information. However,
creating intuitive sensor data visualizations presents three key challenges:
the variability of sensor readings, gaps in domain comprehension, and the
dynamic nature of sensor data. To address these issues, we develop Vivar, a
novel AR system that integrates multi-modal sensor data and presents 3D
volumetric content for visualization. In particular, we introduce a cross-modal
embedding approach that maps sensor data into a pre-trained visual embedding
space through barycentric interpolation. This allows for accurate and
continuous integration of multi-modal sensor information. Vivar also
incorporates sensor-aware AR scene generation using foundation models and 3D
Gaussian Splatting (3DGS) without requiring domain expertise. In addition,
Vivar leverages latent reuse and caching strategies to accelerate 2D and AR
content generation. Our extensive experiments demonstrate that our system
achieves 11$\times$ latency reduction without compromising quality. A user
study involving over 485 participants, including domain experts, demonstrates
Vivar's effectiveness in accuracy, consistency, and real-world applicability,
paving the way for more intuitive sensor data visualization.";Yunqi Guo<author:sep>Kaiyuan Hou<author:sep>Heming Fu<author:sep>Hongkai Chen<author:sep>Zhenyu Yan<author:sep>Guoliang Xing<author:sep>Xiaofan Jiang;http://arxiv.org/pdf/2412.13509v1;cs.HC;;gaussian splatting
2412.13654v1;http://arxiv.org/abs/2412.13654v1;2024-12-18;GAGS: Granularity-Aware Feature Distillation for Language Gaussian  Splatting;"3D open-vocabulary scene understanding, which accurately perceives complex
semantic properties of objects in space, has gained significant attention in
recent years. In this paper, we propose GAGS, a framework that distills 2D CLIP
features into 3D Gaussian splatting, enabling open-vocabulary queries for
renderings on arbitrary viewpoints. The main challenge of distilling 2D
features for 3D fields lies in the multiview inconsistency of extracted 2D
features, which provides unstable supervision for the 3D feature field. GAGS
addresses this challenge with two novel strategies. First, GAGS associates the
prompt point density of SAM with the camera distances, which significantly
improves the multiview consistency of segmentation results. Second, GAGS
further decodes a granularity factor to guide the distillation process and this
granularity factor can be learned in a unsupervised manner to only select the
multiview consistent 2D features in the distillation process. Experimental
results on two datasets demonstrate significant performance and stability
improvements of GAGS in visual grounding and semantic segmentation, with an
inference speed 2$\times$ faster than baseline methods. The code and additional
results are available at https://pz0826.github.io/GAGS-Webpage/ .";Yuning Peng<author:sep>Haiping Wang<author:sep>Yuan Liu<author:sep>Chenglu Wen<author:sep>Zhen Dong<author:sep>Bisheng Yang;http://arxiv.org/pdf/2412.13654v1;cs.CV;Project page: https://pz0826.github.io/GAGS-Webpage/;gaussian splatting
2412.13639v1;http://arxiv.org/abs/2412.13639v1;2024-12-18;4D Radar-Inertial Odometry based on Gaussian Modeling and  Multi-Hypothesis Scan Matching;"4D millimeter-wave (mmWave) radars are sensors that provide robustness
against adverse weather conditions (rain, snow, fog, etc.), and as such they
are increasingly being used for odometry and SLAM applications. However, the
noisy and sparse nature of the returned scan data proves to be a challenging
obstacle for existing point cloud matching based solutions, especially those
originally intended for more accurate sensors such as LiDAR. Inspired by visual
odometry research around 3D Gaussian Splatting, in this paper we propose using
freely positioned 3D Gaussians to create a summarized representation of a radar
point cloud tolerant to sensor noise, and subsequently leverage its inherent
probability distribution function for registration (similar to NDT). Moreover,
we propose simultaneously optimizing multiple scan matching hypotheses in order
to further increase the robustness of the system against local optima of the
function. Finally, we fuse our Gaussian modeling and scan matching algorithms
into an EKF radar-inertial odometry system designed after current best
practices. Experiments show that our Gaussian-based odometry is able to
outperform current baselines on a well-known 4D radar dataset used for
evaluation.";Fernando Amodeo<author:sep>Luis Merino<author:sep>Fernando Caballero;http://arxiv.org/pdf/2412.13639v1;cs.RO;"Our code and results can be publicly accessed at:
  https://github.com/robotics-upo/gaussian-rio";gaussian splatting
2412.13983v1;http://arxiv.org/abs/2412.13983v1;2024-12-18;GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians;"Rendering photorealistic head avatars from arbitrary viewpoints is crucial
for various applications like virtual reality. Although previous methods based
on Neural Radiance Fields (NeRF) can achieve impressive results, they lack
fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have
improved rendering quality and real-time performance but still require
significant storage overhead. In this paper, we introduce a method called
GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians
for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an
appearance GNN to generate the attributes of the 3D Gaussians from the tracked
mesh. Therefore, our method can store the GNN models instead of the 3D
Gaussians, significantly reducing the storage overhead to just 10MB. To reduce
the impact of face-tracking errors, we also present a novel graph-guided
optimization module to refine face-tracking parameters during training.
Finally, we introduce a 3D-aware enhancer for post-processing to enhance the
rendering quality. We conduct comprehensive experiments to demonstrate the
advantages of GraphAvatar, surpassing existing methods in visual fidelity and
storage consumption. The ablation study sheds light on the trade-offs between
rendering quality and model size. The code will be released at:
https://github.com/ucwxb/GraphAvatar";Xiaobao Wei<author:sep>Peng Chen<author:sep>Ming Lu<author:sep>Hui Chen<author:sep>Feng Tian;http://arxiv.org/pdf/2412.13983v1;cs.CV;accepted by AAAI2025;gaussian splatting<tag:sep>nerf
2412.13183v1;http://arxiv.org/abs/2412.13183v1;2024-12-17;Real-time Free-view Human Rendering from Sparse-view RGB Videos using  Double Unprojected Textures;"Real-time free-view human rendering from sparse-view RGB inputs is a
challenging task due to the sensor scarcity and the tight time budget. To
ensure efficiency, recent methods leverage 2D CNNs operating in texture space
to learn rendering primitives. However, they either jointly learn geometry and
appearance, or completely ignore sparse image information for geometry
estimation, significantly harming visual quality and robustness to unseen body
poses. To address these issues, we present Double Unprojected Textures, which
at the core disentangles coarse geometric deformation estimation from
appearance synthesis, enabling robust and photorealistic 4K rendering in
real-time. Specifically, we first introduce a novel image-conditioned template
deformation network, which estimates the coarse deformation of the human
template from a first unprojected texture. This updated geometry is then used
to apply a second and more accurate texture unprojection. The resulting texture
map has fewer artifacts and better alignment with input views, which benefits
our learning of finer-level geometry and appearance represented by Gaussian
splats. We validate the effectiveness and efficiency of the proposed method in
quantitative and qualitative experiments, which significantly surpasses other
state-of-the-art methods.";Guoxing Sun<author:sep>Rishabh Dabral<author:sep>Heming Zhu<author:sep>Pascal Fua<author:sep>Christian Theobalt<author:sep>Marc Habermann;http://arxiv.org/pdf/2412.13183v1;cs.CV;Project page: https://vcai.mpi-inf.mpg.de/projects/DUT/;
2412.12919v1;http://arxiv.org/abs/2412.12919v1;2024-12-17;4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel  Reconstruction from Sparse-View Dynamic DSA Images;"Reconstructing 3D vessel structures from sparse-view dynamic digital
subtraction angiography (DSA) images enables accurate medical assessment while
reducing radiation exposure. Existing methods often produce suboptimal results
or require excessive computation time. In this work, we propose 4D radiative
Gaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently.
In detail, we represent the vessels with 4D radiative Gaussian kernels. Each
kernel has time-invariant geometry parameters, including position, rotation,
and scale, to model static vessel structures. The time-dependent central
attenuation of each kernel is predicted from a compact neural network to
capture the temporal varying response of contrast agent flow. We splat these
Gaussian kernels to synthesize DSA images via X-ray rasterization and optimize
the model with real captured ones. The final 3D vessel volume is voxelized from
the well-trained kernels. Moreover, we introduce accumulated attenuation
pruning and bounded scaling activation to improve reconstruction quality.
Extensive experiments on real-world patient data demonstrate that 4DRGS
achieves impressive results in 5 minutes training, which is 32x faster than the
state-of-the-art method. This underscores the potential of 4DRGS for real-world
clinics.";Zhentao Liu<author:sep>Ruyi Zha<author:sep>Huangxuan Zhao<author:sep>Hongdong Li<author:sep>Zhiming Cui;http://arxiv.org/pdf/2412.12919v1;eess.IV;Zhentao Liu and Ruyi Zha made equal contributions;gaussian splatting
2412.12906v1;http://arxiv.org/abs/2412.12906v1;2024-12-17;CATSplat: Context-Aware Transformer with Spatial Guidance for  Generalizable 3D Gaussian Splatting from A Single-View Image;"Recently, generalizable feed-forward methods based on 3D Gaussian Splatting
have gained significant attention for their potential to reconstruct 3D scenes
using finite resources. These approaches create a 3D radiance field,
parameterized by per-pixel 3D Gaussian primitives, from just a few images in a
single forward pass. However, unlike multi-view methods that benefit from
cross-view correspondences, 3D scene reconstruction with a single-view image
remains an underexplored area. In this work, we introduce CATSplat, a novel
generalizable transformer-based framework designed to break through the
inherent constraints in monocular settings. First, we propose leveraging
textual guidance from a visual-language model to complement insufficient
information from a single image. By incorporating scene-specific contextual
details from text embeddings through cross-attention, we pave the way for
context-aware 3D scene reconstruction beyond relying solely on visual cues.
Moreover, we advocate utilizing spatial guidance from 3D point features toward
comprehensive geometric understanding under single-view settings. With 3D
priors, image features can capture rich structural insights for predicting 3D
Gaussians without multi-view techniques. Extensive experiments on large-scale
datasets demonstrate the state-of-the-art performance of CATSplat in
single-view 3D scene reconstruction with high-quality novel view synthesis.";Wonseok Roh<author:sep>Hwanhee Jung<author:sep>Jong Wook Kim<author:sep>Seunggwan Lee<author:sep>Innfarn Yoo<author:sep>Andreas Lugmayr<author:sep>Seunggeun Chi<author:sep>Karthik Ramani<author:sep>Sangpil Kim;http://arxiv.org/pdf/2412.12906v1;cs.CV;;gaussian splatting
2412.13047v1;http://arxiv.org/abs/2412.13047v1;2024-12-17;EOGS: Gaussian Splatting for Earth Observation;"Recently, Gaussian splatting has emerged as a strong alternative to NeRF,
demonstrating impressive 3D modeling capabilities while requiring only a
fraction of the training and rendering time. In this paper, we show how the
standard Gaussian splatting framework can be adapted for remote sensing,
retaining its high efficiency. This enables us to achieve state-of-the-art
performance in just a few minutes, compared to the day-long optimization
required by the best-performing NeRF-based Earth observation methods. The
proposed framework incorporates remote-sensing improvements from EO-NeRF, such
as radiometric correction and shadow modeling, while introducing novel
components, including sparsity, view consistency, and opacity regularizations.";Luca Savant Aira<author:sep>Gabriele Facciolo<author:sep>Thibaud Ehret;http://arxiv.org/pdf/2412.13047v1;cs.CV;;gaussian splatting<tag:sep>nerf
2412.12849v1;http://arxiv.org/abs/2412.12849v1;2024-12-17;HyperGS: Hyperspectral 3D Gaussian Splatting;"We introduce HyperGS, a novel framework for Hyperspectral Novel View
Synthesis (HNVS), based on a new latent 3D Gaussian Splatting (3DGS) technique.
Our approach enables simultaneous spatial and spectral renderings by encoding
material properties from multi-view 3D hyperspectral datasets. HyperGS
reconstructs high-fidelity views from arbitrary perspectives with improved
accuracy and speed, outperforming currently existing methods. To address the
challenges of high-dimensional data, we perform view synthesis in a learned
latent space, incorporating a pixel-wise adaptive density function and a
pruning technique for increased training stability and efficiency.
Additionally, we introduce the first HNVS benchmark, implementing a number of
new baselines based on recent SOTA RGB-NVS techniques, alongside the small
number of prior works on HNVS. We demonstrate HyperGS's robustness through
extensive evaluation of real and simulated hyperspectral scenes with a 14db
accuracy improvement upon previously published models.";Christopher Thirgood<author:sep>Oscar Mendez<author:sep>Erin Chao Ling<author:sep>Jon Storey<author:sep>Simon Hadfield;http://arxiv.org/pdf/2412.12849v1;cs.CV;;gaussian splatting
2412.12507v1;http://arxiv.org/abs/2412.12507v1;2024-12-17;3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian  Splatting;"3D Gaussian Splatting (3DGS) has shown great potential for efficient
reconstruction and high-fidelity real-time rendering of complex scenes on
consumer hardware. However, due to its rasterization-based formulation, 3DGS is
constrained to ideal pinhole cameras and lacks support for secondary lighting
effects. Recent methods address these limitations by tracing volumetric
particles instead, however, this comes at the cost of significantly slower
rendering speeds. In this work, we propose 3D Gaussian Unscented Transform
(3DGUT), replacing the EWA splatting formulation in 3DGS with the Unscented
Transform that approximates the particles through sigma points, which can be
projected exactly under any nonlinear projection function. This modification
enables trivial support of distorted cameras with time dependent effects such
as rolling shutter, while retaining the efficiency of rasterization.
Additionally, we align our rendering formulation with that of tracing-based
methods, enabling secondary ray tracing required to represent phenomena such as
reflections and refraction within the same 3D representation.";Qi Wu<author:sep>Janick Martinez Esturo<author:sep>Ashkan Mirzaei<author:sep>Nicolas Moenne-Loccoz<author:sep>Zan Gojcic;http://arxiv.org/pdf/2412.12507v1;cs.GR;;gaussian splatting
2412.12772v2;http://arxiv.org/abs/2412.12772v2;2024-12-17;Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior;"Neural Radiance Fields (NeRF) have advanced photorealistic novel view
synthesis, but their reliance on photometric reconstruction introduces
artifacts, commonly known as ""floaters"". These artifacts degrade novel view
quality, especially in areas unseen by the training cameras. We present a fast,
post-hoc NeRF cleanup method that eliminates such artifacts by enforcing our
Free Space Prior, effectively minimizing floaters without disrupting the NeRF's
representation of observed regions. Unlike existing approaches that rely on
either Maximum Likelihood (ML) estimation to fit the data or a complex, local
data-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,
selecting the optimal model parameters under a simple global prior assumption
that unseen regions should remain empty. This enables our method to clean
artifacts in both seen and unseen areas, enhancing novel view quality even in
challenging scene regions. Our method is comparable with existing NeRF cleanup
models while being 2.5x faster in inference time, requires no additional memory
beyond the original NeRF, and achieves cleanup training in less than 30
seconds. Our code will be made publically available.";Leo Segre<author:sep>Shai Avidan;http://arxiv.org/pdf/2412.12772v2;cs.CV;;nerf
2412.12766v1;http://arxiv.org/abs/2412.12766v1;2024-12-17;Towards a Training Free Approach for 3D Scene Editing;"Text driven diffusion models have shown remarkable capabilities in editing
images. However, when editing 3D scenes, existing works mostly rely on training
a NeRF for 3D editing. Recent NeRF editing methods leverages edit operations by
deploying 2D diffusion models and project these edits into 3D space. They
require strong positional priors alongside text prompt to identify the edit
location. These methods are operational on small 3D scenes and are more
generalized to particular scene. They require training for each specific edit
and cannot be exploited in real-time edits. To address these limitations, we
propose a novel method, FreeEdit, to make edits in training free manner using
mesh representations as a substitute for NeRF. Training-free methods are now a
possibility because of the advances in foundation model's space. We leverage
these models to bring a training-free alternative and introduce solutions for
insertion, replacement and deletion. We consider insertion, replacement and
deletion as basic blocks for performing intricate edits with certain
combinations of these operations. Given a text prompt and a 3D scene, our model
is capable of identifying what object should be inserted/replaced or deleted
and location where edit should be performed. We also introduce a novel
algorithm as part of FreeEdit to find the optimal location on grounding object
for placement. We evaluate our model by comparing it with baseline models on a
wide range of scenes using quantitative and qualitative metrics and showcase
the merits of our method with respect to others.";Vivek Madhavaram<author:sep>Shivangana Rawat<author:sep>Chaitanya Devaguptapu<author:sep>Charu Sharma<author:sep>Manohar Kaul;http://arxiv.org/pdf/2412.12766v1;cs.CV;;nerf
2412.12734v1;http://arxiv.org/abs/2412.12734v1;2024-12-17;Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures;"Gaussian Splatting has recently emerged as the go-to representation for
reconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian
primitives has further improved multi-view consistency and surface
reconstruction accuracy. In this work we highlight the similarity between 2D
Gaussian Splatting (2DGS) and billboards from traditional computer graphics.
Both use flat semi-transparent 2D geometry that is positioned, oriented and
scaled in 3D space. However 2DGS uses a solid color per splat and an opacity
modulated by a Gaussian distribution, where billboards are more expressive,
modulating the color with a uv-parameterized texture. We propose to unify these
concepts by presenting Gaussian Billboards, a modification of 2DGS to add
spatially-varying color achieved using per-splat texture interpolation. The
result is a mixture of the two representations, which benefits from both the
robust scene optimization power of 2DGS and the expressiveness of texture
mapping. We show that our method can improve the sharpness and quality of the
scene representation in a wide range of qualitative and quantitative
evaluations compared to the original 2DGS implementation.";Sebastian Weiss<author:sep>Derek Bradley;http://arxiv.org/pdf/2412.12734v1;cs.CV;;gaussian splatting
2412.11579v1;http://arxiv.org/abs/2412.11579v1;2024-12-16;SweepEvGS: Event-Based 3D Gaussian Splatting for Macro and Micro  Radiance Field Rendering from a Single Sweep;"Recent advancements in 3D Gaussian Splatting (3D-GS) have demonstrated the
potential of using 3D Gaussian primitives for high-speed, high-fidelity, and
cost-efficient novel view synthesis from continuously calibrated input views.
However, conventional methods require high-frame-rate dense and high-quality
sharp images, which are time-consuming and inefficient to capture, especially
in dynamic environments. Event cameras, with their high temporal resolution and
ability to capture asynchronous brightness changes, offer a promising
alternative for more reliable scene reconstruction without motion blur. In this
paper, we propose SweepEvGS, a novel hardware-integrated method that leverages
event cameras for robust and accurate novel view synthesis across various
imaging settings from a single sweep. SweepEvGS utilizes the initial static
frame with dense event streams captured during a single camera sweep to
effectively reconstruct detailed scene views. We also introduce different
real-world hardware imaging systems for real-world data collection and
evaluation for future research. We validate the robustness and efficiency of
SweepEvGS through experiments in three different imaging settings: synthetic
objects, real-world macro-level, and real-world micro-level view synthesis. Our
results demonstrate that SweepEvGS surpasses existing methods in visual
rendering quality, rendering speed, and computational efficiency, highlighting
its potential for dynamic practical applications.";Jingqian Wu<author:sep>Shuo Zhu<author:sep>Chutian Wang<author:sep>Boxin Shi<author:sep>Edmund Y. Lam;http://arxiv.org/pdf/2412.11579v1;cs.CV;;gaussian splatting
2412.12096v1;http://arxiv.org/abs/2412.12096v1;2024-12-16;PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting;"With the advent of portable 360{\deg} cameras, panorama has gained
significant attention in applications like virtual reality (VR), virtual tours,
robotics, and autonomous driving. As a result, wide-baseline panorama view
synthesis has emerged as a vital task, where high resolution, fast inference,
and memory efficiency are essential. Nevertheless, existing methods are
typically constrained to lower resolutions (512 $\times$ 1024) due to demanding
memory and computational requirements. In this paper, we present PanSplat, a
generalizable, feed-forward approach that efficiently supports resolution up to
4K (2048 $\times$ 4096). Our approach features a tailored spherical 3D Gaussian
pyramid with a Fibonacci lattice arrangement, enhancing image quality while
reducing information redundancy. To accommodate the demands of high resolution,
we propose a pipeline that integrates a hierarchical spherical cost volume and
Gaussian heads with local operations, enabling two-step deferred
backpropagation for memory-efficient training on a single A100 GPU. Experiments
demonstrate that PanSplat achieves state-of-the-art results with superior
efficiency and image quality across both synthetic and real-world datasets.
Code will be available at \url{https://github.com/chengzhag/PanSplat}.";Cheng Zhang<author:sep>Haofei Xu<author:sep>Qianyi Wu<author:sep>Camilo Cruz Gambardella<author:sep>Dinh Phung<author:sep>Jianfei Cai;http://arxiv.org/pdf/2412.12096v1;cs.CV;"Project Page: https://chengzhag.github.io/publication/pansplat/ Code:
  https://github.com/chengzhag/PanSplat";gaussian splatting
2412.12091v1;http://arxiv.org/abs/2412.12091v1;2024-12-16;Wonderland: Navigating 3D Scenes from a Single Image;"This paper addresses a challenging question: How can we efficiently create
high-quality, wide-scope 3D scenes from a single arbitrary image? Existing
methods face several constraints, such as requiring multi-view data,
time-consuming per-scene optimization, low visual quality in backgrounds, and
distorted reconstructions in unseen areas. We propose a novel pipeline to
overcome these limitations. Specifically, we introduce a large-scale
reconstruction model that uses latents from a video diffusion model to predict
3D Gaussian Splattings for the scenes in a feed-forward manner. The video
diffusion model is designed to create videos precisely following specified
camera trajectories, allowing it to generate compressed video latents that
contain multi-view information while maintaining 3D consistency. We train the
3D reconstruction model to operate on the video latent space with a progressive
training strategy, enabling the efficient generation of high-quality,
wide-scope, and generic 3D scenes. Extensive evaluations across various
datasets demonstrate that our model significantly outperforms existing methods
for single-view 3D scene generation, particularly with out-of-domain images.
For the first time, we demonstrate that a 3D reconstruction model can be
effectively built upon the latent space of a diffusion model to realize
efficient 3D scene generation.";Hanwen Liang<author:sep>Junli Cao<author:sep>Vidit Goel<author:sep>Guocheng Qian<author:sep>Sergei Korolev<author:sep>Demetri Terzopoulos<author:sep>Konstantinos N. Plataniotis<author:sep>Sergey Tulyakov<author:sep>Jian Ren;http://arxiv.org/pdf/2412.12091v1;cs.CV;Project page: https://snap-research.github.io/wonderland/;gaussian splatting
2412.11752v1;http://arxiv.org/abs/2412.11752v1;2024-12-16;Deformable Radial Kernel Splatting;"Recently, Gaussian splatting has emerged as a robust technique for
representing 3D scenes, enabling real-time rasterization and high-fidelity
rendering. However, Gaussians' inherent radial symmetry and smoothness
constraints limit their ability to represent complex shapes, often requiring
thousands of primitives to approximate detailed geometry. We introduce
Deformable Radial Kernel (DRK), which extends Gaussian splatting into a more
general and flexible framework. Through learnable radial bases with adjustable
angles and scales, DRK efficiently models diverse shape primitives while
enabling precise control over edge sharpness and boundary curvature. iven DRK's
planar nature, we further develop accurate ray-primitive intersection
computation for depth sorting and introduce efficient kernel culling strategies
for improved rasterization efficiency. Extensive experiments demonstrate that
DRK outperforms existing methods in both representation efficiency and
rendering quality, achieving state-of-the-art performance while dramatically
reducing primitive count.";Yi-Hua Huang<author:sep>Ming-Xian Lin<author:sep>Yang-Tian Sun<author:sep>Ziyi Yang<author:sep>Xiaoyang Lyu<author:sep>Yan-Pei Cao<author:sep>Xiaojuan Qi;http://arxiv.org/pdf/2412.11752v1;cs.CV;;gaussian splatting
2412.11762v1;http://arxiv.org/abs/2412.11762v1;2024-12-16;GS-ProCams: Gaussian Splatting-based Projector-Camera Systems;"We present GS-ProCams, the first Gaussian Splatting-based framework for
projector-camera systems (ProCams). GS-ProCams significantly enhances the
efficiency of projection mapping (PM) that requires establishing geometric and
radiometric mappings between the projector and the camera. Previous CNN-based
ProCams are constrained to a specific viewpoint, limiting their applicability
to novel perspectives. In contrast, NeRF-based ProCams support view-agnostic
projection mapping, however, they require an additional colocated light source
and demand significant computational and memory resources. To address this
issue, we propose GS-ProCams that employs 2D Gaussian for scene
representations, and enables efficient view-agnostic ProCams applications. In
particular, we explicitly model the complex geometric and photometric mappings
of ProCams using projector responses, the target surface's geometry and
materials represented by Gaussians, and global illumination component. Then, we
employ differentiable physically-based rendering to jointly estimate them from
captured multi-view projections. Compared to state-of-the-art NeRF-based
methods, our GS-ProCams eliminates the need for additional devices, achieving
superior ProCams simulation quality. It is also 600 times faster and uses only
1/10 of the GPU memory.";Qingyue Deng<author:sep>Jijiang Li<author:sep>Haibin Ling<author:sep>Bingyao Huang;http://arxiv.org/pdf/2412.11762v1;cs.CV;;gaussian splatting<tag:sep>nerf
2412.11525v2;http://arxiv.org/abs/2412.11525v2;2024-12-16;Sequence Matters: Harnessing Video Models in 3D Super-Resolution;"3D super-resolution aims to reconstruct high-fidelity 3D models from
low-resolution (LR) multi-view images. Early studies primarily focused on
single-image super-resolution (SISR) models to upsample LR images into
high-resolution images. However, these methods often lack view consistency
because they operate independently on each image. Although various
post-processing techniques have been extensively explored to mitigate these
inconsistencies, they have yet to fully resolve the issues. In this paper, we
perform a comprehensive study of 3D super-resolution by leveraging video
super-resolution (VSR) models. By utilizing VSR models, we ensure a higher
degree of spatial consistency and can reference surrounding spatial
information, leading to more accurate and detailed reconstructions. Our
findings reveal that VSR models can perform remarkably well even on sequences
that lack precise spatial alignment. Given this observation, we propose a
simple yet practical approach to align LR images without involving fine-tuning
or generating 'smooth' trajectory from the trained 3D models over LR images.
The experimental results show that the surprisingly simple algorithms can
achieve the state-of-the-art results of 3D super-resolution tasks on standard
benchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets.
Project page: https://ko-lani.github.io/Sequence-Matters";Hyun-kyu Ko<author:sep>Dongheok Park<author:sep>Youngin Park<author:sep>Byeonghyeon Lee<author:sep>Juhee Han<author:sep>Eunbyung Park;http://arxiv.org/pdf/2412.11525v2;cs.CV;Project page: https://ko-lani.github.io/Sequence-Matters;nerf
2412.11362v1;http://arxiv.org/abs/2412.11362v1;2024-12-16;VRVVC: Variable-Rate NeRF-Based Volumetric Video Compression;"Neural Radiance Field (NeRF)-based volumetric video has revolutionized visual
media by delivering photorealistic Free-Viewpoint Video (FVV) experiences that
provide audiences with unprecedented immersion and interactivity. However, the
substantial data volumes pose significant challenges for storage and
transmission. Existing solutions typically optimize NeRF representation and
compression independently or focus on a single fixed rate-distortion (RD)
tradeoff. In this paper, we propose VRVVC, a novel end-to-end joint
optimization variable-rate framework for volumetric video compression that
achieves variable bitrates using a single model while maintaining superior RD
performance. Specifically, VRVVC introduces a compact tri-plane implicit
residual representation for inter-frame modeling of long-duration dynamic
scenes, effectively reducing temporal redundancy. We further propose a
variable-rate residual representation compression scheme that leverages a
learnable quantization and a tiny MLP-based entropy model. This approach
enables variable bitrates through the utilization of predefined Lagrange
multipliers to manage the quantization error of all latent representations.
Finally, we present an end-to-end progressive training strategy combined with a
multi-rate-distortion loss function to optimize the entire framework. Extensive
experiments demonstrate that VRVVC achieves a wide range of variable bitrates
within a single model and surpasses the RD performance of existing methods
across various datasets.";Qiang Hu<author:sep>Houqiang Zhong<author:sep>Zihan Zheng<author:sep>Xiaoyun Zhang<author:sep>Zhengxue Cheng<author:sep>Li Song<author:sep>Guangtao Zhai<author:sep>Yanfeng Wang;http://arxiv.org/pdf/2412.11362v1;eess.IV;;nerf
2412.11520v1;http://arxiv.org/abs/2412.11520v1;2024-12-16;EditSplat: Multi-View Fusion and Attention-Guided Optimization for  View-Consistent 3D Scene Editing with 3D Gaussian Splatting;"Recent advancements in 3D editing have highlighted the potential of
text-driven methods in real-time, user-friendly AR/VR applications. However,
current methods rely on 2D diffusion models without adequately considering
multi-view information, resulting in multi-view inconsistency. While 3D
Gaussian Splatting (3DGS) significantly improves rendering quality and speed,
its 3D editing process encounters difficulties with inefficient optimization,
as pre-trained Gaussians retain excessive source information, hindering
optimization. To address these limitations, we propose \textbf{EditSplat}, a
novel 3D editing framework that integrates Multi-view Fusion Guidance (MFG) and
Attention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by
incorporating essential multi-view information into the diffusion process,
leveraging classifier-free guidance from the text-to-image diffusion model and
the geometric properties of 3DGS. Additionally, our AGT leverages the explicit
representation of 3DGS to selectively prune and optimize 3D Gaussians,
enhancing optimization efficiency and enabling precise, semantically rich local
edits. Through extensive qualitative and quantitative evaluations, EditSplat
achieves superior multi-view consistency and editing quality over existing
methods, significantly enhancing overall efficiency.";Dong In Lee<author:sep>Hyeongcheol Park<author:sep>Jiyoung Seo<author:sep>Eunbyung Park<author:sep>Hyunje Park<author:sep>Ha Dam Baek<author:sep>Shin Sangheon<author:sep>Sangmin kim<author:sep>Sangpil Kim;http://arxiv.org/pdf/2412.11520v1;cs.CV;;gaussian splatting
2412.10972v1;http://arxiv.org/abs/2412.10972v1;2024-12-14;DCSEG: Decoupled 3D Open-Set Segmentation using Gaussian Splatting;"Open-set 3D segmentation represents a major point of interest for multiple
downstream robotics and augmented/virtual reality applications. Recent advances
introduce 3D Gaussian Splatting as a computationally efficient representation
of the underlying scene. They enable the rendering of novel views while
achieving real-time display rates and matching the quality of computationally
far more expensive methods. We present a decoupled 3D segmentation pipeline to
ensure modularity and adaptability to novel 3D representations and semantic
segmentation foundation models. The pipeline proposes class-agnostic masks
based on a 3D reconstruction of the scene. Given the resulting class-agnostic
masks, we use a class-aware 2D foundation model to add class annotations to the
3D masks. We test this pipeline with 3D Gaussian Splatting and different 2D
segmentation models and achieve better performance than more tailored
approaches while also significantly increasing the modularity.";Luis Wiedmann<author:sep>Luca Wiehe<author:sep>David Rozenberszki;http://arxiv.org/pdf/2412.10972v1;cs.CV;;gaussian splatting
2412.09881v1;http://arxiv.org/abs/2412.09881v1;2024-12-13;Sharpening Your Density Fields: Spiking Neuron Aided Fast Geometry  Learning;"Neural Radiance Fields (NeRF) have achieved remarkable progress in neural
rendering. Extracting geometry from NeRF typically relies on the Marching Cubes
algorithm, which uses a hand-crafted threshold to define the level set.
However, this threshold-based approach requires laborious and scenario-specific
tuning, limiting its practicality for real-world applications. In this work, we
seek to enhance the efficiency of this method during the training time. To this
end, we introduce a spiking neuron mechanism that dynamically adjusts the
threshold, eliminating the need for manual selection. Despite its promise,
directly training with the spiking neuron often results in model collapse and
noisy outputs. To overcome these challenges, we propose a round-robin strategy
that stabilizes the training process and enables the geometry network to
achieve a sharper and more precise density distribution with minimal
computational overhead. We validate our approach through extensive experiments
on both synthetic and real-world datasets. The results show that our method
significantly improves the performance of threshold-based techniques, offering
a more robust and efficient solution for NeRF geometry extraction.";Yi Gu<author:sep>Zhaorui Wang<author:sep>Dongjun Ye<author:sep>Renjing Xu;http://arxiv.org/pdf/2412.09881v1;cs.CV;;nerf
2412.09982v2;http://arxiv.org/abs/2412.09982v2;2024-12-13;SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D  Gaussians from Monocular Video;"Synthesizing novel views from in-the-wild monocular videos is challenging due
to scene dynamics and the lack of multi-view cues. To address this, we propose
SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for
high-quality reconstruction and fast rendering from monocular videos. At its
core is a novel Motion-Adaptive Spline (MAS) method, which represents
continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a
small number of control points. For MAS, we introduce a Motion-Adaptive Control
points Pruning (MACP) method to model the deformation of each dynamic 3D
Gaussian across varying motions, progressively pruning control points while
maintaining dynamic modeling integrity. Additionally, we present a joint
optimization strategy for camera parameter estimation and 3D Gaussian
attributes, leveraging photometric and geometric consistency. This eliminates
the need for Structure-from-Motion preprocessing and enhances SplineGS's
robustness in real-world conditions. Experiments show that SplineGS
significantly outperforms state-of-the-art methods in novel view synthesis
quality for dynamic scenes from monocular videos, achieving thousands times
faster rendering speed.";Jongmin Park<author:sep>Minh-Quan Viet Bui<author:sep>Juan Luis Gonzalez Bello<author:sep>Jaeho Moon<author:sep>Jihyong Oh<author:sep>Munchurl Kim;http://arxiv.org/pdf/2412.09982v2;cs.CV;"The first two authors contributed equally to this work (equal
  contribution). The last two authors advised equally to this work. Please
  visit our project page at this https://kaist-viclab.github.io/splinegs-site/";gaussian splatting
2412.10231v1;http://arxiv.org/abs/2412.10231v1;2024-12-13;SuperGSeg: Open-Vocabulary 3D Segmentation with Structured  Super-Gaussians;"3D Gaussian Splatting has recently gained traction for its efficient training
and real-time rendering. While the vanilla Gaussian Splatting representation is
mainly designed for view synthesis, more recent works investigated how to
extend it with scene understanding and language features. However, existing
methods lack a detailed comprehension of scenes, limiting their ability to
segment and interpret complex structures. To this end, We introduce SuperGSeg,
a novel approach that fosters cohesive, context-aware scene representation by
disentangling segmentation and language field distillation. SuperGSeg first
employs neural Gaussians to learn instance and hierarchical segmentation
features from multi-view images with the aid of off-the-shelf 2D masks. These
features are then leveraged to create a sparse set of what we call
Super-Gaussians. Super-Gaussians facilitate the distillation of 2D language
features into 3D space. Through Super-Gaussians, our method enables
high-dimensional language feature rendering without extreme increases in GPU
memory. Extensive experiments demonstrate that SuperGSeg outperforms prior
works on both open-vocabulary object localization and semantic segmentation
tasks.";Siyun Liang<author:sep>Sen Wang<author:sep>Kunyi Li<author:sep>Michael Niemeyer<author:sep>Stefano Gasperini<author:sep>Nassir Navab<author:sep>Federico Tombari;http://arxiv.org/pdf/2412.10231v1;cs.CV;13 pages, 8 figures;gaussian splatting
2412.10004v1;http://arxiv.org/abs/2412.10004v1;2024-12-13;NeRF-Texture: Synthesizing Neural Radiance Field Textures;"Texture synthesis is a fundamental problem in computer graphics that would
benefit various applications. Existing methods are effective in handling 2D
image textures. In contrast, many real-world textures contain meso-structure in
the 3D geometry space, such as grass, leaves, and fabrics, which cannot be
effectively modeled using only 2D image textures. We propose a novel texture
synthesis method with Neural Radiance Fields (NeRF) to capture and synthesize
textures from given multi-view images. In the proposed NeRF texture
representation, a scene with fine geometric details is disentangled into the
meso-structure textures and the underlying base shape. This allows textures
with meso-structure to be effectively learned as latent features situated on
the base shape, which are fed into a NeRF decoder trained simultaneously to
represent the rich view-dependent appearance. Using this implicit
representation, we can synthesize NeRF-based textures through patch matching of
latent features. However, inconsistencies between the metrics of the
reconstructed content space and the latent feature space may compromise the
synthesis quality. To enhance matching performance, we further regularize the
distribution of latent features by incorporating a clustering constraint. In
addition to generating NeRF textures over a planar domain, our method can also
synthesize NeRF textures over curved surfaces, which are practically useful.
Experimental results and evaluations demonstrate the effectiveness of our
approach.";Yi-Hua Huang<author:sep>Yan-Pei Cao<author:sep>Yu-Kun Lai<author:sep>Ying Shan<author:sep>Lin Gao;http://arxiv.org/pdf/2412.10004v1;cs.CV;;nerf
2412.10209v1;http://arxiv.org/abs/2412.10209v1;2024-12-13;GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view  Diffusion;"We propose a novel approach for reconstructing animatable 3D Gaussian avatars
from monocular videos captured by commodity devices like smartphones.
Photorealistic 3D head avatar reconstruction from such recordings is
challenging due to limited observations, which leaves unobserved regions
under-constrained and can lead to artifacts in novel views. To address this
problem, we introduce a multi-view head diffusion model, leveraging its priors
to fill in missing regions and ensure view consistency in Gaussian splatting
renderings. To enable precise viewpoint control, we use normal maps rendered
from FLAME-based head reconstruction, which provides pixel-aligned inductive
biases. We also condition the diffusion model on VAE features extracted from
the input image to preserve details of facial identity and appearance. For
Gaussian avatar reconstruction, we distill multi-view diffusion priors by using
iteratively denoised images as pseudo-ground truths, effectively mitigating
over-saturation issues. To further improve photorealism, we apply latent
upsampling to refine the denoised latent before decoding it into an image. We
evaluate our method on the NeRSemble dataset, showing that GAF outperforms the
previous state-of-the-art methods in novel view synthesis by a 5.34\% higher
SSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructions
from monocular videos captured on commodity devices.";Jiapeng Tang<author:sep>Davide Davoli<author:sep>Tobias Kirschstein<author:sep>Liam Schoneveld<author:sep>Matthias Niessner;http://arxiv.org/pdf/2412.10209v1;cs.CV;"Paper Video: https://youtu.be/QuIYTljvhyg Project Page:
  https://tangjiapeng.github.io/projects/GAF";gaussian splatting
2412.09868v1;http://arxiv.org/abs/2412.09868v1;2024-12-13;RP-SLAM: Real-time Photorealistic SLAM with Efficient 3D Gaussian  Splatting;"3D Gaussian Splatting has emerged as a promising technique for high-quality
3D rendering, leading to increasing interest in integrating 3DGS into realism
SLAM systems. However, existing methods face challenges such as Gaussian
primitives redundancy, forgetting problem during continuous optimization, and
difficulty in initializing primitives in monocular case due to lack of depth
information. In order to achieve efficient and photorealistic mapping, we
propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular
and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian
primitives optimization and consists of three key components. Firstly, we
propose an efficient incremental mapping approach to achieve a compact and
accurate representation of the scene through adaptive sampling and Gaussian
primitives filtering. Secondly, a dynamic window optimization method is
proposed to mitigate the forgetting problem and improve map consistency.
Finally, for the monocular case, a monocular keyframe initialization method
based on sparse point cloud is proposed to improve the initialization accuracy
of Gaussian primitives, which provides a geometric basis for subsequent
optimization. The results of numerous experiments demonstrate that RP-SLAM
achieves state-of-the-art map rendering accuracy while ensuring real-time
performance and model compactness.";Lizhi Bai<author:sep>Chunqi Tian<author:sep>Jun Yang<author:sep>Siyu Zhang<author:sep>Masanori Suganuma<author:sep>Takayuki Okatani;http://arxiv.org/pdf/2412.09868v1;cs.RO;;gaussian splatting
2412.10051v1;http://arxiv.org/abs/2412.10051v1;2024-12-13;TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting  from Sparse Views;"Recent advances in Gaussian Splatting have significantly advanced the field,
achieving both panoptic and interactive segmentation of 3D scenes. However,
existing methodologies often overlook the critical need for reconstructing
specified targets with complex structures from sparse views. To address this
issue, we introduce TSGaussian, a novel framework that combines semantic
constraints with depth priors to avoid geometry degradation in challenging
novel view synthesis tasks. Our approach prioritizes computational resources on
designated targets while minimizing background allocation. Bounding boxes from
YOLOv9 serve as prompts for Segment Anything Model to generate 2D mask
predictions, ensuring semantic accuracy and cost efficiency. TSGaussian
effectively clusters 3D gaussians by introducing a compact identity encoding
for each Gaussian ellipsoid and incorporating 3D spatial consistency
regularization. Leveraging these modules, we propose a pruning strategy to
effectively reduce redundancy in 3D gaussians. Extensive experiments
demonstrate that TSGaussian outperforms state-of-the-art methods on three
standard datasets and a new challenging dataset we collected, achieving
superior results in novel view synthesis of specific objects. Code is available
at: https://github.com/leon2000-ai/TSGaussian.";Liang Zhao<author:sep>Zehan Bao<author:sep>Yi Xie<author:sep>Hong Chen<author:sep>Yaohui Chen<author:sep>Weifu Li;http://arxiv.org/pdf/2412.10051v1;cs.CV;;gaussian splatting
2412.09573v1;http://arxiv.org/abs/2412.09573v1;2024-12-12;FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D  Reconstruction;"Existing sparse-view reconstruction models heavily rely on accurate known
camera poses. However, deriving camera extrinsics and intrinsics from
sparse-view images presents significant challenges. In this work, we present
FreeSplatter, a highly scalable, feed-forward reconstruction framework capable
of generating high-quality 3D Gaussians from uncalibrated sparse-view images
and recovering their camera parameters in mere seconds. FreeSplatter is built
upon a streamlined transformer architecture, comprising sequential
self-attention blocks that facilitate information exchange among multi-view
image tokens and decode them into pixel-wise 3D Gaussian primitives. The
predicted Gaussian primitives are situated in a unified reference frame,
allowing for high-fidelity 3D modeling and instant camera parameter estimation
using off-the-shelf solvers. To cater to both object-centric and scene-level
reconstruction, we train two model variants of FreeSplatter on extensive
datasets. In both scenarios, FreeSplatter outperforms state-of-the-art
baselines in terms of reconstruction quality and pose estimation accuracy.
Furthermore, we showcase FreeSplatter's potential in enhancing the productivity
of downstream applications, such as text/image-to-3D content creation.";Jiale Xu<author:sep>Shenghua Gao<author:sep>Ying Shan;http://arxiv.org/pdf/2412.09573v1;cs.CV;Project page: https://bluestyle97.github.io/projects/freesplatter/;gaussian splatting
2412.09680v1;http://arxiv.org/abs/2412.09680v1;2024-12-12;PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields;"We tackle the ill-posed inverse rendering problem in 3D reconstruction with a
Neural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR)
theory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and
3D Gaussian Splatting approaches: they estimate view-dependent appearance
without modeling scene materials and illumination. To address this limitation,
we present an inverse rendering (IR) model capable of jointly estimating scene
geometry, materials, and illumination. Our model builds upon recent NeRF-based
IR approaches, but crucially introduces two novel physics-based priors that
better constrain the IR estimation. Our priors are rigorously formulated as
intuitive loss terms and achieve state-of-the-art material estimation without
compromising novel view synthesis quality. Our method is easily adaptable to
other inverse rendering and 3D reconstruction frameworks that require material
estimation. We demonstrate the importance of extending current neural rendering
approaches to fully model scene properties beyond geometry and view-dependent
appearance. Code is publicly available at https://github.com/s3anwu/pbrnerf";Sean Wu<author:sep>Shamik Basu<author:sep>Tim Broedermann<author:sep>Luc Van Gool<author:sep>Christos Sakaridis;http://arxiv.org/pdf/2412.09680v1;cs.CV;"16 pages, 7 figures. Code is publicly available at
  https://github.com/s3anwu/pbrnerf";gaussian splatting<tag:sep>nerf
2412.09511v1;http://arxiv.org/abs/2412.09511v1;2024-12-12;GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency;"Identifying affordance regions on 3D objects from semantic cues is essential
for robotics and human-machine interaction. However, existing 3D affordance
learning methods struggle with generalization and robustness due to limited
annotated data and a reliance on 3D backbones focused on geometric encoding,
which often lack resilience to real-world noise and data corruption. We propose
GEAL, a novel framework designed to enhance the generalization and robustness
of 3D affordance learning by leveraging large-scale pre-trained 2D models. We
employ a dual-branch architecture with Gaussian splatting to establish
consistent mappings between 3D point clouds and 2D representations, enabling
realistic 2D renderings from sparse point clouds. A granularity-adaptive fusion
module and a 2D-3D consistency alignment module further strengthen cross-modal
alignment and knowledge transfer, allowing the 3D branch to benefit from the
rich semantics and generalization capacity of 2D models. To holistically assess
the robustness, we introduce two new corruption-based benchmarks: PIAD-C and
LASO-C. Extensive experiments on public datasets and our benchmarks show that
GEAL consistently outperforms existing methods across seen and novel object
categories, as well as corrupted data, demonstrating robust and adaptable
affordance prediction under diverse conditions. Code and corruption datasets
have been made publicly available.";Dongyue Lu<author:sep>Lingdong Kong<author:sep>Tianxin Huang<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2412.09511v1;cs.CV;"22 pages, 8 figures, 12 tables; Project Page at
  https://dylanorange.github.io/projects/geal";gaussian splatting
2412.09606v1;http://arxiv.org/abs/2412.09606v1;2024-12-12;Feat2GS: Probing Visual Foundation Models with Gaussian Splatting;"Given that visual foundation models (VFMs) are trained on extensive datasets
but often limited to 2D images, a natural question arises: how well do they
understand the 3D world? With the differences in architecture and training
protocols (i.e., objectives, proxy tasks), a unified framework to fairly and
comprehensively probe their 3D awareness is urgently needed. Existing works on
3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or
two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately,
these tasks ignore texture awareness, and require 3D data as ground-truth,
which limits the scale and diversity of their evaluation set. To address these
issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM
features extracted from unposed images. This allows us to probe 3D awareness
for geometry and texture via novel view synthesis, without requiring 3D data.
Additionally, the disentanglement of 3DGS parameters - geometry
($\boldsymbol{x}, \alpha, \Sigma$) and texture ($\boldsymbol{c}$) - enables
separate analysis of texture and geometry awareness. Under Feat2GS, we conduct
extensive experiments to probe the 3D awareness of several VFMs, and
investigate the ingredients that lead to a 3D aware VFM. Building on these
findings, we develop several variants that achieve state-of-the-art across
diverse datasets. This makes Feat2GS useful for probing VFMs, and as a
simple-yet-effective baseline for novel-view synthesis. Code and data will be
made available at https://fanegg.github.io/Feat2GS/.";Yue Chen<author:sep>Xingyu Chen<author:sep>Anpei Chen<author:sep>Gerard Pons-Moll<author:sep>Yuliang Xiu;http://arxiv.org/pdf/2412.09606v1;cs.CV;Project Page: https://fanegg.github.io/Feat2GS/;gaussian splatting
2412.09176v1;http://arxiv.org/abs/2412.09176v1;2024-12-12;LIVE-GS: LLM Powers Interactive VR by Enhancing Gaussian Splatting;"Recently, radiance field rendering, such as 3D Gaussian Splatting (3DGS), has
shown immense potential in VR content creation due to its high-quality
rendering and efficient production process. However, existing physics-based
interaction systems for 3DGS can only perform simple and non-realistic
simulations or demand extensive user input for complex scenes, primarily due to
the absence of scene understanding. In this paper, we propose LIVE-GS, a highly
realistic interactive VR system powered by LLM. After object-aware GS
reconstruction, we prompt GPT-4o to analyze the physical properties of objects
in the scene, which are used to guide physical simulations consistent with real
phenomena. We also design a GPT-assisted GS inpainting module to fill the
unseen area covered by manipulative objects. To perform a precise segmentation
of Gaussian kernels, we propose a feature-mask segmentation strategy. To enable
rich interaction, we further propose a computationally efficient physical
simulation framework through an PBD-based unified interpolation method,
supporting various physical forms such as rigid body, soft body, and granular
materials. Our experimental results show that with the help of LLM's
understanding and enhancement of scenes, our VR system can support complex and
realistic interactions without additional manual design and annotation.";Haotian Mao<author:sep>Zhuoxiong Xu<author:sep>Siyue Wei<author:sep>Yule Quan<author:sep>Nianchen Deng<author:sep>Xubo Yang;http://arxiv.org/pdf/2412.09176v1;cs.HC;;gaussian splatting
2412.09723v1;http://arxiv.org/abs/2412.09723v1;2024-12-12;MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative  Ego-Motion and Photorealistic 3D Reconstruction;"Real-time multi-agent collaboration for ego-motion estimation and
high-fidelity 3D reconstruction is vital for scalable spatial intelligence.
However, traditional methods produce sparse, low-detail maps, while recent
dense mapping approaches struggle with high latency. To overcome these
challenges, we present MAC-Ego3D, a novel framework for real-time collaborative
photorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. MAC-Ego3D
enables agents to independently construct, align, and iteratively refine local
maps using a unified Gaussian splat representation. Through Intra-Agent
Gaussian Consensus, it enforces spatial coherence among neighboring Gaussian
splats within an agent. For global alignment, parallelized Inter-Agent Gaussian
Consensus, which asynchronously aligns and optimizes local maps by regularizing
multi-agent Gaussian splats, seamlessly integrates them into a high-fidelity 3D
model. Leveraging Gaussian primitives, MAC-Ego3D supports efficient RGB-D
rendering, enabling rapid inter-agent Gaussian association and alignment.
MAC-Ego3D bridges local precision and global coherence, delivering higher
efficiency, largely reducing localization error, and improving mapping
fidelity. It establishes a new SOTA on synthetic and real-world benchmarks,
achieving a 15x increase in inference speed, order-of-magnitude reductions in
ego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10
dB. Our code will be made publicly available at
https://github.com/Xiaohao-Xu/MAC-Ego3D .";Xiaohao Xu<author:sep>Feng Xue<author:sep>Shibo Zhao<author:sep>Yike Pan<author:sep>Sebastian Scherer<author:sep>Xiaonan Huang;http://arxiv.org/pdf/2412.09723v1;cs.CV;27 pages, 25 figures;
2412.09597v1;http://arxiv.org/abs/2412.09597v1;2024-12-12;LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video  Generation Priors;"Single-image 3D reconstruction remains a fundamental challenge in computer
vision due to inherent geometric ambiguities and limited viewpoint information.
Recent advances in Latent Video Diffusion Models (LVDMs) offer promising 3D
priors learned from large-scale video data. However, leveraging these priors
effectively faces three key challenges: (1) degradation in quality across large
camera motions, (2) difficulties in achieving precise camera control, and (3)
geometric distortions inherent to the diffusion process that damage 3D
consistency. We address these challenges by proposing LiftImage3D, a framework
that effectively releases LVDMs' generative priors while ensuring 3D
consistency. Specifically, we design an articulated trajectory strategy to
generate video frames, which decomposes video sequences with large camera
motions into ones with controllable small motions. Then we use robust neural
matching models, i.e. MASt3R, to calibrate the camera poses of generated frames
and produce corresponding point clouds. Finally, we propose a distortion-aware
3D Gaussian splatting representation, which can learn independent distortions
between frames and output undistorted canonical Gaussians. Extensive
experiments demonstrate that LiftImage3D achieves state-of-the-art performance
on two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, and
generalizes well to diverse in-the-wild images, from cartoon illustrations to
complex real-world scenes.";Yabo Chen<author:sep>Chen Yang<author:sep>Jiemin Fang<author:sep>Xiaopeng Zhang<author:sep>Lingxi Xie<author:sep>Wei Shen<author:sep>Wenrui Dai<author:sep>Hongkai Xiong<author:sep>Qi Tian;http://arxiv.org/pdf/2412.09597v1;cs.CV;Project page: https://liftimage3d.github.io/;gaussian splatting
2412.08331v1;http://arxiv.org/abs/2412.08331v1;2024-12-11;SLGaussian: Fast Language Gaussian Splatting in Sparse Views;"3D semantic field learning is crucial for applications like autonomous
navigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from
limited viewpoints is essential. Existing methods struggle under sparse view
conditions, relying on inefficient per-scene multi-view optimizations, which
are impractical for many real-world tasks. To address this, we propose
SLGaussian, a feed-forward method for constructing 3D semantic fields from
sparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring
consistent SAM segmentations through video tracking and using low-dimensional
indexing for high-dimensional CLIP features, SLGaussian efficiently embeds
language information in 3D space, offering a robust solution for accurate 3D
scene understanding under sparse view conditions. In experiments on two-view
sparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,
SLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,
and mIoU. Moreover, our model achieves scene inference in under 30 seconds and
open-vocabulary querying in just 0.011 seconds per query.";Kangjie Chen<author:sep>BingQuan Dai<author:sep>Minghan Qin<author:sep>Dongbin Zhang<author:sep>Peihao Li<author:sep>Yingshuang Zou<author:sep>Haoqian Wang;http://arxiv.org/pdf/2412.08331v1;cs.CV;;gaussian splatting
2412.08200v1;http://arxiv.org/abs/2412.08200v1;2024-12-11;GN-FR:Generalizable Neural Radiance Fields for Flare Removal;"Flare, an optical phenomenon resulting from unwanted scattering and
reflections within a lens system, presents a significant challenge in imaging.
The diverse patterns of flares, such as halos, streaks, color bleeding, and
haze, complicate the flare removal process. Existing traditional and
learning-based methods have exhibited limited efficacy due to their reliance on
single-image approaches, where flare removal is highly ill-posed. We address
this by framing flare removal as a multi-view image problem, taking advantage
of the view-dependent nature of flare artifacts. This approach leverages
information from neighboring views to recover details obscured by flare in
individual images. Our proposed framework, GN-FR (Generalizable Neural Radiance
Fields for Flare Removal), can render flare-free views from a sparse set of
input images affected by lens flare and generalizes across different scenes in
an unsupervised manner. GN-FR incorporates several modules within the
Generalizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation
(FMG), View Sampler (VS), and Point Sampler (PS). To overcome the
impracticality of capturing both flare-corrupted and flare-free data, we
introduce a masking loss function that utilizes mask information in an
unsupervised setting. Additionally, we present a 3D multi-view flare dataset,
comprising 17 real flare scenes with 782 images, 80 real flare patterns, and
their corresponding annotated flare-occupancy masks. To our knowledge, this is
the first work to address flare removal within a Neural Radiance Fields (NeRF)
framework.";Gopi Raju Matta<author:sep>Rahul Siddartha<author:sep>Rongali Simhachala Venkata Girish<author:sep>Sumit Sharma<author:sep>Kaushik Mitra;http://arxiv.org/pdf/2412.08200v1;cs.CV;;nerf
2412.08152v1;http://arxiv.org/abs/2412.08152v1;2024-12-11;ProGDF: Progressive Gaussian Differential Field for Controllable and  Flexible 3D Editing;"3D editing plays a crucial role in editing and reusing existing 3D assets,
thereby enhancing productivity. Recently, 3DGS-based methods have gained
increasing attention due to their efficient rendering and flexibility. However,
achieving desired 3D editing results often requires multiple adjustments in an
iterative loop, resulting in tens of minutes of training time cost for each
attempt and a cumbersome trial-and-error cycle for users. This in-the-loop
training paradigm results in a poor user experience. To address this issue, we
introduce the concept of process-oriented modelling for 3D editing and propose
the Progressive Gaussian Differential Field (ProGDF), an out-of-loop training
approach that requires only a single training session to provide users with
controllable editing capability and variable editing results through a
user-friendly interface in real-time. ProGDF consists of two key components:
Progressive Gaussian Splatting (PGS) and Gaussian Differential Field (GDF). PGS
introduces the progressive constraint to extract the diverse intermediate
results of the editing process and employs rendering quality regularization to
improve the quality of these results. Based on these intermediate results, GDF
leverages a lightweight neural network to model the editing process. Extensive
results on two novel applications, namely controllable 3D editing and flexible
fine-grained 3D manipulation, demonstrate the effectiveness, practicality and
flexibility of the proposed ProGDF.";Yian Zhao<author:sep>Wanshi Xu<author:sep>Yang Wu<author:sep>Weiheng Huang<author:sep>Zhongqian Sun<author:sep>Wei Yang;http://arxiv.org/pdf/2412.08152v1;cs.GR;;gaussian splatting
2412.08029v1;http://arxiv.org/abs/2412.08029v1;2024-12-11;NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF  and Neural View Synthesis Methods;"Neural View Synthesis (NVS) has demonstrated efficacy in generating
high-fidelity dense viewpoint videos using a image set with sparse views.
However, existing quality assessment methods like PSNR, SSIM, and LPIPS are not
tailored for the scenes with dense viewpoints synthesized by NVS and NeRF
variants, thus, they often fall short in capturing the perceptual quality,
including spatial and angular aspects of NVS-synthesized scenes. Furthermore,
the lack of dense ground truth views makes the full reference quality
assessment on NVS-synthesized scenes challenging. For instance, datasets such
as LLFF provide only sparse images, insufficient for complete full-reference
assessments. To address the issues above, we propose NeRF-NQA, the first
no-reference quality assessment method for densely-observed scenes synthesized
from the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment
strategy, integrating both viewwise and pointwise approaches, to evaluate the
quality of NVS-generated scenes. The viewwise approach assesses the spatial
quality of each individual synthesized view and the overall inter-views
consistency, while the pointwise approach focuses on the angular qualities of
scene surface points and their compound inter-point quality. Extensive
evaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality
assessment methods (from fields of image, video, and light-field assessment).
The results demonstrate NeRF-NQA outperforms the existing assessment methods
significantly and it shows substantial superiority on assessing NVS-synthesized
scenes without references. An implementation of this paper are available at
https://github.com/VincentQQu/NeRF-NQA.";Qiang Qu<author:sep>Hanxue Liang<author:sep>Xiaoming Chen<author:sep>Yuk Ying Chung<author:sep>Yiran Shen;http://arxiv.org/pdf/2412.08029v1;cs.CV;;nerf
2412.09648v1;http://arxiv.org/abs/2412.09648v1;2024-12-11;DSplats: 3D Generation by Denoising Splats-Based Multiview Diffusion  Models;"Generating high-quality 3D content requires models capable of learning robust
distributions of complex scenes and the real-world objects within them. Recent
Gaussian-based 3D reconstruction techniques have achieved impressive results in
recovering high-fidelity 3D assets from sparse input images by predicting 3D
Gaussians in a feed-forward manner. However, these techniques often lack the
extensive priors and expressiveness offered by Diffusion Models. On the other
hand, 2D Diffusion Models, which have been successfully applied to denoise
multiview images, show potential for generating a wide range of photorealistic
3D outputs but still fall short on explicit 3D priors and consistency. In this
work, we aim to bridge these two approaches by introducing DSplats, a novel
method that directly denoises multiview images using Gaussian Splat-based
Reconstructors to produce a diverse array of realistic 3D assets. To harness
the extensive priors of 2D Diffusion Models, we incorporate a pretrained Latent
Diffusion Model into the reconstructor backbone to predict a set of 3D
Gaussians. Additionally, the explicit 3D representation embedded in the
denoising network provides a strong inductive bias, ensuring geometrically
consistent novel view generation. Our qualitative and quantitative experiments
demonstrate that DSplats not only produces high-quality, spatially consistent
outputs, but also sets a new standard in single-image to 3D reconstruction.
When evaluated on the Google Scanned Objects dataset, DSplats achieves a PSNR
of 20.38, an SSIM of 0.842, and an LPIPS of 0.109.";Kevin Miao<author:sep>Harsh Agrawal<author:sep>Qihang Zhang<author:sep>Federico Semeraro<author:sep>Marco Cavallo<author:sep>Jiatao Gu<author:sep>Alexander Toshev;http://arxiv.org/pdf/2412.09648v1;eess.IV;;
2412.07739v1;http://arxiv.org/abs/2412.07739v1;2024-12-10;GASP: Gaussian Avatars with Synthetic Priors;"Gaussian Splatting has changed the game for real-time photo-realistic
rendering. One of the most popular applications of Gaussian Splatting is to
create animatable avatars, known as Gaussian Avatars. Recent works have pushed
the boundaries of quality and rendering efficiency but suffer from two main
limitations. Either they require expensive multi-camera rigs to produce avatars
with free-view rendering, or they can be trained with a single camera but only
rendered at high quality from this fixed viewpoint. An ideal model would be
trained using a short monocular video or image from available hardware, such as
a webcam, and rendered from any view. To this end, we propose GASP: Gaussian
Avatars with Synthetic Priors. To overcome the limitations of existing
datasets, we exploit the pixel-perfect nature of synthetic data to train a
Gaussian Avatar prior. By fitting this prior model to a single photo or video
and fine-tuning it, we get a high-quality Gaussian Avatar, which supports
360$^\circ$ rendering. Our prior is only required for fitting, not inference,
enabling real-time application. Through our method, we obtain high-quality,
animatable Avatars from limited data which can be animated and rendered at
70fps on commercial hardware. See our project page
(https://microsoft.github.io/GASP/) for results.";Jack Saunders<author:sep>Charlie Hewitt<author:sep>Yanan Jian<author:sep>Marek Kowalski<author:sep>Tadas Baltrusaitis<author:sep>Yiye Chen<author:sep>Darren Cosker<author:sep>Virginia Estellers<author:sep>Nicholas Gyde<author:sep>Vinay P. Namboodiri<author:sep>Benjamin E Lundell;http://arxiv.org/pdf/2412.07739v1;cs.CV;Project page: https://microsoft.github.io/GASP/;gaussian splatting
2412.07984v1;http://arxiv.org/abs/2412.07984v1;2024-12-10;Diffusion-Based Attention Warping for Consistent 3D Scene Editing;"We present a novel method for 3D scene editing using diffusion models,
designed to ensure view consistency and realism across perspectives. Our
approach leverages attention features extracted from a single reference image
to define the intended edits. These features are warped across multiple views
by aligning them with scene geometry derived from Gaussian splatting depth
estimates. Injecting these warped features into other viewpoints enables
coherent propagation of edits, achieving high fidelity and spatial alignment in
3D space. Extensive evaluations demonstrate the effectiveness of our method in
generating versatile edits of 3D scenes, significantly advancing the
capabilities of scene manipulation compared to the existing methods. Project
page: \url{https://attention-warp.github.io}";Eyal Gomel<author:sep>Lior Wolf;http://arxiv.org/pdf/2412.07984v1;cs.CV;;gaussian splatting
2412.07293v1;http://arxiv.org/abs/2412.07293v1;2024-12-10;EventSplat: 3D Gaussian Splatting from Moving Event Cameras for  Real-time Rendering;"We introduce a method for using event camera data in novel view synthesis via
Gaussian Splatting. Event cameras offer exceptional temporal resolution and a
high dynamic range. Leveraging these capabilities allows us to effectively
address the novel view synthesis challenge in the presence of fast camera
motion. For initialization of the optimization process, our approach uses prior
knowledge encoded in an event-to-video model. We also use spline interpolation
for obtaining high quality poses along the event camera trajectory. This
enhances the reconstruction quality from fast-moving cameras while overcoming
the computational limitations traditionally associated with event-based Neural
Radiance Field (NeRF) methods. Our experimental evaluation demonstrates that
our results achieve higher visual fidelity and better performance than existing
event-based NeRF approaches while being an order of magnitude faster to render.";Toshiya Yura<author:sep>Ashkan Mirzaei<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2412.07293v1;cs.CV;;gaussian splatting<tag:sep>nerf
2412.07494v1;http://arxiv.org/abs/2412.07494v1;2024-12-10;ResGS: Residual Densification of 3D Gaussian for Efficient Detail  Recovery;"Recently, 3D Gaussian Splatting (3D-GS) has prevailed in novel view
synthesis, achieving high fidelity and efficiency. However, it often struggles
to capture rich details and complete geometry. Our analysis highlights a key
limitation of 3D-GS caused by the fixed threshold in densification, which
balances geometry coverage against detail recovery as the threshold varies. To
address this, we introduce a novel densification method, residual split, which
adds a downscaled Gaussian as a residual. Our approach is capable of adaptively
retrieving details and complementing missing geometry while enabling
progressive refinement. To further support this method, we propose a pipeline
named ResGS. Specifically, we integrate a Gaussian image pyramid for
progressive supervision and implement a selection scheme that prioritizes the
densification of coarse Gaussians over time. Extensive experiments demonstrate
that our method achieves SOTA rendering quality. Consistent performance
improvements can be achieved by applying our residual split on various 3D-GS
variants, underscoring its versatility and potential for broader application in
3D-GS-based applications.";Yanzhe Lyu<author:sep>Kai Cheng<author:sep>Xin Kang<author:sep>Xuejin Chen;http://arxiv.org/pdf/2412.07494v1;cs.CV;;gaussian splatting
2412.07660v1;http://arxiv.org/abs/2412.07660v1;2024-12-10;Proc-GS: Procedural Building Generation for City Assembly with 3D  Gaussians;"Buildings are primary components of cities, often featuring repeated elements
such as windows and doors. Traditional 3D building asset creation is
labor-intensive and requires specialized skills to develop design rules. Recent
generative models for building creation often overlook these patterns, leading
to low visual fidelity and limited scalability. Drawing inspiration from
procedural modeling techniques used in the gaming and visual effects industry,
our method, Proc-GS, integrates procedural code into the 3D Gaussian Splatting
(3D-GS) framework, leveraging their advantages in high-fidelity rendering and
efficient asset management from both worlds. By manipulating procedural code,
we can streamline this process and generate an infinite variety of buildings.
This integration significantly reduces model size by utilizing shared
foundational assets, enabling scalable generation with precise control over
building assembly. We showcase the potential for expansive cityscape generation
while maintaining high rendering fidelity and precise control on both real and
synthetic cases.";Yixuan Li<author:sep>Xingjian Ran<author:sep>Linning Xu<author:sep>Tao Lu<author:sep>Mulin Yu<author:sep>Zhenzhi Wang<author:sep>Yuanbo Xiangli<author:sep>Dahua Lin<author:sep>Bo Dai;http://arxiv.org/pdf/2412.07660v1;cs.CV;Project page: https://city-super.github.io/procgs/;gaussian splatting
2412.07608v1;http://arxiv.org/abs/2412.07608v1;2024-12-10;Faster and Better 3D Splatting via Group Training;"3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel
view synthesis, demonstrating remarkable capability in high-fidelity scene
reconstruction through its Gaussian primitive representations. However, the
computational overhead induced by the massive number of primitives poses a
significant bottleneck to training efficiency. To overcome this challenge, we
propose Group Training, a simple yet effective strategy that organizes Gaussian
primitives into manageable groups, optimizing training efficiency and improving
rendering quality. This approach shows universal compatibility with existing
3DGS frameworks, including vanilla 3DGS and Mip-Splatting, consistently
achieving accelerated training while maintaining superior synthesis quality.
Extensive experiments reveal that our straightforward Group Training strategy
achieves up to 30% faster convergence and improved rendering quality across
diverse scenarios.";Chengbo Wang<author:sep>Guozheng Ma<author:sep>Yifei Xue<author:sep>Yizhen Lao;http://arxiv.org/pdf/2412.07608v1;cs.CV;;gaussian splatting
2412.06424v1;http://arxiv.org/abs/2412.06424v1;2024-12-09;Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video;"Recent 4D reconstruction methods have yielded impressive results but rely on
sharp videos as supervision. However, motion blur often occurs in videos due to
camera shake and object movement, while existing methods render blurry results
when using such videos for reconstructing 4D models. Although a few NeRF-based
approaches attempted to address the problem, they struggled to produce
high-quality results, due to the inaccuracy in estimating continuous dynamic
representations within the exposure time. Encouraged by recent works in 3D
motion trajectory modeling using 3D Gaussian Splatting (3DGS), we suggest
taking 3DGS as the scene representation manner, and propose the first 4D
Gaussian Splatting framework to reconstruct a high-quality 4D model from blurry
monocular video, named Deblur4DGS. Specifically, we transform continuous
dynamic representations estimation within an exposure time into the exposure
time estimation. Moreover, we introduce exposure regularization to avoid
trivial solutions, as well as multi-frame and multi-resolution consistency ones
to alleviate artifacts. Furthermore, to better represent objects with large
motion, we suggest blur-aware variable canonical Gaussians. Beyond novel-view
synthesis, Deblur4DGS can be applied to improve blurry video from multiple
perspectives, including deblurring, frame interpolation, and video
stabilization. Extensive experiments on the above four tasks show that
Deblur4DGS outperforms state-of-the-art 4D reconstruction methods. The codes
are available at https://github.com/ZcsrenlongZ/Deblur4DGS.";Renlong Wu<author:sep>Zhilu Zhang<author:sep>Mingyang Chen<author:sep>Xiaopeng Fan<author:sep>Zifei Yan<author:sep>Wangmeng Zuo;http://arxiv.org/pdf/2412.06424v1;cs.CV;17 pages;gaussian splatting<tag:sep>nerf
2412.06250v1;http://arxiv.org/abs/2412.06250v1;2024-12-09;Splatter-360: Generalizable 360$^{\circ}$ Gaussian Splatting for  Wide-baseline Panoramic Images;"Wide-baseline panoramic images are frequently used in applications like VR
and simulations to minimize capturing labor costs and storage needs. However,
synthesizing novel views from these panoramic images in real time remains a
significant challenge, especially due to panoramic imagery's high resolution
and inherent distortions. Although existing 3D Gaussian splatting (3DGS)
methods can produce photo-realistic views under narrow baselines, they often
overfit the training views when dealing with wide-baseline panoramic images due
to the difficulty in learning precise geometry from sparse 360$^{\circ}$ views.
This paper presents \textit{Splatter-360}, a novel end-to-end generalizable
3DGS framework designed to handle wide-baseline panoramic images. Unlike
previous approaches, \textit{Splatter-360} performs multi-view matching
directly in the spherical domain by constructing a spherical cost volume
through a spherical sweep algorithm, enhancing the network's depth perception
and geometry estimation. Additionally, we introduce a 3D-aware bi-projection
encoder to mitigate the distortions inherent in panoramic images and integrate
cross-view attention to improve feature interactions across multiple
viewpoints. This enables robust 3D-aware feature representations and real-time
rendering capabilities. Experimental results on the HM3D~\cite{hm3d} and
Replica~\cite{replica} demonstrate that \textit{Splatter-360} significantly
outperforms state-of-the-art NeRF and 3DGS methods (e.g., PanoGRF, MVSplat,
DepthSplat, and HiSplat) in both synthesis quality and generalization
performance for wide-baseline panoramic images. Code and trained models are
available at \url{https://3d-aigc.github.io/Splatter-360/}.";Zheng Chen<author:sep>Chenming Wu<author:sep>Zhelun Shen<author:sep>Chen Zhao<author:sep>Weicai Ye<author:sep>Haocheng Feng<author:sep>Errui Ding<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2412.06250v1;cs.CV;"Project page:https://3d-aigc.github.io/Splatter-360/. Code:
  https://github.com/thucz/splatter360";gaussian splatting<tag:sep>nerf
2412.06299v1;http://arxiv.org/abs/2412.06299v1;2024-12-09;4D Gaussian Splatting with Scale-aware Residual Field and Adaptive  Optimization for Real-time Rendering of Temporally Complex Dynamic Scenes;"Reconstructing dynamic scenes from video sequences is a highly promising task
in the multimedia domain. While previous methods have made progress, they often
struggle with slow rendering and managing temporal complexities such as
significant motion and object appearance/disappearance. In this paper, we
propose SaRO-GS as a novel dynamic scene representation capable of achieving
real-time rendering while effectively handling temporal complexities in dynamic
scenes. To address the issue of slow rendering speed, we adopt a Gaussian
primitive-based representation and optimize the Gaussians in 4D space, which
facilitates real-time rendering with the assistance of 3D Gaussian Splatting.
Additionally, to handle temporally complex dynamic scenes, we introduce a
Scale-aware Residual Field. This field considers the size information of each
Gaussian primitive while encoding its residual feature and aligns with the
self-splitting behavior of Gaussian primitives. Furthermore, we propose an
Adaptive Optimization Schedule, which assigns different optimization strategies
to Gaussian primitives based on their distinct temporal properties, thereby
expediting the reconstruction of dynamic regions. Through evaluations on
monocular and multi-view datasets, our method has demonstrated state-of-the-art
performance. Please see our project page at
https://yjb6.github.io/SaRO-GS.github.io.";Jinbo Yan<author:sep>Rui Peng<author:sep>Luyang Tang<author:sep>Ronggang Wang;http://arxiv.org/pdf/2412.06299v1;cs.CV;;gaussian splatting
2412.06981v1;http://arxiv.org/abs/2412.06981v1;2024-12-09;Diffusing Differentiable Representations;"We introduce a novel, training-free method for sampling differentiable
representations (diffreps) using pretrained diffusion models. Rather than
merely mode-seeking, our method achieves sampling by ""pulling back"" the
dynamics of the reverse-time process--from the image space to the diffrep
parameter space--and updating the parameters according to this pulled-back
process. We identify an implicit constraint on the samples induced by the
diffrep and demonstrate that addressing this constraint significantly improves
the consistency and detail of the generated objects. Our method yields diffreps
with substantially improved quality and diversity for images, panoramas, and 3D
NeRFs compared to existing techniques. Our approach is a general-purpose method
for sampling diffreps, expanding the scope of problems that diffusion models
can tackle.";Yash Savani<author:sep>Marc Finzi<author:sep>J. Zico Kolter;http://arxiv.org/pdf/2412.06981v1;cs.CV;Published at NeurIPS 2024;nerf
2412.06257v2;http://arxiv.org/abs/2412.06257v2;2024-12-09;Advancing Extended Reality with 3D Gaussian Splatting: Innovations and  Prospects;"3D Gaussian Splatting (3DGS) has attracted significant attention for its
potential to revolutionize 3D representation, rendering, and interaction.
Despite the rapid growth of 3DGS research, its direct application to Extended
Reality (XR) remains underexplored. Although many studies recognize the
potential of 3DGS for XR, few have explicitly focused on or demonstrated its
effectiveness within XR environments. In this paper, we aim to synthesize
innovations in 3DGS that show specific potential for advancing XR research and
development. We conduct a comprehensive review of publicly available 3DGS
papers, with a focus on those referencing XR-related concepts. Additionally, we
perform an in-depth analysis of innovations explicitly relevant to XR and
propose a taxonomy to highlight their significance. Building on these insights,
we propose several prospective XR research areas where 3DGS can make promising
contributions, yet remain rarely touched. By investigating the intersection of
3DGS and XR, this paper provides a roadmap to push the boundaries of XR using
cutting-edge 3DGS techniques.";Shi Qiu<author:sep>Binzhu Xie<author:sep>Qixuan Liu<author:sep>Pheng-Ann Heng;http://arxiv.org/pdf/2412.06257v2;cs.CV;IEEE AIxVR 2025;gaussian splatting
2412.06234v2;http://arxiv.org/abs/2412.06234v2;2024-12-09;Generative Densification: Learning to Densify Gaussians for  High-Fidelity Generalizable 3D Reconstruction;"Generalized feed-forward Gaussian models have achieved significant progress
in sparse-view 3D reconstruction by leveraging prior knowledge from large
multi-view datasets. However, these models often struggle to represent
high-frequency details due to the limited number of Gaussians. While the
densification strategy used in per-scene 3D Gaussian splatting (3D-GS)
optimization can be adapted to the feed-forward models, it may not be ideally
suited for generalized scenarios. In this paper, we propose Generative
Densification, an efficient and generalizable method to densify Gaussians
generated by feed-forward models. Unlike the 3D-GS densification strategy,
which iteratively splits and clones raw Gaussian parameters, our method
up-samples feature representations from the feed-forward models and generates
their corresponding fine Gaussians in a single forward pass, leveraging the
embedded prior knowledge for enhanced generalization. Experimental results on
both object-level and scene-level reconstruction tasks demonstrate that our
method outperforms state-of-the-art approaches with comparable or smaller model
sizes, achieving notable improvements in representing fine details.";Seungtae Nam<author:sep>Xiangyu Sun<author:sep>Gyeongjin Kang<author:sep>Younggeun Lee<author:sep>Seungjun Oh<author:sep>Eunbyung Park;http://arxiv.org/pdf/2412.06234v2;cs.CV;Project page: https://stnamjef.github.io/GenerativeDensification/;gaussian splatting
2412.06770v1;http://arxiv.org/abs/2412.06770v1;2024-12-09;Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view  Event Cameras;"Volumetric reconstruction of dynamic scenes is an important problem in
computer vision. It is especially challenging in poor lighting and with fast
motion. It is partly due to the limitations of RGB cameras: To capture fast
motion without much blur, the framerate must be increased, which in turn
requires more lighting. In contrast, event cameras, which record changes in
pixel brightness asynchronously, are much less dependent on lighting, making
them more suitable for recording fast motion. We hence propose the first method
to spatiotemporally reconstruct a scene from sparse multi-view event streams
and sparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF
models, one per short recording segment. The individual segments are supervised
with a set of event- and RGB-based losses and sparse-view regularisation. We
assemble a real-world multi-view camera rig with six static event cameras
around the object and record a benchmark multi-view event stream dataset of
challenging motions. Our work outperforms RGB-based baselines, producing
state-of-the-art results, and opens up the topic of multi-view event-based
reconstruction as a new path for fast scene capture beyond RGB cameras. The
code and the data will be released soon at
https://4dqv.mpi-inf.mpg.de/DynEventNeRF/";Viktor Rudnev<author:sep>Gereon Fox<author:sep>Mohamed Elgharib<author:sep>Christian Theobalt<author:sep>Vladislav Golyanik;http://arxiv.org/pdf/2412.06770v1;cs.CV;15 pages, 11 figures, 6 tables;nerf
2412.06974v1;http://arxiv.org/abs/2412.06974v1;2024-12-09;MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2  Seconds;"Recent sparse multi-view scene reconstruction advances like DUSt3R and MASt3R
no longer require camera calibration and camera pose estimation. However, they
only process a pair of views at a time to infer pixel-aligned pointmaps. When
dealing with more than two views, a combinatorial number of error prone
pairwise reconstructions are usually followed by an expensive global
optimization, which often fails to rectify the pairwise reconstruction errors.
To handle more views, reduce errors, and improve inference time, we propose the
fast single-stage feed-forward network MV-DUSt3R. At its core are multi-view
decoder blocks which exchange information across any number of views while
considering one reference view. To make our method robust to reference view
selection, we further propose MV-DUSt3R+, which employs cross-reference-view
blocks to fuse information across different reference view choices. To further
enable novel view synthesis, we extend both by adding and jointly training
Gaussian splatting heads. Experiments on multi-view stereo reconstruction,
multi-view pose estimation, and novel view synthesis confirm that our methods
improve significantly upon prior art. Code will be released.";Zhenggang Tang<author:sep>Yuchen Fan<author:sep>Dilin Wang<author:sep>Hongyu Xu<author:sep>Rakesh Ranjan<author:sep>Alexander Schwing<author:sep>Zhicheng Yan;http://arxiv.org/pdf/2412.06974v1;cs.CV;;gaussian splatting
2412.05969v2;http://arxiv.org/abs/2412.05969v2;2024-12-08;Efficient Semantic Splatting for Remote Sensing Multi-view Segmentation;"In this paper, we propose a novel semantic splatting approach based on
Gaussian Splatting to achieve efficient and low-latency. Our method projects
the RGB attributes and semantic features of point clouds onto the image plane,
simultaneously rendering RGB images and semantic segmentation results.
Leveraging the explicit structure of point clouds and a one-time rendering
strategy, our approach significantly enhances efficiency during optimization
and rendering. Additionally, we employ SAM2 to generate pseudo-labels for
boundary regions, which often lack sufficient supervision, and introduce
two-level aggregation losses at the 2D feature map and 3D spatial levels to
improve the view-consistent and spatial continuity.";Zipeng Qi<author:sep>Hao Chen<author:sep>Haotian Zhang<author:sep>Zhengxia Zou<author:sep>Zhenwei Shi;http://arxiv.org/pdf/2412.05969v2;cs.CV;;gaussian splatting
2412.05908v1;http://arxiv.org/abs/2412.05908v1;2024-12-08;GBR: Generative Bundle Refinement for High-fidelity Gaussian Splatting  and Meshing;"Gaussian splatting has gained attention for its efficient representation and
rendering of 3D scenes using continuous Gaussian primitives. However, it
struggles with sparse-view inputs due to limited geometric and photometric
information, causing ambiguities in depth, shape, and texture.
  we propose GBR: Generative Bundle Refinement, a method for high-fidelity
Gaussian splatting and meshing using only 4-6 input views. GBR integrates a
neural bundle adjustment module to enhance geometry accuracy and a generative
depth refinement module to improve geometry fidelity. More specifically, the
neural bundle adjustment module integrates a foundation network to produce
initial 3D point maps and point matches from unposed images, followed by bundle
adjustment optimization to improve multiview consistency and point cloud
accuracy. The generative depth refinement module employs a diffusion-based
strategy to enhance geometric details and fidelity while preserving the scale.
Finally, for Gaussian splatting optimization, we propose a multimodal loss
function incorporating depth and normal consistency, geometric regularization,
and pseudo-view supervision, providing robust guidance under sparse-view
conditions. Experiments on widely used datasets show that GBR significantly
outperforms existing methods under sparse-view inputs. Additionally, GBR
demonstrates the ability to reconstruct and render large-scale real-world
scenes, such as the Pavilion of Prince Teng and the Great Wall, with remarkable
details using only 6 views.";Jianing Zhang<author:sep>Yuchao Zheng<author:sep>Ziwei Li<author:sep>Qionghai Dai<author:sep>Xiaoyun Yuan;http://arxiv.org/pdf/2412.05908v1;cs.CV;;gaussian splatting
2412.05700v1;http://arxiv.org/abs/2412.05700v1;2024-12-07;Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes;"Recent advancements in high-fidelity dynamic scene reconstruction have
leveraged dynamic 3D Gaussians and 4D Gaussian Splatting for realistic scene
representation. However, to make these methods viable for real-time
applications such as AR/VR, gaming, and rendering on low-power devices,
substantial reductions in memory usage and improvements in rendering efficiency
are required. While many state-of-the-art methods prioritize lightweight
implementations, they struggle in handling scenes with complex motions or long
sequences. In this work, we introduce Temporally Compressed 3D Gaussian
Splatting (TC3DGS), a novel technique designed specifically to effectively
compress dynamic 3D Gaussian representations. TC3DGS selectively prunes
Gaussians based on their temporal relevance and employs gradient-aware
mixed-precision quantization to dynamically compress Gaussian parameters. It
additionally relies on a variation of the Ramer-Douglas-Peucker algorithm in a
post-processing step to further reduce storage by interpolating Gaussian
trajectories across frames. Our experiments across multiple datasets
demonstrate that TC3DGS achieves up to 67$\times$ compression with minimal or
no degradation in visual quality.";Saqib Javed<author:sep>Ahmad Jarrar Khan<author:sep>Corentin Dumery<author:sep>Chen Zhao<author:sep>Mathieu Salzmann;http://arxiv.org/pdf/2412.05700v1;cs.CV;Code will be released soon;gaussian splatting
2412.05546v1;http://arxiv.org/abs/2412.05546v1;2024-12-07;Radiant: Large-scale 3D Gaussian Rendering based on Hierarchical  Framework;"With the advancement of computer vision, the recently emerged 3D Gaussian
Splatting (3DGS) has increasingly become a popular scene reconstruction
algorithm due to its outstanding performance. Distributed 3DGS can efficiently
utilize edge devices to directly train on the collected images, thereby
offloading computational demands and enhancing efficiency. However, traditional
distributed frameworks often overlook computational and communication
challenges in real-world environments, hindering large-scale deployment and
potentially posing privacy risks. In this paper, we propose Radiant, a
hierarchical 3DGS algorithm designed for large-scale scene reconstruction that
considers system heterogeneity, enhancing the model performance and training
efficiency. Via extensive empirical study, we find that it is crucial to
partition the regions for each edge appropriately and allocate varying camera
positions to each device for image collection and training. The core of Radiant
is partitioning regions based on heterogeneous environment information and
allocating workloads to each device accordingly. Furthermore, we provide a 3DGS
model aggregation algorithm that enhances the quality and ensures the
continuity of models' boundaries. Finally, we develop a testbed, and
experiments demonstrate that Radiant improved reconstruction quality by up to
25.7\% and reduced up to 79.6\% end-to-end latency.";Haosong Peng<author:sep>Tianyu Qi<author:sep>Yufeng Zhan<author:sep>Hao Li<author:sep>Yalun Dai<author:sep>Yuanqing Xia;http://arxiv.org/pdf/2412.05546v1;cs.CV;;
2412.05570v1;http://arxiv.org/abs/2412.05570v1;2024-12-07;Template-free Articulated Gaussian Splatting for Real-time Reposable  Dynamic View Synthesis;"While novel view synthesis for dynamic scenes has made significant progress,
capturing skeleton models of objects and re-posing them remains a challenging
task. To tackle this problem, in this paper, we propose a novel approach to
automatically discover the associated skeleton model for dynamic objects from
videos without the need for object-specific templates. Our approach utilizes 3D
Gaussian Splatting and superpoints to reconstruct dynamic objects. Treating
superpoints as rigid parts, we can discover the underlying skeleton model
through intuitive cues and optimize it using the kinematic model. Besides, an
adaptive control strategy is applied to avoid the emergence of redundant
superpoints. Extensive experiments demonstrate the effectiveness and efficiency
of our method in obtaining re-posable 3D objects. Not only can our approach
achieve excellent visual fidelity, but it also allows for the real-time
rendering of high-resolution images.";Diwen Wan<author:sep>Yuxiang Wang<author:sep>Ruijie Lu<author:sep>Gang Zeng;http://arxiv.org/pdf/2412.05570v1;cs.CV;Accepted by NeurIPS 2024;gaussian splatting
2412.05560v1;http://arxiv.org/abs/2412.05560v1;2024-12-07;Text-to-3D Gaussian Splatting with Physics-Grounded Motion Generation;"Text-to-3D generation is a valuable technology in virtual reality and digital
content creation. While recent works have pushed the boundaries of text-to-3D
generation, producing high-fidelity 3D objects with inefficient prompts and
simulating their physics-grounded motion accurately still remain unsolved
challenges. To address these challenges, we present an innovative framework
that utilizes the Large Language Model (LLM)-refined prompts and diffusion
priors-guided Gaussian Splatting (GS) for generating 3D models with accurate
appearances and geometric structures. We also incorporate a continuum
mechanics-based deformation map and color regularization to synthesize vivid
physics-grounded motion for the generated 3D Gaussians, adhering to the
conservation of mass and momentum. By integrating text-to-3D generation with
physics-grounded motion synthesis, our framework renders photo-realistic 3D
objects that exhibit physics-aware motion, accurately reflecting the behaviors
of the objects under various forces and constraints across different materials.
Extensive experiments demonstrate that our approach achieves high-quality 3D
generations with realistic physics-grounded motion.";Wenqing Wang<author:sep>Yun Fu;http://arxiv.org/pdf/2412.05560v1;cs.CV;;gaussian splatting
2412.05695v1;http://arxiv.org/abs/2412.05695v1;2024-12-07;WATER-GS: Toward Copyright Protection for 3D Gaussian Splatting via  Universal Watermarking;"3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for 3D scene
representation, providing rapid rendering speeds and high fidelity. As 3DGS
gains prominence, safeguarding its intellectual property becomes increasingly
crucial since 3DGS could be used to imitate unauthorized scene creations and
raise copyright issues. Existing watermarking methods for implicit NeRFs cannot
be directly applied to 3DGS due to its explicit representation and real-time
rendering process, leaving watermarking for 3DGS largely unexplored. In
response, we propose WATER-GS, a novel method designed to protect 3DGS
copyrights through a universal watermarking strategy. First, we introduce a
pre-trained watermark decoder, treating raw 3DGS generative modules as
potential watermark encoders to ensure imperceptibility. Additionally, we
implement novel 3D distortion layers to enhance the robustness of the embedded
watermark against common real-world distortions of point cloud data.
Comprehensive experiments and ablation studies demonstrate that WATER-GS
effectively embeds imperceptible and robust watermarks into 3DGS without
compromising rendering efficiency and quality. Our experiments indicate that
the 3D distortion layers can yield up to a 20% improvement in accuracy rate.
Notably, our method is adaptable to different 3DGS variants, including 3DGS
compression frameworks and 2D Gaussian splatting.";Yuqi Tan<author:sep>Xiang Liu<author:sep>Shuzhao Xie<author:sep>Bin Chen<author:sep>Shu-Tao Xia<author:sep>Zhi Wang;http://arxiv.org/pdf/2412.05695v1;cs.CR;;gaussian splatting<tag:sep>nerf
2412.05256v2;http://arxiv.org/abs/2412.05256v2;2024-12-06;Extrapolated Urban View Synthesis Benchmark;"Photorealistic simulators are essential for the training and evaluation of
vision-centric autonomous vehicles (AVs). At their core is Novel View Synthesis
(NVS), a crucial capability that generates diverse unseen viewpoints to
accommodate the broad and continuous pose distribution of AVs. Recent advances
in radiance fields, such as 3D Gaussian Splatting, achieve photorealistic
rendering at real-time speeds and have been widely used in modeling large-scale
driving scenes. However, their performance is commonly evaluated using an
interpolated setup with highly correlated training and test views. In contrast,
extrapolation, where test views largely deviate from training views, remains
underexplored, limiting progress in generalizable simulation technology. To
address this gap, we leverage publicly available AV datasets with multiple
traversals, multiple vehicles, and multiple cameras to build the first
Extrapolated Urban View Synthesis (EUVS) benchmark. Meanwhile, we conduct
quantitative and qualitative evaluations of state-of-the-art Gaussian Splatting
methods across different difficulty levels. Our results show that Gaussian
Splatting is prone to overfitting to training views. Besides, incorporating
diffusion priors and improving geometry cannot fundamentally improve NVS under
large view changes, highlighting the need for more robust approaches and
large-scale training. We have released our data to help advance self-driving
and urban robotics simulation technology.";Xiangyu Han<author:sep>Zhen Jia<author:sep>Boyi Li<author:sep>Yan Wang<author:sep>Boris Ivanovic<author:sep>Yurong You<author:sep>Lingjie Liu<author:sep>Yue Wang<author:sep>Marco Pavone<author:sep>Chen Feng<author:sep>Yiming Li;http://arxiv.org/pdf/2412.05256v2;cs.CV;Project page: https://ai4ce.github.io/EUVS-Benchmark/;gaussian splatting
2412.04955v2;http://arxiv.org/abs/2412.04955v2;2024-12-06;MixedGaussianAvatar: Realistically and Geometrically Accurate Head  Avatar via Mixed 2D-3D Gaussian Splatting;"Reconstructing high-fidelity 3D head avatars is crucial in various
applications such as virtual reality. The pioneering methods reconstruct
realistic head avatars with Neural Radiance Fields (NeRF), which have been
limited by training and rendering speed. Recent methods based on 3D Gaussian
Splatting (3DGS) significantly improve the efficiency of training and
rendering. However, the surface inconsistency of 3DGS results in subpar
geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy
at the expense of rendering fidelity. To leverage the benefits of both 2DGS and
3DGS, we propose a novel method named MixedGaussianAvatar for realistically and
geometrically accurate head avatar reconstruction. Our main idea is to utilize
2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric
accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model
and connect additional 3D Gaussians to those 2D Gaussians where the rendering
quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation.
These 2D-3D Gaussians can then be animated using FLAME parameters. We further
introduce a progressive training strategy that first trains the 2D Gaussians
and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority
of MixedGaussianAvatar through comprehensive experiments. The code will be
released at: https://github.com/ChenVoid/MGA/.";Peng Chen<author:sep>Xiaobao Wei<author:sep>Qingpo Wuwu<author:sep>Xinyi Wang<author:sep>Xingyu Xiao<author:sep>Ming Lu;http://arxiv.org/pdf/2412.04955v2;cs.CV;Project: https://chenvoid.github.io/MGA/;gaussian splatting<tag:sep>nerf
2412.04887v1;http://arxiv.org/abs/2412.04887v1;2024-12-06;Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large  Scene Reconstruction;"3D Gaussian Splatting has demonstrated notable success in large-scale scene
reconstruction, but challenges persist due to high training memory consumption
and storage overhead. Hybrid representations that integrate implicit and
explicit features offer a way to mitigate these limitations. However, when
applied in parallelized block-wise training, two critical issues arise since
reconstruction accuracy deteriorates due to reduced data diversity when
training each block independently, and parallel training restricts the number
of divided blocks to the available number of GPUs. To address these issues, we
propose Momentum-GS, a novel approach that leverages momentum-based
self-distillation to promote consistency and accuracy across the blocks while
decoupling the number of blocks from the physical GPU count. Our method
maintains a teacher Gaussian decoder updated with momentum, ensuring a stable
reference during training. This teacher provides each block with global
guidance in a self-distillation manner, promoting spatial consistency in
reconstruction. To further ensure consistency across the blocks, we incorporate
block weighting, dynamically adjusting each block's weight according to its
reconstruction accuracy. Extensive experiments on large-scale scenes show that
our method consistently outperforms existing techniques, achieving a 12.8%
improvement in LPIPS over CityGaussian with much fewer divided blocks and
establishing a new state of the art. Project page:
https://jixuan-fan.github.io/Momentum-GS_Page/";Jixuan Fan<author:sep>Wanhua Li<author:sep>Yifei Han<author:sep>Yansong Tang;http://arxiv.org/pdf/2412.04887v1;cs.CV;;gaussian splatting
2412.04826v1;http://arxiv.org/abs/2412.04826v1;2024-12-06;Pushing Rendering Boundaries: Hard Gaussian Splatting;"3D Gaussian Splatting (3DGS) has demonstrated impressive Novel View Synthesis
(NVS) results in a real-time rendering manner. During training, it relies
heavily on the average magnitude of view-space positional gradients to grow
Gaussians to reduce rendering loss. However, this average operation smooths the
positional gradients from different viewpoints and rendering errors from
different pixels, hindering the growth and optimization of many defective
Gaussians. This leads to strong spurious artifacts in some areas. To address
this problem, we propose Hard Gaussian Splatting, dubbed HGS, which considers
multi-view significant positional gradients and rendering errors to grow hard
Gaussians that fill the gaps of classical Gaussian Splatting on 3D scenes, thus
achieving superior NVS results. In detail, we present positional gradient
driven HGS, which leverages multi-view significant positional gradients to
uncover hard Gaussians. Moreover, we propose rendering error guided HGS, which
identifies noticeable pixel rendering errors and potentially over-large
Gaussians to jointly mine hard Gaussians. By growing and optimizing these hard
Gaussians, our method helps to resolve blurring and needle-like artifacts.
Experiments on various datasets demonstrate that our method achieves
state-of-the-art rendering quality while maintaining real-time efficiency.";Qingshan Xu<author:sep>Jiequan Cui<author:sep>Xuanyu Yi<author:sep>Yuxuan Wang<author:sep>Yuan Zhou<author:sep>Yew-Soon Ong<author:sep>Hanwang Zhang;http://arxiv.org/pdf/2412.04826v1;cs.CV;;gaussian splatting
2412.05279v1;http://arxiv.org/abs/2412.05279v1;2024-12-06;Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories;"The fields of 3D reconstruction and text-based 3D editing have advanced
significantly with the evolution of text-based diffusion models. While existing
3D editing methods excel at modifying color, texture, and style, they struggle
with extensive geometric or appearance changes, thus limiting their
applications. We propose Perturb-and-Revise, which makes possible a variety of
NeRF editing. First, we perturb the NeRF parameters with random initializations
to create a versatile initialization. We automatically determine the
perturbation magnitude through analysis of the local loss landscape. Then, we
revise the edited NeRF via generative trajectories. Combined with the
generative process, we impose identity-preserving gradients to refine the
edited NeRF. Extensive experiments demonstrate that Perturb-and-Revise
facilitates flexible, effective, and consistent editing of color, appearance,
and geometry in 3D. For 360{\deg} results, please visit our project page:
https://susunghong.github.io/Perturb-and-Revise.";Susung Hong<author:sep>Johanna Karras<author:sep>Ricardo Martin-Brualla<author:sep>Ira Kemelmacher-Shlizerman;http://arxiv.org/pdf/2412.05279v1;cs.CV;Project page: https://susunghong.github.io/Perturb-and-Revise;nerf
2412.04832v1;http://arxiv.org/abs/2412.04832v1;2024-12-06;WRF-GS: Wireless Radiation Field Reconstruction with 3D Gaussian  Splatting;"Wireless channel modeling plays a pivotal role in designing, analyzing, and
optimizing wireless communication systems. Nevertheless, developing an
effective channel modeling approach has been a longstanding challenge. This
issue has been escalated due to the denser network deployment, larger antenna
arrays, and wider bandwidth in 5G and beyond networks. To address this
challenge, we put forth WRF-GS, a novel framework for channel modeling based on
wireless radiation field (WRF) reconstruction using 3D Gaussian splatting.
WRF-GS employs 3D Gaussian primitives and neural networks to capture the
interactions between the environment and radio signals, enabling efficient WRF
reconstruction and visualization of the propagation characteristics. The
reconstructed WRF can then be used to synthesize the spatial spectrum for
comprehensive wireless channel characterization. Notably, with a small number
of measurements, WRF-GS can synthesize new spatial spectra within milliseconds
for a given scene, thereby enabling latency-sensitive applications.
Experimental results demonstrate that WRF-GS outperforms existing methods for
spatial spectrum synthesis, such as ray tracing and other deep-learning
approaches. Moreover, WRF-GS achieves superior performance in the channel state
information prediction task, surpassing existing methods by a significant
margin of more than 2.43 dB.";Chaozheng Wen<author:sep>Jingwen Tong<author:sep>Yingdong Hu<author:sep>Zehong Lin<author:sep>Jun Zhang;http://arxiv.org/pdf/2412.04832v1;cs.NI;"accepted to the IEEE International Conference on Computer
  Communications (INFOCOM 2025)";gaussian splatting
2412.04459v1;http://arxiv.org/abs/2412.04459v1;2024-12-05;Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field  Rendering;"We propose an efficient radiance field rendering algorithm that incorporates
a rasterization process on sparse voxels without neural networks or 3D
Gaussians. There are two key contributions coupled with the proposed system.
The first is to render sparse voxels in the correct depth order along pixel
rays by using dynamic Morton ordering. This avoids the well-known popping
artifact found in Gaussian splatting. Second, we adaptively fit sparse voxels
to different levels of detail within scenes, faithfully reproducing scene
details while achieving high rendering frame rates. Our method improves the
previous neural-free voxel grid representation by over 4db PSNR and more than
10x rendering FPS speedup, achieving state-of-the-art comparable novel-view
synthesis results. Additionally, our neural-free sparse voxels are seamlessly
compatible with grid-based 3D processing algorithms. We achieve promising mesh
reconstruction accuracy by integrating TSDF-Fusion and Marching Cubes into our
sparse grid system.";Cheng Sun<author:sep>Jaesung Choe<author:sep>Charles Loop<author:sep>Wei-Chiu Ma<author:sep>Yu-Chiang Frank Wang;http://arxiv.org/pdf/2412.04459v1;cs.CV;Code release in progress;gaussian splatting
2412.04469v1;http://arxiv.org/abs/2412.04469v1;2024-12-05;QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming  Free-viewpoint Videos;"Online free-viewpoint video (FVV) streaming is a challenging problem, which
is relatively under-explored. It requires incremental on-the-fly updates to a
volumetric representation, fast training and rendering to satisfy real-time
constraints and a small memory footprint for efficient transmission. If
achieved, it can enhance user experience by enabling novel applications, e.g.,
3D video conferencing and live volumetric video broadcast, among others. In
this work, we propose a novel framework for QUantized and Efficient ENcoding
(QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly
learns Gaussian attribute residuals between consecutive frames at each
time-step without imposing any structural constraints on them, allowing for
high quality reconstruction and generalizability. To efficiently store the
residuals, we further propose a quantization-sparsity framework, which contains
a learned latent-decoder for effectively quantizing attribute residuals other
than Gaussian positions and a learned gating module to sparsify position
residuals. We propose to use the Gaussian viewspace gradient difference vector
as a signal to separate the static and dynamic content of the scene. It acts as
a guide for effective sparsity learning and speeds up training. On diverse FVV
benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all
metrics. Notably, for several highly dynamic scenes, it reduces the model size
to just 0.7 MB per frame while training in under 5 sec and rendering at 350
FPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen";Sharath Girish<author:sep>Tianye Li<author:sep>Amrita Mazumdar<author:sep>Abhinav Shrivastava<author:sep>David Luebke<author:sep>Shalini De Mello;http://arxiv.org/pdf/2412.04469v1;cs.CV;"Accepted at NeurIPS 2024, Project website:
  https://research.nvidia.com/labs/amri/projects/queen";gaussian splatting
2412.03844v1;http://arxiv.org/abs/2412.03844v1;2024-12-05;HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian  Splatting;"Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS)
in scenes featuring transient objects is challenging. We propose a novel hybrid
representation, termed as HybridGS, using 2D Gaussians for transient objects
per image and maintaining traditional 3D Gaussians for the whole static scenes.
Note that, the 3DGS itself is better suited for modeling static scenes that
assume multi-view consistency, but the transient objects appear occasionally
and do not adhere to the assumption, thus we model them as planar objects from
a single view, represented with 2D Gaussians. Our novel representation
decomposes the scene from the perspective of fundamental viewpoint consistency,
making it more reasonable. Additionally, we present a novel multi-view
regulated supervision method for 3DGS that leverages information from
co-visible regions, further enhancing the distinctions between the transients
and statics. Then, we propose a straightforward yet effective multi-stage
training strategy to ensure robust training and high-quality view synthesis
across various settings. Experiments on benchmark datasets show our
state-of-the-art performance of novel view synthesis in both indoor and outdoor
scenes, even in the presence of distracting elements.";Jingyu Lin<author:sep>Jiaqi Gu<author:sep>Lubin Fan<author:sep>Bojian Wu<author:sep>Yujing Lou<author:sep>Renjie Chen<author:sep>Ligang Liu<author:sep>Jieping Ye;http://arxiv.org/pdf/2412.03844v1;cs.CV;Project page: https://gujiaqivadin.github.io/hybridgs/;gaussian splatting
2412.03910v1;http://arxiv.org/abs/2412.03910v1;2024-12-05;DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for  Monocular Dynamic 3D Reconstruction;"Dynamic scene reconstruction from monocular video is critical for real-world
applications. This paper tackles the dual challenges of dynamic novel-view
synthesis and 3D geometry reconstruction by introducing a hybrid framework:
Deformable Gaussian Splatting and Dynamic Neural Surfaces (DGNS), in which both
modules can leverage each other for both tasks. During training, depth maps
generated by the deformable Gaussian splatting module guide the ray sampling
for faster processing and provide depth supervision within the dynamic neural
surface module to improve geometry reconstruction. Simultaneously, the dynamic
neural surface directs the distribution of Gaussian primitives around the
surface, enhancing rendering quality. To further refine depth supervision, we
introduce a depth-filtering process on depth maps derived from Gaussian
rasterization. Extensive experiments on public datasets demonstrate that DGNS
achieves state-of-the-art performance in both novel-view synthesis and 3D
reconstruction.";Xuesong Li<author:sep>Jinguang Tong<author:sep>Jie Hong<author:sep>Vivien Rolland<author:sep>Lars Petersson;http://arxiv.org/pdf/2412.03910v1;cs.CV;;gaussian splatting
2412.04457v1;http://arxiv.org/abs/2412.04457v1;2024-12-05;Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth  Motion Helps;"Gaussian splatting methods are emerging as a popular approach for converting
multi-view image data into scene representations that allow view synthesis. In
particular, there is interest in enabling view synthesis for dynamic scenes
using only monocular input data -- an ill-posed and challenging problem. The
fast pace of work in this area has produced multiple simultaneous papers that
claim to work best, which cannot all be true. In this work, we organize,
benchmark, and analyze many Gaussian-splatting-based methods, providing
apples-to-apples comparisons that prior works have lacked. We use multiple
existing datasets and a new instructive synthetic dataset designed to isolate
factors that affect reconstruction quality. We systematically categorize
Gaussian splatting methods into specific motion representation types and
quantify how their differences impact performance. Empirically, we find that
their rank order is well-defined in synthetic data, but the complexity of
real-world data currently overwhelms the differences. Furthermore, the fast
rendering speed of all Gaussian-based methods comes at the cost of brittleness
in optimization. We summarize our experiments into a list of findings that can
help to further progress in this lively problem setting. Project Webpage:
https://lynl7130.github.io/MonoDyGauBench.github.io/";Yiqing Liang<author:sep>Mikhail Okunev<author:sep>Mikaela Angelina Uy<author:sep>Runfeng Li<author:sep>Leonidas Guibas<author:sep>James Tompkin<author:sep>Adam W. Harley;http://arxiv.org/pdf/2412.04457v1;cs.CV;37 pages, 39 figures, 9 tables;gaussian splatting
2412.04433v2;http://arxiv.org/abs/2412.04433v2;2024-12-05;PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human  Avatars;"This paper introduces a novel clothed human model that can be learned from
multiview RGB videos, with a particular emphasis on recovering physically
accurate body and cloth movements. Our method, Position Based Dynamic Gaussians
(PBDyG), realizes ``movement-dependent'' cloth deformation via physical
simulation, rather than merely relying on ``pose-dependent'' rigid
transformations. We model the clothed human holistically but with two distinct
physical entities in contact: clothing modeled as 3D Gaussians, which are
attached to a skinned SMPL body that follows the movement of the person in the
input videos. The articulation of the SMPL body also drives physically-based
simulation of the clothes' Gaussians to transform the avatar to novel poses. In
order to run position based dynamics simulation, physical properties including
mass and material stiffness are estimated from the RGB videos through Dynamic
3D Gaussian Splatting. Experiments demonstrate that our method not only
accurately reproduces appearance but also enables the reconstruction of avatars
wearing highly deformable garments, such as skirts or coats, which have been
challenging to reconstruct using existing methods.";Shota Sasaki<author:sep>Jane Wu<author:sep>Ko Nishino;http://arxiv.org/pdf/2412.04433v2;cs.CV;;gaussian splatting
2412.03911v1;http://arxiv.org/abs/2412.03911v1;2024-12-05;Multi-View Pose-Agnostic Change Localization with Zero Labels;"Autonomous agents often require accurate methods for detecting and localizing
changes in their environment, particularly when observations are captured from
unconstrained and inconsistent viewpoints. We propose a novel label-free,
pose-agnostic change detection method that integrates information from multiple
viewpoints to construct a change-aware 3D Gaussian Splatting (3DGS)
representation of the scene. With as few as 5 images of the post-change scene,
our approach can learn additional change channels in a 3DGS and produce change
masks that outperform single-view techniques. Our change-aware 3D scene
representation additionally enables the generation of accurate change masks for
unseen viewpoints. Experimental results demonstrate state-of-the-art
performance in complex multi-object scenes, achieving a 1.7$\times$ and
1.6$\times$ improvement in Mean Intersection Over Union and F1 score
respectively over other baselines. We also contribute a new real-world dataset
to benchmark change detection in diverse challenging scenes in the presence of
lighting variations.";Chamuditha Jayanga Galappaththige<author:sep>Jason Lai<author:sep>Lloyd Windrim<author:sep>Donald Dansereau<author:sep>Niko Suenderhauf<author:sep>Dimity Miller;http://arxiv.org/pdf/2412.03911v1;cs.CV;;gaussian splatting
2412.04470v1;http://arxiv.org/abs/2412.04470v1;2024-12-05;Turbo3D: Ultra-fast Text-to-3D Generation;"We present Turbo3D, an ultra-fast text-to-3D system capable of generating
high-quality Gaussian splatting assets in under one second. Turbo3D employs a
rapid 4-step, 4-view diffusion generator and an efficient feed-forward Gaussian
reconstructor, both operating in latent space. The 4-step, 4-view generator is
a student model distilled through a novel Dual-Teacher approach, which
encourages the student to learn view consistency from a multi-view teacher and
photo-realism from a single-view teacher. By shifting the Gaussian
reconstructor's inputs from pixel space to latent space, we eliminate the extra
image decoding time and halve the transformer sequence length for maximum
efficiency. Our method demonstrates superior 3D generation results compared to
previous baselines, while operating in a fraction of their runtime.";Hanzhe Hu<author:sep>Tianwei Yin<author:sep>Fujun Luan<author:sep>Yiwei Hu<author:sep>Hao Tan<author:sep>Zexiang Xu<author:sep>Sai Bi<author:sep>Shubham Tulsiani<author:sep>Kai Zhang;http://arxiv.org/pdf/2412.04470v1;cs.CV;project page: https://turbo-3d.github.io/;gaussian splatting
2412.03077v1;http://arxiv.org/abs/2412.03077v1;2024-12-04;RoDyGS: Robust Dynamic Gaussian Splatting for Casual Videos;"Dynamic view synthesis (DVS) has advanced remarkably in recent years,
achieving high-fidelity rendering while reducing computational costs. Despite
the progress, optimizing dynamic neural fields from casual videos remains
challenging, as these videos do not provide direct 3D information, such as
camera trajectories or the underlying scene geometry. In this work, we present
RoDyGS, an optimization pipeline for dynamic Gaussian Splatting from casual
videos. It effectively learns motion and underlying geometry of scenes by
separating dynamic and static primitives, and ensures that the learned motion
and geometry are physically plausible by incorporating motion and geometric
regularization terms. We also introduce a comprehensive benchmark, Kubric-MRig,
that provides extensive camera and object motion along with simultaneous
multi-view captures, features that are absent in previous benchmarks.
Experimental results demonstrate that the proposed method significantly
outperforms previous pose-free dynamic neural fields and achieves competitive
rendering quality compared to existing pose-free static neural fields. The code
and data are publicly available at https://rodygs.github.io/.";Yoonwoo Jeong<author:sep>Junmyeong Lee<author:sep>Hoseung Choi<author:sep>Minsu Cho;http://arxiv.org/pdf/2412.03077v1;cs.CV;Project Page: https://rodygs.github.io/;gaussian splatting
2412.03263v1;http://arxiv.org/abs/2412.03263v1;2024-12-04;NeRF and Gaussian Splatting SLAM in the Wild;"Navigating outdoor environments with visual Simultaneous Localization and
Mapping (SLAM) systems poses significant challenges due to dynamic scenes,
lighting variations, and seasonal changes, requiring robust solutions. While
traditional SLAM methods struggle with adaptability, deep learning-based
approaches and emerging neural radiance fields as well as Gaussian
Splatting-based SLAM methods, offer promising alternatives. However, these
methods have primarily been evaluated in controlled indoor environments with
stable conditions, leaving a gap in understanding their performance in
unstructured and variable outdoor settings. This study addresses this gap by
evaluating these methods in natural outdoor environments, focusing on camera
tracking accuracy, robustness to environmental factors, and computational
efficiency, highlighting distinct trade-offs. Extensive evaluations demonstrate
that neural SLAM methods achieve superior robustness, particularly under
challenging conditions such as low light, but at a high computational cost. At
the same time, traditional methods perform the best across seasons but are
highly sensitive to variations in lighting conditions. The code of the
benchmark is publicly available at
https://github.com/iis-esslingen/nerf-3dgs-benchmark.";Fabian Schmidt<author:sep>Markus Enzweiler<author:sep>Abhinav Valada;http://arxiv.org/pdf/2412.03263v1;cs.RO;5 pages, 2 figures, 4 tables;gaussian splatting<tag:sep>nerf
2412.03526v1;http://arxiv.org/abs/2412.03526v1;2024-12-04;Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular  Videos;"Recent advancements in static feed-forward scene reconstruction have
demonstrated significant progress in high-quality novel view synthesis.
However, these models often struggle with generalizability across diverse
environments and fail to effectively handle dynamic content. We present BTimer
(short for BulletTimer), the first motion-aware feed-forward model for
real-time reconstruction and novel view synthesis of dynamic scenes. Our
approach reconstructs the full scene in a 3D Gaussian Splatting representation
at a given target ('bullet') timestamp by aggregating information from all the
context frames. Such a formulation allows BTimer to gain scalability and
generalization by leveraging both static and dynamic scene datasets. Given a
casual monocular dynamic video, BTimer reconstructs a bullet-time scene within
150ms while reaching state-of-the-art performance on both static and dynamic
scene datasets, even compared with optimization-based approaches.";Hanxue Liang<author:sep>Jiawei Ren<author:sep>Ashkan Mirzaei<author:sep>Antonio Torralba<author:sep>Ziwei Liu<author:sep>Igor Gilitschenski<author:sep>Sanja Fidler<author:sep>Cengiz Oztireli<author:sep>Huan Ling<author:sep>Zan Gojcic<author:sep>Jiahui Huang;http://arxiv.org/pdf/2412.03526v1;cs.CV;"Project website:
  https://research.nvidia.com/labs/toronto-ai/bullet-timer/";gaussian splatting
2412.03371v1;http://arxiv.org/abs/2412.03371v1;2024-12-04;SGSST: Scaling Gaussian Splatting StyleTransfer;"Applying style transfer to a full 3D environment is a challenging task that
has seen many developments since the advent of neural rendering. 3D Gaussian
splatting (3DGS) has recently pushed further many limits of neural rendering in
terms of training speed and reconstruction quality. This work introduces SGSST:
Scaling Gaussian Splatting Style Transfer, an optimization-based method to
apply style transfer to pretrained 3DGS scenes. We demonstrate that a new
multiscale loss based on global neural statistics, that we name SOS for
Simultaneously Optimized Scales, enables style transfer to ultra-high
resolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such
high image resolutions, it also produces superior visual quality as assessed by
thorough qualitative, quantitative and perceptual comparisons.";Bruno Galerne<author:sep>Jianling Wang<author:sep>Lara Raad<author:sep>Jean-Michel Morel;http://arxiv.org/pdf/2412.03371v1;cs.CV;;gaussian splatting
2412.03121v1;http://arxiv.org/abs/2412.03121v1;2024-12-04;Splats in Splats: Embedding Invisible 3D Watermark within Gaussian  Splatting;"3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction
performance with explicit scene representations. Given the widespread
application of 3DGS in 3D reconstruction and generation tasks, there is an
urgent need to protect the copyright of 3DGS assets. However, existing
copyright protection techniques for 3DGS overlook the usability of 3D assets,
posing challenges for practical deployment. Here we describe WaterGS, the first
3DGS watermarking framework that embeds 3D content in 3DGS itself without
modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep
insight into spherical harmonics (SH) and devise an importance-graded SH
coefficient encryption strategy to embed the hidden SH coefficients.
Furthermore, we employ a convolutional autoencoder to establish a mapping
between the original Gaussian primitives' opacity and the hidden Gaussian
primitives' opacity. Extensive experiments indicate that WaterGS significantly
outperforms existing 3D steganography techniques, with 5.31% higher scene
fidelity and 3X faster rendering speed, while ensuring security, robustness,
and user experience. Codes and data will be released at
https://water-gs.github.io.";Yijia Guo<author:sep>Wenkai Huang<author:sep>Yang Li<author:sep>Gaolei Li<author:sep>Hang Zhang<author:sep>Liwen Hu<author:sep>Jianhua Li<author:sep>Tiejun Huang<author:sep>Lei Ma;http://arxiv.org/pdf/2412.03121v1;cs.CV;;gaussian splatting
2412.03428v1;http://arxiv.org/abs/2412.03428v1;2024-12-04;2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains  for High-Fidelity Indoor Scene Reconstruction;"The reconstruction of indoor scenes remains challenging due to the inherent
complexity of spatial structures and the prevalence of textureless regions.
Recent advancements in 3D Gaussian Splatting have improved novel view synthesis
with accelerated processing but have yet to deliver comparable performance in
surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method
leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction.
Specifically, we employ a seed-guided mechanism to control the distribution of
2D Gaussians, with the density of seed points dynamically optimized through
adaptive growth and pruning mechanisms. To further improve geometric accuracy,
we incorporate monocular depth and normal priors to provide constraints for
details and textureless regions respectively. Additionally, multi-view
consistency constraints are employed to mitigate artifacts and further enhance
reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets
demonstrate that our method achieves state-of-the-art performance in indoor
scene reconstruction.";Wanting Zhang<author:sep>Haodong Xiang<author:sep>Zhichao Liao<author:sep>Xiansong Lai<author:sep>Xinghui Li<author:sep>Long Zeng;http://arxiv.org/pdf/2412.03428v1;cs.CV;;gaussian splatting
2412.03473v1;http://arxiv.org/abs/2412.03473v1;2024-12-04;Urban4D: Semantic-Guided 4D Gaussian Splatting for Urban Scene  Reconstruction;"Reconstructing dynamic urban scenes presents significant challenges due to
their intrinsic geometric structures and spatiotemporal dynamics. Existing
methods that attempt to model dynamic urban scenes without leveraging priors on
potentially moving regions often produce suboptimal results. Meanwhile,
approaches based on manual 3D annotations yield improved reconstruction quality
but are impractical due to labor-intensive labeling. In this paper, we revisit
the potential of 2D semantic maps for classifying dynamic and static Gaussians
and integrating spatial and temporal dimensions for urban scene representation.
We introduce Urban4D, a novel framework that employs a semantic-guided
decomposition strategy inspired by advances in deep 2D semantic map generation.
Our approach distinguishes potentially dynamic objects through reliable
semantic Gaussians. To explicitly model dynamic objects, we propose an
intuitive and effective 4D Gaussian splatting (4DGS) representation that
aggregates temporal information through learnable time embeddings for each
Gaussian, predicting their deformations at desired timestamps using a
multilayer perceptron (MLP). For more accurate static reconstruction, we also
design a k-nearest neighbor (KNN)-based consistency regularization to handle
the ground surface due to its low-texture characteristic. Extensive experiments
on real-world datasets demonstrate that Urban4D not only achieves comparable or
better quality than previous state-of-the-art methods but also effectively
captures dynamic objects while maintaining high visual fidelity for static
elements.";Ziwen Li<author:sep>Jiaxin Huang<author:sep>Runnan Chen<author:sep>Yunlong Che<author:sep>Yandong Guo<author:sep>Tongliang Liu<author:sep>Fakhri Karray<author:sep>Mingming Gong;http://arxiv.org/pdf/2412.03473v1;cs.CV;;gaussian splatting
2412.03518v1;http://arxiv.org/abs/2412.03518v1;2024-12-04;Dense Scene Reconstruction from Light-Field Images Affected by Rolling  Shutter;"This paper presents a dense depth estimation approach from light-field (LF)
images that is able to compensate for strong rolling shutter (RS) effects. Our
method estimates RS compensated views and dense RS compensated disparity maps.
We present a two-stage method based on a 2D Gaussians Splatting that allows for
a ``render and compare"" strategy with a point cloud formulation. In the first
stage, a subset of sub-aperture images is used to estimate an RS agnostic 3D
shape that is related to the scene target shape ``up to a motion"". In the
second stage, the deformation of the 3D shape is computed by estimating an
admissible camera motion. We demonstrate the effectiveness and advantages of
this approach through several experiments conducted for different scenes and
types of motions. Due to lack of suitable datasets for evaluation, we also
present a new carefully designed synthetic dataset of RS LF images. The source
code, trained models and dataset will be made publicly available at:
https://github.com/ICB-Vision-AI/DenseRSLF";Hermes McGriff<author:sep>Renato Martins<author:sep>Nicolas Andreff<author:sep>Cedric Demonceaux;http://arxiv.org/pdf/2412.03518v1;cs.CV;;
2412.03378v1;http://arxiv.org/abs/2412.03378v1;2024-12-04;Volumetrically Consistent 3D Gaussian Rasterization;"Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view
synthesis at high inference speeds. However, its splatting-based rendering
model makes several approximations to the rendering equation, reducing physical
accuracy. We show that splatting and its approximations are unnecessary, even
within a rasterizer; we instead volumetrically integrate 3D Gaussians directly
to compute the transmittance across them analytically. We use this analytic
transmittance to derive more physically-accurate alpha values than 3DGS, which
can directly be used within their framework. The result is a method that more
closely follows the volume rendering equation (similar to ray-tracing) while
enjoying the speed benefits of rasterization. Our method represents opaque
surfaces with higher accuracy and fewer points than 3DGS. This enables it to
outperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being
volumetrically consistent also enables our method to work out of the box for
tomography. We match the state-of-the-art 3DGS-based tomography method with
fewer points. Being volumetrically consistent also enables our method to work
out of the box for tomography. We match the state-of-the-art 3DGS-based
tomography method with fewer points.";Chinmay Talegaonkar<author:sep>Yash Belhe<author:sep>Ravi Ramamoorthi<author:sep>Nicholas Antipa;http://arxiv.org/pdf/2412.03378v1;cs.CV;;gaussian splatting
2412.02075v1;http://arxiv.org/abs/2412.02075v1;2024-12-03;Gaussian Object Carver: Object-Compositional Gaussian Splatting with  surfaces completion;"3D scene reconstruction is a foundational problem in computer vision. Despite
recent advancements in Neural Implicit Representations (NIR), existing methods
often lack editability and compositional flexibility, limiting their use in
scenarios requiring high interactivity and object-level manipulation. In this
paper, we introduce the Gaussian Object Carver (GOC), a novel, efficient, and
scalable framework for object-compositional 3D scene reconstruction. GOC
leverages 3D Gaussian Splatting (GS), enriched with monocular geometry priors
and multi-view geometry regularization, to achieve high-quality and flexible
reconstruction. Furthermore, we propose a zero-shot Object Surface Completion
(OSC) model, which uses 3D priors from 3d object data to reconstruct unobserved
surfaces, ensuring object completeness even in occluded areas. Experimental
results demonstrate that GOC improves reconstruction efficiency and geometric
fidelity. It holds promise for advancing the practical application of digital
twins in embodied AI, AR/VR, and interactive simulation environments.";Liu Liu<author:sep>Xinjie Wang<author:sep>Jiaxiong Qiu<author:sep>Tianwei Lin<author:sep>Xiaolin Zhou<author:sep>Zhizhong Su;http://arxiv.org/pdf/2412.02075v1;cs.CV;;gaussian splatting
2412.02249v1;http://arxiv.org/abs/2412.02249v1;2024-12-03;Multi-robot autonomous 3D reconstruction using Gaussian splatting with  Semantic guidance;"Implicit neural representations and 3D Gaussian splatting (3DGS) have shown
great potential for scene reconstruction. Recent studies have expanded their
applications in autonomous reconstruction through task assignment methods.
However, these methods are mainly limited to single robot, and rapid
reconstruction of large-scale scenes remains challenging. Additionally,
task-driven planning based on surface uncertainty is prone to being trapped in
local optima. To this end, we propose the first 3DGS-based centralized
multi-robot autonomous 3D reconstruction framework. To further reduce time cost
of task generation and improve reconstruction quality, we integrate online
open-vocabulary semantic segmentation with surface uncertainty of 3DGS,
focusing view sampling on regions with high instance uncertainty. Finally, we
develop a multi-robot collaboration strategy with mode and task assignments
improving reconstruction quality while ensuring planning efficiency. Our method
demonstrates the highest reconstruction quality among all planning methods and
superior planning efficiency compared to existing multi-robot methods. We
deploy our method on multiple robots, and results show that it can effectively
plan view paths and reconstruct scenes with high quality.";Jing Zeng<author:sep>Qi Ye<author:sep>Tianle Liu<author:sep>Yang Xu<author:sep>Jin Li<author:sep>Jinming Xu<author:sep>Liang Li<author:sep>Jiming Chen;http://arxiv.org/pdf/2412.02249v1;cs.RO;;gaussian splatting
2412.02684v1;http://arxiv.org/abs/2412.02684v1;2024-12-03;AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent  Gaussian Reconstruction;"Generating animatable human avatars from a single image is essential for
various digital human modeling applications. Existing 3D reconstruction methods
often struggle to capture fine details in animatable models, while generative
approaches for controllable animation, though avoiding explicit 3D modeling,
suffer from viewpoint inconsistencies in extreme poses and computational
inefficiencies. In this paper, we address these challenges by leveraging the
power of generative models to produce detailed multi-view canonical pose
images, which help resolve ambiguities in animatable human reconstruction. We
then propose a robust method for 3D reconstruction of inconsistent images,
enabling real-time rendering during inference. Specifically, we adapt a
transformer-based video generation model to generate multi-view canonical pose
images and normal maps, pretraining on a large-scale video dataset to improve
generalization. To handle view inconsistencies, we recast the reconstruction
problem as a 4D task and introduce an efficient 3D modeling approach using 4D
Gaussian Splatting. Experiments demonstrate that our method achieves
photorealistic, real-time animation of 3D human avatars from in-the-wild
images, showcasing its effectiveness and generalization capability.";Lingteng Qiu<author:sep>Shenhao Zhu<author:sep>Qi Zuo<author:sep>Xiaodong Gu<author:sep>Yuan Dong<author:sep>Junfei Zhang<author:sep>Chao Xu<author:sep>Zhe Li<author:sep>Weihao Yuan<author:sep>Liefeng Bo<author:sep>Guanying Chen<author:sep>Zilong Dong;http://arxiv.org/pdf/2412.02684v1;cs.CV;Project Page: https://lingtengqiu.github.io/2024/AniGS/;gaussian splatting
2412.02421v1;http://arxiv.org/abs/2412.02421v1;2024-12-03;TimeWalker: Personalized Neural Space for Lifelong Head Avatars;"We present TimeWalker, a novel framework that models realistic, full-scale 3D
head avatars of a person on lifelong scale. Unlike current human head avatar
pipelines that capture identity at the momentary level(e.g., instant
photography or short videos), TimeWalker constructs a person's comprehensive
identity from unstructured data collection over his/her various life stages,
offering a paradigm to achieve full reconstruction and animation of that person
at different moments of life. At the heart of TimeWalker's success is a novel
neural parametric model that learns personalized representation with the
disentanglement of shape, expression, and appearance across ages. Central to
our methodology are the concepts of two aspects: (1) We track back to the
principle of modeling a person's identity in an additive combination of average
head representation in the canonical space, and moment-specific head attribute
representations driven from a set of neural head basis. To learn the set of
head basis that could represent the comprehensive head variations in a compact
manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It
dynamically adjusts the number and blend weights of neural head bases,
according to both shared and specific traits of the target person over ages.
(2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian
splatting representation, to model head motion deformations like facial
expressions without losing the realism of rendering and reconstruction.
DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that
utilize the priors from parametric model, and move/rotate with the change of
expression. Through extensive experimental evaluations, we show TimeWalker's
ability to reconstruct and animate avatars across decoupled dimensions with
realistic rendering effects, demonstrating a way to achieve personalized 'time
traveling' in a breeze.";Dongwei Pan<author:sep>Yang Li<author:sep>Hongsheng Li<author:sep>Kwan-Yee Lin;http://arxiv.org/pdf/2412.02421v1;cs.CV;"Project Page: https://timewalker2024.github.io/timewalker.github.io/
  , Video: https://www.youtube.com/watch?v=x8cpOVMY_ko";gaussian splatting
2412.02493v1;http://arxiv.org/abs/2412.02493v1;2024-12-03;RelayGS: Reconstructing Dynamic Scenes with Large-Scale and Complex  Motions via Relay Gaussians;"Reconstructing dynamic scenes with large-scale and complex motions remains a
significant challenge. Recent techniques like Neural Radiance Fields and 3D
Gaussian Splatting (3DGS) have shown promise but still struggle with scenes
involving substantial movement. This paper proposes RelayGS, a novel method
based on 3DGS, specifically designed to represent and reconstruct highly
dynamic scenes. Our RelayGS learns a complete 4D representation with canonical
3D Gaussians and a compact motion field, consisting of three stages. First, we
learn a fundamental 3DGS from all frames, ignoring temporal scene variations,
and use a learnable mask to separate the highly dynamic foreground from the
minimally moving background. Second, we replicate multiple copies of the
decoupled foreground Gaussians from the first stage, each corresponding to a
temporal segment, and optimize them using pseudo-views constructed from
multiple frames within each segment. These Gaussians, termed Relay Gaussians,
act as explicit relay nodes, simplifying and breaking down large-scale motion
trajectories into smaller, manageable segments. Finally, we jointly learn the
scene's temporal motion and refine the canonical Gaussians learned from the
first two stages. We conduct thorough experiments on two dynamic scene datasets
featuring large and complex motions, where our RelayGS outperforms
state-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs
real-world basketball game scenes in a much more complete and coherent manner,
whereas previous methods usually struggle to capture the complex motion of
players. Code will be publicly available at https://github.com/gqk/RelayGS";Qiankun Gao<author:sep>Yanmin Wu<author:sep>Chengxiang Wen<author:sep>Jiarui Meng<author:sep>Luyang Tang<author:sep>Jie Chen<author:sep>Ronggang Wang<author:sep>Jian Zhang;http://arxiv.org/pdf/2412.02493v1;cs.CV;Technical Report. GitHub: https://github.com/gqk/RelayGS;gaussian splatting
2412.02140v1;http://arxiv.org/abs/2412.02140v1;2024-12-03;SparseGrasp: Robotic Grasping via 3D Semantic Gaussian Splatting from  Sparse Multi-View RGB Images;"Language-guided robotic grasping is a rapidly advancing field where robots
are instructed using human language to grasp specific objects. However,
existing methods often depend on dense camera views and struggle to quickly
update scenes, limiting their effectiveness in changeable environments.
  In contrast, we propose SparseGrasp, a novel open-vocabulary robotic grasping
system that operates efficiently with sparse-view RGB images and handles scene
updates fastly. Our system builds upon and significantly enhances existing
computer vision modules in robotic learning. Specifically, SparseGrasp utilizes
DUSt3R to generate a dense point cloud as the initialization for 3D Gaussian
Splatting (3DGS), maintaining high fidelity even under sparse supervision.
Importantly, SparseGrasp incorporates semantic awareness from recent vision
foundation models. To further improve processing efficiency, we repurpose
Principal Component Analysis (PCA) to compress features from 2D models.
Additionally, we introduce a novel render-and-compare strategy that ensures
rapid scene updates, enabling multi-turn grasping in changeable environments.
  Experimental results show that SparseGrasp significantly outperforms
state-of-the-art methods in terms of both speed and adaptability, providing a
robust solution for multi-turn grasping in changeable environment.";Junqiu Yu<author:sep>Xinlin Ren<author:sep>Yongchong Gu<author:sep>Haitao Lin<author:sep>Tianyu Wang<author:sep>Yi Zhu<author:sep>Hang Xu<author:sep>Yu-Gang Jiang<author:sep>Xiangyang Xue<author:sep>Yanwei Fu;http://arxiv.org/pdf/2412.02140v1;cs.RO;;gaussian splatting
2412.02803v1;http://arxiv.org/abs/2412.02803v1;2024-12-03;Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D  Objects;"3D Gaussian Splatting has advanced radiance field reconstruction, enabling
high-quality view synthesis and fast rendering in 3D modeling. While
adversarial attacks on object detection models are well-studied for 2D images,
their impact on 3D models remains underexplored. This work introduces the
Masked Iterative Fast Gradient Sign Method (M-IFGSM), designed to generate
adversarial noise targeting the CLIP vision-language model. M-IFGSM
specifically alters the object of interest by focusing perturbations on masked
regions, degrading the performance of CLIP's zero-shot object detection
capability when applied to 3D models. Using eight objects from the Common
Objects 3D (CO3D) dataset, we demonstrate that our method effectively reduces
the accuracy and confidence of the model, with adversarial noise being nearly
imperceptible to human observers. The top-1 accuracy in original model renders
drops from 95.4\% to 12.5\% for train images and from 91.2\% to 35.4\% for test
images, with confidence levels reflecting this shift from true classification
to misclassification, underscoring the risks of adversarial attacks on 3D
models in applications such as autonomous driving, robotics, and surveillance.
The significance of this research lies in its potential to expose
vulnerabilities in modern 3D vision models, including radiance fields,
prompting the development of more robust defenses and security measures in
critical real-world applications.";Abdurrahman Zeybey<author:sep>Mehmet Ergezer<author:sep>Tommy Nguyen;http://arxiv.org/pdf/2412.02803v1;cs.CV;"Accepted to Safe Generative AI Workshop @ NeurIPS 2024:
  https://neurips.cc/virtual/2024/workshop/84705";gaussian splatting
2412.02225v1;http://arxiv.org/abs/2412.02225v1;2024-12-03;How to Use Diffusion Priors under Sparse Views?;"Novel view synthesis under sparse views has been a long-term important
challenge in 3D reconstruction. Existing works mainly rely on introducing
external semantic or depth priors to supervise the optimization of 3D
representations. However, the diffusion model, as an external prior that can
directly provide visual supervision, has always underperformed in sparse-view
3D reconstruction using Score Distillation Sampling (SDS) due to the low
information entropy of sparse views compared to text, leading to optimization
challenges caused by mode deviation. To this end, we present a thorough
analysis of SDS from the mode-seeking perspective and propose Inline Prior
Guided Score Matching (IPSM), which leverages visual inline priors provided by
pose relationships between viewpoints to rectify the rendered image
distribution and decomposes the original optimization objective of SDS, thereby
offering effective diffusion visual guidance without any fine-tuning or
pre-training. Furthermore, we propose the IPSM-Gaussian pipeline, which adopts
3D Gaussian Splatting as the backbone and supplements depth and geometry
consistency regularization based on IPSM to further improve inline priors and
rectified distribution. Experimental results on different public datasets show
that our method achieves state-of-the-art reconstruction quality. The code is
released at https://github.com/iCVTEAM/IPSM.";Qisen Wang<author:sep>Yifan Zhao<author:sep>Jiawei Ma<author:sep>Jia Li;http://arxiv.org/pdf/2412.02225v1;cs.CV;;gaussian splatting
2412.02267v1;http://arxiv.org/abs/2412.02267v1;2024-12-03;GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos;"Tracking the 6DoF pose of unknown objects in monocular RGB video sequences is
crucial for robotic manipulation. However, existing approaches typically rely
on accurate depth information, which is non-trivial to obtain in real-world
scenarios. Although depth estimation algorithms can be employed, geometric
inaccuracy can lead to failures in RGBD-based pose tracking methods. To address
this challenge, we introduce GSGTrack, a novel RGB-based pose tracking
framework that jointly optimizes geometry and pose. Specifically, we adopt 3D
Gaussian Splatting to create an optimizable 3D representation, which is learned
simultaneously with a graph-based geometry optimization to capture the object's
appearance features and refine its geometry. However, the joint optimization
process is susceptible to perturbations from noisy pose and geometry data.
Thus, we propose an object silhouette loss to address the issue of pixel-wise
loss being overly sensitive to pose noise during tracking. To mitigate the
geometric ambiguities caused by inaccurate depth information, we propose a
geometry-consistent image pair selection strategy, which filters out
low-confidence pairs and ensures robust geometric optimization. Extensive
experiments on the OnePose and HO3D datasets demonstrate the effectiveness of
GSGTrack in both 6DoF pose tracking and object reconstruction.";Zhiyuan Chen<author:sep>Fan Lu<author:sep>Guo Yu<author:sep>Bin Li<author:sep>Sanqing Qu<author:sep>Yuan Huang<author:sep>Changhong Fu<author:sep>Guang Chen;http://arxiv.org/pdf/2412.02267v1;cs.CV;;gaussian splatting
2412.02245v2;http://arxiv.org/abs/2412.02245v2;2024-12-03;SparseLGS: Sparse View Language Embedded Gaussian Splatting;"Recently, several studies have combined Gaussian Splatting to obtain scene
representations with language embeddings for open-vocabulary 3D scene
understanding. While these methods perform well, they essentially require very
dense multi-view inputs, limiting their applicability in real-world scenarios.
In this work, we propose SparseLGS to address the challenge of 3D scene
understanding with pose-free and sparse view input images. Our method leverages
a learning-based dense stereo model to handle pose-free and sparse inputs, and
a three-step region matching approach to address the multi-view semantic
inconsistency problem, which is especially important for sparse inputs.
Different from directly learning high-dimensional CLIP features, we extract
low-dimensional information and build bijections to avoid excessive learning
and storage costs. We introduce a reconstruction loss during semantic training
to improve Gaussian positions and shapes. To the best of our knowledge, we are
the first to address the 3D semantic field problem with sparse pose-free
inputs. Experimental results show that SparseLGS achieves comparable quality
when reconstructing semantic fields with fewer inputs (3-4 views) compared to
previous SOTA methods with dense input. Besides, when using the same sparse
input, SparseLGS leads significantly in quality and heavily improves the
computation speed (5$\times$speedup). Project page:
https://ustc3dv.github.io/SparseLGS";Jun Hu<author:sep>Zhang Chen<author:sep>Zhong Li<author:sep>Yi Xu<author:sep>Juyong Zhang;http://arxiv.org/pdf/2412.02245v2;cs.CV;Project Page: https://ustc3dv.github.io/SparseLGS;gaussian splatting
2412.01718v1;http://arxiv.org/abs/2412.01718v1;2024-12-02;HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for  Autonomous Driving;"In the past few decades, autonomous driving algorithms have made significant
progress in perception, planning, and control. However, evaluating individual
components does not fully reflect the performance of entire systems,
highlighting the need for more holistic assessment methods. This motivates the
development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator
for evaluating autonomous driving algorithms. We achieve this by lifting
captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving
the rendering quality for closed-loop scenarios, and building the closed-loop
environment. In terms of rendering, We tackle challenges of novel view
synthesis in closed-loop scenarios, including viewpoint extrapolation and
360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further
enables the full closed simulation loop, dynamically updating the ego and actor
states and observations based on control commands. Moreover, HUGSIM offers a
comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo,
nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair
and realistic evaluation platform for existing autonomous driving algorithms.
HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks
the potential for fine-tuning autonomous driving algorithms in a photorealistic
closed-loop setting.";Hongyu Zhou<author:sep>Longzhong Lin<author:sep>Jiabao Wang<author:sep>Yichong Lu<author:sep>Dongfeng Bai<author:sep>Bingbing Liu<author:sep>Yue Wang<author:sep>Andreas Geiger<author:sep>Yiyi Liao;http://arxiv.org/pdf/2412.01718v1;cs.CV;Our project page is at https://xdimlab.github.io/HUGSIM;gaussian splatting
2412.01543v1;http://arxiv.org/abs/2412.01543v1;2024-12-02;6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting;"Efficient and accurate object pose estimation is an essential component for
modern vision systems in many applications such as Augmented Reality,
autonomous driving, and robotics. While research in model-based 6D object pose
estimation has delivered promising results, model-free methods are hindered by
the high computational load in rendering and inferring consistent poses of
arbitrary objects in a live RGB-D video stream. To address this issue, we
present 6DOPE-GS, a novel method for online 6D object pose estimation \&
tracking with a single RGB-D camera by effectively leveraging advances in
Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of
Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses
and 3D object reconstruction. To achieve the necessary efficiency and accuracy
for live tracking, our method uses incremental 2D Gaussian Splatting with an
intelligent dynamic keyframe selection procedure to achieve high spatial object
coverage and prevent erroneous pose updates. We also propose an opacity
statistic-based pruning mechanism for adaptive Gaussian density control, to
ensure training stability and efficiency. We evaluate our method on the HO3D
and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of
state-of-the-art baselines for model-free simultaneous 6D pose tracking and
reconstruction while providing a 5$\times$ speedup. We also demonstrate the
method's suitability for live, dynamic object tracking and reconstruction in a
real-world setting.";Yufeng Jin<author:sep>Vignesh Prasad<author:sep>Snehal Jauhri<author:sep>Mathias Franzius<author:sep>Georgia Chalvatzaki;http://arxiv.org/pdf/2412.01543v1;cs.CV;;gaussian splatting
2412.01807v1;http://arxiv.org/abs/2412.01807v1;2024-12-02;Occam's LGS: A Simple Approach for Language Gaussian Splatting;"TL;DR: Gaussian Splatting is a widely adopted approach for 3D scene
representation that offers efficient, high-quality 3D reconstruction and
rendering. A major reason for the success of 3DGS is its simplicity of
representing a scene with a set of Gaussians, which makes it easy to interpret
and adapt. To enhance scene understanding beyond the visual representation,
approaches have been developed that extend 3D Gaussian Splatting with semantic
vision-language features, especially allowing for open-set tasks. In this
setting, the language features of 3D Gaussian Splatting are often aggregated
from multiple 2D views. Existing works address this aggregation problem using
cumbersome techniques that lead to high computational cost and training time.
  In this work, we show that the sophisticated techniques for language-grounded
3D Gaussian Splatting are simply unnecessary. Instead, we apply Occam's razor
to the task at hand and perform weighted multi-view feature aggregation using
the weights derived from the standard rendering process, followed by a simple
heuristic-based noisy Gaussian filtration. Doing so offers us state-of-the-art
results with a speed-up of two orders of magnitude. We showcase our results in
two commonly used benchmark datasets: LERF and 3D-OVS. Our simple approach
allows us to perform reasoning directly in the language features, without any
compression whatsoever. Such modeling in turn offers easy scene manipulation,
unlike the existing methods -- which we illustrate using an application of
object insertion in the scene. Furthermore, we provide a thorough discussion
regarding the significance of our contributions within the context of the
current literature. Project Page: https://insait-institute.github.io/OccamLGS/";Jiahuan Cheng<author:sep>Jan-Nico Zaech<author:sep>Luc Van Gool<author:sep>Danda Pani Paudel;http://arxiv.org/pdf/2412.01807v1;cs.CV;Project Page: https://insait-institute.github.io/OccamLGS/;gaussian splatting
2412.01717v1;http://arxiv.org/abs/2412.01717v1;2024-12-02;Driving Scene Synthesis on Free-form Trajectories with Generative Prior;"Driving scene synthesis along free-form trajectories is essential for driving
simulations to enable closed-loop evaluation of end-to-end driving policies.
While existing methods excel at novel view synthesis on recorded trajectories,
they face challenges with novel trajectories due to limited views of driving
videos and the vastness of driving environments. To tackle this challenge, we
propose a novel free-form driving view synthesis approach, dubbed DriveX, by
leveraging video generative prior to optimize a 3D model across a variety of
trajectories. Concretely, we crafted an inverse problem that enables a video
diffusion model to be utilized as a prior for many-trajectory optimization of a
parametric 3D model (e.g., Gaussian splatting). To seamlessly use the
generative prior, we iteratively conduct this process during optimization. Our
resulting model can produce high-fidelity virtual driving environments outside
the recorded trajectory, enabling free-form trajectory driving simulation.
Beyond real driving scenes, DriveX can also be utilized to simulate virtual
driving worlds from AI-generated videos.";Zeyu Yang<author:sep>Zijie Pan<author:sep>Yuankun Yang<author:sep>Xiatian Zhu<author:sep>Li Zhang;http://arxiv.org/pdf/2412.01717v1;cs.CV;;gaussian splatting
2412.01552v2;http://arxiv.org/abs/2412.01552v2;2024-12-02;GFreeDet: Exploiting Gaussian Splatting and Foundation Models for  Model-free Unseen Object Detection in the BOP Challenge 2024;"In this report, we provide the technical details of the submitted method
GFreeDet, which exploits Gaussian splatting and vision Foundation models for
the model-free unseen object Detection track in the BOP 2024 Challenge.";Xingyu Liu<author:sep>Yingyue Li<author:sep>Chengxi Li<author:sep>Gu Wang<author:sep>Chenyangguang Zhang<author:sep>Ziqin Huang<author:sep>Xiangyang Ji;http://arxiv.org/pdf/2412.01552v2;cs.CV;;gaussian splatting
2412.01931v1;http://arxiv.org/abs/2412.01931v1;2024-12-02;Planar Gaussian Splatting;"This paper presents Planar Gaussian Splatting (PGS), a novel neural rendering
approach to learn the 3D geometry and parse the 3D planes of a scene, directly
from multiple RGB images. The PGS leverages Gaussian primitives to model the
scene and employ a hierarchical Gaussian mixture approach to group them.
Similar Gaussians are progressively merged probabilistically in the
tree-structured Gaussian mixtures to identify distinct 3D plane instances and
form the overall 3D scene geometry. In order to enable the grouping, the
Gaussian primitives contain additional parameters, such as plane descriptors
derived by lifting 2D masks from a general 2D segmentation model and surface
normals. Experiments show that the proposed PGS achieves state-of-the-art
performance in 3D planar reconstruction without requiring either 3D plane
labels or depth supervision. In contrast to existing supervised methods that
have limited generalizability and struggle under domain shift, PGS maintains
its performance across datasets thanks to its neural rendering and
scene-specific optimization mechanism, while also being significantly faster
than existing optimization-based approaches.";Farhad G. Zanjani<author:sep>Hong Cai<author:sep>Hanno Ackermann<author:sep>Leila Mirvakhabova<author:sep>Fatih Porikli;http://arxiv.org/pdf/2412.01931v1;cs.CV;"IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),
  2025";gaussian splatting
2412.01745v1;http://arxiv.org/abs/2412.01745v1;2024-12-02;Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale  Aerial-to-Ground Scenes;"Seamless integration of both aerial and street view images remains a
significant challenge in neural scene reconstruction and rendering. Existing
methods predominantly focus on single domain, limiting their applications in
immersive environments, which demand extensive free view exploration with large
view changes both horizontally and vertically. We introduce Horizon-GS, a novel
approach built upon Gaussian Splatting techniques, tackles the unified
reconstruction and rendering for aerial and street views. Our method addresses
the key challenges of combining these perspectives with a new training
strategy, overcoming viewpoint discrepancies to generate high-fidelity scenes.
We also curate a high-quality aerial-to-ground views dataset encompassing both
synthetic and real-world scene to advance further research. Experiments across
diverse urban scene datasets confirm the effectiveness of our method.";Lihan Jiang<author:sep>Kerui Ren<author:sep>Mulin Yu<author:sep>Linning Xu<author:sep>Junting Dong<author:sep>Tao Lu<author:sep>Feng Zhao<author:sep>Dahua Lin<author:sep>Bo Dai;http://arxiv.org/pdf/2412.01745v1;cs.CV;;gaussian splatting
2412.01823v1;http://arxiv.org/abs/2412.01823v1;2024-12-02;HDGS: Textured 2D Gaussian Splatting for Enhanced Scene Rendering;"Recent advancements in neural rendering, particularly 2D Gaussian Splatting
(2DGS), have shown promising results for jointly reconstructing fine appearance
and geometry by leveraging 2D Gaussian surfels. However, current methods face
significant challenges when rendering at arbitrary viewpoints, such as
anti-aliasing for down-sampled rendering, and texture detail preservation for
high-resolution rendering. We proposed a novel method to align the 2D surfels
with texture maps and augment it with per-ray depth sorting and fisher-based
pruning for rendering consistency and efficiency. With correct order,
per-surfel texture maps significantly improve the capabilities to capture fine
details. Additionally, to render high-fidelity details in varying viewpoints,
we designed a frustum-based sampling method to mitigate the aliasing artifacts.
Experimental results on benchmarks and our custom texture-rich dataset
demonstrate that our method surpasses existing techniques, particularly in
detail preservation and anti-aliasing.";Yunzhou Song<author:sep>Heguang Lin<author:sep>Jiahui Lei<author:sep>Lingjie Liu<author:sep>Kostas Daniilidis;http://arxiv.org/pdf/2412.01823v1;cs.CV;Project Page: https://timsong412.github.io/HDGS-ProjPage/;gaussian splatting
2412.01792v1;http://arxiv.org/abs/2412.01792v1;2024-12-02;CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D  Diffusion;"Recent advances in 3D representations, such as Neural Radiance Fields and 3D
Gaussian Splatting, have greatly improved realistic scene modeling and
novel-view synthesis. However, achieving controllable and consistent editing in
dynamic 3D scenes remains a significant challenge. Previous work is largely
constrained by its editing backbones, resulting in inconsistent edits and
limited controllability. In our work, we introduce a novel framework that first
fine-tunes the InstructPix2Pix model, followed by a two-stage optimization of
the scene based on deformable 3D Gaussians. Our fine-tuning enables the model
to ""learn"" the editing ability from a single edited reference image,
transforming the complex task of dynamic scene editing into a simple 2D image
editing process. By directly learning editing regions and styles from the
reference, our approach enables consistent and precise local edits without the
need for tracking desired editing regions, effectively addressing key
challenges in dynamic scene editing. Then, our two-stage optimization
progressively edits the trained dynamic scene, using a designed edited image
buffer to accelerate convergence and improve temporal consistency. Compared to
state-of-the-art methods, our approach offers more flexible and controllable
local scene editing, achieving high-quality and consistent results.";Kai He<author:sep>Chin-Hsuan Wu<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2412.01792v1;cs.CV;Project page: https://ihe-kaii.github.io/CTRL-D/;gaussian splatting
2412.01682v2;http://arxiv.org/abs/2412.01682v2;2024-12-02;Diffusion Models with Anisotropic Gaussian Splatting for Image  Inpainting;"Image inpainting is a fundamental task in computer vision, aiming to restore
missing or corrupted regions in images realistically. While recent deep
learning approaches have significantly advanced the state-of-the-art,
challenges remain in maintaining structural continuity and generating coherent
textures, particularly in large missing areas. Diffusion models have shown
promise in generating high-fidelity images but often lack the structural
guidance necessary for realistic inpainting. We propose a novel inpainting
method that combines diffusion models with anisotropic Gaussian splatting to
capture both local structures and global context effectively. By modeling
missing regions using anisotropic Gaussian functions that adapt to local image
gradients, our approach provides structural guidance to the diffusion-based
inpainting network. The Gaussian splat maps are integrated into the diffusion
process, enhancing the model's ability to generate high-fidelity and
structurally coherent inpainting results. Extensive experiments demonstrate
that our method outperforms state-of-the-art techniques, producing visually
plausible results with enhanced structural integrity and texture realism.";Jacob Fein-Ashley<author:sep>Benjamin Fein-Ashley;http://arxiv.org/pdf/2412.01682v2;cs.CV;;gaussian splatting
2412.01583v1;http://arxiv.org/abs/2412.01583v1;2024-12-02;3DSceneEditor: Controllable 3D Scene Editing with Gaussian Splatting;"The creation of 3D scenes has traditionally been both labor-intensive and
costly, requiring designers to meticulously configure 3D assets and
environments. Recent advancements in generative AI, including text-to-3D and
image-to-3D methods, have dramatically reduced the complexity and cost of this
process. However, current techniques for editing complex 3D scenes continue to
rely on generally interactive multi-step, 2D-to-3D projection methods and
diffusion-based techniques, which often lack precision in control and hamper
real-time performance. In this work, we propose 3DSceneEditor, a fully 3D-based
paradigm for real-time, precise editing of intricate 3D scenes using Gaussian
Splatting. Unlike conventional methods, 3DSceneEditor operates through a
streamlined 3D pipeline, enabling direct manipulation of Gaussians for
efficient, high-quality edits based on input prompts.The proposed framework (i)
integrates a pre-trained instance segmentation model for semantic labeling;
(ii) employs a zero-shot grounding approach with CLIP to align target objects
with user prompts; and (iii) applies scene modifications, such as object
addition, repositioning, recoloring, replacing, and deletion directly on
Gaussians. Extensive experimental results show that 3DSceneEditor achieves
superior editing precision and speed with respect to current SOTA 3D scene
editing approaches, establishing a new benchmark for efficient and interactive
3D scene customization.";Ziyang Yan<author:sep>Lei Li<author:sep>Yihua Shao<author:sep>Siyu Chen<author:sep>Wuzong Kai<author:sep>Jenq-Neng Hwang<author:sep>Hao Zhao<author:sep>Fabio Remondino;http://arxiv.org/pdf/2412.01583v1;cs.CV;Project Page: https://ziyangyan.github.io/3DSceneEditor;gaussian splatting
2412.01217v1;http://arxiv.org/abs/2412.01217v1;2024-12-02;RGBDS-SLAM: A RGB-D Semantic Dense SLAM Based on 3D Multi Level Pyramid  Gaussian Splatting;"High-quality reconstruction is crucial for dense SLAM. Recent popular
approaches utilize 3D Gaussian Splatting (3D GS) techniques for RGB, depth, and
semantic reconstruction of scenes. However, these methods often overlook issues
of detail and consistency in different parts of the scene. To address this, we
propose RGBDS-SLAM, a RGB-D semantic dense SLAM system based on 3D multi-level
pyramid gaussian splatting, which enables high-quality dense reconstruction of
scene RGB, depth, and semantics.In this system, we introduce a 3D multi-level
pyramid gaussian splatting method that restores scene details by extracting
multi-level image pyramids for gaussian splatting training, ensuring
consistency in RGB, depth, and semantic reconstructions. Additionally, we
design a tightly-coupled multi-features reconstruction optimization mechanism,
allowing the reconstruction accuracy of RGB, depth, and semantic maps to
mutually enhance each other during the rendering optimization process.
Extensive quantitative, qualitative, and ablation experiments on the Replica
and ScanNet public datasets demonstrate that our proposed method outperforms
current state-of-the-art methods. The open-source code will be available at:
https://github.com/zhenzhongcao/RGBDS-SLAM.";Zhenzhong Cao;http://arxiv.org/pdf/2412.01217v1;cs.CV;;gaussian splatting
2412.01553v1;http://arxiv.org/abs/2412.01553v1;2024-12-02;SfM-Free 3D Gaussian Splatting via Hierarchical Training;"Standard 3D Gaussian Splatting (3DGS) relies on known or pre-computed camera
poses and a sparse point cloud, obtained from structure-from-motion (SfM)
preprocessing, to initialize and grow 3D Gaussians. We propose a novel SfM-Free
3DGS (SFGS) method for video input, eliminating the need for known camera poses
and SfM preprocessing. Our approach introduces a hierarchical training strategy
that trains and merges multiple 3D Gaussian representations -- each optimized
for specific scene regions -- into a single, unified 3DGS model representing
the entire scene. To compensate for large camera motions, we leverage video
frame interpolation models. Additionally, we incorporate multi-source
supervision to reduce overfitting and enhance representation. Experimental
results reveal that our approach significantly surpasses state-of-the-art
SfM-free novel view synthesis methods. On the Tanks and Temples dataset, we
improve PSNR by an average of 2.25dB, with a maximum gain of 3.72dB in the best
scene. On the CO3D-V2 dataset, we achieve an average PSNR boost of 1.74dB, with
a top gain of 3.90dB. The code is available at
https://github.com/jibo27/3DGS_Hierarchical_Training.";Bo Ji<author:sep>Angela Yao;http://arxiv.org/pdf/2412.01553v1;cs.CV;;gaussian splatting
2412.01402v1;http://arxiv.org/abs/2412.01402v1;2024-12-02;ULSR-GS: Ultra Large-scale Surface Reconstruction Gaussian Splatting  with Multi-View Geometric Consistency;"While Gaussian Splatting (GS) demonstrates efficient and high-quality scene
rendering and small area surface extraction ability, it falls short in handling
large-scale aerial image surface extraction tasks. To overcome this, we present
ULSR-GS, a framework dedicated to high-fidelity surface extraction in
ultra-large-scale scenes, addressing the limitations of existing GS-based mesh
extraction methods. Specifically, we propose a point-to-photo partitioning
approach combined with a multi-view optimal view matching principle to select
the best training images for each sub-region. Additionally, during training,
ULSR-GS employs a densification strategy based on multi-view geometric
consistency to enhance surface extraction details. Experimental results
demonstrate that ULSR-GS outperforms other state-of-the-art GS-based works on
large-scale aerial photogrammetry benchmark datasets, significantly improving
surface extraction accuracy in complex urban environments. Project page:
https://ulsrgs.github.io.";Zhuoxiao Li<author:sep>Shanliang Yao<author:sep>Qizhong Gao<author:sep>Angel F. Garcia-Fernandez<author:sep>Yong Yue<author:sep>Xiaohui Zhu;http://arxiv.org/pdf/2412.01402v1;cs.CV;Project page: https://ulsrgs.github.io;gaussian splatting
2412.00905v1;http://arxiv.org/abs/2412.00905v1;2024-12-01;Ref-GS: Directional Factorization for 2D Gaussian Splatting;"In this paper, we introduce Ref-GS, a novel approach for directional light
factorization in 2D Gaussian splatting, which enables photorealistic
view-dependent appearance rendering and precise geometry recovery. Ref-GS
builds upon the deferred rendering of Gaussian splatting and applies
directional encoding to the deferred-rendered surface, effectively reducing the
ambiguity between orientation and viewing angle. Next, we introduce a spherical
Mip-grid to capture varying levels of surface roughness, enabling
roughness-aware Gaussian shading. Additionally, we propose a simple yet
efficient geometry-lighting factorization that connects geometry and lighting
via the vector outer product, significantly reducing renderer overhead when
integrating volumetric attributes. Our method achieves superior photorealistic
rendering for a range of open-world scenes while also accurately recovering
geometry.";Youjia Zhang<author:sep>Anpei Chen<author:sep>Yumin Wan<author:sep>Zikai Song<author:sep>Junqing Yu<author:sep>Yawei Luo<author:sep>Wei Yang;http://arxiv.org/pdf/2412.00905v1;cs.CV;Project page: https://ref-gs.github.io/;gaussian splatting
2412.00851v1;http://arxiv.org/abs/2412.00851v1;2024-12-01;DynSUP: Dynamic Gaussian Splatting from An Unposed Image Pair;"Recent advances in 3D Gaussian Splatting have shown promising results.
Existing methods typically assume static scenes and/or multiple images with
prior poses. Dynamics, sparse views, and unknown poses significantly increase
the problem complexity due to insufficient geometric constraints. To overcome
this challenge, we propose a method that can use only two images without prior
poses to fit Gaussians in dynamic environments. To achieve this, we introduce
two technical contributions. First, we propose an object-level two-view bundle
adjustment. This strategy decomposes dynamic scenes into piece-wise rigid
components, and jointly estimates the camera pose and motions of dynamic
objects. Second, we design an SE(3) field-driven Gaussian training method. It
enables fine-grained motion modeling through learnable per-Gaussian
transformations. Our method leads to high-fidelity novel view synthesis of
dynamic scenes while accurately preserving temporal consistency and object
motion. Experiments on both synthetic and real-world datasets demonstrate that
our method significantly outperforms state-of-the-art approaches designed for
the cases of static environments, multiple images, and/or known poses. Our
project page is available at https://colin-de.github.io/DynSUP/.";Weihang Li<author:sep>Weirong Chen<author:sep>Shenhan Qian<author:sep>Jiajie Chen<author:sep>Daniel Cremers<author:sep>Haoang Li;http://arxiv.org/pdf/2412.00851v1;cs.CV;;gaussian splatting
2412.00734v1;http://arxiv.org/abs/2412.00734v1;2024-12-01;ChatSplat: 3D Conversational Gaussian Splatting;"Humans naturally interact with their 3D surroundings using language, and
modeling 3D language fields for scene understanding and interaction has gained
growing interest. This paper introduces ChatSplat, a system that constructs a
3D language field, enabling rich chat-based interaction within 3D space. Unlike
existing methods that primarily use CLIP-derived language features focused
solely on segmentation, ChatSplat facilitates interaction on three levels:
objects, views, and the entire 3D scene. For view-level interaction, we
designed an encoder that encodes the rendered feature map of each view into
tokens, which are then processed by a large language model (LLM) for
conversation. At the scene level, ChatSplat combines multi-view tokens,
enabling interactions that consider the entire scene. For object-level
interaction, ChatSplat uses a patch-wise language embedding, unlike LangSplat's
pixel-wise language embedding that implicitly includes mask and embedding.
Here, we explicitly decouple the language embedding into separate mask and
feature map representations, allowing more flexible object-level interaction.
To address the challenge of learning 3D Gaussians posed by the complex and
diverse distribution of language embeddings used in the LLM, we introduce a
learnable normalization technique to standardize these embeddings, facilitating
effective learning. Extensive experimental results demonstrate that ChatSplat
supports multi-level interactions -- object, view, and scene -- within 3D
space, enhancing both understanding and engagement.";Hanlin Chen<author:sep>Fangyin Wei<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2412.00734v1;cs.CV;;gaussian splatting
2412.00682v1;http://arxiv.org/abs/2412.00682v1;2024-12-01;FlashSLAM: Accelerated RGB-D SLAM for Real-Time 3D Scene Reconstruction  with Gaussian Splatting;"We present FlashSLAM, a novel SLAM approach that leverages 3D Gaussian
Splatting for efficient and robust 3D scene reconstruction. Existing 3DGS-based
SLAM methods often fall short in sparse view settings and during large camera
movements due to their reliance on gradient descent-based optimization, which
is both slow and inaccurate. FlashSLAM addresses these limitations by combining
3DGS with a fast vision-based camera tracking technique, utilizing a pretrained
feature matching model and point cloud registration for precise pose estimation
in under 80 ms - a 90% reduction in tracking time compared to SplaTAM - without
costly iterative rendering. In sparse settings, our method achieves up to a 92%
improvement in average tracking accuracy over previous methods. Additionally,
it accounts for noise in depth sensors, enhancing robustness when using
unspecialized devices such as smartphones. Extensive experiments show that
FlashSLAM performs reliably across both sparse and dense settings, in synthetic
and real-world environments. Evaluations on benchmark datasets highlight its
superior accuracy and efficiency, establishing FlashSLAM as a versatile and
high-performance solution for SLAM, advancing the state-of-the-art in 3D
reconstruction across diverse applications.";Phu Pham<author:sep>Damon Conover<author:sep>Aniket Bera;http://arxiv.org/pdf/2412.00682v1;cs.CV;16 pages, 9 figures, 13 tables;gaussian splatting
2412.00845v1;http://arxiv.org/abs/2412.00845v1;2024-12-01;SAGA: Surface-Aligned Gaussian Avatar;"This paper presents a Surface-Aligned Gaussian representation for creating
animatable human avatars from monocular videos,aiming at improving the novel
view and pose synthesis performance while ensuring fast training and real-time
rendering. Recently,3DGS has emerged as a more efficient and expressive
alternative to NeRF, and has been used for creating dynamic human avatars.
However,when applied to the severely ill-posed task of monocular dynamic
reconstruction, the Gaussians tend to overfit the constantly changing regions
such as clothes wrinkles or shadows since these regions cannot provide
consistent supervision, resulting in noisy geometry and abrupt deformation that
typically fail to generalize under novel views and poses.To address these
limitations, we present SAGA,i.e.,Surface-Aligned Gaussian Avatar,which aligns
the Gaussians with a mesh to enforce well-defined geometry and consistent
deformation, thereby improving generalization under novel views and poses.
Unlike existing strict alignment methods that suffer from limited expressive
power and low realism,SAGA employs a two-stage alignment strategy where the
Gaussians are first adhered on while then detached from the mesh, thus
facilitating both good geometry and high expressivity. In the Adhered Stage, we
improve the flexibility of Adhered-on-Mesh Gaussians by allowing them to flow
on the mesh, in contrast to existing methods that rigidly bind Gaussians to
fixed location. In the second Detached Stage, we introduce a Gaussian-Mesh
Alignment regularization, which allows us to unleash the expressivity by
detaching the Gaussians but maintain the geometric alignment by minimizing
their location and orientation offsets from the bound triangles. Finally, since
the Gaussians may drift outside the bound triangles during optimization, an
efficient Walking-on-Mesh strategy is proposed to dynamically update the bound
triangles.";Ronghan Chen<author:sep>Yang Cong<author:sep>Jiayue Liu;http://arxiv.org/pdf/2412.00845v1;cs.CV;"Submitted to TPAMI. Major Revision. Project page:
  https://gostinshell.github.io/SAGA/";nerf
2412.00754v1;http://arxiv.org/abs/2412.00754v1;2024-12-01;CtrlNeRF: The Generative Neural Radiation Fields for the Controllable  Synthesis of High-fidelity 3D-Aware Images;"The neural radiance field (NERF) advocates learning the continuous
representation of 3D geometry through a multilayer perceptron (MLP). By
integrating this into a generative model, the generative neural radiance field
(GRAF) is capable of producing images from random noise z without 3D
supervision. In practice, the shape and appearance are modeled by z_s and z_a,
respectively, to manipulate them separately during inference. However, it is
challenging to represent multiple scenes using a solitary MLP and precisely
control the generation of 3D geometry in terms of shape and appearance. In this
paper, we introduce a controllable generative model (i.e. \textbf{CtrlNeRF})
that uses a single MLP network to represent multiple scenes with shared
weights. Consequently, we manipulated the shape and appearance codes to realize
the controllable generation of high-fidelity images with 3D consistency.
Moreover, the model enables the synthesis of novel views that do not exist in
the training sets via camera pose alteration and feature interpolation.
Extensive experiments were conducted to demonstrate its superiority in 3D-aware
image generation compared to its counterparts.";Jian Liu<author:sep>Zhen Yu;http://arxiv.org/pdf/2412.00754v1;cs.CV;;nerf
2412.00814v1;http://arxiv.org/abs/2412.00814v1;2024-12-01;VR-Doh: Hands-on 3D Modeling in Virtual Reality;"We present VR-Doh, a hands-on 3D modeling system designed for creating and
manipulating elastoplastic objects in virtual reality (VR). The system employs
the Material Point Method (MPM) for simulating realistic large deformations and
incorporates optimized Gaussian Splatting for seamless rendering. With direct,
hand-based interactions, users can naturally sculpt, deform, and edit objects
interactively. To achieve real-time performance, we developed localized
simulation techniques, optimized collision handling, and separated appearance
and physical representations, ensuring smooth and responsive user interaction.
The system supports both freeform creation and precise adjustments, catering to
diverse modeling tasks. A user study involving novice and experienced users
highlights the system's intuitive design, immersive feedback, and creative
potential. Compared to traditional geometry-based modeling tools, our approach
offers improved accessibility and natural interaction in specific contexts.";Zhaofeng Luo<author:sep>Zhitong Cui<author:sep>Shijian Luo<author:sep>Mengyu Chu<author:sep>Minchen Li;http://arxiv.org/pdf/2412.00814v1;cs.GR;;gaussian splatting
2411.19548v1;http://arxiv.org/abs/2411.19548v1;2024-11-29;ReconDreamer: Crafting World Models for Driving Scene Reconstruction via  Online Restoration;"Closed-loop simulation is crucial for end-to-end autonomous driving. Existing
sensor simulation methods (e.g., NeRF and 3DGS) reconstruct driving scenes
based on conditions that closely mirror training data distributions. However,
these methods struggle with rendering novel trajectories, such as lane changes.
Recent works have demonstrated that integrating world model knowledge
alleviates these issues. Despite their efficiency, these approaches still
encounter difficulties in the accurate representation of more complex
maneuvers, with multi-lane shifts being a notable example. Therefore, we
introduce ReconDreamer, which enhances driving scene reconstruction through
incremental integration of world model knowledge. Specifically, DriveRestorer
is proposed to mitigate artifacts via online restoration. This is complemented
by a progressive data update strategy designed to ensure high-quality rendering
for more complex maneuvers. To the best of our knowledge, ReconDreamer is the
first method to effectively render in large maneuvers. Experimental results
demonstrate that ReconDreamer outperforms Street Gaussians in the NTA-IoU,
NTL-IoU, and FID, with relative improvements by 24.87%, 6.72%, and 29.97%.
Furthermore, ReconDreamer surpasses DriveDreamer4D with PVG during large
maneuver rendering, as verified by a relative improvement of 195.87% in the
NTA-IoU metric and a comprehensive user study.";Chaojun Ni<author:sep>Guosheng Zhao<author:sep>Xiaofeng Wang<author:sep>Zheng Zhu<author:sep>Wenkang Qin<author:sep>Guan Huang<author:sep>Chen Liu<author:sep>Yuyin Chen<author:sep>Yida Wang<author:sep>Xueyang Zhang<author:sep>Yifei Zhan<author:sep>Kun Zhan<author:sep>Peng Jia<author:sep>Xianpeng Lang<author:sep>Xingang Wang<author:sep>Wenjun Mei;http://arxiv.org/pdf/2411.19548v1;cs.CV;Project Page: https://recondreamer.github.io;nerf
2411.19525v1;http://arxiv.org/abs/2411.19525v1;2024-11-29;LokiTalk: Learning Fine-Grained and Generalizable Correspondences to  Enhance NeRF-based Talking Head Synthesis;"Despite significant progress in talking head synthesis since the introduction
of Neural Radiance Fields (NeRF), visual artifacts and high training costs
persist as major obstacles to large-scale commercial adoption. We propose that
identifying and establishing fine-grained and generalizable correspondences
between driving signals and generated results can simultaneously resolve both
problems. Here we present LokiTalk, a novel framework designed to enhance
NeRF-based talking heads with lifelike facial dynamics and improved training
efficiency. To achieve fine-grained correspondences, we introduce
Region-Specific Deformation Fields, which decompose the overall portrait motion
into lip movements, eye blinking, head pose, and torso movements. By
hierarchically modeling the driving signals and their associated regions
through two cascaded deformation fields, we significantly improve dynamic
accuracy and minimize synthetic artifacts. Furthermore, we propose ID-Aware
Knowledge Transfer, a plug-and-play module that learns generalizable dynamic
and static correspondences from multi-identity videos, while simultaneously
extracting ID-specific dynamic and static features to refine the depiction of
individual characters. Comprehensive evaluations demonstrate that LokiTalk
delivers superior high-fidelity results and training efficiency compared to
previous methods. The code will be released upon acceptance.";Tianqi Li<author:sep>Ruobing Zheng<author:sep>Bonan Li<author:sep>Zicheng Zhang<author:sep>Meng Wang<author:sep>Jingdong Chen<author:sep>Ming Yang;http://arxiv.org/pdf/2411.19525v1;cs.CV;;nerf
2411.19903v1;http://arxiv.org/abs/2411.19903v1;2024-11-29;$C^{3}$-NeRF: Modeling Multiple Scenes via Conditional-cum-Continual  Neural Radiance Fields;"Neural radiance fields (NeRF) have exhibited highly photorealistic rendering
of novel views through per-scene optimization over a single 3D scene. With the
growing popularity of NeRF and its variants, they have become ubiquitous and
have been identified as efficient 3D resources. However, they are still far
from being scalable since a separate model needs to be stored for each scene,
and the training time increases linearly with every newly added scene.
Surprisingly, the idea of encoding multiple 3D scenes into a single NeRF model
is heavily under-explored. In this work, we propose a novel
conditional-cum-continual framework, called $C^{3}$-NeRF, to accommodate
multiple scenes into the parameters of a single neural radiance field. Unlike
conventional approaches that leverage feature extractors and pre-trained priors
for scene conditioning, we use simple pseudo-scene labels to model multiple
scenes in NeRF. Interestingly, we observe the framework is also inherently
continual (via generative replay) with minimal, if not no, forgetting of the
previously learned scenes. Consequently, the proposed framework adapts to
multiple new scenes without necessarily accessing the old data. Through
extensive qualitative and quantitative evaluation using synthetic and real
datasets, we demonstrate the inherent capacity of the NeRF model to accommodate
multiple scenes with high-quality novel-view renderings without adding
additional parameters. We provide implementation details and dynamic
visualizations of our results in the supplementary file.";Prajwal Singh<author:sep>Ashish Tiwari<author:sep>Gautam Vashishtha<author:sep>Shanmuganathan Raman;http://arxiv.org/pdf/2411.19903v1;cs.CV;;nerf
2411.19895v2;http://arxiv.org/abs/2411.19895v2;2024-11-29;GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting;"3D Gaussian Splatting (3DGS) has recently created impressive assets for
various applications. However, the copyright of these assets is not well
protected as existing watermarking methods are not suited for 3DGS considering
security, capacity, and invisibility. Besides, these methods often require
hours or even days for optimization, limiting the application scenarios. In
this paper, we propose GuardSplat, an innovative and efficient framework that
effectively protects the copyright of 3DGS assets. Specifically, 1) We first
propose a CLIP-guided Message Decoupling Optimization module for training the
message decoder, leveraging CLIP's aligning capability and rich representations
to achieve a high extraction accuracy with minimal optimization costs,
presenting exceptional capability and efficiency. 2) Then, we propose a
Spherical-harmonic-aware (SH-aware) Message Embedding module tailored for 3DGS,
which employs a set of SH offsets to seamlessly embed the message into the SH
features of each 3D Gaussian while maintaining the original 3D structure. It
enables the 3DGS assets to be watermarked with minimal fidelity trade-offs and
prevents malicious users from removing the messages from the model files,
meeting the demands for invisibility and security. 3) We further propose an
Anti-distortion Message Extraction module to improve robustness against various
visual distortions. Extensive experiments demonstrate that GuardSplat
outperforms the state-of-the-art methods and achieves fast optimization speed.";Zixuan Chen<author:sep>Guangcong Wang<author:sep>Jiahao Zhu<author:sep>Jianhuang Lai<author:sep>Xiaohua Xie;http://arxiv.org/pdf/2411.19895v2;cs.CV;"Project page: https://narcissusex.github.io/GuardSplat and Code:
  https://github.com/NarcissusEx/GuardSplat";gaussian splatting
2411.19420v1;http://arxiv.org/abs/2411.19420v1;2024-11-29;RF-3DGS: Wireless Channel Modeling with Radio Radiance Field and 3D  Gaussian Splatting;"Precisely modeling radio propagation in complex environments has been a
significant challenge, especially with the advent of 5G and beyond networks,
where managing massive antenna arrays demands more detailed information.
Traditional methods, such as empirical models and ray tracing, often fall
short, either due to insufficient details or with challenges for real-time
applications. Inspired by the newly proposed 3D Gaussian Splatting method in
computer vision domain, which outperforms in reconstructing optical radiance
fields, we propose RF-3DGS, a novel approach that enables precise site-specific
reconstruction of radio radiance fields from sparse samples. RF-3DGS can render
spatial spectra at arbitrary positions within 2 ms following a brief 3-minute
training period, effectively identifying dominant propagation paths at these
locations. Furthermore, RF-3DGS can provide fine-grained Channel State
Information (CSI) of these paths, including the angle of departure and delay.
Our experiments, calibrated through real-world measurements, demonstrate that
RF-3DGS not only significantly improves rendering quality, training speed, and
rendering speed compared to state-of-the-art methods but also holds great
potential for supporting wireless communication and advanced applications such
as Integrated Sensing and Communication (ISAC).";Lihao Zhang<author:sep>Haijian Sun<author:sep>Samuel Berweger<author:sep>Camillo Gentile<author:sep>Rose Qingyang Hu;http://arxiv.org/pdf/2411.19420v1;cs.NI;in submission to IEEE journals;gaussian splatting
2411.19551v1;http://arxiv.org/abs/2411.19551v1;2024-11-29;Bootstraping Clustering of Gaussians for View-consistent 3D Scene  Understanding;"Injecting semantics into 3D Gaussian Splatting (3DGS) has recently garnered
significant attention. While current approaches typically distill 3D semantic
features from 2D foundational models (e.g., CLIP and SAM) to facilitate novel
view segmentation and semantic understanding, their heavy reliance on 2D
supervision can undermine cross-view semantic consistency and necessitate
complex data preparation processes, therefore hindering view-consistent scene
understanding. In this work, we present FreeGS, an unsupervised
semantic-embedded 3DGS framework that achieves view-consistent 3D scene
understanding without the need for 2D labels. Instead of directly learning
semantic features, we introduce the IDentity-coupled Semantic Field (IDSF) into
3DGS, which captures both semantic representations and view-consistent instance
indices for each Gaussian. We optimize IDSF with a two-step alternating
strategy: semantics help to extract coherent instances in 3D space, while the
resulting instances regularize the injection of stable semantics from 2D space.
Additionally, we adopt a 2D-3D joint contrastive loss to enhance the
complementarity between view-consistent 3D geometry and rich semantics during
the bootstrapping process, enabling FreeGS to uniformly perform tasks such as
novel-view semantic segmentation, object selection, and 3D object detection.
Extensive experiments on LERF-Mask, 3D-OVS, and ScanNet datasets demonstrate
that FreeGS performs comparably to state-of-the-art methods while avoiding the
complex data preprocessing workload.";Wenbo Zhang<author:sep>Lu Zhang<author:sep>Ping Hu<author:sep>Liqian Ma<author:sep>Yunzhi Zhuge<author:sep>Huchuan Lu;http://arxiv.org/pdf/2411.19551v1;cs.CV;;gaussian splatting
2411.19454v2;http://arxiv.org/abs/2411.19454v2;2024-11-29;GausSurf: Geometry-Guided 3D Gaussian Splatting for Surface  Reconstruction;"3D Gaussian Splatting has achieved impressive performance in novel view
synthesis with real-time rendering capabilities. However, reconstructing
high-quality surfaces with fine details using 3D Gaussians remains a
challenging task. In this work, we introduce GausSurf, a novel approach to
high-quality surface reconstruction by employing geometry guidance from
multi-view consistency in texture-rich areas and normal priors in texture-less
areas of a scene. We observe that a scene can be mainly divided into two
primary regions: 1) texture-rich and 2) texture-less areas. To enforce
multi-view consistency at texture-rich areas, we enhance the reconstruction
quality by incorporating a traditional patch-match based Multi-View Stereo
(MVS) approach to guide the geometry optimization in an iterative scheme. This
scheme allows for mutual reinforcement between the optimization of Gaussians
and patch-match refinement, which significantly improves the reconstruction
results and accelerates the training process. Meanwhile, for the texture-less
areas, we leverage normal priors from a pre-trained normal estimation model to
guide optimization. Extensive experiments on the DTU and Tanks and Temples
datasets demonstrate that our method surpasses state-of-the-art methods in
terms of reconstruction quality and computation time.";Jiepeng Wang<author:sep>Yuan Liu<author:sep>Peng Wang<author:sep>Cheng Lin<author:sep>Junhui Hou<author:sep>Xin Li<author:sep>Taku Komura<author:sep>Wenping Wang;http://arxiv.org/pdf/2411.19454v2;cs.CV;Project page: https://jiepengwang.github.io/GausSurf/;gaussian splatting
2411.19594v1;http://arxiv.org/abs/2411.19594v1;2024-11-29;Tortho-Gaussian: Splatting True Digital Orthophoto Maps;"True Digital Orthophoto Maps (TDOMs) are essential products for digital twins
and Geographic Information Systems (GIS). Traditionally, TDOM generation
involves a complex set of traditional photogrammetric process, which may
deteriorate due to various challenges, including inaccurate Digital Surface
Model (DSM), degenerated occlusion detections, and visual artifacts in weak
texture regions and reflective surfaces, etc. To address these challenges, we
introduce TOrtho-Gaussian, a novel method inspired by 3D Gaussian Splatting
(3DGS) that generates TDOMs through orthogonal splatting of optimized
anisotropic Gaussian kernel. More specifically, we first simplify the
orthophoto generation by orthographically splatting the Gaussian kernels onto
2D image planes, formulating a geometrically elegant solution that avoids the
need for explicit DSM and occlusion detection. Second, to produce TDOM of
large-scale area, a divide-and-conquer strategy is adopted to optimize memory
usage and time efficiency of training and rendering for 3DGS. Lastly, we design
a fully anisotropic Gaussian kernel that adapts to the varying characteristics
of different regions, particularly improving the rendering quality of
reflective surfaces and slender structures. Extensive experimental evaluations
demonstrate that our method outperforms existing commercial software in several
aspects, including the accuracy of building boundaries, the visual quality of
low-texture regions and building facades. These results underscore the
potential of our approach for large-scale urban scene reconstruction, offering
a robust alternative for enhancing TDOM quality and scalability.";Xin Wang<author:sep>Wendi Zhang<author:sep>Hong Xie<author:sep>Haibin Ai<author:sep>Qiangqiang Yuan<author:sep>Zongqian Zhan;http://arxiv.org/pdf/2411.19594v1;cs.CV;"This work has been submitted to the IEEE Transactions on Geoscience
  and Remote Sensing for possible publication";gaussian splatting
2411.19654v1;http://arxiv.org/abs/2411.19654v1;2024-11-29;TexGaussian: Generating High-quality PBR Material via Octree-based 3D  Gaussian Splatting;"Physically Based Rendering (PBR) materials play a crucial role in modern
graphics, enabling photorealistic rendering across diverse environment maps.
Developing an effective and efficient algorithm that is capable of
automatically generating high-quality PBR materials rather than RGB texture for
3D meshes can significantly streamline the 3D content creation. Most existing
methods leverage pre-trained 2D diffusion models for multi-view image
synthesis, which often leads to severe inconsistency between the generated
textures and input 3D meshes. This paper presents TexGaussian, a novel method
that uses octant-aligned 3D Gaussian Splatting for rapid PBR material
generation. Specifically, we place each 3D Gaussian on the finest leaf node of
the octree built from the input 3D mesh to render the multiview images not only
for the albedo map but also for roughness and metallic. Moreover, our model is
trained in a regression manner instead of diffusion denoising, capable of
generating the PBR material for a 3D mesh in a single feed-forward process.
Extensive experiments on publicly available benchmarks demonstrate that our
method synthesizes more visually pleasing PBR materials and runs faster than
previous methods in both unconditional and text-conditional scenarios, which
exhibit better consistency with the given geometry. Our code and trained models
are available at https://3d-aigc.github.io/TexGaussian.";Bojun Xiong<author:sep>Jialun Liu<author:sep>Jiakui Hu<author:sep>Chenming Wu<author:sep>Jinbo Wu<author:sep>Xing Liu<author:sep>Chen Zhao<author:sep>Errui Ding<author:sep>Zhouhui Lian;http://arxiv.org/pdf/2411.19654v1;cs.CV;Technical Report;gaussian splatting
2411.19756v1;http://arxiv.org/abs/2411.19756v1;2024-11-29;DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering;"Gaussian splatting enables fast novel view synthesis in static 3D
environments. However, reconstructing real-world environments remains
challenging as distractors or occluders break the multi-view consistency
assumption required for accurate 3D reconstruction. Most existing methods rely
on external semantic information from pre-trained models, introducing
additional computational overhead as pre-processing steps or during
optimization. In this work, we propose a novel method, DeSplat, that directly
separates distractors and static scene elements purely based on volume
rendering of Gaussian primitives. We initialize Gaussians within each camera
view for reconstructing the view-specific distractors to separately model the
static 3D scene and distractors in the alpha compositing stages. DeSplat yields
an explicit scene separation of static elements and distractors, achieving
comparable results to prior distractor-free approaches without sacrificing
rendering speed. We demonstrate DeSplat's effectiveness on three benchmark data
sets for distractor-free novel view synthesis. See the project website at
https://aaltoml.github.io/desplat/.";Yihao Wang<author:sep>Marcus Klasson<author:sep>Matias Turkulainen<author:sep>Shuzhe Wang<author:sep>Juho Kannala<author:sep>Arno Solin;http://arxiv.org/pdf/2411.19756v1;cs.CV;;gaussian splatting
2411.19588v1;http://arxiv.org/abs/2411.19588v1;2024-11-29;Gaussian Splashing: Direct Volumetric Rendering Underwater;"In underwater images, most useful features are occluded by water. The extent
of the occlusion depends on imaging geometry and can vary even across a
sequence of burst images. As a result, 3D reconstruction methods robust on
in-air scenes, like Neural Radiance Field methods (NeRFs) or 3D Gaussian
Splatting (3DGS), fail on underwater scenes. While a recent underwater
adaptation of NeRFs achieved state-of-the-art results, it is impractically
slow: reconstruction takes hours and its rendering rate, in frames per second
(FPS), is less than 1. Here, we present a new method that takes only a few
minutes for reconstruction and renders novel underwater scenes at 140 FPS.
Named Gaussian Splashing, our method unifies the strengths and speed of 3DGS
with an image formation model for capturing scattering, introducing innovations
in the rendering and depth estimation procedures and in the 3DGS loss function.
Despite the complexities of underwater adaptation, our method produces images
at unparalleled speeds with superior details. Moreover, it reveals distant
scene details with far greater clarity than other methods, dramatically
improving reconstructed and rendered images. We demonstrate results on existing
datasets and a new dataset we have collected.
  Additional visual results are available at:
https://bgu-cs-vil.github.io/gaussiansplashingUW.github.io/ .";Nir Mualem<author:sep>Roy Amoyal<author:sep>Oren Freifeld<author:sep>Derya Akkaynak;http://arxiv.org/pdf/2411.19588v1;cs.CV;;nerf
2411.19271v1;http://arxiv.org/abs/2411.19271v1;2024-11-28;AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors  for Indoor Room Reconstruction Using Smartphones;"Geometric priors are often used to enhance 3D reconstruction. With many
smartphones featuring low-resolution depth sensors and the prevalence of
off-the-shelf monocular geometry estimators, incorporating geometric priors as
regularization signals has become common in 3D vision tasks. However, the
accuracy of depth estimates from mobile devices is typically poor for highly
detailed geometry, and monocular estimators often suffer from poor multi-view
consistency and precision. In this work, we propose an approach for joint
surface depth and normal refinement of Gaussian Splatting methods for accurate
3D reconstruction of indoor scenes. We develop supervision strategies that
adaptively filters low-quality depth and normal estimates by comparing the
consistency of the priors during optimization. We mitigate regularization in
regions where prior estimates have high uncertainty or ambiguities. Our
filtering strategy and optimization design demonstrate significant improvements
in both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian
Splatting-based methods on challenging indoor room datasets. Furthermore, we
explore the use of alternative meshing strategies for finer geometry
extraction. We develop a scale-aware meshing strategy inspired by TSDF and
octree-based isosurface extraction, which recovers finer details from Gaussian
models compared to other commonly used open-source meshing tools. Our code is
released in https://xuqianren.github.io/ags_mesh_website/.";Xuqian Ren<author:sep>Matias Turkulainen<author:sep>Jiepeng Wang<author:sep>Otto Seiskari<author:sep>Iaroslav Melekhov<author:sep>Juho Kannala<author:sep>Esa Rahtu;http://arxiv.org/pdf/2411.19271v1;cs.CV;;gaussian splatting
2411.19233v1;http://arxiv.org/abs/2411.19233v1;2024-11-28;Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes;"State-of-the-art novel view synthesis methods achieve impressive results for
multi-view captures of static 3D scenes. However, the reconstructed scenes
still lack ""liveliness,"" a key component for creating engaging 3D experiences.
Recently, novel video diffusion models generate realistic videos with complex
motion and enable animations of 2D images, however they cannot naively be used
to animate 3D scenes as they lack multi-view consistency. To breathe life into
the static world, we propose Gaussians2Life, a method for animating parts of
high-quality 3D scenes in a Gaussian Splatting representation. Our key idea is
to leverage powerful video diffusion models as the generative component of our
model and to combine these with a robust technique to lift 2D videos into
meaningful 3D motion. We find that, in contrast to prior work, this enables
realistic animations of complex, pre-existing 3D scenes and further enables the
animation of a large variety of object classes, while related work is mostly
focused on prior-based character animation, or single 3D objects. Our model
enables the creation of consistent, immersive 3D experiences for arbitrary
scenes.";Thomas Wimmer<author:sep>Michael Oechsle<author:sep>Michael Niemeyer<author:sep>Federico Tombari;http://arxiv.org/pdf/2411.19233v1;cs.CV;Project website: https://wimmerth.github.io/gaussians2life.html;gaussian splatting
2411.19290v1;http://arxiv.org/abs/2411.19290v1;2024-11-28;SADG: Segment Any Dynamic Gaussian Without Object Trackers;"Understanding dynamic 3D scenes is fundamental for various applications,
including extended reality (XR) and autonomous driving. Effectively integrating
semantic information into 3D reconstruction enables holistic representation
that opens opportunities for immersive and interactive applications. We
introduce SADG, Segment Any Dynamic Gaussian Without Object Trackers, a novel
approach that combines dynamic Gaussian Splatting representation and semantic
information without reliance on object IDs. In contrast to existing works, we
do not rely on supervision based on object identities to enable consistent
segmentation of dynamic 3D objects. To this end, we propose to learn
semantically-aware features by leveraging masks generated from the Segment
Anything Model (SAM) and utilizing our novel contrastive learning objective
based on hard pixel mining. The learned Gaussian features can be effectively
clustered without further post-processing. This enables fast computation for
further object-level editing, such as object removal, composition, and style
transfer by manipulating the Gaussians in the scene. We further extend several
dynamic novel-view datasets with segmentation benchmarks to enable testing of
learned feature fields from unseen viewpoints. We evaluate SADG on proposed
benchmarks and demonstrate the superior performance of our approach in
segmenting objects within dynamic scenes along with its effectiveness for
further downstream editing tasks.";Yun-Jin Li<author:sep>Mariia Gladkova<author:sep>Yan Xia<author:sep>Daniel Cremers;http://arxiv.org/pdf/2411.19290v1;cs.CV;Project page https://yunjinli.github.io/project-sadg;gaussian splatting
2411.18866v1;http://arxiv.org/abs/2411.18866v1;2024-11-28;RIGI: Rectifying Image-to-3D Generation Inconsistency via  Uncertainty-aware Learning;"Given a single image of a target object, image-to-3D generation aims to
reconstruct its texture and geometric shape. Recent methods often utilize
intermediate media, such as multi-view images or videos, to bridge the gap
between input image and the 3D target, thereby guiding the generation of both
shape and texture. However, inconsistencies in the generated multi-view
snapshots frequently introduce noise and artifacts along object boundaries,
undermining the 3D reconstruction process. To address this challenge, we
leverage 3D Gaussian Splatting (3DGS) for 3D reconstruction, and explicitly
integrate uncertainty-aware learning into the reconstruction process. By
capturing the stochasticity between two Gaussian models, we estimate an
uncertainty map, which is subsequently used for uncertainty-aware
regularization to rectify the impact of inconsistencies. Specifically, we
optimize both Gaussian models simultaneously, calculating the uncertainty map
by evaluating the discrepancies between rendered images from identical
viewpoints. Based on the uncertainty map, we apply adaptive pixel-wise loss
weighting to regularize the models, reducing reconstruction intensity in
high-uncertainty regions. This approach dynamically detects and mitigates
conflicts in multi-view labels, leading to smoother results and effectively
reducing artifacts. Extensive experiments show the effectiveness of our method
in improving 3D generation quality by reducing inconsistencies and artifacts.";Jiacheng Wang<author:sep>Zhedong Zheng<author:sep>Wei Xu<author:sep>Ping Liu;http://arxiv.org/pdf/2411.18866v1;cs.CV;Project Page: https://rigi3d.github.io/;gaussian splatting
2411.18966v1;http://arxiv.org/abs/2411.18966v1;2024-11-28;SuperGaussians: Enhancing Gaussian Splatting Using Primitives with  Spatially Varying Colors;"Gaussian Splattings demonstrate impressive results in multi-view
reconstruction based on Gaussian explicit representations. However, the current
Gaussian primitives only have a single view-dependent color and an opacity to
represent the appearance and geometry of the scene, resulting in a non-compact
representation. In this paper, we introduce a new method called SuperGaussians
that utilizes spatially varying colors and opacity in a single Gaussian
primitive to improve its representation ability. We have implemented bilinear
interpolation, movable kernels, and even tiny neural networks as spatially
varying functions. Quantitative and qualitative experimental results
demonstrate that all three functions outperform the baseline, with the best
movable kernels achieving superior novel view synthesis performance on multiple
datasets, highlighting the strong potential of spatially varying functions.";Rui Xu<author:sep>Wenyue Chen<author:sep>Jiepeng Wang<author:sep>Yuan Liu<author:sep>Peng Wang<author:sep>Lin Gao<author:sep>Shiqing Xin<author:sep>Taku Komura<author:sep>Xin Li<author:sep>Wenping Wang;http://arxiv.org/pdf/2411.18966v1;cs.CV;;gaussian splatting
2411.19322v1;http://arxiv.org/abs/2411.19322v1;2024-11-28;SAMa: Material-aware 3D Selection and Segmentation;"Decomposing 3D assets into material parts is a common task for artists and
creators, yet remains a highly manual process. In this work, we introduce
Select Any Material (SAMa), a material selection approach for various 3D
representations. Building on the recently introduced SAM2 video selection
model, we extend its capabilities to the material domain. We leverage the
model's cross-view consistency to create a 3D-consistent intermediate
material-similarity representation in the form of a point cloud from a sparse
set of views. Nearest-neighbour lookups in this similarity cloud allow us to
efficiently reconstruct accurate continuous selection masks over objects'
surfaces that can be inspected from any view. Our method is
multiview-consistent by design, alleviating the need for contrastive learning
or feature-field pre-processing, and performs optimization-free selection in
seconds. Our approach works on arbitrary 3D representations and outperforms
several strong baselines in terms of selection accuracy and multiview
consistency. It enables several compelling applications, such as replacing the
diffuse-textured materials on a text-to-3D output, or selecting and editing
materials on NeRFs and 3D-Gaussians.";Michael Fischer<author:sep>Iliyan Georgiev<author:sep>Thibault Groueix<author:sep>Vladimir G. Kim<author:sep>Tobias Ritschel<author:sep>Valentin Deschaintre;http://arxiv.org/pdf/2411.19322v1;cs.CV;Project Page: https://mfischer-ucl.github.io/sama;nerf
2411.19235v1;http://arxiv.org/abs/2411.19235v1;2024-11-28;InstanceGaussian: Appearance-Semantic Joint Gaussian Representation for  3D Instance-Level Perception;"3D scene understanding has become an essential area of research with
applications in autonomous driving, robotics, and augmented reality. Recently,
3D Gaussian Splatting (3DGS) has emerged as a powerful approach, combining
explicit modeling with neural adaptability to provide efficient and detailed
scene representations. However, three major challenges remain in leveraging
3DGS for scene understanding: 1) an imbalance between appearance and semantics,
where dense Gaussian usage for fine-grained texture modeling does not align
with the minimal requirements for semantic attributes; 2) inconsistencies
between appearance and semantics, as purely appearance-based Gaussians often
misrepresent object boundaries; and 3) reliance on top-down instance
segmentation methods, which struggle with uneven category distributions,
leading to over- or under-segmentation. In this work, we propose
InstanceGaussian, a method that jointly learns appearance and semantic features
while adaptively aggregating instances. Our contributions include: i) a novel
Semantic-Scaffold-GS representation balancing appearance and semantics to
improve feature representations and boundary delineation; ii) a progressive
appearance-semantic joint training strategy to enhance stability and
segmentation accuracy; and iii) a bottom-up, category-agnostic instance
aggregation approach that addresses segmentation challenges through farthest
point sampling and connected component analysis. Our approach achieves
state-of-the-art performance in category-agnostic, open-vocabulary 3D
point-level segmentation, highlighting the effectiveness of the proposed
representation and training strategies. Project page:
https://lhj-git.github.io/InstanceGaussian/";Haijie Li<author:sep>Yanmin Wu<author:sep>Jiarui Meng<author:sep>Qiankun Gao<author:sep>Zhiyao Zhang<author:sep>Ronggang Wang<author:sep>Jian Zhang;http://arxiv.org/pdf/2411.19235v1;cs.CV;technical report, 13 pages;gaussian splatting
2411.18311v1;http://arxiv.org/abs/2411.18311v1;2024-11-27;Neural Surface Priors for Editable Gaussian Splatting;"In computer graphics, there is a need to recover easily modifiable
representations of 3D geometry and appearance from image data. We introduce a
novel method for this task using 3D Gaussian Splatting, which enables intuitive
scene editing through mesh adjustments. Starting with input images and camera
poses, we reconstruct the underlying geometry using a neural Signed Distance
Field and extract a high-quality mesh. Our model then estimates a set of
Gaussians, where each component is flat, and the opacity is conditioned on the
recovered neural surface. To facilitate editing, we produce a proxy
representation that encodes information about the Gaussians' shape and
position. Unlike other methods, our pipeline allows modifications applied to
the extracted mesh to be propagated to the proxy representation, from which we
recover the updated parameters of the Gaussians. This effectively transfers the
mesh edits back to the recovered appearance representation. By leveraging
mesh-guided transformations, our approach simplifies 3D scene editing and
offers improvements over existing methods in terms of usability and visual
fidelity of edits. The complete source code for this project can be accessed at
\url{https://github.com/WJakubowska/NeuralSurfacePriors}";Jakub Szymkowiak<author:sep>Weronika Jakubowska<author:sep>Dawid Malarz<author:sep>Weronika Smolak-Dyżewska<author:sep>Maciej Zięba<author:sep>Przemysław Musialski<author:sep>Wojtek Pałubicki<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2411.18311v1;cs.CV;9 pages, 7 figures;gaussian splatting
2411.18652v1;http://arxiv.org/abs/2411.18652v1;2024-11-27;Surf-NeRF: Surface Regularised Neural Radiance Fields;"Neural Radiance Fields (NeRFs) provide a high fidelity, continuous scene
representation that can realistically represent complex behaviour of light.
Despite recent works like Ref-NeRF improving geometry through physics-inspired
models, the ability for a NeRF to overcome shape-radiance ambiguity and
converge to a representation consistent with real geometry remains limited. We
demonstrate how curriculum learning of a surface light field model helps a NeRF
converge towards a more geometrically accurate scene representation. We
introduce four additional regularisation terms to impose geometric smoothness,
consistency of normals and a separation of Lambertian and specular appearance
at geometry in the scene, conforming to physical models. Our approach yields
improvements of 14.4% to normals on positionally encoded NeRFs and 9.2% on
grid-based models compared to current reflection-based NeRF variants. This
includes a separated view-dependent appearance, conditioning a NeRF to have a
geometric representation consistent with the captured scene. We demonstrate
compatibility of our method with existing NeRF variants, as a key step in
enabling radiance-based representations for geometry critical applications.";Jack Naylor<author:sep>Viorela Ila<author:sep>Donald G. Dansereau;http://arxiv.org/pdf/2411.18652v1;cs.CV;"20 pages, 17 figures, 9 tables, project page can be found at
  http://roboticimaging.org/Projects/SurfNeRF";nerf
2411.18473v1;http://arxiv.org/abs/2411.18473v1;2024-11-27;HEMGS: A Hybrid Entropy Model for 3D Gaussian Splatting Data Compression;"Fast progress in 3D Gaussian Splatting (3DGS) has made 3D Gaussians popular
for 3D modeling and image rendering, but this creates big challenges in data
storage and transmission. To obtain a highly compact 3DGS representation, we
propose a hybrid entropy model for Gaussian Splatting (HEMGS) data compression,
which comprises two primary components, a hyperprior network and an
autoregressive network. To effectively reduce structural redundancy across
attributes, we apply a progressive coding algorithm to generate hyperprior
features, in which we use previously compressed attributes and location as
prior information. In particular, to better extract the location features from
these compressed attributes, we adopt a domain-aware and instance-aware
architecture to respectively capture domain-aware structural relations without
additional storage costs and reveal scene-specific features through MLPs.
Additionally, to reduce redundancy within each attribute, we leverage
relationships between neighboring compressed elements within the attributes
through an autoregressive network. Given its unique structure, we propose an
adaptive context coding algorithm with flexible receptive fields to effectively
capture adjacent compressed elements. Overall, we integrate our HEMGS into an
end-to-end optimized 3DGS compression framework and the extensive experimental
results on four benchmarks indicate that our method achieves about 40\% average
reduction in size while maintaining the rendering quality over our baseline
method and achieving state-of-the-art compression results.";Lei Liu<author:sep>Zhenghao Chen<author:sep>Dong Xu;http://arxiv.org/pdf/2411.18473v1;cs.CV;;gaussian splatting
2411.18197v1;http://arxiv.org/abs/2411.18197v1;2024-11-27;Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready  3D Characters;"3D characters are essential to modern creative industries, but making them
animatable often demands extensive manual work in tasks like rigging and
skinning. Existing automatic rigging tools face several limitations, including
the necessity for manual annotations, rigid skeleton topologies, and limited
generalization across diverse shapes and poses. An alternative approach is to
generate animatable avatars pre-bound to a rigged template mesh. However, this
method often lacks flexibility and is typically limited to realistic human
shapes. To address these issues, we present Make-It-Animatable, a novel
data-driven method to make any 3D humanoid model ready for character animation
in less than one second, regardless of its shapes and poses. Our unified
framework generates high-quality blend weights, bones, and pose
transformations. By incorporating a particle-based shape autoencoder, our
approach supports various 3D representations, including meshes and 3D Gaussian
splats. Additionally, we employ a coarse-to-fine representation and a
structure-aware modeling strategy to ensure both accuracy and robustness, even
for characters with non-standard skeleton structures. We conducted extensive
experiments to validate our framework's effectiveness. Compared to existing
methods, our approach demonstrates significant improvements in both quality and
speed.";Zhiyang Guo<author:sep>Jinxu Xiang<author:sep>Kai Ma<author:sep>Wengang Zhou<author:sep>Houqiang Li<author:sep>Ran Zhang;http://arxiv.org/pdf/2411.18197v1;cs.GR;Project Page: https://jasongzy.github.io/Make-It-Animatable/;
2411.18066v1;http://arxiv.org/abs/2411.18066v1;2024-11-27;GLS: Geometry-aware 3D Language Gaussian Splatting;"Recently, 3D Gaussian Splatting (3DGS) has achieved significant performance
on indoor surface reconstruction and open-vocabulary segmentation. This paper
presents GLS, a unified framework of surface reconstruction and open-vocabulary
segmentation based on 3DGS. GLS extends two fields by exploring the correlation
between them. For indoor surface reconstruction, we introduce surface normal
prior as a geometric cue to guide the rendered normal, and use the normal error
to optimize the rendered depth. For open-vocabulary segmentation, we employ 2D
CLIP features to guide instance features and utilize DEVA masks to enhance
their view consistency. Extensive experiments demonstrate the effectiveness of
jointly optimizing surface reconstruction and open-vocabulary segmentation,
where GLS surpasses state-of-the-art approaches of each task on MuSHRoom,
ScanNet++, and LERF-OVS datasets. Code will be available at
https://github.com/JiaxiongQ/GLS.";Jiaxiong Qiu<author:sep>Liu Liu<author:sep>Zhizhong Su<author:sep>Tianwei Lin;http://arxiv.org/pdf/2411.18066v1;cs.CV;Technical Report;gaussian splatting
2411.18675v1;http://arxiv.org/abs/2411.18675v1;2024-11-27;GaussianSpeech: Audio-Driven Gaussian Avatars;"We introduce GaussianSpeech, a novel approach that synthesizes high-fidelity
animation sequences of photo-realistic, personalized 3D human head avatars from
spoken audio. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
speech signal with 3D Gaussian splatting to create realistic, temporally
coherent motion sequences. We propose a compact and efficient 3DGS-based avatar
representation that generates expression-dependent color and leverages wrinkle-
and perceptually-based losses to synthesize facial details, including wrinkles
that occur with different expressions. To enable sequence modeling of 3D
Gaussian splats with audio, we devise an audio-conditioned transformer model
capable of extracting lip and expression features directly from audio input.
Due to the absence of high-quality datasets of talking humans in correspondence
with audio, we captured a new large-scale multi-view dataset of audio-visual
sequences of talking humans with native English accents and diverse facial
geometry. GaussianSpeech consistently achieves state-of-the-art performance
with visually natural motion at real time rendering rates, while encompassing
diverse facial expressions and styles.";Shivangi Aneja<author:sep>Artem Sevastopolsky<author:sep>Tobias Kirschstein<author:sep>Justus Thies<author:sep>Angela Dai<author:sep>Matthias Nießner;http://arxiv.org/pdf/2411.18675v1;cs.CV;"Paper Video: https://youtu.be/2VqYoFlYcwQ Project Page:
  https://shivangi-aneja.github.io/projects/gaussianspeech";gaussian splatting
2411.18548v1;http://arxiv.org/abs/2411.18548v1;2024-11-27;PhyCAGE: Physically Plausible Compositional 3D Asset Generation from a  Single Image;"We present PhyCAGE, the first approach for physically plausible compositional
3D asset generation from a single image. Given an input image, we first
generate consistent multi-view images for components of the assets. These
images are then fitted with 3D Gaussian Splatting representations. To ensure
that the Gaussians representing objects are physically compatible with each
other, we introduce a Physical Simulation-Enhanced Score Distillation Sampling
(PSE-SDS) technique to further optimize the positions of the Gaussians. It is
achieved by setting the gradient of the SDS loss as the initial velocity of the
physical simulation, allowing the simulator to act as a physics-guided
optimizer that progressively corrects the Gaussians' positions to a physically
compatible state. Experimental results demonstrate that the proposed method can
generate physically plausible compositional 3D assets given a single image.";Han Yan<author:sep>Mingrui Zhang<author:sep>Yang Li<author:sep>Chao Ma<author:sep>Pan Ji;http://arxiv.org/pdf/2411.18548v1;cs.CV;Project page: https://wolfball.github.io/phycage/;gaussian splatting
2411.18667v1;http://arxiv.org/abs/2411.18667v1;2024-11-27;Point Cloud Unsupervised Pre-training via 3D Gaussian Splatting;"Pre-training on large-scale unlabeled datasets contribute to the model
achieving powerful performance on 3D vision tasks, especially when annotations
are limited. However, existing rendering-based self-supervised frameworks are
computationally demanding and memory-intensive during pre-training due to the
inherent nature of volume rendering. In this paper, we propose an efficient
framework named GS$^3$ to learn point cloud representation, which seamlessly
integrates fast 3D Gaussian Splatting into the rendering-based framework. The
core idea behind our framework is to pre-train the point cloud encoder by
comparing rendered RGB images with real RGB images, as only Gaussian points
enriched with learned rich geometric and appearance information can produce
high-quality renderings. Specifically, we back-project the input RGB-D images
into 3D space and use a point cloud encoder to extract point-wise features.
Then, we predict 3D Gaussian points of the scene from the learned point cloud
features and uses a tile-based rasterizer for image rendering. Finally, the
pre-trained point cloud encoder can be fine-tuned to adapt to various
downstream 3D tasks, including high-level perception tasks such as 3D
segmentation and detection, as well as low-level tasks such as 3D scene
reconstruction. Extensive experiments on downstream tasks demonstrate the
strong transferability of the pre-trained point cloud encoder and the
effectiveness of our self-supervised learning framework. In addition, our
GS$^3$ framework is highly efficient, achieving approximately 9$\times$
pre-training speedup and less than 0.25$\times$ memory cost compared to the
previous rendering-based framework Ponder.";Hao Liu<author:sep>Minglin Chen<author:sep>Yanni Ma<author:sep>Haihong Xiao<author:sep>Ying He;http://arxiv.org/pdf/2411.18667v1;cs.CV;14 pages, 4 figures, 15 tables;gaussian splatting
2411.18072v1;http://arxiv.org/abs/2411.18072v1;2024-11-27;SmileSplat: Generalizable Gaussian Splats for Unconstrained Sparse  Images;"Sparse Multi-view Images can be Learned to predict explicit radiance fields
via Generalizable Gaussian Splatting approaches, which can achieve wider
application prospects in real-life when ground-truth camera parameters are not
required as inputs. In this paper, a novel generalizable Gaussian Splatting
method, SmileSplat, is proposed to reconstruct pixel-aligned Gaussian surfels
for diverse scenarios only requiring unconstrained sparse multi-view images.
First, Gaussian surfels are predicted based on the multi-head Gaussian
regression decoder, which can are represented with less degree-of-freedom but
have better multi-view consistency. Furthermore, the normal vectors of Gaussian
surfel are enhanced based on high-quality of normal priors. Second, the
Gaussians and camera parameters (both extrinsic and intrinsic) are optimized to
obtain high-quality Gaussian radiance fields for novel view synthesis tasks
based on the proposed Bundle-Adjusting Gaussian Splatting module. Extensive
experiments on novel view rendering and depth map prediction tasks are
conducted on public datasets, demonstrating that the proposed method achieves
state-of-the-art performance in various 3D vision tasks. More information can
be found on our project page
(https://yanyan-li.github.io/project/gs/smilesplat)";Yanyan Li<author:sep>Yixin Fang<author:sep>Federico Tombari<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2411.18072v1;cs.CV;;gaussian splatting
2411.17982v1;http://arxiv.org/abs/2411.17982v1;2024-11-27;HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene  Reconstruction;"We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast
and accurate monocular scene reconstruction using only RGB input. Existing
Neural SLAM or 3DGS-based SLAM methods often trade off between rendering
quality and geometry accuracy, our research demonstrates that both can be
achieved simultaneously with RGB input alone. The key idea of our approach is
to enhance the ability for geometry estimation by combining easy-to-obtain
monocular priors with learning-based dense SLAM, and then using 3D Gaussian
splatting as our core map representation to efficiently model the scene. Upon
loop closure, our method ensures on-the-fly global consistency through
efficient pose graph bundle adjustment and instant map updates by explicitly
deforming the 3D Gaussian units based on anchored keyframe updates.
Furthermore, we introduce a grid-based scale alignment strategy to maintain
improved scale consistency in prior depths for finer depth details. Through
extensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate
significant improvements over existing Neural SLAM methods and even surpass
RGB-D-based methods in both reconstruction and rendering quality. The project
page and source code will be made available at https://hi-slam2.github.io/.";Wei Zhang<author:sep>Qing Cheng<author:sep>David Skuddis<author:sep>Niclas Zeller<author:sep>Daniel Cremers<author:sep>Norbert Haala;http://arxiv.org/pdf/2411.17982v1;cs.RO;Under review process;
2411.18625v1;http://arxiv.org/abs/2411.18625v1;2024-11-27;Textured Gaussians for Enhanced 3D Scene Appearance Modeling;"3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.";Brian Chao<author:sep>Hung-Yu Tseng<author:sep>Lorenzo Porzi<author:sep>Chen Gao<author:sep>Tuotuo Li<author:sep>Qinbo Li<author:sep>Ayush Saraf<author:sep>Jia-Bin Huang<author:sep>Johannes Kopf<author:sep>Gordon Wetzstein<author:sep>Changil Kim;http://arxiv.org/pdf/2411.18625v1;cs.CV;Project website: https://textured-gaussians.github.io/;gaussian splatting
2411.17660v2;http://arxiv.org/abs/2411.17660v2;2024-11-26;DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting;"Recent progress in scene synthesis makes standalone SLAM systems purely based
on optimizing hyperprimitives with a Rendering objective possible. However, the
tracking performance still lacks behind traditional and end-to-end SLAM
systems. An optimal trade-off between robustness, speed and accuracy has not
yet been reached, especially for monocular video. In this paper, we introduce a
SLAM system based on an end-to-end Tracker and extend it with a Renderer based
on recent 3D Gaussian Splatting techniques. Our framework \textbf{DroidSplat}
achieves both SotA tracking and rendering results on common SLAM benchmarks. We
implemented multiple building blocks of modern SLAM systems to run in parallel,
allowing for fast inference on common consumer GPU's. Recent progress in
monocular depth prediction and camera calibration allows our system to achieve
strong results even on in-the-wild data without known camera intrinsics. Code
will be available at \url{https://github.com/ChenHoy/DROID-Splat}.";Christian Homeyer<author:sep>Leon Begiristain<author:sep>Christoph Schnörr;http://arxiv.org/pdf/2411.17660v2;cs.CV;;gaussian splatting
2411.17235v1;http://arxiv.org/abs/2411.17235v1;2024-11-26;MLI-NeRF: Multi-Light Intrinsic-Aware Neural Radiance Fields;"Current methods for extracting intrinsic image components, such as
reflectance and shading, primarily rely on statistical priors. These methods
focus mainly on simple synthetic scenes and isolated objects and struggle to
perform well on challenging real-world data. To address this issue, we propose
MLI-NeRF, which integrates \textbf{M}ultiple \textbf{L}ight information in
\textbf{I}ntrinsic-aware \textbf{Ne}ural \textbf{R}adiance \textbf{F}ields. By
leveraging scene information provided by different light source positions
complementing the multi-view information, we generate pseudo-label images for
reflectance and shading to guide intrinsic image decomposition without the need
for ground truth data. Our method introduces straightforward supervision for
intrinsic component separation and ensures robustness across diverse scene
types. We validate our approach on both synthetic and real-world datasets,
outperforming existing state-of-the-art methods. Additionally, we demonstrate
its applicability to various image editing tasks. The code and data are
publicly available.";Yixiong Yang<author:sep>Shilin Hu<author:sep>Haoyu Wu<author:sep>Ramon Baldrich<author:sep>Dimitris Samaras<author:sep>Maria Vanrell;http://arxiv.org/pdf/2411.17235v1;cs.CV;"Accepted paper for the International Conference on 3D Vision 2025.
  Project page: https://github.com/liulisixin/MLI-NeRF";nerf
2411.17605v1;http://arxiv.org/abs/2411.17605v1;2024-11-26;Distractor-free Generalizable 3D Gaussian Splatting;"We present DGGS, a novel framework addressing the previously unexplored
challenge of Distractor-free Generalizable 3D Gaussian Splatting (3DGS). It
accomplishes two key objectives: fortifying generalizable 3DGS against
distractor-laden data during both training and inference phases, while
successfully extending cross-scene adaptation capabilities to conventional
distractor-free approaches. To achieve these objectives, DGGS introduces a
scene-agnostic reference-based mask prediction and refinement methodology
during training phase, coupled with a training view selection strategy,
effectively improving distractor prediction accuracy and training stability.
Moreover, to address distractor-induced voids and artifacts during inference
stage, we propose a two-stage inference framework for better reference
selection based on the predicted distractor masks, complemented by a distractor
pruning module to eliminate residual distractor effects. Extensive
generalization experiments demonstrate DGGS's advantages under distractor-laden
conditions. Additionally, experimental results show that our scene-agnostic
mask inference achieves accuracy comparable to scene-specific trained methods.
Homepage is \url{https://github.com/bbbbby-99/DGGS}.";Yanqi Bao<author:sep>Jing Liao<author:sep>Jing Huo<author:sep>Yang Gao;http://arxiv.org/pdf/2411.17605v1;cs.CV;;gaussian splatting
2411.17190v2;http://arxiv.org/abs/2411.17190v2;2024-11-26;SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian  Splatting;"We propose SelfSplat, a novel 3D Gaussian Splatting model designed to perform
pose-free and 3D prior-free generalizable 3D reconstruction from unposed
multi-view images. These settings are inherently ill-posed due to the lack of
ground-truth data, learned geometric information, and the need to achieve
accurate 3D reconstruction without finetuning, making it difficult for
conventional methods to achieve high-quality results. Our model addresses these
challenges by effectively integrating explicit 3D representations with
self-supervised depth and pose estimation techniques, resulting in reciprocal
improvements in both pose accuracy and 3D reconstruction quality. Furthermore,
we incorporate a matching-aware pose estimation network and a depth refinement
module to enhance geometry consistency across views, ensuring more accurate and
stable 3D reconstructions. To present the performance of our method, we
evaluated it on large-scale real-world datasets, including RealEstate10K, ACID,
and DL3DV. SelfSplat achieves superior results over previous state-of-the-art
methods in both appearance and geometry quality, also demonstrates strong
cross-dataset generalization capabilities. Extensive ablation studies and
analysis also validate the effectiveness of our proposed methods. Code and
pretrained models are available at https://gynjn.github.io/selfsplat/";Gyeongjin Kang<author:sep>Jisang Yoo<author:sep>Jihyeon Park<author:sep>Seungtae Nam<author:sep>Hyeonsoo Im<author:sep>Sangheon Shin<author:sep>Sangpil Kim<author:sep>Eunbyung Park;http://arxiv.org/pdf/2411.17190v2;cs.CV;Project page: https://gynjn.github.io/selfsplat/;gaussian splatting
2411.17044v1;http://arxiv.org/abs/2411.17044v1;2024-11-26;4D Scaffold Gaussian Splatting for Memory Efficient Dynamic Scene  Reconstruction;"Existing 4D Gaussian methods for dynamic scene reconstruction offer high
visual fidelity and fast rendering. However, these methods suffer from
excessive memory and storage demands, which limits their practical deployment.
This paper proposes a 4D anchor-based framework that retains visual quality and
rendering speed of 4D Gaussians while significantly reducing storage costs. Our
method extends 3D scaffolding to 4D space, and leverages sparse 4D grid-aligned
anchors with compressed feature vectors. Each anchor models a set of neural 4D
Gaussians, each of which represent a local spatiotemporal region. In addition,
we introduce a temporal coverage-aware anchor growing strategy to effectively
assign additional anchors to under-reconstructed dynamic regions. Our method
adjusts the accumulated gradients based on Gaussians' temporal coverage,
improving reconstruction quality in dynamic regions. To reduce the number of
anchors, we further present enhanced formulations of neural 4D Gaussians. These
include the neural velocity, and the temporal opacity derived from a
generalized Gaussian distribution. Experimental results demonstrate that our
method achieves state-of-the-art visual quality and 97.8% storage reduction
over 4DGS.";Woong Oh Cho<author:sep>In Cho<author:sep>Seoha Kim<author:sep>Jeongmin Bae<author:sep>Youngjung Uh<author:sep>Seon Joo Kim;http://arxiv.org/pdf/2411.17044v1;cs.CV;;gaussian splatting
2411.16180v1;http://arxiv.org/abs/2411.16180v1;2024-11-25;Event-boosted Deformable 3D Gaussians for Fast Dynamic Scene  Reconstruction;"3D Gaussian Splatting (3D-GS) enables real-time rendering but struggles with
fast motion due to low temporal resolution of RGB cameras. To address this, we
introduce the first approach combining event cameras, which capture
high-temporal-resolution, continuous motion data, with deformable 3D-GS for
fast dynamic scene reconstruction. We observe that threshold modeling for
events plays a crucial role in achieving high-quality reconstruction.
Therefore, we propose a GS-Threshold Joint Modeling (GTJM) strategy, creating a
mutually reinforcing process that greatly improves both 3D reconstruction and
threshold modeling. Moreover, we introduce a Dynamic-Static Decomposition (DSD)
strategy that first identifies dynamic areas by exploiting the inability of
static Gaussians to represent motions, then applies a buffer-based soft
decomposition to separate dynamic and static areas. This strategy accelerates
rendering by avoiding unnecessary deformation in static areas, and focuses on
dynamic areas to enhance fidelity. Our approach achieves high-fidelity dynamic
reconstruction at 156 FPS with a 400$\times$400 resolution on an RTX 3090 GPU.";Wenhao Xu<author:sep>Wenming Weng<author:sep>Yueyi Zhang<author:sep>Ruikang Xu<author:sep>Zhiwei Xiong;http://arxiv.org/pdf/2411.16180v1;cs.CV;;gaussian splatting
2411.16779v1;http://arxiv.org/abs/2411.16779v1;2024-11-25;NovelGS: Consistent Novel-view Denoising via Large Gaussian  Reconstruction Model;"We introduce NovelGS, a diffusion model for Gaussian Splatting (GS) given
sparse-view images. Recent works leverage feed-forward networks to generate
pixel-aligned Gaussians, which could be fast rendered. Unfortunately, the
method was unable to produce satisfactory results for areas not covered by the
input images due to the formulation of these methods. In contrast, we leverage
the novel view denoising through a transformer-based network to generate 3D
Gaussians. Specifically, by incorporating both conditional views and noisy
target views, the network predicts pixel-aligned Gaussians for each view.
During training, the rendered target and some additional views of the Gaussians
are supervised. During inference, the target views are iteratively rendered and
denoised from pure noise. Our approach demonstrates state-of-the-art
performance in addressing the multi-view image reconstruction challenge. Due to
generative modeling of unseen regions, NovelGS effectively reconstructs 3D
objects with consistent and sharp textures. Experimental results on publicly
available datasets indicate that NovelGS substantially surpasses existing
image-to-3D frameworks, both qualitatively and quantitatively. We also
demonstrate the potential of NovelGS in generative tasks, such as text-to-3D
and image-to-3D, by integrating it with existing multiview diffusion models. We
will make the code publicly accessible.";Jinpeng Liu<author:sep>Jiale Xu<author:sep>Weihao Cheng<author:sep>Yiming Gao<author:sep>Xintao Wang<author:sep>Ying Shan<author:sep>Yansong Tang;http://arxiv.org/pdf/2411.16779v1;cs.CV;;gaussian splatting
2411.16392v1;http://arxiv.org/abs/2411.16392v1;2024-11-25;Quadratic Gaussian Splatting for Efficient and Detailed Surface  Reconstruction;"Recently, 3D Gaussian Splatting (3DGS) has attracted attention for its
superior rendering quality and speed over Neural Radiance Fields (NeRF). To
address 3DGS's limitations in surface representation, 2D Gaussian Splatting
(2DGS) introduced disks as scene primitives to model and reconstruct geometries
from multi-view images, offering view-consistent geometry. However, the disk's
first-order linear approximation often leads to over-smoothed results. We
propose Quadratic Gaussian Splatting (QGS), a novel method that replaces disks
with quadric surfaces, enhancing geometric fitting, whose code will be
open-sourced. QGS defines Gaussian distributions in non-Euclidean space,
allowing primitives to capture more complex textures. As a second-order surface
approximation, QGS also renders spatial curvature to guide the normal
consistency term, to effectively reduce over-smoothing. Moreover, QGS is a
generalized version of 2DGS that achieves more accurate and detailed
reconstructions, as verified by experiments on DTU and TNT, demonstrating its
effectiveness in surpassing current state-of-the-art methods in geometry
reconstruction. Our code willbe released as open source.";Ziyu Zhang<author:sep>Binbin Huang<author:sep>Hanqing Jiang<author:sep>Liyang Zhou<author:sep>Xiaojun Xiang<author:sep>Shunhan Shen;http://arxiv.org/pdf/2411.16392v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.16172v1;http://arxiv.org/abs/2411.16172v1;2024-11-25;U2NeRF: Unsupervised Underwater Image Restoration and Neural Radiance  Fields;"Underwater images suffer from colour shifts, low contrast, and haziness due
to light absorption, refraction, scattering and restoring these images has
warranted much attention. In this work, we present Unsupervised Underwater
Neural Radiance Field U2NeRF, a transformer-based architecture that learns to
render and restore novel views conditioned on multi-view geometry
simultaneously. Due to the absence of supervision, we attempt to implicitly
bake restoring capabilities onto the NeRF pipeline and disentangle the
predicted color into several components - scene radiance, direct transmission
map, backscatter transmission map, and global background light, and when
combined reconstruct the underwater image in a self-supervised manner. In
addition, we release an Underwater View Synthesis UVS dataset consisting of 12
underwater scenes, containing both synthetically-generated and real-world data.
Our experiments demonstrate that when optimized on a single scene, U2NeRF
outperforms several baselines by as much LPIPS 11%, UIQM 5%, UCIQE 4% (on
average) and showcases improved rendering and restoration capabilities. Code
will be made available upon acceptance.";Vinayak Gupta<author:sep>Manoj S<author:sep>Mukund Varma T<author:sep>Kaushik Mitra;http://arxiv.org/pdf/2411.16172v1;cs.CV;"ICLR Tiny Papers 2024. arXiv admin note: text overlap with
  arXiv:2207.13298";nerf
2411.16898v1;http://arxiv.org/abs/2411.16898v1;2024-11-25;G2SDF: Surface Reconstruction from Explicit Gaussians with Implicit SDFs;"State-of-the-art novel view synthesis methods such as 3D Gaussian Splatting
(3DGS) achieve remarkable visual quality. While 3DGS and its variants can be
rendered efficiently using rasterization, many tasks require access to the
underlying 3D surface, which remains challenging to extract due to the sparse
and explicit nature of this representation. In this paper, we introduce G2SDF,
a novel approach that addresses this limitation by integrating a neural
implicit Signed Distance Field (SDF) into the Gaussian Splatting framework. Our
method links the opacity values of Gaussians with their distances to the
surface, ensuring a closer alignment of Gaussians with the scene surface. To
extend this approach to unbounded scenes at varying scales, we propose a
normalization function that maps any range to a fixed interval. To further
enhance reconstruction quality, we leverage an off-the-shelf depth estimator as
pseudo ground truth during Gaussian Splatting optimization. By establishing a
differentiable connection between the explicit Gaussians and the implicit SDF,
our approach enables high-quality surface reconstruction and rendering.
Experimental results on several real-world datasets demonstrate that G2SDF
achieves superior reconstruction quality than prior works while maintaining the
efficiency of 3DGS.";Kunyi Li<author:sep>Michael Niemeyer<author:sep>Zeyu Chen<author:sep>Nassir Navab<author:sep>Federico Tombari;http://arxiv.org/pdf/2411.16898v1;cs.CV;;gaussian splatting
2411.16940v1;http://arxiv.org/abs/2411.16940v1;2024-11-25;The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic  Robotic Simulation;"As robots increasingly coexist with humans, they must navigate complex,
dynamic environments rich in visual information and implicit social dynamics,
like when to yield or move through crowds. Addressing these challenges requires
significant advances in vision-based sensing and a deeper understanding of
socio-dynamic factors, particularly in tasks like navigation. To facilitate
this, robotics researchers need advanced simulation platforms offering dynamic,
photorealistic environments with realistic actors. Unfortunately, most existing
simulators fall short, prioritizing geometric accuracy over visual fidelity,
and employing unrealistic agents with fixed trajectories and low-quality
visuals. To overcome these limitations, we developed a simulator that
incorporates three essential elements: (1) photorealistic neural rendering of
environments, (2) neurally animated human entities with behavior management,
and (3) an ego-centric robotic agent providing multi-sensor output. By
utilizing advanced neural rendering techniques in a dual-NeRF simulator, our
system produces high-fidelity, photorealistic renderings of both environments
and human entities. Additionally, it integrates a state-of-the-art Social Force
Model to model dynamic human-human and human-robot interactions, creating the
first photorealistic and accessible human-robot simulation system powered by
neural rendering.";Georgina Nuthall<author:sep>Richard Bowden<author:sep>Oscar Mendez;http://arxiv.org/pdf/2411.16940v1;cs.RO;8 pages, 5 figures;nerf
2411.16443v1;http://arxiv.org/abs/2411.16443v1;2024-11-25;SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting  Synthesis;"Text-based generation and editing of 3D scenes hold significant potential for
streamlining content creation through intuitive user interactions. While recent
advances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time
rendering, existing methods are often specialized and task-focused, lacking a
unified framework for both generation and editing. In this paper, we introduce
SplatFlow, a comprehensive framework that addresses this gap by enabling direct
3DGS generation and editing. SplatFlow comprises two main components: a
multi-view rectified flow (RF) model and a Gaussian Splatting Decoder
(GSDecoder). The multi-view RF model operates in latent space, generating
multi-view images, depths, and camera poses simultaneously, conditioned on text
prompts, thus addressing challenges like diverse scene scales and complex
camera trajectories in real-world settings. Then, the GSDecoder efficiently
translates these latent outputs into 3DGS representations through a
feed-forward 3DGS method. Leveraging training-free inversion and inpainting
techniques, SplatFlow enables seamless 3DGS editing and supports a broad range
of 3D tasks-including object editing, novel view synthesis, and camera pose
estimation-within a unified framework without requiring additional complex
pipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K
datasets, demonstrating its versatility and effectiveness in various 3D
generation, editing, and inpainting-based tasks.";Hyojun Go<author:sep>Byeongjun Park<author:sep>Jiho Jang<author:sep>Jin-Young Kim<author:sep>Soonwoo Kwon<author:sep>Changick Kim;http://arxiv.org/pdf/2411.16443v1;cs.CV;Project Page: https://gohyojun15.github.io/SplatFlow/;gaussian splatting
2411.16816v1;http://arxiv.org/abs/2411.16816v1;2024-11-25;SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting  for Autonomous Driving;"Ensuring the safety of autonomous robots, such as self-driving vehicles,
requires extensive testing across diverse driving scenarios. Simulation is a
key ingredient for conducting such testing in a cost-effective and scalable
way. Neural rendering methods have gained popularity, as they can build
simulation environments from collected logs in a data-driven manner. However,
existing neural radiance field (NeRF) methods for sensor-realistic rendering of
camera and lidar data suffer from low rendering speeds, limiting their
applicability for large-scale testing. While 3D Gaussian Splatting (3DGS)
enables real-time rendering, current methods are limited to camera data and are
unable to render lidar data essential for autonomous driving. To address these
limitations, we propose SplatAD, the first 3DGS-based method for realistic,
real-time rendering of dynamic scenes for both camera and lidar data. SplatAD
accurately models key sensor-specific phenomena such as rolling shutter
effects, lidar intensity, and lidar ray dropouts, using purpose-built
algorithms to optimize rendering efficiency. Evaluation across three autonomous
driving datasets demonstrates that SplatAD achieves state-of-the-art rendering
quality with up to +2 PSNR for NVS and +3 PSNR for reconstruction while
increasing rendering speed over NeRF-based methods by an order of magnitude.
See https://research.zenseact.com/publications/splatad/ for our project page.";Georg Hess<author:sep>Carl Lindström<author:sep>Maryam Fatemi<author:sep>Christoffer Petersson<author:sep>Lennart Svensson;http://arxiv.org/pdf/2411.16816v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.16053v1;http://arxiv.org/abs/2411.16053v1;2024-11-25;UnitedVLN: Generalizable Gaussian Splatting for Continuous  Vision-Language Navigation;"Vision-and-Language Navigation (VLN), where an agent follows instructions to
reach a target destination, has recently seen significant advancements. In
contrast to navigation in discrete environments with predefined trajectories,
VLN in Continuous Environments (VLN-CE) presents greater challenges, as the
agent is free to navigate any unobstructed location and is more vulnerable to
visual occlusions or blind spots. Recent approaches have attempted to address
this by imagining future environments, either through predicted future visual
images or semantic features, rather than relying solely on current
observations. However, these RGB-based and feature-based methods lack intuitive
appearance-level information or high-level semantic complexity crucial for
effective navigation. To overcome these limitations, we introduce a novel,
generalizable 3DGS-based pre-training paradigm, called UnitedVLN, which enables
agents to better explore future environments by unitedly rendering
high-fidelity 360 visual images and semantic features. UnitedVLN employs two
key schemes: search-then-query sampling and separate-then-united rendering,
which facilitate efficient exploitation of neural primitives, helping to
integrate both appearance and semantic information for more robust navigation.
Extensive experiments demonstrate that UnitedVLN outperforms state-of-the-art
methods on existing VLN-CE benchmarks.";Guangzhao Dai<author:sep>Jian Zhao<author:sep>Yuantao Chen<author:sep>Yusen Qin<author:sep>Hao Zhao<author:sep>Guosen Xie<author:sep>Yazhou Yao<author:sep>Xiangbo Shu<author:sep>Xuelong Li;http://arxiv.org/pdf/2411.16053v1;cs.CV;;gaussian splatting
2411.16877v1;http://arxiv.org/abs/2411.16877v1;2024-11-25;PreF3R: Pose-Free Feed-Forward 3D Gaussian Splatting from  Variable-length Image Sequence;"We present PreF3R, Pose-Free Feed-forward 3D Reconstruction from an image
sequence of variable length. Unlike previous approaches, PreF3R removes the
need for camera calibration and reconstructs the 3D Gaussian field within a
canonical coordinate frame directly from a sequence of unposed images, enabling
efficient novel-view rendering. We leverage DUSt3R's ability for pair-wise 3D
structure reconstruction, and extend it to sequential multi-view input via a
spatial memory network, eliminating the need for optimization-based global
alignment. Additionally, PreF3R incorporates a dense Gaussian parameter
prediction head, which enables subsequent novel-view synthesis with
differentiable rasterization. This allows supervising our model with the
combination of photometric loss and pointmap regression loss, enhancing both
photorealism and structural accuracy. Given a sequence of ordered images,
PreF3R incrementally reconstructs the 3D Gaussian field at 20 FPS, therefore
enabling real-time novel-view rendering. Empirical experiments demonstrate that
PreF3R is an effective solution for the challenging task of pose-free
feed-forward novel-view synthesis, while also exhibiting robust generalization
to unseen scenes.";Zequn Chen<author:sep>Jiezhi Yang<author:sep>Heng Yang;http://arxiv.org/pdf/2411.16877v1;cs.CV;project page: https://computationalrobotics.seas.harvard.edu/PreF3R/;gaussian splatting
2411.15732v1;http://arxiv.org/abs/2411.15732v1;2024-11-24;DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and  Precise Editing with Diffusion Models;"Generating and editing dynamic 3D head avatars are crucial tasks in virtual
reality and film production. However, existing methods often suffer from facial
distortions, inaccurate head movements, and limited fine-grained editing
capabilities. To address these challenges, we present DynamicAvatars, a dynamic
model that generates photorealistic, moving 3D head avatars from video clips
and parameters associated with facial positions and expressions. Our approach
enables precise editing through a novel prompt-based editing model, which
integrates user-provided prompts with guiding parameters derived from large
language models (LLMs). To achieve this, we propose a dual-tracking framework
based on Gaussian Splatting and introduce a prompt preprocessing module to
enhance editing stability. By incorporating a specialized GAN algorithm and
connecting it to our control module, which generates precise guiding parameters
from LLMs, we successfully address the limitations of existing methods.
Additionally, we develop a dynamic editing strategy that selectively utilizes
specific training datasets to improve the efficiency and adaptability of the
model for dynamic editing tasks.";Yangyang Qian<author:sep>Yuan Sun<author:sep>Yu Guo;http://arxiv.org/pdf/2411.15732v1;cs.GR;;gaussian splatting
2411.16758v1;http://arxiv.org/abs/2411.16758v1;2024-11-24;Bundle Adjusted Gaussian Avatars Deblurring;"The development of 3D human avatars from multi-view videos represents a
significant yet challenging task in the field. Recent advancements, including
3D Gaussian Splattings (3DGS), have markedly progressed this domain.
Nonetheless, existing techniques necessitate the use of high-quality sharp
images, which are often impractical to obtain in real-world settings due to
variations in human motion speed and intensity. In this study, we attempt to
explore deriving sharp intrinsic 3D human Gaussian avatars from blurry video
footage in an end-to-end manner. Our approach encompasses a 3D-aware,
physics-oriented model of blur formation attributable to human movement,
coupled with a 3D human motion model to clarify ambiguities found in
motion-induced blurry images. This methodology facilitates the concurrent
learning of avatar model parameters and the refinement of sub-frame motion
parameters from a coarse initialization. We have established benchmarks for
this task through a synthetic dataset derived from existing multi-view
captures, alongside a real-captured dataset acquired through a 360-degree
synchronous hybrid-exposure camera system. Comprehensive evaluations
demonstrate that our model surpasses existing baselines.";Muyao Niu<author:sep>Yifan Zhan<author:sep>Qingtian Zhu<author:sep>Zhuoxiao Li<author:sep>Wei Wang<author:sep>Zhihang Zhong<author:sep>Xiao Sun<author:sep>Yinqiang Zheng;http://arxiv.org/pdf/2411.16758v1;cs.CV;Codes and Data: https://github.com/MyNiuuu/BAGA;gaussian splatting
2411.15779v1;http://arxiv.org/abs/2411.15779v1;2024-11-24;ZeroGS: Training 3D Gaussian Splatting from Unposed Images;"Neural radiance fields (NeRF) and 3D Gaussian Splatting (3DGS) are popular
techniques to reconstruct and render photo-realistic images. However, the
pre-requisite of running Structure-from-Motion (SfM) to get camera poses limits
their completeness. While previous methods can reconstruct from a few unposed
images, they are not applicable when images are unordered or densely captured.
In this work, we propose ZeroGS to train 3DGS from hundreds of unposed and
unordered images. Our method leverages a pretrained foundation model as the
neural scene representation. Since the accuracy of the predicted pointmaps does
not suffice for accurate image registration and high-fidelity image rendering,
we propose to mitigate the issue by initializing and finetuning the pretrained
model from a seed image. Images are then progressively registered and added to
the training buffer, which is further used to train the model. We also propose
to refine the camera poses and pointmaps by minimizing a point-to-camera ray
consistency loss across multiple views. Experiments on the LLFF dataset, the
MipNeRF360 dataset, and the Tanks-and-Temples dataset show that our method
recovers more accurate camera poses than state-of-the-art pose-free NeRF/3DGS
methods, and even renders higher quality images than 3DGS with COLMAP poses.
Our project page is available at https://aibluefisher.github.io/ZeroGS.";Yu Chen<author:sep>Rolandos Alexandros Potamias<author:sep>Evangelos Ververas<author:sep>Jifei Song<author:sep>Jiankang Deng<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2411.15779v1;cs.CV;16 pages, 12 figures;gaussian splatting<tag:sep>nerf
2411.15800v1;http://arxiv.org/abs/2411.15800v1;2024-11-24;PG-SLAM: Photo-realistic and Geometry-aware RGB-D SLAM in Dynamic  Environments;"Simultaneous localization and mapping (SLAM) has achieved impressive
performance in static environments. However, SLAM in dynamic environments
remains an open question. Many methods directly filter out dynamic objects,
resulting in incomplete scene reconstruction and limited accuracy of camera
localization. The other works express dynamic objects by point clouds, sparse
joints, or coarse meshes, which fails to provide a photo-realistic
representation. To overcome the above limitations, we propose a photo-realistic
and geometry-aware RGB-D SLAM method by extending Gaussian splatting. Our
method is composed of three main modules to 1) map the dynamic foreground
including non-rigid humans and rigid items, 2) reconstruct the static
background, and 3) localize the camera. To map the foreground, we focus on
modeling the deformations and/or motions. We consider the shape priors of
humans and exploit geometric and appearance constraints of humans and items.
For background mapping, we design an optimization strategy between neighboring
local maps by integrating appearance constraint into geometric alignment. As to
camera localization, we leverage both static background and dynamic foreground
to increase the observations for noise compensation. We explore the geometric
and appearance constraints by associating 3D Gaussians with 2D optical flows
and pixel patches. Experiments on various real-world datasets demonstrate that
our method outperforms state-of-the-art approaches in terms of camera
localization and scene representation. Source codes will be publicly available
upon paper acceptance.";Haoang Li<author:sep>Xiangqi Meng<author:sep>Xingxing Zuo<author:sep>Zhe Liu<author:sep>Hesheng Wang<author:sep>Daniel Cremers;http://arxiv.org/pdf/2411.15800v1;cs.RO;;gaussian splatting
2411.15723v1;http://arxiv.org/abs/2411.15723v1;2024-11-24;GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian  Supervision;"Surface reconstruction from multi-view images is a core challenge in 3D
vision. Recent studies have explored signed distance fields (SDF) within Neural
Radiance Fields (NeRF) to achieve high-fidelity surface reconstructions.
However, these approaches often suffer from slow training and rendering speeds
compared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques
attempt to fuse depth information to extract geometry from 3DGS, but frequently
result in incomplete reconstructions and fragmented surfaces. In this paper, we
introduce GSurf, a novel end-to-end method for learning a signed distance field
directly from Gaussian primitives. The continuous and smooth nature of SDF
addresses common issues in the 3DGS family, such as holes resulting from noisy
or missing depth data. By using Gaussian splatting for rendering, GSurf avoids
the redundant volume rendering typically required in other GS and SDF
integrations. Consequently, GSurf achieves faster training and rendering speeds
while delivering 3D reconstruction quality comparable to neural implicit
surface methods, such as VolSDF and NeuS. Experimental results across various
benchmark datasets demonstrate the effectiveness of our method in producing
high-fidelity 3D reconstructions.";Xu Baixin<author:sep>Hu Jiangbei<author:sep>Li Jiaze<author:sep>He Ying;http://arxiv.org/pdf/2411.15723v1;cs.CV;see https://github.com/xubaixinxbx/Gsurf;gaussian splatting<tag:sep>nerf
2411.15551v1;http://arxiv.org/abs/2411.15551v1;2024-11-23;NeRF Inpainting with Geometric Diffusion Prior and Balanced Score  Distillation;"Recent advances in NeRF inpainting have leveraged pretrained diffusion models
to enhance performance. However, these methods often yield suboptimal results
due to their ineffective utilization of 2D diffusion priors. The limitations
manifest in two critical aspects: the inadequate capture of geometric
information by pretrained diffusion models and the suboptimal guidance provided
by existing Score Distillation Sampling (SDS) methods. To address these
problems, we introduce GB-NeRF, a novel framework that enhances NeRF inpainting
through improved utilization of 2D diffusion priors. Our approach incorporates
two key innovations: a fine-tuning strategy that simultaneously learns
appearance and geometric priors and a specialized normal distillation loss that
integrates these geometric priors into NeRF inpainting. We propose a technique
called Balanced Score Distillation (BSD) that surpasses existing methods such
as Score Distillation (SDS) and the improved version, Conditional Score
Distillation (CSD). BSD offers improved inpainting quality in appearance and
geometric aspects. Extensive experiments show that our method provides superior
appearance fidelity and geometric consistency compared to existing approaches.";Menglin Zhang<author:sep>Xin Luo<author:sep>Yunwei Lan<author:sep>Chang Liu<author:sep>Rui Li<author:sep>Kaidong Zhang<author:sep>Ganlin Yang<author:sep>Dong Liu;http://arxiv.org/pdf/2411.15551v1;cs.CV;;nerf
2411.15468v1;http://arxiv.org/abs/2411.15468v1;2024-11-23;SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion;"A signed distance function (SDF) is a useful representation for
continuous-space geometry and many related operations, including rendering,
collision checking, and mesh generation. Hence, reconstructing SDF from image
observations accurately and efficiently is a fundamental problem. Recently,
neural implicit SDF (SDF-NeRF) techniques, trained using volumetric rendering,
have gained a lot of attention. Compared to earlier truncated SDF (TSDF) fusion
algorithms that rely on depth maps and voxelize continuous space, SDF-NeRF
enables continuous-space SDF reconstruction with better geometric and
photometric accuracy. However, the accuracy and convergence speed of
scene-level SDF reconstruction require further improvements for many
applications. With the advent of 3D Gaussian Splatting (3DGS) as an explicit
representation with excellent rendering quality and speed, several works have
focused on improving SDF-NeRF by introducing consistency losses on depth and
surface normals between 3DGS and SDF-NeRF. However, loss-level connections
alone lead to incremental improvements. We propose a novel neural implicit SDF
called ""SplatSDF"" to fuse 3DGSandSDF-NeRF at an architecture level with
significant boosts to geometric and photometric accuracy and convergence speed.
Our SplatSDF relies on 3DGS as input only during training, and keeps the same
complexity and efficiency as the original SDF-NeRF during inference. Our method
outperforms state-of-the-art SDF-NeRF models on geometric and photometric
evaluation by the time of submission.";Runfa Blark Li<author:sep>Keito Suzuki<author:sep>Bang Du<author:sep>Ki Myung Brian Le<author:sep>Nikolay Atanasov<author:sep>Truong Nguyen;http://arxiv.org/pdf/2411.15468v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.15482v1;http://arxiv.org/abs/2411.15482v1;2024-11-23;SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion  Flow Field for Autonomous Driving;"Most existing Dynamic Gaussian Splatting methods for complex dynamic urban
scenarios rely on accurate object-level supervision from expensive manual
labeling, limiting their scalability in real-world applications. In this paper,
we introduce SplatFlow, a Self-Supervised Dynamic Gaussian Splatting within
Neural Motion Flow Fields (NMFF) to learn 4D space-time representations without
requiring tracked 3D bounding boxes, enabling accurate dynamic scene
reconstruction and novel view RGB, depth and flow synthesis. SplatFlow designs
a unified framework to seamlessly integrate time-dependent 4D Gaussian
representation within NMFF, where NMFF is a set of implicit functions to model
temporal motions of both LiDAR points and Gaussians as continuous motion flow
fields. Leveraging NMFF, SplatFlow effectively decomposes static background and
dynamic objects, representing them with 3D and 4D Gaussian primitives,
respectively. NMFF also models the status correspondences of each 4D Gaussian
across time, which aggregates temporal features to enhance cross-view
consistency of dynamic components. SplatFlow further improves dynamic scene
identification by distilling features from 2D foundational models into 4D
space-time representation. Comprehensive evaluations conducted on the Waymo
Open Dataset and KITTI Dataset validate SplatFlow's state-of-the-art (SOTA)
performance for both image reconstruction and novel view synthesis in dynamic
urban scenarios.";Su Sun<author:sep>Cheng Zhao<author:sep>Zhuoyang Sun<author:sep>Yingjie Victor Chen<author:sep>Mei Chen;http://arxiv.org/pdf/2411.15482v1;cs.CV;;gaussian splatting
2411.15476v1;http://arxiv.org/abs/2411.15476v1;2024-11-23;Gassidy: Gaussian Splatting SLAM in Dynamic Environments;"3D Gaussian Splatting (3DGS) allows flexible adjustments to scene
representation, enabling continuous optimization of scene quality during dense
visual simultaneous localization and mapping (SLAM) in static environments.
However, 3DGS faces challenges in handling environmental disturbances from
dynamic objects with irregular movement, leading to degradation in both camera
tracking accuracy and map reconstruction quality. To address this challenge, we
develop an RGB-D dense SLAM which is called Gaussian Splatting SLAM in Dynamic
Environments (Gassidy). This approach calculates Gaussians to generate
rendering loss flows for each environmental component based on a designed
photometric-geometric loss function. To distinguish and filter environmental
disturbances, we iteratively analyze rendering loss flows to detect features
characterized by changes in loss values between dynamic objects and static
components. This process ensures a clean environment for accurate scene
reconstruction. Compared to state-of-the-art SLAM methods, experimental results
on open datasets show that Gassidy improves camera tracking precision by up to
97.9% and enhances map quality by up to 6%.";Long Wen<author:sep>Shixin Li<author:sep>Yu Zhang<author:sep>Yuhong Huang<author:sep>Jianjie Lin<author:sep>Fengjunjie Pan<author:sep>Zhenshan Bing<author:sep>Alois Knoll;http://arxiv.org/pdf/2411.15476v1;cs.RO;This paper is currently under reviewed for ICRA 2025;gaussian splatting
2411.15582v1;http://arxiv.org/abs/2411.15582v1;2024-11-23;EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting;"Photorealistic reconstruction of street scenes is essential for developing
real-world simulators in autonomous driving. While recent methods based on
3D/4D Gaussian Splatting (GS) have demonstrated promising results, they still
encounter challenges in complex street scenes due to the unpredictable motion
of dynamic objects. Current methods typically decompose street scenes into
static and dynamic objects, learning the Gaussians in either a supervised
manner (e.g., w/ 3D bounding-box) or a self-supervised manner (e.g., w/o 3D
bounding-box). However, these approaches do not effectively model the motions
of dynamic objects (e.g., the motion speed of pedestrians is clearly different
from that of vehicles), resulting in suboptimal scene decomposition. To address
this, we propose Explicit Motion Decomposition (EMD), which models the motions
of dynamic objects by introducing learnable motion embeddings to the Gaussians,
enhancing the decomposition in street scenes. The proposed EMD is a
plug-and-play approach applicable to various baseline methods. We also propose
tailored training strategies to apply EMD to both supervised and
self-supervised baselines. Through comprehensive experimentation, we illustrate
the effectiveness of our approach with various established baselines. The code
will be released at: https://qingpowuwu.github.io/emdgaussian.github.io/.";Xiaobao Wei<author:sep>Qingpo Wuwu<author:sep>Zhongyu Zhao<author:sep>Zhuangzhe Wu<author:sep>Nan Huang<author:sep>Ming Lu<author:sep>Ningning MA<author:sep>Shanghang Zhang;http://arxiv.org/pdf/2411.15582v1;cs.CV;;gaussian splatting
2411.14847v1;http://arxiv.org/abs/2411.14847v1;2024-11-22;Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly  Training for 4D Reconstruction;"The recent development of 3D Gaussian Splatting (3DGS) has led to great
interest in 4D dynamic spatial reconstruction from multi-view visual inputs.
While existing approaches mainly rely on processing full-length multi-view
videos for 4D reconstruction, there has been limited exploration of iterative
online reconstruction methods that enable on-the-fly training and per-frame
streaming. Current 3DGS-based streaming methods treat the Gaussian primitives
uniformly and constantly renew the densified Gaussians, thereby overlooking the
difference between dynamic and static features and also neglecting the temporal
continuity in the scene. To address these limitations, we propose a novel
three-stage pipeline for iterative streamable 4D dynamic spatial
reconstruction. Our pipeline comprises a selective inheritance stage to
preserve temporal continuity, a dynamics-aware shift stage for distinguishing
dynamic and static primitives and optimizing their movements, and an
error-guided densification stage to accommodate emerging objects. Our method
achieves state-of-the-art performance in online 4D reconstruction,
demonstrating a 20% improvement in on-the-fly training speed, superior
representation quality, and real-time rendering capability. Project page:
https://www.liuzhening.top/DASS";Zhening Liu<author:sep>Yingdong Hu<author:sep>Xinjie Zhang<author:sep>Jiawei Shao<author:sep>Zehong Lin<author:sep>Jun Zhang;http://arxiv.org/pdf/2411.14847v1;cs.CV;Project page: https://www.liuzhening.top/DASS;gaussian splatting
2411.15355v1;http://arxiv.org/abs/2411.15355v1;2024-11-22;UniGaussian: Driving Scene Reconstruction from Multiple Camera Models  via Unified Gaussian Representations;"Urban scene reconstruction is crucial for real-world autonomous driving
simulators. Although existing methods have achieved photorealistic
reconstruction, they mostly focus on pinhole cameras and neglect fisheye
cameras. In fact, how to effectively simulate fisheye cameras in driving scene
remains an unsolved problem. In this work, we propose UniGaussian, a novel
approach that learns a unified 3D Gaussian representation from multiple camera
models for urban scene reconstruction in autonomous driving. Our contributions
are two-fold. First, we propose a new differentiable rendering method that
distorts 3D Gaussians using a series of affine transformations tailored to
fisheye camera models. This addresses the compatibility issue of 3D Gaussian
splatting with fisheye cameras, which is hindered by light ray distortion
caused by lenses or mirrors. Besides, our method maintains real-time rendering
while ensuring differentiability. Second, built on the differentiable rendering
method, we design a new framework that learns a unified Gaussian representation
from multiple camera models. By applying affine transformations to adapt
different camera models and regularizing the shared Gaussians with supervision
from different modalities, our framework learns a unified 3D Gaussian
representation with input data from multiple sources and achieves holistic
driving scene understanding. As a result, our approach models multiple sensors
(pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR
point clouds). Our experiments show that our method achieves superior rendering
quality and fast rendering speed for driving scene simulation.";Yuan Ren<author:sep>Guile Wu<author:sep>Runhao Li<author:sep>Zheyuan Yang<author:sep>Yibo Liu<author:sep>Xingxin Chen<author:sep>Tongtong Cao<author:sep>Bingbing Liu;http://arxiv.org/pdf/2411.15355v1;cs.CV;Technical report;
2411.14974v2;http://arxiv.org/abs/2411.14974v2;2024-11-22;3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes;"Recent advances in radiance field reconstruction, such as 3D Gaussian
Splatting (3DGS), have achieved high-quality novel view synthesis and fast
rendering by representing scenes with compositions of Gaussian primitives.
However, 3D Gaussians present several limitations for scene reconstruction.
Accurately capturing hard edges is challenging without significantly increasing
the number of Gaussians, creating a large memory footprint. Moreover, they
struggle to represent flat surfaces, as they are diffused in space. Without
hand-crafted regularizers, they tend to disperse irregularly around the actual
surface. To circumvent these issues, we introduce a novel method, named 3D
Convex Splatting (3DCS), which leverages 3D smooth convexes as primitives for
modeling geometrically-meaningful radiance fields from multi-view images.
Smooth convex shapes offer greater flexibility than Gaussians, allowing for a
better representation of 3D scenes with hard edges and dense volumes using
fewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achieves
superior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks and
Temples, and Deep Blending. Specifically, our method attains an improvement of
up to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining high
rendering speeds and reducing the number of required primitives. Our results
highlight the potential of 3D Convex Splatting to become the new standard for
high-quality scene reconstruction and novel view synthesis. Project page:
convexsplatting.github.io.";Jan Held<author:sep>Renaud Vandeghen<author:sep>Abdullah Hamdi<author:sep>Adrien Deliege<author:sep>Anthony Cioppa<author:sep>Silvio Giancola<author:sep>Andrea Vedaldi<author:sep>Bernard Ghanem<author:sep>Marc Van Droogenbroeck;http://arxiv.org/pdf/2411.14974v2;cs.CV;13 pages, 13 figures, 10 tables;nerf
2411.14716v1;http://arxiv.org/abs/2411.14716v1;2024-11-22;VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving;"This paper introduces VisionPAD, a novel self-supervised pre-training
paradigm designed for vision-centric algorithms in autonomous driving. In
contrast to previous approaches that employ neural rendering with explicit
depth supervision, VisionPAD utilizes more efficient 3D Gaussian Splatting to
reconstruct multi-view representations using only images as supervision.
Specifically, we introduce a self-supervised method for voxel velocity
estimation. By warping voxels to adjacent frames and supervising the rendered
outputs, the model effectively learns motion cues in the sequential data.
Furthermore, we adopt a multi-frame photometric consistency approach to enhance
geometric perception. It projects adjacent frames to the current frame based on
rendered depths and relative poses, boosting the 3D geometric representation
through pure image supervision. Extensive experiments on autonomous driving
datasets demonstrate that VisionPAD significantly improves performance in 3D
object detection, occupancy prediction and map segmentation, surpassing
state-of-the-art pre-training strategies by a considerable margin.";Haiming Zhang<author:sep>Wending Zhou<author:sep>Yiyao Zhu<author:sep>Xu Yan<author:sep>Jiantao Gao<author:sep>Dongfeng Bai<author:sep>Yingjie Cai<author:sep>Bingbing Liu<author:sep>Shuguang Cui<author:sep>Zhen Li;http://arxiv.org/pdf/2411.14716v1;cs.CV;;gaussian splatting
2411.15018v1;http://arxiv.org/abs/2411.15018v1;2024-11-22;Neural 4D Evolution under Large Topological Changes from 2D Images;"In the literature, it has been shown that the evolution of the known explicit
3D surface to the target one can be learned from 2D images using the
instantaneous flow field, where the known and target 3D surfaces may largely
differ in topology. We are interested in capturing 4D shapes whose topology
changes largely over time. We encounter that the straightforward extension of
the existing 3D-based method to the desired 4D case performs poorly.
  In this work, we address the challenges in extending 3D neural evolution to
4D under large topological changes by proposing two novel modifications. More
precisely, we introduce (i) a new architecture to discretize and encode the
deformation and learn the SDF and (ii) a technique to impose the temporal
consistency. (iii) Also, we propose a rendering scheme for color prediction
based on Gaussian splatting. Furthermore, to facilitate learning directly from
2D images, we propose a learning framework that can disentangle the geometry
and appearance from RGB images. This method of disentanglement, while also
useful for the 4D evolution problem that we are concentrating on, is also novel
and valid for static scenes. Our extensive experiments on various data provide
awesome results and, most importantly, open a new approach toward
reconstructing challenging scenes with significant topological changes and
deformations. Our source code and the dataset are publicly available at
https://github.com/insait-institute/N4DE.";AmirHossein Naghi Razlighi<author:sep>Tiago Novello<author:sep>Asen Nachkov<author:sep>Thomas Probst<author:sep>Danda Paudel;http://arxiv.org/pdf/2411.15018v1;cs.CV;15 pages, 21 figures;gaussian splatting
2411.14322v1;http://arxiv.org/abs/2411.14322v1;2024-11-21;SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting  and Dense Feature Matching;"Experience Goal Visual Rearrangement task stands as a foundational challenge
within Embodied AI, requiring an agent to construct a robust world model that
accurately captures the goal state. The agent uses this world model to restore
a shuffled scene to its original configuration, making an accurate
representation of the world essential for successfully completing the task. In
this work, we present a novel framework that leverages on 3D Gaussian Splatting
as a 3D scene representation for experience goal visual rearrangement task.
Recent advances in volumetric scene representation like 3D Gaussian Splatting,
offer fast rendering of high quality and photo-realistic novel views. Our
approach enables the agent to have consistent views of the current and the goal
setting of the rearrangement task, which enables the agent to directly compare
the goal state and the shuffled state of the world in image space. To compare
these views, we propose to use a dense feature matching method with visual
features extracted from a foundation model, leveraging its advantages of a more
universal feature representation, which facilitates robustness, and
generalization. We validate our approach on the AI2-THOR rearrangement
challenge benchmark and demonstrate improvements over the current state of the
art methods";Arjun P S<author:sep>Andrew Melnik<author:sep>Gora Chand Nandi;http://arxiv.org/pdf/2411.14322v1;cs.RO;;gaussian splatting
2411.14423v1;http://arxiv.org/abs/2411.14423v1;2024-11-21;Unleashing the Potential of Multi-modal Foundation Models and Video  Diffusion for 4D Dynamic Physical Scene Simulation;"Realistic simulation of dynamic scenes requires accurately capturing diverse
material properties and modeling complex object interactions grounded in
physical principles. However, existing methods are constrained to basic
material types with limited predictable parameters, making them insufficient to
represent the complexity of real-world materials. We introduce a novel approach
that leverages multi-modal foundation models and video diffusion to achieve
enhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to
identify material types and initialize material parameters through image
queries, while simultaneously inferring 3D Gaussian splats for detailed scene
representation. We further refine these material parameters using video
diffusion with a differentiable Material Point Method (MPM) and optical flow
guidance rather than render loss or Score Distillation Sampling (SDS) loss.
This integrated framework enables accurate prediction and realistic simulation
of dynamic interactions in real-world scenarios, advancing both accuracy and
flexibility in physics-based simulations.";Zhuoman Liu<author:sep>Weicai Ye<author:sep>Yan Luximon<author:sep>Pengfei Wan<author:sep>Di Zhang;http://arxiv.org/pdf/2411.14423v1;cs.CV;Homepage: https://zhuomanliu.github.io/PhysFlow/;
2411.14514v2;http://arxiv.org/abs/2411.14514v2;2024-11-21;NexusSplats: Efficient 3D Gaussian Splatting in the Wild;"While 3D Gaussian Splatting (3DGS) has recently demonstrated remarkable
rendering quality and efficiency in 3D scene reconstruction, it struggles with
varying lighting conditions and incidental occlusions in real-world scenarios.
To accommodate varying lighting conditions, existing 3DGS extensions apply
color mapping to the massive Gaussian primitives with individually optimized
appearance embeddings. To handle occlusions, they predict pixel-wise
uncertainties via 2D image features for occlusion capture. Nevertheless, such
massive color mapping and pixel-wise uncertainty prediction strategies suffer
from not only additional computational costs but also coarse-grained lighting
and occlusion handling. In this work, we propose a nexus kernel-driven
approach, termed NexusSplats, for efficient and finer 3D scene reconstruction
under complex lighting and occlusion conditions. In particular, NexusSplats
leverages a novel light decoupling strategy where appearance embeddings are
optimized based on nexus kernels instead of massive Gaussian primitives, thus
accelerating reconstruction speeds while ensuring local color consistency for
finer textures. Additionally, a Gaussian-wise uncertainty mechanism is
developed, aligning 3D structures with 2D image features for fine-grained
occlusion handling. Experimental results demonstrate that NexusSplats achieves
state-of-the-art rendering quality while reducing reconstruction time by up to
70.4% compared to the current best in quality.";Yuzhou Tang<author:sep>Dejun Xu<author:sep>Yongjie Hou<author:sep>Zhenzhong Wang<author:sep>Min Jiang;http://arxiv.org/pdf/2411.14514v2;cs.CV;Project page: https://nexus-splats.github.io/;gaussian splatting
2411.14384v1;http://arxiv.org/abs/2411.14384v1;2024-11-21;Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable  Single-stage Image-to-3D Generation;"Existing feed-forward image-to-3D methods mainly rely on 2D multi-view
diffusion models that cannot guarantee 3D consistency. These methods easily
collapse when changing the prompt view direction and mainly handle
object-centric prompt images. In this paper, we propose a novel single-stage 3D
diffusion model, DiffusionGS, for object and scene generation from a single
view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to
enforce view consistency and allow the model to generate robustly given prompt
views of any directions, beyond object-centric inputs. Plus, to improve the
capability and generalization ability of DiffusionGS, we scale up 3D training
data by developing a scene-object mixed training strategy. Experiments show
that our method enjoys better generation quality (2.20 dB higher in PSNR and
23.25 lower in FID) and over 5x faster speed (~6s on an A100 GPU) than SOTA
methods. The user study and text-to-3D applications also reveals the practical
values of our method. Our Project page at
https://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and
interactive generation results.";Yuanhao Cai<author:sep>He Zhang<author:sep>Kai Zhang<author:sep>Yixun Liang<author:sep>Mengwei Ren<author:sep>Fujun Luan<author:sep>Qing Liu<author:sep>Soo Ye Kim<author:sep>Jianming Zhang<author:sep>Zhifei Zhang<author:sep>Yuqian Zhou<author:sep>Zhe Lin<author:sep>Alan Yuille;http://arxiv.org/pdf/2411.14384v1;cs.CV;"A novel one-stage 3DGS-based diffusion generates objects and scenes
  from a single view in ~6 seconds";gaussian splatting
2411.12981v1;http://arxiv.org/abs/2411.12981v1;2024-11-20;GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting;"Gaze estimation encounters generalization challenges when dealing with
out-of-distribution data. To address this problem, recent methods use neural
radiance fields (NeRF) to generate augmented data. However, existing methods
based on NeRF are computationally expensive and lack facial details. 3D
Gaussian Splatting (3DGS) has become the prevailing representation of neural
fields. While 3DGS has been extensively examined in head avatars, it faces
challenges with accurate gaze control and generalization across different
subjects. In this work, we propose GazeGaussian, a high-fidelity gaze
redirection method that uses a two-stream 3DGS model to represent the face and
eye regions separately. By leveraging the unstructured nature of 3DGS, we
develop a novel eye representation for rigid eye rotation based on the target
gaze direction. To enhance synthesis generalization across various subjects, we
integrate an expression-conditional module to guide the neural renderer.
Comprehensive experiments show that GazeGaussian outperforms existing methods
in rendering speed, gaze redirection accuracy, and facial synthesis across
multiple datasets. We also demonstrate that existing gaze estimation methods
can leverage GazeGaussian to improve their generalization performance. The code
will be available at: https://ucwxb.github.io/GazeGaussian/.";Xiaobao Wei<author:sep>Peng Chen<author:sep>Guangyu Li<author:sep>Ming Lu<author:sep>Hui Chen<author:sep>Feng Tian;http://arxiv.org/pdf/2411.12981v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.13549v1;http://arxiv.org/abs/2411.13549v1;2024-11-20;Generating 3D-Consistent Videos from Unposed Internet Photos;"We address the problem of generating videos from unposed internet photos. A
handful of input images serve as keyframes, and our model interpolates between
them to simulate a path moving between the cameras. Given random images, a
model's ability to capture underlying geometry, recognize scene identity, and
relate frames in terms of camera position and orientation reflects a
fundamental understanding of 3D structure and scene layout. However, existing
video models such as Luma Dream Machine fail at this task. We design a
self-supervised method that takes advantage of the consistency of videos and
variability of multiview internet photos to train a scalable, 3D-aware video
model without any 3D annotations such as camera parameters. We validate that
our method outperforms all baselines in terms of geometric and appearance
consistency. We also show our model benefits applications that enable camera
control, such as 3D Gaussian Splatting. Our results suggest that we can scale
up scene-level 3D learning using only 2D data such as videos and multiview
internet photos.";Gene Chou<author:sep>Kai Zhang<author:sep>Sai Bi<author:sep>Hao Tan<author:sep>Zexiang Xu<author:sep>Fujun Luan<author:sep>Bharath Hariharan<author:sep>Noah Snavely;http://arxiv.org/pdf/2411.13549v1;cs.CV;;gaussian splatting
2411.13753v1;http://arxiv.org/abs/2411.13753v1;2024-11-20;FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian  Splatting;"We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting,
which seeks to address the main limitations of existing semantic Gaussian
Splatting methods, namely: slow training and rendering speeds; high memory
usage; and ambiguous semantic object localization. In deriving FAST-Splat , we
formulate open-vocabulary semantic Gaussian Splatting as the problem of
extending closed-set semantic distillation to the open-set (open-vocabulary)
setting, enabling FAST-Splat to provide precise semantic object localization
results, even when prompted with ambiguous user-provided natural-language
queries. Further, by exploiting the explicit form of the Gaussian Splatting
scene representation to the fullest extent, FAST-Splat retains the remarkable
training and rendering speeds of Gaussian Splatting. Specifically, while
existing semantic Gaussian Splatting methods distill semantics into a separate
neural field or utilize neural models for dimensionality reduction, FAST-Splat
directly augments each Gaussian with specific semantic codes, preserving the
training, rendering, and memory-usage advantages of Gaussian Splatting over
neural field methods. These Gaussian-specific semantic codes, together with a
hash-table, enable semantic similarity to be measured with open-vocabulary user
prompts and further enable FAST-Splat to respond with unambiguous semantic
object labels and 3D masks, unlike prior methods. In experiments, we
demonstrate that FAST-Splat is 4x to 6x faster to train with a 13x faster data
pre-processing step, achieves between 18x to 75x faster rendering speeds, and
requires about 3x smaller GPU memory, compared to the best-competing semantic
Gaussian Splatting methods. Further, FAST-Splat achieves relatively similar or
better semantic segmentation performance compared to existing methods. After
the review period, we will provide links to the project website and the
codebase.";Ola Shorinwa<author:sep>Jiankai Sun<author:sep>Mac Schwager;http://arxiv.org/pdf/2411.13753v1;cs.CV;;gaussian splatting
2411.13610v1;http://arxiv.org/abs/2411.13610v1;2024-11-20;Video2BEV: Transforming Drone Videos to BEVs for Video-based  Geo-localization;"Existing approaches to drone visual geo-localization predominantly adopt the
image-based setting, where a single drone-view snapshot is matched with images
from other platforms. Such task formulation, however, underutilizes the
inherent video output of the drone and is sensitive to occlusions and
environmental constraints. To address these limitations, we formulate a new
video-based drone geo-localization task and propose the Video2BEV paradigm.
This paradigm transforms the video into a Bird's Eye View (BEV), simplifying
the subsequent matching process. In particular, we employ Gaussian Splatting to
reconstruct a 3D scene and obtain the BEV projection. Different from the
existing transform methods, \eg, polar transform, our BEVs preserve more
fine-grained details without significant distortion. To further improve model
scalability toward diverse BEVs and satellite figures, our Video2BEV paradigm
also incorporates a diffusion-based module for generating hard negative
samples, which facilitates discriminative feature learning. To validate our
approach, we introduce UniV, a new video-based geo-localization dataset that
extends the image-based University-1652 dataset. UniV features flight paths at
$30^\circ$ and $45^\circ$ elevation angles with increased frame rates of up to
10 frames per second (FPS). Extensive experiments on the UniV dataset show that
our Video2BEV paradigm achieves competitive recall rates and outperforms
conventional video-based methods. Compared to other methods, our proposed
approach exhibits robustness at lower elevations with more occlusions.";Hao Ju<author:sep>Zhedong Zheng;http://arxiv.org/pdf/2411.13610v1;cs.CV;;gaussian splatting
2411.13631v1;http://arxiv.org/abs/2411.13631v1;2024-11-20;Sparse Input View Synthesis: 3D Representations and Reliable Priors;"Novel view synthesis refers to the problem of synthesizing novel viewpoints
of a scene given the images from a few viewpoints. This is a fundamental
problem in computer vision and graphics, and enables a vast variety of
applications such as meta-verse, free-view watching of events, video gaming,
video stabilization and video compression. Recent 3D representations such as
radiance fields and multi-plane images significantly improve the quality of
images rendered from novel viewpoints. However, these models require a dense
sampling of input views for high quality renders. Their performance goes down
significantly when only a few input views are available. In this thesis, we
focus on the sparse input novel view synthesis problem for both static and
dynamic scenes. In the first part of this work, we mainly focus on sparse input
novel view synthesis of static scenes using neural radiance fields (NeRF). We
study the design of reliable and dense priors to better regularize the NeRF in
such situations. In particular, we propose a prior on the visibility of the
pixels in a pair of input views. We show that this visibility prior, which is
related to the relative depth of objects, is dense and more reliable than
existing priors on absolute depth. We compute the visibility prior using plane
sweep volumes without the need to train a neural network on large datasets. We
evaluate our approach on multiple datasets and show that our model outperforms
existing approaches for sparse input novel view synthesis. In the second part,
we aim to further improve the regularization by learning a scene-specific prior
that does not suffer from generalization issues. We achieve this by learning
the prior on the given scene alone without pre-training on large datasets. In
particular, we design augmented NeRFs to obtain better depth supervision in
certain regions of the scene for the main NeRF. Further, we extend this
framework to also apply to newer and faster radiance field models such as
TensoRF and ZipNeRF. Through extensive experiments on multiple datasets, we
show the superiority of our approach in sparse input novel view synthesis. The
design of sparse input fast dynamic radiance fields is severely constrained by
the lack of suitable representations and reliable priors for motion. We address
the first challenge by designing an explicit motion model based on factorized
volumes that is compact and optimizes quickly. We also introduce reliable
sparse flow priors to constrain the motion field, since we find that the
popularly employed dense optical flow priors are unreliable. We show the
benefits of our motion representation and reliable priors on multiple datasets.
In the final part of this thesis, we study the application of view synthesis
for frame rate upsampling in video gaming. Specifically, we consider the
problem of temporal view synthesis, where the goal is to predict the future
frames given the past frames and the camera motion. The key challenge here is
in predicting the future motion of the objects by estimating their past motion
and extrapolating it. We explore the use of multi-plane image representations
and scene depth to reliably estimate the object motion, particularly in the
occluded regions. We design a new database to effectively evaluate our approach
for temporal view synthesis of dynamic scenes and show that we achieve
state-of-the-art performance.";Nagabhushan Somraj;http://arxiv.org/pdf/2411.13631v1;cs.CV;"PhD Thesis of Nagabhushan S N, Dept of ECE, Indian Institute of
  Science (IISc); Advisor: Dr. Rajiv Soundararajan; Thesis Reviewers: Dr.
  Kaushik Mitra (IIT Madras), Dr. Aniket Bera (Purdue University); Submitted:
  May 2024; Accepted and Defended: Sep 2024; Abstract condensed, please check
  the PDF for full abstract";nerf
2411.13620v1;http://arxiv.org/abs/2411.13620v1;2024-11-20;Robust SG-NeRF: Robust Scene Graph Aided Neural Surface Reconstruction;"Neural surface reconstruction relies heavily on accurate camera poses as
input. Despite utilizing advanced pose estimators like COLMAP or ARKit, camera
poses can still be noisy. Existing pose-NeRF joint optimization methods handle
poses with small noise (inliers) effectively but struggle with large noise
(outliers), such as mirrored poses. In this work, we focus on mitigating the
impact of outlier poses. Our method integrates an inlier-outlier confidence
estimation scheme, leveraging scene graph information gathered during the data
preparation phase. Unlike previous works directly using rendering metrics as
the reference, we employ a detached color network that omits the viewing
direction as input to minimize the impact caused by shape-radiance ambiguities.
This enhanced confidence updating strategy effectively differentiates between
inlier and outlier poses, allowing us to sample more rays from inlier poses to
construct more reliable radiance fields. Additionally, we introduce a
re-projection loss based on the current Signed Distance Function (SDF) and pose
estimations, strengthening the constraints between matching image pairs. For
outlier poses, we adopt a Monte Carlo re-localization method to find better
solutions. We also devise a scene graph updating strategy to provide more
accurate information throughout the training process. We validate our approach
on the SG-NeRF and DTU datasets. Experimental results on various datasets
demonstrate that our methods can consistently improve the reconstruction
qualities and pose accuracies.";Yi Gu<author:sep>Dongjun Ye<author:sep>Zhaorui Wang<author:sep>Jiaxu Wang<author:sep>Jiahang Cao<author:sep>Renjing Xu;http://arxiv.org/pdf/2411.13620v1;cs.CV;https://rsg-nerf.github.io/RSG-NeRF/;nerf
2411.12168v2;http://arxiv.org/abs/2411.12168v2;2024-11-19;Sketch-guided Cage-based 3D Gaussian Splatting Deformation;"3D Gaussian Splatting (GS) is one of the most promising novel 3D
representations that has received great interest in computer graphics and
computer vision. While various systems have introduced editing capabilities for
3D GS, such as those guided by text prompts, fine-grained control over
deformation remains an open challenge. In this work, we present a novel
sketch-guided 3D GS deformation system that allows users to intuitively modify
the geometry of a 3D GS model by drawing a silhouette sketch from a single
viewpoint. Our approach introduces a new deformation method that combines
cage-based deformations with a variant of Neural Jacobian Fields, enabling
precise, fine-grained control. Additionally, it leverages large-scale 2D
diffusion priors and ControlNet to ensure the generated deformations are
semantically plausible. Through a series of experiments, we demonstrate the
effectiveness of our method and showcase its ability to animate static 3D GS
models as one of its key applications.";Tianhao Xie<author:sep>Noam Aigerman<author:sep>Eugene Belilovsky<author:sep>Tiberiu Popa;http://arxiv.org/pdf/2411.12168v2;cs.CV;"10 pages, 9 figures, project page:
  https://tianhaoxie.github.io/project/gs_deform/";gaussian splatting
2411.12440v2;http://arxiv.org/abs/2411.12440v2;2024-11-19;Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear  Kernels;"Recent advancements in 3D Gaussian Splatting (3DGS) have substantially
improved novel view synthesis, enabling high-quality reconstruction and
real-time rendering. However, blurring artifacts, such as floating primitives
and over-reconstruction, remain challenging. Current methods address these
issues by refining scene structure, enhancing geometric representations,
addressing blur in training images, improving rendering consistency, and
optimizing density control, yet the role of kernel design remains
underexplored. We identify the soft boundaries of Gaussian ellipsoids as one of
the causes of these artifacts, limiting detail capture in high-frequency
regions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which
replaces Gaussian kernels with linear kernels to achieve sharper and more
precise results, particularly in high-frequency regions. Through evaluations on
three datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along
with a 30% FPS improvement over baseline 3DGS. The implementation will be made
publicly available upon acceptance.";Haodong Chen<author:sep>Runnan Chen<author:sep>Qiang Qu<author:sep>Zhaoqing Wang<author:sep>Tongliang Liu<author:sep>Xiaoming Chen<author:sep>Yuk Ying Chung;http://arxiv.org/pdf/2411.12440v2;cs.CV;;gaussian splatting
2411.12788v1;http://arxiv.org/abs/2411.12788v1;2024-11-19;Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive  Gaussian Densification;"In this study, we explore the essential challenge of fast scene optimization
for Gaussian Splatting. Through a thorough analysis of the geometry modeling
process, we reveal that dense point clouds can be effectively reconstructed
early in optimization through Gaussian representations. This insight leads to
our approach of aggressive Gaussian densification, which provides a more
efficient alternative to conventional progressive densification methods. By
significantly increasing the number of critical Gaussians, we enhance the model
capacity to capture dense scene geometry at the early stage of optimization.
This strategy is seamlessly integrated into the Mini-Splatting densification
and simplification framework, enabling rapid convergence without compromising
quality. Additionally, we introduce visibility culling within Gaussian
Splatting, leveraging per-view Gaussian importance as precomputed visibility to
accelerate the optimization process. Our Mini-Splatting2 achieves a balanced
trade-off among optimization time, the number of Gaussians, and rendering
quality, establishing a strong baseline for future Gaussian-Splatting-based
works. Our work sets the stage for more efficient, high-quality 3D scene
modeling in real-world applications, and the code will be made available no
matter acceptance.";Guangchi Fang<author:sep>Bing Wang;http://arxiv.org/pdf/2411.12788v1;cs.CV;;gaussian splatting
2411.12510v1;http://arxiv.org/abs/2411.12510v1;2024-11-19;PR-ENDO: Physically Based Relightable Gaussian Splatting for Endoscopy;"Endoscopic procedures are crucial for colorectal cancer diagnosis, and
three-dimensional reconstruction of the environment for real-time novel-view
synthesis can significantly enhance diagnosis. We present PR-ENDO, a framework
that leverages 3D Gaussian Splatting within a physically based, relightable
model tailored for the complex acquisition conditions in endoscopy, such as
restricted camera rotations and strong view-dependent illumination. By
exploiting the connection between the camera and light source, our approach
introduces a relighting model to capture the intricate interactions between
light and tissue using physically based rendering and MLP. Existing methods
often produce artifacts and inconsistencies under these conditions, which
PR-ENDO overcomes by incorporating a specialized diffuse MLP that utilizes
light angles and normal vectors, achieving stable reconstructions even with
limited training camera rotations. We benchmarked our framework using a
publicly available dataset and a newly introduced dataset with wider camera
rotations. Our methods demonstrated superior image quality compared to baseline
approaches.";Joanna Kaleta<author:sep>Weronika Smolak-Dyżewska<author:sep>Dawid Malarz<author:sep>Diego Dall'Alba<author:sep>Przemysław Korzeniowski<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2411.12510v1;cs.CV;;gaussian splatting
2411.12471v1;http://arxiv.org/abs/2411.12471v1;2024-11-19;SCIGS: 3D Gaussians Splatting from a Snapshot Compressive Image;"Snapshot Compressive Imaging (SCI) offers a possibility for capturing
information in high-speed dynamic scenes, requiring efficient reconstruction
method to recover scene information. Despite promising results, current deep
learning-based and NeRF-based reconstruction methods face challenges: 1) deep
learning-based reconstruction methods struggle to maintain 3D structural
consistency within scenes, and 2) NeRF-based reconstruction methods still face
limitations in handling dynamic scenes. To address these challenges, we propose
SCIGS, a variant of 3DGS, and develop a primitive-level transformation network
that utilizes camera pose stamps and Gaussian primitive coordinates as
embedding vectors. This approach resolves the necessity of camera pose in
vanilla 3DGS and enhances multi-view 3D structural consistency in dynamic
scenes by utilizing transformed primitives. Additionally, a high-frequency
filter is introduced to eliminate the artifacts generated during the
transformation. The proposed SCIGS is the first to reconstruct a 3D explicit
scene from a single compressed image, extending its application to dynamic 3D
scenes. Experiments on both static and dynamic scenes demonstrate that SCIGS
not only enhances SCI decoding but also outperforms current state-of-the-art
methods in reconstructing dynamic 3D scenes from a single compressed image. The
code will be made available upon publication.";Zixu Wang<author:sep>Hao Yang<author:sep>Yu Guo<author:sep>Fei Wang;http://arxiv.org/pdf/2411.12471v1;cs.CV;;nerf
2411.12185v1;http://arxiv.org/abs/2411.12185v1;2024-11-19;LiV-GS: LiDAR-Vision Integration for 3D Gaussian Splatting SLAM in  Outdoor Environments;"We present LiV-GS, a LiDAR-visual SLAM system in outdoor environments that
leverages 3D Gaussian as a differentiable spatial representation. Notably,
LiV-GS is the first method that directly aligns discrete and sparse LiDAR data
with continuous differentiable Gaussian maps in large-scale outdoor scenes,
overcoming the limitation of fixed resolution in traditional LiDAR mapping. The
system aligns point clouds with Gaussian maps using shared covariance
attributes for front-end tracking and integrates the normal orientation into
the loss function to refines the Gaussian map. To reliably and stably update
Gaussians outside the LiDAR field of view, we introduce a novel conditional
Gaussian constraint that aligns these Gaussians closely with the nearest
reliable ones. The targeted adjustment enables LiV-GS to achieve fast and
accurate mapping with novel view synthesis at a rate of 7.98 FPS. Extensive
comparative experiments demonstrate LiV-GS's superior performance in SLAM,
image rendering and mapping. The successful cross-modal radar-LiDAR
localization highlights the potential of LiV-GS for applications in cross-modal
semantic positioning and object segmentation with Gaussian maps.";Renxiang Xiao<author:sep>Wei Liu<author:sep>Yushuai Chen<author:sep>Liang Hu;http://arxiv.org/pdf/2411.12185v1;cs.RO;;gaussian splatting
2411.12452v1;http://arxiv.org/abs/2411.12452v1;2024-11-19;GaussianPretrain: A Simple Unified 3D Gaussian Representation for Visual  Pre-training in Autonomous Driving;"Self-supervised learning has made substantial strides in image processing,
while visual pre-training for autonomous driving is still in its infancy.
Existing methods often focus on learning geometric scene information while
neglecting texture or treating both aspects separately, hindering comprehensive
scene understanding. In this context, we are excited to introduce
GaussianPretrain, a novel pre-training paradigm that achieves a holistic
understanding of the scene by uniformly integrating geometric and texture
representations. Conceptualizing 3D Gaussian anchors as volumetric LiDAR
points, our method learns a deepened understanding of scenes to enhance
pre-training performance with detailed spatial structure and texture, achieving
that 40.6% faster than NeRF-based method UniPAD with 70% GPU memory only. We
demonstrate the effectiveness of GaussianPretrain across multiple 3D perception
tasks, showing significant performance improvements, such as a 7.05% increase
in NDS for 3D object detection, boosts mAP by 1.9% in HD map construction and
0.8% improvement on Occupancy prediction. These significant gains highlight
GaussianPretrain's theoretical innovation and strong practical potential,
promoting visual pre-training development for autonomous driving. Source code
will be available at https://github.com/Public-BOTs/GaussianPretrain";Shaoqing Xu<author:sep>Fang Li<author:sep>Shengyin Jiang<author:sep>Ziying Song<author:sep>Li Liu<author:sep>Zhi-xin Yang;http://arxiv.org/pdf/2411.12452v1;cs.CV;10 pages, 5 figures;nerf
2411.12789v1;http://arxiv.org/abs/2411.12789v1;2024-11-19;Automated 3D Physical Simulation of Open-world Scene with Gaussian  Splatting;"Recent advancements in 3D generation models have opened new possibilities for
simulating dynamic 3D object movements and customizing behaviors, yet creating
this content remains challenging. Current methods often require manual
assignment of precise physical properties for simulations or rely on video
generation models to predict them, which is computationally intensive. In this
paper, we rethink the usage of multi-modal large language model (MLLM) in
physics-based simulation, and present Sim Anything, a physics-based approach
that endows static 3D objects with interactive dynamics. We begin with detailed
scene reconstruction and object-level 3D open-vocabulary segmentation,
progressing to multi-view image in-painting. Inspired by human visual
reasoning, we propose MLLM-based Physical Property Perception (MLLM-P3) to
predict mean physical properties of objects in a zero-shot manner. Based on the
mean values and the object's geometry, the Material Property Distribution
Prediction model (MPDP) model then estimates the full distribution,
reformulating the problem as probability distribution estimation to reduce
computational costs. Finally, we simulate objects in an open-world scene with
particles sampled via the Physical-Geometric Adaptive Sampling (PGAS) strategy,
efficiently capturing complex deformations and significantly reducing
computational costs. Extensive experiments and user studies demonstrate our Sim
Anything achieves more realistic motion than state-of-the-art methods within 2
minutes on a single GPU.";Haoyu Zhao<author:sep>Hao Wang<author:sep>Xingyue Zhao<author:sep>Hongqiu Wang<author:sep>Zhiyu Wu<author:sep>Chengjiang Long<author:sep>Hua Zou;http://arxiv.org/pdf/2411.12789v1;cs.CV;;
2411.11941v1;http://arxiv.org/abs/2411.11941v1;2024-11-18;TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians  for Robust Reconstruction;"Dynamic scene reconstruction is a long-term challenge in 3D vision. Recent
methods extend 3D Gaussian Splatting to dynamic scenes via additional
deformation fields and apply explicit constraints like motion flow to guide the
deformation. However, they learn motion changes from individual timestamps
independently, making it challenging to reconstruct complex scenes,
particularly when dealing with violent movement, extreme-shaped geometries, or
reflective surfaces. To address the above issue, we design a plug-and-play
module called TimeFormer to enable existing deformable 3D Gaussians
reconstruction methods with the ability to implicitly model motion patterns
from a learning perspective. Specifically, TimeFormer includes a Cross-Temporal
Transformer Encoder, which adaptively learns the temporal relationships of
deformable 3D Gaussians. Furthermore, we propose a two-stream optimization
strategy that transfers the motion knowledge learned from TimeFormer to the
base stream during the training phase. This allows us to remove TimeFormer
during inference, thereby preserving the original rendering speed. Extensive
experiments in the multi-view and monocular dynamic scenes validate qualitative
and quantitative improvement brought by TimeFormer. Project Page:
https://patrickddj.github.io/TimeFormer/";DaDong Jiang<author:sep>Zhihui Ke<author:sep>Xiaobo Zhou<author:sep>Zhi Hou<author:sep>Xianghui Yang<author:sep>Wenbo Hu<author:sep>Tie Qiu<author:sep>Chunchao Guo;http://arxiv.org/pdf/2411.11941v1;cs.CV;;gaussian splatting
2411.11363v1;http://arxiv.org/abs/2411.11363v1;2024-11-18;GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for  Real-Time Human-Scene Rendering from Sparse Views;"Differentiable rendering techniques have recently shown promising results for
free-viewpoint video synthesis of characters. However, such methods, either
Gaussian Splatting or neural implicit rendering, typically necessitate
per-subject optimization which does not meet the requirement of real-time
rendering in an interactive application. We propose a generalizable Gaussian
Splatting approach for high-resolution image rendering under a sparse-view
camera setting. To this end, we introduce Gaussian parameter maps defined on
the source views and directly regress Gaussian properties for instant novel
view synthesis without any fine-tuning or optimization. We train our Gaussian
parameter regression module on human-only data or human-scene data, jointly
with a depth estimation module to lift 2D parameter maps to 3D space. The
proposed framework is fully differentiable with both depth and rendering
supervision or with only rendering supervision. We further introduce a
regularization term and an epipolar attention mechanism to preserve geometry
consistency between two source views, especially when neglecting depth
supervision. Experiments on several datasets demonstrate that our method
outperforms state-of-the-art methods while achieving an exceeding rendering
speed.";Boyao Zhou<author:sep>Shunyuan Zheng<author:sep>Hanzhang Tu<author:sep>Ruizhi Shao<author:sep>Boning Liu<author:sep>Shengping Zhang<author:sep>Liqiang Nie<author:sep>Yebin Liu;http://arxiv.org/pdf/2411.11363v1;cs.CV;"Journal extension of CVPR 2024,Project
  page:https://yaourtb.github.io/GPS-Gaussian+";gaussian splatting
2411.11839v1;http://arxiv.org/abs/2411.11839v1;2024-11-18;RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator;"Efficient acquisition of real-world embodied data has been increasingly
critical. However, large-scale demonstrations captured by remote operation tend
to take extremely high costs and fail to scale up the data size in an efficient
manner. Sampling the episodes under a simulated environment is a promising way
for large-scale collection while existing simulators fail to high-fidelity
modeling on texture and physics. To address these limitations, we introduce the
RoboGSim, a real2sim2real robotic simulator, powered by 3D Gaussian Splatting
and the physics engine. RoboGSim mainly includes four parts: Gaussian
Reconstructor, Digital Twins Builder, Scene Composer, and Interactive Engine.
It can synthesize the simulated data with novel views, objects, trajectories,
and scenes. RoboGSim also provides an online, reproducible, and safe evaluation
for different manipulation policies. The real2sim and sim2real transfer
experiments show a high consistency in the texture and physics. Moreover, the
effectiveness of synthetic data is validated under the real-world manipulated
tasks. We hope RoboGSim serves as a closed-loop simulator for fair comparison
on policy learning. More information can be found on our project page
https://robogsim.github.io/ .";Xinhai Li<author:sep>Jialin Li<author:sep>Ziheng Zhang<author:sep>Rui Zhang<author:sep>Fan Jia<author:sep>Tiancai Wang<author:sep>Haoqiang Fan<author:sep>Kuo-Kun Tseng<author:sep>Ruiping Wang;http://arxiv.org/pdf/2411.11839v1;cs.RO;;gaussian splatting
2411.11374v1;http://arxiv.org/abs/2411.11374v1;2024-11-18;LeC$^2$O-NeRF: Learning Continuous and Compact Large-Scale Occupancy for  Urban Scenes;"In NeRF, a critical problem is to effectively estimate the occupancy to guide
empty-space skipping and point sampling. Grid-based methods work well for
small-scale scenes. However, on large-scale scenes, they are limited by
predefined bounding boxes, grid resolutions, and high memory usage for grid
updates, and thus struggle to speed up training for large-scale, irregularly
bounded and complex urban scenes without sacrificing accuracy. In this paper,
we propose to learn a continuous and compact large-scale occupancy network,
which can classify 3D points as occupied or unoccupied points. We train this
occupancy network end-to-end together with the radiance field in a
self-supervised manner by three designs. First, we propose a novel imbalanced
occupancy loss to regularize the occupancy network. It makes the occupancy
network effectively control the ratio of unoccupied and occupied points,
motivated by the prior that most of 3D scene points are unoccupied. Second, we
design an imbalanced architecture containing a large scene network and a small
empty space network to separately encode occupied and unoccupied points
classified by the occupancy network. This imbalanced structure can effectively
model the imbalanced nature of occupied and unoccupied regions. Third, we
design an explicit density loss to guide the occupancy network, making the
density of unoccupied points smaller. As far as we know, we are the first to
learn a continuous and compact occupancy of large-scale NeRF by a network. In
our experiments, our occupancy network can quickly learn more compact, accurate
and smooth occupancy compared to the occupancy grid. With our learned occupancy
as guidance for empty space skipping on challenging large-scale benchmarks, our
method consistently obtains higher accuracy compared to the occupancy grid, and
our method can speed up state-of-the-art NeRF methods without sacrificing
accuracy.";Zhenxing Mi<author:sep>Dan Xu;http://arxiv.org/pdf/2411.11374v1;cs.CV;13 pages;nerf
2411.12089v2;http://arxiv.org/abs/2411.12089v2;2024-11-18;FruitNinja: 3D Object Interior Texture Generation with Gaussian  Splatting;"In the real world, objects reveal internal textures when sliced or cut, yet
this behavior is not well-studied in 3D generation tasks today. For example,
slicing a virtual 3D watermelon should reveal flesh and seeds. Given that no
available dataset captures an object's full internal structure and collecting
data from all slices is impractical, generative methods become the obvious
approach. However, current 3D generation and inpainting methods often focus on
visible appearance and overlook internal textures. To bridge this gap, we
introduce FruitNinja, the first method to generate internal textures for 3D
objects undergoing geometric and topological changes. Our approach produces
objects via 3D Gaussian Splatting (3DGS) with both surface and interior
textures synthesized, enabling real-time slicing and rendering without
additional optimization. FruitNinja leverages a pre-trained diffusion model to
progressively inpaint cross-sectional views and applies voxel-grid-based
smoothing to achieve cohesive textures throughout the object. Our OpaqueAtom GS
strategy overcomes 3DGS limitations by employing densely distributed opaque
Gaussians, avoiding biases toward larger particles that destabilize training
and sharp color transitions for fine-grained textures. Experimental results
show that FruitNinja substantially outperforms existing approaches, showcasing
unmatched visual quality in real-time rendered internal views across arbitrary
geometry manipulations.";Fangyu Wu<author:sep>Yuhao Chen;http://arxiv.org/pdf/2411.12089v2;cs.CV;;gaussian splatting
2411.11921v1;http://arxiv.org/abs/2411.11921v1;2024-11-18;DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and  Surface Reconstruction for Urban Driving Scenes;"We present DeSiRe-GS, a self-supervised gaussian splatting representation,
enabling effective static-dynamic decomposition and high-fidelity surface
reconstruction in complex driving scenarios. Our approach employs a two-stage
optimization pipeline of dynamic street Gaussians. In the first stage, we
extract 2D motion masks based on the observation that 3D Gaussian Splatting
inherently can reconstruct only the static regions in dynamic environments.
These extracted 2D motion priors are then mapped into the Gaussian space in a
differentiable manner, leveraging an efficient formulation of dynamic Gaussians
in the second stage. Combined with the introduced geometric regularizations,
our method are able to address the over-fitting issues caused by data sparsity
in autonomous driving, reconstructing physically plausible Gaussians that align
with object surfaces rather than floating in air. Furthermore, we introduce
temporal cross-view consistency to ensure coherence across time and viewpoints,
resulting in high-quality surface reconstruction. Comprehensive experiments
demonstrate the efficiency and effectiveness of DeSiRe-GS, surpassing prior
self-supervised arts and achieving accuracy comparable to methods relying on
external 3D bounding box annotations. Code is available at
\url{https://github.com/chengweialan/DeSiRe-GS}";Chensheng Peng<author:sep>Chengwei Zhang<author:sep>Yixiao Wang<author:sep>Chenfeng Xu<author:sep>Yichen Xie<author:sep>Wenzhao Zheng<author:sep>Kurt Keutzer<author:sep>Masayoshi Tomizuka<author:sep>Wei Zhan;http://arxiv.org/pdf/2411.11921v1;cs.CV;;gaussian splatting
2411.11691v1;http://arxiv.org/abs/2411.11691v1;2024-11-18;Towards Degradation-Robust Reconstruction in Generalizable NeRF;"Generalizable Neural Radiance Field (GNeRF) across scenes has been proven to
be an effective way to avoid per-scene optimization by representing a scene
with deep image features of source images. However, despite its potential for
real-world applications, there has been limited research on the robustness of
GNeRFs to different types of degradation present in the source images. The lack
of such research is primarily attributed to the absence of a large-scale
dataset fit for training a degradation-robust generalizable NeRF model. To
address this gap and facilitate investigations into the degradation robustness
of 3D reconstruction tasks, we construct the Objaverse Blur Dataset, comprising
50,000 images from over 1000 settings featuring multiple levels of blur
degradation. In addition, we design a simple and model-agnostic module for
enhancing the degradation robustness of GNeRFs. Specifically, by extracting
3D-aware features through a lightweight depth estimator and denoiser, the
proposed module shows improvement on different popular methods in GNeRFs in
terms of both quantitative and visual quality over varying degradation types
and levels. Our dataset and code will be made publicly available.";Chan Ho Park<author:sep>Ka Leong Cheng<author:sep>Zhicheng Wang<author:sep>Qifeng Chen;http://arxiv.org/pdf/2411.11691v1;cs.CV;;nerf
2411.11024v1;http://arxiv.org/abs/2411.11024v1;2024-11-17;VeGaS: Video Gaussian Splatting;"Implicit Neural Representations (INRs) employ neural networks to approximate
discrete data as continuous functions. In the context of video data, such
models can be utilized to transform the coordinates of pixel locations along
with frame occurrence times (or indices) into RGB color values. Although INRs
facilitate effective compression, they are unsuitable for editing purposes. One
potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such
as the Video Gaussian Representation (VGR), which is capable of encoding video
as a multitude of 3D Gaussians and is applicable for numerous video processing
operations, including editing. Nevertheless, in this case, the capacity for
modification is constrained to a limited set of basic transformations. To
address this issue, we introduce the Video Gaussian Splatting (VeGaS) model,
which enables realistic modifications of video data. To construct VeGaS, we
propose a novel family of Folded-Gaussian distributions designed to capture
nonlinear dynamics in a video stream and model consecutive frames by 2D
Gaussians obtained as respective conditional distributions. Our experiments
demonstrate that VeGaS outperforms state-of-the-art solutions in frame
reconstruction tasks and allows realistic modifications of video data. The code
is available at: https://github.com/gmum/VeGaS.";Weronika Smolak-Dyżewska<author:sep>Dawid Malarz<author:sep>Kornel Howil<author:sep>Jan Kaczmarczyk<author:sep>Marcin Mazur<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2411.11024v1;cs.CV;;gaussian splatting
2411.10947v1;http://arxiv.org/abs/2411.10947v1;2024-11-17;Direct and Explicit 3D Generation from a Single Image;"Current image-to-3D approaches suffer from high computational costs and lack
scalability for high-resolution outputs. In contrast, we introduce a novel
framework to directly generate explicit surface geometry and texture using
multi-view 2D depth and RGB images along with 3D Gaussian features using a
repurposed Stable Diffusion model. We introduce a depth branch into U-Net for
efficient and high quality multi-view, cross-domain generation and incorporate
epipolar attention into the latent-to-pixel decoder for pixel-level multi-view
consistency. By back-projecting the generated depth pixels into 3D space, we
create a structured 3D representation that can be either rendered via Gaussian
splatting or extracted to high-quality meshes, thereby leveraging additional
novel view synthesis loss to further improve our performance. Extensive
experiments demonstrate that our method surpasses existing baselines in
geometry and texture quality while achieving significantly faster generation
time.";Haoyu Wu<author:sep>Meher Gitika Karumuri<author:sep>Chuhang Zou<author:sep>Seungbae Bang<author:sep>Yuelong Li<author:sep>Dimitris Samaras<author:sep>Sunil Hadap;http://arxiv.org/pdf/2411.10947v1;cs.CV;3DV 2025, Project page: https://hao-yu-wu.github.io/gen3d/;
2411.10722v1;http://arxiv.org/abs/2411.10722v1;2024-11-16;DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment;"We introduce Dynamic Gaussian Splatting SLAM (DGS-SLAM), the first dynamic
SLAM framework built on the foundation of Gaussian Splatting. While recent
advancements in dense SLAM have leveraged Gaussian Splatting to enhance scene
representation, most approaches assume a static environment, making them
vulnerable to photometric and geometric inconsistencies caused by dynamic
objects. To address these challenges, we integrate Gaussian Splatting SLAM with
a robust filtering process to handle dynamic objects throughout the entire
pipeline, including Gaussian insertion and keyframe selection. Within this
framework, to further improve the accuracy of dynamic object removal, we
introduce a robust mask generation method that enforces photometric consistency
across keyframes, reducing noise from inaccurate segmentation and artifacts
such as shadows. Additionally, we propose the loop-aware window selection
mechanism, which utilizes unique keyframe IDs of 3D Gaussians to detect loops
between the current and past frames, facilitating joint optimization of the
current camera poses and the Gaussian map. DGS-SLAM achieves state-of-the-art
performance in both camera tracking and novel view synthesis on various dynamic
SLAM benchmarks, proving its effectiveness in handling real-world dynamic
scenes.";Mangyu Kong<author:sep>Jaewon Lee<author:sep>Seongwon Lee<author:sep>Euntai Kim;http://arxiv.org/pdf/2411.10722v1;cs.RO;Preprint, Under review;gaussian splatting
2411.10033v1;http://arxiv.org/abs/2411.10033v1;2024-11-15;GSEditPro: 3D Gaussian Splatting Editing with Attention-based  Progressive Localization;"With the emergence of large-scale Text-to-Image(T2I) models and implicit 3D
representations like Neural Radiance Fields (NeRF), many text-driven generative
editing methods based on NeRF have appeared. However, the implicit encoding of
geometric and textural information poses challenges in accurately locating and
controlling objects during editing. Recently, significant advancements have
been made in the editing methods of 3D Gaussian Splatting, a real-time
rendering technology that relies on explicit representation. However, these
methods still suffer from issues including inaccurate localization and limited
manipulation over editing. To tackle these challenges, we propose GSEditPro, a
novel 3D scene editing framework which allows users to perform various creative
and precise editing using text prompts only. Leveraging the explicit nature of
the 3D Gaussian distribution, we introduce an attention-based progressive
localization module to add semantic labels to each Gaussian during rendering.
This enables precise localization on editing areas by classifying Gaussians
based on their relevance to the editing prompts derived from cross-attention
layers of the T2I model. Furthermore, we present an innovative editing
optimization method based on 3D Gaussian Splatting, obtaining stable and
refined editing results through the guidance of Score Distillation Sampling and
pseudo ground truth. We prove the efficacy of our method through extensive
experiments.";Yanhao Sun<author:sep>RunZe Tian<author:sep>Xiao Han<author:sep>XinYao Liu<author:sep>Yan Zhang<author:sep>Kai Xu;http://arxiv.org/pdf/2411.10033v1;cs.CV;Pacific Graphics 2024;gaussian splatting<tag:sep>nerf
2411.12592v1;http://arxiv.org/abs/2411.12592v1;2024-11-15;SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D  Reconstruction;"Recent efforts in Gaussian-Splat-based Novel View Synthesis can achieve
photorealistic rendering; however, such capability is limited in sparse-view
scenarios due to sparse initialization and over-fitting floaters. Recent
progress in depth estimation and alignment can provide dense point cloud with
few views; however, the resulting pose accuracy is suboptimal. In this work, we
present SPARS3R, which combines the advantages of accurate pose estimation from
Structure-from-Motion and dense point cloud from depth estimation. To this end,
SPARS3R first performs a Global Fusion Alignment process that maps a prior
dense point cloud to a sparse point cloud from Structure-from-Motion based on
triangulated correspondences. RANSAC is applied during this process to
distinguish inliers and outliers. SPARS3R then performs a second, Semantic
Outlier Alignment step, which extracts semantically coherent regions around the
outliers and performs local alignment in these regions. Along with several
improvements in the evaluation process, we demonstrate that SPARS3R can achieve
photorealistic rendering with sparse images and significantly outperforms
existing approaches.";Yutao Tang<author:sep>Yuxiang Guo<author:sep>Deming Li<author:sep>Cheng Peng;http://arxiv.org/pdf/2411.12592v1;cs.CV;;
2411.10546v1;http://arxiv.org/abs/2411.10546v1;2024-11-15;The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual  Localisation, Reconstruction and Radiance Field Methods;"This paper introduces a large-scale multi-modal dataset captured in and
around well-known landmarks in Oxford using a custom-built multi-sensor
perception unit as well as a millimetre-accurate map from a Terrestrial LiDAR
Scanner (TLS). The perception unit includes three synchronised global shutter
colour cameras, an automotive 3D LiDAR scanner, and an inertial sensor - all
precisely calibrated. We also establish benchmarks for tasks involving
localisation, reconstruction, and novel-view synthesis, which enable the
evaluation of Simultaneous Localisation and Mapping (SLAM) methods,
Structure-from-Motion (SfM) and Multi-view Stereo (MVS) methods as well as
radiance field methods such as Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting. To evaluate 3D reconstruction the TLS 3D models are used as ground
truth. Localisation ground truth is computed by registering the mobile LiDAR
scans to the TLS 3D models. Radiance field methods are evaluated not only with
poses sampled from the input trajectory, but also from viewpoints that are from
trajectories which are distant from the training poses. Our evaluation
demonstrates a key limitation of state-of-the-art radiance field methods: we
show that they tend to overfit to the training poses/images and do not
generalise well to out-of-sequence poses. They also underperform in 3D
reconstruction compared to MVS systems using the same visual inputs. Our
dataset and benchmarks are intended to facilitate better integration of
radiance field methods and SLAM systems. The raw and processed data, along with
software for parsing and evaluation, can be accessed at
https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/.";Yifu Tao<author:sep>Miguel Ángel Muñoz-Bañón<author:sep>Lintong Zhang<author:sep>Jiahao Wang<author:sep>Lanke Frank Tarimo Fu<author:sep>Maurice Fallon;http://arxiv.org/pdf/2411.10546v1;cs.CV;Website: https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/;nerf
2411.10133v1;http://arxiv.org/abs/2411.10133v1;2024-11-15;Efficient Density Control for 3D Gaussian Splatting;"3D Gaussian Splatting (3DGS) excels in novel view synthesis, balancing
advanced rendering quality with real-time performance. However, in trained
scenes, a large number of Gaussians with low opacity significantly increase
rendering costs. This issue arises due to flaws in the split and clone
operations during the densification process, which lead to extensive Gaussian
overlap and subsequent opacity reduction. To enhance the efficiency of Gaussian
utilization, we improve the adaptive density control of 3DGS. First, we
introduce a more efficient long-axis split operation to replace the original
clone and split, which mitigates Gaussian overlap and improves densification
efficiency.Second, we propose a simple adaptive pruning technique to reduce the
number of low-opacity Gaussians. Finally, by dynamically lowering the splitting
threshold and applying importance weighting, the efficiency of Gaussian
utilization is further improved.We evaluate our proposed method on various
challenging real-world datasets. Experimental results show that our Efficient
Density Control (EDC) can enhance both the rendering speed and quality.";Xiaobin Deng<author:sep>Changyu Diao<author:sep>Min Li<author:sep>Ruohan Yu<author:sep>Duanqing Xu;http://arxiv.org/pdf/2411.10133v1;cs.CV;;gaussian splatting
2411.09952v1;http://arxiv.org/abs/2411.09952v1;2024-11-15;GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars  from Monocular Video;"Avatar modelling has broad applications in human animation and virtual
try-ons. Recent advancements in this field have focused on high-quality and
comprehensive human reconstruction but often overlook the separation of
clothing from the body. To bridge this gap, this paper introduces GGAvatar
(Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular
videos. Through advanced parameterized templates and unique phased training,
this model effectively achieves decoupled, editable, and realistic
reconstruction of clothed humans. Comparative evaluations with other costly
models confirm GGAvatar's superior quality and efficiency in modelling both
clothed humans and separable garments. The paper also showcases applications in
clothing editing, as illustrated in Figure 1, highlighting the model's benefits
and the advantages of effective disentanglement. The code is available at
https://github.com/J-X-Chen/GGAvatar/.";Jingxuan Chen;http://arxiv.org/pdf/2411.09952v1;cs.CV;MMAsia'24 Accepted;gaussian splatting
2411.10504v1;http://arxiv.org/abs/2411.10504v1;2024-11-15;USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction  and Gaussian Splatting;"Spike cameras, as an innovative neuromorphic camera that captures scenes with
the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D
reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting
(3DGS). Previous spike-based 3D reconstruction approaches often employ a
casecased pipeline: starting with high-quality image reconstruction from spike
streams based on established spike-to-image reconstruction algorithms, then
progressing to camera pose estimation and 3D reconstruction. However, this
cascaded approach suffers from substantial cumulative errors, where quality
limitations of initial image reconstructions negatively impact pose estimation,
ultimately degrading the fidelity of the 3D reconstruction. To address these
issues, we propose a synergistic optimization framework, \textbf{USP-Gaussian},
that unifies spike-based image reconstruction, pose correction, and Gaussian
splatting into an end-to-end framework. Leveraging the multi-view consistency
afforded by 3DGS and the motion capture capability of the spike camera, our
framework enables a joint iterative optimization that seamlessly integrates
information between the spike-to-image network and 3DGS. Experiments on
synthetic datasets with accurate poses demonstrate that our method surpasses
previous approaches by effectively eliminating cascading errors. Moreover, we
integrate pose optimization to achieve robust 3D reconstruction in real-world
scenarios with inaccurate initial poses, outperforming alternative methods by
effectively reducing noise and preserving fine texture details. Our code, data
and trained models will be available at
\url{https://github.com/chenkang455/USP-Gaussian}.";Kang Chen<author:sep>Jiyuan Zhang<author:sep>Zecheng Hao<author:sep>Yajing Zheng<author:sep>Tiejun Huang<author:sep>Zhaofei Yu;http://arxiv.org/pdf/2411.10504v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.09749v1;http://arxiv.org/abs/2411.09749v1;2024-11-14;Adversarial Attacks Using Differentiable Rendering: A Survey;"Differentiable rendering methods have emerged as a promising means for
generating photo-realistic and physically plausible adversarial attacks by
manipulating 3D objects and scenes that can deceive deep neural networks
(DNNs). Recently, differentiable rendering capabilities have evolved
significantly into a diverse landscape of libraries, such as Mitsuba,
PyTorch3D, and methods like Neural Radiance Fields and 3D Gaussian Splatting
for solving inverse rendering problems that share conceptually similar
properties commonly used to attack DNNs, such as back-propagation and
optimization. However, the adversarial machine learning research community has
not yet fully explored or understood such capabilities for generating attacks.
Some key reasons are that researchers often have different attack goals, such
as misclassification or misdetection, and use different tasks to accomplish
these goals by manipulating different representation in a scene, such as the
mesh or texture of an object. This survey adopts a task-oriented unifying
framework that systematically summarizes common tasks, such as manipulating
textures, altering illumination, and modifying 3D meshes to exploit
vulnerabilities in DNNs. Our framework enables easy comparison of existing
works, reveals research gaps and spotlights exciting future research directions
in this rapidly evolving field. Through focusing on how these tasks enable
attacks on various DNNs such as image classification, facial recognition,
object detection, optical flow and depth estimation, our survey helps
researchers and practitioners better understand the vulnerabilities of computer
vision systems against photorealistic adversarial attacks that could threaten
real-world applications.";Matthew Hull<author:sep>Chao Zhang<author:sep>Zsolt Kira<author:sep>Duen Horng Chau;http://arxiv.org/pdf/2411.09749v1;cs.LG;;gaussian splatting
2411.09156v1;http://arxiv.org/abs/2411.09156v1;2024-11-14;DyGASR: Dynamic Generalized Exponential Splatting with Surface Alignment  for Accelerated 3D Mesh Reconstruction;"Recent advancements in 3D Gaussian Splatting (3DGS), which lead to
high-quality novel view synthesis and accelerated rendering, have remarkably
improved the quality of radiance field reconstruction. However, the extraction
of mesh from a massive number of minute 3D Gaussian points remains great
challenge due to the large volume of Gaussians and difficulty of representation
of sharp signals caused by their inherent low-pass characteristics. To address
this issue, we propose DyGASR, which utilizes generalized exponential function
instead of traditional 3D Gaussian to decrease the number of particles and
dynamically optimize the representation of the captured signal. In addition, it
is observed that reconstructing mesh with Generalized Exponential
Splatting(GES) without modifications frequently leads to failures since the
generalized exponential distribution centroids may not precisely align with the
scene surface. To overcome this, we adopt Sugar's approach and introduce
Generalized Surface Regularization (GSR), which reduces the smallest scaling
vector of each point cloud to zero and ensures normal alignment perpendicular
to the surface, facilitating subsequent Poisson surface mesh reconstruction.
Additionally, we propose a dynamic resolution adjustment strategy that utilizes
a cosine schedule to gradually increase image resolution from low to high
during the training stage, thus avoiding constant full resolution, which
significantly boosts the reconstruction speed. Our approach surpasses existing
3DGS-based mesh reconstruction methods, as evidenced by extensive evaluations
on various scene datasets, demonstrating a 25\% increase in speed, and a 30\%
reduction in memory usage.";Shengchao Zhao<author:sep>Yundong Li;http://arxiv.org/pdf/2411.09156v1;cs.CV;;gaussian splatting
2411.08879v1;http://arxiv.org/abs/2411.08879v1;2024-11-13;4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization;"Novel view synthesis of dynamic scenes is becoming important in various
applications, including augmented and virtual reality. We propose a novel 4D
Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded
monocular videos. To overcome the overfitting problem of existing work for
these real-world videos, we introduce an uncertainty-aware regularization that
identifies uncertain regions with few observations and selectively imposes
additional priors based on diffusion models and depth smoothness on such
regions. This approach improves both the performance of novel view synthesis
and the quality of training image reconstruction. We also identify the
initialization problem of 4DGS in fast-moving dynamic regions, where the
Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks.
To initialize Gaussian primitives in such regions, we present a dynamic region
densification method using the estimated depth maps and scene flow. Our
experiments show that the proposed method improves the performance of 4DGS
reconstruction from a video captured by a handheld monocular camera and also
exhibits promising results in few-shot static scene reconstruction.";Mijeong Kim<author:sep>Jongwoo Lim<author:sep>Bohyung Han;http://arxiv.org/pdf/2411.08879v1;cs.CV;NeurIPS 2024;gaussian splatting
2411.08279v1;http://arxiv.org/abs/2411.08279v1;2024-11-13;MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields  Representation;"Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and
3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in
Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering,
particularly when using high-quality video sequences as input. However,
existing methods struggle with motion-blurred frames, which are common in
real-world scenarios like low-light or long-exposure conditions. This often
results in a significant reduction in both camera localization accuracy and map
reconstruction quality. To address this challenge, we propose a dense visual
SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our
approach integrates an efficient motion blur-aware tracker with either neural
radiance fields or Gaussian Splatting based mapper. By accurately modeling the
physical image formation process of motion-blurred images, our method
simultaneously learns 3D scene representation and estimates the cameras' local
trajectory during exposure time, enabling proactive compensation for motion
blur caused by camera movement. In our experiments, we demonstrate that
MBA-SLAM surpasses previous state-of-the-art methods in both camera
localization and map reconstruction, showcasing superior performance across a
range of datasets, including synthetic and real datasets featuring sharp images
as well as those affected by motion blur, highlighting the versatility and
robustness of our approach. Code is available at
https://github.com/WU-CVGL/MBA-SLAM.";Peng Wang<author:sep>Lingzhe Zhao<author:sep>Yin Zhang<author:sep>Shiyu Zhao<author:sep>Peidong Liu;http://arxiv.org/pdf/2411.08279v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.08373v1;http://arxiv.org/abs/2411.08373v1;2024-11-13;DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose  Optimization;"Achieving robust and precise pose estimation in dynamic scenes is a
significant research challenge in Visual Simultaneous Localization and Mapping
(SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems
have proven effective in creating high-quality renderings using explicit 3D
Gaussian models, significantly improving environmental reconstruction fidelity.
However, these approaches depend on a static environment assumption and face
challenges in dynamic environments due to inconsistent observations of geometry
and photometry. To address this problem, we propose DG-SLAM, the first robust
dynamic visual SLAM system grounded in 3D Gaussians, which provides precise
camera pose estimation alongside high-fidelity reconstructions. Specifically,
we propose effective strategies, including motion mask generation, adaptive
Gaussian point management, and a hybrid camera tracking algorithm to improve
the accuracy and robustness of pose estimation. Extensive experiments
demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose
estimation, map reconstruction, and novel-view synthesis in dynamic scenes,
outperforming existing methods meanwhile preserving real-time rendering
ability.";Yueming Xu<author:sep>Haochen Jiang<author:sep>Zhongyang Xiao<author:sep>Jianfeng Feng<author:sep>Li Zhang;http://arxiv.org/pdf/2411.08373v1;cs.RO;;gaussian splatting
2411.08508v1;http://arxiv.org/abs/2411.08508v1;2024-11-13;BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel  View Synthesis;"We present billboard Splatting (BBSplat) - a novel approach for 3D scene
representation based on textured geometric primitives. BBSplat represents the
scene as a set of optimizable textured planar primitives with learnable RGB
textures and alpha-maps to control their shape. BBSplat primitives can be used
in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our
method's qualitative and quantitative improvements over 3D and 2D Gaussians are
most noticeable when fewer primitives are used, when BBSplat achieves over 1200
FPS. Our novel regularization term encourages textures to have a sparser
structure, unlocking an efficient compression that leads to a reduction in
storage space of the model. Our experiments show the efficiency of BBSplat on
standard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,
and Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics
compared to the state-of-the-art, especially for the case when fewer primitives
are used, which, on the other hand, leads to up to 2 times inference speed
improvement for the same rendering quality.";David Svitov<author:sep>Pietro Morerio<author:sep>Lourdes Agapito<author:sep>Alessio Del Bue;http://arxiv.org/pdf/2411.08508v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.08453v1;http://arxiv.org/abs/2411.08453v1;2024-11-13;Biomass phenotyping of oilseed rape through UAV multi-view oblique  imaging with 3DGS and SAM model;"Biomass estimation of oilseed rape is crucial for optimizing crop
productivity and breeding strategies. While UAV-based imaging has advanced
high-throughput phenotyping, current methods often rely on orthophoto images,
which struggle with overlapping leaves and incomplete structural information in
complex field environments. This study integrates 3D Gaussian Splatting (3DGS)
with the Segment Anything Model (SAM) for precise 3D reconstruction and biomass
estimation of oilseed rape. UAV multi-view oblique images from 36 angles were
used to perform 3D reconstruction, with the SAM module enhancing point cloud
segmentation. The segmented point clouds were then converted into point cloud
volumes, which were fitted to ground-measured biomass using linear regression.
The results showed that 3DGS (7k and 30k iterations) provided high accuracy,
with peak signal-to-noise ratios (PSNR) of 27.43 and 29.53 and training times
of 7 and 49 minutes, respectively. This performance exceeded that of structure
from motion (SfM) and mipmap Neural Radiance Fields (Mip-NeRF), demonstrating
superior efficiency. The SAM module achieved high segmentation accuracy, with a
mean intersection over union (mIoU) of 0.961 and an F1-score of 0.980.
Additionally, a comparison of biomass extraction models found the point cloud
volume model to be the most accurate, with an determination coefficient (R2) of
0.976, root mean square error (RMSE) of 2.92 g/plant, and mean absolute
percentage error (MAPE) of 6.81%, outperforming both the plot crop volume and
individual crop volume models. This study highlights the potential of combining
3DGS with multi-view UAV imaging for improved biomass phenotyping.";Yutao Shen<author:sep>Hongyu Zhou<author:sep>Xin Yang<author:sep>Xuqi Lu<author:sep>Ziyue Guo<author:sep>Lixi Jiang<author:sep>Yong He<author:sep>Haiyan Cen;http://arxiv.org/pdf/2411.08453v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.08642v1;http://arxiv.org/abs/2411.08642v1;2024-11-13;Towards More Accurate Fake Detection on Images Generated from Advanced  Generative and Neural Rendering Models;"The remarkable progress in neural-network-driven visual data generation,
especially with neural rendering techniques like Neural Radiance Fields and 3D
Gaussian splatting, offers a powerful alternative to GANs and diffusion models.
These methods can produce high-fidelity images and lifelike avatars,
highlighting the need for robust detection methods. In response, an
unsupervised training technique is proposed that enables the model to extract
comprehensive features from the Fourier spectrum magnitude, thereby overcoming
the challenges of reconstructing the spectrum due to its centrosymmetric
properties. By leveraging the spectral domain and dynamically combining it with
spatial domain information, we create a robust multimodal detector that
demonstrates superior generalization capabilities in identifying challenging
synthetic images generated by the latest image synthesis techniques. To address
the absence of a 3D neural rendering-based fake image database, we develop a
comprehensive database that includes images generated by diverse neural
rendering techniques, providing a robust foundation for evaluating and
advancing detection methods.";Chengdong Dong<author:sep>Vijayakumar Bhagavatula<author:sep>Zhenyu Zhou<author:sep>Ajay Kumar;http://arxiv.org/pdf/2411.08642v1;cs.CV;13 pages, 8 Figures;gaussian splatting
2411.07541v1;http://arxiv.org/abs/2411.07541v1;2024-11-12;HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D  Gaussian Splatting;"The online reconstruction of dynamic scenes from multi-view streaming videos
faces significant challenges in training, rendering and storage efficiency.
Harnessing superior learning speed and real-time rendering capabilities, 3D
Gaussian Splatting (3DGS) has recently demonstrated considerable potential in
this field. However, 3DGS can be inefficient in terms of storage and prone to
overfitting by excessively growing Gaussians, particularly with limited views.
This paper proposes an efficient framework, dubbed HiCoM, with three key
components. First, we construct a compact and robust initial 3DGS
representation using a perturbation smoothing strategy. Next, we introduce a
Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform
distribution and local consistency of 3D Gaussians to swiftly and accurately
learn motions across frames. Finally, we continually refine the 3DGS with
additional Gaussians, which are later merged into the initial 3DGS to maintain
consistency with the evolving scene. To preserve a compact representation, an
equivalent number of low-opacity Gaussians that minimally impact the
representation are removed before processing subsequent frames. Extensive
experiments conducted on two widely used datasets show that our framework
improves learning efficiency of the state-of-the-art methods by about $20\%$
and reduces the data storage by $85\%$, achieving competitive free-viewpoint
video synthesis quality but with higher robustness and stability. Moreover, by
parallel learning multiple frames simultaneously, our HiCoM decreases the
average training wall time to $<2$ seconds per frame with negligible
performance degradation, substantially boosting real-world applicability and
responsiveness.";Qiankun Gao<author:sep>Jiarui Meng<author:sep>Chengxiang Wen<author:sep>Jie Chen<author:sep>Jian Zhang;http://arxiv.org/pdf/2411.07541v1;cs.CV;"Accepted to NeurIPS 2024; Code is avaliable at
  https://github.com/gqk/HiCoM";gaussian splatting
2411.08037v1;http://arxiv.org/abs/2411.08037v1;2024-11-12;Material Transforms from Disentangled NeRF Representations;"In this paper, we first propose a novel method for transferring material
transformations across different scenes. Building on disentangled Neural
Radiance Field (NeRF) representations, our approach learns to map Bidirectional
Reflectance Distribution Functions (BRDF) from pairs of scenes observed in
varying conditions, such as dry and wet. The learned transformations can then
be applied to unseen scenes with similar materials, therefore effectively
rendering the transformation learned with an arbitrary level of intensity.
Extensive experiments on synthetic scenes and real-world objects validate the
effectiveness of our approach, showing that it can learn various
transformations such as wetness, painting, coating, etc. Our results highlight
not only the versatility of our method but also its potential for practical
applications in computer graphics. We publish our method implementation, along
with our synthetic/real datasets on
https://github.com/astra-vision/BRDFTransform";Ivan Lopes<author:sep>Jean-François Lalonde<author:sep>Raoul de Charette;http://arxiv.org/pdf/2411.08037v1;cs.CV;;nerf
2411.07478v1;http://arxiv.org/abs/2411.07478v1;2024-11-12;GUS-IR: Gaussian Splatting with Unified Shading for Inverse Rendering;"Recovering the intrinsic physical attributes of a scene from images,
generally termed as the inverse rendering problem, has been a central and
challenging task in computer vision and computer graphics. In this paper, we
present GUS-IR, a novel framework designed to address the inverse rendering
problem for complicated scenes featuring rough and glossy surfaces. This paper
starts by analyzing and comparing two prominent shading techniques popularly
used for inverse rendering, forward shading and deferred shading, effectiveness
in handling complex materials. More importantly, we propose a unified shading
solution that combines the advantages of both techniques for better
decomposition. In addition, we analyze the normal modeling in 3D Gaussian
Splatting (3DGS) and utilize the shortest axis as normal for each particle in
GUS-IR, along with a depth-related regularization, resulting in improved
geometric representation and better shape reconstruction. Furthermore, we
enhance the probe-based baking scheme proposed by GS-IR to achieve more
accurate ambient occlusion modeling to better handle indirect illumination.
Extensive experiments have demonstrated the superior performance of GUS-IR in
achieving precise intrinsic decomposition and geometric representation,
supporting many downstream tasks (such as relighting, retouching) in computer
vision, graphics, and extended reality.";Zhihao Liang<author:sep>Hongdong Li<author:sep>Kui Jia<author:sep>Kailing Guo<author:sep>Qi Zhang;http://arxiv.org/pdf/2411.07478v1;cs.CV;15 pages, 11 figures;gaussian splatting
2411.07555v1;http://arxiv.org/abs/2411.07555v1;2024-11-12;GaussianCut: Interactive segmentation via graph cut for 3D Gaussian  Splatting;"We introduce GaussianCut, a new method for interactive multiview segmentation
of scenes represented as 3D Gaussians. Our approach allows for selecting the
objects to be segmented by interacting with a single view. It accepts intuitive
user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian
Splatting (3DGS) as the underlying scene representation simplifies the
extraction of objects of interest which are considered to be a subset of the
scene's Gaussians. Our key idea is to represent the scene as a graph and use
the graph-cut algorithm to minimize an energy function to effectively partition
the Gaussians into foreground and background. To achieve this, we construct a
graph based on scene Gaussians and devise a segmentation-aligned energy
function on the graph to combine user inputs with scene properties. To obtain
an initial coarse segmentation, we leverage 2D image/video segmentation models
and further refine these coarse estimates using our graph construction. Our
empirical evaluations show the adaptability of GaussianCut across a diverse set
of scenes. GaussianCut achieves competitive performance with state-of-the-art
approaches for 3D segmentation without requiring any additional
segmentation-aware training.";Umangi Jain<author:sep>Ashkan Mirzaei<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2411.07555v1;cs.CV;;
2411.08158v1;http://arxiv.org/abs/2411.08158v1;2024-11-12;TomoGRAF: A Robust and Generalizable Reconstruction Network for  Single-View Computed Tomography;"Computed tomography (CT) provides high spatial resolution visualization of 3D
structures for scientific and clinical applications. Traditional
analytical/iterative CT reconstruction algorithms require hundreds of angular
data samplings, a condition that may not be met in practice due to physical and
mechanical limitations. Sparse view CT reconstruction has been proposed using
constrained optimization and machine learning methods with varying success,
less so for ultra-sparse view CT reconstruction with one to two views. Neural
radiance field (NeRF) is a powerful tool for reconstructing and rendering 3D
natural scenes from sparse views, but its direct application to 3D medical
image reconstruction has been minimally successful due to the differences
between optical and X-ray photon transportation. Here, we develop a novel
TomoGRAF framework incorporating the unique X-ray transportation physics to
reconstruct high-quality 3D volumes using ultra-sparse projections without
prior. TomoGRAF captures the CT imaging geometry, simulates the X-ray casting
and tracing process, and penalizes the difference between simulated and ground
truth CT sub-volume during training. We evaluated the performance of TomoGRAF
on an unseen dataset of distinct imaging characteristics from the training data
and demonstrated a vast leap in performance compared with state-of-the-art deep
learning and NeRF methods. TomoGRAF provides the first generalizable solution
for image-guided radiotherapy and interventional radiology applications, where
only one or a few X-ray views are available, but 3D volumetric information is
desired.";Di Xu<author:sep>Yang Yang<author:sep>Hengjie Liu<author:sep>Qihui Lyu<author:sep>Martina Descovich<author:sep>Dan Ruan<author:sep>Ke Sheng;http://arxiv.org/pdf/2411.08158v1;eess.IV;;nerf
2411.07579v3;http://arxiv.org/abs/2411.07579v3;2024-11-12;Projecting Gaussian Ellipsoids While Avoiding Affine Projection  Approximation;"Recently, 3D Gaussian Splatting has dominated novel-view synthesis with its
real-time rendering speed and state-of-the-art rendering quality. However,
during the rendering process, the use of the Jacobian of the affine
approximation of the projection transformation leads to inevitable errors,
resulting in blurriness, artifacts and a lack of scene consistency in the final
rendered images. To address this issue, we introduce an ellipsoid-based
projection method to calculate the projection of Gaussian ellipsoid onto the
image plane, which is the primitive of 3D Gaussian Splatting. As our proposed
ellipsoid-based projection method cannot handle Gaussian ellipsoids with camera
origins inside them or parts lying below $z=0$ plane in the camera space, we
designed a pre-filtering strategy. Experiments over multiple widely adopted
benchmark datasets show that our ellipsoid-based projection method can enhance
the rendering quality of 3D Gaussian Splatting and its extensions.";Han Qi<author:sep>Tao Cai<author:sep>Xiyue Han;http://arxiv.org/pdf/2411.07579v3;cs.CV;;gaussian splatting
2411.06757v1;http://arxiv.org/abs/2411.06757v1;2024-11-11;LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes;"Neural Radiance Fields (NeRFs) have shown remarkable performances in
producing novel-view images from high-quality scene images. However, hand-held
low-light photography challenges NeRFs as the captured images may
simultaneously suffer from low visibility, noise, and camera shakes. While
existing NeRF methods may handle either low light or motion, directly combining
them or incorporating additional image-based enhancement methods does not work
as these degradation factors are highly coupled. We observe that noise in
low-light images is always sharp regardless of camera shakes, which implies an
implicit order of these degradation factors within the image formation process.
To this end, we propose in this paper a novel model, named LuSh-NeRF, which can
reconstruct a clean and sharp NeRF from a group of hand-held low-light images.
The key idea of LuSh-NeRF is to sequentially model noise and blur in the images
via multi-view feature consistency and frequency information of NeRF,
respectively. Specifically, LuSh-NeRF includes a novel Scene-Noise
Decomposition (SND) module for decoupling the noise from the scene
representation and a novel Camera Trajectory Prediction (CTP) module for the
estimation of camera motions based on low-frequency scene information. To
facilitate training and evaluations, we construct a new dataset containing both
synthetic and real images. Experiments show that LuSh-NeRF outperforms existing
approaches. Our code and dataset can be found here:
https://github.com/quzefan/LuSh-NeRF.";Zefan Qu<author:sep>Ke Xu<author:sep>Gerhard Petrus Hancke<author:sep>Rynson W. H. Lau;http://arxiv.org/pdf/2411.06757v1;cs.CV;Accepted by NeurIPS 2024;nerf
2411.06976v1;http://arxiv.org/abs/2411.06976v1;2024-11-11;A Hierarchical Compression Technique for 3D Gaussian Splatting  Compression;"3D Gaussian Splatting (GS) demonstrates excellent rendering quality and
generation speed in novel view synthesis. However, substantial data size poses
challenges for storage and transmission, making 3D GS compression an essential
technology. Current 3D GS compression research primarily focuses on developing
more compact scene representations, such as converting explicit 3D GS data into
implicit forms. In contrast, compression of the GS data itself has hardly been
explored. To address this gap, we propose a Hierarchical GS Compression (HGSC)
technique. Initially, we prune unimportant Gaussians based on importance scores
derived from both global and local significance, effectively reducing
redundancy while maintaining visual quality. An Octree structure is used to
compress 3D positions. Based on the 3D GS Octree, we implement a hierarchical
attribute compression strategy by employing a KD-tree to partition the 3D GS
into multiple blocks. We apply farthest point sampling to select anchor
primitives within each block and others as non-anchor primitives with varying
Levels of Details (LoDs). Anchor primitives serve as reference points for
predicting non-anchor primitives across different LoDs to reduce spatial
redundancy. For anchor primitives, we use the region adaptive hierarchical
transform to achieve near-lossless compression of various attributes. For
non-anchor primitives, each is predicted based on the k-nearest anchor
primitives. To further minimize prediction errors, the reconstructed LoD and
anchor primitives are combined to form new anchor primitives to predict the
next LoD. Our method notably achieves superior compression quality and a
significant data size reduction of over 4.5 times compared to the
state-of-the-art compression method on small scenes datasets.";He Huang<author:sep>Wenjie Huang<author:sep>Qi Yang<author:sep>Yiling Xu<author:sep>Zhu li;http://arxiv.org/pdf/2411.06976v1;cs.CV;;gaussian splatting
2411.06602v1;http://arxiv.org/abs/2411.06602v1;2024-11-10;Adaptive and Temporally Consistent Gaussian Surfels for Multi-view  Dynamic Reconstruction;"3D Gaussian Splatting has recently achieved notable success in novel view
synthesis for dynamic scenes and geometry reconstruction in static scenes.
Building on these advancements, early methods have been developed for dynamic
surface reconstruction by globally optimizing entire sequences. However,
reconstructing dynamic scenes with significant topology changes, emerging or
disappearing objects, and rapid movements remains a substantial challenge,
particularly for long sequences. To address these issues, we propose AT-GS, a
novel method for reconstructing high-quality dynamic surfaces from multi-view
videos through per-frame incremental optimization. To avoid local minima across
frames, we introduce a unified and adaptive gradient-aware densification
strategy that integrates the strengths of conventional cloning and splitting
techniques. Additionally, we reduce temporal jittering in dynamic surfaces by
ensuring consistency in curvature maps across consecutive frames. Our method
achieves superior accuracy and temporal coherence in dynamic surface
reconstruction, delivering high-fidelity space-time novel view synthesis, even
in complex and challenging scenes. Extensive experiments on diverse multi-view
video datasets demonstrate the effectiveness of our approach, showing clear
advantages over baseline methods. Project page:
\url{https://fraunhoferhhi.github.io/AT-GS}";Decai Chen<author:sep>Brianne Oberson<author:sep>Ingo Feldmann<author:sep>Oliver Schreer<author:sep>Anna Hilsmann<author:sep>Peter Eisert;http://arxiv.org/pdf/2411.06602v1;cs.CV;;gaussian splatting
2411.06365v1;http://arxiv.org/abs/2411.06365v1;2024-11-10;Through the Curved Cover: Synthesizing Cover Aberrated Scenes with  Refractive Field;"Recent extended reality headsets and field robots have adopted covers to
protect the front-facing cameras from environmental hazards and falls. The
surface irregularities on the cover can lead to optical aberrations like
blurring and non-parametric distortions. Novel view synthesis methods like NeRF
and 3D Gaussian Splatting are ill-equipped to synthesize from sequences with
optical aberrations. To address this challenge, we introduce SynthCover to
enable novel view synthesis through protective covers for downstream extended
reality applications. SynthCover employs a Refractive Field that estimates the
cover's geometry, enabling precise analytical calculation of refracted rays.
Experiments on synthetic and real-world scenes demonstrate our method's ability
to accurately model scenes viewed through protective covers, achieving a
significant improvement in rendering quality compared to prior methods. We also
show that the model can adjust well to various cover geometries with synthetic
sequences captured with covers of different surface curvatures. To motivate
further studies on this problem, we provide the benchmarked dataset containing
real and synthetic walkable scenes captured with protective cover optical
aberrations.";Liuyue Xie<author:sep>Jiancong Guo<author:sep>Laszlo A. Jeni<author:sep>Zhiheng Jia<author:sep>Mingyang Li<author:sep>Yunwen Zhou<author:sep>Chao Guo;http://arxiv.org/pdf/2411.06365v1;cs.CV;WACV 2025;gaussian splatting<tag:sep>nerf
2411.06390v2;http://arxiv.org/abs/2411.06390v2;2024-11-10;SplatFormer: Point Transformer for Robust 3D Gaussian Splatting;"3D Gaussian Splatting (3DGS) has recently transformed photorealistic
reconstruction, achieving high visual fidelity and real-time performance.
However, rendering quality significantly deteriorates when test views deviate
from the camera angles used during training, posing a major challenge for
applications in immersive free-viewpoint rendering and navigation. In this
work, we conduct a comprehensive evaluation of 3DGS and related novel view
synthesis methods under out-of-distribution (OOD) test camera scenarios. By
creating diverse test cases with synthetic and real-world datasets, we
demonstrate that most existing methods, including those incorporating various
regularization techniques and data-driven priors, struggle to generalize
effectively to OOD views. To address this limitation, we introduce SplatFormer,
the first point transformer model specifically designed to operate on Gaussian
splats. SplatFormer takes as input an initial 3DGS set optimized under limited
training views and refines it in a single forward pass, effectively removing
potential artifacts in OOD test views. To our knowledge, this is the first
successful application of point transformers directly on 3DGS sets, surpassing
the limitations of previous multi-scene training methods, which could handle
only a restricted number of input views during inference. Our model
significantly improves rendering quality under extreme novel views, achieving
state-of-the-art performance in these challenging scenarios and outperforming
various 3DGS regularization techniques, multi-scene models tailored for sparse
view synthesis, and diffusion-based frameworks.";Yutong Chen<author:sep>Marko Mihajlovic<author:sep>Xiyi Chen<author:sep>Yiming Wang<author:sep>Sergey Prokudin<author:sep>Siyu Tang;http://arxiv.org/pdf/2411.06390v2;cs.CV;"Code and dataset: https://github.com/ChenYutongTHU/SplatFormer
  Project page: https://sergeyprokudin.github.io/splatformer/";gaussian splatting
2411.06019v1;http://arxiv.org/abs/2411.06019v1;2024-11-09;"GaussianSpa: An ""Optimizing-Sparsifying"" Simplification Framework for  Compact and High-Quality 3D Gaussian Splatting";"3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view
synthesis, leveraging continuous aggregations of Gaussian functions to model
scene geometry. However, 3DGS suffers from substantial memory requirements to
store the multitude of Gaussians, hindering its practicality. To address this
challenge, we introduce GaussianSpa, an optimization-based simplification
framework for compact and high-quality 3DGS. Specifically, we formulate the
simplification as an optimization problem associated with the 3DGS training.
Correspondingly, we propose an efficient ""optimizing-sparsifying"" solution that
alternately solves two independent sub-problems, gradually imposing strong
sparsity onto the Gaussians in the training process. Our comprehensive
evaluations on various datasets show the superiority of GaussianSpa over
existing state-of-the-art approaches. Notably, GaussianSpa achieves an average
PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with
10$\times$ fewer Gaussians compared to the vanilla 3DGS. Our project page is
available at https://gaussianspa.github.io/.";Yangming Zhang<author:sep>Wenqi Jia<author:sep>Wei Niu<author:sep>Miao Yin;http://arxiv.org/pdf/2411.06019v1;cs.CV;Project page at https://gaussianspa.github.io/;gaussian splatting
2411.06067v1;http://arxiv.org/abs/2411.06067v1;2024-11-09;AI-Driven Stylization of 3D Environments;"In this system, we discuss methods to stylize a scene of 3D primitive objects
into a higher fidelity 3D scene using novel 3D representations like NeRFs and
3D Gaussian Splatting. Our approach leverages existing image stylization
systems and image-to-3D generative models to create a pipeline that iteratively
stylizes and composites 3D objects into scenes. We show our results on adding
generated objects into a scene and discuss limitations.";Yuanbo Chen<author:sep>Yixiao Kang<author:sep>Yukun Song<author:sep>Cyrus Vachha<author:sep>Sining Huang;http://arxiv.org/pdf/2411.06067v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.05557v1;http://arxiv.org/abs/2411.05557v1;2024-11-08;A Nerf-Based Color Consistency Method for Remote Sensing Images;"Due to different seasons, illumination, and atmospheric conditions, the
photometric of the acquired image varies greatly, which leads to obvious
stitching seams at the edges of the mosaic image. Traditional methods can be
divided into two categories, one is absolute radiation correction and the other
is relative radiation normalization. We propose a NeRF-based method of color
consistency correction for multi-view images, which weaves image features
together using implicit expressions, and then re-illuminates feature space to
generate a fusion image with a new perspective. We chose Superview-1 satellite
images and UAV images with large range and time difference for the experiment.
Experimental results show that the synthesize image generated by our method has
excellent visual effect and smooth color transition at the edges.";Zongcheng Zuo<author:sep>Yuanxiang Li<author:sep>Tongtong Zhang;http://arxiv.org/pdf/2411.05557v1;cs.CV;"4 pages, 4 figures, The International Geoscience and Remote Sensing
  Symposium (IGARSS2023)";nerf
2411.05322v1;http://arxiv.org/abs/2411.05322v1;2024-11-08;Rate-aware Compression for NeRF-based Volumetric Video;"The neural radiance fields (NeRF) have advanced the development of 3D
volumetric video technology, but the large data volumes they involve pose
significant challenges for storage and transmission. To address these problems,
the existing solutions typically compress these NeRF representations after the
training stage, leading to a separation between representation training and
compression. In this paper, we try to directly learn a compact NeRF
representation for volumetric video in the training stage based on the proposed
rate-aware compression framework. Specifically, for volumetric video, we use a
simple yet effective modeling strategy to reduce temporal redundancy for the
NeRF representation. Then, during the training phase, an implicit entropy model
is utilized to estimate the bitrate of the NeRF representation. This entropy
model is then encoded into the bitstream to assist in the decoding of the NeRF
representation. This approach enables precise bitrate estimation, thereby
leading to a compact NeRF representation. Furthermore, we propose an adaptive
quantization strategy and learn the optimal quantization step for the NeRF
representations. Finally, the NeRF representation can be optimized by using the
rate-distortion trade-off. Our proposed compression framework can be used for
different representations and experimental results demonstrate that our
approach significantly reduces the storage size with marginal distortion and
achieves state-of-the-art rate-distortion performance for volumetric video on
the HumanRF and ReRF datasets. Compared to the previous state-of-the-art method
TeTriRF, we achieved an approximately -80% BD-rate on the HumanRF dataset and
-60% BD-rate on the ReRF dataset.";Zhiyu Zhang<author:sep>Guo Lu<author:sep>Huanxiong Liang<author:sep>Zhengxue Cheng<author:sep>Anni Tang<author:sep>Li Song;http://arxiv.org/pdf/2411.05322v1;cs.MM;Accepted by ACM MM 2024 (Oral);nerf
2411.04386v2;http://arxiv.org/abs/2411.04386v2;2024-11-07;SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on Larger  Objects for Mobile-Manipulation;"Grasp planning and estimation have been a longstanding research problem in
robotics, with two main approaches to find graspable poses on the objects: 1)
geometric approach, which relies on 3D models of objects and the gripper to
estimate valid grasp poses, and 2) data-driven, learning-based approach, with
models trained to identify grasp poses from raw sensor observations. The latter
assumes comprehensive geometric coverage during the training phase. However,
the data-driven approach is typically biased toward tabletop scenarios and
struggle to generalize to out-of-distribution scenarios with larger objects
(e.g. chair). Additionally, raw sensor data (e.g. RGB-D data) from a single
view of these larger objects is often incomplete and necessitates additional
observations. In this paper, we take a geometric approach, leveraging
advancements in object modeling (e.g. NeRF) to build an implicit model by
taking RGB images from views around the target object. This model enables the
extraction of explicit mesh model while also capturing the visual appearance
from novel viewpoints that is useful for perception tasks like object detection
and pose estimation. We further decompose the NeRF-reconstructed 3D mesh into
superquadrics (SQs) -- parametric geometric primitives, each mapped to a set of
precomputed grasp poses, allowing grasp composition on the target object based
on these primitives. Our proposed pipeline overcomes the problems: a) noisy
depth and incomplete view of the object, with a modeling step, and b)
generalization to objects of any size. For more qualitative results, refer to
the supplementary video and webpage https://bit.ly/3ZrOanU";Xun Tu<author:sep>Karthik Desingh;http://arxiv.org/pdf/2411.04386v2;cs.RO;8 pages, 7 figures, submitted to ICRA 2025 for review;nerf
2411.05006v1;http://arxiv.org/abs/2411.05006v1;2024-11-07;ProEdit: Simple Progression is All You Need for High-Quality 3D Scene  Editing;"This paper proposes ProEdit - a simple yet effective framework for
high-quality 3D scene editing guided by diffusion distillation in a novel
progressive manner. Inspired by the crucial observation that multi-view
inconsistency in scene editing is rooted in the diffusion model's large
feasible output space (FOS), our framework controls the size of FOS and reduces
inconsistency by decomposing the overall editing task into several subtasks,
which are then executed progressively on the scene. Within this framework, we
design a difficulty-aware subtask decomposition scheduler and an adaptive 3D
Gaussian splatting (3DGS) training strategy, ensuring high quality and
efficiency in performing each subtask. Extensive evaluation shows that our
ProEdit achieves state-of-the-art results in various scenes and challenging
editing tasks, all through a simple framework without any expensive or
sophisticated add-ons like distillation losses, components, or training
procedures. Notably, ProEdit also provides a new way to control, preview, and
select the ""aggressivity"" of editing operation during the editing process.";Jun-Kun Chen<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2411.05006v1;cs.CV;NeurIPS 2024. Project Page: https://immortalco.github.io/ProEdit/;gaussian splatting
2411.04810v1;http://arxiv.org/abs/2411.04810v1;2024-11-07;GANESH: Generalizable NeRF for Lensless Imaging;"Lensless imaging offers a significant opportunity to develop ultra-compact
cameras by removing the conventional bulky lens system. However, without a
focusing element, the sensor's output is no longer a direct image but a complex
multiplexed scene representation. Traditional methods have attempted to address
this challenge by employing learnable inversions and refinement models, but
these methods are primarily designed for 2D reconstruction and do not
generalize well to 3D reconstruction. We introduce GANESH, a novel framework
designed to enable simultaneous refinement and novel view synthesis from
multi-view lensless images. Unlike existing methods that require scene-specific
training, our approach supports on-the-fly inference without retraining on each
scene. Moreover, our framework allows us to tune our model to specific scenes,
enhancing the rendering and refinement quality. To facilitate research in this
area, we also present the first multi-view lensless dataset, LenslessScenes.
Extensive experiments demonstrate that our method outperforms current
approaches in reconstruction accuracy and refinement quality. Code and video
results are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/";Rakesh Raj Madavan<author:sep>Akshat Kaimal<author:sep>Badhrinarayanan K V<author:sep>Vinayak Gupta<author:sep>Rohit Choudhary<author:sep>Chandrakala Shanmuganathan<author:sep>Kaushik Mitra;http://arxiv.org/pdf/2411.04810v1;cs.CV;;nerf
2411.04924v1;http://arxiv.org/abs/2411.04924v1;2024-11-07;MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views;"We introduce MVSplat360, a feed-forward approach for 360{\deg} novel view
synthesis (NVS) of diverse real-world scenes, using only sparse observations.
This setting is inherently ill-posed due to minimal overlap among input views
and insufficient visual information provided, making it challenging for
conventional methods to achieve high-quality results. Our MVSplat360 addresses
this by effectively combining geometry-aware 3D reconstruction with temporally
consistent video generation. Specifically, it refactors a feed-forward 3D
Gaussian Splatting (3DGS) model to render features directly into the latent
space of a pre-trained Stable Video Diffusion (SVD) model, where these features
then act as pose and visual cues to guide the denoising process and produce
photorealistic 3D-consistent views. Our model is end-to-end trainable and
supports rendering arbitrary views with as few as 5 sparse input views. To
evaluate MVSplat360's performance, we introduce a new benchmark using the
challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual
quality compared to state-of-the-art methods on wide-sweeping or even 360{\deg}
NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the
effectiveness of our model. The video results are available on our project
page: https://donydchen.github.io/mvsplat360.";Yuedong Chen<author:sep>Chuanxia Zheng<author:sep>Haofei Xu<author:sep>Bohan Zhuang<author:sep>Andrea Vedaldi<author:sep>Tat-Jen Cham<author:sep>Jianfei Cai;http://arxiv.org/pdf/2411.04924v1;cs.CV;"NeurIPS 2024, Project page: https://donydchen.github.io/mvsplat360,
  Code: https://github.com/donydchen/mvsplat360";gaussian splatting
2411.04984v1;http://arxiv.org/abs/2411.04984v1;2024-11-07;Planar Reflection-Aware Neural Radiance Fields;"Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in
reconstructing complex scenes with high fidelity. However, NeRF's view
dependency can only handle low-frequency reflections. It falls short when
handling complex planar reflections, often interpreting them as erroneous scene
geometries and leading to duplicated and inaccurate scene representations. To
address this challenge, we introduce a reflection-aware NeRF that jointly
models planar reflectors, such as windows, and explicitly casts reflected rays
to capture the source of the high-frequency reflections. We query a single
radiance field to render the primary color and the source of the reflection. We
propose a sparse edge regularization to help utilize the true sources of
reflections for rendering planar reflections rather than creating a duplicate
along the primary ray at the same depth. As a result, we obtain accurate scene
geometry. Rendering along the primary ray results in a clean, reflection-free
view, while explicitly rendering along the reflected ray allows us to
reconstruct highly detailed reflections. Our extensive quantitative and
qualitative evaluations of real-world datasets demonstrate our method's
enhanced performance in accurately handling reflections.";Chen Gao<author:sep>Yipeng Wang<author:sep>Changil Kim<author:sep>Jia-Bin Huang<author:sep>Johannes Kopf;http://arxiv.org/pdf/2411.04984v1;cs.CV;;nerf
2411.03637v1;http://arxiv.org/abs/2411.03637v1;2024-11-06;Structure Consistent Gaussian Splatting with Matching Prior for Few-shot  Novel View Synthesis;"Despite the substantial progress of novel view synthesis, existing methods,
either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian
Splatting (3DGS), suffer significant degradation when the input becomes sparse.
Numerous efforts have been introduced to alleviate this problem, but they still
struggle to synthesize satisfactory results efficiently, especially in the
large scene. In this paper, we propose SCGaussian, a Structure Consistent
Gaussian Splatting method using matching priors to learn 3D consistent scene
structure. Considering the high interdependence of Gaussian attributes, we
optimize the scene structure in two folds: rendering geometry and, more
importantly, the position of Gaussian primitives, which is hard to be directly
constrained in the vanilla 3DGS due to the non-structure property. To achieve
this, we present a hybrid Gaussian representation. Besides the ordinary
non-structure Gaussian primitives, our model also consists of ray-based
Gaussian primitives that are bound to matching rays and whose optimization of
their positions is restricted along the ray. Thus, we can utilize the matching
correspondence to directly enforce the position of these Gaussian primitives to
converge to the surface points where rays intersect. Extensive experiments on
forward-facing, surrounding, and complex large scenes show the effectiveness of
our approach with state-of-the-art performance and high efficiency. Code is
available at https://github.com/prstrive/SCGaussian.";Rui Peng<author:sep>Wangze Xu<author:sep>Luyang Tang<author:sep>Liwei Liao<author:sep>Jianbo Jiao<author:sep>Ronggang Wang;http://arxiv.org/pdf/2411.03637v1;cs.CV;NeurIPS 2024 Accepted;gaussian splatting<tag:sep>nerf
2411.03807v3;http://arxiv.org/abs/2411.03807v3;2024-11-06;GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian  Splatting;"This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.";Jilan Mei<author:sep>Junbo Li<author:sep>Cai Meng;http://arxiv.org/pdf/2411.03807v3;cs.CV;;gaussian splatting
2411.03706v1;http://arxiv.org/abs/2411.03706v1;2024-11-06;3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical  Object Rearrangement;"We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for
detecting physical object rearrangements in 3D scenes. Our approach estimates
3D object-level changes by comparing two sets of unaligned images taken at
different times. Leveraging 3DGS's novel view rendering and EfficientSAM's
zero-shot segmentation capabilities, we detect 2D object-level changes, which
are then associated and fused across views to estimate 3D changes. Our method
can detect changes in cluttered environments using sparse post-change images
within as little as 18s, using as few as a single new image. It does not rely
on depth input, user instructions, object classes, or object models -- An
object is recognized simply if it has been re-arranged. Our approach is
evaluated on both public and self-collected real-world datasets, achieving up
to 14% higher accuracy and three orders of magnitude faster performance
compared to the state-of-the-art radiance-field-based change detection method.
This significant performance boost enables a broad range of downstream
applications, where we highlight three key use cases: object reconstruction,
robot workspace reset, and 3DGS model update. Our code and data will be made
available at https://github.com/520xyxyzq/3DGS-CD.";Ziqi Lu<author:sep>Jianbo Ye<author:sep>John Leonard;http://arxiv.org/pdf/2411.03706v1;cs.CV;;gaussian splatting
2411.03086v1;http://arxiv.org/abs/2411.03086v1;2024-11-05;HFGaussian: Learning Generalizable Gaussian Human with Integrated Human  Features;"Recent advancements in radiance field rendering show promising results in 3D
scene representation, where Gaussian splatting-based techniques emerge as
state-of-the-art due to their quality and efficiency. Gaussian splatting is
widely used for various applications, including 3D human representation.
However, previous 3D Gaussian splatting methods either use parametric body
models as additional information or fail to provide any underlying structure,
like human biomechanical features, which are essential for different
applications. In this paper, we present a novel approach called HFGaussian that
can estimate novel views and human features, such as the 3D skeleton, 3D key
points, and dense pose, from sparse input images in real time at 25 FPS. The
proposed method leverages generalizable Gaussian splatting technique to
represent the human subject and its associated features, enabling efficient and
generalizable reconstruction. By incorporating a pose regression network and
the feature splatting technique with Gaussian splatting, HFGaussian
demonstrates improved capabilities over existing 3D human methods, showcasing
the potential of 3D human representations with integrated biomechanics. We
thoroughly evaluate our HFGaussian method against the latest state-of-the-art
techniques in human Gaussian splatting and pose estimation, demonstrating its
real-time, state-of-the-art performance.";Arnab Dey<author:sep>Cheng-You Lu<author:sep>Andrew I. Comport<author:sep>Srinath Sridhar<author:sep>Chin-Teng Lin<author:sep>Jean Martinet;http://arxiv.org/pdf/2411.03086v1;cs.CV;;gaussian splatting
2411.02703v1;http://arxiv.org/abs/2411.02703v1;2024-11-05;LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian  Splatting;"3D Gaussian Splatting (3DGS) has shown its ability in rapid rendering and
high-fidelity mapping. In this paper, we introduce LVI-GS, a tightly-coupled
LiDAR-Visual-Inertial mapping framework with 3DGS, which leverages the
complementary characteristics of LiDAR and image sensors to capture both
geometric structures and visual details of 3D scenes. To this end, the 3D
Gaussians are initialized from colourized LiDAR points and optimized using
differentiable rendering. In order to achieve high-fidelity mapping, we
introduce a pyramid-based training approach to effectively learn multi-level
features and incorporate depth loss derived from LiDAR measurements to improve
geometric feature perception. Through well-designed strategies for Gaussian-Map
expansion, keyframe selection, thread management, and custom CUDA acceleration,
our framework achieves real-time photo-realistic mapping. Numerical experiments
are performed to evaluate the superior performance of our method compared to
state-of-the-art 3D reconstruction systems.";Huibin Zhao<author:sep>Weipeng Guan<author:sep>Peng Lu;http://arxiv.org/pdf/2411.02703v1;cs.RO;;gaussian splatting
2411.03555v1;http://arxiv.org/abs/2411.03555v1;2024-11-05;Object and Contact Point Tracking in Demonstrations Using 3D Gaussian  Splatting;"This paper introduces a method to enhance Interactive Imitation Learning
(IIL) by extracting touch interaction points and tracking object movement from
video demonstrations. The approach extends current IIL systems by providing
robots with detailed knowledge of both where and how to interact with objects,
particularly complex articulated ones like doors and drawers. By leveraging
cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for
tracking, this method allows robots to better understand and manipulate objects
in dynamic environments. The research lays the foundation for more effective
task learning and execution in autonomous robotic systems.";Michael Büttner<author:sep>Jonathan Francis<author:sep>Helge Rhodin<author:sep>Andrew Melnik;http://arxiv.org/pdf/2411.03555v1;cs.CV;"CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich,
  Germany";gaussian splatting
2411.02969v1;http://arxiv.org/abs/2411.02969v1;2024-11-05;Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation;"LiDAR Semantic Segmentation is a fundamental task in autonomous driving
perception consisting of associating each LiDAR point to a semantic label.
Fully-supervised models have widely tackled this task, but they require labels
for each scan, which either limits their domain or requires impractical amounts
of expensive annotations. Camera images, which are generally recorded alongside
LiDAR pointclouds, can be processed by the widely available 2D foundation
models, which are generic and dataset-agnostic. However, distilling knowledge
from 2D data to improve LiDAR perception raises domain adaptation challenges.
For example, the classical perspective projection suffers from the parallax
effect produced by the position shift between both sensors at their respective
capture times. We propose a Semi-Supervised Learning setup to leverage
unlabeled LiDAR pointclouds alongside distilled knowledge from the camera
images. To self-supervise our model on the unlabeled scans, we add an auxiliary
NeRF head and cast rays from the camera viewpoint over the unlabeled voxel
features. The NeRF head predicts densities and semantic logits at each sampled
ray location which are used for rendering pixel semantics. Concurrently, we
query the Segment-Anything (SAM) foundation model with the camera image to
generate a set of unlabeled generic masks. We fuse the masks with the rendered
pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel
predictions. During inference, we drop the NeRF head and run our model with
only LiDAR. We show the effectiveness of our approach in three public LiDAR
Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.";Xavier Timoneda<author:sep>Markus Herb<author:sep>Fabian Duerr<author:sep>Daniel Goehring<author:sep>Fisher Yu;http://arxiv.org/pdf/2411.02969v1;cs.CV;"IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS) 2024";nerf
2411.02972v1;http://arxiv.org/abs/2411.02972v1;2024-11-05;Exploring Seasonal Variability in the Context of Neural Radiance Fields  for 3D Reconstruction on Satellite Imagery;"In this work, the seasonal predictive capabilities of Neural Radiance Fields
(NeRF) applied to satellite images are investigated. Focusing on the
utilization of satellite data, the study explores how Sat-NeRF, a novel
approach in computer vision, performs in predicting seasonal variations across
different months. Through comprehensive analysis and visualization, the study
examines the model's ability to capture and predict seasonal changes,
highlighting specific challenges and strengths. Results showcase the impact of
the sun direction on predictions, revealing nuanced details in seasonal
transitions, such as snow cover, color accuracy, and texture representation in
different landscapes. Given these results, we propose Planet-NeRF, an extension
to Sat-NeRF capable of incorporating seasonal variability through a set of
month embedding vectors. Comparative evaluations reveal that Planet-NeRF
outperforms prior models in the case where seasonal changes are present. The
extensive evaluation combined with the proposed method offers promising avenues
for future research in this domain.";Liv Kåreborn<author:sep>Erica Ingerstad<author:sep>Amanda Berg<author:sep>Justus Karlsson<author:sep>Leif Haglund;http://arxiv.org/pdf/2411.02972v1;cs.CV;;nerf
2411.02979v1;http://arxiv.org/abs/2411.02979v1;2024-11-05;CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model  Retrieval;"Reconstructing from multi-view images is a longstanding problem in 3D vision,
where neural radiance fields (NeRFs) have shown great potential and get
realistic rendered images of novel views. Currently, most NeRF methods either
require accurate camera poses or a large number of input images, or even both.
Reconstructing NeRF from few-view images without poses is challenging and
highly ill-posed. To address this problem, we propose CAD-NeRF, a method
reconstructed from less than 10 images without any known poses. Specifically,
we build a mini library of several CAD models from ShapeNet and render them
from many random views. Given sparse-view input images, we run a model and pose
retrieval from the library, to get a model with similar shapes, serving as the
density supervision and pose initializations. Here we propose a multi-view pose
retrieval method to avoid pose conflicts among views, which is a new and unseen
problem in uncalibrated NeRF methods. Then, the geometry of the object is
trained by the CAD guidance. The deformation of the density field and camera
poses are optimized jointly. Then texture and density are trained and
fine-tuned as well. All training phases are in self-supervised manners.
Comprehensive evaluations of synthetic and real images show that CAD-NeRF
successfully learns accurate densities with a large deformation from retrieved
CAD models, showing the generalization abilities.";Xin Wen<author:sep>Xuening Zhu<author:sep>Renjiao Yi<author:sep>Zhifeng Wang<author:sep>Chenyang Zhu<author:sep>Kai Xu;http://arxiv.org/pdf/2411.02979v1;cs.CV;The article has been accepted by Frontiers of Computer Science (FCS);nerf
2411.03487v1;http://arxiv.org/abs/2411.03487v1;2024-11-05;Enhancing Exploratory Capability of Visual Navigation Using Uncertainty  of Implicit Scene Representation;"In the context of visual navigation in unknown scenes, both ""exploration"" and
""exploitation"" are equally crucial. Robots must first establish environmental
cognition through exploration and then utilize the cognitive information to
accomplish target searches. However, most existing methods for image-goal
navigation prioritize target search over the generation of exploratory
behavior. To address this, we propose the Navigation with Uncertainty-driven
Exploration (NUE) pipeline, which uses an implicit and compact scene
representation, NeRF, as a cognitive structure. We estimate the uncertainty of
NeRF and augment the exploratory ability by the uncertainty to in turn
facilitate the construction of implicit representation. Simultaneously, we
extract memory information from NeRF to enhance the robot's reasoning ability
for determining the location of the target. Ultimately, we seamlessly combine
the two generated abilities to produce navigational actions. Our pipeline is
end-to-end, with the environmental cognitive structure being constructed
online. Extensive experimental results on image-goal navigation demonstrate the
capability of our pipeline to enhance exploratory behaviors, while also
enabling a natural transition from the exploration to exploitation phase. This
enables our model to outperform existing memory-based cognitive navigation
structures in terms of navigation performance.";Yichen Wang<author:sep>Qiming Liu<author:sep>Zhe Liu<author:sep>Hesheng Wang;http://arxiv.org/pdf/2411.03487v1;cs.RO;;nerf
2411.02547v1;http://arxiv.org/abs/2411.02547v1;2024-11-04;Modeling Uncertainty in 3D Gaussian Splatting through Continuous  Semantic Splatting;"In this paper, we present a novel algorithm for probabilistically updating
and rasterizing semantic maps within 3D Gaussian Splatting (3D-GS). Although
previous methods have introduced algorithms which learn to rasterize features
in 3D-GS for enhanced scene understanding, 3D-GS can fail without warning which
presents a challenge for safety-critical robotic applications. To address this
gap, we propose a method which advances the literature of continuous semantic
mapping from voxels to ellipsoids, combining the precise structure of 3D-GS
with the ability to quantify uncertainty of probabilistic robotic maps. Given a
set of images, our algorithm performs a probabilistic semantic update directly
on the 3D ellipsoids to obtain an expectation and variance through the use of
conjugate priors. We also propose a probabilistic rasterization which returns
per-pixel segmentation predictions with quantifiable uncertainty. We compare
our method with similar probabilistic voxel-based methods to verify our
extension to 3D ellipsoids, and perform ablation studies on uncertainty
quantification and temporal smoothing.";Joey Wilson<author:sep>Marcelino Almeida<author:sep>Min Sun<author:sep>Sachit Mahajan<author:sep>Maani Ghaffari<author:sep>Parker Ewen<author:sep>Omid Ghasemalizadeh<author:sep>Cheng-Hao Kuo<author:sep>Arnie Sen;http://arxiv.org/pdf/2411.02547v1;cs.RO;;gaussian splatting
2411.02332v2;http://arxiv.org/abs/2411.02332v2;2024-11-04;SplatOverflow: Asynchronous Hardware Troubleshooting;"As tools for designing and manufacturing hardware become more accessible,
smaller producers can develop and distribute novel hardware. However, there
aren't established tools to support end-user hardware troubleshooting or
routine maintenance. As a result, technical support for hardware remains ad-hoc
and challenging to scale. Inspired by software troubleshooting workflows like
StackOverflow, we propose a workflow for asynchronous hardware troubleshooting:
SplatOverflow. SplatOverflow creates a novel boundary object, the SplatOverflow
scene, that users reference to communicate about hardware. The scene comprises
a 3D Gaussian Splat of the user's hardware registered onto the hardware's CAD
model. The splat captures the current state of the hardware, and the registered
CAD model acts as a referential anchor for troubleshooting instructions. With
SplatOverflow, maintainers can directly address issues and author instructions
in the user's workspace. The instructions define workflows that can easily be
shared between users and recontextualized in new environments. In this paper,
we describe the design of SplatOverflow, detail the workflows it enables, and
illustrate its utility to different kinds of users. We also validate that
non-experts can use SplatOverflow to troubleshoot common problems with a 3D
printer in a user study.";Amritansh Kwatra<author:sep>Tobias Wienberg<author:sep>Ilan Mandel<author:sep>Ritik Batra<author:sep>Peter He<author:sep>Francois Guimbretiere<author:sep>Thijs Roumen;http://arxiv.org/pdf/2411.02332v2;cs.HC;"Our accompanying video figure is available at:
  https://youtu.be/m4TKeBDuZkU";
2411.01853v2;http://arxiv.org/abs/2411.01853v2;2024-11-04;GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface  Reconstruction in Open Scenes;"In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.";Gaochao Song<author:sep>Chong Cheng<author:sep>Hao Wang;http://arxiv.org/pdf/2411.01853v2;cs.CV;NeurIPS 2024;gaussian splatting<tag:sep>nerf
2411.02482v1;http://arxiv.org/abs/2411.02482v1;2024-11-04;NeRF-Aug: Data Augmentation for Robotics with Neural Radiance Fields;"Training a policy that can generalize to unknown objects is a long standing
challenge within the field of robotics. The performance of a policy often drops
significantly in situations where an object in the scene was not seen during
training. To solve this problem, we present NeRF-Aug, a novel method that is
capable of teaching a policy to interact with objects that are not present in
the dataset. This approach differs from existing approaches by leveraging the
speed and photorealism of a neural radiance field for augmentation. NeRF- Aug
both creates more photorealistic data and runs 3.83 times faster than existing
methods. We demonstrate the effectiveness of our method on 4 tasks with 11
novel objects that have no expert demonstration data. We achieve an average
69.1% success rate increase over existing methods. See video results at
https://nerf-aug.github.io.";Eric Zhu<author:sep>Mara Levy<author:sep>Matthew Gwilliam<author:sep>Abhinav Shrivastava;http://arxiv.org/pdf/2411.02482v1;cs.RO;;nerf
2411.01725v1;http://arxiv.org/abs/2411.01725v1;2024-11-04;A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields;"In this paper we reexamine the process through which a Neural Radiance Field
(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image
applications where camera pixels integrate light over time, LiDAR pulses arrive
at specific times. As such, multiple LiDAR returns are possible for any given
detector and the classification of these returns is inherently probabilistic.
Applying a traditional NeRF training routine can result in the network learning
phantom surfaces in free space between conflicting range measurements, similar
to how floater aberrations may be produced by an image model. We show that by
formulating loss as an integral of probability (rather than as an integral of
optical density) the network can learn multiple peaks for a given ray, allowing
the sampling of first, nth, or strongest returns from a single output channel.
Code is available at https://github.com/mcdermatt/PLINK";Matthew McDermott<author:sep>Jason Rife;http://arxiv.org/pdf/2411.01725v1;cs.CV;;nerf
2411.02229v2;http://arxiv.org/abs/2411.02229v2;2024-11-04;FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage  Training;"The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.";Ruihong Yin<author:sep>Vladimir Yugay<author:sep>Yue Li<author:sep>Sezer Karaoglu<author:sep>Theo Gevers;http://arxiv.org/pdf/2411.02229v2;cs.CV;Accepted by NeurIPS2024;gaussian splatting<tag:sep>nerf
2411.01218v1;http://arxiv.org/abs/2411.01218v1;2024-11-02;Real-Time Spatio-Temporal Reconstruction of Dynamic Endoscopic Scenes  with 4D Gaussian Splatting;"Dynamic scene reconstruction is essential in robotic minimally invasive
surgery, providing crucial spatial information that enhances surgical precision
and outcomes. However, existing methods struggle to address the complex,
temporally dynamic nature of endoscopic scenes. This paper presents
ST-Endo4DGS, a novel framework that models the spatio-temporal volume of
dynamic endoscopic scenes using unbiased 4D Gaussian Splatting (4DGS)
primitives, parameterized by anisotropic ellipses with flexible 4D rotations.
This approach enables precise representation of deformable tissue dynamics,
capturing intricate spatial and temporal correlations in real time.
Additionally, we extend spherindrical harmonics to represent time-evolving
appearance, achieving realistic adaptations to lighting and view changes. A new
endoscopic normal alignment constraint (ENAC) further enhances geometric
fidelity by aligning rendered normals with depth-derived geometry. Extensive
evaluations show that ST-Endo4DGS outperforms existing methods in both visual
quality and real-time performance, establishing a new state-of-the-art in
dynamic scene reconstruction for endoscopic surgery.";Fengze Li<author:sep>Jishuai He<author:sep>Jieming Ma<author:sep>Zhijing Wu;http://arxiv.org/pdf/2411.01218v1;cs.CV;;gaussian splatting
2411.00626v1;http://arxiv.org/abs/2411.00626v1;2024-11-01;ZIM: Zero-Shot Image Matting for Anything;"The recent segmentation foundation model, Segment Anything Model (SAM),
exhibits strong zero-shot segmentation capabilities, but it falls short in
generating fine-grained precise masks. To address this limitation, we propose a
novel zero-shot image matting model, called ZIM, with two key contributions:
First, we develop a label converter that transforms segmentation labels into
detailed matte labels, constructing the new SA1B-Matte dataset without costly
manual annotations. Training SAM with this dataset enables it to generate
precise matte masks while maintaining its zero-shot capability. Second, we
design the zero-shot matting model equipped with a hierarchical pixel decoder
to enhance mask representation, along with a prompt-aware masked attention
mechanism to improve performance by enabling the model to focus on regions
specified by visual prompts. We evaluate ZIM using the newly introduced
MicroMat-3K test set, which contains high-quality micro-level matte labels.
Experimental results show that ZIM outperforms existing methods in fine-grained
mask generation and zero-shot generalization. Furthermore, we demonstrate the
versatility of ZIM in various downstream tasks requiring precise masks, such as
image inpainting and 3D NeRF. Our contributions provide a robust foundation for
advancing zero-shot matting and its downstream applications across a wide range
of computer vision tasks. The code is available at
\url{https://github.com/naver-ai/ZIM}.";Beomyoung Kim<author:sep>Chanyong Shin<author:sep>Joonhyun Jeong<author:sep>Hyungsik Jung<author:sep>Se-Yun Lee<author:sep>Sewhan Chun<author:sep>Dong-Hyun Hwang<author:sep>Joonsang Yu;http://arxiv.org/pdf/2411.00626v1;cs.CV;preprint (21 pages, 16 figures, and 8 tables);nerf
2411.00771v1;http://arxiv.org/abs/2411.00771v1;2024-11-01;CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for  Large-Scale Scenes;"Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field
reconstruction, manifesting efficient and high-fidelity novel view synthesis.
However, accurately representing surfaces, especially in large and complex
scenarios, remains a significant challenge due to the unstructured nature of
3DGS. In this paper, we present CityGaussianV2, a novel approach for
large-scale scene reconstruction that addresses critical challenges related to
geometric accuracy and efficiency. Building on the favorable generalization
capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and
scalability issues. Specifically, we implement a decomposed-gradient-based
densification and depth regression technique to eliminate blurry artifacts and
accelerate convergence. To scale up, we introduce an elongation filter that
mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we
optimize the CityGaussian pipeline for parallel training, achieving up to
10$\times$ compression, at least 25% savings in training time, and a 50%
decrease in memory usage. We also established standard geometry benchmarks
under large-scale scenes. Experimental results demonstrate that our method
strikes a promising balance between visual quality, geometric accuracy, as well
as storage and training costs. The project page is available at
https://dekuliutesla.github.io/CityGaussianV2/.";Yang Liu<author:sep>Chuanchen Luo<author:sep>Zhongkai Mao<author:sep>Junran Peng<author:sep>Zhaoxiang Zhang;http://arxiv.org/pdf/2411.00771v1;cs.CV;Project Page: https://dekuliutesla.github.io/CityGaussianV2/;gaussian splatting
2411.00632v1;http://arxiv.org/abs/2411.00632v1;2024-11-01;PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud  Understanding;"In this paper, we present PCoTTA, an innovative, pioneering framework for
Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding,
enhancing the model's transferability towards the continually changing target
domain. We introduce a multi-task setting for PCoTTA, which is practical and
realistic, handling multiple tasks within one unified model during the
continual adaptation. Our PCoTTA involves three key components: automatic
prototype mixture (APM), Gaussian Splatted feature shifting (GSFS), and
contrastive prototype repulsion (CPR). Firstly, APM is designed to
automatically mix the source prototypes with the learnable prototypes with a
similarity balancing factor, avoiding catastrophic forgetting. Then, GSFS
dynamically shifts the testing sample toward the source domain, mitigating
error accumulation in an online manner. In addition, CPR is proposed to pull
the nearest learnable prototype close to the testing feature and push it away
from other prototypes, making each prototype distinguishable during the
adaptation. Experimental comparisons lead to a new benchmark, demonstrating
PCoTTA's superiority in boosting the model's transferability towards the
continually changing target domain.";Jincen Jiang<author:sep>Qianyu Zhou<author:sep>Yuhang Li<author:sep>Xinkui Zhao<author:sep>Meili Wang<author:sep>Lizhuang Ma<author:sep>Jian Chang<author:sep>Jian Jun Zhang<author:sep>Xuequan Lu;http://arxiv.org/pdf/2411.00632v1;cs.CV;Accepted to NeurIPS 2024;
2411.00144v1;http://arxiv.org/abs/2411.00144v1;2024-10-31;Self-Ensembling Gaussian Splatting for Few-shot Novel View Synthesis;"3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness for
novel view synthesis (NVS). However, the 3DGS model tends to overfit when
trained with sparse posed views, limiting its generalization capacity for
broader pose variations. In this paper, we alleviate the overfitting problem by
introducing a self-ensembling Gaussian Splatting (SE-GS) approach. We present
two Gaussian Splatting models named the $\mathbf{\Sigma}$-model and the
$\mathbf{\Delta}$-model. The $\mathbf{\Sigma}$-model serves as the primary
model that generates novel-view images during inference. At the training stage,
the $\mathbf{\Sigma}$-model is guided away from specific local optima by an
uncertainty-aware perturbing strategy. We dynamically perturb the
$\mathbf{\Delta}$-model based on the uncertainties of novel-view renderings
across different training steps, resulting in diverse temporal models sampled
from the Gaussian parameter space without additional training costs. The
geometry of the $\mathbf{\Sigma}$-model is regularized by penalizing
discrepancies between the $\mathbf{\Sigma}$-model and the temporal samples.
Therefore, our SE-GS conducts an effective and efficient regularization across
a large number of Gaussian Splatting models, resulting in a robust ensemble,
the $\mathbf{\Sigma}$-model. Experimental results on the LLFF, Mip-NeRF360,
DTU, and MVImgNet datasets show that our approach improves NVS quality with
few-shot training views, outperforming existing state-of-the-art methods. The
code is released at https://github.com/sailor-z/SE-GS.";Chen Zhao<author:sep>Xuan Wang<author:sep>Tong Zhang<author:sep>Saqib Javed<author:sep>Mathieu Salzmann;http://arxiv.org/pdf/2411.00144v1;cs.CV;;gaussian splatting<tag:sep>nerf
2410.23658v1;http://arxiv.org/abs/2410.23658v1;2024-10-31;GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring;"To train a deblurring network, an appropriate dataset with paired blurry and
sharp images is essential. Existing datasets collect blurry images either
synthetically by aggregating consecutive sharp frames or using sophisticated
camera systems to capture real blur. However, these methods offer limited
diversity in blur types (blur trajectories) or require extensive human effort
to reconstruct large-scale datasets, failing to fully reflect real-world blur
scenarios. To address this, we propose GS-Blur, a dataset of synthesized
realistic blurry images created using a novel approach. To this end, we first
reconstruct 3D scenes from multi-view images using 3D Gaussian Splatting
(3DGS), then render blurry images by moving the camera view along the randomly
generated motion trajectories. By adopting various camera trajectories in
reconstructing our GS-Blur, our dataset contains realistic and diverse types of
blur, offering a large-scale dataset that generalizes well to real-world blur.
Using GS-Blur with various deblurring methods, we demonstrate its ability to
generalize effectively compared to previous synthetic or real blur datasets,
showing significant improvements in deblurring performance.";Dongwoo Lee<author:sep>Joonkyu Park<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2410.23658v1;cs.CV;Accepted at NeurIPS 2024 Datasets & Benchmarks Track;gaussian splatting
2410.23690v1;http://arxiv.org/abs/2410.23690v1;2024-10-31;XRDSLAM: A Flexible and Modular Framework for Deep Learning based SLAM;"In this paper, we propose a flexible SLAM framework, XRDSLAM. It adopts a
modular code design and a multi-process running mechanism, providing highly
reusable foundational modules such as unified dataset management, 3d
visualization, algorithm configuration, and metrics evaluation. It can help
developers quickly build a complete SLAM system, flexibly combine different
algorithm modules, and conduct standardized benchmarking for accuracy and
efficiency comparison. Within this framework, we integrate several
state-of-the-art SLAM algorithms with different types, including NeRF and 3DGS
based SLAM, and even odometry or reconstruction algorithms, which demonstrates
the flexibility and extensibility. We also conduct a comprehensive comparison
and evaluation of these integrated algorithms, analyzing the characteristics of
each. Finally, we contribute all the code, configuration and data to the
open-source community, which aims to promote the widespread research and
development of SLAM technology within the open-source ecosystem.";Xiaomeng Wang<author:sep>Nan Wang<author:sep>Guofeng Zhang;http://arxiv.org/pdf/2410.23690v1;cs.CV;;nerf
2410.23718v1;http://arxiv.org/abs/2410.23718v1;2024-10-31;GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian  Splatting;"3D Gaussian Splatting (3DGS) has become a crucial method for acquiring 3D
assets. To protect the copyright of these assets, digital watermarking
techniques can be applied to embed ownership information discreetly within 3DGS
models. However, existing watermarking methods for meshes, point clouds, and
implicit radiance fields cannot be directly applied to 3DGS models, as 3DGS
models use explicit 3D Gaussians with distinct structures and do not rely on
neural networks. Naively embedding the watermark on a pre-trained 3DGS can
cause obvious distortion in rendered images. In our work, we propose an
uncertainty-based method that constrains the perturbation of model parameters
to achieve invisible watermarking for 3DGS. At the message decoding stage, the
copyright messages can be reliably extracted from both 3D Gaussians and 2D
rendered images even under various forms of 3D and 2D distortions. We conduct
extensive experiments on the Blender, LLFF and MipNeRF-360 datasets to validate
the effectiveness of our proposed method, demonstrating state-of-the-art
performance on both message decoding accuracy and view synthesis quality.";Xiufeng Huang<author:sep>Ruiqi Li<author:sep>Yiu-ming Cheung<author:sep>Ka Chun Cheung<author:sep>Simon See<author:sep>Renjie Wan;http://arxiv.org/pdf/2410.23718v1;cs.CV;;gaussian splatting<tag:sep>nerf
2411.00239v1;http://arxiv.org/abs/2411.00239v1;2024-10-31;Aquatic-GS: A Hybrid 3D Representation for Underwater Scenes;"Representing underwater 3D scenes is a valuable yet complex task, as
attenuation and scattering effects during underwater imaging significantly
couple the information of the objects and the water. This coupling presents a
significant challenge for existing methods in effectively representing both the
objects and the water medium simultaneously. To address this challenge, we
propose Aquatic-GS, a hybrid 3D representation approach for underwater scenes
that effectively represents both the objects and the water medium.
Specifically, we construct a Neural Water Field (NWF) to implicitly model the
water parameters, while extending the latest 3D Gaussian Splatting (3DGS) to
model the objects explicitly. Both components are integrated through a
physics-based underwater image formation model to represent complex underwater
scenes. Moreover, to construct more precise scene geometry and details, we
design a Depth-Guided Optimization (DGO) mechanism that uses a pseudo-depth map
as auxiliary guidance. After optimization, Aquatic-GS enables the rendering of
novel underwater viewpoints and supports restoring the true appearance of
underwater scenes, as if the water medium were absent. Extensive experiments on
both simulated and real-world datasets demonstrate that Aquatic-GS surpasses
state-of-the-art underwater 3D representation methods, achieving better
rendering quality and real-time rendering performance with a 410x increase in
speed. Furthermore, regarding underwater image restoration, Aquatic-GS
outperforms representative dewatering methods in color correction, detail
recovery, and stability. Our models, code, and datasets can be accessed at
https://aquaticgs.github.io.";Shaohua Liu<author:sep>Junzhe Lu<author:sep>Zuoya Gu<author:sep>Jiajun Li<author:sep>Yue Deng;http://arxiv.org/pdf/2411.00239v1;cs.CV;13 pages, 7 figures;gaussian splatting
2410.23742v1;http://arxiv.org/abs/2410.23742v1;2024-10-31;Scaled Inverse Graphics: Efficiently Learning Large Sets of 3D Scenes;"While the field of inverse graphics has been witnessing continuous growth,
techniques devised thus far predominantly focus on learning individual scene
representations. In contrast, learning large sets of scenes has been a
considerable bottleneck in NeRF developments, as repeatedly applying inverse
graphics on a sequence of scenes, though essential for various applications,
remains largely prohibitive in terms of resource costs. We introduce a
framework termed ""scaled inverse graphics"", aimed at efficiently learning large
sets of scene representations, and propose a novel method to this end. It
operates in two stages: (i) training a compression model on a subset of scenes,
then (ii) training NeRF models on the resulting smaller representations,
thereby reducing the optimization space per new scene. In practice, we compact
the representation of scenes by learning NeRFs in a latent space to reduce the
image resolution, and sharing information across scenes to reduce NeRF
representation complexity. We experimentally show that our method presents both
the lowest training time and memory footprint in scaled inverse graphics
compared to other methods applied independently on each scene. Our codebase is
publicly available as open-source. Our project page can be found at
https://scaled-ig.github.io .";Karim Kassab<author:sep>Antoine Schnepf<author:sep>Jean-Yves Franceschi<author:sep>Laurent Caraffa<author:sep>Flavian Vasile<author:sep>Jeremie Mary<author:sep>Andrew Comport<author:sep>Valérie Gouet-Brunet;http://arxiv.org/pdf/2410.23742v1;cs.CV;;nerf
2410.24204v2;http://arxiv.org/abs/2410.24204v2;2024-10-31;GeoSplatting: Towards Geometry Guided Gaussian Splatting for  Physically-based Inverse Rendering;"We consider the problem of physically-based inverse rendering using 3D
Gaussian Splatting (3DGS) representations. While recent 3DGS methods have
achieved remarkable results in novel view synthesis (NVS), accurately capturing
high-fidelity geometry, physically interpretable materials and lighting remains
challenging, as it requires precise geometry modeling to provide accurate
surface normals, along with physically-based rendering (PBR) techniques to
ensure correct material and lighting disentanglement. Previous 3DGS methods
resort to approximating surface normals, but often struggle with noisy local
geometry, leading to inaccurate normal estimation and suboptimal
material-lighting decomposition. In this paper, we introduce GeoSplatting, a
novel hybrid representation that augments 3DGS with explicit geometric guidance
and differentiable PBR equations. Specifically, we bridge isosurface and 3DGS
together, where we first extract isosurface mesh from a scalar field, then
convert it into 3DGS points and formulate PBR equations for them in a fully
differentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,
enabling precise surface normal modeling, which facilitates the use of PBR
frameworks for material decomposition. This approach further maintains the
efficiency and quality of NVS from 3DGS while ensuring accurate geometry from
the isosurface. Comprehensive evaluations across diverse datasets demonstrate
the superiority of GeoSplatting, consistently outperforming existing methods
both quantitatively and qualitatively.";Kai Ye<author:sep>Chong Gao<author:sep>Guanbin Li<author:sep>Wenzheng Chen<author:sep>Baoquan Chen;http://arxiv.org/pdf/2410.24204v2;cs.CV;Project page: https://pku-vcl-geometry.github.io/GeoSplatting/;gaussian splatting
2410.24207v1;http://arxiv.org/abs/2410.24207v1;2024-10-31;No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse  Unposed Images;"We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D
scenes parameterized by 3D Gaussians from \textit{unposed} sparse multi-view
images. Our model, trained exclusively with photometric loss, achieves
real-time 3D Gaussian reconstruction during inference. To eliminate the need
for accurate pose input during reconstruction, we anchor one input view's local
camera coordinates as the canonical space and train the network to predict
Gaussian primitives for all views within this space. This approach obviates the
need to transform Gaussian primitives from local coordinates into a global
coordinate system, thus avoiding errors associated with per-frame Gaussians and
pose estimation. To resolve scale ambiguity, we design and compare various
intrinsic embedding methods, ultimately opting to convert camera intrinsics
into a token embedding and concatenate it with image tokens as input to the
model, enabling accurate scene scale prediction. We utilize the reconstructed
3D Gaussians for novel view synthesis and pose estimation tasks and propose a
two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental
results demonstrate that our pose-free approach can achieve superior novel view
synthesis quality compared to pose-required methods, particularly in scenarios
with limited input image overlap. For pose estimation, our method, trained
without ground truth depth or explicit matching loss, significantly outperforms
the state-of-the-art methods with substantial improvements. This work makes
significant advances in pose-free generalizable 3D reconstruction and
demonstrates its applicability to real-world scenarios. Code and trained models
are available at https://noposplat.github.io/.";Botao Ye<author:sep>Sifei Liu<author:sep>Haofei Xu<author:sep>Xueting Li<author:sep>Marc Pollefeys<author:sep>Ming-Hsuan Yang<author:sep>Songyou Peng;http://arxiv.org/pdf/2410.24207v1;cs.CV;Project page: https://noposplat.github.io/;
2410.23701v1;http://arxiv.org/abs/2410.23701v1;2024-10-31;Get a Grip: Multi-Finger Grasp Evaluation at Scale Enables Robust  Sim-to-Real Transfer;"This work explores conditions under which multi-finger grasping algorithms
can attain robust sim-to-real transfer. While numerous large datasets
facilitate learning generative models for multi-finger grasping at scale,
reliable real-world dexterous grasping remains challenging, with most methods
degrading when deployed on hardware. An alternate strategy is to use
discriminative grasp evaluation models for grasp selection and refinement,
conditioned on real-world sensor measurements. This paradigm has produced
state-of-the-art results for vision-based parallel-jaw grasping, but remains
unproven in the multi-finger setting. In this work, we find that existing
datasets and methods have been insufficient for training discriminitive models
for multi-finger grasping. To train grasp evaluators at scale, datasets must
provide on the order of millions of grasps, including both positive and
negative examples, with corresponding visual data resembling measurements at
inference time. To that end, we release a new, open-source dataset of 3.5M
grasps on 4.3K objects annotated with RGB images, point clouds, and trained
NeRFs. Leveraging this dataset, we train vision-based grasp evaluators that
outperform both analytic and generative modeling-based baselines on extensive
simulated and real-world trials across a diverse range of objects. We show via
numerous ablations that the key factor for performance is indeed the evaluator,
and that its quality degrades as the dataset shrinks, demonstrating the
importance of our new dataset. Project website at:
https://sites.google.com/view/get-a-grip-dataset.";Tyler Ga Wei Lum<author:sep>Albert H. Li<author:sep>Preston Culbertson<author:sep>Krishnan Srinivasan<author:sep>Aaron D. Ames<author:sep>Mac Schwager<author:sep>Jeannette Bohg;http://arxiv.org/pdf/2410.23701v1;cs.RO;;nerf
2410.22705v1;http://arxiv.org/abs/2410.22705v1;2024-10-30;Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted  Images;"Single-view 3D reconstruction methods like Triplane Gaussian Splatting (TGS)
have enabled high-quality 3D model generation from just a single image input
within seconds. However, this capability raises concerns about potential
misuse, where malicious users could exploit TGS to create unauthorized 3D
models from copyrighted images. To prevent such infringement, we propose a
novel image protection approach that embeds invisible geometry perturbations,
termed ""geometry cloaks"", into images before supplying them to TGS. These
carefully crafted perturbations encode a customized message that is revealed
when TGS attempts 3D reconstructions of the cloaked image. Unlike conventional
adversarial attacks that simply degrade output quality, our method forces TGS
to fail the 3D reconstruction in a specific way - by generating an identifiable
customized pattern that acts as a watermark. This watermark allows copyright
holders to assert ownership over any attempted 3D reconstructions made from
their protected images. Extensive experiments have verified the effectiveness
of our geometry cloak. Our project is available at
https://qsong2001.github.io/geometry_cloak.";Qi Song<author:sep>Ziyuan Luo<author:sep>Ka Chun Cheung<author:sep>Simon See<author:sep>Renjie Wan;http://arxiv.org/pdf/2410.22705v1;cs.CV;Accepted by NeurIPS 2024;gaussian splatting
2410.23213v1;http://arxiv.org/abs/2410.23213v1;2024-10-30;ELMGS: Enhancing memory and computation scaLability through coMpression  for 3D Gaussian Splatting;"3D models have recently been popularized by the potentiality of end-to-end
training offered first by Neural Radiance Fields and most recently by 3D
Gaussian Splatting models. The latter has the big advantage of naturally
providing fast training convergence and high editability. However, as the
research around these is still in its infancy, there is still a gap in the
literature regarding the model's scalability. In this work, we propose an
approach enabling both memory and computation scalability of such models. More
specifically, we propose an iterative pruning strategy that removes redundant
information encoded in the model. We also enhance compressibility for the model
by including in the optimization strategy a differentiable quantization and
entropy coding estimator. Our results on popular benchmarks showcase the
effectiveness of the proposed approach and open the road to the broad
deployability of such a solution even on resource-constrained devices.";Muhammad Salman Ali<author:sep>Sung-Ho Bae<author:sep>Enzo Tartaglione;http://arxiv.org/pdf/2410.23213v1;cs.CV;;gaussian splatting
2410.22817v2;http://arxiv.org/abs/2410.22817v2;2024-10-30;Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View  Synthesis;"Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from
sparse-view observations in a feed-forward inference manner, eliminating the
need for scene-specific retraining required in conventional 3DGS. However,
existing methods rely heavily on epipolar priors, which can be unreliable in
complex realworld scenes, particularly in non-overlapping and occluded regions.
In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based
model for generalizable novel view synthesis that operates independently of
epipolar line constraints. To enhance multiview feature extraction with 3D
perception, we employ a selfsupervised Vision Transformer (ViT) with cross-view
completion pre-training on large-scale datasets. Additionally, we introduce an
Iterative Cross-view Gaussians Alignment method to ensure consistent depth
scales across different views. Our eFreeSplat represents an innovative approach
for generalizable novel view synthesis. Different from the existing pure
geometry-free methods, eFreeSplat focuses more on achieving epipolar-free
feature matching and encoding by providing 3D priors through cross-view
pretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks
using the RealEstate10K and ACID datasets. Extensive experiments demonstrate
that eFreeSplat surpasses state-of-the-art baselines that rely on epipolar
priors, achieving superior geometry reconstruction and novel view synthesis
quality. Project page: https://tatakai1.github.io/efreesplat/.";Zhiyuan Min<author:sep>Yawei Luo<author:sep>Jianwen Sun<author:sep>Yi Yang;http://arxiv.org/pdf/2410.22817v2;cs.CV;NeurIPS 2024;gaussian splatting
2410.22936v1;http://arxiv.org/abs/2410.22936v1;2024-10-30;Bringing NeRFs to the Latent Space: Inverse Graphics Autoencoder;"While pre-trained image autoencoders are increasingly utilized in computer
vision, the application of inverse graphics in 2D latent spaces has been
under-explored. Yet, besides reducing the training and rendering complexity,
applying inverse graphics in the latent space enables a valuable
interoperability with other latent-based 2D methods. The major challenge is
that inverse graphics cannot be directly applied to such image latent spaces
because they lack an underlying 3D geometry. In this paper, we propose an
Inverse Graphics Autoencoder (IG-AE) that specifically addresses this issue. To
this end, we regularize an image autoencoder with 3D-geometry by aligning its
latent space with jointly trained latent 3D scenes. We utilize the trained
IG-AE to bring NeRFs to the latent space with a latent NeRF training pipeline,
which we implement in an open-source extension of the Nerfstudio framework,
thereby unlocking latent scene learning for its supported methods. We
experimentally confirm that Latent NeRFs trained with IG-AE present an improved
quality compared to a standard autoencoder, all while exhibiting training and
rendering accelerations with respect to NeRFs trained in the image space. Our
project page can be found at https://ig-ae.github.io .";Antoine Schnepf<author:sep>Karim Kassab<author:sep>Jean-Yves Franceschi<author:sep>Laurent Caraffa<author:sep>Flavian Vasile<author:sep>Jeremie Mary<author:sep>Andrew Comport<author:sep>Valerie Gouet-Brunet;http://arxiv.org/pdf/2410.22936v1;cs.CV;;nerf
2410.21955v1;http://arxiv.org/abs/2410.21955v1;2024-10-29;ActiveSplat: High-Fidelity Scene Reconstruction through Active Gaussian  Splatting;"We propose ActiveSplat, an autonomous high-fidelity reconstruction system
leveraging Gaussian splatting. Taking advantage of efficient and realistic
rendering, the system establishes a unified framework for online mapping,
viewpoint selection, and path planning. The key to ActiveSplat is a hybrid map
representation that integrates both dense information about the environment and
a sparse abstraction of the workspace. Therefore, the system leverages sparse
topology for efficient viewpoint sampling and path planning, while exploiting
view-dependent dense prediction for viewpoint selection, facilitating efficient
decision-making with promising accuracy and completeness. A hierarchical
planning strategy based on the topological map is adopted to mitigate
repetitive trajectories and improve local granularity given limited budgets,
ensuring high-fidelity reconstruction with photorealistic view synthesis.
Extensive experiments and ablation studies validate the efficacy of the
proposed method in terms of reconstruction accuracy, data coverage, and
exploration efficiency. Project page: https://li-yuetao.github.io/ActiveSplat/.";Yuetao Li<author:sep>Zijia Kuang<author:sep>Ting Li<author:sep>Guyue Zhou<author:sep>Shaohui Zhang<author:sep>Zike Yan;http://arxiv.org/pdf/2410.21955v1;cs.RO;;gaussian splatting
2410.22070v1;http://arxiv.org/abs/2410.22070v1;2024-10-29;FreeGaussian: Guidance-free Controllable 3D Gaussian Splats with Flow  Derivatives;"Reconstructing controllable Gaussian splats from monocular video is a
challenging task due to its inherently insufficient constraints. Widely adopted
approaches supervise complex interactions with additional masks and control
signal annotations, limiting their real-world applications. In this paper, we
propose an annotation guidance-free method, dubbed FreeGaussian, that
mathematically derives dynamic Gaussian motion from optical flow and camera
motion using novel dynamic Gaussian constraints. By establishing a connection
between 2D flows and 3D Gaussian dynamic control, our method enables
self-supervised optimization and continuity of dynamic Gaussian motions from
flow priors. Furthermore, we introduce a 3D spherical vector controlling
scheme, which represents the state with a 3D Gaussian trajectory, thereby
eliminating the need for complex 1D control signal calculations and simplifying
controllable Gaussian modeling. Quantitative and qualitative evaluations on
extensive experiments demonstrate the state-of-the-art visual performance and
control capability of our method. Project page: https://freegaussian.github.io.";Qizhi Chen<author:sep>Delin Qu<author:sep>Yiwen Tang<author:sep>Haoming Song<author:sep>Yiting Zhang<author:sep>Dong Wang<author:sep>Bin Zhao<author:sep>Xuelong Li;http://arxiv.org/pdf/2410.22070v1;cs.CV;;
2410.22128v1;http://arxiv.org/abs/2410.22128v1;2024-10-29;PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting;"We consider the problem of novel view synthesis from unposed images in a
single feed-forward. Our framework capitalizes on fast speed, scalability, and
high-quality 3D reconstruction and view synthesis capabilities of 3DGS, where
we further extend it to offer a practical solution that relaxes common
assumptions such as dense image views, accurate camera poses, and substantial
image overlaps. We achieve this through identifying and addressing unique
challenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians
across different views induce noisy or sparse gradients that destabilize
training and hinder convergence, especially when above assumptions are not met.
To mitigate this, we employ pre-trained monocular depth estimation and visual
correspondence models to achieve coarse alignments of 3D Gaussians. We then
introduce lightweight, learnable modules to refine depth and pose estimates
from the coarse alignments, improving the quality of 3D reconstruction and
novel view synthesis. Furthermore, the refined estimates are leveraged to
estimate geometry confidence scores, which assess the reliability of 3D
Gaussian centers and condition the prediction of Gaussian parameters
accordingly. Extensive evaluations on large-scale real-world datasets
demonstrate that PF3plat sets a new state-of-the-art across all benchmarks,
supported by comprehensive ablation studies validating our design choices.";Sunghwan Hong<author:sep>Jaewoo Jung<author:sep>Heeseong Shin<author:sep>Jisang Han<author:sep>Jiaolong Yang<author:sep>Chong Luo<author:sep>Seungryong Kim;http://arxiv.org/pdf/2410.22128v1;cs.CV;project page: https://cvlab-kaist.github.io/PF3plat/;gaussian splatting
2410.20723v1;http://arxiv.org/abs/2410.20723v1;2024-10-28;CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via  Dynamically Optimizing 3D Gaussians;"Recent breakthroughs in text-guided image generation have significantly
advanced the field of 3D generation. While generating a single high-quality 3D
object is now feasible, generating multiple objects with reasonable
interactions within a 3D space, a.k.a. compositional 3D generation, presents
substantial challenges. This paper introduces CompGS, a novel generative
framework that employs 3D Gaussian Splatting (GS) for efficient, compositional
text-to-3D content generation. To achieve this goal, two core designs are
proposed: (1) 3D Gaussians Initialization with 2D compositionality: We transfer
the well-established 2D compositionality to initialize the Gaussian parameters
on an entity-by-entity basis, ensuring both consistent 3D priors for each
entity and reasonable interactions among multiple entities; (2) Dynamic
Optimization: We propose a dynamic strategy to optimize 3D Gaussians using
Score Distillation Sampling (SDS) loss. CompGS first automatically decomposes
3D Gaussians into distinct entity parts, enabling optimization at both the
entity and composition levels. Additionally, CompGS optimizes across objects of
varying scales by dynamically adjusting the spatial parameters of each entity,
enhancing the generation of fine-grained details, particularly in smaller
entities. Qualitative comparisons and quantitative evaluations on T3Bench
demonstrate the effectiveness of CompGS in generating compositional 3D objects
with superior image quality and semantic alignment over existing methods.
CompGS can also be easily extended to controllable 3D editing, facilitating
scene generation. We hope CompGS will provide new insights to the compositional
3D generation. Project page: https://chongjiange.github.io/compgs.html.";Chongjian Ge<author:sep>Chenfeng Xu<author:sep>Yuanfeng Ji<author:sep>Chensheng Peng<author:sep>Masayoshi Tomizuka<author:sep>Ping Luo<author:sep>Mingyu Ding<author:sep>Varun Jampani<author:sep>Wei Zhan;http://arxiv.org/pdf/2410.20723v1;cs.CV;;gaussian splatting
2410.20981v2;http://arxiv.org/abs/2410.20981v2;2024-10-28;EEG-Driven 3D Object Reconstruction with Color Consistency and Diffusion  Prior;"EEG-based visual perception reconstruction has become a current research
hotspot. Neuroscientific studies have shown that humans can perceive various
types of visual information, such as color, shape, and texture, when observing
objects. However, existing technical methods often face issues such as
inconsistencies in texture, shape, and color between the visual stimulus images
and the reconstructed images. In this paper, we propose a method for
reconstructing 3D objects with color consistency based on EEG signals. The
method adopts a two-stage strategy: in the first stage, we train an implicit
neural EEG encoder with the capability of perceiving 3D objects, enabling it to
capture regional semantic features; in the second stage, based on the latent
EEG codes obtained in the first stage, we integrate a diffusion model, neural
style loss, and NeRF to implicitly decode the 3D objects. Finally, through
experimental validation, we demonstrate that our method can reconstruct 3D
objects with color consistency using EEG.";Xin Xiang<author:sep>Wenhui Zhou<author:sep>Guojun Dai;http://arxiv.org/pdf/2410.20981v2;cs.CV;;nerf
2410.21566v1;http://arxiv.org/abs/2410.21566v1;2024-10-28;MVSDet: Multi-View Indoor 3D Object Detection via Efficient Plane Sweeps;"The key challenge of multi-view indoor 3D object detection is to infer
accurate geometry information from images for precise 3D detection. Previous
method relies on NeRF for geometry reasoning. However, the geometry extracted
from NeRF is generally inaccurate, which leads to sub-optimal detection
performance. In this paper, we propose MVSDet which utilizes plane sweep for
geometry-aware 3D object detection. To circumvent the requirement for a large
number of depth planes for accurate depth prediction, we design a probabilistic
sampling and soft weighting mechanism to decide the placement of pixel features
on the 3D volume. We select multiple locations that score top in the
probability volume for each pixel and use their probability score to indicate
the confidence. We further apply recent pixel-aligned Gaussian Splatting to
regularize depth prediction and improve detection performance with little
computation overhead. Extensive experiments on ScanNet and ARKitScenes datasets
are conducted to show the superiority of our model. Our code is available at
https://github.com/Pixie8888/MVSDet.";Yating Xu<author:sep>Chen Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2410.21566v1;cs.CV;Accepted by NeurIPS 2024;gaussian splatting<tag:sep>nerf
2410.20815v1;http://arxiv.org/abs/2410.20815v1;2024-10-28;Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic Gaussian  Splatting;"Recently, Gaussian splatting has received more and more attention in the
field of static scene rendering. Due to the low computational overhead and
inherent flexibility of explicit representations, plane-based explicit methods
are popular ways to predict deformations for Gaussian-based dynamic scene
rendering models. However, plane-based methods rely on the inappropriate
low-rank assumption and excessively decompose the space-time 4D encoding,
resulting in overmuch feature overlap and unsatisfactory rendering quality. To
tackle these problems, we propose Grid4D, a dynamic scene rendering model based
on Gaussian splatting and employing a novel explicit encoding method for the 4D
input through the hash encoding. Different from plane-based explicit
representations, we decompose the 4D encoding into one spatial and three
temporal 3D hash encodings without the low-rank assumption. Additionally, we
design a novel attention module that generates the attention scores in a
directional range to aggregate the spatial and temporal features. The
directional attention enables Grid4D to more accurately fit the diverse
deformations across distinct scene components based on the spatial encoded
features. Moreover, to mitigate the inherent lack of smoothness in explicit
representation methods, we introduce a smooth regularization term that keeps
our model from the chaos of deformation prediction. Our experiments demonstrate
that Grid4D significantly outperforms the state-of-the-art models in visual
quality and rendering speed.";Jiawei Xu<author:sep>Zexin Fan<author:sep>Jian Yang<author:sep>Jin Xie;http://arxiv.org/pdf/2410.20815v1;cs.CV;Accepted by NeurIPS 2024;gaussian splatting
2410.20789v1;http://arxiv.org/abs/2410.20789v1;2024-10-28;LoDAvatar: Hierarchical Embedding and Adaptive Levels of Detail with  Gaussian Splatting for Enhanced Human Avatars;"With the advancement of virtual reality, the demand for 3D human avatars is
increasing. The emergence of Gaussian Splatting technology has enabled the
rendering of Gaussian avatars with superior visual quality and reduced
computational costs. Despite numerous methods researchers propose for
implementing drivable Gaussian avatars, limited attention has been given to
balancing visual quality and computational costs. In this paper, we introduce
LoDAvatar, a method that introduces levels of detail into Gaussian avatars
through hierarchical embedding and selective detail enhancement methods. The
key steps of LoDAvatar encompass data preparation, Gaussian embedding, Gaussian
optimization, and selective detail enhancement. We conducted experiments
involving Gaussian avatars at various levels of detail, employing both
objective assessments and subjective evaluations. The outcomes indicate that
incorporating levels of detail into Gaussian avatars can decrease computational
costs during rendering while upholding commendable visual quality, thereby
enhancing runtime frame rates. We advocate adopting LoDAvatar to render
multiple dynamic Gaussian avatars or extensive Gaussian scenes to balance
visual quality and computational costs.";Xiaonuo Dongye<author:sep>Hanzhi Guo<author:sep>Le Luo<author:sep>Haiyan Jiang<author:sep>Yihua Bao<author:sep>Zeyu Tian<author:sep>Dongdong Weng;http://arxiv.org/pdf/2410.20789v1;cs.GR;9 pages, 7 figures, submitted to IEEE VR 2025;gaussian splatting
2410.20686v1;http://arxiv.org/abs/2410.20686v1;2024-10-28;ODGS: 3D Scene Reconstruction from Omnidirectional Images with 3D  Gaussian Splattings;"Omnidirectional (or 360-degree) images are increasingly being used for 3D
applications since they allow the rendering of an entire scene with a single
image. Existing works based on neural radiance fields demonstrate successful 3D
reconstruction quality on egocentric videos, yet they suffer from long training
and rendering times. Recently, 3D Gaussian splatting has gained attention for
its fast optimization and real-time rendering. However, directly using a
perspective rasterizer to omnidirectional images results in severe distortion
due to the different optical properties between two image domains. In this
work, we present ODGS, a novel rasterization pipeline for omnidirectional
images, with geometric interpretation. For each Gaussian, we define a tangent
plane that touches the unit sphere and is perpendicular to the ray headed
toward the Gaussian center. We then leverage a perspective camera rasterizer to
project the Gaussian onto the corresponding tangent plane. The projected
Gaussians are transformed and combined into the omnidirectional image,
finalizing the omnidirectional rasterization process. This interpretation
reveals the implicit assumptions within the proposed pipeline, which we verify
through mathematical proofs. The entire rasterization process is parallelized
using CUDA, achieving optimization and rendering speeds 100 times faster than
NeRF-based methods. Our comprehensive experiments highlight the superiority of
ODGS by delivering the best reconstruction and perceptual quality across
various datasets. Additionally, results on roaming datasets demonstrate that
ODGS restores fine details effectively, even when reconstructing large 3D
scenes. The source code is available on our project page
(https://github.com/esw0116/ODGS).";Suyoung Lee<author:sep>Jaeyoung Chung<author:sep>Jaeyoo Huh<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2410.20686v1;cs.CV;;gaussian splatting<tag:sep>nerf
2410.20306v1;http://arxiv.org/abs/2410.20306v1;2024-10-27;GUMBEL-NERF: Representing Unseen Objects as Part-Compositional Neural  Radiance Fields;"We propose Gumbel-NeRF, a mixture-of-expert (MoE) neural radiance fields
(NeRF) model with a hindsight expert selection mechanism for synthesizing novel
views of unseen objects. Previous studies have shown that the MoE structure
provides high-quality representations of a given large-scale scene consisting
of many objects. However, we observe that such a MoE NeRF model often produces
low-quality representations in the vicinity of experts' boundaries when applied
to the task of novel view synthesis of an unseen object from one/few-shot
input. We find that this deterioration is primarily caused by the foresight
expert selection mechanism, which may leave an unnatural discontinuity in the
object shape near the experts' boundaries. Gumbel-NeRF adopts a hindsight
expert selection mechanism, which guarantees continuity in the density field
even near the experts' boundaries. Experiments using the SRN cars dataset
demonstrate the superiority of Gumbel-NeRF over the baselines in terms of
various image quality metrics.";Yusuke Sekikawa<author:sep>Chingwei Hsu<author:sep>Satoshi Ikehata<author:sep>Rei Kawakami<author:sep>Ikuro Sato;http://arxiv.org/pdf/2410.20306v1;cs.CV;7 pages. Presented at ICIP2024;nerf
2410.20593v1;http://arxiv.org/abs/2410.20593v1;2024-10-27;Normal-GS: 3D Gaussian Splatting with Normal-Involved Rendering;"Rendering and reconstruction are long-standing topics in computer vision and
graphics. Achieving both high rendering quality and accurate geometry is a
challenge. Recent advancements in 3D Gaussian Splatting (3DGS) have enabled
high-fidelity novel view synthesis at real-time speeds. However, the noisy and
discrete nature of 3D Gaussian primitives hinders accurate surface estimation.
Previous attempts to regularize 3D Gaussian normals often degrade rendering
quality due to the fundamental disconnect between normal vectors and the
rendering pipeline in 3DGS-based methods. Therefore, we introduce Normal-GS, a
novel approach that integrates normal vectors into the 3DGS rendering pipeline.
The core idea is to model the interaction between normals and incident lighting
using the physically-based rendering equation. Our approach re-parameterizes
surface colors as the product of normals and a designed Integrated Directional
Illumination Vector (IDIV). To optimize memory usage and simplify optimization,
we employ an anchor-based 3DGS to implicitly encode locally-shared IDIVs.
Additionally, Normal-GS leverages optimized normals and Integrated Directional
Encoding (IDE) to accurately model specular effects, enhancing both rendering
quality and surface normal precision. Extensive experiments demonstrate that
Normal-GS achieves near state-of-the-art visual quality while obtaining
accurate surface normals and preserving real-time rendering performance.";Meng Wei<author:sep>Qianyi Wu<author:sep>Jianmin Zheng<author:sep>Hamid Rezatofighi<author:sep>Jianfei Cai;http://arxiv.org/pdf/2410.20593v1;cs.CV;9 pages, 5 figures, accepted at NeurIPS 2024;gaussian splatting
2410.20220v1;http://arxiv.org/abs/2410.20220v1;2024-10-26;Neural Fields in Robotics: A Survey;"Neural Fields have emerged as a transformative approach for 3D scene
representation in computer vision and robotics, enabling accurate inference of
geometry, 3D semantics, and dynamics from posed 2D data. Leveraging
differentiable rendering, Neural Fields encompass both continuous implicit and
explicit neural representations enabling high-fidelity 3D reconstruction,
integration of multi-modal sensor data, and generation of novel viewpoints.
This survey explores their applications in robotics, emphasizing their
potential to enhance perception, planning, and control. Their compactness,
memory efficiency, and differentiability, along with seamless integration with
foundation and generative models, make them ideal for real-time applications,
improving robot adaptability and decision-making. This paper provides a
thorough review of Neural Fields in robotics, categorizing applications across
various domains and evaluating their strengths and limitations, based on over
200 papers. First, we present four key Neural Fields frameworks: Occupancy
Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian
Splatting. Second, we detail Neural Fields' applications in five major robotics
domains: pose estimation, manipulation, navigation, physics, and autonomous
driving, highlighting key works and discussing takeaways and open challenges.
Finally, we outline the current limitations of Neural Fields in robotics and
propose promising directions for future research. Project page:
https://robonerf.github.io";Muhammad Zubair Irshad<author:sep>Mauro Comi<author:sep>Yen-Chen Lin<author:sep>Nick Heppert<author:sep>Abhinav Valada<author:sep>Rares Ambrus<author:sep>Zsolt Kira<author:sep>Jonathan Tremblay;http://arxiv.org/pdf/2410.20220v1;cs.RO;20 pages, 20 figures. Project Page: https://robonerf.github.io;nerf
2410.19483v1;http://arxiv.org/abs/2410.19483v1;2024-10-25;Content-Aware Radiance Fields: Aligning Model Complexity with Scene  Intricacy Through Learned Bitwidth Quantization;"The recent popular radiance field models, exemplified by Neural Radiance
Fields (NeRF), Instant-NGP and 3D Gaussian Splat?ting, are designed to
represent 3D content by that training models for each individual scene. This
unique characteristic of scene representation and per-scene training
distinguishes radiance field models from other neural models, because complex
scenes necessitate models with higher representational capacity and vice versa.
In this paper, we propose content?aware radiance fields, aligning the model
complexity with the scene intricacies through Adversarial Content-Aware
Quantization (A-CAQ). Specifically, we make the bitwidth of parameters
differentiable and train?able, tailored to the unique characteristics of
specific scenes and requirements. The proposed framework has been assessed on
Instant-NGP, a well-known NeRF variant and evaluated using various datasets.
Experimental results demonstrate a notable reduction in computational
complexity, while preserving the requisite reconstruction and rendering
quality, making it beneficial for practical deployment of radiance fields
models. Codes are available at
https://github.com/WeihangLiu2024/Content_Aware_NeRF.";Weihang Liu<author:sep>Xue Xian Zheng<author:sep>Jingyi Yu<author:sep>Xin Lou;http://arxiv.org/pdf/2410.19483v1;cs.CV;accepted by ECCV2024;nerf
2410.19459v1;http://arxiv.org/abs/2410.19459v1;2024-10-25;Evaluation of strategies for efficient rate-distortion NeRF streaming;"Neural Radiance Fields (NeRF) have revolutionized the field of 3D visual
representation by enabling highly realistic and detailed scene reconstructions
from a sparse set of images. NeRF uses a volumetric functional representation
that maps 3D points to their corresponding colors and opacities, allowing for
photorealistic view synthesis from arbitrary viewpoints. Despite its
advancements, the efficient streaming of NeRF content remains a significant
challenge due to the large amount of data involved. This paper investigates the
rate-distortion performance of two NeRF streaming strategies: pixel-based and
neural network (NN) parameter-based streaming. While in the former, images are
coded and then transmitted throughout the network, in the latter, the
respective NeRF model parameters are coded and transmitted instead. This work
also highlights the trade-offs in complexity and performance, demonstrating
that the NN parameter-based strategy generally offers superior efficiency,
making it suitable for one-to-many streaming scenarios.";Pedro Martin<author:sep>António Rodrigues<author:sep>João Ascenso<author:sep>Maria Paula Queluz;http://arxiv.org/pdf/2410.19459v1;cs.MM;;nerf
2410.21310v1;http://arxiv.org/abs/2410.21310v1;2024-10-25;ArCSEM: Artistic Colorization of SEM Images via Gaussian Splatting;"Scanning Electron Microscopes (SEMs) are widely renowned for their ability to
analyze the surface structures of microscopic objects, offering the capability
to capture highly detailed, yet only grayscale, images. To create more
expressive and realistic illustrations, these images are typically manually
colorized by an artist with the support of image editing software. This task
becomes highly laborious when multiple images of a scanned object require
colorization. We propose facilitating this process by using the underlying 3D
structure of the microscopic scene to propagate the color information to all
the captured images, from as little as one colorized view. We explore several
scene representation techniques and achieve high-quality colorized novel view
synthesis of a SEM scene. In contrast to prior work, there is no manual
intervention or labelling involved in obtaining the 3D representation. This
enables an artist to color a single or few views of a sequence and
automatically retrieve a fully colored scene or video. Project page:
https://ronly2460.github.io/ArCSEM";Takuma Nishimura<author:sep>Andreea Dogaru<author:sep>Martin Oeggerli<author:sep>Bernhard Egger;http://arxiv.org/pdf/2410.21310v1;cs.CV;"presented and published at AI for Visual Arts Workshop and Challenges
  (AI4VA) in conjunction with European Conference on Computer Vision (ECCV)
  2024, Milano, Italy";gaussian splatting
2410.19564v1;http://arxiv.org/abs/2410.19564v1;2024-10-25;Robotic Learning in your Backyard: A Neural Simulator from Open Source  Components;"The emergence of 3D Gaussian Splatting for fast and high-quality novel view
synthesize has opened up the possibility to construct photo-realistic
simulations from video for robotic reinforcement learning. While the approach
has been demonstrated in several research papers, the software tools used to
build such a simulator remain unavailable or proprietary. We present SplatGym,
an open source neural simulator for training data-driven robotic control
policies. The simulator creates a photorealistic virtual environment from a
single video. It supports ego camera view generation, collision detection, and
virtual object in-painting. We demonstrate training several visual navigation
policies via reinforcement learning. SplatGym represents a notable first step
towards an open-source general-purpose neural environment for robotic learning.
It broadens the range of applications that can effectively utilise
reinforcement learning by providing convenient and unrestricted tooling, and by
eliminating the need for the manual development of conventional 3D
environments.";Liyou Zhou<author:sep>Oleg Sinavski<author:sep>Athanasios Polydoros;http://arxiv.org/pdf/2410.19564v1;cs.RO;"Accepted for Oral Presentation at IEEE International Conference on
  Robotic Computing (IRC)";gaussian splatting
2410.19657v2;http://arxiv.org/abs/2410.19657v2;2024-10-25;DiffGS: Functional Gaussian Splatting Diffusion;"3D Gaussian Splatting (3DGS) has shown convincing performance in rendering
speed and fidelity, yet the generation of Gaussian Splatting remains a
challenge due to its discreteness and unstructured nature. In this work, we
propose DiffGS, a general Gaussian generator based on latent diffusion models.
DiffGS is a powerful and efficient 3D generative model which is capable of
generating Gaussian primitives at arbitrary numbers for high-fidelity rendering
with rasterization. The key insight is to represent Gaussian Splatting in a
disentangled manner via three novel functions to model Gaussian probabilities,
colors and transforms. Through the novel disentanglement of 3DGS, we represent
the discrete and unstructured 3DGS with continuous Gaussian Splatting
functions, where we then train a latent diffusion model with the target of
generating these Gaussian Splatting functions both unconditionally and
conditionally. Meanwhile, we introduce a discretization algorithm to extract
Gaussians at arbitrary numbers from the generated functions via octree-guided
sampling and optimization. We explore DiffGS for various tasks, including
unconditional generation, conditional generation from text, image, and partial
3DGS, as well as Point-to-Gaussian generation. We believe that DiffGS provides
a new direction for flexibly modeling and generating Gaussian Splatting.";Junsheng Zhou<author:sep>Weiqi Zhang<author:sep>Yu-Shen Liu;http://arxiv.org/pdf/2410.19657v2;cs.CV;"Accepted by NeurIPS 2024. Project page:
  https://junshengzhou.github.io/DiffGS";gaussian splatting
2410.18355v1;http://arxiv.org/abs/2410.18355v1;2024-10-24;Real-time 3D-aware Portrait Video Relighting;"Synthesizing realistic videos of talking faces under custom lighting
conditions and viewing angles benefits various downstream applications like
video conferencing. However, most existing relighting methods are either
time-consuming or unable to adjust the viewpoints. In this paper, we present
the first real-time 3D-aware method for relighting in-the-wild videos of
talking faces based on Neural Radiance Fields (NeRF). Given an input portrait
video, our method can synthesize talking faces under both novel views and novel
lighting conditions with a photo-realistic and disentangled 3D representation.
Specifically, we infer an albedo tri-plane, as well as a shading tri-plane
based on a desired lighting condition for each video frame with fast
dual-encoders. We also leverage a temporal consistency network to ensure smooth
transitions and reduce flickering artifacts. Our method runs at 32.98 fps on
consumer-level hardware and achieves state-of-the-art results in terms of
reconstruction quality, lighting error, lighting instability, temporal
consistency and inference speed. We demonstrate the effectiveness and
interactivity of our method on various portrait videos with diverse lighting
and viewing conditions.";Ziqi Cai<author:sep>Kaiwen Jiang<author:sep>Shu-Yu Chen<author:sep>Yu-Kun Lai<author:sep>Hongbo Fu<author:sep>Boxin Shi<author:sep>Lin Gao;http://arxiv.org/pdf/2410.18355v1;cs.CV;"Accepted to CVPR 2024 (Highlight). Project page:
  http://geometrylearning.com/VideoRelighting";nerf
2410.18912v1;http://arxiv.org/abs/2410.18912v1;2024-10-24;Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling;"Videos of robots interacting with objects encode rich information about the
objects' dynamics. However, existing video prediction approaches typically do
not explicitly account for the 3D information from videos, such as robot
actions and objects' 3D states, limiting their use in real-world robotic
applications. In this work, we introduce a framework to learn object dynamics
directly from multi-view RGB videos by explicitly considering the robot's
action trajectories and their effects on scene dynamics. We utilize the 3D
Gaussian representation of 3D Gaussian Splatting (3DGS) to train a
particle-based dynamics model using Graph Neural Networks. This model operates
on sparse control particles downsampled from the densely tracked 3D Gaussian
reconstructions. By learning the neural dynamics model on offline robot
interaction data, our method can predict object motions under varying initial
configurations and unseen robot actions. The 3D transformations of Gaussians
can be interpolated from the motions of control particles, enabling the
rendering of predicted future object states and achieving action-conditioned
video prediction. The dynamics model can also be applied to model-based
planning frameworks for object manipulation tasks. We conduct experiments on
various kinds of deformable materials, including ropes, clothes, and stuffed
animals, demonstrating our framework's ability to model complex shapes and
dynamics. Our project page is available at https://gs-dynamics.github.io.";Mingtong Zhang<author:sep>Kaifeng Zhang<author:sep>Yunzhu Li;http://arxiv.org/pdf/2410.18912v1;cs.RO;Project Page: https://gs-dynamics.github.io;gaussian splatting
2410.18974v1;http://arxiv.org/abs/2410.18974v1;2024-10-24;3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D  Generation;"Multi-view image diffusion models have significantly advanced open-domain 3D
object generation. However, most existing models rely on 2D network
architectures that lack inherent 3D biases, resulting in compromised geometric
consistency. To address this challenge, we introduce 3D-Adapter, a plug-in
module designed to infuse 3D geometry awareness into pretrained image diffusion
models. Central to our approach is the idea of 3D feedback augmentation: for
each denoising step in the sampling loop, 3D-Adapter decodes intermediate
multi-view features into a coherent 3D representation, then re-encodes the
rendered RGBD views to augment the pretrained base model through feature
addition. We study two variants of 3D-Adapter: a fast feed-forward version
based on Gaussian splatting and a versatile training-free version utilizing
neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter
not only greatly enhances the geometry quality of text-to-multi-view models
such as Instant3D and Zero123++, but also enables high-quality 3D generation
using the plain text-to-image Stable Diffusion. Furthermore, we showcase the
broad application potential of 3D-Adapter by presenting high quality results in
text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.";Hansheng Chen<author:sep>Bokui Shen<author:sep>Yulin Liu<author:sep>Ruoxi Shi<author:sep>Linqi Zhou<author:sep>Connor Z. Lin<author:sep>Jiayuan Gu<author:sep>Hao Su<author:sep>Gordon Wetzstein<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2410.18974v1;cs.CV;Project page: https://lakonik.github.io/3d-adapter/;gaussian splatting
2410.18822v2;http://arxiv.org/abs/2410.18822v2;2024-10-24;Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse  View Synthesis;"Novel view synthesis from sparse inputs is a vital yet challenging task in 3D
computer vision. Previous methods explore 3D Gaussian Splatting with neural
priors (e.g. depth priors) as an additional supervision, demonstrating
promising quality and efficiency compared to the NeRF based methods. However,
the neural priors from 2D pretrained models are often noisy and blurry, which
struggle to precisely guide the learning of radiance fields. In this paper, We
propose a novel method for synthesizing novel views from sparse views with
Gaussian Splatting that does not require external prior as supervision. Our key
idea lies in exploring the self-supervisions inherent in the binocular stereo
consistency between each pair of binocular images constructed with
disparity-guided image warping. To this end, we additionally introduce a
Gaussian opacity constraint which regularizes the Gaussian locations and avoids
Gaussian redundancy for improving the robustness and efficiency of inferring 3D
Gaussians from sparse views. Extensive experiments on the LLFF, DTU, and
Blender datasets demonstrate that our method significantly outperforms the
state-of-the-art methods.";Liang Han<author:sep>Junsheng Zhou<author:sep>Yu-Shen Liu<author:sep>Zhizhong Han;http://arxiv.org/pdf/2410.18822v2;cs.CV;"Accepted by NeurIPS 2024. Project page:
  https://hanl2010.github.io/Binocular3DGS/";gaussian splatting<tag:sep>nerf
2410.18931v1;http://arxiv.org/abs/2410.18931v1;2024-10-24;Sort-free Gaussian Splatting via Weighted Sum Rendering;"Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant
advancement in 3D scene reconstruction, attracting considerable attention due
to its ability to recover high-fidelity details while maintaining low
complexity. Despite the promising results achieved by 3DGS, its rendering
performance is constrained by its dependence on costly non-commutative
alpha-blending operations. These operations mandate complex view dependent
sorting operations that introduce computational overhead, especially on the
resource-constrained platforms such as mobile phones. In this paper, we propose
Weighted Sum Rendering, which approximates alpha blending with weighted sums,
thereby removing the need for sorting. This simplifies implementation, delivers
superior performance, and eliminates the ""popping"" artifacts caused by sorting.
Experimental results show that optimizing a generalized Gaussian splatting
formulation to the new differentiable rendering yields competitive image
quality. The method was implemented and tested in a mobile device GPU,
achieving on average $1.23\times$ faster rendering.";Qiqi Hou<author:sep>Randall Rauwendaal<author:sep>Zifeng Li<author:sep>Hoang Le<author:sep>Farzad Farhadzadeh<author:sep>Fatih Porikli<author:sep>Alexei Bourd<author:sep>Amir Said;http://arxiv.org/pdf/2410.18931v1;cs.CV;;gaussian splatting
2410.17932v1;http://arxiv.org/abs/2410.17932v1;2024-10-23;VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian  Splatting and Neural Points;"Recent advances in novel view synthesis (NVS), particularly neural radiance
fields (NeRF) and Gaussian splatting (3DGS), have demonstrated impressive
results in photorealistic scene rendering. These techniques hold great
potential for applications in virtual tourism and teleportation, where
immersive realism is crucial. However, the high-performance demands of virtual
reality (VR) systems present challenges in directly utilizing even such
fast-to-render scene representations like 3DGS due to latency and computational
constraints.
  In this paper, we propose foveated rendering as a promising solution to these
obstacles. We analyze state-of-the-art NVS methods with respect to their
rendering performance and compatibility with the human visual system. Our
approach introduces a novel foveated rendering approach for Virtual Reality,
that leverages the sharp, detailed output of neural point rendering for the
foveal region, fused with a smooth rendering of 3DGS for the peripheral vision.
  Our evaluation confirms that perceived sharpness and detail-richness are
increased by our approach compared to a standard VR-ready 3DGS configuration.
Our system meets the necessary performance requirements for real-time VR
interactions, ultimately enhancing the user's immersive experience.
  Project page: https://lfranke.github.io/vr_splatting";Linus Franke<author:sep>Laura Fink<author:sep>Marc Stamminger;http://arxiv.org/pdf/2410.17932v1;cs.CV;;gaussian splatting<tag:sep>nerf
2410.17839v1;http://arxiv.org/abs/2410.17839v1;2024-10-23;Few-shot NeRF by Adaptive Rendering Loss Regularization;"Novel view synthesis with sparse inputs poses great challenges to Neural
Radiance Field (NeRF). Recent works demonstrate that the frequency
regularization of Positional Encoding (PE) can achieve promising results for
few-shot NeRF. In this work, we reveal that there exists an inconsistency
between the frequency regularization of PE and rendering loss. This prevents
few-shot NeRF from synthesizing higher-quality novel views. To mitigate this
inconsistency, we propose Adaptive Rendering loss regularization for few-shot
NeRF, dubbed AR-NeRF. Specifically, we present a two-phase rendering
supervision and an adaptive rendering loss weight learning strategy to align
the frequency relationship between PE and 2D-pixel supervision. In this way,
AR-NeRF can learn global structures better in the early training phase and
adaptively learn local details throughout the training process. Extensive
experiments show that our AR-NeRF achieves state-of-the-art performance on
different datasets, including object-level and complex scenes.";Qingshan Xu<author:sep>Xuanyu Yi<author:sep>Jianyao Xu<author:sep>Wenbing Tao<author:sep>Yew-Soon Ong<author:sep>Hanwang Zhang;http://arxiv.org/pdf/2410.17839v1;cs.CV;Accepted by ECCV2024;nerf
2410.17505v1;http://arxiv.org/abs/2410.17505v1;2024-10-23;PLGS: Robust Panoptic Lifting with 3D Gaussian Splatting;"Previous methods utilize the Neural Radiance Field (NeRF) for panoptic
lifting, while their training and rendering speed are unsatisfactory. In
contrast, 3D Gaussian Splatting (3DGS) has emerged as a prominent technique due
to its rapid training and rendering speed. However, unlike NeRF, the
conventional 3DGS may not satisfy the basic smoothness assumption as it does
not rely on any parameterized structures to render (e.g., MLPs). Consequently,
the conventional 3DGS is, in nature, more susceptible to noisy 2D mask
supervision. In this paper, we propose a new method called PLGS that enables
3DGS to generate consistent panoptic segmentation masks from noisy 2D
segmentation masks while maintaining superior efficiency compared to NeRF-based
methods. Specifically, we build a panoptic-aware structured 3D Gaussian model
to introduce smoothness and design effective noise reduction strategies. For
the semantic field, instead of initialization with structure from motion, we
construct reliable semantic anchor points to initialize the 3D Gaussians. We
then use these anchor points as smooth regularization during training.
Additionally, we present a self-training approach using pseudo labels generated
by merging the rendered masks with the noisy masks to enhance the robustness of
PLGS. For the instance field, we project the 2D instance masks into 3D space
and match them with oriented bounding boxes to generate cross-view consistent
instance masks for supervision. Experiments on various benchmarks demonstrate
that our method outperforms previous state-of-the-art methods in terms of both
segmentation quality and speed.";Yu Wang<author:sep>Xiaobao Wei<author:sep>Ming Lu<author:sep>Guoliang Kang;http://arxiv.org/pdf/2410.17505v1;cs.CV;;gaussian splatting<tag:sep>nerf
2410.17741v1;http://arxiv.org/abs/2410.17741v1;2024-10-23;Efficient Neural Implicit Representation for 3D Human Reconstruction;"High-fidelity digital human representations are increasingly in demand in the
digital world, particularly for interactive telepresence, AR/VR, 3D graphics,
and the rapidly evolving metaverse. Even though they work well in small spaces,
conventional methods for reconstructing 3D human motion frequently require the
use of expensive hardware and have high processing costs. This study presents
HumanAvatar, an innovative approach that efficiently reconstructs precise human
avatars from monocular video sources. At the core of our methodology, we
integrate the pre-trained HuMoR, a model celebrated for its proficiency in
human motion estimation. This is adeptly fused with the cutting-edge neural
radiance field technology, Instant-NGP, and the state-of-the-art articulated
model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By
combining these two technologies, a system is created that can render quickly
and effectively while also providing estimation of human pose parameters that
are unmatched in accuracy. We have enhanced our system with an advanced
posture-sensitive space reduction technique, which optimally balances rendering
quality with computational efficiency. In our detailed experimental analysis
using both artificial and real-world monocular videos, we establish the
advanced performance of our approach. HumanAvatar consistently equals or
surpasses contemporary leading-edge reconstruction techniques in quality.
Furthermore, it achieves these complex reconstructions in minutes, a fraction
of the time typically required by existing methods. Our models achieve a
training speed that is 110X faster than that of State-of-The-Art (SoTA)
NeRF-based models. Our technique performs noticeably better than SoTA dynamic
human NeRF methods if given an identical runtime limit. HumanAvatar can provide
effective visuals after only 30 seconds of training.";Zexu Huang<author:sep>Sarah Monazam Erfani<author:sep>Siying Lu<author:sep>Mingming Gong;http://arxiv.org/pdf/2410.17741v1;cs.CV;;nerf
2410.17242v1;http://arxiv.org/abs/2410.17242v1;2024-10-22;LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias;"We propose the Large View Synthesis Model (LVSM), a novel transformer-based
approach for scalable and generalizable novel view synthesis from sparse-view
inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which
encodes input image tokens into a fixed number of 1D latent tokens, functioning
as a fully learned scene representation, and decodes novel-view images from
them; and (2) a decoder-only LVSM, which directly maps input images to
novel-view outputs, completely eliminating intermediate scene representations.
Both models bypass the 3D inductive biases used in previous methods -- from 3D
representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar
projections, plane sweeps) -- addressing novel view synthesis with a fully
data-driven approach. While the encoder-decoder model offers faster inference
due to its independent latent representation, the decoder-only LVSM achieves
superior quality, scalability, and zero-shot generalization, outperforming
previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive
evaluations across multiple datasets demonstrate that both LVSM variants
achieve state-of-the-art novel view synthesis quality. Notably, our models
surpass all previous methods even with reduced computational resources (1-2
GPUs). Please see our website for more details:
https://haian-jin.github.io/projects/LVSM/ .";Haian Jin<author:sep>Hanwen Jiang<author:sep>Hao Tan<author:sep>Kai Zhang<author:sep>Sai Bi<author:sep>Tianyuan Zhang<author:sep>Fujun Luan<author:sep>Noah Snavely<author:sep>Zexiang Xu;http://arxiv.org/pdf/2410.17242v1;cs.CV;project page: https://haian-jin.github.io/projects/LVSM/;nerf
2410.16995v1;http://arxiv.org/abs/2410.16995v1;2024-10-22;E-3DGS: Gaussian Splatting with Exposure and Motion Events;"Estimating Neural Radiance Fields (NeRFs) from images captured under optimal
conditions has been extensively explored in the vision community. However,
robotic applications often face challenges such as motion blur, insufficient
illumination, and high computational overhead, which adversely affect
downstream tasks like navigation, inspection, and scene visualization. To
address these challenges, we propose E-3DGS, a novel event-based approach that
partitions events into motion (from camera or object movement) and exposure
(from camera exposure), using the former to handle fast-motion scenes and using
the latter to reconstruct grayscale images for high-quality training and
optimization of event-based 3D Gaussian Splatting (3DGS). We introduce a novel
integration of 3DGS with exposure events for high-quality reconstruction of
explicit scene representations. Our versatile framework can operate on motion
events alone for 3D reconstruction, enhance quality using exposure events, or
adopt a hybrid mode that balances quality and effectiveness by optimizing with
initial exposure events followed by high-speed motion events. We also introduce
EME-3D, a real-world 3D dataset with exposure events, motion events, camera
calibration parameters, and sparse point clouds. Our method is faster and
delivers better reconstruction quality than event-based NeRF while being more
cost-effective than NeRF methods that combine event and RGB data by using a
single event sensor. By combining motion and exposure events, E-3DGS sets a new
benchmark for event-based 3D reconstruction with robust performance in
challenging conditions and lower hardware demands. The source code and dataset
will be available at https://github.com/MasterHow/E-3DGS.";Xiaoting Yin<author:sep>Hao Shi<author:sep>Yuhan Bao<author:sep>Zhenshan Bing<author:sep>Yiyi Liao<author:sep>Kailun Yang<author:sep>Kaiwei Wang;http://arxiv.org/pdf/2410.16995v1;cs.CV;"The source code and dataset will be available at
  https://github.com/MasterHow/E-3DGS";gaussian splatting<tag:sep>nerf
2410.17249v1;http://arxiv.org/abs/2410.17249v1;2024-10-22;SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes;"We present SpectroMotion, a novel approach that combines 3D Gaussian
Splatting (3DGS) with physically-based rendering (PBR) and deformation fields
to reconstruct dynamic specular scenes. Previous methods extending 3DGS to
model dynamic scenes have struggled to accurately represent specular surfaces.
Our method addresses this limitation by introducing a residual correction
technique for accurate surface normal computation during deformation,
complemented by a deformable environment map that adapts to time-varying
lighting conditions. We implement a coarse-to-fine training strategy that
significantly enhances both scene geometry and specular color prediction. We
demonstrate that our model outperforms prior methods for view synthesis of
scenes containing dynamic specular objects and that it is the only existing
3DGS method capable of synthesizing photorealistic real-world dynamic specular
scenes, outperforming state-of-the-art methods in rendering complex, dynamic,
and specular scenes.";Cheng-De Fan<author:sep>Chen-Wei Chang<author:sep>Yi-Ruei Liu<author:sep>Jie-Ying Lee<author:sep>Jiun-Long Huang<author:sep>Yu-Chee Tseng<author:sep>Yu-Lun Liu;http://arxiv.org/pdf/2410.17249v1;cs.CV;Project page: https://cdfan0627.github.io/spectromotion/;
2410.18137v1;http://arxiv.org/abs/2410.18137v1;2024-10-22;Advancing Super-Resolution in Neural Radiance Fields via Variational  Diffusion Strategies;"We present a novel method for diffusion-guided frameworks for view-consistent
super-resolution (SR) in neural rendering. Our approach leverages existing 2D
SR models in conjunction with advanced techniques such as Variational Score
Distilling (VSD) and a LoRA fine-tuning helper, with spatial training to
significantly boost the quality and consistency of upscaled 2D images compared
to the previous methods in the literature, such as Renoised Score Distillation
(RSD) proposed in DiSR-NeRF (1), or SDS proposed in DreamFusion. The VSD score
facilitates precise fine-tuning of SR models, resulting in high-quality,
view-consistent images. To address the common challenge of inconsistencies
among independent SR 2D images, we integrate Iterative 3D Synchronization
(I3DS) from the DiSR-NeRF framework. Our quantitative benchmarks and
qualitative results on the LLFF dataset demonstrate the superior performance of
our system compared to existing methods such as DiSR-NeRF.";Shrey Vishen<author:sep>Jatin Sarabu<author:sep>Chinmay Bharathulwar<author:sep>Rithwick Lakshmanan<author:sep>Vishnu Srinivas;http://arxiv.org/pdf/2410.18137v1;cs.CV;"All our code is available at
  https://github.com/shreyvish5678/Advancing-Super-Resolution-in-Neural-Radiance-Fields-via-Variational-Diffusion-Strategies";nerf
2410.17422v1;http://arxiv.org/abs/2410.17422v1;2024-10-22;AG-SLAM: Active Gaussian Splatting SLAM;"We present AG-SLAM, the first active SLAM system utilizing 3D Gaussian
Splatting (3DGS) for online scene reconstruction. In recent years, radiance
field scene representations, including 3DGS have been widely used in SLAM and
exploration, but actively planning trajectories for robotic exploration is
still unvisited. In particular, many exploration methods assume precise
localization and thus do not mitigate the significant risk of constructing a
trajectory, which is difficult for a SLAM system to operate on. This can cause
camera tracking failure and lead to failures in real-world robotic
applications. Our method leverages Fisher Information to balance the dual
objectives of maximizing the information gain for the environment while
minimizing the cost of localization errors. Experiments conducted on the Gibson
and Habitat-Matterport 3D datasets demonstrate state-of-the-art results of the
proposed method.";Wen Jiang<author:sep>Boshu Lei<author:sep>Katrina Ashton<author:sep>Kostas Daniilidis;http://arxiv.org/pdf/2410.17422v1;cs.RO;;gaussian splatting
2410.16978v1;http://arxiv.org/abs/2410.16978v1;2024-10-22;Multi-Layer Gaussian Splatting for Immersive Anatomy Visualization;"In medical image visualization, path tracing of volumetric medical data like
CT scans produces lifelike three-dimensional visualizations. Immersive VR
displays can further enhance the understanding of complex anatomies. Going
beyond the diagnostic quality of traditional 2D slices, they enable interactive
3D evaluation of anatomies, supporting medical education and planning.
Rendering high-quality visualizations in real-time, however, is computationally
intensive and impractical for compute-constrained devices like mobile headsets.
  We propose a novel approach utilizing GS to create an efficient but static
intermediate representation of CT scans. We introduce a layered GS
representation, incrementally including different anatomical structures while
minimizing overlap and extending the GS training to remove inactive Gaussians.
We further compress the created model with clustering across layers.
  Our approach achieves interactive frame rates while preserving anatomical
structures, with quality adjustable to the target hardware. Compared to
standard GS, our representation retains some of the explorative qualities
initially enabled by immersive path tracing. Selective activation and clipping
of layers are possible at rendering time, adding a degree of interactivity to
otherwise static GS models. This could enable scenarios where high
computational demands would otherwise prohibit using path-traced medical
volumes.";Constantin Kleinbeck<author:sep>Hannah Schieber<author:sep>Klaus Engel<author:sep>Ralf Gutjahr<author:sep>Daniel Roth;http://arxiv.org/pdf/2410.16978v1;cs.GR;;gaussian splatting
2410.15629v2;http://arxiv.org/abs/2410.15629v2;2024-10-21;Fully Explicit Dynamic Gaussian Splatting;"3D Gaussian Splatting has shown fast and high-quality rendering results in
static scenes by leveraging dense 3D prior and explicit representations.
Unfortunately, the benefits of the prior and representation do not involve
novel view synthesis for dynamic motions. Ironically, this is because the main
barrier is the reliance on them, which requires increasing training and
rendering times to account for dynamic motions. In this paper, we design a
Explicit 4D Gaussian Splatting(Ex4DGS). Our key idea is to firstly separate
static and dynamic Gaussians during training, and to explicitly sample
positions and rotations of the dynamic Gaussians at sparse timestamps. The
sampled positions and rotations are then interpolated to represent both
spatially and temporally continuous motions of objects in dynamic scenes as
well as reducing computational cost. Additionally, we introduce a progressive
training scheme and a point-backtracking technique that improves Ex4DGS's
convergence. We initially train Ex4DGS using short timestamps and progressively
extend timestamps, which makes it work well with a few point clouds. The
point-backtracking is used to quantify the cumulative error of each Gaussian
over time, enabling the detection and removal of erroneous Gaussians in dynamic
scenes. Comprehensive experiments on various scenes demonstrate the
state-of-the-art rendering quality from our method, achieving fast rendering of
62 fps on a single 2080Ti GPU.";Junoh Lee<author:sep>Chang-Yeon Won<author:sep>Hyunjun Jung<author:sep>Inhwan Bae<author:sep>Hae-Gon Jeon;http://arxiv.org/pdf/2410.15629v2;cs.CV;Accepted at NeurIPS 2024;gaussian splatting
2410.16395v1;http://arxiv.org/abs/2410.16395v1;2024-10-21;Joker: Conditional 3D Head Synthesis with Extreme Facial Expressions;"We introduce Joker, a new method for the conditional synthesis of 3D human
heads with extreme expressions. Given a single reference image of a person, we
synthesize a volumetric human head with the reference identity and a new
expression. We offer control over the expression via a 3D morphable model
(3DMM) and textual inputs. This multi-modal conditioning signal is essential
since 3DMMs alone fail to define subtle emotional changes and extreme
expressions, including those involving the mouth cavity and tongue
articulation. Our method is built upon a 2D diffusion-based prior that
generalizes well to out-of-domain samples, such as sculptures, heavy makeup,
and paintings while achieving high levels of expressiveness. To improve view
consistency, we propose a new 3D distillation technique that converts
predictions of our 2D prior into a neural radiance field (NeRF). Both the 2D
prior and our distillation technique produce state-of-the-art results, which
are confirmed by our extensive evaluations. Also, to the best of our knowledge,
our method is the first to achieve view-consistent extreme tongue articulation.";Malte Prinzler<author:sep>Egor Zakharov<author:sep>Vanessa Sklyarova<author:sep>Berna Kabadayi<author:sep>Justus Thies;http://arxiv.org/pdf/2410.16395v1;cs.CV;Project Page: https://malteprinzler.github.io/projects/joker/;nerf
2410.16271v1;http://arxiv.org/abs/2410.16271v1;2024-10-21;FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without  Learned Priors;"Neural Radiance Fields (NeRF) face significant challenges in few-shot
scenarios, primarily due to overfitting and long training times for
high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use
frequency regularization or pre-trained priors but struggle with complex
scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework
that leverages weight-sharing voxels across multiple scales to efficiently
represent scene details. Our key contribution is a cross-scale geometric
adaptation scheme that selects pseudo ground truth depth based on reprojection
errors across scales. This guides training without relying on externally
learned priors, enabling full utilization of the training data. It can also
integrate pre-trained priors, enhancing quality without slowing convergence.
Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms
other few-shot NeRF methods while significantly reducing training time, making
it a practical solution for efficient and accurate 3D scene reconstruction.";Chin-Yang Lin<author:sep>Chung-Ho Wu<author:sep>Chang-Han Yeh<author:sep>Shih-Han Yen<author:sep>Cheng Sun<author:sep>Yu-Lun Liu;http://arxiv.org/pdf/2410.16271v1;cs.CV;Project page: https://linjohnss.github.io/frugalnerf/;nerf
2410.16266v1;http://arxiv.org/abs/2410.16266v1;2024-10-21;3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with  View-consistent 2D Diffusion Priors;"Novel-view synthesis aims to generate novel views of a scene from multiple
input images or videos, and recent advancements like 3D Gaussian splatting
(3DGS) have achieved notable success in producing photorealistic renderings
with efficient pipelines. However, generating high-quality novel views under
challenging settings, such as sparse input views, remains difficult due to
insufficient information in under-sampled areas, often resulting in noticeable
artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing
the representation quality of 3DGS representations. We leverage 2D video
diffusion priors to address the challenging 3D view consistency problem,
reformulating it as achieving temporal consistency within a video generation
process. 3DGS-Enhancer restores view-consistent latent features of rendered
novel views and integrates them with the input views through a spatial-temporal
decoder. The enhanced views are then used to fine-tune the initial 3DGS model,
significantly improving its rendering performance. Extensive experiments on
large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields
superior reconstruction performance and high-fidelity rendering results
compared to state-of-the-art methods. The project webpage is
https://xiliu8006.github.io/3DGS-Enhancer-project .";Xi Liu<author:sep>Chaoyi Zhou<author:sep>Siyu Huang;http://arxiv.org/pdf/2410.16266v1;cs.CV;Accepted by NeurIPS 2024 Spotlight;gaussian splatting
2410.15730v1;http://arxiv.org/abs/2410.15730v1;2024-10-21;MSGField: A Unified Scene Representation Integrating Motion, Semantics,  and Geometry for Robotic Manipulation;"Combining accurate geometry with rich semantics has been proven to be highly
effective for language-guided robotic manipulation. Existing methods for
dynamic scenes either fail to update in real-time or rely on additional depth
sensors for simple scene editing, limiting their applicability in real-world.
In this paper, we introduce MSGField, a representation that uses a collection
of 2D Gaussians for high-quality reconstruction, further enhanced with
attributes to encode semantic and motion information. Specially, we represent
the motion field compactly by decomposing each primitive's motion into a
combination of a limited set of motion bases. Leveraging the differentiable
real-time rendering of Gaussian splatting, we can quickly optimize object
motion, even for complex non-rigid motions, with image supervision from only
two camera views. Additionally, we designed a pipeline that utilizes object
priors to efficiently obtain well-defined semantics. In our challenging
dataset, which includes flexible and extremely small objects, our method
achieve a success rate of 79.2% in static and 63.3% in dynamic environments for
language-guided manipulation. For specified object grasping, we achieve a
success rate of 90%, on par with point cloud-based methods. Code and dataset
will be released at:https://shengyu724.github.io/MSGField.github.io.";Yu Sheng<author:sep>Runfeng Lin<author:sep>Lidian Wang<author:sep>Quecheng Qiu<author:sep>YanYong Zhang<author:sep>Yu Zhang<author:sep>Bei Hua<author:sep>Jianmin Ji;http://arxiv.org/pdf/2410.15730v1;cs.RO;;gaussian splatting
2410.15392v2;http://arxiv.org/abs/2410.15392v2;2024-10-20;EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting;"Scene reconstruction from casually captured videos has wide applications in
real-world scenarios. With recent advancements in differentiable rendering
techniques, several methods have attempted to simultaneously optimize scene
representations (NeRF or 3DGS) and camera poses. Despite recent progress,
existing methods relying on traditional camera input tend to fail in high-speed
(or equivalently low-frame-rate) scenarios. Event cameras, inspired by
biological vision, record pixel-wise intensity changes asynchronously with high
temporal resolution, providing valuable scene and motion information in blind
inter-frame intervals. In this paper, we introduce the event camera to aid
scene construction from a casually captured video for the first time, and
propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly
integrates the advantages of event cameras into 3DGS through three key
components. First, we leverage the Event Generation Model (EGM) to fuse events
and frames, supervising the rendered views observed by the event stream.
Second, we adopt the Contrast Maximization (CMax) framework in a piece-wise
manner to extract motion information by maximizing the contrast of the Image of
Warped Events (IWE), thereby calibrating the estimated poses. Besides, based on
the Linear Event Generation Model (LEGM), the brightness information encoded in
the IWE is also utilized to constrain the 3DGS in the gradient domain. Third,
to mitigate the absence of color information of events, we introduce
photometric bundle adjustment (PBA) to ensure view consistency across events
and frames. We evaluate our method on the public Tanks and Temples benchmark
and a newly collected real-world dataset, RealEv-DAVIS. Our project page is
https://lbh666.github.io/ef-3dgs/.";Bohao Liao<author:sep>Wei Zhai<author:sep>Zengyu Wan<author:sep>Tianzhu Zhang<author:sep>Yang Cao<author:sep>Zheng-Jun Zha;http://arxiv.org/pdf/2410.15392v2;cs.CV;Project Page: https://lbh666.github.io/ef-3dgs/;gaussian splatting<tag:sep>nerf
2410.14958v1;http://arxiv.org/abs/2410.14958v1;2024-10-19;Neural Radiance Field Image Refinement through End-to-End Sampling Point  Optimization;"Neural Radiance Field (NeRF), capable of synthesizing high-quality novel
viewpoint images, suffers from issues like artifact occurrence due to its fixed
sampling points during rendering. This study proposes a method that optimizes
sampling points to reduce artifacts and produce more detailed images.";Kazuhiro Ohta<author:sep>Satoshi Ono;http://arxiv.org/pdf/2410.14958v1;cs.CV;;nerf
2410.17084v1;http://arxiv.org/abs/2410.17084v1;2024-10-18;GS-LIVM: Real-Time Photo-Realistic LiDAR-Inertial-Visual Mapping with  Gaussian Splatting;"In this paper, we introduce GS-LIVM, a real-time photo-realistic
LiDAR-Inertial-Visual mapping framework with Gaussian Splatting tailored for
outdoor scenes. Compared to existing methods based on Neural Radiance Fields
(NeRF) and 3D Gaussian Splatting (3DGS), our approach enables real-time
photo-realistic mapping while ensuring high-quality image rendering in
large-scale unbounded outdoor environments. In this work, Gaussian Process
Regression (GPR) is employed to mitigate the issues resulting from sparse and
unevenly distributed LiDAR observations. The voxel-based 3D Gaussians map
representation facilitates real-time dense mapping in large outdoor
environments with acceleration governed by custom CUDA kernels. Moreover, the
overall framework is designed in a covariance-centered manner, where the
estimated covariance is used to initialize the scale and rotation of 3D
Gaussians, as well as update the parameters of the GPR. We evaluate our
algorithm on several outdoor datasets, and the results demonstrate that our
method achieves state-of-the-art performance in terms of mapping efficiency and
rendering quality. The source code is available on GitHub.";Yusen Xie<author:sep>Zhenmin Huang<author:sep>Jin Wu<author:sep>Jun Ma;http://arxiv.org/pdf/2410.17084v1;cs.RO;15 pages, 13 figures;gaussian splatting<tag:sep>nerf
2410.14177v1;http://arxiv.org/abs/2410.14177v1;2024-10-18;Learning autonomous driving from aerial imagery;"In this work, we consider the problem of learning end to end perception to
control for ground vehicles solely from aerial imagery. Photogrammetric
simulators allow the synthesis of novel views through the transformation of
pre-generated assets into novel views.However, they have a large setup cost,
require careful collection of data and often human effort to create usable
simulators. We use a Neural Radiance Field (NeRF) as an intermediate
representation to synthesize novel views from the point of view of a ground
vehicle. These novel viewpoints can then be used for several downstream
autonomous navigation applications. In this work, we demonstrate the utility of
novel view synthesis though the application of training a policy for end to end
learning from images and depth data. In a traditional real to sim to real
framework, the collected data would be transformed into a visual simulator
which could then be used to generate novel views. In contrast, using a NeRF
allows a compact representation and the ability to optimize over the parameters
of the visual simulator as more data is gathered in the environment. We
demonstrate the efficacy of our method in a custom built mini-city environment
through the deployment of imitation policies on robotic cars. We additionally
consider the task of place localization and demonstrate that our method is able
to relocalize the car in the real world.";Varun Murali<author:sep>Guy Rosman<author:sep>Sertac Karaman<author:sep>Daniela Rus;http://arxiv.org/pdf/2410.14177v1;cs.RO;Presented at IROS 2024;nerf
2410.14169v1;http://arxiv.org/abs/2410.14169v1;2024-10-18;DaRePlane: Direction-aware Representations for Dynamic Scene  Reconstruction;"Numerous recent approaches to modeling and re-rendering dynamic scenes
leverage plane-based explicit representations, addressing slow training times
associated with models like neural radiance fields (NeRF) and Gaussian
splatting (GS). However, merely decomposing 4D dynamic scenes into multiple 2D
plane-based representations is insufficient for high-fidelity re-rendering of
scenes with complex motions. In response, we present DaRePlane, a novel
direction-aware representation approach that captures scene dynamics from six
different directions. This learned representation undergoes an inverse
dual-tree complex wavelet transformation (DTCWT) to recover plane-based
information. Within NeRF pipelines, DaRePlane computes features for each
space-time point by fusing vectors from these recovered planes, then passed to
a tiny MLP for color regression. When applied to Gaussian splatting, DaRePlane
computes the features of Gaussian points, followed by a tiny multi-head MLP for
spatial-time deformation prediction. Notably, to address redundancy introduced
by the six real and six imaginary direction-aware wavelet coefficients, we
introduce a trainable masking approach, mitigating storage issues without
significant performance decline. To demonstrate the generality and efficiency
of DaRePlane, we test it on both regular and surgical dynamic scenes, for both
NeRF and GS systems. Extensive experiments show that DaRePlane yields
state-of-the-art performance in novel view synthesis for various complex
dynamic scenes.";Ange Lou<author:sep>Benjamin Planche<author:sep>Zhongpai Gao<author:sep>Yamin Li<author:sep>Tianyu Luan<author:sep>Hao Ding<author:sep>Meng Zheng<author:sep>Terrence Chen<author:sep>Ziyan Wu<author:sep>Jack Noble;http://arxiv.org/pdf/2410.14169v1;cs.CV;arXiv admin note: substantial text overlap with arXiv:2403.02265;gaussian splatting<tag:sep>nerf
2410.14189v1;http://arxiv.org/abs/2410.14189v1;2024-10-18;Neural Signed Distance Function Inference through Splatting 3D Gaussians  Pulled on Zero-Level Set;"It is vital to infer a signed distance function (SDF) in multi-view based
surface reconstruction. 3D Gaussian splatting (3DGS) provides a novel
perspective for volume rendering, and shows advantages in rendering efficiency
and quality. Although 3DGS provides a promising neural rendering option, it is
still hard to infer SDFs for surface reconstruction with 3DGS due to the
discreteness, the sparseness, and the off-surface drift of 3D Gaussians. To
resolve these issues, we propose a method that seamlessly merge 3DGS with the
learning of neural SDFs. Our key idea is to more effectively constrain the SDF
inference with the multi-view consistency. To this end, we dynamically align 3D
Gaussians on the zero-level set of the neural SDF using neural pulling, and
then render the aligned 3D Gaussians through the differentiable rasterization.
Meanwhile, we update the neural SDF by pulling neighboring space to the pulled
3D Gaussians, which progressively refine the signed distance field near the
surface. With both differentiable pulling and splatting, we jointly optimize 3D
Gaussians and the neural SDF with both RGB and geometry constraints, which
recovers more accurate, smooth, and complete surfaces with more geometry
details. Our numerical and visual comparisons show our superiority over the
state-of-the-art results on the widely used benchmarks.";Wenyuan Zhang<author:sep>Yu-Shen Liu<author:sep>Zhizhong Han;http://arxiv.org/pdf/2410.14189v1;cs.CV;"Accepted by NeurIPS 2024. Project page:
  https://wen-yuan-zhang.github.io/GS-Pull/";gaussian splatting
2410.14462v1;http://arxiv.org/abs/2410.14462v1;2024-10-18;LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian  Splatting scenes;"We address the task of uplifting visual features or semantic masks from 2D
vision models to 3D scenes represented by Gaussian Splatting. Whereas common
approaches rely on iterative optimization-based procedures, we show that a
simple yet effective aggregation technique yields excellent results. Applied to
semantic masks from Segment Anything (SAM), our uplifting approach leads to
segmentation quality comparable to the state of the art. We then extend this
method to generic DINOv2 features, integrating 3D scene geometry through graph
diffusion, and achieve competitive segmentation results despite DINOv2 not
being trained on millions of annotated masks like SAM.";Juliette Marrie<author:sep>Romain Ménégaux<author:sep>Michael Arbel<author:sep>Diane Larlus<author:sep>Julien Mairal;http://arxiv.org/pdf/2410.14462v1;cs.CV;;gaussian splatting
2410.13465v1;http://arxiv.org/abs/2410.13465v1;2024-10-17;Object Pose Estimation Using Implicit Representation For Transparent  Objects;"Object pose estimation is a prominent task in computer vision. The object
pose gives the orientation and translation of the object in real-world space,
which allows various applications such as manipulation, augmented reality, etc.
Various objects exhibit different properties with light, such as reflections,
absorption, etc. This makes it challenging to understand the object's structure
in RGB and depth channels. Recent research has been moving toward
learning-based methods, which provide a more flexible and generalizable
approach to object pose estimation utilizing deep learning. One such approach
is the render-and-compare method, which renders the object from multiple views
and compares it against the given 2D image, which often requires an object
representation in the form of a CAD model. We reason that the synthetic texture
of the CAD model may not be ideal for rendering and comparing operations. We
showed that if the object is represented as an implicit (neural) representation
in the form of Neural Radiance Field (NeRF), it exhibits a more realistic
rendering of the actual scene and retains the crucial spatial features, which
makes the comparison more versatile. We evaluated our NeRF implementation of
the render-and-compare method on transparent datasets and found that it
surpassed the current state-of-the-art results.";Varun Burde<author:sep>Artem Moroz<author:sep>Vit Zeman<author:sep>Pavel Burget;http://arxiv.org/pdf/2410.13465v1;cs.CV;;nerf
2410.13613v1;http://arxiv.org/abs/2410.13613v1;2024-10-17;MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes;"4D Gaussian Splatting (4DGS) has recently emerged as a promising technique
for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D
Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering
speeds. Despite its advantages, 4DGS faces significant challenges, notably the
requirement of millions of 4D Gaussians, each with extensive associated
attributes, leading to substantial memory and storage cost. This paper
introduces a memory-efficient framework for 4DGS. We streamline the color
attribute by decomposing it into a per-Gaussian direct color component with
only 3 parameters and a shared lightweight alternating current color predictor.
This approach eliminates the need for spherical harmonics coefficients, which
typically involve up to 144 parameters in classic 4DGS, thereby creating a
memory-efficient 4D Gaussian representation. Furthermore, we introduce an
entropy-constrained Gaussian deformation technique that uses a deformation
field to expand the action range of each Gaussian and integrates an
opacity-based entropy loss to limit the number of Gaussians, thus forcing our
model to use as few Gaussians as possible to fit a dynamic scene well. With
simple half-precision storage and zip compression, our framework achieves a
storage reduction by approximately 190$\times$ and 125$\times$ on the
Technicolor and Neural 3D Video datasets, respectively, compared to the
original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene
representation quality, setting a new standard in the field.";Xinjie Zhang<author:sep>Zhening Liu<author:sep>Yifan Zhang<author:sep>Xingtong Ge<author:sep>Dailan He<author:sep>Tongda Xu<author:sep>Yan Wang<author:sep>Zehong Lin<author:sep>Shuicheng Yan<author:sep>Jun Zhang;http://arxiv.org/pdf/2410.13613v1;cs.CV;;gaussian splatting
2410.13851v1;http://arxiv.org/abs/2410.13851v1;2024-10-17;Differentiable Robot Rendering;"Vision foundation models trained on massive amounts of visual data have shown
unprecedented reasoning and planning skills in open-world settings. A key
challenge in applying them to robotic tasks is the modality gap between visual
data and action data. We introduce differentiable robot rendering, a method
allowing the visual appearance of a robot body to be directly differentiable
with respect to its control parameters. Our model integrates a kinematics-aware
deformable model and Gaussians Splatting and is compatible with any robot form
factors and degrees of freedom. We demonstrate its capability and usage in
applications including reconstruction of robot poses from images and
controlling robots through vision language models. Quantitative and qualitative
results show that our differentiable rendering model provides effective
gradients for robotic control directly from pixels, setting the foundation for
the future applications of vision foundation models in robotics.";Ruoshi Liu<author:sep>Alper Canberk<author:sep>Shuran Song<author:sep>Carl Vondrick;http://arxiv.org/pdf/2410.13851v1;cs.RO;Project Page: https://drrobot.cs.columbia.edu/;
2410.13349v1;http://arxiv.org/abs/2410.13349v1;2024-10-17;GlossyGS: Inverse Rendering of Glossy Objects with 3D Gaussian Splatting;"Reconstructing objects from posed images is a crucial and complex task in
computer graphics and computer vision. While NeRF-based neural reconstruction
methods have exhibited impressive reconstruction ability, they tend to be
time-comsuming. Recent strategies have adopted 3D Gaussian Splatting (3D-GS)
for inverse rendering, which have led to quick and effective outcomes. However,
these techniques generally have difficulty in producing believable geometries
and materials for glossy objects, a challenge that stems from the inherent
ambiguities of inverse rendering. To address this, we introduce GlossyGS, an
innovative 3D-GS-based inverse rendering framework that aims to precisely
reconstruct the geometry and materials of glossy objects by integrating
material priors. The key idea is the use of micro-facet geometry segmentation
prior, which helps to reduce the intrinsic ambiguities and improve the
decomposition of geometries and materials. Additionally, we introduce a normal
map prefiltering strategy to more accurately simulate the normal distribution
of reflective surfaces. These strategies are integrated into a hybrid geometry
and material representation that employs both explicit and implicit methods to
depict glossy objects. We demonstrate through quantitative analysis and
qualitative visualization that the proposed method is effective to reconstruct
high-fidelity geometries and materials of glossy objects, and performs
favorably against state-of-the-arts.";Shuichang Lai<author:sep>Letian Huang<author:sep>Jie Guo<author:sep>Kai Cheng<author:sep>Bowen Pan<author:sep>Xiaoxiao Long<author:sep>Jiangjing Lyu<author:sep>Chengfei Lv<author:sep>Yanwen Guo;http://arxiv.org/pdf/2410.13349v1;cs.CV;;gaussian splatting<tag:sep>nerf
2410.13607v1;http://arxiv.org/abs/2410.13607v1;2024-10-17;DN-4DGS: Denoised Deformable Network with Temporal-Spatial Aggregation  for Dynamic Scene Rendering;"Dynamic scenes rendering is an intriguing yet challenging problem. Although
current methods based on NeRF have achieved satisfactory performance, they
still can not reach real-time levels. Recently, 3D Gaussian Splatting (3DGS)
has gar?nered researchers attention due to their outstanding rendering quality
and real?time speed. Therefore, a new paradigm has been proposed: defining a
canonical 3D gaussians and deforming it to individual frames in deformable
fields. How?ever, since the coordinates of canonical 3D gaussians are filled
with noise, which can transfer noise into the deformable fields, and there is
currently no method that adequately considers the aggregation of 4D
information. Therefore, we pro?pose Denoised Deformable Network with
Temporal-Spatial Aggregation for Dy?namic Scene Rendering (DN-4DGS).
Specifically, a Noise Suppression Strategy is introduced to change the
distribution of the coordinates of the canonical 3D gaussians and suppress
noise. Additionally, a Decoupled Temporal-Spatial Ag?gregation Module is
designed to aggregate information from adjacent points and frames. Extensive
experiments on various real-world datasets demonstrate that our method achieves
state-of-the-art rendering quality under a real-time level.";Jiahao Lu<author:sep>Jiacheng Deng<author:sep>Ruijie Zhu<author:sep>Yanzhe Liang<author:sep>Wenfei Yang<author:sep>Tianzhu Zhang<author:sep>Xu Zhou;http://arxiv.org/pdf/2410.13607v1;cs.CV;Accepted by NeurIPS 2024;gaussian splatting<tag:sep>nerf
2410.13862v1;http://arxiv.org/abs/2410.13862v1;2024-10-17;DepthSplat: Connecting Gaussian Splatting and Depth;"Gaussian splatting and single/multi-view depth estimation are typically
studied in isolation. In this paper, we present DepthSplat to connect Gaussian
splatting and depth estimation and study their interactions. More specifically,
we first contribute a robust multi-view depth model by leveraging pre-trained
monocular depth features, leading to high-quality feed-forward 3D Gaussian
splatting reconstructions. We also show that Gaussian splatting can serve as an
unsupervised pre-training objective for learning powerful depth models from
large-scale unlabelled datasets. We validate the synergy between Gaussian
splatting and depth estimation through extensive ablation and cross-task
transfer experiments. Our DepthSplat achieves state-of-the-art performance on
ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and
novel view synthesis, demonstrating the mutual benefits of connecting both
tasks. Our code, models, and video results are available at
https://haofeixu.github.io/depthsplat/.";Haofei Xu<author:sep>Songyou Peng<author:sep>Fangjinhua Wang<author:sep>Hermann Blum<author:sep>Daniel Barath<author:sep>Andreas Geiger<author:sep>Marc Pollefeys;http://arxiv.org/pdf/2410.13862v1;cs.CV;Project page: https://haofeixu.github.io/depthsplat/;gaussian splatting
2410.13571v2;http://arxiv.org/abs/2410.13571v2;2024-10-17;DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving  Scene Representation;"Closed-loop simulation is essential for advancing end-to-end autonomous
driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS,
rely predominantly on conditions closely aligned with training data
distributions, which are largely confined to forward-driving scenarios.
Consequently, these methods face limitations when rendering complex maneuvers
(e.g., lane change, acceleration, deceleration). Recent advancements in
autonomous-driving world models have demonstrated the potential to generate
diverse driving videos. However, these approaches remain constrained to 2D
video generation, inherently lacking the spatiotemporal coherence required to
capture intricacies of dynamic driving environments. In this paper, we
introduce DriveDreamer4D, which enhances 4D driving scene representation
leveraging world model priors. Specifically, we utilize the world model as a
data machine to synthesize novel trajectory videos based on real-world driving
data. Notably, we explicitly leverage structured conditions to control the
spatial-temporal consistency of foreground and background elements, thus the
generated data adheres closely to traffic constraints. To our knowledge,
DriveDreamer4D is the first to utilize video generation models for improving 4D
reconstruction in driving scenarios. Experimental results reveal that
DriveDreamer4D significantly enhances generation quality under novel trajectory
views, achieving a relative improvement in FID by 24.5%, 39.0%, and 10.5%
compared to PVG, S3Gaussian, and Deformable-GS. Moreover, DriveDreamer4D
markedly enhances the spatiotemporal coherence of driving agents, which is
verified by a comprehensive user study and the relative increases of 20.3%,
42.0%, and 13.7% in the NTA-IoU metric.";Guosheng Zhao<author:sep>Chaojun Ni<author:sep>Xiaofeng Wang<author:sep>Zheng Zhu<author:sep>Xueyang Zhang<author:sep>Yida Wang<author:sep>Guan Huang<author:sep>Xinze Chen<author:sep>Boyuan Wang<author:sep>Youyi Zhang<author:sep>Wenjun Mei<author:sep>Xingang Wang;http://arxiv.org/pdf/2410.13571v2;cs.CV;Project Page: https://drivedreamer4d.github.io;nerf
2410.12781v1;http://arxiv.org/abs/2410.12781v1;2024-10-16;Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage  Gaussian Splats;"We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is
capable of reconstructing a large scene from a long sequence of input images.
Specifically, our model can process 32 source images at 960x540 resolution
within only 1.3 seconds on a single A100 80G GPU. Our architecture features a
mixture of the recent Mamba2 blocks and the classical transformer blocks which
allowed many more tokens to be processed than prior work, enhanced by efficient
token merging and Gaussian pruning steps that balance between quality and
efficiency. Unlike previous feed-forward models that are limited to processing
1~4 input images and can only reconstruct a small portion of a large scene,
Long-LRM reconstructs the entire scene in a single feed-forward step. On
large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method
achieves performance comparable to optimization-based approaches while being
two orders of magnitude more efficient. Project page:
https://arthurhero.github.io/projects/llrm";Chen Ziwen<author:sep>Hao Tan<author:sep>Kai Zhang<author:sep>Sai Bi<author:sep>Fujun Luan<author:sep>Yicong Hong<author:sep>Li Fuxin<author:sep>Zexiang Xu;http://arxiv.org/pdf/2410.12781v1;cs.CV;;
2410.12242v1;http://arxiv.org/abs/2410.12242v1;2024-10-16;EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior  for Sparse View;"Generalizable neural radiance field (NeRF) enables neural-based digital human
rendering without per-scene retraining. When combined with human prior
knowledge, high-quality human rendering can be achieved even with sparse input
views. However, the inference of these methods is still slow, as a large number
of neural network queries on each ray are required to ensure the rendering
quality. Moreover, occluded regions often suffer from artifacts, especially
when the input views are sparse. To address these issues, we propose a
generalizable human NeRF framework that achieves high-quality and real-time
rendering with sparse input views by extensively leveraging human prior
knowledge. We accelerate the rendering with a two-stage sampling reduction
strategy: first constructing boundary meshes around the human geometry to
reduce the number of ray samples for sampling guidance regression, and then
volume rendering using fewer guided samples. To improve rendering quality,
especially in occluded regions, we propose an occlusion-aware attention
mechanism to extract occlusion information from the human priors, followed by
an image space refinement network to improve rendering quality. Furthermore,
for volume rendering, we adopt a signed ray distance function (SRDF)
formulation, which allows us to propose an SRDF loss at every sample position
to improve the rendering quality further. Our experiments demonstrate that our
method outperforms the state-of-the-art methods in rendering quality and has a
competitive rendering speed compared with speed-prioritized novel view
synthesis methods.";Zhaorong Wang<author:sep>Yoshihiro Kanamori<author:sep>Yuki Endo;http://arxiv.org/pdf/2410.12242v1;cs.CV;project page: https://github.com/LarsPh/EG-HumanNeRF;nerf
2410.12262v1;http://arxiv.org/abs/2410.12262v1;2024-10-16;3D Gaussian Splatting in Robotics: A Survey;"Dense 3D representations of the environment have been a long-term goal in the
robotics field. While previous Neural Radiance Fields (NeRF) representation
have been prevalent for its implicit, coordinate-based model, the recent
emergence of 3D Gaussian Splatting (3DGS) has demonstrated remarkable potential
in its explicit radiance field representation. By leveraging 3D Gaussian
primitives for explicit scene representation and enabling differentiable
rendering, 3DGS has shown significant advantages over other radiance fields in
real-time rendering and photo-realistic performance, which is beneficial for
robotic applications. In this survey, we provide a comprehensive understanding
of 3DGS in the field of robotics. We divide our discussion of the related works
into two main categories: the application of 3DGS and the advancements in 3DGS
techniques. In the application section, we explore how 3DGS has been utilized
in various robotics tasks from scene understanding and interaction
perspectives. The advance of 3DGS section focuses on the improvements of 3DGS
own properties in its adaptability and efficiency, aiming to enhance its
performance in robotics. We then summarize the most commonly used datasets and
evaluation metrics in robotics. Finally, we identify the challenges and
limitations of current 3DGS methods and discuss the future development of 3DGS
in robotics.";Siting Zhu<author:sep>Guangming Wang<author:sep>Dezhi Kong<author:sep>Hesheng Wang;http://arxiv.org/pdf/2410.12262v1;cs.RO;;gaussian splatting<tag:sep>nerf
2410.11394v1;http://arxiv.org/abs/2410.11394v1;2024-10-15;MCGS: Multiview Consistency Enhancement for Sparse-View 3D Gaussian  Radiance Fields;"Radiance fields represented by 3D Gaussians excel at synthesizing novel
views, offering both high training efficiency and fast rendering. However, with
sparse input views, the lack of multi-view consistency constraints results in
poorly initialized point clouds and unreliable heuristics for optimization and
densification, leading to suboptimal performance. Existing methods often
incorporate depth priors from dense estimation networks but overlook the
inherent multi-view consistency in input images. Additionally, they rely on
multi-view stereo (MVS)-based initialization, which limits the efficiency of
scene representation. To overcome these challenges, we propose a view synthesis
framework based on 3D Gaussian Splatting, named MCGS, enabling photorealistic
scene reconstruction from sparse input views. The key innovations of MCGS in
enhancing multi-view consistency are as follows: i) We introduce an
initialization method by leveraging a sparse matcher combined with a random
filling strategy, yielding a compact yet sufficient set of initial points. This
approach enhances the initial geometry prior, promoting efficient scene
representation. ii) We develop a multi-view consistency-guided progressive
pruning strategy to refine the Gaussian field by strengthening consistency and
eliminating low-contribution Gaussians. These modular, plug-and-play strategies
enhance robustness to sparse input views, accelerate rendering, and reduce
memory consumption, making MCGS a practical and efficient framework for 3D
Gaussian Splatting.";Yuru Xiao<author:sep>Deming Zhai<author:sep>Wenbo Zhao<author:sep>Kui Jiang<author:sep>Junjun Jiang<author:sep>Xianming Liu;http://arxiv.org/pdf/2410.11394v1;cs.CV;;gaussian splatting
2410.11505v1;http://arxiv.org/abs/2410.11505v1;2024-10-15;LoGS: Visual Localization via Gaussian Splatting with Fewer Training  Images;"Visual localization involves estimating a query image's 6-DoF (degrees of
freedom) camera pose, which is a fundamental component in various computer
vision and robotic tasks. This paper presents LoGS, a vision-based localization
pipeline utilizing the 3D Gaussian Splatting (GS) technique as scene
representation. This novel representation allows high-quality novel view
synthesis. During the mapping phase, structure-from-motion (SfM) is applied
first, followed by the generation of a GS map. During localization, the initial
position is obtained through image retrieval, local feature matching coupled
with a PnP solver, and then a high-precision pose is achieved through the
analysis-by-synthesis manner on the GS map. Experimental results on four
large-scale datasets demonstrate the proposed approach's SoTA accuracy in
estimating camera poses and robustness under challenging few-shot conditions.";Yuzhou Cheng<author:sep>Jianhao Jiao<author:sep>Yue Wang<author:sep>Dimitrios Kanoulas;http://arxiv.org/pdf/2410.11505v1;cs.CV;8 pages;gaussian splatting
2410.11419v1;http://arxiv.org/abs/2410.11419v1;2024-10-15;GS^3: Efficient Relighting with Triple Gaussian Splatting;"We present a spatial and angular Gaussian based representation and a triple
splatting process, for real-time, high-quality novel lighting-and-view
synthesis from multi-view point-lit input images. To describe complex
appearance, we employ a Lambertian plus a mixture of angular Gaussians as an
effective reflectance function for each spatial Gaussian. To generate
self-shadow, we splat all spatial Gaussians towards the light source to obtain
shadow values, which are further refined by a small multi-layer perceptron. To
compensate for other effects like global illumination, another network is
trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness
of our representation is demonstrated on 30 samples with a wide variation in
geometry (from solid to fluffy) and appearance (from translucent to
anisotropic), as well as using different forms of input data, including
rendered images of synthetic/reconstructed objects, photographs captured with a
handheld camera and a flash, or from a professional lightstage. We achieve a
training time of 40-70 minutes and a rendering speed of 90 fps on a single
commodity GPU. Our results compare favorably with state-of-the-art techniques
in terms of quality/performance. Our code and data are publicly available at
https://GSrelight.github.io/.";Zoubin Bi<author:sep>Yixin Zeng<author:sep>Chong Zeng<author:sep>Fan Pei<author:sep>Xiang Feng<author:sep>Kun Zhou<author:sep>Hongzhi Wu;http://arxiv.org/pdf/2410.11419v1;cs.CV;"Accepted to SIGGRAPH Asia 2024. Project page:
  https://gsrelight.github.io/";gaussian splatting
2410.11356v1;http://arxiv.org/abs/2410.11356v1;2024-10-15;GSORB-SLAM: Gaussian Splatting SLAM benefits from ORB features and  Transmittance information;"The emergence of 3D Gaussian Splatting (3DGS) has recently sparked a renewed
wave of dense visual SLAM research. However, current methods face challenges
such as sensitivity to artifacts and noise, sub-optimal selection of training
viewpoints, and a lack of light global optimization. In this paper, we propose
a dense SLAM system that tightly couples 3DGS with ORB features. We design a
joint optimization approach for robust tracking and effectively reducing the
impact of noise and artifacts. This involves combining novel geometric
observations, derived from accumulated transmittance, with ORB features
extracted from pixel data. Furthermore, to improve mapping quality, we propose
an adaptive Gaussian expansion and regularization method that enables Gaussian
primitives to represent the scene compactly. This is coupled with a viewpoint
selection strategy based on the hybrid graph to mitigate over-fitting effects
and enhance convergence quality. Finally, our approach achieves compact and
high-quality scene representations and accurate localization. GSORB-SLAM has
been evaluated on different datasets, demonstrating outstanding performance.
The code will be available.";Wancai Zheng<author:sep>Xinyi Yu<author:sep>Jintao Rong<author:sep>Linlin Ou<author:sep>Yan Wei<author:sep>Libo Zhou;http://arxiv.org/pdf/2410.11356v1;cs.RO;;gaussian splatting
2410.12080v1;http://arxiv.org/abs/2410.12080v1;2024-10-15;SplatPose+: Real-time Image-Based Pose-Agnostic 3D Anomaly Detection;"Image-based Pose-Agnostic 3D Anomaly Detection is an important task that has
emerged in industrial quality control. This task seeks to find anomalies from
query images of a tested object given a set of reference images of an
anomaly-free object. The challenge is that the query views (a.k.a poses) are
unknown and can be different from the reference views. Currently, new methods
such as OmniposeAD and SplatPose have emerged to bridge the gap by synthesizing
pseudo reference images at the query views for pixel-to-pixel comparison.
However, none of these methods can infer in real-time, which is critical in
industrial quality control for massive production. For this reason, we propose
SplatPose+, which employs a hybrid representation consisting of a Structure
from Motion (SfM) model for localization and a 3D Gaussian Splatting (3DGS)
model for Novel View Synthesis. Although our proposed pipeline requires the
computation of an additional SfM model, it offers real-time inference speeds
and faster training compared to SplatPose. Quality-wise, we achieved a new SOTA
on the Pose-agnostic Anomaly Detection benchmark with the Multi-Pose Anomaly
Detection (MAD-SIM) dataset.";Yizhe Liu<author:sep>Yan Song Hu<author:sep>Yuhao Chen<author:sep>John Zelek;http://arxiv.org/pdf/2410.12080v1;cs.CV;;gaussian splatting
2410.11285v1;http://arxiv.org/abs/2410.11285v1;2024-10-15;Scalable Indoor Novel-View Synthesis using Drone-Captured 360 Imagery  with 3D Gaussian Splatting;"Scene reconstruction and novel-view synthesis for large, complex,
multi-story, indoor scenes is a challenging and time-consuming task. Prior
methods have utilized drones for data capture and radiance fields for scene
reconstruction, both of which present certain challenges. First, in order to
capture diverse viewpoints with the drone's front-facing camera, some
approaches fly the drone in an unstable zig-zag fashion, which hinders
drone-piloting and generates motion blur in the captured data. Secondly, most
radiance field methods do not easily scale to arbitrarily large number of
images. This paper proposes an efficient and scalable pipeline for indoor
novel-view synthesis from drone-captured 360 videos using 3D Gaussian
Splatting. 360 cameras capture a wide set of viewpoints, allowing for
comprehensive scene capture under a simple straightforward drone trajectory. To
scale our method to large scenes, we devise a divide-and-conquer strategy to
automatically split the scene into smaller blocks that can be reconstructed
individually and in parallel. We also propose a coarse-to-fine alignment
strategy to seamlessly match these blocks together to compose the entire scene.
Our experiments demonstrate marked improvement in both reconstruction quality,
i.e. PSNR and SSIM, and computation time compared to prior approaches.";Yuanbo Chen<author:sep>Chengyu Zhang<author:sep>Jason Wang<author:sep>Xuefan Gao<author:sep>Avideh Zakhor;http://arxiv.org/pdf/2410.11285v1;cs.CV;Accepted to ECCV 2024 S3DSGR Workshop;gaussian splatting
2410.10719v2;http://arxiv.org/abs/2410.10719v2;2024-10-14;4-LEGS: 4D Language Embedded Gaussian Splatting;"The emergence of neural representations has revolutionized our means for
digitally viewing a wide range of 3D scenes, enabling the synthesis of
photorealistic images rendered from novel views. Recently, several techniques
have been proposed for connecting these low-level representations with the
high-level semantics understanding embodied within the scene. These methods
elevate the rich semantic understanding from 2D imagery to 3D representations,
distilling high-dimensional spatial features onto 3D space. In our work, we are
interested in connecting language with a dynamic modeling of the world. We show
how to lift spatio-temporal features to a 4D representation based on 3D
Gaussian Splatting. This enables an interactive interface where the user can
spatiotemporally localize events in the video from text prompts. We demonstrate
our system on public 3D video datasets of people and animals performing various
actions.";Gal Fiebelman<author:sep>Tamir Cohen<author:sep>Ayellet Morgenstern<author:sep>Peter Hedman<author:sep>Hadar Averbuch-Elor;http://arxiv.org/pdf/2410.10719v2;cs.CV;Project webpage: https://tau-vailab.github.io/4-LEGS/;gaussian splatting
2410.10085v1;http://arxiv.org/abs/2410.10085v1;2024-10-14;NeRF-enabled Analysis-Through-Synthesis for ISAR Imaging of Small  Everyday Objects with Sparse and Noisy UWB Radar Data;"Inverse Synthetic Aperture Radar (ISAR) imaging presents a formidable
challenge when it comes to small everyday objects due to their limited Radar
Cross-Section (RCS) and the inherent resolution constraints of radar systems.
Existing ISAR reconstruction methods including backprojection (BP) often
require complex setups and controlled environments, rendering them impractical
for many real-world noisy scenarios. In this paper, we propose a novel
Analysis-through-Synthesis (ATS) framework enabled by Neural Radiance Fields
(NeRF) for high-resolution coherent ISAR imaging of small objects using sparse
and noisy Ultra-Wideband (UWB) radar data with an inexpensive and portable
setup. Our end-to-end framework integrates ultra-wideband radar wave
propagation, reflection characteristics, and scene priors, enabling efficient
2D scene reconstruction without the need for costly anechoic chambers or
complex measurement test beds. With qualitative and quantitative comparisons,
we demonstrate that the proposed method outperforms traditional techniques and
generates ISAR images of complex scenes with multiple targets and complex
structures in Non-Line-of-Sight (NLOS) and noisy scenarios, particularly with
limited number of views and sparse UWB radar scans. This work represents a
significant step towards practical, cost-effective ISAR imaging of small
everyday objects, with broad implications for robotics and mobile sensing
applications.";Md Farhan Tasnim Oshim<author:sep>Albert Reed<author:sep>Suren Jayasuriya<author:sep>Tauhidur Rahman;http://arxiv.org/pdf/2410.10085v1;cs.RO;;nerf
2410.10782v1;http://arxiv.org/abs/2410.10782v1;2024-10-14;3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for  Human-Object Interaction (HOI) and Autonomous Driving Applications;"Human-object interaction (HOI) and human-scene interaction (HSI) are crucial
for human-centric scene understanding applications in Embodied Artificial
Intelligence (EAI), robotics, and augmented reality (AR). A common limitation
faced in these research areas is the data scarcity problem: insufficient
labeled human-scene object pairs on the input images, and limited interaction
complexity and granularity between them. Recent HOI and HSI methods have
addressed this issue by generating dynamic interactions with rigid objects. But
more complex dynamic interactions such as a human rider pedaling an articulated
bicycle have been unexplored. To address this limitation, and to enable
research on complex dynamic human-articulated object interactions, in this
paper we propose a method to generate simulated 3D dynamic cyclist assets and
interactions. We designed a methodology for creating a new part-based
multi-view articulated synthetic 3D bicycle dataset that we call 3DArticBikes
that can be used to train NeRF and 3DGS-based 3D reconstruction methods. We
then propose a 3DGS-based parametric bicycle composition model to assemble
8-DoF pose-controllable 3D bicycles. Finally, using dynamic information from
cyclist videos, we build a complete synthetic dynamic 3D cyclist (rider
pedaling a bicycle) by re-posing a selectable synthetic 3D person while
automatically placing the rider onto one of our new articulated 3D bicycles
using a proposed 3D Keypoint optimization-based Inverse Kinematics pose
refinement. We present both, qualitative and quantitative results where we
compare our generated cyclists against those from a recent stable
diffusion-based method.";Eduardo R. Corral-Soto<author:sep>Yang Liu<author:sep>Tongtong Cao<author:sep>Yuan Ren<author:sep>Liu Bingbing;http://arxiv.org/pdf/2410.10782v1;cs.CV;;nerf
2410.11080v1;http://arxiv.org/abs/2410.11080v1;2024-10-14;Few-shot Novel View Synthesis using Depth Aware 3D Gaussian Splatting;"3D Gaussian splatting has surpassed neural radiance field methods in novel
view synthesis by achieving lower computational costs and real-time
high-quality rendering. Although it produces a high-quality rendering with a
lot of input views, its performance drops significantly when only a few views
are available. In this work, we address this by proposing a depth-aware
Gaussian splatting method for few-shot novel view synthesis. We use monocular
depth prediction as a prior, along with a scale-invariant depth loss, to
constrain the 3D shape under just a few input views. We also model color using
lower-order spherical harmonics to avoid overfitting. Further, we observe that
removing splats with lower opacity periodically, as performed in the original
work, leads to a very sparse point cloud and, hence, a lower-quality rendering.
To mitigate this, we retain all the splats, leading to a better reconstruction
in a few view settings. Experimental results show that our method outperforms
the traditional 3D Gaussian splatting methods by achieving improvements of
10.5% in peak signal-to-noise ratio, 6% in structural similarity index, and
14.1% in perceptual similarity, thereby validating the effectiveness of our
approach. The code will be made available at:
https://github.com/raja-kumar/depth-aware-3DGS";Raja Kumar<author:sep>Vanshika Vats;http://arxiv.org/pdf/2410.11080v1;cs.CV;Presented in ECCV 2024 workshop S3DSGR;gaussian splatting
2410.10412v1;http://arxiv.org/abs/2410.10412v1;2024-10-14;4DStyleGaussian: Zero-shot 4D Style Transfer with Gaussian Splatting;"3D neural style transfer has gained significant attention for its potential
to provide user-friendly stylization with spatial consistency. However,
existing 3D style transfer methods often fall short in terms of inference
efficiency, generalization ability, and struggle to handle dynamic scenes with
temporal consistency. In this paper, we introduce 4DStyleGaussian, a novel 4D
style transfer framework designed to achieve real-time stylization of arbitrary
style references while maintaining reasonable content affinity, multi-view
consistency, and temporal coherence. Our approach leverages an embedded 4D
Gaussian Splatting technique, which is trained using a reversible neural
network for reducing content loss in the feature distillation process.
Utilizing the 4D embedded Gaussians, we predict a 4D style transformation
matrix that facilitates spatially and temporally consistent style transfer with
Gaussian Splatting. Experiments demonstrate that our method can achieve
high-quality and zero-shot stylization for 4D scenarios with enhanced
efficiency and spatial-temporal consistency.";Wanlin Liang<author:sep>Hongbin Xu<author:sep>Weitao Chen<author:sep>Feng Xiao<author:sep>Wenxiong Kang;http://arxiv.org/pdf/2410.10412v1;cs.CV;;gaussian splatting
2410.09740v1;http://arxiv.org/abs/2410.09740v1;2024-10-13;Gaussian Splatting Visual MPC for Granular Media Manipulation;"Recent advancements in learned 3D representations have enabled significant
progress in solving complex robotic manipulation tasks, particularly for
rigid-body objects. However, manipulating granular materials such as beans,
nuts, and rice, remains challenging due to the intricate physics of particle
interactions, high-dimensional and partially observable state, inability to
visually track individual particles in a pile, and the computational demands of
accurate dynamics prediction. Current deep latent dynamics models often
struggle to generalize in granular material manipulation due to a lack of
inductive biases. In this work, we propose a novel approach that learns a
visual dynamics model over Gaussian splatting representations of scenes and
leverages this model for manipulating granular media via Model-Predictive
Control. Our method enables efficient optimization for complex manipulation
tasks on piles of granular media. We evaluate our approach in both simulated
and real-world settings, demonstrating its ability to solve unseen planning
tasks and generalize to new environments in a zero-shot transfer. We also show
significant prediction and manipulation performance improvements compared to
existing granular media manipulation methods.";Wei-Cheng Tseng<author:sep>Ellina Zhang<author:sep>Krishna Murthy Jatavallabhula<author:sep>Florian Shkurti;http://arxiv.org/pdf/2410.09740v1;cs.RO;project website https://weichengtseng.github.io/gs-granular-mani/;gaussian splatting
2410.09771v1;http://arxiv.org/abs/2410.09771v1;2024-10-13;Magnituder Layers for Implicit Neural Representations in 3D;"Improving the efficiency and performance of implicit neural representations
in 3D, particularly Neural Radiance Fields (NeRF) and Signed Distance Fields
(SDF) is crucial for enabling their use in real-time applications. These
models, while capable of generating photo-realistic novel views and detailed 3D
reconstructions, often suffer from high computational costs and slow inference
times. To address this, we introduce a novel neural network layer called the
""magnituder"", designed to reduce the number of training parameters in these
models without sacrificing their expressive power. By integrating magnituders
into standard feed-forward layer stacks, we achieve improved inference speed
and adaptability. Furthermore, our approach enables a zero-shot performance
boost in trained implicit neural representation models through layer-wise
knowledge transfer without backpropagation, leading to more efficient scene
reconstruction in dynamic environments.";Sang Min Kim<author:sep>Byeongchan Kim<author:sep>Arijit Sehanobish<author:sep>Krzysztof Choromanski<author:sep>Dongseok Shim<author:sep>Avinava Dubey<author:sep>Min-hwan Oh;http://arxiv.org/pdf/2410.09771v1;cs.CV;;nerf
2410.09467v1;http://arxiv.org/abs/2410.09467v1;2024-10-12;Enhancing Single Image to 3D Generation using Gaussian Splatting and  Hybrid Diffusion Priors;"3D object generation from a single image involves estimating the full 3D
geometry and texture of unseen views from an unposed RGB image captured in the
wild. Accurately reconstructing an object's complete 3D structure and texture
has numerous applications in real-world scenarios, including robotic
manipulation, grasping, 3D scene understanding, and AR/VR. Recent advancements
in 3D object generation have introduced techniques that reconstruct an object's
3D shape and texture by optimizing the efficient representation of Gaussian
Splatting, guided by pre-trained 2D or 3D diffusion models. However, a notable
disparity exists between the training datasets of these models, leading to
distinct differences in their outputs. While 2D models generate highly detailed
visuals, they lack cross-view consistency in geometry and texture. In contrast,
3D models ensure consistency across different views but often result in overly
smooth textures. We propose bridging the gap between 2D and 3D diffusion models
to address this limitation by integrating a two-stage frequency-based
distillation loss with Gaussian Splatting. Specifically, we leverage geometric
priors in the low-frequency spectrum from a 3D diffusion model to maintain
consistent geometry and use a 2D diffusion model to refine the fidelity and
texture in the high-frequency spectrum of the generated 3D structure, resulting
in more detailed and fine-grained outcomes. Our approach enhances geometric
consistency and visual quality, outperforming the current SOTA. Additionally,
we demonstrate the easy adaptability of our method for efficient object pose
estimation and tracking.";Hritam Basak<author:sep>Hadi Tabatabaee<author:sep>Shreekant Gayaka<author:sep>Ming-Feng Li<author:sep>Xin Yang<author:sep>Cheng-Hao Kuo<author:sep>Arnie Sen<author:sep>Min Sun<author:sep>Zhaozheng Yin;http://arxiv.org/pdf/2410.09467v1;cs.CV;;gaussian splatting
2410.09582v1;http://arxiv.org/abs/2410.09582v1;2024-10-12;Improving 3D Finger Traits Recognition via Generalizable Neural  Rendering;"3D biometric techniques on finger traits have become a new trend and have
demonstrated a powerful ability for recognition and anti-counterfeiting.
Existing methods follow an explicit 3D pipeline that reconstructs the models
first and then extracts features from 3D models. However, these explicit 3D
methods suffer from the following problems: 1) Inevitable information dropping
during 3D reconstruction; 2) Tight coupling between specific hardware and
algorithm for 3D reconstruction. It leads us to a question: Is it indispensable
to reconstruct 3D information explicitly in recognition tasks? Hence, we
consider this problem in an implicit manner, leaving the nerve-wracking 3D
reconstruction problem for learnable neural networks with the help of neural
radiance fields (NeRFs). We propose FingerNeRF, a novel generalizable NeRF for
3D finger biometrics. To handle the shape-radiance ambiguity problem that may
result in incorrect 3D geometry, we aim to involve extra geometric priors based
on the correspondence of binary finger traits like fingerprints or finger
veins. First, we propose a novel Trait Guided Transformer (TGT) module to
enhance the feature correspondence with the guidance of finger traits. Second,
we involve extra geometric constraints on the volume rendering loss with the
proposed Depth Distillation Loss and Trait Guided Rendering Loss. To evaluate
the performance of the proposed method on different modalities, we collect two
new datasets: SCUT-Finger-3D with finger images and SCUT-FingerVein-3D with
finger vein images. Moreover, we also utilize the UNSW-3D dataset with
fingerprint images for evaluation. In experiments, our FingerNeRF can achieve
4.37% EER on SCUT-Finger-3D dataset, 8.12% EER on SCUT-FingerVein-3D dataset,
and 2.90% EER on UNSW-3D dataset, showing the superiority of the proposed
implicit method in 3D finger biometrics.";Hongbin Xu<author:sep>Junduan Huang<author:sep>Yuer Ma<author:sep>Zifeng Li<author:sep>Wenxiong Kang;http://arxiv.org/pdf/2410.09582v1;cs.CV;"This paper is accepted in IJCV. For further information and access to
  the code, please visit our project page:
  https://scut-bip-lab.github.io/fingernerf/";nerf
2410.09292v1;http://arxiv.org/abs/2410.09292v1;2024-10-11;SurgicalGS: Dynamic 3D Gaussian Splatting for Accurate Robotic-Assisted  Surgical Scene Reconstruction;"Accurate 3D reconstruction of dynamic surgical scenes from endoscopic video
is essential for robotic-assisted surgery. While recent 3D Gaussian Splatting
methods have shown promise in achieving high-quality reconstructions with fast
rendering speeds, their use of inverse depth loss functions compresses depth
variations. This can lead to a loss of fine geometric details, limiting their
ability to capture precise 3D geometry and effectiveness in intraoperative
application. To address these challenges, we present SurgicalGS, a dynamic 3D
Gaussian Splatting framework specifically designed for surgical scene
reconstruction with improved geometric accuracy. Our approach first initialises
a Gaussian point cloud using depth priors, employing binary motion masks to
identify pixels with significant depth variations and fusing point clouds from
depth maps across frames for initialisation. We use the Flexible Deformation
Model to represent dynamic scene and introduce a normalised depth
regularisation loss along with an unsupervised depth smoothness constraint to
ensure more accurate geometric reconstruction. Extensive experiments on two
real surgical datasets demonstrate that SurgicalGS achieves state-of-the-art
reconstruction quality, especially in terms of accurate geometry, advancing the
usability of 3D Gaussian Splatting in robotic-assisted surgery.";Jialei Chen<author:sep>Xin Zhang<author:sep>Mobarakol Islam<author:sep>Francisco Vasconcelos<author:sep>Danail Stoyanov<author:sep>Daniel S. Elson<author:sep>Baoru Huang;http://arxiv.org/pdf/2410.09292v1;cs.CV;7 pages;gaussian splatting
2410.08840v1;http://arxiv.org/abs/2410.08840v1;2024-10-11;Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand  Avatars;"In this paper, we propose to create animatable avatars for interacting hands
with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based
methods designed for single subjects often yield unsatisfactory results due to
limited input views, various hand poses, and occlusions. To address these
challenges, we introduce a novel two-stage interaction-aware GS framework that
exploits cross-subject hand priors and refines 3D Gaussians in interacting
areas. Particularly, to handle hand variations, we disentangle the 3D
presentation of hands into optimization-based identity maps and learning-based
latent geometric features and neural texture maps. Learning-based features are
captured by trained networks to provide reliable priors for poses, shapes, and
textures, while optimization-based identity maps enable efficient one-shot
fitting of out-of-distribution hands. Furthermore, we devise an
interaction-aware attention module and a self-adaptive Gaussian refinement
module. These modules enhance image rendering quality in areas with intra- and
inter-hand interactions, overcoming the limitations of existing GS-based
methods. Our proposed method is validated via extensive experiments on the
large-scale InterHand2.6M dataset, and it significantly improves the
state-of-the-art performance in image quality. Project Page:
\url{https://github.com/XuanHuang0/GuassianHand}.";Xuan Huang<author:sep>Hanhui Li<author:sep>Wanquan Liu<author:sep>Xiaodan Liang<author:sep>Yiqiang Yan<author:sep>Yuhao Cheng<author:sep>Chengqiang Gao;http://arxiv.org/pdf/2410.08840v1;cs.CV;Accepted to NeurIPS 2024;gaussian splatting
2410.08941v1;http://arxiv.org/abs/2410.08941v1;2024-10-11;MeshGS: Adaptive Mesh-Aligned Gaussian Splatting for High-Quality  Rendering;"Recently, 3D Gaussian splatting has gained attention for its capability to
generate high-fidelity rendering results. At the same time, most applications
such as games, animation, and AR/VR use mesh-based representations to represent
and render 3D scenes. We propose a novel approach that integrates mesh
representation with 3D Gaussian splats to perform high-quality rendering of
reconstructed real-world scenes. In particular, we introduce a distance-based
Gaussian splatting technique to align the Gaussian splats with the mesh surface
and remove redundant Gaussian splats that do not contribute to the rendering.
We consider the distance between each Gaussian splat and the mesh surface to
distinguish between tightly-bound and loosely-bound Gaussian splats. The
tightly-bound splats are flattened and aligned well with the mesh geometry. The
loosely-bound Gaussian splats are used to account for the artifacts in
reconstructed 3D meshes in terms of rendering. We present a training strategy
of binding Gaussian splats to the mesh geometry, and take into account both
types of splats. In this context, we introduce several regularization
techniques aimed at precisely aligning tightly-bound Gaussian splats with the
mesh surface during the training process. We validate the effectiveness of our
method on large and unbounded scene from mip-NeRF 360 and Deep Blending
datasets. Our method surpasses recent mesh-based neural rendering techniques by
achieving a 2dB higher PSNR, and outperforms mesh-based Gaussian splatting
methods by 1.3 dB PSNR, particularly on the outdoor mip-NeRF 360 dataset,
demonstrating better rendering quality. We provide analyses for each type of
Gaussian splat and achieve a reduction in the number of Gaussian splats by 30%
compared to the original 3D Gaussian splatting.";Jaehoon Choi<author:sep>Yonghan Lee<author:sep>Hyungtae Lee<author:sep>Heesung Kwon<author:sep>Dinesh Manocha;http://arxiv.org/pdf/2410.08941v1;cs.CV;ACCV (Asian Conference on Computer Vision) 2024;gaussian splatting<tag:sep>nerf
2410.08743v1;http://arxiv.org/abs/2410.08743v1;2024-10-11;Look Gauss, No Pose: Novel View Synthesis using Gaussian Splatting  without Accurate Pose Initialization;"3D Gaussian Splatting has recently emerged as a powerful tool for fast and
accurate novel-view synthesis from a set of posed input images. However, like
most novel-view synthesis approaches, it relies on accurate camera pose
information, limiting its applicability in real-world scenarios where acquiring
accurate camera poses can be challenging or even impossible. We propose an
extension to the 3D Gaussian Splatting framework by optimizing the extrinsic
camera parameters with respect to photometric residuals. We derive the
analytical gradients and integrate their computation with the existing
high-performance CUDA implementation. This enables downstream tasks such as
6-DoF camera pose estimation as well as joint reconstruction and camera
refinement. In particular, we achieve rapid convergence and high accuracy for
pose estimation on real-world scenes. Our method enables fast reconstruction of
3D scenes without requiring accurate pose information by jointly optimizing
geometry and camera poses, while achieving state-of-the-art results in
novel-view synthesis. Our approach is considerably faster to optimize than most
competing methods, and several times faster in rendering. We show results on
real-world scenes and complex trajectories through simulated environments,
achieving state-of-the-art results on LLFF while reducing runtime by two to
four times compared to the most efficient competing method. Source code will be
available at https://github.com/Schmiddo/noposegs .";Christian Schmidt<author:sep>Jens Piekenbrinck<author:sep>Bastian Leibe;http://arxiv.org/pdf/2410.08743v1;cs.CV;Accepted in IROS 2024;gaussian splatting
2410.09049v1;http://arxiv.org/abs/2410.09049v1;2024-10-11;SceneCraft: Layout-Guided 3D Scene Generation;"The creation of complex 3D scenes tailored to user specifications has been a
tedious and challenging task with traditional 3D modeling tools. Although some
pioneering methods have achieved automatic text-to-3D generation, they are
generally limited to small-scale scenes with restricted control over the shape
and texture. We introduce SceneCraft, a novel method for generating detailed
indoor scenes that adhere to textual descriptions and spatial layout
preferences provided by users. Central to our method is a rendering-based
technique, which converts 3D semantic layouts into multi-view 2D proxy maps.
Furthermore, we design a semantic and depth conditioned diffusion model to
generate multi-view images, which are used to learn a neural radiance field
(NeRF) as the final scene representation. Without the constraints of panorama
image generation, we surpass previous methods in supporting complicated indoor
space generation beyond a single room, even as complicated as a whole
multi-bedroom apartment with irregular shapes and layouts. Through experimental
analysis, we demonstrate that our method significantly outperforms existing
approaches in complex indoor scene generation with diverse textures, consistent
geometry, and realistic visual quality. Code and more results are available at:
https://orangesodahub.github.io/SceneCraft";Xiuyu Yang<author:sep>Yunze Man<author:sep>Jun-Kun Chen<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2410.09049v1;cs.CV;"NeurIPS 2024. Code: https://github.com/OrangeSodahub/SceneCraft
  Project Page: https://orangesodahub.github.io/SceneCraft";nerf
2410.08780v1;http://arxiv.org/abs/2410.08780v1;2024-10-11;Optimizing NeRF-based SLAM with Trajectory Smoothness Constraints;"The joint optimization of Neural Radiance Fields (NeRF) and camera
trajectories has been widely applied in SLAM tasks due to its superior dense
mapping quality and consistency. NeRF-based SLAM learns camera poses using
constraints by implicit map representation. A widely observed phenomenon that
results from the constraints of this form is jerky and physically unrealistic
estimated camera motion, which in turn affects the map quality. To address this
deficiency of current NeRF-based SLAM, we propose in this paper TS-SLAM (TS for
Trajectory Smoothness). It introduces smoothness constraints on camera
trajectories by representing them with uniform cubic B-splines with continuous
acceleration that guarantees smooth camera motion. Benefiting from the
differentiability and local control properties of B-splines, TS-SLAM can
incrementally learn the control points end-to-end using a sliding window
paradigm. Additionally, we regularize camera trajectories by exploiting the
dynamics prior to further smooth trajectories. Experimental results demonstrate
that TS-SLAM achieves superior trajectory accuracy and improves mapping quality
versus NeRF-based SLAM that does not employ the above smoothness constraints.";Yicheng He<author:sep>Guangcheng Chen<author:sep>Hong Zhang;http://arxiv.org/pdf/2410.08780v1;cs.RO;;nerf
2410.08129v1;http://arxiv.org/abs/2410.08129v1;2024-10-10;Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid  Transparency;"3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both
for inverse rendering as well as real-time exploration of scenes. In these
applications, coherence across camera frames and multiple views is crucial, be
it for robust convergence of a scene reconstruction or for artifact-free
fly-throughs. Recent work started mitigating artifacts that break multi-view
coherence, including popping artifacts due to inconsistent transparency sorting
and perspective-correct outlines of (2D) splats. At the same time, real-time
requirements forced such implementations to accept compromises in how
transparency of large assemblies of 3D Gaussians is resolved, in turn breaking
coherence in other ways. In our work, we aim at achieving maximum coherence, by
rendering fully perspective-correct 3D Gaussians while using a high-quality
approximation of accurate blending, hybrid transparency, on a per-pixel level,
in order to retain real-time frame rates. Our fast and perspectively accurate
approach for evaluation of 3D Gaussians does not require matrix inversions,
thereby ensuring numerical stability and eliminating the need for special
handling of degenerate splats, and the hybrid transparency formulation for
blending maintains similar quality as fully resolved per-pixel transparencies
at a fraction of the rendering costs. We further show that each of these two
components can be independently integrated into Gaussian splatting systems. In
combination, they achieve up to 2$\times$ higher frame rates, 2$\times$ faster
optimization, and equal or better image quality with fewer rendering artifacts
compared to traditional 3DGS on common benchmarks.";Florian Hahlbohm<author:sep>Fabian Friederichs<author:sep>Tim Weyrich<author:sep>Linus Franke<author:sep>Moritz Kappel<author:sep>Susana Castillo<author:sep>Marc Stamminger<author:sep>Martin Eisemann<author:sep>Marcus Magnor;http://arxiv.org/pdf/2410.08129v1;cs.GR;Project page: https://fhahlbohm.github.io/htgs/;gaussian splatting
2410.07872v1;http://arxiv.org/abs/2410.07872v1;2024-10-10;L-VITeX: Light-weight Visual Intuition for Terrain Exploration;"This paper presents L-VITeX, a lightweight visual intuition system for
terrain exploration designed for resource-constrained robots and swarms.
L-VITeX aims to provide a hint of Regions of Interest (RoIs) without
computationally expensive processing. By utilizing the Faster Objects, More
Objects (FOMO) tinyML architecture, the system achieves high accuracy (>99%) in
RoI detection while operating on minimal hardware resources (Peak RAM usage <
50 KB) with near real-time inference (<200 ms). The paper evaluates L-VITeX's
performance across various terrains, including mountainous areas, underwater
shipwreck debris regions, and Martian rocky surfaces. Additionally, it
demonstrates the system's application in 3D mapping using a small mobile robot
run by ESP32-Cam and Gaussian Splats (GS), showcasing its potential to enhance
exploration efficiency and decision-making.";Antar Mazumder<author:sep>Zarin Anjum Madhiha;http://arxiv.org/pdf/2410.07872v1;cs.RO;;
2410.08282v1;http://arxiv.org/abs/2410.08282v1;2024-10-10;FusionSense: Bridging Common Sense, Vision, and Touch for Robust  Sparse-View Reconstruction;"Humans effortlessly integrate common-sense knowledge with sensory input from
vision and touch to understand their surroundings. Emulating this capability,
we introduce FusionSense, a novel 3D reconstruction framework that enables
robots to fuse priors from foundation models with highly sparse observations
from vision and tactile sensors. FusionSense addresses three key challenges:
(i) How can robots efficiently acquire robust global shape information about
the surrounding scene and objects? (ii) How can robots strategically select
touch points on the object using geometric and common-sense priors? (iii) How
can partial observations such as tactile signals improve the overall
representation of the object? Our framework employs 3D Gaussian Splatting as a
core representation and incorporates a hierarchical optimization strategy
involving global structure construction, object visual hull pruning and local
geometric constraints. This advancement results in fast and robust perception
in environments with traditionally challenging objects that are transparent,
reflective, or dark, enabling more downstream manipulation or navigation tasks.
Experiments on real-world data suggest that our framework outperforms
previously state-of-the-art sparse-view methods. All code and data are
open-sourced on the project website.";Irving Fang<author:sep>Kairui Shi<author:sep>Xujin He<author:sep>Siqi Tan<author:sep>Yifan Wang<author:sep>Hanwen Zhao<author:sep>Hung-Jui Huang<author:sep>Wenzhen Yuan<author:sep>Chen Feng<author:sep>Jing Zhang;http://arxiv.org/pdf/2410.08282v1;cs.RO;;gaussian splatting
2410.07707v1;http://arxiv.org/abs/2410.07707v1;2024-10-10;MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian  Splatting;"Dynamic scene reconstruction is a long-term challenge in the field of 3D
vision. Recently, the emergence of 3D Gaussian Splatting has provided new
insights into this problem. Although subsequent efforts rapidly extend static
3D Gaussian to dynamic scenes, they often lack explicit constraints on object
motion, leading to optimization difficulties and performance degradation. To
address the above issues, we propose a novel deformable 3D Gaussian splatting
framework called MotionGS, which explores explicit motion priors to guide the
deformation of 3D Gaussians. Specifically, we first introduce an optical flow
decoupling module that decouples optical flow into camera flow and motion flow,
corresponding to camera movement and object motion respectively. Then the
motion flow can effectively constrain the deformation of 3D Gaussians, thus
simulating the motion of dynamic objects. Additionally, a camera pose
refinement module is proposed to alternately optimize 3D Gaussians and camera
poses, mitigating the impact of inaccurate camera poses. Extensive experiments
in the monocular dynamic scenes validate that MotionGS surpasses
state-of-the-art methods and exhibits significant superiority in both
qualitative and quantitative results. Project page:
https://ruijiezhu94.github.io/MotionGS_page";Ruijie Zhu<author:sep>Yanzhe Liang<author:sep>Hanzhi Chang<author:sep>Jiacheng Deng<author:sep>Jiahao Lu<author:sep>Wenfei Yang<author:sep>Tianzhu Zhang<author:sep>Yongdong Zhang;http://arxiv.org/pdf/2410.07707v1;cs.CV;Accepted by NeurIPS 2024. 21 pages, 14 figures,7 tables;gaussian splatting
2410.08181v1;http://arxiv.org/abs/2410.08181v1;2024-10-10;RGM: Reconstructing High-fidelity 3D Car Assets with Relightable 3D-GS  Generative Model from a Single Image;"The generation of high-quality 3D car assets is essential for various
applications, including video games, autonomous driving, and virtual reality.
Current 3D generation methods utilizing NeRF or 3D-GS as representations for 3D
objects, generate a Lambertian object under fixed lighting and lack separated
modelings for material and global illumination. As a result, the generated
assets are unsuitable for relighting under varying lighting conditions,
limiting their applicability in downstream tasks. To address this challenge, we
propose a novel relightable 3D object generative framework that automates the
creation of 3D car assets, enabling the swift and accurate reconstruction of a
vehicle's geometry, texture, and material properties from a single input image.
Our approach begins with introducing a large-scale synthetic car dataset
comprising over 1,000 high-precision 3D vehicle models. We represent 3D objects
using global illumination and relightable 3D Gaussian primitives integrating
with BRDF parameters. Building on this representation, we introduce a
feed-forward model that takes images as input and outputs both relightable 3D
Gaussians and global illumination parameters. Experimental results demonstrate
that our method produces photorealistic 3D car assets that can be seamlessly
integrated into road scenes with different illuminations, which offers
substantial practical benefits for industrial applications.";Xiaoxue Chen<author:sep>Jv Zheng<author:sep>Hao Huang<author:sep>Haoran Xu<author:sep>Weihao Gu<author:sep>Kangliang Chen<author:sep>He xiang<author:sep>Huan-ang Gao<author:sep>Hao Zhao<author:sep>Guyue Zhou<author:sep>Yaqin Zhang;http://arxiv.org/pdf/2410.08181v1;cs.CV;;nerf
2410.08257v1;http://arxiv.org/abs/2410.08257v1;2024-10-10;Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics;"While humans effortlessly discern intrinsic dynamics and adapt to new
scenarios, modern AI systems often struggle. Current methods for visual
grounding of dynamics either use pure neural-network-based simulators (black
box), which may violate physical laws, or traditional physical simulators
(white box), which rely on expert-defined equations that may not fully capture
actual dynamics. We propose the Neural Material Adaptor (NeuMA), which
integrates existing physical laws with learned corrections, facilitating
accurate learning of actual dynamics while maintaining the generalizability and
interpretability of physical priors. Additionally, we propose Particle-GS, a
particle-driven 3D Gaussian Splatting variant that bridges simulation and
observed images, allowing back-propagate image gradients to optimize the
simulator. Comprehensive experiments on various dynamics in terms of grounded
particle accuracy, dynamic rendering quality, and generalization ability
demonstrate that NeuMA can accurately capture intrinsic dynamics.";Junyi Cao<author:sep>Shanyan Guan<author:sep>Yanhao Ge<author:sep>Wei Li<author:sep>Xiaokang Yang<author:sep>Chao Ma;http://arxiv.org/pdf/2410.08257v1;cs.CV;"NeurIPS 2024, the project page:
  https://xjay18.github.io/projects/neuma.html";gaussian splatting
2410.07577v1;http://arxiv.org/abs/2410.07577v1;2024-10-10;3D Vision-Language Gaussian Splatting;"Recent advancements in 3D reconstruction methods and vision-language models
have propelled the development of multi-modal 3D scene understanding, which has
vital applications in robotics, autonomous driving, and virtual/augmented
reality. However, current multi-modal scene understanding approaches have
naively embedded semantic representations into 3D reconstruction methods
without striking a balance between visual and language modalities, which leads
to unsatisfying semantic rasterization of translucent or reflective objects, as
well as over-fitting on color modality. To alleviate these limitations, we
propose a solution that adequately handles the distinct visual and semantic
modalities, i.e., a 3D vision-language Gaussian splatting model for scene
understanding, to put emphasis on the representation learning of language
modality. We propose a novel cross-modal rasterizer, using modality fusion
along with a smoothed semantic indicator for enhancing semantic rasterization.
We also employ a camera-view blending technique to improve semantic consistency
between existing and synthesized views, thereby effectively mitigating
over-fitting. Extensive experiments demonstrate that our method achieves
state-of-the-art performance in open-vocabulary semantic segmentation,
surpassing existing methods by a significant margin.";Qucheng Peng<author:sep>Benjamin Planche<author:sep>Zhongpai Gao<author:sep>Meng Zheng<author:sep>Anwesa Choudhuri<author:sep>Terrence Chen<author:sep>Chen Chen<author:sep>Ziyan Wu;http://arxiv.org/pdf/2410.07577v1;cs.CV;main paper + supplementary material;gaussian splatting
2410.08188v1;http://arxiv.org/abs/2410.08188v1;2024-10-10;DifFRelight: Diffusion-Based Facial Performance Relighting;"We present a novel framework for free-viewpoint facial performance relighting
using diffusion-based image-to-image translation. Leveraging a subject-specific
dataset containing diverse facial expressions captured under various lighting
conditions, including flat-lit and one-light-at-a-time (OLAT) scenarios, we
train a diffusion model for precise lighting control, enabling high-fidelity
relit facial images from flat-lit inputs. Our framework includes
spatially-aligned conditioning of flat-lit captures and random noise, along
with integrated lighting information for global control, utilizing prior
knowledge from the pre-trained Stable Diffusion model. This model is then
applied to dynamic facial performances captured in a consistent flat-lit
environment and reconstructed for novel-view synthesis using a scalable dynamic
3D Gaussian Splatting method to maintain quality and consistency in the relit
results. In addition, we introduce unified lighting control by integrating a
novel area lighting representation with directional lighting, allowing for
joint adjustments in light size and direction. We also enable high dynamic
range imaging (HDRI) composition using multiple directional lights to produce
dynamic sequences under complex lighting conditions. Our evaluations
demonstrate the models efficiency in achieving precise lighting control and
generalizing across various facial expressions while preserving detailed
features such as skintexture andhair. The model accurately reproduces complex
lighting effects like eye reflections, subsurface scattering, self-shadowing,
and translucency, advancing photorealism within our framework.";Mingming He<author:sep>Pascal Clausen<author:sep>Ahmet Levent Taşel<author:sep>Li Ma<author:sep>Oliver Pilarski<author:sep>Wenqi Xian<author:sep>Laszlo Rikker<author:sep>Xueming Yu<author:sep>Ryan Burgert<author:sep>Ning Yu<author:sep>Paul Debevec;http://arxiv.org/pdf/2410.08188v1;cs.CV;"18 pages, SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers
  '24), December 3--6, 2024, Tokyo, Japan. Project page:
  https://www.eyelinestudios.com/research/diffrelight.html";gaussian splatting
2410.08107v1;http://arxiv.org/abs/2410.08107v1;2024-10-10;IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera;"Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for
novel view synthesis have achieved remarkable progress with frame-based camera
(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel
type of bio-inspired visual sensor, i.e. event camera, has demonstrated
advantages in high temporal resolution, high dynamic range, low power
consumption and low latency. Due to its unique asynchronous and irregular data
capturing process, limited work has been proposed to apply neural
representation or 3D Gaussian splatting for an event camera. In this work, we
present IncEventGS, an incremental 3D Gaussian Splatting reconstruction
algorithm with a single event camera. To recover the 3D scene representation
incrementally, we exploit the tracking and mapping paradigm of conventional
SLAM pipelines for IncEventGS. Given the incoming event stream, the tracker
firstly estimates an initial camera motion based on prior reconstructed 3D-GS
scene representation. The mapper then jointly refines both the 3D scene
representation and camera motion based on the previously estimated motion
trajectory from the tracker. The experimental results demonstrate that
IncEventGS delivers superior performance compared to prior NeRF-based methods
and other related baselines, even we do not have the ground-truth camera poses.
Furthermore, our method can also deliver better performance compared to
state-of-the-art event visual odometry methods in terms of camera motion
estimation. Code is publicly available at:
https://github.com/wu-cvgl/IncEventGS.";Jian Huang<author:sep>Chengrui Dong<author:sep>Peidong Liu;http://arxiv.org/pdf/2410.08107v1;cs.CV;Code Page: https://github.com/wu-cvgl/IncEventGS;gaussian splatting<tag:sep>nerf
2410.08190v1;http://arxiv.org/abs/2410.08190v1;2024-10-10;Poison-splat: Computation Cost Attack on 3D Gaussian Splatting;"3D Gaussian splatting (3DGS), known for its groundbreaking performance and
efficiency, has become a dominant 3D representation and brought progress to
many 3D vision tasks. However, in this work, we reveal a significant security
vulnerability that has been largely overlooked in 3DGS: the computation cost of
training 3DGS could be maliciously tampered by poisoning the input data. By
developing an attack named Poison-splat, we reveal a novel attack surface where
the adversary can poison the input images to drastically increase the
computation memory and time needed for 3DGS training, pushing the algorithm
towards its worst computation complexity. In extreme cases, the attack can even
consume all allocable memory, leading to a Denial-of-Service (DoS) that
disrupts servers, resulting in practical damages to real-world 3DGS service
vendors. Such a computation cost attack is achieved by addressing a bi-level
optimization problem through three tailored strategies: attack objective
approximation, proxy model rendering, and optional constrained optimization.
These strategies not only ensure the effectiveness of our attack but also make
it difficult to defend with simple defensive measures. We hope the revelation
of this novel attack surface can spark attention to this crucial yet overlooked
vulnerability of 3DGS systems.";Jiahao Lu<author:sep>Yifan Zhang<author:sep>Qiuhong Shen<author:sep>Xinchao Wang<author:sep>Shuicheng Yan;http://arxiv.org/pdf/2410.08190v1;cs.CV;Our code is available at https://github.com/jiahaolu97/poison-splat;gaussian splatting
2410.08017v2;http://arxiv.org/abs/2410.08017v2;2024-10-10;Fast Feedforward 3D Gaussian Splatting Compression;"With 3D Gaussian Splatting (3DGS) advancing real-time and high-fidelity
rendering for novel view synthesis, storage requirements pose challenges for
their widespread adoption. Although various compression techniques have been
proposed, previous art suffers from a common limitation: for any existing 3DGS,
per-scene optimization is needed to achieve compression, making the compression
sluggish and slow. To address this issue, we introduce Fast Compression of 3D
Gaussian Splatting (FCGS), an optimization-free model that can compress 3DGS
representations rapidly in a single feed-forward pass, which significantly
reduces compression time from minutes to seconds. To enhance compression
efficiency, we propose a multi-path entropy module that assigns Gaussian
attributes to different entropy constraint paths for balance between size and
fidelity. We also carefully design both inter- and intra-Gaussian context
models to remove redundancies among the unstructured Gaussian blobs. Overall,
FCGS achieves a compression ratio of over 20X while maintaining fidelity,
surpassing most per-scene SOTA optimization-based methods. Our code is
available at: https://github.com/YihangChen-ee/FCGS.";Yihang Chen<author:sep>Qianyi Wu<author:sep>Mengyao Li<author:sep>Weiyao Lin<author:sep>Mehrtash Harandi<author:sep>Jianfei Cai;http://arxiv.org/pdf/2410.08017v2;cs.CV;"Project Page: https://yihangchen-ee.github.io/project_fcgs/ Code:
  https://github.com/yihangchen-ee/fcgs/";gaussian splatting
2410.06475v1;http://arxiv.org/abs/2410.06475v1;2024-10-09;3D Representation Methods: A Survey;"The field of 3D representation has experienced significant advancements,
driven by the increasing demand for high-fidelity 3D models in various
applications such as computer graphics, virtual reality, and autonomous
systems. This review examines the development and current state of 3D
representation methods, highlighting their research trajectories, innovations,
strength and weakness. Key techniques such as Voxel Grid, Point Cloud, Mesh,
Signed Distance Function (SDF), Neural Radiance Field (NeRF), 3D Gaussian
Splatting, Tri-Plane, and Deep Marching Tetrahedra (DMTet) are reviewed. The
review also introduces essential datasets that have been pivotal in advancing
the field, highlighting their characteristics and impact on research progress.
Finally, we explore potential research directions that hold promise for further
expanding the capabilities and applications of 3D representation methods.";Zhengren Wang;http://arxiv.org/pdf/2410.06475v1;cs.CV;Preliminary Draft;nerf
2410.06613v1;http://arxiv.org/abs/2410.06613v1;2024-10-09;ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian  Completion;"Accurate and affordable indoor 3D reconstruction is critical for effective
robot navigation and interaction. Traditional LiDAR-based mapping provides high
precision but is costly, heavy, and power-intensive, with limited ability for
novel view rendering. Vision-based mapping, while cost-effective and capable of
capturing visual data, often struggles with high-quality 3D reconstruction due
to sparse point clouds. We propose ES-Gaussian, an end-to-end system using a
low-altitude camera and single-line LiDAR for high-quality 3D indoor
reconstruction. Our system features Visual Error Construction (VEC) to enhance
sparse point clouds by identifying and correcting areas with insufficient
geometric detail from 2D error maps. Additionally, we introduce a novel 3DGS
initialization method guided by single-line LiDAR, overcoming the limitations
of traditional multi-view setups and enabling effective reconstruction in
resource-constrained environments. Extensive experimental results on our new
Dreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian
outperforms existing methods, particularly in challenging scenarios. The
project page is available at https://chenlu-china.github.io/ES-Gaussian/.";Lu Chen<author:sep>Yingfu Zeng<author:sep>Haoang Li<author:sep>Zhitao Deng<author:sep>Jiafu Yan<author:sep>Zhenjun Zhao;http://arxiv.org/pdf/2410.06613v1;cs.CV;Project page: https://chenlu-china.github.io/ES-Gaussian/;gaussian splatting
2410.07418v2;http://arxiv.org/abs/2410.07418v2;2024-10-09;NeRF-Accelerated Ecological Monitoring in Mixed-Evergreen Redwood Forest;"Forest mapping provides critical observational data needed to understand the
dynamics of forest environments. Notably, tree diameter at breast height (DBH)
is a metric used to estimate forest biomass and carbon dioxide sequestration.
Manual methods of forest mapping are labor intensive and time consuming, a
bottleneck for large-scale mapping efforts. Automated mapping relies on
acquiring dense forest reconstructions, typically in the form of point clouds.
Terrestrial laser scanning (TLS) and mobile laser scanning (MLS) generate point
clouds using expensive LiDAR sensing, and have been used successfully to
estimate tree diameter. Neural radiance fields (NeRFs) are an emergent
technology enabling photorealistic, vision-based reconstruction by training a
neural network on a sparse set of input views. In this paper, we present a
comparison of MLS and NeRF forest reconstructions for the purpose of trunk
diameter estimation in a mixed-evergreen Redwood forest. In addition, we
propose an improved DBH-estimation method using convex-hull modeling. Using
this approach, we achieved 1.68 cm RMSE, which consistently outperformed
standard cylinder modeling approaches. Our code contributions and forest
datasets are freely available at https://github.com/harelab-ucsc/RedwoodNeRF.";Adam Korycki<author:sep>Cory Yeaton<author:sep>Gregory S. Gilbert<author:sep>Colleen Josephson<author:sep>Steve McGuire;http://arxiv.org/pdf/2410.07418v2;cs.CV;;nerf
2410.06756v1;http://arxiv.org/abs/2410.06756v1;2024-10-09;DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh  Hybrid Representation;"Recent advancements in 2D/3D generative techniques have facilitated the
generation of dynamic 3D objects from monocular videos. Previous methods mainly
rely on the implicit neural radiance fields (NeRF) or explicit Gaussian
Splatting as the underlying representation, and struggle to achieve
satisfactory spatial-temporal consistency and surface appearance. Drawing
inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a
novel framework combining mesh representation with geometric skinning technique
to generate high-quality 4D object from a monocular video. Instead of utilizing
classical texture map for appearance, we bind Gaussian splats to triangle face
of mesh for differentiable optimization of both the texture and mesh vertices.
In particular, DreamMesh4D begins with a coarse mesh obtained through an
image-to-3D generation procedure. Sparse points are then uniformly sampled
across the mesh surface, and are used to build a deformation graph to drive the
motion of the 3D object for the sake of computational efficiency and providing
additional constraint. For each step, transformations of sparse control points
are predicted using a deformation network, and the mesh vertices as well as the
surface Gaussians are deformed via a novel geometric skinning algorithm, which
is a hybrid approach combining LBS (linear blending skinning) and DQS
(dual-quaternion skinning), mitigating drawbacks associated with both
approaches. The static surface Gaussians and mesh vertices as well as the
deformation network are learned via reference view photometric loss, score
distillation loss as well as other regularizers in a two-stage manner.
Extensive experiments demonstrate superior performance of our method.
Furthermore, our method is compatible with modern graphic pipelines, showcasing
its potential in the 3D gaming and film industry.";Zhiqi Li<author:sep>Yiming Chen<author:sep>Peidong Liu;http://arxiv.org/pdf/2410.06756v1;cs.CV;NeurIPS 2024;nerf
2410.06734v1;http://arxiv.org/abs/2410.06734v1;2024-10-09;MimicTalk: Mimicking a personalized and expressive 3D talking face in  minutes;"Talking face generation (TFG) aims to animate a target identity's face to
create realistic talking videos. Personalized TFG is a variant that emphasizes
the perceptual identity similarity of the synthesized result (from the
perspective of appearance and talking style). While previous works typically
solve this problem by learning an individual neural radiance field (NeRF) for
each identity to implicitly store its static and dynamic information, we find
it inefficient and non-generalized due to the per-identity-per-training
framework and the limited training data. To this end, we propose MimicTalk, the
first attempt that exploits the rich knowledge from a NeRF-based
person-agnostic generic model for improving the efficiency and robustness of
personalized TFG. To be specific, (1) we first come up with a person-agnostic
3D TFG model as the base model and propose to adapt it into a specific
identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help
the model learn the personalized static appearance and facial dynamic features;
(3) To generate the facial motion of the personalized talking style, we propose
an in-context stylized audio-to-motion model that mimics the implicit talking
style provided in the reference video without information loss by an explicit
style representation. The adaptation process to an unseen identity can be
performed in 15 minutes, which is 47 times faster than previous
person-dependent methods. Experiments show that our MimicTalk surpasses
previous baselines regarding video quality, efficiency, and expressiveness.
Source code and video samples are available at https://mimictalk.github.io .";Zhenhui Ye<author:sep>Tianyun Zhong<author:sep>Yi Ren<author:sep>Ziyue Jiang<author:sep>Jiawei Huang<author:sep>Rongjie Huang<author:sep>Jinglin Liu<author:sep>Jinzheng He<author:sep>Chen Zhang<author:sep>Zehan Wang<author:sep>Xize Chen<author:sep>Xiang Yin<author:sep>Zhou Zhao;http://arxiv.org/pdf/2410.06734v1;cs.CV;Accepted by NeurIPS 2024;nerf
2410.07266v1;http://arxiv.org/abs/2410.07266v1;2024-10-09;Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction  via Spiking Neuron-based Gaussian Splatting;"3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes.
Despite recent advances in improving surface reconstruction accuracy, the
reconstructed results still exhibit bias and suffer from inefficiency in
storage and training. This paper provides a different observation on the cause
of the inefficiency and the reconstruction bias, which is attributed to the
integration of the low-opacity parts (LOPs) of the generated Gaussians. We show
that LOPs consist of Gaussians with overall low-opacity (LOGs) and the
low-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two
types of LOPs by integrating spiking neurons into the Gaussian Splatting
pipeline. Specifically, we introduce global and local full-precision
integrate-and-fire spiking neurons to the opacity and representation function
of flattened 3D Gaussians, respectively. Furthermore, we enhance the density
control strategy with spiking neurons' thresholds and an new criterion on the
scale of Gaussians. Our method can represent more accurate reconstructed
surfaces at a lower cost. The code is available at
\url{https://github.com/shippoT/Spiking_GS}.";Weixing Zhang<author:sep>Zongrui Li<author:sep>De Ma<author:sep>Huajin Tang<author:sep>Xudong Jiang<author:sep>Qian Zheng<author:sep>Gang Pan;http://arxiv.org/pdf/2410.07266v1;cs.CV;;gaussian splatting
2410.06014v1;http://arxiv.org/abs/2410.06014v1;2024-10-08;SplaTraj: Camera Trajectory Generation with Semantic Gaussian Splatting;"Many recent developments for robots to represent environments have focused on
photorealistic reconstructions. This paper particularly focuses on generating
sequences of images from the photorealistic Gaussian Splatting models, that
match instructions that are given by user-inputted language. We contribute a
novel framework, SplaTraj, which formulates the generation of images within
photorealistic environment representations as a continuous-time trajectory
optimization problem. Costs are designed so that a camera following the
trajectory poses will smoothly traverse through the environment and render the
specified spatial information in a photogenic manner. This is achieved by
querying a photorealistic representation with language embedding to isolate
regions that correspond to the user-specified inputs. These regions are then
projected to the camera's view as it moves over time and a cost is constructed.
We can then apply gradient-based optimization and differentiate through the
rendering to optimize the trajectory for the defined cost. The resulting
trajectory moves to photogenically view each of the specified objects. We
empirically evaluate our approach on a suite of environments and instructions,
and demonstrate the quality of generated image sequences.";Xinyi Liu<author:sep>Tianyi Zhang<author:sep>Matthew Johnson-Roberson<author:sep>Weiming Zhi;http://arxiv.org/pdf/2410.06014v1;cs.RO;;gaussian splatting
2410.06231v2;http://arxiv.org/abs/2410.06231v2;2024-10-08;RelitLRM: Generative Relightable Radiance for Large Reconstruction  Models;"We propose RelitLRM, a Large Reconstruction Model (LRM) for generating
high-quality Gaussian splatting representations of 3D objects under novel
illuminations from sparse (4-8) posed images captured under unknown static
lighting. Unlike prior inverse rendering methods requiring dense captures and
slow optimization, often causing artifacts like incorrect highlights or shadow
baking, RelitLRM adopts a feed-forward transformer-based model with a novel
combination of a geometry reconstructor and a relightable appearance generator
based on diffusion. The model is trained end-to-end on synthetic multi-view
renderings of objects under varying known illuminations. This architecture
design enables to effectively decompose geometry and appearance, resolve the
ambiguity between material and lighting, and capture the multi-modal
distribution of shadows and specularity in the relit appearance. We show our
sparse-view feed-forward RelitLRM offers competitive relighting results to
state-of-the-art dense-view optimization-based baselines while being
significantly faster. Our project page is available at:
https://relit-lrm.github.io/.";Tianyuan Zhang<author:sep>Zhengfei Kuang<author:sep>Haian Jin<author:sep>Zexiang Xu<author:sep>Sai Bi<author:sep>Hao Tan<author:sep>He Zhang<author:sep>Yiwei Hu<author:sep>Milos Hasan<author:sep>William T. Freeman<author:sep>Kai Zhang<author:sep>Fujun Luan;http://arxiv.org/pdf/2410.06231v2;cs.CV;webpage: https://relit-lrm.github.io/;gaussian splatting
2410.06165v1;http://arxiv.org/abs/2410.06165v1;2024-10-08;GSLoc: Visual Localization with 3D Gaussian Splatting;"We present GSLoc: a new visual localization method that performs dense camera
alignment using 3D Gaussian Splatting as a map representation of the scene.
GSLoc backpropagates pose gradients over the rendering pipeline to align the
rendered and target images, while it adopts a coarse-to-fine strategy by
utilizing blurring kernels to mitigate the non-convexity of the problem and
improve the convergence. The results show that our approach succeeds at visual
localization in challenging conditions of relatively small overlap between
initial and target frames inside textureless environments when state-of-the-art
neural sparse methods provide inferior results. Using the byproduct of
realistic rendering from the 3DGS map representation, we show how to enhance
localization results by mixing a set of observed and virtual reference
keyframes when solving the image retrieval problem. We evaluate our method both
on synthetic and real-world data, discussing its advantages and application
potential.";Kazii Botashev<author:sep>Vladislav Pyatov<author:sep>Gonzalo Ferrer<author:sep>Stamatios Lefkimmiatis;http://arxiv.org/pdf/2410.06165v1;cs.RO;;gaussian splatting
2410.06245v1;http://arxiv.org/abs/2410.06245v1;2024-10-08;HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable  Sparse-View Reconstruction;"Reconstructing 3D scenes from multiple viewpoints is a fundamental task in
stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have
enabled high-quality novel view synthesis for unseen scenes from sparse input
views by feed-forward predicting per-pixel Gaussian parameters without extra
optimization. However, existing methods typically generate single-scale 3D
Gaussians, which lack representation of both large-scale structure and texture
details, resulting in mislocation and artefacts. In this paper, we propose a
novel framework, HiSplat, which introduces a hierarchical manner in
generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via
a coarse-to-fine strategy. Specifically, HiSplat generates large coarse-grained
Gaussians to capture large-scale structures, followed by fine-grained Gaussians
to enhance delicate texture details. To promote inter-scale interactions, we
propose an Error Aware Module for Gaussian compensation and a Modulating Fusion
Module for Gaussian repair. Our method achieves joint optimization of
hierarchical representations, allowing for novel view synthesis using only
two-view reference images. Comprehensive experiments on various datasets
demonstrate that HiSplat significantly enhances reconstruction quality and
cross-dataset generalization compared to prior single-scale methods. The
corresponding ablation study and analysis of different-scale 3D Gaussians
reveal the mechanism behind the effectiveness. Project website:
https://open3dvlab.github.io/HiSplat/";Shengji Tang<author:sep>Weicai Ye<author:sep>Peng Ye<author:sep>Weihao Lin<author:sep>Yang Zhou<author:sep>Tao Chen<author:sep>Wanli Ouyang;http://arxiv.org/pdf/2410.06245v1;cs.CV;;gaussian splatting
2410.05772v1;http://arxiv.org/abs/2410.05772v1;2024-10-08;Comparative Analysis of Novel View Synthesis and Photogrammetry for 3D  Forest Stand Reconstruction and extraction of individual tree parameters;"Accurate and efficient 3D reconstruction of trees is crucial for forest
resource assessments and management. Close-Range Photogrammetry (CRP) is
commonly used for reconstructing forest scenes but faces challenges like low
efficiency and poor quality. Recently, Novel View Synthesis (NVS) technologies,
including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have
shown promise for 3D plant reconstruction with limited images. However,
existing research mainly focuses on small plants in orchards or individual
trees, leaving uncertainty regarding their application in larger, complex
forest stands. In this study, we collected sequential images of forest plots
with varying complexity and performed dense reconstruction using NeRF and 3DGS.
The resulting point clouds were compared with those from photogrammetry and
laser scanning. Results indicate that NVS methods significantly enhance
reconstruction efficiency. Photogrammetry struggles with complex stands,
leading to point clouds with excessive canopy noise and incorrectly
reconstructed trees, such as duplicated trunks. NeRF, while better for canopy
regions, may produce errors in ground areas with limited views. The 3DGS method
generates sparser point clouds, particularly in trunk areas, affecting diameter
at breast height (DBH) accuracy. All three methods can extract tree height
information, with NeRF yielding the highest accuracy; however, photogrammetry
remains superior for DBH accuracy. These findings suggest that NVS methods have
significant potential for 3D reconstruction of forest stands, offering valuable
support for complex forest resource inventory and visualization tasks.";Guoji Tian<author:sep>Chongcheng Chen<author:sep>Hongyu Huang;http://arxiv.org/pdf/2410.05772v1;cs.CV;31page,15figures;gaussian splatting<tag:sep>nerf
2410.04680v1;http://arxiv.org/abs/2410.04680v1;2024-10-07;Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian  Splatting;"We propose a framework for active next best view and touch selection for
robotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a
useful explicit 3D scene representation for robotics, as it has the ability to
represent scenes in a both photorealistic and geometrically accurate manner.
However, in real-world, online robotic scenes where the number of views is
limited given efficiency requirements, random view selection for 3DGS becomes
impractical as views are often overlapping and redundant. We address this issue
by proposing an end-to-end online training and active view selection pipeline,
which enhances the performance of 3DGS in few-view robotics settings. We first
elevate the performance of few-shot 3DGS with a novel semantic depth alignment
method using Segment Anything Model 2 (SAM2) that we supplement with Pearson
depth and surface normal loss to improve color and depth reconstruction of
real-world scenes. We then extend FisherRF, a next-best-view selection method
for 3DGS, to select views and touch poses based on depth uncertainty. We
perform online view selection on a real robot system during live 3DGS training.
We motivate our improvements to few-shot GS scenes, and extend depth-based
FisherRF to them, where we demonstrate both qualitative and quantitative
improvements on challenging robot scenes. For more information, please see our
project page at https://armlabstanford.github.io/next-best-sense.";Matthew Strong<author:sep>Boshu Lei<author:sep>Aiden Swann<author:sep>Wen Jiang<author:sep>Kostas Daniilidis<author:sep>Monroe Kennedy III;http://arxiv.org/pdf/2410.04680v1;cs.RO;;gaussian splatting
2410.04974v2;http://arxiv.org/abs/2410.04974v2;2024-10-07;6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric  Rendering;"Novel view synthesis has advanced significantly with the development of
neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However,
achieving high quality without compromising real-time rendering remains
challenging, particularly for physically-based ray tracing with view-dependent
effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D
spatial-angular representation to better incorporate view-dependent effects,
but the Gaussian representation and control scheme are sub-optimal. In this
paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS),
which enhances color and opacity representations and leverages the additional
directional information in the 6D space for optimized Gaussian control. Our
approach is fully compatible with the 3DGS framework and significantly improves
real-time radiance field rendering by better modeling view-dependent effects
and fine details. Experiments demonstrate that 6DGS significantly outperforms
3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction
of 66.5% Gaussian points compared to 3DGS. The project page is:
https://gaozhongpai.github.io/6dgs/";Zhongpai Gao<author:sep>Benjamin Planche<author:sep>Meng Zheng<author:sep>Anwesa Choudhuri<author:sep>Terrence Chen<author:sep>Ziyan Wu;http://arxiv.org/pdf/2410.04974v2;cs.CV;"Project: https://gaozhongpai.github.io/6dgs/ and fixed iteration
  typos";gaussian splatting<tag:sep>nerf
2410.05259v1;http://arxiv.org/abs/2410.05259v1;2024-10-07;GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting;"Diffusion-based 2D virtual try-on (VTON) techniques have recently
demonstrated strong performance, while the development of 3D VTON has largely
lagged behind. Despite recent advances in text-guided 3D scene editing,
integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains
challenging. The reasons are twofold. First, text prompts cannot provide
sufficient details in describing clothing. Second, 2D VTON results generated
from different viewpoints of the same 3D scene lack coherence and spatial
relationships, hence frequently leading to appearance inconsistencies and
geometric distortions. To resolve these problems, we introduce an
image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian
Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained
knowledge from 2D VTON models to 3D while improving cross-view consistency. (1)
Specifically, we propose a personalized diffusion model that utilizes low-rank
adaptation (LoRA) fine-tuning to incorporate personalized information into
pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a
reference-driven image editing approach that enables the simultaneous editing
of multi-view images while ensuring consistency. (2) Furthermore, we propose a
persona-aware 3DGS editing framework to facilitate effective editing while
maintaining consistent cross-view appearance and high-quality 3D geometry. (3)
Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which
facilitates comprehensive qualitative and quantitative 3D VTON evaluations.
Through extensive experiments and comparative analyses with existing methods,
the proposed \OM has demonstrated superior fidelity and advanced editing
capabilities, affirming its effectiveness for 3D VTON.";Yukang Cao<author:sep>Masoud Hadi<author:sep>Liang Pan<author:sep>Ziwei Liu;http://arxiv.org/pdf/2410.05259v1;cs.CV;21 pages, 11 figures;gaussian splatting
2410.05468v1;http://arxiv.org/abs/2410.05468v1;2024-10-07;PH-Dropout: Prctical Epistemic Uncertainty Quantification for View  Synthesis;"View synthesis using Neural Radiance Fields (NeRF) and Gaussian Splatting
(GS) has demonstrated impressive fidelity in rendering real-world scenarios.
However, practical methods for accurate and efficient epistemic Uncertainty
Quantification (UQ) in view synthesis are lacking. Existing approaches for NeRF
either introduce significant computational overhead (e.g., ``10x increase in
training time"" or ``10x repeated training"") or are limited to specific
uncertainty conditions or models. Notably, GS models lack any systematic
approach for comprehensive epistemic UQ. This capability is crucial for
improving the robustness and scalability of neural view synthesis, enabling
active model updates, error estimation, and scalable ensemble modeling based on
uncertainty. In this paper, we revisit NeRF and GS-based methods from a
function approximation perspective, identifying key differences and connections
in 3D representation learning. Building on these insights, we introduce
PH-Dropout (Post hoc Dropout), the first real-time and accurate method for
epistemic uncertainty estimation that operates directly on pre-trained NeRF and
GS models. Extensive evaluations validate our theoretical findings and
demonstrate the effectiveness of PH-Dropout.";Chuanhao Sun<author:sep>Thanos Triantafyllou<author:sep>Anthos Makris<author:sep>Maja Drmač<author:sep>Kai Xu<author:sep>Luo Mai<author:sep>Mahesh K. Marina;http://arxiv.org/pdf/2410.05468v1;cs.CV;21 pages, in submision;gaussian splatting<tag:sep>nerf
2410.05514v1;http://arxiv.org/abs/2410.05514v1;2024-10-07;Toward General Object-level Mapping from Sparse Views with 3D Diffusion  Priors;"Object-level mapping builds a 3D map of objects in a scene with detailed
shapes and poses from multi-view sensor observations. Conventional methods
struggle to build complete shapes and estimate accurate poses due to partial
occlusions and sensor noise. They require dense observations to cover all
objects, which is challenging to achieve in robotics trajectories. Recent work
introduces generative shape priors for object-level mapping from sparse views,
but is limited to single-category objects. In this work, we propose a General
Object-level Mapping system, GOM, which leverages a 3D diffusion model as shape
prior with multi-category support and outputs Neural Radiance Fields (NeRFs)
for both texture and geometry for all objects in a scene. GOM includes an
effective formulation to guide a pre-trained diffusion model with extra
nonlinear constraints from sensor measurements without finetuning. We also
develop a probabilistic optimization formulation to fuse multi-view sensor
observations and diffusion priors for joint 3D object pose and shape
estimation. Our GOM system demonstrates superior multi-category mapping
performance from sparse views, and achieves more accurate mapping results
compared to state-of-the-art methods on the real-world benchmarks. We will
release our code: https://github.com/TRAILab/GeneralObjectMapping.";Ziwei Liao<author:sep>Binbin Xu<author:sep>Steven L. Waslander;http://arxiv.org/pdf/2410.05514v1;cs.CV;Accepted by CoRL 2024;nerf
2410.05111v1;http://arxiv.org/abs/2410.05111v1;2024-10-07;LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting;"LiDAR simulation plays a crucial role in closed-loop simulation for
autonomous driving. Although recent advancements, such as the use of
reconstructed mesh and Neural Radiance Fields (NeRF), have made progress in
simulating the physical properties of LiDAR, these methods have struggled to
achieve satisfactory frame rates and rendering quality. To address these
limitations, we present LiDAR-GS, the first LiDAR Gaussian Splatting method,
for real-time high-fidelity re-simulation of LiDAR sensor scans in public urban
road scenes. The vanilla Gaussian Splatting, designed for camera models, cannot
be directly applied to LiDAR re-simulation. To bridge the gap between passive
camera and active LiDAR, our LiDAR-GS designs a differentiable laser beam
splatting, grounded in the LiDAR range view model. This innovation allows for
precise surface splatting by projecting lasers onto micro cross-sections,
effectively eliminating artifacts associated with local affine approximations.
Additionally, LiDAR-GS leverages Neural Gaussian Fields, which further
integrate view-dependent clues, to represent key LiDAR properties that are
influenced by the incident angle and external factors. Combining these
practices with some essential adaptations, e.g., dynamic instances
decomposition, our approach succeeds in simultaneously re-simulating depth,
intensity, and ray-drop channels, achieving state-of-the-art results in both
rendering frame rate and quality on publically available large scene datasets.
Our source code will be made publicly available.";Qifeng Chen<author:sep>Sheng Yang<author:sep>Sicong Du<author:sep>Tao Tang<author:sep>Peng Chen<author:sep>Yuchi Huo;http://arxiv.org/pdf/2410.05111v1;cs.CV;;gaussian splatting<tag:sep>nerf
2410.05097v1;http://arxiv.org/abs/2410.05097v1;2024-10-07;DreamSat: Towards a General 3D Model for Novel View Synthesis of Space  Objects;"Novel view synthesis (NVS) enables to generate new images of a scene or
convert a set of 2D images into a comprehensive 3D model. In the context of
Space Domain Awareness, since space is becoming increasingly congested, NVS can
accurately map space objects and debris, improving the safety and efficiency of
space operations. Similarly, in Rendezvous and Proximity Operations missions,
3D models can provide details about a target object's shape, size, and
orientation, allowing for better planning and prediction of the target's
behavior. In this work, we explore the generalization abilities of these
reconstruction techniques, aiming to avoid the necessity of retraining for each
new scene, by presenting a novel approach to 3D spacecraft reconstruction from
single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art
single-view reconstruction model, on a high-quality dataset of 190 high-quality
spacecraft models and integrating it into the DreamGaussian framework. We
demonstrate consistent improvements in reconstruction quality across multiple
metrics, including Contrastive Language-Image Pretraining (CLIP) score
(+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity
Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS)
(+0.16%) on a test set of 30 previously unseen spacecraft images. Our method
addresses the lack of domain-specific 3D reconstruction tools in the space
industry by leveraging state-of-the-art diffusion models and 3D Gaussian
splatting techniques. This approach maintains the efficiency of the
DreamGaussian framework while enhancing the accuracy and detail of spacecraft
reconstructions. The code for this work can be accessed on GitHub
(https://github.com/ARCLab-MIT/space-nvs).";Nidhi Mathihalli<author:sep>Audrey Wei<author:sep>Giovanni Lavezzi<author:sep>Peng Mun Siew<author:sep>Victor Rodriguez-Fernandez<author:sep>Hodei Urrutxua<author:sep>Richard Linares;http://arxiv.org/pdf/2410.05097v1;cs.CV;"Presented at the 75th International Astronautical Congress, October
  2024, Milan, Italy";
2410.04873v1;http://arxiv.org/abs/2410.04873v1;2024-10-07;TeX-NeRF: Neural Radiance Fields from Pseudo-TeX Vision;"Neural radiance fields (NeRF) has gained significant attention for its
exceptional visual effects. However, most existing NeRF methods reconstruct 3D
scenes from RGB images captured by visible light cameras. In practical
scenarios like darkness, low light, or bad weather, visible light cameras
become ineffective. Therefore, we propose TeX-NeRF, a 3D reconstruction method
using only infrared images, which introduces the object material emissivity as
a priori, preprocesses the infrared images using Pseudo-TeX vision, and maps
the temperatures (T), emissivities (e), and textures (X) of the scene into the
saturation (S), hue (H), and value (V) channels of the HSV color space,
respectively. Novel view synthesis using the processed images has yielded
excellent results. Additionally, we introduce 3D-TeX Datasets, the first
dataset comprising infrared images and their corresponding Pseudo-TeX vision
images. Experiments demonstrate that our method not only matches the quality of
scene reconstruction achieved with high-quality RGB images but also provides
accurate temperature estimations for objects in the scene.";Chonghao Zhong<author:sep>Chao Xu;http://arxiv.org/pdf/2410.04873v1;cs.CV;;nerf
2410.05044v1;http://arxiv.org/abs/2410.05044v1;2024-10-07;PhotoReg: Photometrically Registering 3D Gaussian Splatting Models;"Building accurate representations of the environment is critical for
intelligent robots to make decisions during deployment. Advances in
photorealistic environment models have enabled robots to develop
hyper-realistic reconstructions, which can be used to generate images that are
intuitive for human inspection. In particular, the recently introduced
\ac{3DGS}, which describes the scene with up to millions of primitive
ellipsoids, can be rendered in real time. \ac{3DGS} has rapidly gained
prominence. However, a critical unsolved problem persists: how can we fuse
multiple \ac{3DGS} into a single coherent model? Solving this problem will
enable robot teams to jointly build \ac{3DGS} models of their surroundings. A
key insight of this work is to leverage the {duality} between photorealistic
reconstructions, which render realistic 2D images from 3D structure, and
\emph{3D foundation models}, which predict 3D structure from image pairs. To
this end, we develop PhotoReg, a framework to register multiple photorealistic
\ac{3DGS} models with 3D foundation models. As \ac{3DGS} models are generally
built from monocular camera images, they have \emph{arbitrary scale}. To
resolve this, PhotoReg actively enforces scale consistency among the different
\ac{3DGS} models by considering depth estimates within these models. Then, the
alignment is iteratively refined with fine-grained photometric losses to
produce high-quality fused \ac{3DGS} models. We rigorously evaluate PhotoReg on
both standard benchmark datasets and our custom-collected datasets, including
with two quadruped robots. The code is released at
\url{ziweny11.github.io/photoreg}.";Ziwen Yuan<author:sep>Tianyi Zhang<author:sep>Matthew Johnson-Roberson<author:sep>Weiming Zhi;http://arxiv.org/pdf/2410.05044v1;cs.RO;;gaussian splatting
2410.04354v1;http://arxiv.org/abs/2410.04354v1;2024-10-06;StreetSurfGS: Scalable Urban Street Surface Reconstruction with  Planar-based Gaussian Splatting;"Reconstructing urban street scenes is crucial due to its vital role in
applications such as autonomous driving and urban planning. These scenes are
characterized by long and narrow camera trajectories, occlusion, complex object
relationships, and data sparsity across multiple scales. Despite recent
advancements, existing surface reconstruction methods, which are primarily
designed for object-centric scenarios, struggle to adapt effectively to the
unique characteristics of street scenes. To address this challenge, we
introduce StreetSurfGS, the first method to employ Gaussian Splatting
specifically tailored for scalable urban street scene surface reconstruction.
StreetSurfGS utilizes a planar-based octree representation and segmented
training to reduce memory costs, accommodate unique camera characteristics, and
ensure scalability. Additionally, to mitigate depth inaccuracies caused by
object overlap, we propose a guided smoothing strategy within regularization to
eliminate inaccurate boundary points and outliers. Furthermore, to address
sparse views and multi-scale challenges, we use a dual-step matching strategy
that leverages adjacent and long-term information. Extensive experiments
validate the efficacy of StreetSurfGS in both novel view synthesis and surface
reconstruction.";Xiao Cui<author:sep>Weicai Ye<author:sep>Yifan Wang<author:sep>Guofeng Zhang<author:sep>Wengang Zhou<author:sep>Tong He<author:sep>Houqiang Li;http://arxiv.org/pdf/2410.04354v1;cs.CV;;gaussian splatting
2410.04646v1;http://arxiv.org/abs/2410.04646v1;2024-10-06;Mode-GS: Monocular Depth Guided Anchored 3D Gaussian Splatting for  Robust Ground-View Scene Rendering;"We present a novel-view rendering algorithm, Mode-GS, for ground-robot
trajectory datasets. Our approach is based on using anchored Gaussian splats,
which are designed to overcome the limitations of existing 3D Gaussian
splatting algorithms. Prior neural rendering methods suffer from severe splat
drift due to scene complexity and insufficient multi-view observation, and can
fail to fix splats on the true geometry in ground-robot datasets. Our method
integrates pixel-aligned anchors from monocular depths and generates Gaussian
splats around these anchors using residual-form Gaussian decoders. To address
the inherent scale ambiguity of monocular depth, we parameterize anchors with
per-view depth-scales and employ scale-consistent depth loss for online scale
calibration. Our method results in improved rendering performance, based on
PSNR, SSIM, and LPIPS metrics, in ground scenes with free trajectory patterns,
and achieves state-of-the-art rendering performance on the R3LIVE odometry
dataset and the Tanks and Temples dataset.";Yonghan Lee<author:sep>Jaehoon Choi<author:sep>Dongki Jung<author:sep>Jaeseong Yun<author:sep>Soohyun Ryu<author:sep>Dinesh Manocha<author:sep>Suyong Yeon;http://arxiv.org/pdf/2410.04646v1;cs.CV;;gaussian splatting
2410.04402v1;http://arxiv.org/abs/2410.04402v1;2024-10-06;Deformable NeRF using Recursively Subdivided Tetrahedra;"While neural radiance fields (NeRF) have shown promise in novel view
synthesis, their implicit representation limits explicit control over object
manipulation. Existing research has proposed the integration of explicit
geometric proxies to enable deformation. However, these methods face two
primary challenges: firstly, the time-consuming and computationally demanding
tetrahedralization process; and secondly, handling complex or thin structures
often leads to either excessive, storage-intensive tetrahedral meshes or
poor-quality ones that impair deformation capabilities. To address these
challenges, we propose DeformRF, a method that seamlessly integrates the
manipulability of tetrahedral meshes with the high-quality rendering
capabilities of feature grid representations. To avoid ill-shaped tetrahedra
and tetrahedralization for each object, we propose a two-stage training
strategy. Starting with an almost-regular tetrahedral grid, our model initially
retains key tetrahedra surrounding the object and subsequently refines object
details using finer-granularity mesh in the second stage. We also present the
concept of recursively subdivided tetrahedra to create higher-resolution meshes
implicitly. This enables multi-resolution encoding while only necessitating the
storage of the coarse tetrahedral mesh generated in the first training stage.
We conduct a comprehensive evaluation of our DeformRF on both synthetic and
real-captured datasets. Both quantitative and qualitative results demonstrate
the effectiveness of our method for novel view synthesis and deformation tasks.
Project page: https://ustc3dv.github.io/DeformRF/";Zherui Qiu<author:sep>Chenqu Ren<author:sep>Kaiwen Song<author:sep>Xiaoyi Zeng<author:sep>Leyuan Yang<author:sep>Juyong Zhang;http://arxiv.org/pdf/2410.04402v1;cs.CV;"Accepted by ACM Multimedia 2024. Project Page:
  https://ustc3dv.github.io/DeformRF/";nerf
2410.04041v1;http://arxiv.org/abs/2410.04041v1;2024-10-05;Hybrid NeRF-Stereo Vision: Pioneering Depth Estimation and 3D  Reconstruction in Endoscopy;"The 3D reconstruction of the surgical field in minimally invasive endoscopic
surgery has posed a formidable challenge when using conventional monocular
endoscopes. Existing 3D reconstruction methodologies are frequently encumbered
by suboptimal accuracy and limited generalization capabilities. In this study,
we introduce an innovative pipeline using Neural Radiance Fields (NeRF) for 3D
reconstruction. Our approach utilizes a preliminary NeRF reconstruction that
yields a coarse model, then creates a binocular scene within the reconstructed
environment, which derives an initial depth map via stereo vision. This initial
depth map serves as depth supervision for subsequent NeRF iterations,
progressively refining the 3D reconstruction with enhanced accuracy. The
binocular depth is iteratively recalculated, with the refinement process
continuing until the depth map converges, and exhibits negligible variations.
Through this recursive process, high-fidelity depth maps are generated from
monocular endoscopic video of a realistic cranial phantom. By repeated measures
of the final 3D reconstruction compared to X-ray computed tomography, all
differences of relevant clinical distances result in sub-millimeter accuracy.";Pengcheng Chen<author:sep>Wenhao Li<author:sep>Nicole Gunderson<author:sep>Jeremy Ruthberg<author:sep>Randall Bly<author:sep>Waleed M. Abuzeid<author:sep>Zhenglong Sun<author:sep>Eric J. Seibel;http://arxiv.org/pdf/2410.04041v1;eess.IV;;nerf
2410.03592v1;http://arxiv.org/abs/2410.03592v1;2024-10-04;Variational Bayes Gaussian Splatting;"Recently, 3D Gaussian Splatting has emerged as a promising approach for
modeling 3D scenes using mixtures of Gaussians. The predominant optimization
method for these models relies on backpropagating gradients through a
differentiable rendering pipeline, which struggles with catastrophic forgetting
when dealing with continuous streams of data. To address this limitation, we
propose Variational Bayes Gaussian Splatting (VBGS), a novel approach that
frames training a Gaussian splat as variational inference over model
parameters. By leveraging the conjugacy properties of multivariate Gaussians,
we derive a closed-form variational update rule, allowing efficient updates
from partial, sequential observations without the need for replay buffers. Our
experiments show that VBGS not only matches state-of-the-art performance on
static datasets, but also enables continual learning from sequentially streamed
2D and 3D data, drastically improving performance in this setting.";Toon Van de Maele<author:sep>Ozan Catal<author:sep>Alexander Tschantz<author:sep>Christopher L. Buckley<author:sep>Tim Verbelen;http://arxiv.org/pdf/2410.03592v1;cs.CV;;gaussian splatting
2410.02764v1;http://arxiv.org/abs/2410.02764v1;2024-10-03;Flash-Splat: 3D Reflection Removal with Flash Cues and Gaussian Splats;"We introduce a simple yet effective approach for separating transmitted and
reflected light. Our key insight is that the powerful novel view synthesis
capabilities provided by modern inverse rendering methods (e.g.,~3D Gaussian
splatting) allow one to perform flash/no-flash reflection separation using
unpaired measurements -- this relaxation dramatically simplifies image
acquisition over conventional paired flash/no-flash reflection separation
methods. Through extensive real-world experiments, we demonstrate our method,
Flash-Splat, accurately reconstructs both transmitted and reflected scenes in
3D. Our method outperforms existing 3D reflection separation methods, which do
not leverage illumination control, by a large margin. Our project webpage is at
https://flash-splat.github.io/.";Mingyang Xie<author:sep>Haoming Cai<author:sep>Sachin Shah<author:sep>Yiran Xu<author:sep>Brandon Y. Feng<author:sep>Jia-Bin Huang<author:sep>Christopher A. Metzler;http://arxiv.org/pdf/2410.02764v1;cs.CV;;
2410.02619v1;http://arxiv.org/abs/2410.02619v1;2024-10-03;GI-GS: Global Illumination Decomposition on Gaussian Splatting for  Inverse Rendering;"We present GI-GS, a novel inverse rendering framework that leverages 3D
Gaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel
view synthesis and relighting. In inverse rendering, accurately modeling the
shading processes of objects is essential for achieving high-fidelity results.
Therefore, it is critical to incorporate global illumination to account for
indirect lighting that reaches an object after multiple bounces across the
scene. Previous 3DGS-based methods have attempted to model indirect lighting by
characterizing indirect illumination as learnable lighting volumes or
additional attributes of each Gaussian, while using baked occlusion to
represent shadow effects. These methods, however, fail to accurately model the
complex physical interactions between light and objects, making it impossible
to construct realistic indirect illumination during relighting. To address this
limitation, we propose to calculate indirect lighting using efficient path
tracing with deferred shading. In our framework, we first render a G-buffer to
capture the detailed geometry and material properties of the scene. Then, we
perform physically-based rendering (PBR) only for direct lighting. With the
G-buffer and previous rendering results, the indirect lighting can be
calculated through a lightweight path tracing. Our method effectively models
indirect lighting under any given lighting conditions, thereby achieving better
novel view synthesis and relighting. Quantitative and qualitative results show
that our GI-GS outperforms existing baselines in both rendering quality and
efficiency.";Hongze Chen<author:sep>Zehong Lin<author:sep>Jun Zhang;http://arxiv.org/pdf/2410.02619v1;cs.CV;;gaussian splatting
2410.02571v2;http://arxiv.org/abs/2410.02571v2;2024-10-03;SuperGS: Super-Resolution 3D Gaussian Splatting via Latent Feature Field  and Gradient-guided Splitting;"Recently, 3D Gaussian Splatting (3DGS) has exceled in novel view synthesis
with its real-time rendering capabilities and superior quality. However, it
faces challenges for high-resolution novel view synthesis (HRNVS) due to the
coarse nature of primitives derived from low-resolution input views. To address
this issue, we propose Super-Resolution 3DGS (SuperGS), which is an expansion
of 3DGS designed with a two-stage coarse-to-fine training framework, utilizing
pretrained low-resolution scene representation as an initialization for
super-resolution optimization. Moreover, we introduce Multi-resolution Feature
Gaussian Splatting (MFGS) to incorporates a latent feature field for flexible
feature sampling and Gradient-guided Selective Splitting (GSS) for effective
Gaussian upsampling. By integrating these strategies within the coarse-to-fine
framework ensure both high fidelity and memory efficiency. Extensive
experiments demonstrate that SuperGS surpasses state-of-the-art HRNVS methods
on challenging real-world datasets using only low-resolution inputs.";Shiyun Xie<author:sep>Zhiru Wang<author:sep>Yinghao Zhu<author:sep>Chengwei Pan;http://arxiv.org/pdf/2410.02571v2;cs.CV;;gaussian splatting
2410.01521v1;http://arxiv.org/abs/2410.01521v1;2024-10-02;MiraGe: Editable 2D Images using Gaussian Splatting;"Implicit Neural Representations (INRs) approximate discrete data through
continuous functions and are commonly used for encoding 2D images. Traditional
image-based INRs employ neural networks to map pixel coordinates to RGB values,
capturing shapes, colors, and textures within the network's weights. Recently,
GaussianImage has been proposed as an alternative, using Gaussian functions
instead of neural networks to achieve comparable quality and compression. Such
a solution obtains a quality and compression ratio similar to classical INR
models but does not allow image modification. In contrast, our work introduces
a novel method, MiraGe, which uses mirror reflections to perceive 2D images in
3D space and employs flat-controlled Gaussians for precise 2D image editing.
Our approach improves the rendering quality and allows realistic image
modifications, including human-inspired perception of photos in the 3D world.
Thanks to modeling images in 3D space, we obtain the illusion of 3D-based
modification in 2D images. We also show that our Gaussian representation can be
easily combined with a physics engine to produce physics-based modification of
2D images. Consequently, MiraGe allows for better quality than the standard
approach and natural modification of 2D images.";Joanna Waczyńska<author:sep>Tomasz Szczepanik<author:sep>Piotr Borycki<author:sep>Sławomir Tadeja<author:sep>Thomas Bohné<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2410.01521v1;cs.CV;;gaussian splatting
2410.02103v1;http://arxiv.org/abs/2410.02103v1;2024-10-02;MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis;"Recent works in volume rendering, \textit{e.g.} NeRF and 3D Gaussian
Splatting (3DGS), significantly advance the rendering quality and efficiency
with the help of the learned implicit neural radiance field or 3D Gaussians.
Rendering on top of an explicit representation, the vanilla 3DGS and its
variants deliver real-time efficiency by optimizing the parametric model with
single-view supervision per iteration during training which is adopted from
NeRF. Consequently, certain views are overfitted, leading to unsatisfying
appearance in novel-view synthesis and imprecise 3D geometries. To solve
aforementioned problems, we propose a new 3DGS optimization method embodying
four key novel contributions: 1) We transform the conventional single-view
training paradigm into a multi-view training strategy. With our proposed
multi-view regulation, 3D Gaussian attributes are further optimized without
overfitting certain training views. As a general solution, we improve the
overall accuracy in a variety of scenarios and different Gaussian variants. 2)
Inspired by the benefit introduced by additional views, we further propose a
cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure
concerning different resolutions. 3) Built on top of our multi-view regulated
training, we further propose a cross-ray densification strategy, densifying
more Gaussian kernels in the ray-intersect regions from a selection of views.
4) By further investigating the densification strategy, we found that the
effect of densification should be enhanced when certain views are distinct
dramatically. As a solution, we propose a novel multi-view augmented
densification strategy, where 3D Gaussians are encouraged to get densified to a
sufficient number accordingly, resulting in improved reconstruction accuracy.";Xiaobiao Du<author:sep>Yida Wang<author:sep>Xin Yu;http://arxiv.org/pdf/2410.02103v1;cs.CV;Project Page:https://xiaobiaodu.github.io/mvgs-project/;gaussian splatting<tag:sep>nerf
2410.01535v2;http://arxiv.org/abs/2410.01535v2;2024-10-02;GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene  by Primitives and Gaussians;"Recently, with the development of Neural Radiance Fields and Gaussian
Splatting, 3D reconstruction techniques have achieved remarkably high fidelity.
However, the latent representations learnt by these methods are highly
entangled and lack interpretability. In this paper, we propose a novel
part-aware compositional reconstruction method, called GaussianBlock, that
enables semantically coherent and disentangled representations, allowing for
precise and physical editing akin to building blocks, while simultaneously
maintaining high fidelity. Our GaussianBlock introduces a hybrid representation
that leverages the advantages of both primitives, known for their flexible
actionability and editability, and 3D Gaussians, which excel in reconstruction
quality. Specifically, we achieve semantically coherent primitives through a
novel attention-guided centering loss derived from 2D semantic priors,
complemented by a dynamic splitting and fusion strategy. Furthermore, we
utilize 3D Gaussians that hybridize with primitives to refine structural
details and enhance fidelity. Additionally, a binding inheritance strategy is
employed to strengthen and maintain the connection between the two. Our
reconstructed scenes are evidenced to be disentangled, compositional, and
compact across diverse benchmarks, enabling seamless, direct and precise
editing while maintaining high quality.";Shuyi Jiang<author:sep>Qihao Zhao<author:sep>Hossein Rahmani<author:sep>De Wen Soh<author:sep>Jun Liu<author:sep>Na Zhao;http://arxiv.org/pdf/2410.01535v2;cs.CV;;
2410.01804v3;http://arxiv.org/abs/2410.01804v3;2024-10-02;EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis;"We present Exact Volumetric Ellipsoid Rendering (EVER), a method for
real-time differentiable emission-only volume rendering. Unlike recent
rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive
based representation allows for exact volume rendering, rather than alpha
compositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does
not suffer from popping artifacts and view dependent density, but still
achieves frame rates of $\sim\!30$ FPS at 720p on an NVIDIA RTX4090. Since our
approach is built upon ray tracing it enables effects such as defocus blur and
camera distortion (e.g. such as from fisheye cameras), which are difficult to
achieve by rasterization. We show that our method is more accurate with fewer
blending issues than 3DGS and follow-up work on view-consistent rendering,
especially on the challenging large-scale scenes from the Zip-NeRF dataset
where it achieves sharpest results among real-time techniques.";Alexander Mai<author:sep>Peter Hedman<author:sep>George Kopanas<author:sep>Dor Verbin<author:sep>David Futschik<author:sep>Qiangeng Xu<author:sep>Falko Kuester<author:sep>Jonathan T. Barron<author:sep>Yinda Zhang;http://arxiv.org/pdf/2410.01804v3;cs.CV;Project page: https://half-potato.gitlab.io/posts/ever;gaussian splatting<tag:sep>nerf
2410.01425v1;http://arxiv.org/abs/2410.01425v1;2024-10-02;EVA-Gaussian: 3D Gaussian-based Real-time Human Novel View Synthesis  under Diverse Camera Settings;"The feed-forward based 3D Gaussian Splatting method has demonstrated
exceptional capability in real-time human novel view synthesis. However,
existing approaches are restricted to dense viewpoint settings, which limits
their flexibility in free-viewpoint rendering across a wide range of camera
view angle discrepancies. To address this limitation, we propose a real-time
pipeline named EVA-Gaussian for 3D human novel view synthesis across diverse
camera settings. Specifically, we first introduce an Efficient cross-View
Attention (EVA) module to accurately estimate the position of each 3D Gaussian
from the source images. Then, we integrate the source images with the estimated
Gaussian position map to predict the attributes and feature embeddings of the
3D Gaussians. Moreover, we employ a recurrent feature refiner to correct
artifacts caused by geometric errors in position estimation and enhance visual
fidelity.To further improve synthesis quality, we incorporate a powerful anchor
loss function for both 3D Gaussian attributes and human face landmarks.
Experimental results on the THuman2.0 and THumansit datasets showcase the
superiority of our EVA-Gaussian approach in rendering quality across diverse
camera settings. Project page:
https://zhenliuzju.github.io/huyingdong/EVA-Gaussian.";Yingdong Hu<author:sep>Zhening Liu<author:sep>Jiawei Shao<author:sep>Zehong Lin<author:sep>Jun Zhang;http://arxiv.org/pdf/2410.01425v1;cs.CV;;gaussian splatting
2410.01517v1;http://arxiv.org/abs/2410.01517v1;2024-10-02;UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater  Scene Reconstruction;"3D Gaussian splatting (3DGS) offers the capability to achieve real-time high
quality 3D scene rendering. However, 3DGS assumes that the scene is in a clear
medium environment and struggles to generate satisfactory representations in
underwater scenes, where light absorption and scattering are prevalent and
moving objects are involved. To overcome these, we introduce a novel Gaussian
Splatting-based method, UW-GS, designed specifically for underwater
applications. It introduces a color appearance that models distance-dependent
color variation, employs a new physics-based density control strategy to
enhance clarity for distant objects, and uses a binary motion mask to handle
dynamic content. Optimized with a well-designed loss function supporting for
scattering media and strengthened by pseudo-depth maps, UW-GS outperforms
existing methods with PSNR gains up to 1.26dB. To fully verify the
effectiveness of the model, we also developed a new underwater dataset, S-UW,
with dynamic object masks.";Haoran Wang<author:sep>Nantheera Anantrasirichai<author:sep>Fan Zhang<author:sep>David Bull;http://arxiv.org/pdf/2410.01517v1;cs.CV;;gaussian splatting
2410.01614v1;http://arxiv.org/abs/2410.01614v1;2024-10-02;Gaussian Splatting in Mirrors: Reflection-Aware Rendering via Virtual  Camera Optimization;"Recent advancements in 3D Gaussian Splatting (3D-GS) have revolutionized
novel view synthesis, facilitating real-time, high-quality image rendering.
However, in scenarios involving reflective surfaces, particularly mirrors,
3D-GS often misinterprets reflections as virtual spaces, resulting in blurred
and inconsistent multi-view rendering within mirrors. Our paper presents a
novel method aimed at obtaining high-quality multi-view consistent reflection
rendering by modelling reflections as physically-based virtual cameras. We
estimate mirror planes with depth and normal estimates from 3D-GS and define
virtual cameras that are placed symmetrically about the mirror plane. These
virtual cameras are then used to explain mirror reflections in the scene. To
address imperfections in mirror plane estimates, we propose a straightforward
yet effective virtual camera optimization method to enhance reflection quality.
We collect a new mirror dataset including three real-world scenarios for more
diverse evaluation. Experimental validation on both Mirror-Nerf and our
real-world dataset demonstrate the efficacy of our approach. We achieve
comparable or superior results while significantly reducing training time
compared to previous state-of-the-art.";Zihan Wang<author:sep>Shuzhe Wang<author:sep>Matias Turkulainen<author:sep>Junyuan Fang<author:sep>Juho Kannala;http://arxiv.org/pdf/2410.01614v1;cs.CV;To be published on 2024 British Machine Vision Conference;gaussian splatting<tag:sep>nerf
2410.01404v1;http://arxiv.org/abs/2410.01404v1;2024-10-02;Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection;"Skins wrapping around our bodies, leathers covering over the sofa, sheet
metal coating the car - it suggests that objects are enclosed by a series of
continuous surfaces, which provides us with informative geometry prior for
objectness deduction. In this paper, we propose Gaussian-Det which leverages
Gaussian Splatting as surface representation for multi-view based 3D object
detection. Unlike existing monocular or NeRF-based methods which depict the
objects via discrete positional data, Gaussian-Det models the objects in a
continuous manner by formulating the input Gaussians as feature descriptors on
a mass of partial surfaces. Furthermore, to address the numerous outliers
inherently introduced by Gaussian splatting, we accordingly devise a Closure
Inferring Module (CIM) for the comprehensive surface-based objectness
deduction. CIM firstly estimates the probabilistic feature residuals for
partial surfaces given the underdetermined nature of Gaussian Splatting, which
are then coalesced into a holistic representation on the overall surface
closure of the object proposal. In this way, the surface information
Gaussian-Det exploits serves as the prior on the quality and reliability of
objectness and the information basis of proposal refinement. Experiments on
both synthetic and real-world datasets demonstrate that Gaussian-Det
outperforms various existing approaches, in terms of both average precision and
recall.";Hongru Yan<author:sep>Yu Zheng<author:sep>Yueqi Duan;http://arxiv.org/pdf/2410.01404v1;cs.CV;;gaussian splatting<tag:sep>nerf
2410.01647v1;http://arxiv.org/abs/2410.01647v1;2024-10-02;3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and  Box-Focused Sampling for 3D Object Detection;"Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and
have been adapted for 3D Object Detection (3DOD), offering a promising approach
to 3DOD through view-synthesis representation. However, NeRF faces inherent
limitations: (i) limited representational capacity for 3DOD due to its implicit
nature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS)
has emerged as an explicit 3D representation that addresses these limitations.
Inspired by these advantages, this paper introduces 3DGS into 3DOD for the
first time, identifying two main challenges: (i) Ambiguous spatial distribution
of Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision,
resulting in unclear 3D spatial distribution of Gaussian blobs and poor
differentiation between objects and background, which hinders 3DOD; (ii)
Excessive background blobs: 2D images often include numerous background pixels,
leading to densely reconstructed 3DGS with many noisy Gaussian blobs
representing the background, negatively affecting detection. To tackle the
challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D
images, and propose an elegant and efficient solution by incorporating 2D
Boundary Guidance to significantly enhance the spatial distribution of Gaussian
blobs, resulting in clearer differentiation between objects and their
background. To address the challenge (ii), we propose a Box-Focused Sampling
strategy using 2D boxes to generate object probability distribution in 3D
spaces, allowing effective probabilistic sampling in 3D to retain more object
blobs and reduce noisy background blobs. Benefiting from our designs, our
3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det,
achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet
dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.";Yang Cao<author:sep>Yuanliang Jv<author:sep>Dan Xu;http://arxiv.org/pdf/2410.01647v1;cs.CV;Code Page: https://github.com/yangcaoai/3DGS-DET;gaussian splatting<tag:sep>nerf
2410.00672v1;http://arxiv.org/abs/2410.00672v1;2024-10-01;GMT: Enhancing Generalizable Neural Rendering via Geometry-Driven  Multi-Reference Texture Transfer;"Novel view synthesis (NVS) aims to generate images at arbitrary viewpoints
using multi-view images, and recent insights from neural radiance fields (NeRF)
have contributed to remarkable improvements. Recently, studies on generalizable
NeRF (G-NeRF) have addressed the challenge of per-scene optimization in NeRFs.
The construction of radiance fields on-the-fly in G-NeRF simplifies the NVS
process, making it well-suited for real-world applications. Meanwhile, G-NeRF
still struggles in representing fine details for a specific scene due to the
absence of per-scene optimization, even with texture-rich multi-view source
inputs. As a remedy, we propose a Geometry-driven Multi-reference Texture
transfer network (GMT) available as a plug-and-play module designed for G-NeRF.
Specifically, we propose ray-imposed deformable convolution (RayDCN), which
aligns input and reference features reflecting scene geometry. Additionally,
the proposed texture preserving transformer (TP-Former) aggregates multi-view
source features while preserving texture information. Consequently, our module
enables direct interaction between adjacent pixels during the image enhancement
process, which is deficient in G-NeRF models with an independent rendering
process per pixel. This addresses constraints that hinder the ability to
capture high-frequency details. Experiments show that our plug-and-play module
consistently improves G-NeRF models on various benchmark datasets.";Youngho Yoon<author:sep>Hyun-Kurl Jang<author:sep>Kuk-Jin Yoon;http://arxiv.org/pdf/2410.00672v1;cs.CV;"Accepted at ECCV 2024. Code available at
  https://github.com/yh-yoon/GMT";nerf
2410.00486v2;http://arxiv.org/abs/2410.00486v2;2024-10-01;CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM;"Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with
photorealistic scene reconstruction emerging as a key challenge. To address
this, we introduce Computational Alignment for Real-Time Gaussian Splatting
SLAM (CaRtGS), a novel method enhancing the efficiency and quality of
photorealistic scene reconstruction in real-time environments. Leveraging 3D
Gaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and
processing speed, which is crucial for scene photorealistic reconstruction. Our
approach tackles computational misalignment in Gaussian Splatting SLAM
(GS-SLAM) through an adaptive strategy that optimizes training, addresses
long-tail optimization, and refines densification. Experiments on Replica and
TUM-RGBD datasets demonstrate CaRtGS's effectiveness in achieving high-fidelity
rendering with fewer Gaussian primitives. This work propels SLAM towards
real-time, photorealistic dense rendering, significantly advancing
photorealistic scene representation. For the benefit of the research community,
we release the code on our project website:
https://dapengfeng.github.io/cartgs.";Dapeng Feng<author:sep>Zhiqiang Chen<author:sep>Yizhen Yin<author:sep>Shipeng Zhong<author:sep>Yuhua Qi<author:sep>Hongbo Chen;http://arxiv.org/pdf/2410.00486v2;cs.CV;"Upon a thorough internal review, we have identified that our
  manuscript lacks proper citation for a critical expression within the
  methodology section. In this revised version, we add Taming-3DGS as a
  citation in the splat-wise backpropagation statement";gaussian splatting
2410.00386v1;http://arxiv.org/abs/2410.00386v1;2024-10-01;Seamless Augmented Reality Integration in Arthroscopy: A Pipeline for  Articular Reconstruction and Guidance;"Arthroscopy is a minimally invasive surgical procedure used to diagnose and
treat joint problems. The clinical workflow of arthroscopy typically involves
inserting an arthroscope into the joint through a small incision, during which
surgeons navigate and operate largely by relying on their visual assessment
through the arthroscope. However, the arthroscope's restricted field of view
and lack of depth perception pose challenges in navigating complex articular
structures and achieving surgical precision during procedures. Aiming at
enhancing intraoperative awareness, we present a robust pipeline that
incorporates simultaneous localization and mapping, depth estimation, and 3D
Gaussian splatting to realistically reconstruct intra-articular structures
solely based on monocular arthroscope video. Extending 3D reconstruction to
Augmented Reality (AR) applications, our solution offers AR assistance for
articular notch measurement and annotation anchoring in a human-in-the-loop
manner. Compared to traditional Structure-from-Motion and Neural Radiance
Field-based methods, our pipeline achieves dense 3D reconstruction and
competitive rendering fidelity with explicit 3D representation in 7 minutes on
average. When evaluated on four phantom datasets, our method achieves RMSE =
2.21mm reconstruction error, PSNR = 32.86 and SSIM = 0.89 on average. Because
our pipeline enables AR reconstruction and guidance directly from monocular
arthroscopy without any additional data and/or hardware, our solution may hold
the potential for enhancing intraoperative awareness and facilitating surgical
precision in arthroscopy. Our AR measurement tool achieves accuracy within 1.59
+/- 1.81mm and the AR annotation tool achieves a mIoU of 0.721.";Hongchao Shu<author:sep>Mingxu Liu<author:sep>Lalithkumar Seenivasan<author:sep>Suxi Gu<author:sep>Ping-Cheng Ku<author:sep>Jonathan Knopf<author:sep>Russell Taylor<author:sep>Mathias Unberath;http://arxiv.org/pdf/2410.00386v1;cs.CV;"8 pages, with 2 additional pages as the supplementary. Accepted by
  AE-CAI 2024";gaussian splatting
2410.00299v1;http://arxiv.org/abs/2410.00299v1;2024-10-01;GSPR: Multimodal Place Recognition Using 3D Gaussian Splatting for  Autonomous Driving;"Place recognition is a crucial module to ensure autonomous vehicles obtain
usable localization information in GPS-denied environments. In recent years,
multimodal place recognition methods have gained increasing attention due to
their ability to overcome the weaknesses of unimodal sensor systems by
leveraging complementary information from different modalities. However,
challenges arise from the necessity of harmonizing data across modalities and
exploiting the spatio-temporal correlations between them sufficiently. In this
paper, we propose a 3D Gaussian Splatting-based multimodal place recognition
neural network dubbed GSPR. It explicitly combines multi-view RGB images and
LiDAR point clouds into a spatio-temporally unified scene representation with
the proposed Multimodal Gaussian Splatting. A network composed of 3D graph
convolution and transformer is designed to extract high-level spatio-temporal
features and global descriptors from the Gaussian scenes for place recognition.
We evaluate our method on the nuScenes dataset, and the experimental results
demonstrate that our method can effectively leverage complementary strengths of
both multi-view cameras and LiDAR, achieving SOTA place recognition performance
while maintaining solid generalization ability. Our open-source code is
available at https://github.com/QiZS-BIT/GSPR.";Zhangshuo Qi<author:sep>Junyi Ma<author:sep>Jingyi Xu<author:sep>Zijie Zhou<author:sep>Luqi Cheng<author:sep>Guangming Xiong;http://arxiv.org/pdf/2410.00299v1;cs.CV;8 pages, 6 figures;gaussian splatting
2409.20289v1;http://arxiv.org/abs/2409.20289v1;2024-09-30;Distributed NeRF Learning for Collaborative Multi-Robot Perception;"Effective environment perception is crucial for enabling downstream robotic
applications. Individual robotic agents often face occlusion and limited
visibility issues, whereas multi-agent systems can offer a more comprehensive
mapping of the environment, quicker coverage, and increased fault tolerance. In
this paper, we propose a collaborative multi-agent perception system where
agents collectively learn a neural radiance field (NeRF) from posed RGB images
to represent a scene. Each agent processes its local sensory data and shares
only its learned NeRF model with other agents, reducing communication overhead.
Given NeRF's low memory footprint, this approach is well-suited for robotic
systems with limited bandwidth, where transmitting all raw data is impractical.
Our distributed learning framework ensures consistency across agents' local
NeRF models, enabling convergence to a unified scene representation. We show
the effectiveness of our method through an extensive set of experiments on
datasets containing challenging real-world scenes, achieving performance
comparable to centralized mapping of the environment where data is sent to a
central server for processing. Additionally, we find that multi-agent learning
provides regularization benefits, improving geometric consistency in scenarios
with sparse input views. We show that in such scenarios, multi-agent mapping
can even outperform centralized training.";Hongrui Zhao<author:sep>Boris Ivanovic<author:sep>Negar Mehr;http://arxiv.org/pdf/2409.20289v1;cs.RO;;nerf
2409.20111v1;http://arxiv.org/abs/2409.20111v1;2024-09-30;Robust Gaussian Splatting SLAM by Leveraging Loop Closure;"3D Gaussian Splatting algorithms excel in novel view rendering applications
and have been adapted to extend the capabilities of traditional SLAM systems.
However, current Gaussian Splatting SLAM methods, designed mainly for hand-held
RGB or RGB-D sensors, struggle with tracking drifts when used with rotating
RGB-D camera setups. In this paper, we propose a robust Gaussian Splatting SLAM
architecture that utilizes inputs from rotating multiple RGB-D cameras to
achieve accurate localization and photorealistic rendering performance. The
carefully designed Gaussian Splatting Loop Closure module effectively addresses
the issue of accumulated tracking and mapping errors found in conventional
Gaussian Splatting SLAM systems. First, each Gaussian is associated with an
anchor frame and categorized as historical or novel based on its timestamp. By
rendering different types of Gaussians at the same viewpoint, the proposed loop
detection strategy considers both co-visibility relationships and distinct
rendering outcomes. Furthermore, a loop closure optimization approach is
proposed to remove camera pose drift and maintain the high quality of 3D
Gaussian models. The approach uses a lightweight pose graph optimization
algorithm to correct pose drift and updates Gaussians based on the optimized
poses. Additionally, a bundle adjustment scheme further refines camera poses
using photometric and geometric constraints, ultimately enhancing the global
consistency of scenarios. Quantitative and qualitative evaluations on both
synthetic and real-world datasets demonstrate that our method outperforms
state-of-the-art methods in camera pose estimation and novel view rendering
tasks. The code will be open-sourced for the community.";Zunjie Zhu<author:sep>Youxu Fang<author:sep>Xin Li<author:sep>Chengang Yan<author:sep>Feng Xu<author:sep>Chau Yuen<author:sep>Yanyan Li;http://arxiv.org/pdf/2409.20111v1;cs.RO;;gaussian splatting
2409.20276v1;http://arxiv.org/abs/2409.20276v1;2024-09-30;Active Neural Mapping at Scale;"We introduce a NeRF-based active mapping system that enables efficient and
robust exploration of large-scale indoor environments. The key to our approach
is the extraction of a generalized Voronoi graph (GVG) from the continually
updated neural map, leading to the synergistic integration of scene geometry,
appearance, topology, and uncertainty. Anchoring uncertain areas induced by the
neural map to the vertices of GVG allows the exploration to undergo adaptive
granularity along a safe path that traverses unknown areas efficiently.
Harnessing a modern hybrid NeRF representation, the proposed system achieves
competitive results in terms of reconstruction accuracy, coverage completeness,
and exploration efficiency even when scaling up to large indoor environments.
Extensive results at different scales validate the efficacy of the proposed
system.";Zijia Kuang<author:sep>Zike Yan<author:sep>Hao Zhao<author:sep>Guyue Zhou<author:sep>Hongbin Zha;http://arxiv.org/pdf/2409.20276v1;cs.CV;;nerf
2409.20291v1;http://arxiv.org/abs/2409.20291v1;2024-09-30;RL-GSBridge: 3D Gaussian Splatting Based Real2Sim2Real Method for  Robotic Manipulation Learning;"Sim-to-Real refers to the process of transferring policies learned in
simulation to the real world, which is crucial for achieving practical robotics
applications. However, recent Sim2real methods either rely on a large amount of
augmented data or large learning models, which is inefficient for specific
tasks. In recent years, radiance field-based reconstruction methods, especially
the emergence of 3D Gaussian Splatting, making it possible to reproduce
realistic real-world scenarios. To this end, we propose a novel
real-to-sim-to-real reinforcement learning framework, RL-GSBridge, which
introduces a mesh-based 3D Gaussian Splatting method to realize zero-shot
sim-to-real transfer for vision-based deep reinforcement learning. We improve
the mesh-based 3D GS modeling method by using soft binding constraints,
enhancing the rendering quality of mesh models. We then employ a GS editing
approach to synchronize rendering with the physics simulator, reflecting the
interactions of the physical robot more accurately. Through a series of
sim-to-real robotic arm experiments, including grasping and pick-and-place
tasks, we demonstrate that RL-GSBridge maintains a satisfactory success rate in
real-world task completion during sim-to-real transfer. Furthermore, a series
of rendering metrics and visualization results indicate that our proposed
mesh-based 3D Gaussian reduces artifacts in unstructured objects, demonstrating
more realistic rendering performance.";Yuxuan Wu<author:sep>Lei Pan<author:sep>Wenhua Wu<author:sep>Guangming Wang<author:sep>Yanzi Miao<author:sep>Hesheng Wang;http://arxiv.org/pdf/2409.20291v1;cs.RO;7 pages, 5 figures, 4 tables, under review by ICRA2025;gaussian splatting
2409.20043v1;http://arxiv.org/abs/2409.20043v1;2024-09-30;OPONeRF: One-Point-One NeRF for Robust Neural Rendering;"In this paper, we propose a One-Point-One NeRF (OPONeRF) framework for robust
scene rendering. Existing NeRFs are designed based on a key assumption that the
target scene remains unchanged between the training and test time. However,
small but unpredictable perturbations such as object movements, light changes
and data contaminations broadly exist in real-life 3D scenes, which lead to
significantly defective or failed rendering results even for the recent
state-of-the-art generalizable methods. To address this, we propose a
divide-and-conquer framework in OPONeRF that adaptively responds to local scene
variations via personalizing appropriate point-wise parameters, instead of
fitting a single set of NeRF parameters that are inactive to test-time unseen
changes. Moreover, to explicitly capture the local uncertainty, we decompose
the point representation into deterministic mapping and probabilistic
inference. In this way, OPONeRF learns the sharable invariance and
unsupervisedly models the unexpected scene variations between the training and
testing scenes. To validate the effectiveness of the proposed method, we
construct benchmarks from both realistic and synthetic data with diverse
test-time perturbations including foreground motions, illumination variations
and multi-modality noises, which are more challenging than conventional
generalization and temporal reconstruction benchmarks. Experimental results
show that our OPONeRF outperforms state-of-the-art NeRFs on various evaluation
metrics through benchmark experiments and cross-scene evaluations. We further
show the efficacy of the proposed method via experimenting on other existing
generalization-based benchmarks and incorporating the idea of One-Point-One
NeRF into other advanced baseline methods.";Yu Zheng<author:sep>Yueqi Duan<author:sep>Kangfu Zheng<author:sep>Hongru Yan<author:sep>Jiwen Lu<author:sep>Jie Zhou;http://arxiv.org/pdf/2409.20043v1;cs.CV;;nerf
2409.19702v2;http://arxiv.org/abs/2409.19702v2;2024-09-29;RNG: Relightable Neural Gaussians;"3D Gaussian Splatting (3DGS) has shown its impressive power in novel view
synthesis. However, creating relightable 3D assets, especially for objects with
ill-defined shapes (e.g., fur), is still a challenging task. For these scenes,
the decomposition between the light, geometry, and material is more ambiguous,
as neither the surface constraints nor the analytical shading model hold. To
address this issue, we propose RNG, a novel representation of relightable
neural Gaussians, enabling the relighting of objects with both hard surfaces or
fluffy boundaries. We avoid any assumptions in the shading model but maintain
feature vectors, which can be further decoded by an MLP into colors, in each
Gaussian point. Following prior work, we utilize a point light to reduce the
ambiguity and introduce a shadow-aware condition to the network. We
additionally propose a depth refinement network to help the shadow computation
under the 3DGS framework, leading to better shadow effects under point lights.
Furthermore, to avoid the blurriness brought by the alpha-blending in 3DGS, we
design a hybrid forward-deferred optimization strategy. As a result, we achieve
about $20\times$ faster in training and about $600\times$ faster in rendering
than prior work based on neural radiance fields, with $60$ frames per second on
an RTX4090.";Jiahui Fan<author:sep>Fujun Luan<author:sep>Jian Yang<author:sep>Miloš Hašan<author:sep>Beibei Wang;http://arxiv.org/pdf/2409.19702v2;cs.CV;;gaussian splatting
2409.19405v1;http://arxiv.org/abs/2409.19405v1;2024-09-28;G3R: Gradient Guided Generalizable Reconstruction;"Large scale 3D scene reconstruction is important for applications such as
virtual reality and simulation. Existing neural rendering approaches (e.g.,
NeRF, 3DGS) have achieved realistic reconstructions on large scenes, but
optimize per scene, which is expensive and slow, and exhibit noticeable
artifacts under large view changes due to overfitting. Generalizable approaches
or large reconstruction models are fast, but primarily work for small
scenes/objects and often produce lower quality rendering results. In this work,
we introduce G3R, a generalizable reconstruction approach that can efficiently
predict high-quality 3D scene representations for large scenes. We propose to
learn a reconstruction network that takes the gradient feedback signals from
differentiable rendering to iteratively update a 3D scene representation,
combining the benefits of high photorealism from per-scene optimization with
data-driven priors from fast feed-forward prediction methods. Experiments on
urban-driving and drone datasets show that G3R generalizes across diverse large
scenes and accelerates the reconstruction process by at least 10x while
achieving comparable or better realism compared to 3DGS, and also being more
robust to large view changes.";Yun Chen<author:sep>Jingkang Wang<author:sep>Ze Yang<author:sep>Sivabalan Manivasagam<author:sep>Raquel Urtasun;http://arxiv.org/pdf/2409.19405v1;cs.CV;ECCV 2024. Project page: https://waabi.ai/g3r;nerf
2409.19228v1;http://arxiv.org/abs/2409.19228v1;2024-09-28;GS-EVT: Cross-Modal Event Camera Tracking based on Gaussian Splatting;"Reliable self-localization is a foundational skill for many intelligent
mobile platforms. This paper explores the use of event cameras for motion
tracking thereby providing a solution with inherent robustness under difficult
dynamics and illumination. In order to circumvent the challenge of event
camera-based mapping, the solution is framed in a cross-modal way. It tracks a
map representation that comes directly from frame-based cameras. Specifically,
the proposed method operates on top of gaussian splatting, a state-of-the-art
representation that permits highly efficient and realistic novel view
synthesis. The key of our approach consists of a novel pose parametrization
that uses a reference pose plus first order dynamics for local differential
image rendering. The latter is then compared against images of integrated
events in a staggered coarse-to-fine optimization scheme. As demonstrated by
our results, the realistic view rendering ability of gaussian splatting leads
to stable and accurate tracking across a variety of both publicly available and
newly recorded data sequences.";Tao Liu<author:sep>Runze Yuan<author:sep>Yi'ang Ju<author:sep>Xun Xu<author:sep>Jiaqi Yang<author:sep>Xiangting Meng<author:sep>Xavier Lagorce<author:sep>Laurent Kneip;http://arxiv.org/pdf/2409.19228v1;cs.CV;;gaussian splatting
2409.19215v1;http://arxiv.org/abs/2409.19215v1;2024-09-28;1st Place Solution to the 8th HANDS Workshop Challenge -- ARCTIC Track:  3DGS-based Bimanual Category-agnostic Interaction Reconstruction;"This report describes our 1st place solution to the 8th HANDS workshop
challenge (ARCTIC track) in conjunction with ECCV 2024. In this challenge, we
address the task of bimanual category-agnostic hand-object interaction
reconstruction, which aims to generate 3D reconstructions of both hands and the
object from a monocular video, without relying on predefined templates. This
task is particularly challenging due to the significant occlusion and dynamic
contact between the hands and the object during bimanual manipulation. We
worked to resolve these issues by introducing a mask loss and a 3D contact
loss, respectively. Moreover, we applied 3D Gaussian Splatting (3DGS) to this
task. As a result, our method achieved a value of 38.69 in the main metric,
CD$_h$, on the ARCTIC test set.";Jeongwan On<author:sep>Kyeonghwan Gwak<author:sep>Gunyoung Kang<author:sep>Hyein Hwang<author:sep>Soohyun Hwang<author:sep>Junuk Cha<author:sep>Jaewook Han<author:sep>Seungryul Baek;http://arxiv.org/pdf/2409.19215v1;cs.CV;;gaussian splatting
2409.18852v1;http://arxiv.org/abs/2409.18852v1;2024-09-27;Space-time 2D Gaussian Splatting for Accurate Surface Reconstruction  under Complex Dynamic Scenes;"Previous surface reconstruction methods either suffer from low geometric
accuracy or lengthy training times when dealing with real-world complex dynamic
scenes involving multi-person activities, and human-object interactions. To
tackle the dynamic contents and the occlusions in complex scenes, we present a
space-time 2D Gaussian Splatting approach. Specifically, to improve geometric
quality in dynamic scenes, we learn canonical 2D Gaussian splats and deform
these 2D Gaussian splats while enforcing the disks of the Gaussian located on
the surface of the objects by introducing depth and normal regularizers.
Further, to tackle the occlusion issues in complex scenes, we introduce a
compositional opacity deformation strategy, which further reduces the surface
recovery of those occluded areas. Experiments on real-world sparse-view video
datasets and monocular dynamic datasets demonstrate that our reconstructions
outperform state-of-the-art methods, especially for the surface of the details.
The project page and more visualizations can be found at:
https://tb2-sy.github.io/st-2dgs/.";Shuo Wang<author:sep>Binbin Huang<author:sep>Ruoyu Wang<author:sep>Shenghua Gao;http://arxiv.org/pdf/2409.18852v1;cs.CV;Project page: https://tb2-sy.github.io/st-2dgs/;gaussian splatting
2409.19039v1;http://arxiv.org/abs/2409.19039v1;2024-09-27;Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated  Object Segmentation;"The creation of digital replicas of physical objects has valuable
applications for the preservation and dissemination of tangible cultural
heritage. However, existing methods are often slow, expensive, and require
expert knowledge. We propose a pipeline to generate a 3D replica of a scene
using only RGB images (e.g. photos of a museum) and then extract a model for
each item of interest (e.g. pieces in the exhibit). We do this by leveraging
the advancements in novel view synthesis and Gaussian Splatting, modified to
enable efficient 3D segmentation. This approach does not need manual
annotation, and the visual inputs can be captured using a standard smartphone,
making it both affordable and easy to deploy. We provide an overview of the
method and baseline evaluation of the accuracy of object segmentation. The code
is available at https://mahtaabdn.github.io/gaussian_heritage.github.io/.";Mahtab Dahaghin<author:sep>Myrna Castillo<author:sep>Kourosh Riahidehkordi<author:sep>Matteo Toso<author:sep>Alessio Del Bue;http://arxiv.org/pdf/2409.19039v1;cs.CV;;gaussian splatting
2409.17729v1;http://arxiv.org/abs/2409.17729v1;2024-09-26;Neural Implicit Representation for Highly Dynamic LiDAR Mapping and  Odometry;"Recent advancements in Simultaneous Localization and Mapping (SLAM) have
increasingly highlighted the robustness of LiDAR-based techniques. At the same
time, Neural Radiance Fields (NeRF) have introduced new possibilities for 3D
scene reconstruction, exemplified by SLAM systems. Among these, NeRF-LOAM has
shown notable performance in NeRF-based SLAM applications. However, despite its
strengths, these systems often encounter difficulties in dynamic outdoor
environments due to their inherent static assumptions. To address these
limitations, this paper proposes a novel method designed to improve
reconstruction in highly dynamic outdoor scenes. Based on NeRF-LOAM, the
proposed approach consists of two primary components. First, we separate the
scene into static background and dynamic foreground. By identifying and
excluding dynamic elements from the mapping process, this segmentation enables
the creation of a dense 3D map that accurately represents the static background
only. The second component extends the octree structure to support
multi-resolution representation. This extension not only enhances
reconstruction quality but also aids in the removal of dynamic objects
identified by the first module. Additionally, Fourier feature encoding is
applied to the sampled points, capturing high-frequency information and leading
to more complete reconstruction results. Evaluations on various datasets
demonstrate that our method achieves more competitive results compared to
current state-of-the-art approaches.";Qi Zhang<author:sep>He Wang<author:sep>Ru Li<author:sep>Wenbin Li;http://arxiv.org/pdf/2409.17729v1;cs.CV;;nerf
2409.17988v1;http://arxiv.org/abs/2409.17988v1;2024-09-26;Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or  Low-light Conditions;"The stark contrast in the design philosophy of an event camera makes it
particularly ideal for operating under high-speed, high dynamic range and
low-light conditions, where standard cameras underperform. Nonetheless, event
cameras still suffer from some amount of motion blur, especially under these
challenging conditions, in contrary to what most think. This is attributed to
the limited bandwidth of the event sensor pixel, which is mostly proportional
to the light intensity. Thus, to ensure that event cameras can truly excel in
such conditions where it has an edge over standard cameras, it is crucial to
account for event motion blur in downstream applications, especially
reconstruction. However, none of the recent works on reconstructing Neural
Radiance Fields (NeRFs) from events, nor event simulators, have considered the
full effects of event motion blur. To this end, we propose, Deblur e-NeRF, a
novel method to directly and effectively reconstruct blur-minimal NeRFs from
motion-blurred events generated under high-speed motion or low-light
conditions. The core component of this work is a physically-accurate pixel
bandwidth model proposed to account for event motion blur under arbitrary speed
and lighting conditions. We also introduce a novel threshold-normalized total
variation loss to improve the regularization of large textureless patches.
Experiments on real and novel realistically simulated sequences verify our
effectiveness. Our code, event simulator and synthetic event dataset will be
open-sourced.";Weng Fei Low<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2409.17988v1;cs.CV;"Accepted to ECCV 2024. Project website is accessible at
  https://wengflow.github.io/deblur-e-nerf. arXiv admin note: text overlap with
  arXiv:2006.07722 by other authors";nerf
2409.18122v1;http://arxiv.org/abs/2409.18122v1;2024-09-26;RT-GuIDE: Real-Time Gaussian splatting for Information-Driven  Exploration;"We propose a framework for active mapping and exploration that leverages
Gaussian splatting for constructing information-rich maps. Further, we develop
a parallelized motion planning algorithm that can exploit the Gaussian map for
real-time navigation. The Gaussian map constructed onboard the robot is
optimized for both photometric and geometric quality while enabling real-time
situational awareness for autonomy. We show through simulation experiments that
our method is competitive with approaches that use alternate information gain
metrics, while being orders of magnitude faster to compute. In real-world
experiments, our algorithm achieves better map quality (10% higher Peak
Signal-to-Noise Ratio (PSNR) and 30% higher geometric reconstruction accuracy)
than Gaussian maps constructed by traditional exploration baselines. Experiment
videos and more details can be found on our project page:
https://tyuezhan.github.io/RT_GuIDE/";Yuezhan Tao<author:sep>Dexter Ong<author:sep>Varun Murali<author:sep>Igor Spasojevic<author:sep>Pratik Chaudhari<author:sep>Vijay Kumar;http://arxiv.org/pdf/2409.18122v1;cs.RO;Submitted to ICRA2025;gaussian splatting
2409.17624v1;http://arxiv.org/abs/2409.17624v1;2024-09-26;HGS-Planner: Hierarchical Planning Framework for Active Scene  Reconstruction Using 3D Gaussian Splatting;"In complex missions such as search and rescue,robots must make intelligent
decisions in unknown environments, relying on their ability to perceive and
understand their surroundings. High-quality and real-time reconstruction
enhances situational awareness and is crucial for intelligent robotics.
Traditional methods often struggle with poor scene representation or are too
slow for real-time use. Inspired by the efficacy of 3D Gaussian Splatting
(3DGS), we propose a hierarchical planning framework for fast and high-fidelity
active reconstruction. Our method evaluates completion and quality gain to
adaptively guide reconstruction, integrating global and local planning for
efficiency. Experiments in simulated and real-world environments show our
approach outperforms existing real-time methods.";Zijun Xu<author:sep>Rui Jin<author:sep>Ke Wu<author:sep>Yi Zhao<author:sep>Zhiwei Zhang<author:sep>Jieru Zhao<author:sep>Zhongxue Gan<author:sep>Wenchao Ding;http://arxiv.org/pdf/2409.17624v1;cs.RO;;gaussian splatting
2409.17459v1;http://arxiv.org/abs/2409.17459v1;2024-09-26;TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic  Scene;"Despite advancements in Neural Implicit models for 3D surface reconstruction,
handling dynamic environments with arbitrary rigid, non-rigid, or deformable
entities remains challenging. Many template-based methods are entity-specific,
focusing on humans, while generic reconstruction methods adaptable to such
dynamic scenes often require additional inputs like depth or optical flow or
rely on pre-trained image features for reasonable outcomes. These methods
typically use latent codes to capture frame-by-frame deformations. In contrast,
some template-free methods bypass these requirements and adopt traditional LBS
(Linear Blend Skinning) weights for a detailed representation of deformable
object motions, although they involve complex optimizations leading to lengthy
training times. To this end, as a remedy, this paper introduces TFS-NeRF, a
template-free 3D semantic NeRF for dynamic scenes captured from sparse or
single-view RGB videos, featuring interactions among various entities and more
time-efficient than other LBS-based approaches. Our framework uses an
Invertible Neural Network (INN) for LBS prediction, simplifying the training
process. By disentangling the motions of multiple entities and optimizing
per-entity skinning weights, our method efficiently generates accurate,
semantically separable geometries. Extensive experiments demonstrate that our
approach produces high-quality reconstructions of both deformable and
non-deformable objects in complex interactions, with improved training
efficiency compared to existing methods.";Sandika Biswas<author:sep>Qianyi Wu<author:sep>Biplab Banerjee<author:sep>Hamid Rezatofighi;http://arxiv.org/pdf/2409.17459v1;cs.CV;Accepted in NeuRIPS 2024;nerf
2409.18108v1;http://arxiv.org/abs/2409.18108v1;2024-09-26;Language-Embedded Gaussian Splats (LEGS): Incrementally Building  Room-Scale Representations with a Mobile Robot;"Building semantic 3D maps is valuable for searching for objects of interest
in offices, warehouses, stores, and homes. We present a mapping system that
incrementally builds a Language-Embedded Gaussian Splat (LEGS): a detailed 3D
scene representation that encodes both appearance and semantics in a unified
representation. LEGS is trained online as a robot traverses its environment to
enable localization of open-vocabulary object queries. We evaluate LEGS on 4
room-scale scenes where we query for objects in the scene to assess how LEGS
can capture semantic meaning. We compare LEGS to LERF and find that while both
systems have comparable object query success rates, LEGS trains over 3.5x
faster than LERF. Results suggest that a multi-camera setup and incremental
bundle adjustment can boost visual reconstruction quality in constrained robot
trajectories, and suggest LEGS can localize open-vocabulary and long-tail
object queries with up to 66% accuracy.";Justin Yu<author:sep>Kush Hari<author:sep>Kishore Srinivas<author:sep>Karim El-Refai<author:sep>Adam Rashid<author:sep>Chung Min Kim<author:sep>Justin Kerr<author:sep>Richard Cheng<author:sep>Muhammad Zubair Irshad<author:sep>Ashwin Balakrishna<author:sep>Thomas Kollar<author:sep>Ken Goldberg;http://arxiv.org/pdf/2409.18108v1;cs.RO;;
2409.17917v1;http://arxiv.org/abs/2409.17917v1;2024-09-26;WaSt-3D: Wasserstein-2 Distance for Scene-to-Scene Stylization on 3D  Gaussians;"While style transfer techniques have been well-developed for 2D image
stylization, the extension of these methods to 3D scenes remains relatively
unexplored. Existing approaches demonstrate proficiency in transferring colors
and textures but often struggle with replicating the geometry of the scenes. In
our work, we leverage an explicit Gaussian Splatting (GS) representation and
directly match the distributions of Gaussians between style and content scenes
using the Earth Mover's Distance (EMD). By employing the entropy-regularized
Wasserstein-2 distance, we ensure that the transformation maintains spatial
smoothness. Additionally, we decompose the scene stylization problem into
smaller chunks to enhance efficiency. This paradigm shift reframes stylization
from a pure generative process driven by latent space losses to an explicit
matching of distributions between two Gaussian representations. Our method
achieves high-resolution 3D stylization by faithfully transferring details from
3D style scenes onto the content scene. Furthermore, WaSt-3D consistently
delivers results across diverse content and style scenes without necessitating
any training, as it relies solely on optimization-based techniques. See our
project page for additional results and source code:
$\href{https://compvis.github.io/wast3d/}{https://compvis.github.io/wast3d/}$.";Dmytro Kotovenko<author:sep>Olga Grebenkova<author:sep>Nikolaos Sarafianos<author:sep>Avinash Paliwal<author:sep>Pingchuan Ma<author:sep>Omid Poursaeed<author:sep>Sreyas Mohan<author:sep>Yuchen Fan<author:sep>Yilei Li<author:sep>Rakesh Ranjan<author:sep>Björn Ommer;http://arxiv.org/pdf/2409.17917v1;cs.CV;;gaussian splatting
2409.18057v1;http://arxiv.org/abs/2409.18057v1;2024-09-26;LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field;"Recent works have shown that neural radiance fields (NeRFs) on top of
parametric models have reached SOTA quality to build photorealistic head
avatars from a monocular video. However, one major limitation of the NeRF-based
avatars is the slow rendering speed due to the dense point sampling of NeRF,
preventing them from broader utility on resource-constrained devices. We
introduce LightAvatar, the first head avatar model based on neural light fields
(NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose
via a single network forward pass, without using mesh or volume rendering. The
proposed approach, while being conceptually appealing, poses a significant
challenge towards real-time efficiency and training stability. To resolve them,
we introduce dedicated network designs to obtain proper representations for the
NeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a
distillation-based training strategy that uses a pretrained avatar model as
teacher to synthesize abundant pseudo data for training. A warping field
network is introduced to correct the fitting error in the real data so that the
model can learn better. Extensive experiments suggest that our method can
achieve new SOTA image quality quantitatively or qualitatively, while being
significantly faster than the counterparts, reporting 174.1 FPS (512x512
resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.";Huan Wang<author:sep>Feitong Tan<author:sep>Ziqian Bai<author:sep>Yinda Zhang<author:sep>Shichen Liu<author:sep>Qiangeng Xu<author:sep>Menglei Chai<author:sep>Anish Prabhu<author:sep>Rohit Pandey<author:sep>Sean Fanello<author:sep>Zeng Huang<author:sep>Yun Fu;http://arxiv.org/pdf/2409.18057v1;cs.CV;"Appear in ECCV'24 CADL Workshop. Code:
  https://github.com/MingSun-Tse/LightAvatar-TensorFlow";nerf
2409.16666v1;http://arxiv.org/abs/2409.16666v1;2024-09-25;TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans;"We introduce a novel framework that learns a dynamic neural radiance field
(NeRF) for full-body talking humans from monocular videos. Prior work
represents only the body pose or the face. However, humans communicate with
their full body, combining body pose, hand gestures, as well as facial
expressions. In this work, we propose TalkinNeRF, a unified NeRF-based network
that represents the holistic 4D human motion. Given a monocular video of a
subject, we learn corresponding modules for the body, face, and hands, that are
combined together to generate the final result. To capture complex finger
articulation, we learn an additional deformation field for the hands. Our
multi-identity representation enables simultaneous training for multiple
subjects, as well as robust animation under completely unseen poses. It can
also generalize to novel identities, given only a short video as input. We
demonstrate state-of-the-art performance for animating full-body talking
humans, with fine-grained hand articulation and facial expressions.";Aggelina Chatziagapi<author:sep>Bindita Chaudhuri<author:sep>Amit Kumar<author:sep>Rakesh Ranjan<author:sep>Dimitris Samaras<author:sep>Nikolaos Sarafianos;http://arxiv.org/pdf/2409.16666v1;cs.CV;"Accepted by ECCVW 2024. Project page:
  https://aggelinacha.github.io/TalkinNeRF/";nerf
2409.16938v1;http://arxiv.org/abs/2409.16938v1;2024-09-25;Generative Object Insertion in Gaussian Splatting with a Multi-View  Diffusion Model;"Generating and inserting new objects into 3D content is a compelling approach
for achieving versatile scene recreation. Existing methods, which rely on SDS
optimization or single-view inpainting, often struggle to produce high-quality
results. To address this, we propose a novel method for object insertion in 3D
content represented by Gaussian Splatting. Our approach introduces a multi-view
diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable
video diffusion model to facilitate view-consistent object inpainting. Within
MVInpainter, we incorporate a ControlNet-based conditional injection module to
enable controlled and more predictable multi-view generation. After generating
the multi-view inpainted results, we further propose a mask-aware 3D
reconstruction technique to refine Gaussian Splatting reconstruction from these
sparse inpainted views. By leveraging these fabricate techniques, our approach
yields diverse results, ensures view-consistent and harmonious insertions, and
produces better object quality. Extensive experiments demonstrate that our
approach outperforms existing methods.";Hongliang Zhong<author:sep>Can Wang<author:sep>Jingbo Zhang<author:sep>Jing Liao;http://arxiv.org/pdf/2409.16938v1;cs.CV;Project Page: https://github.com/JiuTongBro/MultiView_Inpaint;gaussian splatting
2409.17345v1;http://arxiv.org/abs/2409.17345v1;2024-09-25;SeaSplat: Representing Underwater Scenes with 3D Gaussian Splatting and  a Physically Grounded Image Formation Model;"We introduce SeaSplat, a method to enable real-time rendering of underwater
scenes leveraging recent advances in 3D radiance fields. Underwater scenes are
challenging visual environments, as rendering through a medium such as water
introduces both range and color dependent effects on image capture. We
constrain 3D Gaussian Splatting (3DGS), a recent advance in radiance fields
enabling rapid training and real-time rendering of full 3D scenes, with a
physically grounded underwater image formation model. Applying SeaSplat to the
real-world scenes from SeaThru-NeRF dataset, a scene collected by an underwater
vehicle in the US Virgin Islands, and simulation-degraded real-world scenes,
not only do we see increased quantitative performance on rendering novel
viewpoints from the scene with the medium present, but are also able to recover
the underlying true color of the scene and restore renders to be without the
presence of the intervening medium. We show that the underwater image formation
helps learn scene structure, with better depth maps, as well as show that our
improvements maintain the significant computational improvements afforded by
leveraging a 3D Gaussian representation.";Daniel Yang<author:sep>John J. Leonard<author:sep>Yogesh Girdhar;http://arxiv.org/pdf/2409.17345v1;cs.CV;Project page here: https://seasplat.github.io;gaussian splatting<tag:sep>nerf
2409.16915v1;http://arxiv.org/abs/2409.16915v1;2024-09-25;Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized  Gaussian Splat;"Neural Radiance Fields and Gaussian Splatting have transformed the field of
computer vision by enabling photo-realistic representation of complex scenes.
Despite this success, they have seen only limited use in real-world robotics
tasks such as trajectory optimization. Two key factors have contributed to this
limited success. First, it is challenging to reason about collisions in
radiance models. Second, it is difficult to perform inference of radiance
models fast enough for real-time trajectory synthesis. This paper addresses
these challenges by proposing SPLANNING, a risk-aware trajectory optimizer that
operates in a Gaussian Splatting model. This paper first derives a method for
rigorously upper-bounding the probability of collision between a robot and a
radiance field. Second, this paper introduces a normalized reformulation of
Gaussian Splatting that enables the efficient computation of the collision
bound in a Gaussian Splat. Third, a method is presented to optimize
trajectories while avoiding collisions with a scene represented by a Gaussian
Splat. Experiments demonstrate that SPLANNING outperforms state-of-the-art
methods in generating collision-free trajectories in highly cluttered
environments. The proposed system is also tested on a real-world robot
manipulator. A project page is available at
https://roahmlab.github.io/splanning.";Jonathan Michaux<author:sep>Seth Isaacson<author:sep>Challen Enninful Adu<author:sep>Adam Li<author:sep>Rahul Kashyap Swayampakula<author:sep>Parker Ewen<author:sep>Sean Rice<author:sep>Katherine A. Skinner<author:sep>Ram Vasudevan;http://arxiv.org/pdf/2409.16915v1;cs.RO;"First two authors contributed equally. Project Page:
  https://roahmlab.github.io/splanning";gaussian splatting
2409.17280v1;http://arxiv.org/abs/2409.17280v1;2024-09-25;Disco4D: Disentangled 4D Human Generation and Animation from a Single  Image;"We present \textbf{Disco4D}, a novel Gaussian Splatting framework for 4D
human generation and animation from a single image. Different from existing
methods, Disco4D distinctively disentangles clothings (with Gaussian models)
from the human body (with SMPL-X model), significantly enhancing the generation
details and flexibility. It has the following technical innovations.
\textbf{1)} Disco4D learns to efficiently fit the clothing Gaussians over the
SMPL-X Gaussians. \textbf{2)} It adopts diffusion models to enhance the 3D
generation process, \textit{e.g.}, modeling occluded parts not visible in the
input image. \textbf{3)} It learns an identity encoding for each clothing
Gaussian to facilitate the separation and extraction of clothing assets.
Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics.
Extensive experiments demonstrate the superiority of Disco4D on 4D human
generation and animation tasks. Our visualizations can be found in
\url{https://disco-4d.github.io/}.";Hui En Pang<author:sep>Shuai Liu<author:sep>Zhongang Cai<author:sep>Lei Yang<author:sep>Tianwei Zhang<author:sep>Ziwei Liu;http://arxiv.org/pdf/2409.17280v1;cs.CV;;gaussian splatting
2409.16944v1;http://arxiv.org/abs/2409.16944v1;2024-09-25;Go-SLAM: Grounded Object Segmentation and Localization with Gaussian  Splatting SLAM;"We introduce Go-SLAM, a novel framework that utilizes 3D Gaussian Splatting
SLAM to reconstruct dynamic environments while embedding object-level
information within the scene representations. This framework employs advanced
object segmentation techniques, assigning a unique identifier to each Gaussian
splat that corresponds to the object it represents. Consequently, our system
facilitates open-vocabulary querying, allowing users to locate objects using
natural language descriptions. Furthermore, the framework features an optimal
path generation module that calculates efficient navigation paths for robots
toward queried objects, considering obstacles and environmental uncertainties.
Comprehensive evaluations in various scene settings demonstrate the
effectiveness of our approach in delivering high-fidelity scene
reconstructions, precise object segmentation, flexible object querying, and
efficient robot path planning. This work represents an additional step forward
in bridging the gap between 3D scene reconstruction, semantic object
understanding, and real-time environment interactions.";Phu Pham<author:sep>Dipam Patel<author:sep>Damon Conover<author:sep>Aniket Bera;http://arxiv.org/pdf/2409.16944v1;cs.RO;;gaussian splatting
2409.15715v1;http://arxiv.org/abs/2409.15715v1;2024-09-24;Disentangled Generation and Aggregation for Robust Radiance Fields;"The utilization of the triplane-based radiance fields has gained attention in
recent years due to its ability to effectively disentangle 3D scenes with a
high-quality representation and low computation cost. A key requirement of this
method is the precise input of camera poses. However, due to the local update
property of the triplane, a similar joint estimation as previous joint
pose-NeRF optimization works easily results in local minima. To this end, we
propose the Disentangled Triplane Generation module to introduce global feature
context and smoothness into triplane learning, which mitigates errors caused by
local updating. Then, we propose the Disentangled Plane Aggregation to mitigate
the entanglement caused by the common triplane feature aggregation during
camera pose updating. In addition, we introduce a two-stage warm-start training
strategy to reduce the implicit constraints caused by the triplane generator.
Quantitative and qualitative results demonstrate that our proposed method
achieves state-of-the-art performance in novel view synthesis with noisy or
unknown camera poses, as well as efficient convergence of optimization. Project
page: https://gaohchen.github.io/DiGARR/.";Shihe Shen<author:sep>Huachen Gao<author:sep>Wangze Xu<author:sep>Rui Peng<author:sep>Luyang Tang<author:sep>Kaiqiang Xiong<author:sep>Jianbo Jiao<author:sep>Ronggang Wang;http://arxiv.org/pdf/2409.15715v1;cs.CV;27 pages, 11 figures, Accepted by ECCV'2024;nerf
2409.15959v1;http://arxiv.org/abs/2409.15959v1;2024-09-24;Semantics-Controlled Gaussian Splatting for Outdoor Scene Reconstruction  and Rendering in Virtual Reality;"Advancements in 3D rendering like Gaussian Splatting (GS) allow novel view
synthesis and real-time rendering in virtual reality (VR). However, GS-created
3D environments are often difficult to edit. For scene enhancement or to
incorporate 3D assets, segmenting Gaussians by class is essential. Existing
segmentation approaches are typically limited to certain types of scenes, e.g.,
''circular'' scenes, to determine clear object boundaries. However, this method
is ineffective when removing large objects in non-''circling'' scenes such as
large outdoor scenes. We propose Semantics-Controlled GS (SCGS), a
segmentation-driven GS approach, enabling the separation of large scene parts
in uncontrolled, natural environments. SCGS allows scene editing and the
extraction of scene parts for VR. Additionally, we introduce a challenging
outdoor dataset, overcoming the ''circling'' setup. We outperform the
state-of-the-art in visual quality on our dataset and in segmentation quality
on the 3D-OVS dataset. We conducted an exploratory user study, comparing a
360-video, plain GS, and SCGS in VR with a fixed viewpoint. In our subsequent
main study, users were allowed to move freely, evaluating plain GS and SCGS.
Our main study results show that participants clearly prefer SCGS over plain
GS. We overall present an innovative approach that surpasses the
state-of-the-art both technically and in user experience.";Hannah Schieber<author:sep>Jacob Young<author:sep>Tobias Langlotz<author:sep>Stefanie Zollmann<author:sep>Daniel Roth;http://arxiv.org/pdf/2409.15959v1;cs.CV;;gaussian splatting
2409.15689v1;http://arxiv.org/abs/2409.15689v1;2024-09-24;Plenoptic PNG: Real-Time Neural Radiance Fields in 150 KB;"The goal of this paper is to encode a 3D scene into an extremely compact
representation from 2D images and to enable its transmittance, decoding and
rendering in real-time across various platforms. Despite the progress in NeRFs
and Gaussian Splats, their large model size and specialized renderers make it
challenging to distribute free-viewpoint 3D content as easily as images. To
address this, we have designed a novel 3D representation that encodes the
plenoptic function into sinusoidal function indexed dense volumes. This
approach facilitates feature sharing across different locations, improving
compactness over traditional spatial voxels. The memory footprint of the dense
3D feature grid can be further reduced using spatial decomposition techniques.
This design combines the strengths of spatial hashing functions and voxel
decomposition, resulting in a model size as small as 150 KB for each 3D scene.
Moreover, PPNG features a lightweight rendering pipeline with only 300 lines of
code that decodes its representation into standard GL textures and fragment
shaders. This enables real-time rendering using the traditional GL pipeline,
ensuring universal compatibility and efficiency across various platforms
without additional dependencies.";Jae Yong Lee<author:sep>Yuqun Wu<author:sep>Chuhang Zou<author:sep>Derek Hoiem<author:sep>Shenlong Wang;http://arxiv.org/pdf/2409.15689v1;cs.CV;;nerf
2409.16470v1;http://arxiv.org/abs/2409.16470v1;2024-09-24;Frequency-based View Selection in Gaussian Splatting Reconstruction;"Three-dimensional reconstruction is a fundamental problem in robotics
perception. We examine the problem of active view selection to perform 3D
Gaussian Splatting reconstructions with as few input images as possible.
Although 3D Gaussian Splatting has made significant progress in image rendering
and 3D reconstruction, the quality of the reconstruction is strongly impacted
by the selection of 2D images and the estimation of camera poses through
Structure-from-Motion (SfM) algorithms. Current methods to select views that
rely on uncertainties from occlusions, depth ambiguities, or neural network
predictions directly are insufficient to handle the issue and struggle to
generalize to new scenes. By ranking the potential views in the frequency
domain, we are able to effectively estimate the potential information gain of
new viewpoints without ground truth data. By overcoming current constraints on
model architecture and efficacy, our method achieves state-of-the-art results
in view selection, demonstrating its potential for efficient image-based 3D
reconstruction.";Monica M. Q. Li<author:sep>Pierre-Yves Lajoie<author:sep>Giovanni Beltrame;http://arxiv.org/pdf/2409.16470v1;cs.CV;8 pages, 4 figures;gaussian splatting
2409.16502v1;http://arxiv.org/abs/2409.16502v1;2024-09-24;GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for  Improved Visual Localization;"Although various visual localization approaches exist, such as scene
coordinate and pose regression, these methods often struggle with high memory
consumption or extensive optimization requirements. To address these
challenges, we utilize recent advancements in novel view synthesis,
particularly 3D Gaussian Splatting (3DGS), to enhance localization. 3DGS allows
for the compact encoding of both 3D geometry and scene appearance with its
spatial features. Our method leverages the dense description maps produced by
XFeat's lightweight keypoint detection and description model. We propose
distilling these dense keypoint descriptors into 3DGS to improve the model's
spatial understanding, leading to more accurate camera pose predictions through
2D-3D correspondences. After estimating an initial pose, we refine it using a
photometric warping loss. Benchmarking on popular indoor and outdoor datasets
shows that our approach surpasses state-of-the-art Neural Render Pose (NRP)
methods, including NeRFMatch and PNeRFLoc.";Gennady Sidorov<author:sep>Malik Mohrat<author:sep>Ksenia Lebedeva<author:sep>Ruslan Rakhimov<author:sep>Sergey Kolyubin;http://arxiv.org/pdf/2409.16502v1;cs.CV;Project website at https://gsplatloc.github.io/;gaussian splatting<tag:sep>nerf
2409.14778v1;http://arxiv.org/abs/2409.14778v1;2024-09-23;Human Hair Reconstruction with Strand-Aligned 3D Gaussians;"We introduce a new hair modeling method that uses a dual representation of
classical hair strands and 3D Gaussians to produce accurate and realistic
strand-based reconstructions from multi-view data. In contrast to recent
approaches that leverage unstructured Gaussians to model human avatars, our
method reconstructs the hair using 3D polylines, or strands. This fundamental
difference allows the use of the resulting hairstyles out-of-the-box in modern
computer graphics engines for editing, rendering, and simulation. Our 3D
lifting method relies on unstructured Gaussians to generate multi-view ground
truth data to supervise the fitting of hair strands. The hairstyle itself is
represented in the form of the so-called strand-aligned 3D Gaussians. This
representation allows us to combine strand-based hair priors, which are
essential for realistic modeling of the inner structure of hairstyles, with the
differentiable rendering capabilities of 3D Gaussian Splatting. Our method,
named Gaussian Haircut, is evaluated on synthetic and real scenes and
demonstrates state-of-the-art performance in the task of strand-based hair
reconstruction.";Egor Zakharov<author:sep>Vanessa Sklyarova<author:sep>Michael Black<author:sep>Giljoo Nam<author:sep>Justus Thies<author:sep>Otmar Hilliges;http://arxiv.org/pdf/2409.14778v1;cs.CV;;gaussian splatting
2409.16147v2;http://arxiv.org/abs/2409.16147v2;2024-09-23;Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with  Enhanced Generalization and Personalization Abilities;"Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant
potential for modeling 3D head avatars, providing greater flexibility than
mesh-based methods and more efficient rendering compared to NeRF-based
approaches. Despite these advancements, the creation of controllable 3DGS-based
head avatars remains time-intensive, often requiring tens of minutes to hours.
To expedite this process, we here introduce the ``Gaussian D\'ej\`a-vu""
framework, which first obtains a generalized model of the head avatar and then
personalizes the result. The generalized model is trained on large 2D
(synthetic and real) image datasets. This model provides a well-initialized 3D
Gaussian head that is further refined using a monocular video to achieve the
personalized head avatar. For personalizing, we propose learnable
expression-aware rectification blendmaps to correct the initial 3D Gaussians,
ensuring rapid convergence without the reliance on neural networks. Experiments
demonstrate that the proposed method meets its objectives. It outperforms
state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as
well as reduces training time consumption to at least a quarter of the existing
methods, producing the avatar in minutes.";Peizhi Yan<author:sep>Rabab Ward<author:sep>Qiang Tang<author:sep>Shan Du;http://arxiv.org/pdf/2409.16147v2;cs.CV;11 pages, Accepted by WACV 2025 in Round 1;gaussian splatting<tag:sep>nerf
2409.15487v1;http://arxiv.org/abs/2409.15487v1;2024-09-23;AgriNeRF: Neural Radiance Fields for Agriculture in Challenging Lighting  Conditions;"Neural Radiance Fields (NeRFs) have shown significant promise in 3D scene
reconstruction and novel view synthesis. In agricultural settings, NeRFs can
serve as digital twins, providing critical information about fruit detection
for yield estimation and other important metrics for farmers. However,
traditional NeRFs are not robust to challenging lighting conditions, such as
low-light, extreme bright light and varying lighting. To address these issues,
this work leverages three different sensors: an RGB camera, an event camera and
a thermal camera. Our RGB scene reconstruction shows an improvement in PSNR and
SSIM by +2.06 dB and +8.3% respectively. Our cross-spectral scene
reconstruction enhances downstream fruit detection by +43.0% in mAP50 and
+61.1% increase in mAP50-95. The integration of additional sensors leads to a
more robust and informative NeRF. We demonstrate that our multi-modal system
yields high quality photo-realistic reconstructions under various tree canopy
covers and at different times of the day. This work results in the development
of a resilient NeRF, capable of performing well in visibly degraded scenarios,
as well as a learnt cross-spectral representation, that is used for automated
fruit detection.";Samarth Chopra<author:sep>Fernando Cladera<author:sep>Varun Murali<author:sep>Vijay Kumar;http://arxiv.org/pdf/2409.15487v1;cs.RO;7 pages, 5 figures;nerf
2409.14316v1;http://arxiv.org/abs/2409.14316v1;2024-09-22;MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse  Input Views;"Recently, the Neural Radiance Field (NeRF) advancement has facilitated
few-shot Novel View Synthesis (NVS), which is a significant challenge in 3D
vision applications. Despite numerous attempts to reduce the dense input
requirement in NeRF, it still suffers from time-consumed training and rendering
processes. More recently, 3D Gaussian Splatting (3DGS) achieves real-time
high-quality rendering with an explicit point-based representation. However,
similar to NeRF, it tends to overfit the train views for lack of constraints.
In this paper, we propose \textbf{MVPGS}, a few-shot NVS method that excavates
the multi-view priors based on 3D Gaussian Splatting. We leverage the recent
learning-based Multi-view Stereo (MVS) to enhance the quality of geometric
initialization for 3DGS. To mitigate overfitting, we propose a forward-warping
method for additional appearance constraints conforming to scenes based on the
computed geometry. Furthermore, we introduce a view-consistent geometry
constraint for Gaussian parameters to facilitate proper optimization
convergence and utilize a monocular depth regularization as compensation.
Experiments show that the proposed method achieves state-of-the-art performance
with real-time rendering speed. Project page:
https://zezeaaa.github.io/projects/MVPGS/";Wangze Xu<author:sep>Huachen Gao<author:sep>Shihe Shen<author:sep>Rui Peng<author:sep>Jianbo Jiao<author:sep>Ronggang Wang;http://arxiv.org/pdf/2409.14316v1;cs.CV;"Accepted by ECCV 2024, Project page:
  https://zezeaaa.github.io/projects/MVPGS/";gaussian splatting<tag:sep>nerf
2409.14019v1;http://arxiv.org/abs/2409.14019v1;2024-09-21;MOSE: Monocular Semantic Reconstruction Using NeRF-Lifted Noisy Priors;"Accurately reconstructing dense and semantically annotated 3D meshes from
monocular images remains a challenging task due to the lack of geometry
guidance and imperfect view-dependent 2D priors. Though we have witnessed
recent advancements in implicit neural scene representations enabling precise
2D rendering simply from multi-view images, there have been few works
addressing 3D scene understanding with monocular priors alone. In this paper,
we propose MOSE, a neural field semantic reconstruction approach to lift
inferred image-level noisy priors to 3D, producing accurate semantics and
geometry in both 3D and 2D space. The key motivation for our method is to
leverage generic class-agnostic segment masks as guidance to promote local
consistency of rendered semantics during training. With the help of semantics,
we further apply a smoothness regularization to texture-less regions for better
geometric quality, thus achieving mutual benefits of geometry and semantics.
Experiments on the ScanNet dataset show that our MOSE outperforms relevant
baselines across all metrics on tasks of 3D semantic segmentation, 2D semantic
segmentation and 3D surface reconstruction.";Zhenhua Du<author:sep>Binbin Xu<author:sep>Haoyu Zhang<author:sep>Kai Huo<author:sep>Shuaifeng Zhi;http://arxiv.org/pdf/2409.14019v1;cs.CV;8 pages, 10 figures;nerf
2409.14067v1;http://arxiv.org/abs/2409.14067v1;2024-09-21;SplatLoc: 3D Gaussian Splatting-based Visual Localization for Augmented  Reality;"Visual localization plays an important role in the applications of Augmented
Reality (AR), which enable AR devices to obtain their 6-DoF pose in the
pre-build map in order to render virtual content in real scenes. However, most
existing approaches can not perform novel view rendering and require large
storage capacities for maps. To overcome these limitations, we propose an
efficient visual localization method capable of high-quality rendering with
fewer parameters. Specifically, our approach leverages 3D Gaussian primitives
as the scene representation. To ensure precise 2D-3D correspondences for pose
estimation, we develop an unbiased 3D scene-specific descriptor decoder for
Gaussian primitives, distilled from a constructed feature volume. Additionally,
we introduce a salient 3D landmark selection algorithm that selects a suitable
primitive subset based on the saliency score for localization. We further
regularize key Gaussian primitives to prevent anisotropic effects, which also
improves localization performance. Extensive experiments on two widely used
datasets demonstrate that our method achieves superior or comparable rendering
and localization performance to state-of-the-art implicit-based visual
localization approaches. Project page:
\href{https://zju3dv.github.io/splatloc}{https://zju3dv.github.io/splatloc}.";Hongjia Zhai<author:sep>Xiyu Zhang<author:sep>Boming Zhao<author:sep>Hai Li<author:sep>Yijia He<author:sep>Zhaopeng Cui<author:sep>Hujun Bao<author:sep>Guofeng Zhang;http://arxiv.org/pdf/2409.14067v1;cs.CV;;gaussian splatting
2409.13222v1;http://arxiv.org/abs/2409.13222v1;2024-09-20;3D-GSW: 3D Gaussian Splatting Watermark for Protecting Copyrights in  Radiance Fields;"Recently, 3D Gaussian splatting has been getting a lot of attention as an
innovative method for representing 3D space due to rapid rendering and image
quality. However, copyright protection for the 3D Gaussian splatting has not
yet been introduced. In this paper, we present a novel watermarking method for
3D Gaussian splatting. The proposed method embeds a binary message into 3D
Gaussians by fine-tuning the pre-trained 3D Gaussian splatting model. To
achieve this, we present Frequency-Guided Densification (FGD) that utilizes
Discrete Fourier Transform to find patches with high-frequencies and split 3D
Gaussians based on 3D Gaussian Contribution Vector. It is each 3D Gaussian
contribution to rendered pixel colors, improving both rendering quality and bit
accuracy. Furthermore, we modify an adaptive gradient mask to enhance rendering
quality. Our experiments show that our method can embed a watermark in 3D
Gaussians imperceptibly with increased capacity and robustness against attacks.
Our method reduces optimization cost and achieves state-of-the-art performance
compared to other methods.";Youngdong Jang<author:sep>Hyunje Park<author:sep>Feng Yang<author:sep>Heeju Ko<author:sep>Euijin Choo<author:sep>Sangpil Kim;http://arxiv.org/pdf/2409.13222v1;cs.CV;;gaussian splatting
2409.13392v1;http://arxiv.org/abs/2409.13392v1;2024-09-20;Elite-EvGS: Learning Event-based 3D Gaussian Splatting by Distilling  Event-to-Video Priors;"Event cameras are bio-inspired sensors that output asynchronous and sparse
event streams, instead of fixed frames. Benefiting from their distinct
advantages, such as high dynamic range and high temporal resolution, event
cameras have been applied to address 3D reconstruction, important for robotic
mapping. Recently, neural rendering techniques, such as 3D Gaussian splatting
(3DGS), have been shown successful in 3D reconstruction. However, it still
remains under-explored how to develop an effective event-based 3DGS pipeline.
In particular, as 3DGS typically depends on high-quality initialization and
dense multiview constraints, a potential problem appears for the 3DGS
optimization with events given its inherent sparse property. To this end, we
propose a novel event-based 3DGS framework, named Elite-EvGS. Our key idea is
to distill the prior knowledge from the off-the-shelf event-to-video (E2V)
models to effectively reconstruct 3D scenes from events in a coarse-to-fine
optimization manner. Specifically, to address the complexity of 3DGS
initialization from events, we introduce a novel warm-up initialization
strategy that optimizes a coarse 3DGS from the frames generated by E2V models
and then incorporates events to refine the details. Then, we propose a
progressive event supervision strategy that employs the window-slicing
operation to progressively reduce the number of events used for supervision.
This subtly relives the temporal randomness of the event frames, benefiting the
optimization of local textural and global structural details. Experiments on
the benchmark datasets demonstrate that Elite-EvGS can reconstruct 3D scenes
with better textural and structural details. Meanwhile, our method yields
plausible performance on the captured real-world data, including diverse
challenging conditions, such as fast motion and low light scenes.";Zixin Zhang<author:sep>Kanghao Chen<author:sep>Lin Wang;http://arxiv.org/pdf/2409.13392v1;cs.CV;;gaussian splatting
2409.12774v3;http://arxiv.org/abs/2409.12774v3;2024-09-19;GaRField++: Reinforced Gaussian Radiance Fields for Large-Scale 3D Scene  Reconstruction;"This paper proposes a novel framework for large-scale scene reconstruction
based on 3D Gaussian splatting (3DGS) and aims to address the scalability and
accuracy challenges faced by existing methods. For tackling the scalability
issue, we split the large scene into multiple cells, and the candidate
point-cloud and camera views of each cell are correlated through a
visibility-based camera selection and a progressive point-cloud extension. To
reinforce the rendering quality, three highlighted improvements are made in
comparison with vanilla 3DGS, which are a strategy of the ray-Gaussian
intersection and the novel Gaussians density control for learning efficiency,
an appearance decoupling module based on ConvKAN network to solve uneven
lighting conditions in large-scale scenes, and a refined final loss with the
color loss, the depth distortion loss, and the normal consistency loss.
Finally, the seamless stitching procedure is executed to merge the individual
Gaussian radiance field for novel view synthesis across different cells.
Evaluation of Mill19, Urban3D, and MatrixCity datasets shows that our method
consistently generates more high-fidelity rendering results than
state-of-the-art methods of large-scale scene reconstruction. We further
validate the generalizability of the proposed approach by rendering on
self-collected video clips recorded by a commercial drone.";Hanyue Zhang<author:sep>Zhiliu Yang<author:sep>Xinhe Zuo<author:sep>Yuxin Tong<author:sep>Ying Long<author:sep>Chen Liu;http://arxiv.org/pdf/2409.12774v3;cs.CV;;gaussian splatting
2409.12771v1;http://arxiv.org/abs/2409.12771v1;2024-09-19;Spectral-GS: Taming 3D Gaussian Splatting with Spectral Entropy;"Recently, 3D Gaussian Splatting (3D-GS) has achieved impressive results in
novel view synthesis, demonstrating high fidelity and efficiency. However, it
easily exhibits needle-like artifacts, especially when increasing the sampling
rate. Mip-Splatting tries to remove these artifacts with a 3D smoothing filter
for frequency constraints and a 2D Mip filter for approximated supersampling.
Unfortunately, it tends to produce over-blurred results, and sometimes
needle-like Gaussians still persist. Our spectral analysis of the covariance
matrix during optimization and densification reveals that current 3D-GS lacks
shape awareness, relying instead on spectral radius and view positional
gradients to determine splitting. As a result, needle-like Gaussians with small
positional gradients and low spectral entropy fail to split and overfit
high-frequency details. Furthermore, both the filters used in 3D-GS and
Mip-Splatting reduce the spectral entropy and increase the condition number
during zooming in to synthesize novel view, causing view inconsistencies and
more pronounced artifacts. Our Spectral-GS, based on spectral analysis,
introduces 3D shape-aware splitting and 2D view-consistent filtering
strategies, effectively addressing these issues, enhancing 3D-GS's capability
to represent high-frequency details without noticeable artifacts, and achieving
high-quality photorealistic rendering.";Letian Huang<author:sep>Jie Guo<author:sep>Jialin Dan<author:sep>Ruoyu Fu<author:sep>Shujie Wang<author:sep>Yuanqi Li<author:sep>Yanwen Guo;http://arxiv.org/pdf/2409.12771v1;cs.CV;;gaussian splatting
2409.12518v1;http://arxiv.org/abs/2409.12518v1;2024-09-19;Hi-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical  Gaussian Splatting;"We propose Hi-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a
novel hierarchical categorical representation, which enables accurate global 3D
semantic mapping, scaling-up capability, and explicit semantic label prediction
in the 3D world. The parameter usage in semantic SLAM systems increases
significantly with the growing complexity of the environment, making it
particularly challenging and costly for scene understanding. To address this
problem, we introduce a novel hierarchical representation that encodes semantic
information in a compact form into 3D Gaussian Splatting, leveraging the
capabilities of large language models (LLMs). We further introduce a novel
semantic loss designed to optimize hierarchical semantic information through
both inter-level and cross-level optimization. Furthermore, we enhance the
whole SLAM system, resulting in improved tracking and mapping performance. Our
Hi-SLAM outperforms existing dense SLAM methods in both mapping and tracking
accuracy, while achieving a 2x operation speed-up. Additionally, it exhibits
competitive performance in rendering semantic segmentation in small synthetic
scenes, with significantly reduced storage and training time requirements.
Rendering FPS impressively reaches 2,000 with semantic information and 3,000
without it. Most notably, it showcases the capability of handling the complex
real-world scene with more than 500 semantic classes, highlighting its valuable
scaling-up capability.";Boying Li<author:sep>Zhixi Cai<author:sep>Yuan-Fang Li<author:sep>Ian Reid<author:sep>Hamid Rezatofighi;http://arxiv.org/pdf/2409.12518v1;cs.RO;6 pages, 4 figures;gaussian splatting
2409.12892v1;http://arxiv.org/abs/2409.12892v1;2024-09-19;3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt;"We present 3DGS-LM, a new method that accelerates the reconstruction of 3D
Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored
Levenberg-Marquardt (LM). Existing methods reduce the optimization time by
decreasing the number of Gaussians or by improving the implementation of the
differentiable rasterizer. However, they still rely on the ADAM optimizer to
fit Gaussian parameters of a scene in thousands of iterations, which can take
up to an hour. To this end, we change the optimizer to LM that runs in
conjunction with the 3DGS differentiable rasterizer. For efficient GPU
parallization, we propose a caching data structure for intermediate gradients
that allows us to efficiently calculate Jacobian-vector products in custom CUDA
kernels. In every LM iteration, we calculate update directions from multiple
image subsets using these kernels and combine them in a weighted mean. Overall,
our method is 30% faster than the original 3DGS while obtaining the same
reconstruction quality. Our optimization is also agnostic to other methods that
acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.";Lukas Höllein<author:sep>Aljaž Božič<author:sep>Michael Zollhöfer<author:sep>Matthias Nießner;http://arxiv.org/pdf/2409.12892v1;cs.CV;"project page: https://lukashoel.github.io/3DGS-LM, video:
  https://www.youtube.com/watch?v=tDiGuGMssg8, code:
  https://github.com/lukasHoel/3DGS-LM";gaussian splatting
2409.13055v1;http://arxiv.org/abs/2409.13055v1;2024-09-19;MGSO: Monocular Real-time Photometric SLAM with Efficient 3D Gaussian  Splatting;"Real-time SLAM with dense 3D mapping is computationally challenging,
especially on resource-limited devices. The recent development of 3D Gaussian
Splatting (3DGS) offers a promising approach for real-time dense 3D
reconstruction. However, existing 3DGS-based SLAM systems struggle to balance
hardware simplicity, speed, and map quality. Most systems excel in one or two
of the aforementioned aspects but rarely achieve all. A key issue is the
difficulty of initializing 3D Gaussians while concurrently conducting SLAM. To
address these challenges, we present Monocular GSO (MGSO), a novel real-time
SLAM system that integrates photometric SLAM with 3DGS. Photometric SLAM
provides dense structured point clouds for 3DGS initialization, accelerating
optimization and producing more efficient maps with fewer Gaussians. As a
result, experiments show that our system generates reconstructions with a
balance of quality, memory efficiency, and speed that outperforms the
state-of-the-art. Furthermore, our system achieves all results using RGB
inputs. We evaluate the Replica, TUM-RGBD, and EuRoC datasets against current
live dense reconstruction systems. Not only do we surpass contemporary systems,
but experiments also show that we maintain our performance on laptop hardware,
making it a practical solution for robotics, A/R, and other real-time
applications.";Yan Song Hu<author:sep>Nicolas Abboud<author:sep>Muhammad Qasim Ali<author:sep>Adam Srebrnjak Yang<author:sep>Imad Elhajj<author:sep>Daniel Asmar<author:sep>Yuhao Chen<author:sep>John S. Zelek;http://arxiv.org/pdf/2409.13055v1;cs.RO;"Paper Contribution to the ICRA 2025 Conference. Currently being
  reviewed";
2409.12899v1;http://arxiv.org/abs/2409.12899v1;2024-09-19;LI-GS: Gaussian Splatting with LiDAR Incorporated for Accurate  Large-Scale Reconstruction;"Large-scale 3D reconstruction is critical in the field of robotics, and the
potential of 3D Gaussian Splatting (3DGS) for achieving accurate object-level
reconstruction has been demonstrated. However, ensuring geometric accuracy in
outdoor and unbounded scenes remains a significant challenge. This study
introduces LI-GS, a reconstruction system that incorporates LiDAR and Gaussian
Splatting to enhance geometric accuracy in large-scale scenes. 2D Gaussain
surfels are employed as the map representation to enhance surface alignment.
Additionally, a novel modeling method is proposed to convert LiDAR point clouds
to plane-constrained multimodal Gaussian Mixture Models (GMMs). The GMMs are
utilized during both initialization and optimization stages to ensure
sufficient and continuous supervision over the entire scene while mitigating
the risk of over-fitting. Furthermore, GMMs are employed in mesh extraction to
eliminate artifacts and improve the overall geometric quality. Experiments
demonstrate that our method outperforms state-of-the-art methods in large-scale
3D reconstruction, achieving higher accuracy compared to both LiDAR-based
methods and Gaussian-based methods with improvements of 52.6% and 68.7%,
respectively.";Changjian Jiang<author:sep>Ruilan Gao<author:sep>Kele Shao<author:sep>Yue Wang<author:sep>Rong Xiong<author:sep>Yu Zhang;http://arxiv.org/pdf/2409.12899v1;cs.RO;;gaussian splatting
2409.12886v1;http://arxiv.org/abs/2409.12886v1;2024-09-19;EdgeGaussians -- 3D Edge Mapping via Gaussian Splatting;"With their meaningful geometry and their omnipresence in the 3D world, edges
are extremely useful primitives in computer vision. 3D edges comprise of lines
and curves, and methods to reconstruct them use either multi-view images or
point clouds as input. State-of-the-art image-based methods first learn a 3D
edge point cloud then fit 3D edges to it. The edge point cloud is obtained by
learning a 3D neural implicit edge field from which the 3D edge points are
sampled on a specific level set (0 or 1). However, such methods present two
important drawbacks: i) it is not realistic to sample points on exact level
sets due to float imprecision and training inaccuracies. Instead, they are
sampled within a range of levels so the points do not lie accurately on the 3D
edges and require further processing. ii) Such implicit representations are
computationally expensive and require long training times. In this paper, we
address these two limitations and propose a 3D edge mapping that is simpler,
more efficient, and preserves accuracy. Our method learns explicitly the 3D
edge points and their edge direction hence bypassing the need for point
sampling. It casts a 3D edge point as the center of a 3D Gaussian and the edge
direction as the principal axis of the Gaussian. Such a representation has the
advantage of being not only geometrically meaningful but also compatible with
the efficient training optimization defined in Gaussian Splatting. Results show
that the proposed method produces edges as accurate and complete as the
state-of-the-art while being an order of magnitude faster. Code is released at
https://github.com/kunalchelani/EdgeGaussians.";Kunal Chelani<author:sep>Assia Benbihi<author:sep>Torsten Sattler<author:sep>Fredrik Kahl;http://arxiv.org/pdf/2409.12886v1;cs.CV;;gaussian splatting
2409.12617v1;http://arxiv.org/abs/2409.12617v1;2024-09-19;CrossRT: A cross platform programming technology for  hardware-accelerated ray tracing in CG and CV applications;"We propose a programming technology that bridges cross-platform compatibility
and hardware acceleration in ray tracing applications. Our methodology enables
developers to define algorithms while our translator manages implementation
specifics for different hardware or APIs. Features include: generating
hardware-accelerated code from hardware-agnostic, object-oriented C++ algorithm
descriptions; enabling users to define software fallbacks for
non-hardware-accelerated CPUs and GPUs; producing GPU programming API-based
algorithm implementations resembling manually ported C++ versions. The
generated code is editable and readable, allowing for additional hardware
acceleration. Our translator supports single megakernel and multiple kernel
path tracing implementations without altering the programming model or input
source code. Wavefront mode is crucial for NeRF and SDF, ensuring efficient
evaluation with multiple kernels. Validation on tasks such as BVH tree
build/traversal, ray-surface intersection for SDF, ray-volume intersection for
3D Gaussian Splatting, and complex Path Tracing models showed comparable
performance levels to expert-written implementations for GPUs. Our technology
outperformed existing Path Tracing implementations.";Vladimir Frolov<author:sep>Vadim Sanzharov<author:sep>Garifullin Albert<author:sep>Maxim Raenchuk<author:sep>Alexei Voloboy;http://arxiv.org/pdf/2409.12617v1;cs.GR;;gaussian splatting<tag:sep>nerf
2409.12753v1;http://arxiv.org/abs/2409.12753v1;2024-09-19;DrivingForward: Feed-forward 3D Gaussian Splatting for Driving Scene  Reconstruction from Flexible Surround-view Input;"We propose DrivingForward, a feed-forward Gaussian Splatting model that
reconstructs driving scenes from flexible surround-view input. Driving scene
images from vehicle-mounted cameras are typically sparse, with limited overlap,
and the movement of the vehicle further complicates the acquisition of camera
extrinsics. To tackle these challenges and achieve real-time reconstruction, we
jointly train a pose network, a depth network, and a Gaussian network to
predict the Gaussian primitives that represent the driving scenes. The pose
network and depth network determine the position of the Gaussian primitives in
a self-supervised manner, without using depth ground truth and camera
extrinsics during training. The Gaussian network independently predicts
primitive parameters from each input image, including covariance, opacity, and
spherical harmonics coefficients. At the inference stage, our model can achieve
feed-forward reconstruction from flexible multi-frame surround-view input.
Experiments on the nuScenes dataset show that our model outperforms existing
state-of-the-art feed-forward and scene-optimized reconstruction methods in
terms of reconstruction.";Qijian Tian<author:sep>Xin Tan<author:sep>Yuan Xie<author:sep>Lizhuang Ma;http://arxiv.org/pdf/2409.12753v1;cs.CV;Project page: https://fangzhou2000.github.io/projects/drivingforward/;gaussian splatting
2409.12954v1;http://arxiv.org/abs/2409.12954v1;2024-09-19;GStex: Per-Primitive Texturing of 2D Gaussian Splatting for Decoupled  Appearance and Geometry Modeling;"Gaussian splatting has demonstrated excellent performance for view synthesis
and scene reconstruction. The representation achieves photorealistic quality by
optimizing the position, scale, color, and opacity of thousands to millions of
2D or 3D Gaussian primitives within a scene. However, since each Gaussian
primitive encodes both appearance and geometry, these attributes are strongly
coupled--thus, high-fidelity appearance modeling requires a large number of
Gaussian primitives, even when the scene geometry is simple (e.g., for a
textured planar surface). We propose to texture each 2D Gaussian primitive so
that even a single Gaussian can be used to capture appearance details. By
employing per-primitive texturing, our appearance representation is agnostic to
the topology and complexity of the scene's geometry. We show that our approach,
GStex, yields improved visual quality over prior work in texturing Gaussian
splats. Furthermore, we demonstrate that our decoupling enables improved novel
view synthesis performance compared to 2D Gaussian splatting when reducing the
number of Gaussian primitives, and that GStex can be used for scene appearance
editing and re-texturing.";Victor Rong<author:sep>Jingxiang Chen<author:sep>Sherwin Bahmani<author:sep>Kiriakos N. Kutulakos<author:sep>David B. Lindell;http://arxiv.org/pdf/2409.12954v1;cs.CV;Project page: https://lessvrong.com/cs/gstex;gaussian splatting
2409.12193v1;http://arxiv.org/abs/2409.12193v1;2024-09-18;Vista3D: Unravel the 3D Darkside of a Single Image;"We embark on the age-old quest: unveiling the hidden dimensions of objects
from mere glimpses of their visible parts. To address this, we present Vista3D,
a framework that realizes swift and consistent 3D generation within a mere 5
minutes. At the heart of Vista3D lies a two-phase approach: the coarse phase
and the fine phase. In the coarse phase, we rapidly generate initial geometry
with Gaussian Splatting from a single image. In the fine phase, we extract a
Signed Distance Function (SDF) directly from learned Gaussian Splatting,
optimizing it with a differentiable isosurface representation. Furthermore, it
elevates the quality of generation by using a disentangled representation with
two independent implicit functions to capture both visible and obscured aspects
of objects. Additionally, it harmonizes gradients from 2D diffusion prior with
3D-aware diffusion priors by angular diffusion prior composition. Through
extensive evaluation, we demonstrate that Vista3D effectively sustains a
balance between the consistency and diversity of the generated 3D objects.
Demos and code will be available at https://github.com/florinshen/Vista3D.";Qiuhong Shen<author:sep>Xingyi Yang<author:sep>Michael Bi Mi<author:sep>Xinchao Wang;http://arxiv.org/pdf/2409.12193v1;cs.CV;ECCV'2024;gaussian splatting
2409.12323v1;http://arxiv.org/abs/2409.12323v1;2024-09-18;Depth Estimation Based on 3D Gaussian Splatting Siamese Defocus;"Depth estimation is a fundamental task in 3D geometry. While stereo depth
estimation can be achieved through triangulation methods, it is not as
straightforward for monocular methods, which require the integration of global
and local information. The Depth from Defocus (DFD) method utilizes camera lens
models and parameters to recover depth information from blurred images and has
been proven to perform well. However, these methods rely on All-In-Focus (AIF)
images for depth estimation, which is nearly impossible to obtain in real-world
applications. To address this issue, we propose a self-supervised framework
based on 3D Gaussian splatting and Siamese networks. By learning the blur
levels at different focal distances of the same scene in the focal stack, the
framework predicts the defocus map and Circle of Confusion (CoC) from a single
defocused image, using the defocus map as input to DepthNet for monocular depth
estimation. The 3D Gaussian splatting model renders defocused images using the
predicted CoC, and the differences between these and the real defocused images
provide additional supervision signals for the Siamese Defocus self-supervised
network. This framework has been validated on both artificially synthesized and
real blurred datasets. Subsequent quantitative and visualization experiments
demonstrate that our proposed framework is highly effective as a DFD method.";Jinchang Zhang<author:sep>Ningning Xu<author:sep>Hao Zhang<author:sep>Guoyu Lu;http://arxiv.org/pdf/2409.12323v1;cs.CV;;gaussian splatting
2409.12156v1;http://arxiv.org/abs/2409.12156v1;2024-09-18;JEAN: Joint Expression and Audio-guided NeRF-based Talking Face  Generation;"We introduce a novel method for joint expression and audio-guided talking
face generation. Recent approaches either struggle to preserve the speaker
identity or fail to produce faithful facial expressions. To address these
challenges, we propose a NeRF-based network. Since we train our network on
monocular videos without any ground truth, it is essential to learn
disentangled representations for audio and expression. We first learn audio
features in a self-supervised manner, given utterances from multiple subjects.
By incorporating a contrastive learning technique, we ensure that the learned
audio features are aligned to the lip motion and disentangled from the muscle
motion of the rest of the face. We then devise a transformer-based architecture
that learns expression features, capturing long-range facial expressions and
disentangling them from the speech-specific mouth movements. Through
quantitative and qualitative evaluation, we demonstrate that our method can
synthesize high-fidelity talking face videos, achieving state-of-the-art facial
expression transfer along with lip synchronization to unseen audio.";Sai Tanmay Reddy Chakkera<author:sep>Aggelina Chatziagapi<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2409.12156v1;cs.CV;"Accepted by BMVC 2024. Project Page:
  https://starc52.github.io/publications/2024-07-19-JEAN";nerf
2409.11681v1;http://arxiv.org/abs/2409.11681v1;2024-09-18;Gradient-Driven 3D Segmentation and Affordance Transfer in Gaussian  Splatting Using 2D Masks;"3D Gaussian Splatting has emerged as a powerful 3D scene representation
technique, capturing fine details with high efficiency. In this paper, we
introduce a novel voting-based method that extends 2D segmentation models to 3D
Gaussian splats. Our approach leverages masked gradients, where gradients are
filtered by input 2D masks, and these gradients are used as votes to achieve
accurate segmentation. As a byproduct, we discovered that inference-time
gradients can also be used to prune Gaussians, resulting in up to 21%
compression. Additionally, we explore few-shot affordance transfer, allowing
annotations from 2D images to be effectively transferred onto 3D Gaussian
splats. The robust yet straightforward mathematical formulation underlying this
approach makes it a highly effective tool for numerous downstream applications,
such as augmented reality (AR), object editing, and robotics. The project code
and additional resources are available at
https://jojijoseph.github.io/3dgs-segmentation.";Joji Joseph<author:sep>Bharadwaj Amrutur<author:sep>Shalabh Bhatnagar;http://arxiv.org/pdf/2409.11681v1;cs.CV;Preprint, Under review for ICRA 2025;gaussian splatting
2409.12014v2;http://arxiv.org/abs/2409.12014v2;2024-09-18;BRDF-NeRF: Neural Radiance Fields with Optical Satellite Images and BRDF  Modelling;"Understanding the anisotropic reflectance of complex Earth surfaces from
satellite imagery is crucial for numerous applications. Neural radiance fields
(NeRF) have become popular as a machine learning technique capable of deducing
the bidirectional reflectance distribution function (BRDF) of a scene from
multiple images. However, prior research has largely concentrated on applying
NeRF to close-range imagery, estimating basic Microfacet BRDF models, which
fall short for many Earth surfaces. Moreover, high-quality NeRFs generally
require several images captured simultaneously, a rare occurrence in satellite
imaging. To address these limitations, we propose BRDF-NeRF, developed to
explicitly estimate the Rahman-Pinty-Verstraete (RPV) model, a semi-empirical
BRDF model commonly employed in remote sensing. We assess our approach using
two datasets: (1) Djibouti, captured in a single epoch at varying viewing
angles with a fixed Sun position, and (2) Lanzhou, captured over multiple
epochs with different viewing angles and Sun positions. Our results, based on
only three to four satellite images for training, demonstrate that BRDF-NeRF
can effectively synthesize novel views from directions far removed from the
training data and produce high-quality digital surface models (DSMs).";Lulin Zhang<author:sep>Ewelina Rupnik<author:sep>Tri Dung Nguyen<author:sep>Stéphane Jacquemoud<author:sep>Yann Klinger;http://arxiv.org/pdf/2409.12014v2;cs.CV;;nerf
2409.11682v1;http://arxiv.org/abs/2409.11682v1;2024-09-18;SRIF: Semantic Shape Registration Empowered by Diffusion-based Image  Morphing and Flow Estimation;"In this paper, we propose SRIF, a novel Semantic shape Registration framework
based on diffusion-based Image morphing and Flow estimation. More concretely,
given a pair of extrinsically aligned shapes, we first render them from
multi-views, and then utilize an image interpolation framework based on
diffusion models to generate sequences of intermediate images between them. The
images are later fed into a dynamic 3D Gaussian splatting framework, with which
we reconstruct and post-process for intermediate point clouds respecting the
image morphing processing. In the end, tailored for the above, we propose a
novel registration module to estimate continuous normalizing flow, which
deforms source shape consistently towards the target, with intermediate point
clouds as weak guidance. Our key insight is to leverage large vision models
(LVMs) to associate shapes and therefore obtain much richer semantic
information on the relationship between shapes than the ad-hoc feature
extraction and alignment. As a consequence, SRIF achieves high-quality dense
correspondences on challenging shape pairs, but also delivers smooth,
semantically meaningful interpolation in between. Empirical evidence justifies
the effectiveness and superiority of our method as well as specific design
choices. The code is released at https://github.com/rqhuang88/SRIF.";Mingze Sun<author:sep>Chen Guo<author:sep>Puhua Jiang<author:sep>Shiwei Mao<author:sep>Yurun Chen<author:sep>Ruqi Huang;http://arxiv.org/pdf/2409.11682v1;cs.CV;;gaussian splatting
2409.11211v1;http://arxiv.org/abs/2409.11211v1;2024-09-17;SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction;"Digitizing 3D static scenes and 4D dynamic events from multi-view images has
long been a challenge in computer vision and graphics. Recently, 3D Gaussian
Splatting (3DGS) has emerged as a practical and scalable reconstruction method,
gaining popularity due to its impressive reconstruction quality, real-time
rendering capabilities, and compatibility with widely used visualization tools.
However, the method requires a substantial number of input views to achieve
high-quality scene reconstruction, introducing a significant practical
bottleneck. This challenge is especially severe in capturing dynamic scenes,
where deploying an extensive camera array can be prohibitively costly. In this
work, we identify the lack of spatial autocorrelation of splat features as one
of the factors contributing to the suboptimal performance of the 3DGS technique
in sparse reconstruction settings. To address the issue, we propose an
optimization strategy that effectively regularizes splat features by modeling
them as the outputs of a corresponding implicit neural field. This results in a
consistent enhancement of reconstruction quality across various scenarios. Our
approach effectively handles static and dynamic cases, as demonstrated by
extensive testing across different setups and scene complexities.";Marko Mihajlovic<author:sep>Sergey Prokudin<author:sep>Siyu Tang<author:sep>Robert Maier<author:sep>Federica Bogo<author:sep>Tony Tung<author:sep>Edmond Boyer;http://arxiv.org/pdf/2409.11211v1;cs.CV;"ECCV 2024 paper. The project page and code are available at
  https://markomih.github.io/SplatFields/";
2409.11356v1;http://arxiv.org/abs/2409.11356v1;2024-09-17;RenderWorld: World Model with Self-Supervised 3D Label;"End-to-end autonomous driving with vision-only is not only more
cost-effective compared to LiDAR-vision fusion but also more reliable than
traditional methods. To achieve a economical and robust purely visual
autonomous driving system, we propose RenderWorld, a vision-only end-to-end
autonomous driving framework, which generates 3D occupancy labels using a
self-supervised gaussian-based Img2Occ Module, then encodes the labels by
AM-VAE, and uses world model for forecasting and planning. RenderWorld employs
Gaussian Splatting to represent 3D scenes and render 2D images greatly improves
segmentation accuracy and reduces GPU memory consumption compared with
NeRF-based methods. By applying AM-VAE to encode air and non-air separately,
RenderWorld achieves more fine-grained scene element representation, leading to
state-of-the-art performance in both 4D occupancy forecasting and motion
planning from autoregressive world model.";Ziyang Yan<author:sep>Wenzhen Dong<author:sep>Yihua Shao<author:sep>Yuhang Lu<author:sep>Liu Haiyang<author:sep>Jingwen Liu<author:sep>Haozhe Wang<author:sep>Zhe Wang<author:sep>Yan Wang<author:sep>Fabio Remondino<author:sep>Yuexin Ma;http://arxiv.org/pdf/2409.11356v1;cs.CV;;gaussian splatting<tag:sep>nerf
2409.11307v1;http://arxiv.org/abs/2409.11307v1;2024-09-17;GS-Net: Generalizable Plug-and-Play 3D Gaussian Splatting Module;"3D Gaussian Splatting (3DGS) integrates the strengths of primitive-based
representations and volumetric rendering techniques, enabling real-time,
high-quality rendering. However, 3DGS models typically overfit to single-scene
training and are highly sensitive to the initialization of Gaussian ellipsoids,
heuristically derived from Structure from Motion (SfM) point clouds, which
limits both generalization and practicality. To address these limitations, we
propose GS-Net, a generalizable, plug-and-play 3DGS module that densifies
Gaussian ellipsoids from sparse SfM point clouds, enhancing geometric structure
representation. To the best of our knowledge, GS-Net is the first plug-and-play
3DGS module with cross-scene generalization capabilities. Additionally, we
introduce the CARLA-NVS dataset, which incorporates additional camera
viewpoints to thoroughly evaluate reconstruction and rendering quality.
Extensive experiments demonstrate that applying GS-Net to 3DGS yields a PSNR
improvement of 2.08 dB for conventional viewpoints and 1.86 dB for novel
viewpoints, confirming the method's effectiveness and robustness.";Yichen Zhang<author:sep>Zihan Wang<author:sep>Jiali Han<author:sep>Peilin Li<author:sep>Jiaxun Zhang<author:sep>Jianqiang Wang<author:sep>Lei He<author:sep>Keqiang Li;http://arxiv.org/pdf/2409.11307v1;cs.CV;;gaussian splatting
2409.10982v1;http://arxiv.org/abs/2409.10982v1;2024-09-17;GLC-SLAM: Gaussian Splatting SLAM with Efficient Loop Closure;"3D Gaussian Splatting (3DGS) has gained significant attention for its
application in dense Simultaneous Localization and Mapping (SLAM), enabling
real-time rendering and high-fidelity mapping. However, existing 3DGS-based
SLAM methods often suffer from accumulated tracking errors and map drift,
particularly in large-scale environments. To address these issues, we introduce
GLC-SLAM, a Gaussian Splatting SLAM system that integrates global optimization
of camera poses and scene models. Our approach employs frame-to-model tracking
and triggers hierarchical loop closure using a global-to-local strategy to
minimize drift accumulation. By dividing the scene into 3D Gaussian submaps, we
facilitate efficient map updates following loop corrections in large scenes.
Additionally, our uncertainty-minimized keyframe selection strategy prioritizes
keyframes observing more valuable 3D Gaussians to enhance submap optimization.
Experimental results on various datasets demonstrate that GLC-SLAM achieves
superior or competitive tracking and mapping performance compared to
state-of-the-art dense RGB-D SLAM systems.";Ziheng Xu<author:sep>Qingfeng Li<author:sep>Chen Chen<author:sep>Xuefeng Liu<author:sep>Jianwei Niu;http://arxiv.org/pdf/2409.10982v1;cs.RO;;gaussian splatting
2409.10925v2;http://arxiv.org/abs/2409.10925v2;2024-09-17;HGSLoc: 3DGS-based Heuristic Camera Pose Refinement;"Visual localization refers to the process of determining camera poses and
orientation within a known scene representation. This task is often complicated
by factors such as illumination changes and variations in viewing angles. In
this paper, we propose HGSLoc, a novel lightweight, plug and-play pose
optimization framework, which integrates 3D reconstruction with a heuristic
refinement strategy to achieve higher pose estimation accuracy. Specifically,
we introduce an explicit geometric map for 3D representation and high-fidelity
rendering, allowing the generation of high-quality synthesized views to support
accurate visual localization. Our method demonstrates a faster rendering speed
and higher localization accuracy compared to NeRF-based neural rendering
localization approaches. We introduce a heuristic refinement strategy, its
efficient optimization capability can quickly locate the target node, while we
set the step-level optimization step to enhance the pose accuracy in the
scenarios with small errors. With carefully designed heuristic functions, it
offers efficient optimization capabilities, enabling rapid error reduction in
rough localization estimations. Our method mitigates the dependence on complex
neural network models while demonstrating improved robustness against noise and
higher localization accuracy in challenging environments, as compared to neural
network joint optimization strategies. The optimization framework proposed in
this paper introduces novel approaches to visual localization by integrating
the advantages of 3D reconstruction and heuristic refinement strategy, which
demonstrates strong performance across multiple benchmark datasets, including
7Scenes and DB dataset.";Zhongyan Niu<author:sep>Zhen Tan<author:sep>Jinpu Zhang<author:sep>Xueliang Yang<author:sep>Dewen Hu;http://arxiv.org/pdf/2409.10925v2;cs.CV;;nerf
2409.10161v1;http://arxiv.org/abs/2409.10161v1;2024-09-16;SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using  Gaussian Splatting;"Sim2Real transfer, particularly for manipulation policies relying on RGB
images, remains a critical challenge in robotics due to the significant domain
shift between synthetic and real-world visual data. In this paper, we propose
SplatSim, a novel framework that leverages Gaussian Splatting as the primary
rendering primitive to reduce the Sim2Real gap for RGB-based manipulation
policies. By replacing traditional mesh representations with Gaussian Splats in
simulators, SplatSim produces highly photorealistic synthetic data while
maintaining the scalability and cost-efficiency of simulation. We demonstrate
the effectiveness of our framework by training manipulation policies within
SplatSim}and deploying them in the real world in a zero-shot manner, achieving
an average success rate of 86.25%, compared to 97.5% for policies trained on
real-world data.";Mohammad Nomaan Qureshi<author:sep>Sparsh Garg<author:sep>Francisco Yandun<author:sep>David Held<author:sep>George Kantor<author:sep>Abhishesh Silwal;http://arxiv.org/pdf/2409.10161v1;cs.RO;;gaussian splatting
2409.10327v1;http://arxiv.org/abs/2409.10327v1;2024-09-16;Baking Relightable NeRF for Real-time Direct/Indirect Illumination  Rendering;"Relighting, which synthesizes a novel view under a given lighting condition
(unseen in training time), is a must feature for immersive photo-realistic
experience. However, real-time relighting is challenging due to high
computation cost of the rendering equation which requires shape and material
decomposition and visibility test to model shadow. Additionally, for indirect
illumination, additional computation of rendering equation on each secondary
surface point (where reflection occurs) is required rendering real-time
relighting challenging. We propose a novel method that executes a CNN renderer
to compute primary surface points and rendering parameters, required for direct
illumination. We also present a lightweight hash grid-based renderer, for
indirect illumination, which is recursively executed to perform the secondary
ray tracing process. Both renderers are trained in a distillation from a
pre-trained teacher model and provide real-time physically-based rendering
under unseen lighting condition at a negligible loss of rendering quality.";Euntae Choi<author:sep>Vincent Carpentier<author:sep>Seunghun Shin<author:sep>Sungjoo Yoo;http://arxiv.org/pdf/2409.10327v1;cs.CV;Under review;nerf
2409.10041v1;http://arxiv.org/abs/2409.10041v1;2024-09-16;DENSER: 3D Gaussians Splatting for Scene Reconstruction of Dynamic Urban  Environments;"This paper presents DENSER, an efficient and effective approach leveraging 3D
Gaussian splatting (3DGS) for the reconstruction of dynamic urban environments.
While several methods for photorealistic scene representations, both implicitly
using neural radiance fields (NeRF) and explicitly using 3DGS have shown
promising results in scene reconstruction of relatively complex dynamic scenes,
modeling the dynamic appearance of foreground objects tend to be challenging,
limiting the applicability of these methods to capture subtleties and details
of the scenes, especially far dynamic objects. To this end, we propose DENSER,
a framework that significantly enhances the representation of dynamic objects
and accurately models the appearance of dynamic objects in the driving scene.
Instead of directly using Spherical Harmonics (SH) to model the appearance of
dynamic objects, we introduce and integrate a new method aiming at dynamically
estimating SH bases using wavelets, resulting in better representation of
dynamic objects appearance in both space and time. Besides object appearance,
DENSER enhances object shape representation through densification of its point
cloud across multiple scene frames, resulting in faster convergence of model
training. Extensive evaluations on KITTI dataset show that the proposed
approach significantly outperforms state-of-the-art methods by a wide margin.
Source codes and models will be uploaded to this repository
https://github.com/sntubix/denser";Mahmud A. Mohamad<author:sep>Gamal Elghazaly<author:sep>Arthur Hubert<author:sep>Raphael Frank;http://arxiv.org/pdf/2409.10041v1;cs.CV;;gaussian splatting<tag:sep>nerf
2409.10101v1;http://arxiv.org/abs/2409.10101v1;2024-09-16;Adaptive Segmentation-Based Initialization for Steered Mixture of  Experts Image Regression;"Kernel image regression methods have shown to provide excellent efficiency in
many image processing task, such as image and light-field compression, Gaussian
Splatting, denoising and super-resolution. The estimation of parameters for
these methods frequently employ gradient descent iterative optimization, which
poses significant computational burden for many applications. In this paper, we
introduce a novel adaptive segmentation-based initialization method targeted
for optimizing Steered-Mixture-of Experts (SMoE) gating networks and
Radial-Basis-Function (RBF) networks with steering kernels. The novel
initialization method allocates kernels into pre-calculated image segments. The
optimal number of kernels, kernel positions, and steering parameters are
derived per segment in an iterative optimization and kernel sparsification
procedure. The kernel information from ""local"" segments is then transferred
into a ""global"" initialization, ready for use in iterative optimization of
SMoE, RBF, and related kernel image regression methods. Results show that
drastic objective and subjective quality improvements are achievable compared
to widely used regular grid initialization, ""state-of-the-art"" K-Means
initialization and previously introduced segmentation-based initialization
methods, while also drastically improving the sparsity of the regression
models. For same quality, the novel initialization results in models with
around 50% reduction of kernels. In addition, a significant reduction of
convergence time is achieved, with overall run-time savings of up to 50%. The
segmentation-based initialization strategy itself admits heavy parallel
computation; in theory, it may be divided into as many tasks as there are
segments in the images. By accessing only four parallel GPUs, run-time savings
of already 50% for initialization are achievable.";Yi-Hsin Li<author:sep>Sebastian Knorr<author:sep>Mårten Sjöström<author:sep>Thomas Sikora;http://arxiv.org/pdf/2409.10101v1;cs.CV;;
2409.10216v1;http://arxiv.org/abs/2409.10216v1;2024-09-16;BEINGS: Bayesian Embodied Image-goal Navigation with Gaussian Splatting;"Image-goal navigation enables a robot to reach the location where a target
image was captured, using visual cues for guidance. However, current methods
either rely heavily on data and computationally expensive learning-based
approaches or lack efficiency in complex environments due to insufficient
exploration strategies. To address these limitations, we propose Bayesian
Embodied Image-goal Navigation Using Gaussian Splatting, a novel method that
formulates ImageNav as an optimal control problem within a model predictive
control framework. BEINGS leverages 3D Gaussian Splatting as a scene prior to
predict future observations, enabling efficient, real-time navigation decisions
grounded in the robot's sensory experiences. By integrating Bayesian updates,
our method dynamically refines the robot's strategy without requiring extensive
prior experience or data. Our algorithm is validated through extensive
simulations and physical experiments, showcasing its potential for embodied
robot systems in visually complex scenarios.";Wugang Meng<author:sep>Tianfu Wu<author:sep>Huan Yin<author:sep>Fumin Zhang;http://arxiv.org/pdf/2409.10216v1;cs.RO;;gaussian splatting
2409.10335v1;http://arxiv.org/abs/2409.10335v1;2024-09-16;Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering;"We propose two novel ideas (adoption of deferred rendering and mesh-based
representation) to improve the quality of 3D Gaussian splatting (3DGS) based
inverse rendering. We first report a problem incurred by hidden Gaussians,
where Gaussians beneath the surface adversely affect the pixel color in the
volume rendering adopted by the existing methods. In order to resolve the
problem, we propose applying deferred rendering and report new problems
incurred in a naive application of deferred rendering to the existing
3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based
inverse rendering under deferred rendering, we propose a novel two-step
training approach which (1) exploits mesh extraction and utilizes a hybrid
mesh-3DGS representation and (2) applies novel regularization methods to better
exploit the mesh. Our experiments show that, under relighting, the proposed
method offers significantly better rendering quality than the existing
3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based
inverse rendering method, it gives better rendering quality while offering
real-time rendering.";Euntae Choi<author:sep>Sungjoo Yoo;http://arxiv.org/pdf/2409.10335v1;cs.GR;Under review;gaussian splatting
2409.09829v1;http://arxiv.org/abs/2409.09829v1;2024-09-15;NARF24: Estimating Articulated Object Structure for Implicit Rendering;"Articulated objects and their representations pose a difficult problem for
robots. These objects require not only representations of geometry and texture,
but also of the various connections and joint parameters that make up each
articulation. We propose a method that learns a common Neural Radiance Field
(NeRF) representation across a small number of collected scenes. This
representation is combined with a parts-based image segmentation to produce an
implicit space part localization, from which the connectivity and joint
parameters of the articulated object can be estimated, thus enabling
configuration-conditioned rendering.";Stanley Lewis<author:sep>Tom Gao<author:sep>Odest Chadwicke Jenkins;http://arxiv.org/pdf/2409.09829v1;cs.RO;extended abstract as submitted to ICRA@40 anniversary conference;nerf
2409.09868v1;http://arxiv.org/abs/2409.09868v1;2024-09-15;SAFER-Splat: A Control Barrier Function for Safe Navigation with Online  Gaussian Splatting Maps;"SAFER-Splat (Simultaneous Action Filtering and Environment Reconstruction) is
a real-time, scalable, and minimally invasive action filter, based on control
barrier functions, for safe robotic navigation in a detailed map constructed at
runtime using Gaussian Splatting (GSplat). We propose a novel Control Barrier
Function (CBF) that not only induces safety with respect to all Gaussian
primitives in the scene, but when synthesized into a controller, is capable of
processing hundreds of thousands of Gaussians while maintaining a minimal
memory footprint and operating at 15 Hz during online Splat training. Of the
total compute time, a small fraction of it consumes GPU resources, enabling
uninterrupted training. The safety layer is minimally invasive, correcting
robot actions only when they are unsafe. To showcase the safety filter, we also
introduce SplatBridge, an open-source software package built with ROS for
real-time GSplat mapping for robots. We demonstrate the safety and robustness
of our pipeline first in simulation, where our method is 20-50x faster, safer,
and less conservative than competing methods based on neural radiance fields.
Further, we demonstrate simultaneous GSplat mapping and safety filtering on a
drone hardware platform using only on-board perception. We verify that under
teleoperation a human pilot cannot invoke a collision. Our videos and codebase
can be found at https://chengine.github.io/safer-splat.";Timothy Chen<author:sep>Aiden Swann<author:sep>Javier Yu<author:sep>Ola Shorinwa<author:sep>Riku Murai<author:sep>Monroe Kennedy III<author:sep>Mac Schwager;http://arxiv.org/pdf/2409.09868v1;cs.RO;;gaussian splatting
2409.09756v1;http://arxiv.org/abs/2409.09756v1;2024-09-15;MesonGS: Post-training Compression of 3D Gaussians via Efficient  Attribute Transformation;"3D Gaussian Splatting demonstrates excellent quality and speed in novel view
synthesis. Nevertheless, the huge file size of the 3D Gaussians presents
challenges for transmission and storage. Current works design compact models to
replace the substantial volume and attributes of 3D Gaussians, along with
intensive training to distill information. These endeavors demand considerable
training time, presenting formidable hurdles for practical deployment. To this
end, we propose MesonGS, a codec for post-training compression of 3D Gaussians.
Initially, we introduce a measurement criterion that considers both
view-dependent and view-independent factors to assess the impact of each
Gaussian point on the rendering output, enabling the removal of insignificant
points. Subsequently, we decrease the entropy of attributes through two
transformations that complement subsequent entropy coding techniques to enhance
the file compression rate. More specifically, we first replace rotation
quaternions with Euler angles; then, we apply region adaptive hierarchical
transform to key attributes to reduce entropy. Lastly, we adopt finer-grained
quantization to avoid excessive information loss. Moreover, a well-crafted
finetune scheme is devised to restore quality. Extensive experiments
demonstrate that MesonGS significantly reduces the size of 3D Gaussians while
preserving competitive quality.";Shuzhao Xie<author:sep>Weixiang Zhang<author:sep>Chen Tang<author:sep>Yunpeng Bai<author:sep>Rongwei Lu<author:sep>Shijia Ge<author:sep>Zhi Wang;http://arxiv.org/pdf/2409.09756v1;cs.CV;18 pages, 8 figures, ECCV 2024;gaussian splatting
2409.09295v1;http://arxiv.org/abs/2409.09295v1;2024-09-14;GEVO: Memory-Efficient Monocular Visual Odometry Using Gaussians;"Constructing a high-fidelity representation of the 3D scene using a monocular
camera can enable a wide range of applications on mobile devices, such as
micro-robots, smartphones, and AR/VR headsets. On these devices, memory is
often limited in capacity and its access often dominates the consumption of
compute energy. Although Gaussian Splatting (GS) allows for high-fidelity
reconstruction of 3D scenes, current GS-based SLAM is not memory efficient as a
large number of past images is stored to retrain Gaussians for reducing
catastrophic forgetting. These images often require two-orders-of-magnitude
higher memory than the map itself and thus dominate the total memory usage. In
this work, we present GEVO, a GS-based monocular SLAM framework that achieves
comparable fidelity as prior methods by rendering (instead of storing) them
from the existing map. Novel Gaussian initialization and optimization
techniques are proposed to remove artifacts from the map and delay the
degradation of the rendered images over time. Across a variety of environments,
GEVO achieves comparable map fidelity while reducing the memory overhead to
around 58 MBs, which is up to 94x lower than prior works.";Dasong Gao<author:sep>Peter Zhi Xuan Li<author:sep>Vivienne Sze<author:sep>Sertac Karaman;http://arxiv.org/pdf/2409.09295v1;cs.RO;8 pages;gaussian splatting
2409.08613v1;http://arxiv.org/abs/2409.08613v1;2024-09-13;Dense Point Clouds Matter: Dust-GS for Scene Reconstruction from Sparse  Viewpoints;"3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in scene
synthesis and novel view synthesis tasks. Typically, the initialization of 3D
Gaussian primitives relies on point clouds derived from Structure-from-Motion
(SfM) methods. However, in scenarios requiring scene reconstruction from sparse
viewpoints, the effectiveness of 3DGS is significantly constrained by the
quality of these initial point clouds and the limited number of input images.
In this study, we present Dust-GS, a novel framework specifically designed to
overcome the limitations of 3DGS in sparse viewpoint conditions. Instead of
relying solely on SfM, Dust-GS introduces an innovative point cloud
initialization technique that remains effective even with sparse input data.
Our approach leverages a hybrid strategy that integrates an adaptive
depth-based masking technique, thereby enhancing the accuracy and detail of
reconstructed scenes. Extensive experiments conducted on several benchmark
datasets demonstrate that Dust-GS surpasses traditional 3DGS methods in
scenarios with sparse viewpoints, achieving superior scene reconstruction
quality with a reduced number of input images.";Shan Chen<author:sep>Jiale Zhou<author:sep>Lei Li;http://arxiv.org/pdf/2409.08613v1;cs.CV;;gaussian splatting
2409.08562v1;http://arxiv.org/abs/2409.08562v1;2024-09-13;CSS: Overcoming Pose and Scene Challenges in Crowd-Sourced 3D Gaussian  Splatting;"We introduce Crowd-Sourced Splatting (CSS), a novel 3D Gaussian Splatting
(3DGS) pipeline designed to overcome the challenges of pose-free scene
reconstruction using crowd-sourced imagery. The dream of reconstructing
historically significant but inaccessible scenes from collections of
photographs has long captivated researchers. However, traditional 3D techniques
struggle with missing camera poses, limited viewpoints, and inconsistent
lighting. CSS addresses these challenges through robust geometric priors and
advanced illumination modeling, enabling high-quality novel view synthesis
under complex, real-world conditions. Our method demonstrates clear
improvements over existing approaches, paving the way for more accurate and
flexible applications in AR, VR, and large-scale 3D reconstruction.";Runze Chen<author:sep>Mingyu Xiao<author:sep>Haiyong Luo<author:sep>Fang Zhao<author:sep>Fan Wu<author:sep>Hao Xiong<author:sep>Qi Liu<author:sep>Meng Song;http://arxiv.org/pdf/2409.08562v1;cs.CV;;gaussian splatting
2409.08669v1;http://arxiv.org/abs/2409.08669v1;2024-09-13;AdR-Gaussian: Accelerating Gaussian Splatting with Adaptive Radius;"3D Gaussian Splatting (3DGS) is a recent explicit 3D representation that has
achieved high-quality reconstruction and real-time rendering of complex scenes.
However, the rasterization pipeline still suffers from unnecessary overhead
resulting from avoidable serial Gaussian culling, and uneven load due to the
distinct number of Gaussian to be rendered across pixels, which hinders wider
promotion and application of 3DGS. In order to accelerate Gaussian splatting,
we propose AdR-Gaussian, which moves part of serial culling in Render stage
into the earlier Preprocess stage to enable parallel culling, employing
adaptive radius to narrow the rendering pixel range for each Gaussian, and
introduces a load balancing method to minimize thread waiting time during the
pixel-parallel rendering. Our contributions are threefold, achieving a
rendering speed of 310% while maintaining equivalent or even better quality
than the state-of-the-art. Firstly, we propose to early cull Gaussian-Tile
pairs of low splatting opacity based on an adaptive radius in the
Gaussian-parallel Preprocess stage, which reduces the number of affected tile
through the Gaussian bounding circle, thus reducing unnecessary overhead and
achieving faster rendering speed. Secondly, we further propose early culling
based on axis-aligned bounding box for Gaussian splatting, which achieves a
more significant reduction in ineffective expenses by accurately calculating
the Gaussian size in the 2D directions. Thirdly, we propose a balancing
algorithm for pixel thread load, which compresses the information of heavy-load
pixels to reduce thread waiting time, and enhance information of light-load
pixels to hedge against rendering quality loss. Experiments on three datasets
demonstrate that our algorithm can significantly improve the Gaussian Splatting
rendering speed.";Xinzhe Wang<author:sep>Ran Yi<author:sep>Lizhuang Ma;http://arxiv.org/pdf/2409.08669v1;cs.CV;"SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers '24),
  December 03-06, 2024, Tokyo, Japan";gaussian splatting
2409.08947v2;http://arxiv.org/abs/2409.08947v2;2024-09-13;A Diffusion Approach to Radiance Field Relighting using  Multi-Illumination Synthesis;"Relighting radiance fields is severely underconstrained for multi-view data,
which is most often captured under a single illumination condition; It is
especially hard for full scenes containing multiple objects. We introduce a
method to create relightable radiance fields using such single-illumination
data by exploiting priors extracted from 2D image diffusion models. We first
fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by
light direction, allowing us to augment a single-illumination capture into a
realistic -- but possibly inconsistent -- multi-illumination dataset from
directly defined light directions. We use this augmented data to create a
relightable radiance field represented by 3D Gaussian splats. To allow direct
control of light direction for low-frequency lighting, we represent appearance
with a multi-layer perceptron parameterized on light direction. To enforce
multi-view consistency and overcome inaccuracies we optimize a per-image
auxiliary feature vector. We show results on synthetic and real multi-view data
under single illumination, demonstrating that our method successfully exploits
2D diffusion model priors to allow realistic 3D relighting for complete scenes.
Project site
https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/";Yohan Poirier-Ginter<author:sep>Alban Gauthier<author:sep>Julien Philip<author:sep>Jean-Francois Lalonde<author:sep>George Drettakis;http://arxiv.org/pdf/2409.08947v2;cs.CV;"Project site
  https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/";
2409.08353v1;http://arxiv.org/abs/2409.08353v1;2024-09-12;Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric  Videos;"Volumetric video represents a transformative advancement in visual media,
enabling users to freely navigate immersive virtual experiences and narrowing
the gap between digital and real worlds. However, the need for extensive manual
intervention to stabilize mesh sequences and the generation of excessively
large assets in existing workflows impedes broader adoption. In this paper, we
present a novel Gaussian-based approach, dubbed \textit{DualGS}, for real-time
and high-fidelity playback of complex human performance with excellent
compression ratios. Our key idea in DualGS is to separately represent motion
and appearance using the corresponding skin and joint Gaussians. Such an
explicit disentanglement can significantly reduce motion redundancy and enhance
temporal coherence. We begin by initializing the DualGS and anchoring skin
Gaussians to joint Gaussians at the first frame. Subsequently, we employ a
coarse-to-fine training strategy for frame-by-frame human performance modeling.
It includes a coarse alignment phase for overall motion prediction as well as a
fine-grained optimization for robust tracking and high-fidelity rendering. To
integrate volumetric video seamlessly into VR environments, we efficiently
compress motion using entropy encoding and appearance using codec compression
coupled with a persistent codebook. Our approach achieves a compression ratio
of up to 120 times, only requiring approximately 350KB of storage per frame. We
demonstrate the efficacy of our representation through photo-realistic,
free-view experiences on VR headsets, enabling users to immersively watch
musicians in performance and feel the rhythm of the notes at the performers'
fingertips.";Yuheng Jiang<author:sep>Zhehao Shen<author:sep>Yu Hong<author:sep>Chengcheng Guo<author:sep>Yize Wu<author:sep>Yingliang Zhang<author:sep>Jingyi Yu<author:sep>Lan Xu;http://arxiv.org/pdf/2409.08353v1;cs.GR;"Accepted at SIGGRAPH Asia 2024. Project page:
  https://nowheretrix.github.io/DualGS/";gaussian splatting
2409.08270v1;http://arxiv.org/abs/2409.08270v1;2024-09-12;FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally;"This study addresses the challenge of accurately segmenting 3D Gaussian
Splatting from 2D masks. Conventional methods often rely on iterative gradient
descent to assign each Gaussian a unique label, leading to lengthy optimization
and sub-optimal solutions. Instead, we propose a straightforward yet globally
optimal solver for 3D-GS segmentation. The core insight of our method is that,
with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially
a linear function with respect to the labels of each Gaussian. As such, the
optimal label assignment can be solved via linear programming in closed form.
This solution capitalizes on the alpha blending characteristic of the splatting
process for single step optimization. By incorporating the background bias in
our objective function, our method shows superior robustness in 3D segmentation
against noises. Remarkably, our optimization completes within 30 seconds, about
50$\times$ faster than the best existing methods. Extensive experiments
demonstrate the efficiency and robustness of our method in segmenting various
scenes, and its superior performance in downstream tasks such as object removal
and inpainting. Demos and code will be available at
https://github.com/florinshen/FlashSplat.";Qiuhong Shen<author:sep>Xingyi Yang<author:sep>Xinchao Wang;http://arxiv.org/pdf/2409.08270v1;cs.CV;ECCV'2024;gaussian splatting
2409.07759v1;http://arxiv.org/abs/2409.07759v1;2024-09-12;SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming  with Arbitrary Length;"Recent advances in 3D Gaussian Splatting (3DGS) have garnered significant
attention in computer vision and computer graphics due to its high rendering
speed and remarkable quality. While extant research has endeavored to extend
the application of 3DGS from static to dynamic scenes, such efforts have been
consistently impeded by excessive model sizes, constraints on video duration,
and content deviation. These limitations significantly compromise the
streamability of dynamic 3D Gaussian models, thereby restricting their utility
in downstream applications, including volumetric video, autonomous vehicle, and
immersive technologies such as virtual, augmented, and mixed reality.
  This paper introduces SwinGS, a novel framework for training, delivering, and
rendering volumetric video in a real-time streaming fashion. To address the
aforementioned challenges and enhance streamability, SwinGS integrates
spacetime Gaussian with Markov Chain Monte Carlo (MCMC) to adapt the model to
fit various 3D scenes across frames, in the meantime employing a sliding window
captures Gaussian snapshots for each frame in an accumulative way. We implement
a prototype of SwinGS and demonstrate its streamability across various datasets
and scenes. Additionally, we develop an interactive WebGL viewer enabling
real-time volumetric video playback on most devices with modern browsers,
including smartphones and tablets. Experimental results show that SwinGS
reduces transmission costs by 83.6% compared to previous work with ignorable
compromise in PSNR. Moreover, SwinGS easily scales to long video sequences
without compromising quality.";Bangya Liu<author:sep>Suman Banerjee;http://arxiv.org/pdf/2409.07759v1;cs.MM;;gaussian splatting
2409.08278v1;http://arxiv.org/abs/2409.08278v1;2024-09-12;DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with  Diffusion Priors;"We present DreamHOI, a novel method for zero-shot synthesis of human-object
interactions (HOIs), enabling a 3D human model to realistically interact with
any given object based on a textual description. This task is complicated by
the varying categories and geometries of real-world objects and the scarcity of
datasets encompassing diverse HOIs. To circumvent the need for extensive data,
we leverage text-to-image diffusion models trained on billions of image-caption
pairs. We optimize the articulation of a skinned human mesh using Score
Distillation Sampling (SDS) gradients obtained from these models, which predict
image-space edits. However, directly backpropagating image-space gradients into
complex articulation parameters is ineffective due to the local nature of such
gradients. To overcome this, we introduce a dual implicit-explicit
representation of a skinned mesh, combining (implicit) neural radiance fields
(NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization,
we transition between implicit and explicit forms, grounding the NeRF
generation while refining the mesh articulation. We validate our approach
through extensive experiments, demonstrating its effectiveness in generating
realistic HOIs.";Thomas Hanwen Zhu<author:sep>Ruining Li<author:sep>Tomas Jakab;http://arxiv.org/pdf/2409.08278v1;cs.CV;Project page: https://DreamHOI.github.io/;nerf
2409.08042v1;http://arxiv.org/abs/2409.08042v1;2024-09-12;Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared  Novel-view Synthesis;"Novel-view synthesis based on visible light has been extensively studied. In
comparison to visible light imaging, thermal infrared imaging offers the
advantage of all-weather imaging and strong penetration, providing increased
possibilities for reconstruction in nighttime and adverse weather scenarios.
However, thermal infrared imaging is influenced by physical characteristics
such as atmospheric transmission effects and thermal conduction, hindering the
precise reconstruction of intricate details in thermal infrared scenes,
manifesting as issues of floaters and indistinct edge features in synthesized
images. To address these limitations, this paper introduces a physics-induced
3D Gaussian splatting method named Thermal3D-GS. Thermal3D-GS begins by
modeling atmospheric transmission effects and thermal conduction in
three-dimensional media using neural networks. Additionally, a temperature
consistency constraint is incorporated into the optimization objective to
enhance the reconstruction accuracy of thermal infrared images. Furthermore, to
validate the effectiveness of our method, the first large-scale benchmark
dataset for this field named Thermal Infrared Novel-view Synthesis Dataset
(TI-NSD) is created. This dataset comprises 20 authentic thermal infrared video
scenes, covering indoor, outdoor, and UAV(Unmanned Aerial Vehicle) scenarios,
totaling 6,664 frames of thermal infrared image data. Based on this dataset,
this paper experimentally verifies the effectiveness of Thermal3D-GS. The
results indicate that our method outperforms the baseline method with a 3.03 dB
improvement in PSNR and significantly addresses the issues of floaters and
indistinct edge features present in the baseline method. Our dataset and
codebase will be released in
\href{https://github.com/mzzcdf/Thermal3DGS}{\textcolor{red}{Thermal3DGS}}.";Qian Chen<author:sep>Shihao Shu<author:sep>Xiangzhi Bai;http://arxiv.org/pdf/2409.08042v1;cs.CV;17 pages, 4 figures, 3 tables;gaussian splatting
2409.07200v1;http://arxiv.org/abs/2409.07200v1;2024-09-11;ThermalGaussian: Thermal 3D Gaussian Splatting;"Thermography is especially valuable for the military and other users of
surveillance cameras. Some recent methods based on Neural Radiance Fields
(NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of
thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS)
prevails due to its rapid training and real-time rendering. In this work, we
propose ThermalGaussian, the first thermal 3DGS approach capable of rendering
high-quality images in RGB and thermal modalities. We first calibrate the RGB
camera and the thermal camera to ensure that both modalities are accurately
aligned. Subsequently, we use the registered images to learn the multimodal 3D
Gaussians. To prevent the overfitting of any single modality, we introduce
several multimodal regularization constraints. We also develop smoothing
constraints tailored to the physical characteristics of the thermal modality.
Besides, we contribute a real-world dataset named RGBT-Scenes, captured by a
hand-hold thermal-infrared camera, facilitating future research on thermal
scene reconstruction. We conduct comprehensive experiments to show that
ThermalGaussian achieves photorealistic rendering of thermal images and
improves the rendering quality of RGB images. With the proposed multimodal
regularization constraints, we also reduced the model's storage cost by 90\%.
The code and dataset will be released.";Rongfeng Lu<author:sep>Hangyu Chen<author:sep>Zunjie Zhu<author:sep>Yuhang Qin<author:sep>Ming Lu<author:sep>Le Zhang<author:sep>Chenggang Yan<author:sep>Anke Xue;http://arxiv.org/pdf/2409.07200v1;cs.CV;10 pages, 7 figures;gaussian splatting<tag:sep>nerf
2409.07456v1;http://arxiv.org/abs/2409.07456v1;2024-09-11;Self-Evolving Depth-Supervised 3D Gaussian Splatting from Rendered  Stereo Pairs;"3D Gaussian Splatting (GS) significantly struggles to accurately represent
the underlying 3D scene geometry, resulting in inaccuracies and floating
artifacts when rendering depth maps. In this paper, we address this limitation,
undertaking a comprehensive analysis of the integration of depth priors
throughout the optimization process of Gaussian primitives, and present a novel
strategy for this purpose. This latter dynamically exploits depth cues from a
readily available stereo network, processing virtual stereo pairs rendered by
the GS model itself during training and achieving consistent self-improvement
of the scene representation. Experimental results on three popular datasets,
breaking ground as the first to assess depth accuracy for these models,
validate our findings.";Sadra Safadoust<author:sep>Fabio Tosi<author:sep>Fatma Güney<author:sep>Matteo Poggi;http://arxiv.org/pdf/2409.07456v1;cs.CV;BMVC 2024. Project page: https://kuis-ai.github.io/StereoGS/;gaussian splatting
2409.07454v1;http://arxiv.org/abs/2409.07454v1;2024-09-11;DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for  Text-to-3D Generation;"Learning radiance fields (NeRF) with powerful 2D diffusion models has
garnered popularity for text-to-3D generation. Nevertheless, the implicit 3D
representations of NeRF lack explicit modeling of meshes and textures over
surfaces, and such surface-undefined way may suffer from the issues, e.g.,
noisy surfaces with ambiguous texture details or cross-view inconsistency. To
alleviate this, we present DreamMesh, a novel text-to-3D architecture that
pivots on well-defined surfaces (triangle meshes) to generate high-fidelity
explicit 3D model. Technically, DreamMesh capitalizes on a distinctive
coarse-to-fine scheme. In the coarse stage, the mesh is first deformed by
text-guided Jacobians and then DreamMesh textures the mesh with an interlaced
use of 2D diffusion models in a tuning free manner from multiple viewpoints. In
the fine stage, DreamMesh jointly manipulates the mesh and refines the texture
map, leading to high-quality triangle meshes with high-fidelity textured
materials. Extensive experiments demonstrate that DreamMesh significantly
outperforms state-of-the-art text-to-3D methods in faithfully generating 3D
content with richer textual details and enhanced geometry. Our project page is
available at https://dreammesh.github.io.";Haibo Yang<author:sep>Yang Chen<author:sep>Yingwei Pan<author:sep>Ting Yao<author:sep>Zhineng Chen<author:sep>Zuxuan Wu<author:sep>Yu-Gang Jiang<author:sep>Tao Mei;http://arxiv.org/pdf/2409.07454v1;cs.CV;"ECCV 2024. Project page is available at
  \url{https://dreammesh.github.io}";nerf
2409.07245v1;http://arxiv.org/abs/2409.07245v1;2024-09-11;Single-View 3D Reconstruction via SO(2)-Equivariant Gaussian Sculpting  Networks;"This paper introduces SO(2)-Equivariant Gaussian Sculpting Networks (GSNs) as
an approach for SO(2)-Equivariant 3D object reconstruction from single-view
image observations.
  GSNs take a single observation as input to generate a Gaussian splat
representation describing the observed object's geometry and texture. By using
a shared feature extractor before decoding Gaussian colors, covariances,
positions, and opacities, GSNs achieve extremely high throughput (>150FPS).
Experiments demonstrate that GSNs can be trained efficiently using a multi-view
rendering loss and are competitive, in quality, with expensive diffusion-based
reconstruction algorithms. The GSN model is validated on multiple benchmark
experiments. Moreover, we demonstrate the potential for GSNs to be used within
a robotic manipulation pipeline for object-centric grasping.";Ruihan Xu<author:sep>Anthony Opipari<author:sep>Joshua Mah<author:sep>Stanley Lewis<author:sep>Haoran Zhang<author:sep>Hanzhe Guo<author:sep>Odest Chadwicke Jenkins;http://arxiv.org/pdf/2409.07245v1;cs.CV;"Accepted to RSS 2024 Workshop on Geometric and Algebraic Structure in
  Robot Learning";
2409.07452v1;http://arxiv.org/abs/2409.07452v1;2024-09-11;Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video  Diffusion Models;"Despite having tremendous progress in image-to-3D generation, existing
methods still struggle to produce multi-view consistent images with
high-resolution textures in detail, especially in the paradigm of 2D diffusion
that lacks 3D awareness. In this work, we present High-resolution Image-to-3D
model (Hi3D), a new video diffusion based paradigm that redefines a single
image to multi-view images as 3D-aware sequential image generation (i.e.,
orbital video generation). This methodology delves into the underlying temporal
consistency knowledge in video diffusion model that generalizes well to
geometry consistency across multiple views in 3D generation. Technically, Hi3D
first empowers the pre-trained video diffusion model with 3D-aware prior
(camera pose condition), yielding multi-view images with low-resolution texture
details. A 3D-aware video-to-video refiner is learnt to further scale up the
multi-view images with high-resolution texture details. Such high-resolution
multi-view images are further augmented with novel views through 3D Gaussian
Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D
reconstruction. Extensive experiments on both novel view synthesis and single
view reconstruction demonstrate that our Hi3D manages to produce superior
multi-view consistency images with highly-detailed textures. Source code and
data are available at \url{https://github.com/yanghb22-fdu/Hi3D-Official}.";Haibo Yang<author:sep>Yang Chen<author:sep>Yingwei Pan<author:sep>Ting Yao<author:sep>Zhineng Chen<author:sep>Chong-Wah Ngo<author:sep>Tao Mei;http://arxiv.org/pdf/2409.07452v1;cs.CV;"ACM Multimedia 2024. Source code is available at
  \url{https://github.com/yanghb22-fdu/Hi3D-Official}";
2409.07441v1;http://arxiv.org/abs/2409.07441v1;2024-09-11;Instant Facial Gaussians Translator for Relightable and Interactable  Facial Rendering;"We propose GauFace, a novel Gaussian Splatting representation, tailored for
efficient animation and rendering of physically-based facial assets. Leveraging
strong geometric priors and constrained optimization, GauFace ensures a neat
and structured Gaussian representation, delivering high fidelity and real-time
facial interaction of 30fps@1440p on a Snapdragon 8 Gen 2 mobile platform.
  Then, we introduce TransGS, a diffusion transformer that instantly translates
physically-based facial assets into the corresponding GauFace representations.
Specifically, we adopt a patch-based pipeline to handle the vast number of
Gaussians effectively. We also introduce a novel pixel-aligned sampling scheme
with UV positional encoding to ensure the throughput and rendering quality of
GauFace assets generated by our TransGS. Once trained, TransGS can instantly
translate facial assets with lighting conditions to GauFace representation,
With the rich conditioning modalities, it also enables editing and animation
capabilities reminiscent of traditional CG pipelines.
  We conduct extensive evaluations and user studies, compared to traditional
offline and online renderers, as well as recent neural rendering methods, which
demonstrate the superior performance of our approach for facial asset
rendering. We also showcase diverse immersive applications of facial assets
using our TransGS approach and GauFace representation, across various platforms
like PCs, phones and even VR headsets.";Dafei Qin<author:sep>Hongyang Lin<author:sep>Qixuan Zhang<author:sep>Kaichun Qiao<author:sep>Longwen Zhang<author:sep>Zijun Zhao<author:sep>Jun Saito<author:sep>Jingyi Yu<author:sep>Lan Xu<author:sep>Taku Komura;http://arxiv.org/pdf/2409.07441v1;cs.GR;Project Page: https://dafei-qin.github.io/TransGS.github.io/;gaussian splatting
2409.06703v1;http://arxiv.org/abs/2409.06703v1;2024-09-10;LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation;"Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of
static scenes and objects in 3D, offering unprecedented quality. However,
extending NeRFs to model dynamic objects or object articulations remains a
challenging problem. Previous works have tackled this issue by focusing on
part-level reconstruction and motion estimation for objects, but they often
rely on heuristics regarding the number of moving parts or object categories,
which can limit their practical use. In this work, we introduce LEIA, a novel
approach for representing dynamic 3D objects. Our method involves observing the
object at distinct time steps or ""states"" and conditioning a hypernetwork on
the current state, using this to parameterize our NeRF. This approach allows us
to learn a view-invariant latent representation for each state. We further
demonstrate that by interpolating between these states, we can generate novel
articulation configurations in 3D space that were previously unseen. Our
experimental results highlight the effectiveness of our method in articulating
objects in a manner that is independent of the viewing angle and joint
configuration. Notably, our approach outperforms previous methods that rely on
motion information for articulation registration.";Archana Swaminathan<author:sep>Anubhav Gupta<author:sep>Kamal Gupta<author:sep>Shishira R. Maiya<author:sep>Vatsal Agarwal<author:sep>Abhinav Shrivastava;http://arxiv.org/pdf/2409.06703v1;cs.CV;"Accepted to ECCV 2024. Project Website at
  https://archana1998.github.io/leia/";nerf
2409.06685v1;http://arxiv.org/abs/2409.06685v1;2024-09-10;GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface  Reconstruction;"3D Gaussian Splatting (3DGS) has shown promising performance in novel view
synthesis. Previous methods adapt it to obtaining surfaces of either individual
3D objects or within limited scenes. In this paper, we make the first attempt
to tackle the challenging task of large-scale scene surface reconstruction.
This task is particularly difficult due to the high GPU memory consumption,
different levels of details for geometric representation, and noticeable
inconsistencies in appearance. To this end, we propose GigaGS, the first work
for high-quality surface reconstruction for large-scale scenes using 3DGS.
GigaGS first applies a partitioning strategy based on the mutual visibility of
spatial regions, which effectively grouping cameras for parallel processing. To
enhance the quality of the surface, we also propose novel multi-view
photometric and geometric consistency constraints based on Level-of-Detail
representation. In doing so, our method can reconstruct detailed surface
structures. Comprehensive experiments are conducted on various datasets. The
consistent improvement demonstrates the superiority of GigaGS.";Junyi Chen<author:sep>Weicai Ye<author:sep>Yifan Wang<author:sep>Danpeng Chen<author:sep>Di Huang<author:sep>Wanli Ouyang<author:sep>Guofeng Zhang<author:sep>Yu Qiao<author:sep>Tong He;http://arxiv.org/pdf/2409.06685v1;cs.CV;;gaussian splatting
2409.06765v1;http://arxiv.org/abs/2409.06765v1;2024-09-10;gsplat: An Open-Source Library for Gaussian Splatting;"gsplat is an open-source library designed for training and developing
Gaussian Splatting methods. It features a front-end with Python bindings
compatible with the PyTorch library and a back-end with highly optimized CUDA
kernels. gsplat offers numerous features that enhance the optimization of
Gaussian Splatting models, which include optimization improvements for speed,
memory, and convergence times. Experimental results demonstrate that gsplat
achieves up to 10% less training time and 4x less memory than the original
implementation. Utilized in several research projects, gsplat is actively
maintained on GitHub. Source code is available at
https://github.com/nerfstudio-project/gsplat under Apache License 2.0. We
welcome contributions from the open-source community.";Vickie Ye<author:sep>Ruilong Li<author:sep>Justin Kerr<author:sep>Matias Turkulainen<author:sep>Brent Yi<author:sep>Zhuoyang Pan<author:sep>Otto Seiskari<author:sep>Jianbo Ye<author:sep>Jeffrey Hu<author:sep>Matthew Tancik<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2409.06765v1;cs.CV;17 pages, 2 figures, JMLR MLOSS;gaussian splatting<tag:sep>nerf
2409.06407v1;http://arxiv.org/abs/2409.06407v1;2024-09-10;Sources of Uncertainty in 3D Scene Reconstruction;"The process of 3D scene reconstruction can be affected by numerous
uncertainty sources in real-world scenes. While Neural Radiance Fields (NeRFs)
and 3D Gaussian Splatting (GS) achieve high-fidelity rendering, they lack
built-in mechanisms to directly address or quantify uncertainties arising from
the presence of noise, occlusions, confounding outliers, and imprecise camera
pose inputs. In this paper, we introduce a taxonomy that categorizes different
sources of uncertainty inherent in these methods. Moreover, we extend NeRF- and
GS-based methods with uncertainty estimation techniques, including learning
uncertainty outputs and ensembles, and perform an empirical study to assess
their ability to capture the sensitivity of the reconstruction. Our study
highlights the need for addressing various uncertainty aspects when designing
NeRF/GS-based methods for uncertainty-aware 3D reconstruction.";Marcus Klasson<author:sep>Riccardo Mereu<author:sep>Juho Kannala<author:sep>Arno Solin;http://arxiv.org/pdf/2409.06407v1;cs.CV;"To appear in ECCV 2024 Workshop Proceedings. Project page at
  https://aaltoml.github.io/uncertainty-nerf-gs/";gaussian splatting<tag:sep>nerf
2409.05819v1;http://arxiv.org/abs/2409.05819v1;2024-09-09;GASP: Gaussian Splatting for Physic-Based Simulations;"Physics simulation is paramount for modeling and utilization of 3D scenes in
various real-world applications. However, its integration with state-of-the-art
3D scene rendering techniques such as Gaussian Splatting (GS) remains
challenging. Existing models use additional meshing mechanisms, including
triangle or tetrahedron meshing, marching cubes, or cage meshes. As an
alternative, we can modify the physics grounded Newtonian dynamics to align
with 3D Gaussian components. Current models take the first-order approximation
of a deformation map, which locally approximates the dynamics by linear
transformations. In contrast, our Gaussian Splatting for Physics-Based
Simulations (GASP) model uses such a map (without any modifications) and flat
Gaussian distributions, which are parameterized by three points (mesh faces).
Subsequently, each 3D point (mesh face node) is treated as a discrete entity
within a 3D space. Consequently, the problem of modeling Gaussian components is
reduced to working with 3D points. Additionally, the information on mesh faces
can be used to incorporate further properties into the physics model,
facilitating the use of triangles. Resulting solution can be integrated into
any physics engine that can be treated as a black box. As demonstrated in our
studies, the proposed model exhibits superior performance on a diverse range of
benchmark datasets designed for 3D object rendering.";Piotr Borycki<author:sep>Weronika Smolak<author:sep>Joanna Waczyńska<author:sep>Marcin Mazur<author:sep>Sławomir Tadeja<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2409.05819v1;cs.CV;;gaussian splatting
2409.05617v1;http://arxiv.org/abs/2409.05617v1;2024-09-09;G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel  View Synthesis;"Following the burgeoning interest in implicit neural representation, Neural
Light Field (NeLF) has been introduced to predict the color of a ray directly.
Unlike Neural Radiance Field (NeRF), NeLF does not create a point-wise
representation by predicting color and volume density for each point in space.
However, the current NeLF methods face a challenge as they need to train a NeRF
model first and then synthesize over 10K views to train NeLF for improved
performance. Additionally, the rendering quality of NeLF methods is lower
compared to NeRF methods. In this paper, we propose G-NeLF, a versatile
grid-based NeLF approach that utilizes spatial-aware features to unleash the
potential of the neural network's inference capability, and consequently
overcome the difficulties of NeLF training. Specifically, we employ a
spatial-aware feature sequence derived from a meticulously crafted grid as the
ray's representation. Drawing from our empirical studies on the adaptability of
multi-resolution hash tables, we introduce a novel grid-based ray
representation for NeLF that can represent the entire space with a very limited
number of parameters. To better utilize the sequence feature, we design a
lightweight ray color decoder that simulates the ray propagation process,
enabling a more efficient inference of the ray's color. G-NeLF can be trained
without necessitating significant storage overhead and with the model size of
only 0.95 MB to surpass previous state-of-the-art NeLF. Moreover, compared with
grid-based NeRF methods, e.g., Instant-NGP, we only utilize one-tenth of its
parameters to achieve higher performance. Our code will be released upon
acceptance.";Lutao Jiang<author:sep>Lin Wang;http://arxiv.org/pdf/2409.05617v1;cs.CV;;nerf
2409.05334v1;http://arxiv.org/abs/2409.05334v1;2024-09-09;Lagrangian Hashing for Compressed Neural Field Representations;"We present Lagrangian Hashing, a representation for neural fields combining
the characteristics of fast training NeRF methods that rely on Eulerian grids
(i.e.~InstantNGP), with those that employ points equipped with features as a
way to represent information (e.g. 3D Gaussian Splatting or PointNeRF). We
achieve this by incorporating a point-based representation into the
high-resolution layers of the hierarchical hash tables of an InstantNGP
representation. As our points are equipped with a field of influence, our
representation can be interpreted as a mixture of Gaussians stored within the
hash table. We propose a loss that encourages the movement of our Gaussians
towards regions that require more representation budget to be sufficiently well
represented. Our main finding is that our representation allows the
reconstruction of signals using a more compact representation without
compromising quality.";Shrisudhan Govindarajan<author:sep>Zeno Sambugaro<author:sep> Akhmedkhan<author:sep> Shabanov<author:sep>Towaki Takikawa<author:sep>Daniel Rebain<author:sep>Weiwei Sun<author:sep>Nicola Conci<author:sep>Kwang Moo Yi<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2409.05334v1;cs.CV;Project page: https://theialab.github.io/laghashes/;gaussian splatting<tag:sep>nerf
2409.05310v1;http://arxiv.org/abs/2409.05310v1;2024-09-09;Neural Surface Reconstruction and Rendering for LiDAR-Visual Systems;"This paper presents a unified surface reconstruction and rendering framework
for LiDAR-visual systems, integrating Neural Radiance Fields (NeRF) and Neural
Distance Fields (NDF) to recover both appearance and structural information
from posed images and point clouds. We address the structural visible gap
between NeRF and NDF by utilizing a visible-aware occupancy map to classify
space into the free, occupied, visible unknown, and background regions. This
classification facilitates the recovery of a complete appearance and structure
of the scene. We unify the training of the NDF and NeRF using a spatial-varying
scale SDF-to-density transformation for levels of detail for both structure and
appearance. The proposed method leverages the learned NDF for structure-aware
NeRF training by an adaptive sphere tracing sampling strategy for accurate
structure rendering. In return, NeRF further refines structural in recovering
missing or fuzzy structures in the NDF. Extensive experiments demonstrate the
superior quality and versatility of the proposed method across various
scenarios. To benefit the community, the codes will be released at
\url{https://github.com/hku-mars/M2Mapping}.";Jianheng Liu<author:sep>Chunran Zheng<author:sep>Yunfei Wan<author:sep>Bowen Wang<author:sep>Yixi Cai<author:sep>Fu Zhang;http://arxiv.org/pdf/2409.05310v1;cs.RO;;nerf
2409.06104v1;http://arxiv.org/abs/2409.06104v1;2024-09-09;LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance  Fields with RGB-Event Stereo;"We present a method for reconstructing a clear Neural Radiance Field (NeRF)
even with fast camera motions. To address blur artifacts, we leverage both
(blurry) RGB images and event camera data captured in a binocular
configuration. Importantly, when reconstructing our clear NeRF, we consider the
camera modeling imperfections that arise from the simple pinhole camera model
as learned embeddings for each camera measurement, and further learn a mapper
that connects event camera measurements with RGB data. As no previous dataset
exists for our binocular setting, we introduce an event camera dataset with
captures from a 3D-printed stereo configuration between RGB and event cameras.
Empirically, we evaluate our introduced dataset and EVIMOv2 and show that our
method leads to improved reconstructions. Our code and dataset are available at
https://github.com/ubc-vision/LSENeRF.";Wei Zhi Tang<author:sep>Daniel Rebain<author:sep>Kostantinos G. Derpanis<author:sep>Kwang Moo Yi;http://arxiv.org/pdf/2409.06104v1;cs.CV;;nerf
2409.05413v1;http://arxiv.org/abs/2409.05413v1;2024-09-09;From Words to Poses: Enhancing Novel Object Pose Estimation with Vision  Language Models;"Robots are increasingly envisioned to interact in real-world scenarios, where
they must continuously adapt to new situations. To detect and grasp novel
objects, zero-shot pose estimators determine poses without prior knowledge.
Recently, vision language models (VLMs) have shown considerable advances in
robotics applications by establishing an understanding between language input
and image input. In our work, we take advantage of VLMs zero-shot capabilities
and translate this ability to 6D object pose estimation. We propose a novel
framework for promptable zero-shot 6D object pose estimation using language
embeddings. The idea is to derive a coarse location of an object based on the
relevancy map of a language-embedded NeRF reconstruction and to compute the
pose estimate with a point cloud registration method. Additionally, we provide
an analysis of LERF's suitability for open-set object pose estimation. We
examine hyperparameters, such as activation thresholds for relevancy maps and
investigate the zero-shot capabilities on an instance- and category-level.
Furthermore, we plan to conduct robotic grasping experiments in a real-world
setting.";Tessa Pulli<author:sep>Stefan Thalhammer<author:sep>Simon Schwaiger<author:sep>Markus Vincze;http://arxiv.org/pdf/2409.05413v1;cs.CV;;nerf
2409.06037v1;http://arxiv.org/abs/2409.06037v1;2024-09-09;Online 3D reconstruction and dense tracking in endoscopic videos;"3D scene reconstruction from stereo endoscopic video data is crucial for
advancing surgical interventions. In this work, we present an online framework
for online, dense 3D scene reconstruction and tracking, aimed at enhancing
surgical scene understanding and assisting interventions. Our method
dynamically extends a canonical scene representation using Gaussian splatting,
while modeling tissue deformations through a sparse set of control points. We
introduce an efficient online fitting algorithm that optimizes the scene
parameters, enabling consistent tracking and accurate reconstruction. Through
experiments on the StereoMIS dataset, we demonstrate the effectiveness of our
approach, outperforming state-of-the-art tracking methods and achieving
comparable performance to offline reconstruction techniques. Our work enables
various downstream applications thus contributing to advancing the capabilities
of surgical assistance systems.";Michel Hayoz<author:sep>Christopher Hahne<author:sep>Thomas Kurmann<author:sep>Max Allan<author:sep>Guido Beldi<author:sep>Daniel Candinas<author:sep>ablo Márquez-Neila<author:sep>Raphael Sznitman;http://arxiv.org/pdf/2409.06037v1;cs.CV;;gaussian splatting
2409.05407v1;http://arxiv.org/abs/2409.05407v1;2024-09-09;KRONC: Keypoint-based Robust Camera Optimization for 3D Car  Reconstruction;"The three-dimensional representation of objects or scenes starting from a set
of images has been a widely discussed topic for years and has gained additional
attention after the diffusion of NeRF-based approaches. However, an
underestimated prerequisite is the knowledge of camera poses or, more
specifically, the estimation of the extrinsic calibration parameters. Although
excellent general-purpose Structure-from-Motion methods are available as a
pre-processing step, their computational load is high and they require a lot of
frames to guarantee sufficient overlapping among the views. This paper
introduces KRONC, a novel approach aimed at inferring view poses by leveraging
prior knowledge about the object to reconstruct and its representation through
semantic keypoints. With a focus on vehicle scenes, KRONC is able to estimate
the position of the views as a solution to a light optimization problem
targeting the convergence of keypoints' back-projections to a singular point.
To validate the method, a specific dataset of real-world car scenes has been
collected. Experiments confirm KRONC's ability to generate excellent estimates
of camera poses starting from very coarse initialization. Results are
comparable with Structure-from-Motion methods with huge savings in computation.
Code and data will be made publicly available.";Davide Di Nucci<author:sep>Alessandro Simoni<author:sep>Matteo Tomei<author:sep>Luca Ciuffreda<author:sep>Roberto Vezzani<author:sep>Rita Cucchiara;http://arxiv.org/pdf/2409.05407v1;cs.CV;Accepted at ECCVW;nerf
2409.04963v1;http://arxiv.org/abs/2409.04963v1;2024-09-08;GS-PT: Exploiting 3D Gaussian Splatting for Comprehensive Point Cloud  Understanding via Self-supervised Learning;"Self-supervised learning of point cloud aims to leverage unlabeled 3D data to
learn meaningful representations without reliance on manual annotations.
However, current approaches face challenges such as limited data diversity and
inadequate augmentation for effective feature learning. To address these
challenges, we propose GS-PT, which integrates 3D Gaussian Splatting (3DGS)
into point cloud self-supervised learning for the first time. Our pipeline
utilizes transformers as the backbone for self-supervised pre-training and
introduces novel contrastive learning tasks through 3DGS. Specifically, the
transformers aim to reconstruct the masked point cloud. 3DGS utilizes
multi-view rendered images as input to generate enhanced point cloud
distributions and novel view images, facilitating data augmentation and
cross-modal contrastive learning. Additionally, we incorporate features from
depth maps. By optimizing these tasks collectively, our method enriches the
tri-modal self-supervised learning process, enabling the model to leverage the
correlation across 3D point clouds and 2D images from various modalities. We
freeze the encoder after pre-training and test the model's performance on
multiple downstream tasks. Experimental results indicate that GS-PT outperforms
the off-the-shelf self-supervised learning methods on various downstream tasks
including 3D object classification, real-world classifications, and few-shot
learning and segmentation.";Keyi Liu<author:sep>Yeqi Luo<author:sep>Weidong Yang<author:sep>Jingyi Xu<author:sep>Zhijun Li<author:sep>Wen-Ming Chen<author:sep>Ben Fei;http://arxiv.org/pdf/2409.04963v1;cs.CV;;gaussian splatting
2409.05099v3;http://arxiv.org/abs/2409.05099v3;2024-09-08;DreamMapping: High-Fidelity Text-to-3D Generation via Variational  Distribution Mapping;"Score Distillation Sampling (SDS) has emerged as a prevalent technique for
text-to-3D generation, enabling 3D content creation by distilling
view-dependent information from text-to-2D guidance. However, they frequently
exhibit shortcomings such as over-saturated color and excess smoothness. In
this paper, we conduct a thorough analysis of SDS and refine its formulation,
finding that the core design is to model the distribution of rendered images.
Following this insight, we introduce a novel strategy called Variational
Distribution Mapping (VDM), which expedites the distribution modeling process
by regarding the rendered images as instances of degradation from
diffusion-based generation. This special design enables the efficient training
of variational distribution by skipping the calculations of the Jacobians in
the diffusion U-Net. We also introduce timestep-dependent Distribution
Coefficient Annealing (DCA) to further improve distilling precision. Leveraging
VDM and DCA, we use Gaussian Splatting as the 3D representation and build a
text-to-3D generation framework. Extensive experiments and evaluations
demonstrate the capability of VDM and DCA to generate high-fidelity and
realistic assets with optimization efficiency.";Zeyu Cai<author:sep>Duotun Wang<author:sep>Yixun Liang<author:sep>Zhijing Shao<author:sep>Ying-Cong Chen<author:sep>Xiaohang Zhan<author:sep>Zeyu Wang;http://arxiv.org/pdf/2409.05099v3;cs.CV;15 pages, 14 figures;gaussian splatting
2409.04751v2;http://arxiv.org/abs/2409.04751v2;2024-09-07;Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for  Fisheye Cameras;"Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high
fidelity and real-time rendering. However, adapting 3DGS to different camera
models, particularly fisheye lenses, poses challenges due to the unique 3D to
2D projection calculation. Additionally, there are inefficiencies in the
tile-based splatting, especially for the extreme curvature and wide field of
view of fisheye lenses, which are crucial for its broader real-life
applications. To tackle these challenges, we introduce Fisheye-GS.This
innovative method recalculates the projection transformation and its gradients
for fisheye cameras. Our approach can be seamlessly integrated as a module into
other efficient 3D rendering methods, emphasizing its extensibility,
lightweight nature, and modular design. Since we only modified the projection
component, it can also be easily adapted for use with different camera models.
Compared to methods that train after undistortion, our approach demonstrates a
clear improvement in visual quality.";Zimu Liao<author:sep>Siyan Chen<author:sep>Rong Fu<author:sep>Yi Wang<author:sep>Zhongling Su<author:sep>Hao Luo<author:sep>Li Ma<author:sep>Linning Xu<author:sep>Bo Dai<author:sep>Hengjie Li<author:sep>Zhilin Pei<author:sep>Xingcheng Zhang;http://arxiv.org/pdf/2409.04751v2;cs.CV;;gaussian splatting
2409.04196v1;http://arxiv.org/abs/2409.04196v1;2024-09-06;GST: Precise 3D Human Body from a Single Image with Gaussian Splatting  Transformers;"Reconstructing realistic 3D human models from monocular images has
significant applications in creative industries, human-computer interfaces, and
healthcare. We base our work on 3D Gaussian Splatting (3DGS), a scene
representation composed of a mixture of Gaussians. Predicting such mixtures for
a human from a single input image is challenging, as it is a non-uniform
density (with a many-to-one relationship with input pixels) with strict
physical constraints. At the same time, it needs to be flexible to accommodate
a variety of clothes and poses. Our key observation is that the vertices of
standardized human meshes (such as SMPL) can provide an adequate density and
approximate initial position for Gaussians. We can then train a transformer
model to jointly predict comparatively small adjustments to these positions, as
well as the other Gaussians' attributes and the SMPL parameters. We show
empirically that this combination (using only multi-view supervision) can
achieve fast inference of 3D human models from a single image without test-time
optimization, expensive diffusion models, or 3D points supervision. We also
show that it can improve 3D pose estimation by better fitting human models that
account for clothes and other variations. The code is available on the project
website https://abdullahamdi.com/gst/ .";Lorenza Prospero<author:sep>Abdullah Hamdi<author:sep>Joao F. Henriques<author:sep>Christian Rupprecht;http://arxiv.org/pdf/2409.04196v1;cs.CV;preprint;gaussian splatting
2409.04013v1;http://arxiv.org/abs/2409.04013v1;2024-09-06;3D-GP-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian  Geometric Priors;"Multi-view image compression is vital for 3D-related applications. To
effectively model correlations between views, existing methods typically
predict disparity between two views on a 2D plane, which works well for small
disparities, such as in stereo images, but struggles with larger disparities
caused by significant view changes. To address this, we propose a novel
approach: learning-based multi-view image coding with 3D Gaussian geometric
priors (3D-GP-LMVIC). Our method leverages 3D Gaussian Splatting to derive
geometric priors of the 3D scene, enabling more accurate disparity estimation
across views within the compression model. Additionally, we introduce a depth
map compression model to reduce redundancy in geometric information between
views. A multi-view sequence ordering method is also proposed to enhance
correlations between adjacent views. Experimental results demonstrate that
3D-GP-LMVIC surpasses both traditional and learning-based methods in
performance, while maintaining fast encoding and decoding speed.";Yujun Huang<author:sep>Bin Chen<author:sep>Niu Lian<author:sep>Baoyi An<author:sep>Shu-Tao Xia;http://arxiv.org/pdf/2409.04013v1;cs.CV;19pages, 8 figures, conference;gaussian splatting
2409.04482v1;http://arxiv.org/abs/2409.04482v1;2024-09-06;SCARF: Scalable Continual Learning Framework for Memory-efficient  Multiple Neural Radiance Fields;"This paper introduces a novel continual learning framework for synthesising
novel views of multiple scenes, learning multiple 3D scenes incrementally, and
updating the network parameters only with the training data of the upcoming new
scene. We build on Neural Radiance Fields (NeRF), which uses multi-layer
perceptron to model the density and radiance field of a scene as the implicit
function. While NeRF and its extensions have shown a powerful capability of
rendering photo-realistic novel views in a single 3D scene, managing these
growing 3D NeRF assets efficiently is a new scientific problem. Very few works
focus on the efficient representation or continuous learning capability of
multiple scenes, which is crucial for the practical applications of NeRF. To
achieve these goals, our key idea is to represent multiple scenes as the linear
combination of a cross-scene weight matrix and a set of scene-specific weight
matrices generated from a global parameter generator. Furthermore, we propose
an uncertain surface knowledge distillation strategy to transfer the radiance
field knowledge of previous scenes to the new model. Representing multiple 3D
scenes with such weight matrices significantly reduces memory requirements. At
the same time, the uncertain surface distillation strategy greatly overcomes
the catastrophic forgetting problem and maintains the photo-realistic rendering
quality of previous scenes. Experiments show that the proposed approach
achieves state-of-the-art rendering quality of continual learning NeRF on
NeRF-Synthetic, LLFF, and TanksAndTemples datasets while preserving extra low
storage cost.";Yuze Wang<author:sep>Junyi Wang<author:sep>Chen Wang<author:sep>Wantong Duan<author:sep>Yongtang Bao<author:sep>Yue Qi;http://arxiv.org/pdf/2409.04482v1;cs.CV;;nerf
2409.03456v1;http://arxiv.org/abs/2409.03456v1;2024-09-05;LM-Gaussian: Boost Sparse-view 3D Gaussian Splatting with Large Model  Priors;"We aim to address sparse-view reconstruction of a 3D scene by leveraging
priors from large-scale vision models. While recent advancements such as 3D
Gaussian Splatting (3DGS) have demonstrated remarkable successes in 3D
reconstruction, these methods typically necessitate hundreds of input images
that densely capture the underlying scene, making them time-consuming and
impractical for real-world applications. However, sparse-view reconstruction is
inherently ill-posed and under-constrained, often resulting in inferior and
incomplete outcomes. This is due to issues such as failed initialization,
overfitting on input images, and a lack of details. To mitigate these
challenges, we introduce LM-Gaussian, a method capable of generating
high-quality reconstructions from a limited number of images. Specifically, we
propose a robust initialization module that leverages stereo priors to aid in
the recovery of camera poses and the reliable point clouds. Additionally, a
diffusion-based refinement is iteratively applied to incorporate image
diffusion priors into the Gaussian optimization process to preserve intricate
scene details. Finally, we utilize video diffusion priors to further enhance
the rendered images for realistic visual effects. Overall, our approach
significantly reduces the data acquisition requirements compared to previous
3DGS methods. We validate the effectiveness of our framework through
experiments on various public datasets, demonstrating its potential for
high-quality 360-degree scene reconstruction. Visual results are on our
website.";Hanyang Yu<author:sep>Xiaoxiao Long<author:sep>Ping Tan;http://arxiv.org/pdf/2409.03456v1;cs.CV;Project page: https://hanyangyu1021.github.io/lm-gaussian.github.io/;gaussian splatting
2409.03213v1;http://arxiv.org/abs/2409.03213v1;2024-09-05;Optimizing 3D Gaussian Splatting for Sparse Viewpoint Scene  Reconstruction;"3D Gaussian Splatting (3DGS) has emerged as a promising approach for 3D scene
representation, offering a reduction in computational overhead compared to
Neural Radiance Fields (NeRF). However, 3DGS is susceptible to high-frequency
artifacts and demonstrates suboptimal performance under sparse viewpoint
conditions, thereby limiting its applicability in robotics and computer vision.
To address these limitations, we introduce SVS-GS, a novel framework for Sparse
Viewpoint Scene reconstruction that integrates a 3D Gaussian smoothing filter
to suppress artifacts. Furthermore, our approach incorporates a Depth Gradient
Profile Prior (DGPP) loss with a dynamic depth mask to sharpen edges and 2D
diffusion with Score Distillation Sampling (SDS) loss to enhance geometric
consistency in novel view synthesis. Experimental evaluations on the
MipNeRF-360 and SeaThru-NeRF datasets demonstrate that SVS-GS markedly improves
3D reconstruction from sparse viewpoints, offering a robust and efficient
solution for scene understanding in robotics and computer vision applications.";Shen Chen<author:sep>Jiale Zhou<author:sep>Lei Li;http://arxiv.org/pdf/2409.03213v1;cs.CV;;gaussian splatting<tag:sep>nerf
2409.03424v1;http://arxiv.org/abs/2409.03424v1;2024-09-05;Weight Conditioning for Smooth Optimization of Neural Networks;"In this article, we introduce a novel normalization technique for neural
network weight matrices, which we term weight conditioning. This approach aims
to narrow the gap between the smallest and largest singular values of the
weight matrices, resulting in better-conditioned matrices. The inspiration for
this technique partially derives from numerical linear algebra, where
well-conditioned matrices are known to facilitate stronger convergence results
for iterative solvers. We provide a theoretical foundation demonstrating that
our normalization technique smoothens the loss landscape, thereby enhancing
convergence of stochastic gradient descent algorithms. Empirically, we validate
our normalization across various neural network architectures, including
Convolutional Neural Networks (CNNs), Vision Transformers (ViT), Neural
Radiance Fields (NeRF), and 3D shape modeling. Our findings indicate that our
normalization method is not only competitive but also outperforms existing
weight normalization techniques from the literature.";Hemanth Saratchandran<author:sep>Thomas X. Wang<author:sep>Simon Lucey;http://arxiv.org/pdf/2409.03424v1;cs.CV;ECCV 2024;nerf
2409.02581v1;http://arxiv.org/abs/2409.02581v1;2024-09-04;Object Gaussian for Monocular 6D Pose Estimation from Sparse Views;"Monocular object pose estimation, as a pivotal task in computer vision and
robotics, heavily depends on accurate 2D-3D correspondences, which often demand
costly CAD models that may not be readily available. Object 3D reconstruction
methods offer an alternative, among which recent advancements in 3D Gaussian
Splatting (3DGS) afford a compelling potential. Yet its performance still
suffers and tends to overfit with fewer input views. Embracing this challenge,
we introduce SGPose, a novel framework for sparse view object pose estimation
using Gaussian-based methods. Given as few as ten views, SGPose generates a
geometric-aware representation by starting with a random cuboid initialization,
eschewing reliance on Structure-from-Motion (SfM) pipeline-derived geometry as
required by traditional 3DGS methods. SGPose removes the dependence on CAD
models by regressing dense 2D-3D correspondences between images and the
reconstructed model from sparse input and random initialization, while the
geometric-consistent depth supervision and online synthetic view warping are
key to the success. Experiments on typical benchmarks, especially on the
Occlusion LM-O dataset, demonstrate that SGPose outperforms existing methods
even under sparse view constraints, under-scoring its potential in real-world
applications.";Luqing Luo<author:sep>Shichu Sun<author:sep>Jiangang Yang<author:sep>Linfang Zheng<author:sep>Jinwei Du<author:sep>Jian Liu;http://arxiv.org/pdf/2409.02581v1;cs.CV;;
2409.02851v1;http://arxiv.org/abs/2409.02851v1;2024-09-04;Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video  Diffusion Models;"Generating lifelike 3D humans from a single RGB image remains a challenging
task in computer vision, as it requires accurate modeling of geometry,
high-quality texture, and plausible unseen parts. Existing methods typically
use multi-view diffusion models for 3D generation, but they often face
inconsistent view issues, which hinder high-quality 3D human generation. To
address this, we propose Human-VDM, a novel method for generating 3D human from
a single RGB image using Video Diffusion Models. Human-VDM provides temporally
consistent views for 3D human generation using Gaussian Splatting. It consists
of three modules: a view-consistent human video diffusion module, a video
augmentation module, and a Gaussian Splatting module. First, a single image is
fed into a human video diffusion module to generate a coherent human video.
Next, the video augmentation module applies super-resolution and video
interpolation to enhance the textures and geometric smoothness of the generated
video. Finally, the 3D Human Gaussian Splatting module learns lifelike humans
under the guidance of these high-resolution and view-consistent images.
Experiments demonstrate that Human-VDM achieves high-quality 3D human from a
single image, outperforming state-of-the-art methods in both generation quality
and quantity. Project page: https://human-vdm.github.io/Human-VDM/";Zhibin Liu<author:sep>Haoye Dong<author:sep>Aviral Chharia<author:sep>Hefeng Wu;http://arxiv.org/pdf/2409.02851v1;cs.CV;"14 Pages, 8 figures, Project page:
  https://human-vdm.github.io/Human-VDM/";gaussian splatting
2409.02382v1;http://arxiv.org/abs/2409.02382v1;2024-09-04;GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous  Driving;"We propose GGS, a Generalizable Gaussian Splatting method for Autonomous
Driving which can achieve realistic rendering under large viewpoint changes.
Previous generalizable 3D gaussian splatting methods are limited to rendering
novel views that are very close to the original pair of images, which cannot
handle large differences in viewpoint. Especially in autonomous driving
scenarios, images are typically collected from a single lane. The limited
training perspective makes rendering images of a different lane very
challenging. To further improve the rendering capability of GGS under large
viewpoint changes, we introduces a novel virtual lane generation module into
GSS method to enables high-quality lane switching even without a multi-lane
dataset. Besides, we design a diffusion loss to supervise the generation of
virtual lane image to further address the problem of lack of data in the
virtual lanes. Finally, we also propose a depth refinement module to optimize
depth estimation in the GSS model. Extensive validation of our method, compared
to existing approaches, demonstrates state-of-the-art performance.";Huasong Han<author:sep>Kaixuan Zhou<author:sep>Xiaoxiao Long<author:sep>Yusen Wang<author:sep>Chunxia Xiao;http://arxiv.org/pdf/2409.02382v1;cs.CV;;gaussian splatting
2409.02917v1;http://arxiv.org/abs/2409.02917v1;2024-09-04;UC-NeRF: Uncertainty-aware Conditional Neural Radiance Fields from  Endoscopic Sparse Views;"Visualizing surgical scenes is crucial for revealing internal anatomical
structures during minimally invasive procedures. Novel View Synthesis is a
vital technique that offers geometry and appearance reconstruction, enhancing
understanding, planning, and decision-making in surgical scenes. Despite the
impressive achievements of Neural Radiance Field (NeRF), its direct application
to surgical scenes produces unsatisfying results due to two challenges:
endoscopic sparse views and significant photometric inconsistencies. In this
paper, we propose uncertainty-aware conditional NeRF for novel view synthesis
to tackle the severe shape-radiance ambiguity from sparse surgical views. The
core of UC-NeRF is to incorporate the multi-view uncertainty estimation to
condition the neural radiance field for modeling the severe photometric
inconsistencies adaptively. Specifically, our UC-NeRF first builds a
consistency learner in the form of multi-view stereo network, to establish the
geometric correspondence from sparse views and generate uncertainty estimation
and feature priors. In neural rendering, we design a base-adaptive NeRF network
to exploit the uncertainty estimation for explicitly handling the photometric
inconsistencies. Furthermore, an uncertainty-guided geometry distillation is
employed to enhance geometry learning. Experiments on the SCARED and Hamlyn
datasets demonstrate our superior performance in rendering appearance and
geometry, consistently outperforming the current state-of-the-art approaches.
Our code will be released at \url{https://github.com/wrld/UC-NeRF}.";Jiaxin Guo<author:sep>Jiangliu Wang<author:sep>Ruofeng Wei<author:sep>Di Kang<author:sep>Qi Dou<author:sep>Yun-hui Liu;http://arxiv.org/pdf/2409.02917v1;cs.CV;;nerf
2409.01661v1;http://arxiv.org/abs/2409.01661v1;2024-09-03;$S^2$NeRF: Privacy-preserving Training Framework for NeRF;"Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and
graphics, facilitating novel view synthesis and influencing sectors like
extended reality and e-commerce. However, NeRF's dependence on extensive data
collection, including sensitive scene image data, introduces significant
privacy risks when users upload this data for model training. To address this
concern, we first propose SplitNeRF, a training framework that incorporates
split learning (SL) techniques to enable privacy-preserving collaborative model
training between clients and servers without sharing local data. Despite its
benefits, we identify vulnerabilities in SplitNeRF by developing two attack
methods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which
exploit the shared gradient data and a few leaked scene images to reconstruct
private scene information. To counter these threats, we introduce $S^2$NeRF,
secure SplitNeRF that integrates effective defense mechanisms. By introducing
decaying noise related to the gradient norm into the shared gradient
information, $S^2$NeRF preserves privacy while maintaining a high utility of
the NeRF model. Our extensive evaluations across multiple datasets demonstrate
the effectiveness of $S^2$NeRF against privacy breaches, confirming its
viability for secure NeRF training in sensitive applications.";Bokang Zhang<author:sep>Yanglin Zhang<author:sep>Zhikun Zhang<author:sep>Jinglan Yang<author:sep>Lingying Huang<author:sep>Junfeng Wu;http://arxiv.org/pdf/2409.01661v1;cs.CR;"To appear in the ACM Conference on Computer and Communications
  Security (CCS'24), October 14-18, 2024, Salt Lake City, UT, USA";nerf
2409.02104v1;http://arxiv.org/abs/2409.02104v1;2024-09-03;DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian  Reconstruction;"Reconstructing scenes and tracking motion are two sides of the same coin.
Tracking points allow for geometric reconstruction [14], while geometric
reconstruction of (dynamic) scenes allows for 3D tracking of points over time
[24, 39]. The latter was recently also exploited for 2D point tracking to
overcome occlusion ambiguities by lifting tracking directly into 3D [38].
However, above approaches either require offline processing or multi-view
camera setups both unrealistic for real-world applications like robot
navigation or mixed reality. We target the challenge of online 2D and 3D point
tracking from unposed monocular camera input introducing Dynamic Online
Monocular Reconstruction (DynOMo). We leverage 3D Gaussian splatting to
reconstruct dynamic scenes in an online fashion. Our approach extends 3D
Gaussians to capture new content and object motions while estimating camera
movements from a single RGB frame. DynOMo stands out by enabling emergence of
point trajectories through robust image feature reconstruction and a novel
similarity-enhanced regularization term, without requiring any
correspondence-level supervision. It sets the first baseline for online point
tracking with monocular unposed cameras, achieving performance on par with
existing methods. We aim to inspire the community to advance online point
tracking and reconstruction, expanding the applicability to diverse real-world
scenarios.";Jenny Seidenschwarz<author:sep>Qunjie Zhou<author:sep>Bardienus Duisterhof<author:sep>Deva Ramanan<author:sep>Laura Leal-Taixé;http://arxiv.org/pdf/2409.02104v1;cs.CV;;gaussian splatting
2409.01761v1;http://arxiv.org/abs/2409.01761v1;2024-09-03;PRoGS: Progressive Rendering of Gaussian Splats;"Over the past year, 3D Gaussian Splatting (3DGS) has received significant
attention for its ability to represent 3D scenes in a perceptually accurate
manner. However, it can require a substantial amount of storage since each
splat's individual data must be stored. While compression techniques offer a
potential solution by reducing the memory footprint, they still necessitate
retrieving the entire scene before any part of it can be rendered. In this
work, we introduce a novel approach for progressively rendering such scenes,
aiming to display visible content that closely approximates the final scene as
early as possible without loading the entire scene into memory. This approach
benefits both on-device rendering applications limited by memory constraints
and streaming applications where minimal bandwidth usage is preferred. To
achieve this, we approximate the contribution of each Gaussian to the final
scene and construct an order of prioritization on their inclusion in the
rendering process. Additionally, we demonstrate that our approach can be
combined with existing compression methods to progressively render (and stream)
3DGS scenes, optimizing bandwidth usage by focusing on the most important
splats within a scene. Overall, our work establishes a foundation for making
remotely hosted 3DGS content more quickly accessible to end-users in
over-the-top consumption scenarios, with our results showing significant
improvements in quality across all metrics compared to existing methods.";Brent Zoomers<author:sep>Maarten Wijnants<author:sep>Ivan Molenaers<author:sep>Joni Vanherck<author:sep>Jeroen Put<author:sep>Lode Jorissen<author:sep>Nick Michiels;http://arxiv.org/pdf/2409.01761v1;cs.CV;;gaussian splatting
2409.01581v1;http://arxiv.org/abs/2409.01581v1;2024-09-03;GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color  Point Clouds via 3D Gaussian Splatting;"Dense colored point clouds enhance visual perception and are of significant
value in various robotic applications. However, existing learning-based point
cloud upsampling methods are constrained by computational resources and batch
processing strategies, which often require subdividing point clouds into
smaller patches, leading to distortions that degrade perceptual quality. To
address this challenge, we propose a novel 2D-3D hybrid colored point cloud
upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for
robotic perception. This approach leverages 3DGS to bridge 3D point clouds with
their 2D rendered images in robot vision systems. A dual scale rendered image
restoration network transforms sparse point cloud renderings into dense
representations, which are then input into 3DGS along with precise robot camera
poses and interpolated sparse point clouds to reconstruct dense 3D point
clouds. We have made a series of enhancements to the vanilla 3DGS, enabling
precise control over the number of points and significantly boosting the
quality of the upsampled point cloud for robotic scene understanding. Our
framework supports processing entire point clouds on a single consumer-grade
GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation
and thus producing high-quality, dense colored point clouds with millions of
points for robot navigation and manipulation tasks. Extensive experimental
results on generating million-level point cloud data validate the effectiveness
of our method, substantially improving the quality of colored point clouds and
demonstrating significant potential for applications involving large-scale
point clouds in autonomous robotics and human-robot interaction scenarios.";Zixuan Guo<author:sep>Yifan Xie<author:sep>Weijing Xie<author:sep>Peng Huang<author:sep>Fei Ma<author:sep>Fei Richard Yu;http://arxiv.org/pdf/2409.01581v1;cs.RO;7 pages, 5 figures;gaussian splatting
2409.02084v1;http://arxiv.org/abs/2409.02084v1;2024-09-03;GraspSplats: Efficient Manipulation with 3D Feature Splatting;"The ability for robots to perform efficient and zero-shot grasping of object
parts is crucial for practical applications and is becoming prevalent with
recent advances in Vision-Language Models (VLMs). To bridge the 2D-to-3D gap
for representations to support such a capability, existing methods rely on
neural fields (NeRFs) via differentiable rendering or point-based projection
methods. However, we demonstrate that NeRFs are inappropriate for scene changes
due to their implicitness and point-based methods are inaccurate for part
localization without rendering-based optimization. To amend these issues, we
propose GraspSplats. Using depth supervision and a novel reference feature
computation method, GraspSplats generates high-quality scene representations in
under 60 seconds. We further validate the advantages of Gaussian-based
representation by showing that the explicit and optimized geometry in
GraspSplats is sufficient to natively support (1) real-time grasp sampling and
(2) dynamic and articulated object manipulation with point trackers. With
extensive experiments on a Franka robot, we demonstrate that GraspSplats
significantly outperforms existing methods under diverse task settings. In
particular, GraspSplats outperforms NeRF-based methods like F3RM and LERF-TOGO,
and 2D detection methods.";Mazeyu Ji<author:sep>Ri-Zhao Qiu<author:sep>Xueyan Zou<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2409.02084v1;cs.RO;Project webpage: https://graspsplats.github.io/;nerf
2409.01003v1;http://arxiv.org/abs/2409.01003v1;2024-09-02;Free-DyGS: Camera-Pose-Free Scene Reconstruction based on Gaussian  Splatting for Dynamic Surgical Videos;"Reconstructing endoscopic videos is crucial for high-fidelity visualization
and the efficiency of surgical operations. Despite the importance, existing 3D
reconstruction methods encounter several challenges, including stringent
demands for accuracy, imprecise camera positioning, intricate dynamic scenes,
and the necessity for rapid reconstruction. Addressing these issues, this paper
presents the first camera-pose-free scene reconstruction framework, Free-DyGS,
tailored for dynamic surgical videos, leveraging 3D Gaussian splatting
technology. Our approach employs a frame-by-frame reconstruction strategy and
is delineated into four distinct phases: Scene Initialization, Joint Learning,
Scene Expansion, and Retrospective Learning. We introduce a Generalizable
Gaussians Parameterization module within the Scene Initialization and Expansion
phases to proficiently generate Gaussian attributes for each pixel from the
RGBD frames. The Joint Learning phase is crafted to concurrently deduce scene
deformation and camera pose, facilitated by an innovative flexible deformation
module. In the scene expansion stage, the Gaussian points gradually grow as the
camera moves. The Retrospective Learning phase is dedicated to enhancing the
precision of scene deformation through the reassessment of prior frames. The
efficacy of the proposed Free-DyGS is substantiated through experiments on two
datasets: the StereoMIS and Hamlyn datasets. The experimental outcomes
underscore that Free-DyGS surpasses conventional baseline models in both
rendering fidelity and computational efficiency.";Qian Li<author:sep>Shuojue Yang<author:sep>Daiyun Shen<author:sep>Yueming Jin;http://arxiv.org/pdf/2409.01003v1;cs.CV;;gaussian splatting
2409.00381v2;http://arxiv.org/abs/2409.00381v2;2024-08-31;3D Gaussian Splatting for Large-scale 3D Surface Reconstruction from  Aerial Images;"Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention.
However, the unstructured nature of 3DGS poses challenges for large-scale
surface reconstruction from aerial images. To address this gap, we propose the
first large-scale surface reconstruction method for multi-view stereo (MVS)
aerial images based on 3DGS, named Aerial Gaussian Splatting (AGS). Initially,
we introduce a data chunking method tailored for large-scale aerial imagery,
making the modern 3DGS technology feasible for surface reconstruction over
extensive scenes. Additionally, we integrate the Ray-Gaussian Intersection
method to obtain normal and depth information, facilitating geometric
constraints. Finally, we introduce a multi-view geometric consistency
constraint to enhance global geometric consistency and improve reconstruction
accuracy. Our experiments on multiple datasets demonstrate for the first time
that the GS-based technique can match traditional aerial MVS methods on
geometric accuracy, and beat state-of-the-art GS-based methods on geometry and
rendering quality.";YuanZheng Wu<author:sep>Jin Liu<author:sep>Shunping Ji;http://arxiv.org/pdf/2409.00381v2;cs.CV;"In the writing, some parts of the book were wrong and needed a large
  revision";gaussian splatting
2409.00362v1;http://arxiv.org/abs/2409.00362v1;2024-08-31;UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM;"Recent advancements in monocular neural depth estimation, particularly those
achieved by the UniDepth network, have prompted the investigation of
integrating UniDepth within a Gaussian splatting framework for monocular
SLAM.This study presents UDGS-SLAM, a novel approach that eliminates the
necessity of RGB-D sensors for depth estimation within Gaussian splatting
framework. UDGS-SLAM employs statistical filtering to ensure local consistency
of the estimated depth and jointly optimizes camera trajectory and Gaussian
scene representation parameters. The proposed method achieves high-fidelity
rendered images and low ATERMSE of the camera trajectory. The performance of
UDGS-SLAM is rigorously evaluated using the TUM RGB-D dataset and benchmarked
against several baseline methods, demonstrating superior performance across
various scenarios. Additionally, an ablation study is conducted to validate
design choices and investigate the impact of different network backbone
encoders on system performance.";Mostafa Mansour<author:sep>Ahmed Abdelsalam<author:sep>Ari Happonen<author:sep>Jari Porras<author:sep>Esa Rahtu;http://arxiv.org/pdf/2409.00362v1;cs.CV;;gaussian splatting
2408.16982v1;http://arxiv.org/abs/2408.16982v1;2024-08-30;2DGH: 2D Gaussian-Hermite Splatting for High-quality Rendering and  Better Geometry Reconstruction;"2D Gaussian Splatting has recently emerged as a significant method in 3D
reconstruction, enabling novel view synthesis and geometry reconstruction
simultaneously. While the well-known Gaussian kernel is broadly used, its lack
of anisotropy and deformation ability leads to dim and vague edges at object
silhouettes, limiting the reconstruction quality of current Gaussian splatting
methods. To enhance the representation power, we draw inspiration from quantum
physics and propose to use the Gaussian-Hermite kernel as the new primitive in
Gaussian splatting. The new kernel takes a unified mathematical form and
extends the Gaussian function, which serves as the zero-rank term in the
updated formulation. Our experiments demonstrate the extraordinary performance
of Gaussian-Hermite kernel in both geometry reconstruction and novel-view
synthesis tasks. The proposed kernel outperforms traditional Gaussian Splatting
kernels, showcasing its potential for high-quality 3D reconstruction and
rendering.";Ruihan Yu<author:sep>Tianyu Huang<author:sep>Jingwang Ling<author:sep>Feng Xu;http://arxiv.org/pdf/2408.16982v1;cs.CV;;gaussian splatting
2408.17223v1;http://arxiv.org/abs/2408.17223v1;2024-08-30;OG-Mapping: Octree-based Structured 3D Gaussians for Online Dense  Mapping;"3D Gaussian splatting (3DGS) has recently demonstrated promising advancements
in RGB-D online dense mapping. Nevertheless, existing methods excessively rely
on per-pixel depth cues to perform map densification, which leads to
significant redundancy and increased sensitivity to depth noise. Additionally,
explicitly storing 3D Gaussian parameters of room-scale scene poses a
significant storage challenge. In this paper, we introduce OG-Mapping, which
leverages the robust scene structural representation capability of sparse
octrees, combined with structured 3D Gaussian representations, to achieve
efficient and robust online dense mapping. Moreover, OG-Mapping employs an
anchor-based progressive map refinement strategy to recover the scene
structures at multiple levels of detail. Instead of maintaining a small number
of active keyframes with a fixed keyframe window as previous approaches do, a
dynamic keyframe window is employed to allow OG-Mapping to better tackle false
local minima and forgetting issues. Experimental results demonstrate that
OG-Mapping delivers more robust and superior realism mapping results than
existing Gaussian-based RGB-D online mapping methods with a compact model, and
no additional post-processing is required.";Meng Wang<author:sep>Junyi Wang<author:sep>Changqun Xia<author:sep>Chen Wang<author:sep>Yue Qi;http://arxiv.org/pdf/2408.17223v1;cs.CV;;gaussian splatting
2408.17027v1;http://arxiv.org/abs/2408.17027v1;2024-08-30;ConDense: Consistent 2D/3D Pre-training for Dense and Sparse Features  from Multi-View Images;"To advance the state of the art in the creation of 3D foundation models, this
paper introduces the ConDense framework for 3D pre-training utilizing existing
pre-trained 2D networks and large-scale multi-view datasets. We propose a novel
2D-3D joint training scheme to extract co-embedded 2D and 3D features in an
end-to-end pipeline, where 2D-3D feature consistency is enforced through a
volume rendering NeRF-like ray marching process. Using dense per pixel features
we are able to 1) directly distill the learned priors from 2D models to 3D
models and create useful 3D backbones, 2) extract more consistent and less
noisy 2D features, 3) formulate a consistent embedding space where 2D, 3D, and
other modalities of data (e.g., natural language prompts) can be jointly
queried. Furthermore, besides dense features, ConDense can be trained to
extract sparse features (e.g., key points), also with 2D-3D consistency --
condensing 3D NeRF representations into compact sets of decorated key points.
We demonstrate that our pre-trained model provides good initialization for
various 3D tasks including 3D classification and segmentation, outperforming
other 3D pre-training methods by a significant margin. It also enables, by
exploiting our sparse features, additional useful downstream tasks, such as
matching 2D images to 3D scenes, detecting duplicate 3D scenes, and querying a
repository of 3D scenes through natural language -- all quite efficiently and
without any per-scene fine-tuning.";Xiaoshuai Zhang<author:sep>Zhicheng Wang<author:sep>Howard Zhou<author:sep>Soham Ghosh<author:sep>Danushen Gnanapragasam<author:sep>Varun Jampani<author:sep>Hao Su<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2408.17027v1;cs.CV;ECCV 2024;nerf
2408.16866v1;http://arxiv.org/abs/2408.16866v1;2024-08-29;GameIR: A Large-Scale Synthesized Ground-Truth Dataset for Image  Restoration over Gaming Content;"Image restoration methods like super-resolution and image synthesis have been
successfully used in commercial cloud gaming products like NVIDIA's DLSS.
However, restoration over gaming content is not well studied by the general
public. The discrepancy is mainly caused by the lack of ground-truth gaming
training data that match the test cases. Due to the unique characteristics of
gaming content, the common approach of generating pseudo training data by
degrading the original HR images results in inferior restoration performance.
In this work, we develop GameIR, a large-scale high-quality
computer-synthesized ground-truth dataset to fill in the blanks, targeting at
two different applications. The first is super-resolution with deferred
rendering, to support the gaming solution of rendering and transferring LR
images only and restoring HR images on the client side. We provide 19200 LR-HR
paired ground-truth frames coming from 640 videos rendered at 720p and 1440p
for this task. The second is novel view synthesis (NVS), to support the
multiview gaming solution of rendering and transferring part of the multiview
frames and generating the remaining frames on the client side. This task has
57,600 HR frames from 960 videos of 160 scenes with 6 camera views. In addition
to the RGB frames, the GBuffers during the deferred rendering stage are also
provided, which can be used to help restoration. Furthermore, we evaluate
several SOTA super-resolution algorithms and NeRF-based NVS algorithms over our
dataset, which demonstrates the effectiveness of our ground-truth GameIR data
in improving restoration performance for gaming content. Also, we test the
method of incorporating the GBuffers as additional input information for
helping super-resolution and NVS. We release our dataset and models to the
general public to facilitate research on restoration methods over gaming
content.";Lebin Zhou<author:sep>Kun Han<author:sep>Nam Ling<author:sep>Wei Wang<author:sep>Wei Jiang;http://arxiv.org/pdf/2408.16866v1;cs.CV;;nerf
2408.16355v1;http://arxiv.org/abs/2408.16355v1;2024-08-29;NeRF-CA: Dynamic Reconstruction of X-ray Coronary Angiography with  Extremely Sparse-views;"Dynamic three-dimensional (4D) reconstruction from two-dimensional X-ray
coronary angiography (CA) remains a significant clinical problem. Challenges
include sparse-view settings, intra-scan motion, and complex vessel morphology
such as structure sparsity and background occlusion. Existing CA reconstruction
methods often require extensive user interaction or large training datasets. On
the other hand, Neural Radiance Field (NeRF), a promising deep learning
technique, has successfully reconstructed high-fidelity static scenes for
natural and medical scenes. Recent work, however, identified that sparse-views,
background occlusion, and dynamics still pose a challenge when applying NeRF in
the X-ray angiography context. Meanwhile, many successful works for natural
scenes propose regularization for sparse-view reconstruction or scene
decomposition to handle dynamics. However, these techniques do not directly
translate to the CA context, where both challenges and background occlusion are
significant. This paper introduces NeRF-CA, the first step toward a 4D CA
reconstruction method that achieves reconstructions from sparse coronary
angiograms with cardiac motion. We leverage the motion of the coronary artery
to decouple the scene into a dynamic coronary artery component and static
background. We combine this scene decomposition with tailored regularization
techniques. These techniques enforce the separation of the coronary artery from
the background by enforcing dynamic structure sparsity and scene smoothness. By
uniquely combining these approaches, we achieve 4D reconstructions from as few
as four angiogram sequences. This setting aligns with clinical workflows while
outperforming state-of-the-art X-ray sparse-view NeRF reconstruction
techniques. We validate our approach quantitatively and qualitatively using 4D
phantom datasets and ablation studies.";Kirsten W. H. Maas<author:sep>Danny Ruijters<author:sep>Anna Vilanova<author:sep>Nicola Pezzotti;http://arxiv.org/pdf/2408.16355v1;eess.IV;;nerf
2408.16544v1;http://arxiv.org/abs/2408.16544v1;2024-08-29;Spurfies: Sparse Surface Reconstruction using Local Geometry Priors;"We introduce Spurfies, a novel method for sparse-view surface reconstruction
that disentangles appearance and geometry information to utilize local geometry
priors trained on synthetic data. Recent research heavily focuses on 3D
reconstruction using dense multi-view setups, typically requiring hundreds of
images. However, these methods often struggle with few-view scenarios. Existing
sparse-view reconstruction techniques often rely on multi-view stereo networks
that need to learn joint priors for geometry and appearance from a large amount
of data. In contrast, we introduce a neural point representation that
disentangles geometry and appearance to train a local geometry prior using a
subset of the synthetic ShapeNet dataset only. During inference, we utilize
this surface prior as additional constraint for surface and appearance
reconstruction from sparse input views via differentiable volume rendering,
restricting the space of possible solutions. We validate the effectiveness of
our method on the DTU dataset and demonstrate that it outperforms previous
state of the art by 35% in surface quality while achieving competitive novel
view synthesis quality. Moreover, in contrast to previous works, our method can
be applied to larger, unbounded scenes, such as Mip-NeRF 360.";Kevin Raj<author:sep>Christopher Wewer<author:sep>Raza Yunus<author:sep>Eddy Ilg<author:sep>Jan Eric Lenssen;http://arxiv.org/pdf/2408.16544v1;cs.CV;https://geometric-rl.mpi-inf.mpg.de/spurfies/;nerf
2408.16760v1;http://arxiv.org/abs/2408.16760v1;2024-08-29;OmniRe: Omni Urban Scene Reconstruction;"We introduce OmniRe, a holistic approach for efficiently reconstructing
high-fidelity dynamic urban scenes from on-device logs. Recent methods for
modeling driving sequences using neural radiance fields or Gaussian Splatting
have demonstrated the potential of reconstructing challenging dynamic scenes,
but often overlook pedestrians and other non-vehicle dynamic actors, hindering
a complete pipeline for dynamic urban scene reconstruction. To that end, we
propose a comprehensive 3DGS framework for driving scenes, named OmniRe, that
allows for accurate, full-length reconstruction of diverse dynamic objects in a
driving log. OmniRe builds dynamic neural scene graphs based on Gaussian
representations and constructs multiple local canonical spaces that model
various dynamic actors, including vehicles, pedestrians, and cyclists, among
many others. This capability is unmatched by existing methods. OmniRe allows us
to holistically reconstruct different objects present in the scene,
subsequently enabling the simulation of reconstructed scenarios with all actors
participating in real-time (~60Hz). Extensive evaluations on the Waymo dataset
show that our approach outperforms prior state-of-the-art methods
quantitatively and qualitatively by a large margin. We believe our work fills a
critical gap in driving reconstruction.";Ziyu Chen<author:sep>Jiawei Yang<author:sep>Jiahui Huang<author:sep>Riccardo de Lutio<author:sep>Janick Martinez Esturo<author:sep>Boris Ivanovic<author:sep>Or Litany<author:sep>Zan Gojcic<author:sep>Sanja Fidler<author:sep>Marco Pavone<author:sep>Li Song<author:sep>Yue Wang;http://arxiv.org/pdf/2408.16760v1;cs.CV;"See the project page for code, video results and demos:
  https://ziyc.github.io/omnire/";gaussian splatting
2408.16690v2;http://arxiv.org/abs/2408.16690v2;2024-08-29;Generic Objects as Pose Probes for Few-Shot View Synthesis;"Radiance fields including NeRFs and 3D Gaussians demonstrate great potential
in high-fidelity rendering and scene reconstruction, while they require a
substantial number of posed images as inputs. COLMAP is frequently employed for
preprocessing to estimate poses, while it necessitates a large number of
feature matches to operate effectively, and it struggles with scenes
characterized by sparse features, large baselines between images, or a limited
number of input images. We aim to tackle few-view NeRF reconstruction using
only 3 to 6 unposed scene images. Traditional methods often use calibration
boards but they are not common in images. We propose a novel idea of utilizing
everyday objects, commonly found in both images and real life, as ""pose
probes"". The probe object is automatically segmented by SAM, whose shape is
initialized from a cube. We apply a dual-branch volume rendering optimization
(object NeRF and scene NeRF) to constrain the pose optimization and jointly
refine the geometry. Specifically, object poses of two views are first
estimated by PnP matching in an SDF representation, which serves as initial
poses. PnP matching, requiring only a few features, is suitable for
feature-sparse scenes. Additional views are incrementally incorporated to
refine poses from preceding views. In experiments, PoseProbe achieves
state-of-the-art performance in both pose estimation and novel view synthesis
across multiple datasets. We demonstrate its effectiveness, particularly in
few-view and large-baseline scenes where COLMAP struggles. In ablations, using
different objects in a scene yields comparable performance. Our project page is
available at: \href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this
https URL}";Zhirui Gao<author:sep>Renjiao Yi<author:sep>Chenyang Zhu<author:sep>Ke Zhuang<author:sep>Wei Chen<author:sep>Kai Xu;http://arxiv.org/pdf/2408.16690v2;cs.CV;;nerf
2408.16767v1;http://arxiv.org/abs/2408.16767v1;2024-08-29;ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion  Model;"Advancements in 3D scene reconstruction have transformed 2D images from the
real world into 3D models, producing realistic 3D results from hundreds of
input photos. Despite great success in dense-view reconstruction scenarios,
rendering a detailed scene from insufficient captured views is still an
ill-posed optimization problem, often resulting in artifacts and distortions in
unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction
paradigm that reframes the ambiguous reconstruction challenge as a temporal
generation task. The key insight is to unleash the strong generative prior of
large pre-trained video diffusion models for sparse-view reconstruction.
However, 3D view consistency struggles to be accurately preserved in directly
generated video frames from pre-trained models. To address this, given limited
input views, the proposed ReconX first constructs a global point cloud and
encodes it into a contextual space as the 3D structure condition. Guided by the
condition, the video diffusion model then synthesizes video frames that are
both detail-preserved and exhibit a high degree of 3D consistency, ensuring the
coherence of the scene from various perspectives. Finally, we recover the 3D
scene from the generated video through a confidence-aware 3D Gaussian Splatting
optimization scheme. Extensive experiments on various real-world datasets show
the superiority of our ReconX over state-of-the-art methods in terms of quality
and generalizability.";Fangfu Liu<author:sep>Wenqiang Sun<author:sep>Hanyang Wang<author:sep>Yikai Wang<author:sep>Haowen Sun<author:sep>Junliang Ye<author:sep>Jun Zhang<author:sep>Yueqi Duan;http://arxiv.org/pdf/2408.16767v1;cs.CV;Project page: https://liuff19.github.io/ReconX;gaussian splatting
2408.15695v2;http://arxiv.org/abs/2408.15695v2;2024-08-28;G-Style: Stylized Gaussian Splatting;"We introduce G-Style, a novel algorithm designed to transfer the style of an
image onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting
is a powerful 3D representation for novel view synthesis, as -- compared to
other approaches based on Neural Radiance Fields -- it provides fast scene
renderings and user control over the scene. Recent pre-prints have demonstrated
that the style of Gaussian Splatting scenes can be modified using an image
exemplar. However, since the scene geometry remains fixed during the
stylization process, current solutions fall short of producing satisfactory
results. Our algorithm aims to address these limitations by following a
three-step process: In a pre-processing step, we remove undesirable Gaussians
with large projection areas or highly elongated shapes. Subsequently, we
combine several losses carefully designed to preserve different scales of the
style in the image, while maintaining as much as possible the integrity of the
original scene content. During the stylization process and following the
original design of Gaussian Splatting, we split Gaussians where additional
detail is necessary within our scene by tracking the gradient of the stylized
color. Our experiments demonstrate that G-Style generates high-quality
stylizations within just a few minutes, outperforming existing methods both
qualitatively and quantitatively.";Áron Samuel Kovács<author:sep>Pedro Hermosilla<author:sep>Renata G. Raidou;http://arxiv.org/pdf/2408.15695v2;cs.GR;;gaussian splatting
2408.15708v1;http://arxiv.org/abs/2408.15708v1;2024-08-28;Towards Realistic Example-based Modeling via 3D Gaussian Stitching;"Using parts of existing models to rebuild new models, commonly termed as
example-based modeling, is a classical methodology in the realm of computer
graphics. Previous works mostly focus on shape composition, making them very
hard to use for realistic composition of 3D objects captured from real-world
scenes. This leads to combining multiple NeRFs into a single 3D scene to
achieve seamless appearance blending. However, the current SeamlessNeRF method
struggles to achieve interactive editing and harmonious stitching for
real-world scenes due to its gradient-based strategy and grid-based
representation. To this end, we present an example-based modeling method that
combines multiple Gaussian fields in a point-based representation using
sample-guided synthesis. Specifically, as for composition, we create a GUI to
segment and transform multiple fields in real time, easily obtaining a
semantically meaningful composition of models represented by 3D Gaussian
Splatting (3DGS). For texture blending, due to the discrete and irregular
nature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF
is not supported. Thus, a novel sampling-based cloning method is proposed to
harmonize the blending while preserving the original rich texture and content.
Our workflow consists of three steps: 1) real-time segmentation and
transformation of a Gaussian model using a well-tailored GUI, 2) KNN analysis
to identify boundary points in the intersecting area between the source and
target models, and 3) two-phase optimization of the target model using
sampling-based cloning and gradient constraints. Extensive experimental results
validate that our approach significantly outperforms previous works in terms of
realistic synthesis, demonstrating its practicality. More demos are available
at https://ingra14m.github.io/gs_stitching_website.";Xinyu Gao<author:sep>Ziyi Yang<author:sep>Bingchen Gong<author:sep>Xiaoguang Han<author:sep>Sipeng Yang<author:sep>Xiaogang Jin;http://arxiv.org/pdf/2408.15708v1;cs.CV;;nerf
2408.14873v1;http://arxiv.org/abs/2408.14873v1;2024-08-27;Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm  with Hybrid Representation;"Real2Sim2Real plays a critical role in robotic arm control and reinforcement
learning, yet bridging this gap remains a significant challenge due to the
complex physical properties of robots and the objects they manipulate. Existing
methods lack a comprehensive solution to accurately reconstruct real-world
objects with spatial representations and their associated physics attributes.
  We propose a Real2Sim pipeline with a hybrid representation model that
integrates mesh geometry, 3D Gaussian kernels, and physics attributes to
enhance the digital asset representation of robotic arms.
  This hybrid representation is implemented through a Gaussian-Mesh-Pixel
binding technique, which establishes an isomorphic mapping between mesh
vertices and Gaussian models. This enables a fully differentiable rendering
pipeline that can be optimized through numerical solvers, achieves
high-fidelity rendering via Gaussian Splatting, and facilitates physically
plausible simulation of the robotic arm's interaction with its environment
using mesh-based methods.
  The code,full presentation and datasets will be made publicly available at
our website https://robostudioapp.com";Haozhe Lou<author:sep>Yurong Liu<author:sep>Yike Pan<author:sep>Yiran Geng<author:sep>Jianteng Chen<author:sep>Wenlong Ma<author:sep>Chenglong Li<author:sep>Lin Wang<author:sep>Hengzhen Feng<author:sep>Lu Shi<author:sep>Liyi Luo<author:sep>Yongliang Shi;http://arxiv.org/pdf/2408.14873v1;cs.RO;;gaussian splatting
2408.14724v1;http://arxiv.org/abs/2408.14724v1;2024-08-27;GeoTransfer : Generalizable Few-Shot Multi-View Reconstruction via  Transfer Learning;"This paper presents a novel approach for sparse 3D reconstruction by
leveraging the expressive power of Neural Radiance Fields (NeRFs) and fast
transfer of their features to learn accurate occupancy fields. Existing 3D
reconstruction methods from sparse inputs still struggle with capturing
intricate geometric details and can suffer from limitations in handling
occluded regions. On the other hand, NeRFs excel in modeling complex scenes but
do not offer means to extract meaningful geometry. Our proposed method offers
the best of both worlds by transferring the information encoded in NeRF
features to derive an accurate occupancy field representation. We utilize a
pre-trained, generalizable state-of-the-art NeRF network to capture detailed
scene radiance information, and rapidly transfer this knowledge to train a
generalizable implicit occupancy network. This process helps in leveraging the
knowledge of the scene geometry encoded in the generalizable NeRF prior and
refining it to learn occupancy fields, facilitating a more precise
generalizable representation of 3D space. The transfer learning approach leads
to a dramatic reduction in training time, by orders of magnitude (i.e. from
several days to 3.5 hrs), obviating the need to train generalizable sparse
surface reconstruction methods from scratch. Additionally, we introduce a novel
loss on volumetric rendering weights that helps in the learning of accurate
occupancy fields, along with a normal loss that helps in global smoothing of
the occupancy fields. We evaluate our approach on the DTU dataset and
demonstrate state-of-the-art performance in terms of reconstruction accuracy,
especially in challenging scenarios with sparse input data and occluded
regions. We furthermore demonstrate the generalization capabilities of our
method by showing qualitative results on the Blended MVS dataset without any
retraining.";Shubhendu Jena<author:sep>Franck Multon<author:sep>Adnane Boukhayma;http://arxiv.org/pdf/2408.14724v1;cs.CV;;nerf
2408.15242v1;http://arxiv.org/abs/2408.15242v1;2024-08-27;Drone-assisted Road Gaussian Splatting with Cross-view Uncertainty;"Robust and realistic rendering for large-scale road scenes is essential in
autonomous driving simulation. Recently, 3D Gaussian Splatting (3D-GS) has made
groundbreaking progress in neural rendering, but the general fidelity of
large-scale road scene renderings is often limited by the input imagery, which
usually has a narrow field of view and focuses mainly on the street-level local
area. Intuitively, the data from the drone's perspective can provide a
complementary viewpoint for the data from the ground vehicle's perspective,
enhancing the completeness of scene reconstruction and rendering. However,
training naively with aerial and ground images, which exhibit large view
disparity, poses a significant convergence challenge for 3D-GS, and does not
demonstrate remarkable improvements in performance on road views. In order to
enhance the novel view synthesis of road views and to effectively use the
aerial information, we design an uncertainty-aware training method that allows
aerial images to assist in the synthesis of areas where ground images have poor
learning outcomes instead of weighting all pixels equally in 3D-GS training
like prior work did. We are the first to introduce the cross-view uncertainty
to 3D-GS by matching the car-view ensemble-based rendering uncertainty to
aerial images, weighting the contribution of each pixel to the training
process. Additionally, to systematically quantify evaluation metrics, we
assemble a high-quality synthesized dataset comprising both aerial and ground
images for road scenes.";Saining Zhang<author:sep>Baijun Ye<author:sep>Xiaoxue Chen<author:sep>Yuantao Chen<author:sep>Zongzheng Zhang<author:sep>Cheng Peng<author:sep>Yongliang Shi<author:sep>Hao Zhao;http://arxiv.org/pdf/2408.15242v1;cs.CV;"BMVC2024 Project Page: https://sainingzhang.github.io/project/uc-gs/
  Code: https://github.com/SainingZhang/uc-gs/";gaussian splatting
2408.14823v1;http://arxiv.org/abs/2408.14823v1;2024-08-27;LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive  Streaming;"The rise of Extended Reality (XR) requires efficient streaming of 3D online
worlds, challenging current 3DGS representations to adapt to
bandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS
that supports adaptive streaming and progressive rendering. Our method
constructs a layered structure for cumulative representation, incorporates
dynamic opacity optimization to maintain visual fidelity, and utilizes
occupancy maps to efficiently manage Gaussian splats. This proposed model
offers a progressive representation supporting a continuous rendering quality
adapted for bandwidth-aware streaming. Extensive experiments validate the
effectiveness of our approach in balancing visual fidelity with the compactness
of the model, with up to 50.71% improvement in SSIM, 286.53% improvement in
LPIPS, and 318.41% reduction in model size, and shows its potential for
bandwidth-adapted 3D streaming and rendering applications.";Yuang Shi<author:sep>Simone Gasparini<author:sep>Géraldine Morin<author:sep>Wei Tsang Ooi;http://arxiv.org/pdf/2408.14823v1;cs.CV;;gaussian splatting
2408.15235v1;http://arxiv.org/abs/2408.15235v1;2024-08-27;Learning-based Multi-View Stereo: A Survey;"3D reconstruction aims to recover the dense 3D structure of a scene. It plays
an essential role in various applications such as Augmented/Virtual Reality
(AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene
captured from different viewpoints, Multi-View Stereo (MVS) algorithms
synthesize a comprehensive 3D representation, enabling precise reconstruction
in complex environments. Due to its efficiency and effectiveness, MVS has
become a pivotal method for image-based 3D reconstruction. Recently, with the
success of deep learning, many learning-based MVS methods have been proposed,
achieving impressive performance against traditional methods. We categorize
these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D
Gaussian Splatting-based, and large feed-forward methods. Among these, we focus
significantly on depth map-based methods, which are the main family of MVS due
to their conciseness, flexibility and scalability. In this survey, we provide a
comprehensive review of the literature at the time of this writing. We
investigate these learning-based methods, summarize their performances on
popular benchmarks, and discuss promising future research directions in this
area.";Fangjinhua Wang<author:sep>Qingtian Zhu<author:sep>Di Chang<author:sep>Quankai Gao<author:sep>Junlin Han<author:sep>Tong Zhang<author:sep>Richard Hartley<author:sep>Marc Pollefeys;http://arxiv.org/pdf/2408.15235v1;cs.CV;;gaussian splatting<tag:sep>nerf
2408.14035v2;http://arxiv.org/abs/2408.14035v2;2024-08-26;FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry;"This paper proposes FAST-LIVO2: a fast, direct LiDAR-inertial-visual odometry
framework to achieve accurate and robust state estimation in SLAM tasks and
provide great potential in real-time, onboard robotic applications. FAST-LIVO2
fuses the IMU, LiDAR and image measurements efficiently through an ESIKF. To
address the dimension mismatch between the heterogeneous LiDAR and image
measurements, we use a sequential update strategy in the Kalman filter. To
enhance the efficiency, we use direct methods for both the visual and LiDAR
fusion, where the LiDAR module registers raw points without extracting edge or
plane features and the visual module minimizes direct photometric errors
without extracting ORB or FAST corner features. The fusion of both visual and
LiDAR measurements is based on a single unified voxel map where the LiDAR
module constructs the geometric structure for registering new LiDAR scans and
the visual module attaches image patches to the LiDAR points. To enhance the
accuracy of image alignment, we use plane priors from the LiDAR points in the
voxel map (and even refine the plane prior) and update the reference patch
dynamically after new images are aligned. Furthermore, to enhance the
robustness of image alignment, FAST-LIVO2 employs an on-demanding raycast
operation and estimates the image exposure time in real time. Lastly, we detail
three applications of FAST-LIVO2: UAV onboard navigation demonstrating the
system's computation efficiency for real-time onboard navigation, airborne
mapping showcasing the system's mapping accuracy, and 3D model rendering
(mesh-based and NeRF-based) underscoring the suitability of our reconstructed
dense map for subsequent rendering tasks. We open source our code, dataset and
application on GitHub to benefit the robotics community.";Chunran Zheng<author:sep>Wei Xu<author:sep>Zuhao Zou<author:sep>Tong Hua<author:sep>Chongjian Yuan<author:sep>Dongjiao He<author:sep>Bingyang Zhou<author:sep>Zheng Liu<author:sep>Jiarong Lin<author:sep>Fangcheng Zhu<author:sep>Yunfan Ren<author:sep>Rong Wang<author:sep>Fanle Meng<author:sep>Fu Zhang;http://arxiv.org/pdf/2408.14035v2;cs.RO;"30 pages, 31 figures, due to the limitation that 'The abstract field
  cannot exceed 1,920 characters', the abstract presented here is shorter than
  the one in the PDF file";nerf
2408.13972v1;http://arxiv.org/abs/2408.13972v1;2024-08-26;DynaSurfGS: Dynamic Surface Reconstruction with Planar-based Gaussian  Splatting;"Dynamic scene reconstruction has garnered significant attention in recent
years due to its capabilities in high-quality and real-time rendering. Among
various methodologies, constructing a 4D spatial-temporal representation, such
as 4D-GS, has gained popularity for its high-quality rendered images. However,
these methods often produce suboptimal surfaces, as the discrete 3D Gaussian
point clouds fail to align with the object's surface precisely. To address this
problem, we propose DynaSurfGS to achieve both photorealistic rendering and
high-fidelity surface reconstruction of dynamic scenarios. Specifically, the
DynaSurfGS framework first incorporates Gaussian features from 4D neural voxels
with the planar-based Gaussian Splatting to facilitate precise surface
reconstruction. It leverages normal regularization to enforce the smoothness of
the surface of dynamic objects. It also incorporates the as-rigid-as-possible
(ARAP) constraint to maintain the approximate rigidity of local neighborhoods
of 3D Gaussians between timesteps and ensure that adjacent 3D Gaussians remain
closely aligned throughout. Extensive experiments demonstrate that DynaSurfGS
surpasses state-of-the-art methods in both high-fidelity surface reconstruction
and photorealistic rendering.";Weiwei Cai<author:sep>Weicai Ye<author:sep>Peng Ye<author:sep>Tong He<author:sep>Tao Chen;http://arxiv.org/pdf/2408.13972v1;cs.CV;"homepage: https://open3dvlab.github.io/DynaSurfGS/, code:
  https://github.com/Open3DVLab/DynaSurfGS";gaussian splatting
2408.13995v1;http://arxiv.org/abs/2408.13995v1;2024-08-26;Avatar Concept Slider: Manipulate Concepts In Your Human Avatar With  Fine-grained Control;"Language based editing of 3D human avatars to precisely match user
requirements is challenging due to the inherent ambiguity and limited
expressiveness of natural language. To overcome this, we propose the Avatar
Concept Slider (ACS), a 3D avatar editing method that allows precise
manipulation of semantic concepts in human avatars towards a specified
intermediate point between two extremes of concepts, akin to moving a knob
along a slider track. To achieve this, our ACS has three designs. 1) A Concept
Sliding Loss based on Linear Discriminant Analysis to pinpoint the
concept-specific axis for precise editing. 2) An Attribute Preserving Loss
based on Principal Component Analysis for improved preservation of avatar
identity during editing. 3) A 3D Gaussian Splatting primitive selection
mechanism based on concept-sensitivity, which updates only the primitives that
are the most sensitive to our target concept, to improve efficiency. Results
demonstrate that our ACS enables fine-grained 3D avatar editing with efficient
feedback, without harming the avatar quality or compromising the avatar's
identifying attributes.";Yixuan He<author:sep>Lin Geng Foo<author:sep>Ajmal Saeed Mian<author:sep>Hossein Rahmani<author:sep>Jun Jiu;http://arxiv.org/pdf/2408.13995v1;cs.CV;;gaussian splatting
2408.13770v1;http://arxiv.org/abs/2408.13770v1;2024-08-25;TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View  Images with Transformers;"Compared with previous 3D reconstruction methods like Nerf, recent
Generalizable 3D Gaussian Splatting (G-3DGS) methods demonstrate impressive
efficiency even in the sparse-view setting. However, the promising
reconstruction performance of existing G-3DGS methods relies heavily on
accurate multi-view feature matching, which is quite challenging. Especially
for the scenes that have many non-overlapping areas between various views and
contain numerous similar regions, the matching performance of existing methods
is poor and the reconstruction precision is limited. To address this problem,
we develop a strategy that utilizes a predicted depth confidence map to guide
accurate local feature matching. In addition, we propose to utilize the
knowledge of existing monocular depth estimation models as prior to boost the
depth estimation precision in non-overlapping areas between views. Combining
the proposed strategies, we present a novel G-3DGS method named TranSplat,
which obtains the best performance on both the RealEstate10K and ACID
benchmarks while maintaining competitive speed and presenting strong
cross-dataset generalization ability. Our code, and demos will be available at:
https://xingyoujun.github.io/transplat.";Chuanrui Zhang<author:sep>Yingshuang Zou<author:sep>Zhuoling Li<author:sep>Minmin Yi<author:sep>Haoqian Wang;http://arxiv.org/pdf/2408.13770v1;cs.CV;;gaussian splatting<tag:sep>nerf
2408.13711v1;http://arxiv.org/abs/2408.13711v1;2024-08-25;SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with  Panoramic Gaussian Splatting;"Text-driven 3D scene generation has seen significant advancements recently.
However, most existing methods generate single-view images using generative
models and then stitch them together in 3D space. This independent generation
for each view often results in spatial inconsistency and implausibility in the
3D scenes. To address this challenge, we proposed a novel text-driven
3D-consistent scene generation model: SceneDreamer360. Our proposed method
leverages a text-driven panoramic image generation model as a prior for 3D
scene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency
across multi-view panoramic images. Specifically, SceneDreamer360 enhances the
fine-tuned Panfusion generator with a three-stage panoramic enhancement,
enabling the generation of high-resolution, detail-rich panoramic images.
During the 3D scene construction, a novel point cloud fusion initialization
method is used, producing higher quality and spatially consistent point clouds.
Our extensive experiments demonstrate that compared to other methods,
SceneDreamer360 with its panoramic image generation and 3DGS can produce higher
quality, spatially consistent, and visually appealing 3D scenes from any text
prompt. Our codes are available at
\url{https://github.com/liwrui/SceneDreamer360}.";Wenrui Li<author:sep>Yapeng Mi<author:sep>Fucheng Cai<author:sep>Zhe Yang<author:sep>Wangmeng Zuo<author:sep>Xingtao Wang<author:sep>Xiaopeng Fan;http://arxiv.org/pdf/2408.13711v1;cs.CV;;gaussian splatting
2408.13912v2;http://arxiv.org/abs/2408.13912v2;2024-08-25;Splatt3R: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs;"In this paper, we introduce Splatt3R, a pose-free, feed-forward method for
in-the-wild 3D reconstruction and novel view synthesis from stereo pairs. Given
uncalibrated natural images, Splatt3R can predict 3D Gaussian Splats without
requiring any camera parameters or depth information. For generalizability, we
build Splatt3R upon a ``foundation'' 3D geometry reconstruction method, MASt3R,
by extending it to deal with both 3D structure and appearance. Specifically,
unlike the original MASt3R which reconstructs only 3D point clouds, we predict
the additional Gaussian attributes required to construct a Gaussian primitive
for each point. Hence, unlike other novel view synthesis methods, Splatt3R is
first trained by optimizing the 3D point cloud's geometry loss, and then a
novel view synthesis objective. By doing this, we avoid the local minima
present in training 3D Gaussian Splats from stereo views. We also propose a
novel loss masking strategy that we empirically find is critical for strong
performance on extrapolated viewpoints. We train Splatt3R on the ScanNet++
dataset and demonstrate excellent generalisation to uncalibrated, in-the-wild
images. Splatt3R can reconstruct scenes at 4FPS at 512 x 512 resolution, and
the resultant splats can be rendered in real-time.";Brandon Smart<author:sep>Chuanxia Zheng<author:sep>Iro Laina<author:sep>Victor Adrian Prisacariu;http://arxiv.org/pdf/2408.13912v2;cs.CV;Our project page can be found at: https://splatt3r.active.vision/;gaussian splatting
2408.13508v1;http://arxiv.org/abs/2408.13508v1;2024-08-24;G3DST: Generalizing 3D Style Transfer with Neural Radiance Fields across  Scenes and Styles;"Neural Radiance Fields (NeRF) have emerged as a powerful tool for creating
highly detailed and photorealistic scenes. Existing methods for NeRF-based 3D
style transfer need extensive per-scene optimization for single or multiple
styles, limiting the applicability and efficiency of 3D style transfer. In this
work, we overcome the limitations of existing methods by rendering stylized
novel views from a NeRF without the need for per-scene or per-style
optimization. To this end, we take advantage of a generalizable NeRF model to
facilitate style transfer in 3D, thereby enabling the use of a single learned
model across various scenes. By incorporating a hypernetwork into a
generalizable NeRF, our approach enables on-the-fly generation of stylized
novel views. Moreover, we introduce a novel flow-based multi-view consistency
loss to preserve consistency across multiple views. We evaluate our method
across various scenes and artistic styles and show its performance in
generating high-quality and multi-view consistent stylized images without the
need for a scene-specific implicit model. Our findings demonstrate that this
approach not only achieves a good visual quality comparable to that of
per-scene methods but also significantly enhances efficiency and applicability,
marking a notable advancement in the field of 3D style transfer.";Adil Meric<author:sep>Umut Kocasari<author:sep>Matthias Nießner<author:sep>Barbara Roessle;http://arxiv.org/pdf/2408.13508v1;cs.CV;GCPR 2024, Project page: https://mericadil.github.io/G3DST/;nerf
2408.13285v1;http://arxiv.org/abs/2408.13285v1;2024-08-23;SIn-NeRF2NeRF: Editing 3D Scenes with Instructions through Segmentation  and Inpainting;"TL;DR Perform 3D object editing selectively by disentangling it from the
background scene. Instruct-NeRF2NeRF (in2n) is a promising method that enables
editing of 3D scenes composed of Neural Radiance Field (NeRF) using text
prompts. However, it is challenging to perform geometrical modifications such
as shrinking, scaling, or moving on both the background and object
simultaneously. In this project, we enable geometrical changes of objects
within the 3D scene by selectively editing the object after separating it from
the scene. We perform object segmentation and background inpainting
respectively, and demonstrate various examples of freely resizing or moving
disentangled objects within the three-dimensional space.";Jiseung Hong<author:sep>Changmin Lee<author:sep>Gyusang Yu;http://arxiv.org/pdf/2408.13285v1;cs.CV;Code is available at: https://github.com/KAISTChangmin/SIn-NeRF2NeRF;nerf
2408.13370v1;http://arxiv.org/abs/2408.13370v1;2024-08-23;BiGS: Bidirectional Gaussian Primitives for Relightable 3D Gaussian  Splatting;"We present Bidirectional Gaussian Primitives, an image-based novel view
synthesis technique designed to represent and render 3D objects with surface
and volumetric materials under dynamic illumination. Our approach integrates
light intrinsic decomposition into the Gaussian splatting framework, enabling
real-time relighting of 3D objects. To unify surface and volumetric material
within a cohesive appearance model, we adopt a light- and view-dependent
scattering representation via bidirectional spherical harmonics. Our model does
not use a specific surface normal-related reflectance function, making it more
compatible with volumetric representations like Gaussian splatting, where the
normals are undefined. We demonstrate our method by reconstructing and
rendering objects with complex materials. Using One-Light-At-a-Time (OLAT) data
as input, we can reproduce photorealistic appearances under novel lighting
conditions in real time.";Zhenyuan Liu<author:sep>Yu Guo<author:sep>Xinyuan Li<author:sep>Bernd Bickel<author:sep>Ran Zhang;http://arxiv.org/pdf/2408.13370v1;cs.CV;;gaussian splatting
2408.12894v1;http://arxiv.org/abs/2408.12894v1;2024-08-23;FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting  for Customizable Rendering;"3D Gaussian Splatting (3DGS) achieves fast and high-quality renderings by
using numerous small Gaussians, which leads to significant memory consumption.
This reliance on a large number of Gaussians restricts the application of
3DGS-based models on low-cost devices due to memory limitations. However,
simply reducing the number of Gaussians to accommodate devices with less memory
capacity leads to inferior quality compared to the quality that can be achieved
on high-end hardware. To address this lack of scalability, we propose
integrating a Flexible Level of Detail (FLoD) to 3DGS, to allow a scene to be
rendered at varying levels of detail according to hardware capabilities. While
existing 3DGSs with LoD focus on detailed reconstruction, our method provides
reconstructions using a small number of Gaussians for reduced memory
requirements, and a larger number of Gaussians for greater detail. Experiments
demonstrate our various rendering options with tradeoffs between rendering
quality and memory usage, thereby allowing real-time rendering across different
memory constraints. Furthermore, we show that our method generalizes to
different 3DGS frameworks, indicating its potential for integration into future
state-of-the-art developments. Project page:
https://3dgs-flod.github.io/flod.github.io/";Yunji Seo<author:sep>Young Sun Choi<author:sep>Hyun Seung Son<author:sep>Youngjung Uh;http://arxiv.org/pdf/2408.12894v1;cs.CV;Project page: https://3dgs-flod.github.io/flod.github.io/;gaussian splatting
2408.13036v1;http://arxiv.org/abs/2408.13036v1;2024-08-23;S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D  Control Points;"Recently, the dynamic scene reconstruction using Gaussians has garnered
increased interest. Mainstream approaches typically employ a global deformation
field to warp a 3D scene in the canonical space. However, the inherently
low-frequency nature of implicit neural fields often leads to ineffective
representations of complex motions. Moreover, their structural rigidity can
hinder adaptation to scenes with varying resolutions and durations. To overcome
these challenges, we introduce a novel approach utilizing discrete 3D control
points. This method models local rays physically and establishes a
motion-decoupling coordinate system, which effectively merges traditional
graphics with learnable pipelines for a robust and efficient local
6-degrees-of-freedom (6-DoF) motion representation. Additionally, we have
developed a generalized framework that incorporates our control points with
Gaussians. Starting from an initial 3D reconstruction, our workflow decomposes
the streaming 4D real-world reconstruction into four independent submodules: 3D
segmentation, 3D control points generation, object-wise motion manipulation,
and residual compensation. Our experiments demonstrate that this method
outperforms existing state-of-the-art 4D Gaussian Splatting techniques on both
the Neu3DV and CMU-Panoptic datasets. Our approach also significantly
accelerates training, with the optimization of our 3D control points achievable
within just 2 seconds per frame on a single NVIDIA 4070 GPU.";Bing He<author:sep>Yunuo Chen<author:sep>Guo Lu<author:sep>Li Song<author:sep>Wenjun Zhang;http://arxiv.org/pdf/2408.13036v1;cs.CV;;gaussian splatting
2408.12677v2;http://arxiv.org/abs/2408.12677v2;2024-08-22;GSFusion: Online RGB-D Mapping Where Gaussian Splatting Meets TSDF  Fusion;"Traditional volumetric fusion algorithms preserve the spatial structure of 3D
scenes, which is beneficial for many tasks in computer vision and robotics.
However, they often lack realism in terms of visualization. Emerging 3D
Gaussian splatting bridges this gap, but existing Gaussian-based reconstruction
methods often suffer from artifacts and inconsistencies with the underlying 3D
structure, and struggle with real-time optimization, unable to provide users
with immediate feedback in high quality. One of the bottlenecks arises from the
massive amount of Gaussian parameters that need to be updated during
optimization. Instead of using 3D Gaussian as a standalone map representation,
we incorporate it into a volumetric mapping system to take advantage of
geometric information and propose to use a quadtree data structure on images to
drastically reduce the number of splats initialized. In this way, we
simultaneously generate a compact 3D Gaussian map with fewer artifacts and a
volumetric map on the fly. Our method, GSFusion, significantly enhances
computational efficiency without sacrificing rendering quality, as demonstrated
on both synthetic and real datasets. Code will be available at
https://github.com/goldoak/GSFusion.";Jiaxin Wei<author:sep>Stefan Leutenegger;http://arxiv.org/pdf/2408.12677v2;cs.CV;;gaussian splatting
2408.12282v1;http://arxiv.org/abs/2408.12282v1;2024-08-22;Subsurface Scattering for 3D Gaussian Splatting;"3D reconstruction and relighting of objects made from scattering materials
present a significant challenge due to the complex light transport beneath the
surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at
real-time speeds. While 3D Gaussians efficiently approximate an object's
surface, they fail to capture the volumetric properties of subsurface
scattering. We propose a framework for optimizing an object's shape together
with the radiance transfer field given multi-view OLAT (one light at a time)
data. Our method decomposes the scene into an explicit surface represented as
3D Gaussians, with a spatially varying BRDF, and an implicit volumetric
representation of the scattering component. A learned incident light field
accounts for shadowing. We optimize all parameters jointly via ray-traced
differentiable rendering. Our approach enables material editing, relighting and
novel view synthesis at interactive rates. We show successful application on
synthetic data and introduce a newly acquired multi-view multi-light dataset of
objects in a light-stage setup. Compared to previous work we achieve comparable
or better results at a fraction of optimization and rendering time while
enabling detailed control over material attributes. Project page
https://sss.jdihlmann.com/";Jan-Niklas Dihlmann<author:sep>Arjun Majumdar<author:sep>Andreas Engelhardt<author:sep>Raphael Braun<author:sep>Hendrik P. A. Lensch;http://arxiv.org/pdf/2408.12282v1;cs.CV;Project page: https://sss.jdihlmann.com/;gaussian splatting
2408.11540v2;http://arxiv.org/abs/2408.11540v2;2024-08-21;DeRainGS: Gaussian Splatting for Enhanced Scene Reconstruction in Rainy  Environments;"Reconstruction under adverse rainy conditions poses significant challenges
due to reduced visibility and the distortion of visual perception. These
conditions can severely impair the quality of geometric maps, which is
essential for applications ranging from autonomous planning to environmental
monitoring. In response to these challenges, this study introduces the novel
task of 3D Reconstruction in Rainy Environments (3DRRE), specifically designed
to address the complexities of reconstructing 3D scenes under rainy conditions.
To benchmark this task, we construct the HydroViews dataset that comprises a
diverse collection of both synthesized and real-world scene images
characterized by various intensities of rain streaks and raindrops.
Furthermore, we propose DeRainGS, the first 3DGS method tailored for
reconstruction in adverse rainy environments. Extensive experiments across a
wide range of rain scenarios demonstrate that our method delivers
state-of-the-art performance, remarkably outperforming existing occlusion-free
methods.";Shuhong Liu<author:sep>Xiang Chen<author:sep>Hongming Chen<author:sep>Quanfeng Xu<author:sep>Mingrui Li;http://arxiv.org/pdf/2408.11540v2;cs.CV;;gaussian splatting
2408.11966v1;http://arxiv.org/abs/2408.11966v1;2024-08-21;Visual Localization in 3D Maps: Comparing Point Cloud, Mesh, and NeRF  Representations;"This paper introduces and assesses a cross-modal global visual localization
system that can localize camera images within a color 3D map representation
built using both visual and lidar sensing. We present three different
state-of-the-art methods for creating the color 3D maps: point clouds, meshes,
and neural radiance fields (NeRF). Our system constructs a database of
synthetic RGB and depth image pairs from these representations. This database
serves as the basis for global localization. We present an automatic approach
that builds this database by synthesizing novel images of the scene and
exploiting the 3D structure encoded in the different representations. Next, we
present a global localization system that relies on the synthetic image
database to accurately estimate the 6 DoF camera poses of monocular query
images. Our localization approach relies on different learning-based global
descriptors and feature detectors which enable robust image retrieval and
matching despite the domain gap between (real) query camera images and the
synthetic database images. We assess the system's performance through extensive
real-world experiments in both indoor and outdoor settings, in order to
evaluate the effectiveness of each map representation and the benefits against
traditional structure-from-motion localization approaches. Our results show
that all three map representations can achieve consistent localization success
rates of 55% and higher across various environments. NeRF synthesized images
show superior performance, localizing query images at an average success rate
of 72%. Furthermore, we demonstrate that our synthesized database enables
global localization even when the map creation data and the localization
sequence are captured when travelling in opposite directions. Our system,
operating in real-time on a mobile laptop equipped with a GPU, achieves a
processing rate of 1Hz.";Lintong Zhang<author:sep>Yifu Tao<author:sep>Jiarong Lin<author:sep>Fu Zhang<author:sep>Maurice Fallon;http://arxiv.org/pdf/2408.11966v1;cs.CV;;nerf
2408.11447v1;http://arxiv.org/abs/2408.11447v1;2024-08-21;GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation  with Gaussian Splatting;"We introduce GaussianOcc, a systematic method that investigates the two
usages of Gaussian splatting for fully self-supervised and efficient 3D
occupancy estimation in surround views. First, traditional methods for
self-supervised 3D occupancy estimation still require ground truth 6D poses
from sensors during training. To address this limitation, we propose Gaussian
Splatting for Projection (GSP) module to provide accurate scale information for
fully self-supervised training from adjacent view projection. Additionally,
existing methods rely on volume rendering for final 3D voxel representation
learning using 2D signals (depth maps, semantic maps), which is both
time-consuming and less effective. We propose Gaussian Splatting from Voxel
space (GSV) to leverage the fast rendering properties of Gaussian splatting. As
a result, the proposed GaussianOcc method enables fully self-supervised (no
ground truth pose) 3D occupancy estimation in competitive performance with low
computational cost (2.7 times faster in training and 5 times faster in
rendering).";Wanshui Gan<author:sep>Fang Liu<author:sep>Hongbin Xu<author:sep>Ningkai Mo<author:sep>Naoto Yokoya;http://arxiv.org/pdf/2408.11447v1;cs.CV;Project page: https://ganwanshui.github.io/GaussianOcc/;gaussian splatting
2408.11697v1;http://arxiv.org/abs/2408.11697v1;2024-08-21;Robust 3D Gaussian Splatting for Novel View Synthesis in Presence of  Distractors;"3D Gaussian Splatting has shown impressive novel view synthesis results;
nonetheless, it is vulnerable to dynamic objects polluting the input data of an
otherwise static scene, so called distractors. Distractors have severe impact
on the rendering quality as they get represented as view-dependent effects or
result in floating artifacts. Our goal is to identify and ignore such
distractors during the 3D Gaussian optimization to obtain a clean
reconstruction. To this end, we take a self-supervised approach that looks at
the image residuals during the optimization to determine areas that have likely
been falsified by a distractor. In addition, we leverage a pretrained
segmentation network to provide object awareness, enabling more accurate
exclusion of distractors. This way, we obtain segmentation masks of distractors
to effectively ignore them in the loss formulation. We demonstrate that our
approach is robust to various distractors and strongly improves rendering
quality on distractor-polluted scenes, improving PSNR by 1.86dB compared to 3D
Gaussian Splatting.";Paul Ungermann<author:sep>Armin Ettenhofer<author:sep>Matthias Nießner<author:sep>Barbara Roessle;http://arxiv.org/pdf/2408.11697v1;cs.CV;"GCPR 2024, Project Page:
  https://paulungermann.github.io/Robust3DGaussians , Video:
  https://www.youtube.com/watch?v=P9unyR7yK3E";gaussian splatting
2408.11251v1;http://arxiv.org/abs/2408.11251v1;2024-08-21;Irregularity Inspection using Neural Radiance Field;"With the increasing growth of industrialization, more and more industries are
relying on machine automation for production. However, defect detection in
large-scale production machinery is becoming increasingly important. Due to
their large size and height, it is often challenging for professionals to
conduct defect inspections on such large machinery. For example, the inspection
of aging and misalignment of components on tall machinery like towers requires
companies to assign dedicated personnel. Employees need to climb the towers and
either visually inspect or take photos to detect safety hazards in these large
machines. Direct visual inspection is limited by its low level of automation,
lack of precision, and safety concerns associated with personnel climbing the
towers. Therefore, in this paper, we propose a system based on neural network
modeling (NeRF) of 3D twin models. By comparing two digital models, this system
enables defect detection at the 3D interface of an object.";Tianqi Ding<author:sep>Dawei Xiang;http://arxiv.org/pdf/2408.11251v1;cs.CV;;nerf
2408.11413v2;http://arxiv.org/abs/2408.11413v2;2024-08-21;Pano2Room: Novel View Synthesis from a Single Indoor Panorama;"Recent single-view 3D generative methods have made significant advancements
by leveraging knowledge distilled from extensive 3D object datasets. However,
challenges persist in the synthesis of 3D scenes from a single view, primarily
due to the complexity of real-world environments and the limited availability
of high-quality prior resources. In this paper, we introduce a novel approach
called Pano2Room, designed to automatically reconstruct high-quality 3D indoor
scenes from a single panoramic image. These panoramic images can be easily
generated using a panoramic RGBD inpainter from captures at a single location
with any camera. The key idea is to initially construct a preliminary mesh from
the input panorama, and iteratively refine this mesh using a panoramic RGBD
inpainter while collecting photo-realistic 3D-consistent pseudo novel views.
Finally, the refined mesh is converted into a 3D Gaussian Splatting field and
trained with the collected pseudo novel views. This pipeline enables the
reconstruction of real-world 3D scenes, even in the presence of large
occlusions, and facilitates the synthesis of photo-realistic novel views with
detailed geometry. Extensive qualitative and quantitative experiments have been
conducted to validate the superiority of our method in single-panorama indoor
novel synthesis compared to the state-of-the-art. Our code and data are
available at \url{https://github.com/TrickyGo/Pano2Room}.";Guo Pu<author:sep>Yiming Zhao<author:sep>Zhouhui Lian;http://arxiv.org/pdf/2408.11413v2;cs.CV;"SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers '24),
  December 3--6, 2024, Tokyo, Japan";gaussian splatting
2408.10588v1;http://arxiv.org/abs/2408.10588v1;2024-08-20;DEGAS: Detailed Expressions on Full-Body Gaussian Avatars;"Although neural rendering has made significant advancements in creating
lifelike, animatable full-body and head avatars, incorporating detailed
expressions into full-body avatars remains largely unexplored. We present
DEGAS, the first 3D Gaussian Splatting (3DGS)-based modeling method for
full-body avatars with rich facial expressions. Trained on multiview videos of
a given subject, our method learns a conditional variational autoencoder that
takes both the body motion and facial expression as driving signals to generate
Gaussian maps in the UV layout. To drive the facial expressions, instead of the
commonly used 3D Morphable Models (3DMMs) in 3D head avatars, we propose to
adopt the expression latent space trained solely on 2D portrait images,
bridging the gap between 2D talking faces and 3D avatars. Leveraging the
rendering capability of 3DGS and the rich expressiveness of the expression
latent space, the learned avatars can be reenacted to reproduce photorealistic
rendering images with subtle and accurate facial expressions. Experiments on an
existing dataset and our newly proposed dataset of full-body talking avatars
demonstrate the efficacy of our method. We also propose an audio-driven
extension of our method with the help of 2D talking faces, opening new
possibilities to interactive AI agents.";Zhijing Shao<author:sep>Duotun Wang<author:sep>Qing-Yao Tian<author:sep>Yao-Dong Yang<author:sep>Hengyu Meng<author:sep>Zeyu Cai<author:sep>Bo Dong<author:sep>Yu Zhang<author:sep>Kang Zhang<author:sep>Zeyu Wang;http://arxiv.org/pdf/2408.10588v1;cs.CV;;gaussian splatting
2408.10906v1;http://arxiv.org/abs/2408.10906v1;2024-08-20;ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their  Self-Supervised Pretraining;"3D Gaussian Splatting (3DGS) has become the de facto method of 3D
representation in many vision tasks. This calls for the 3D understanding
directly in this representation space. To facilitate the research in this
direction, we first build a large-scale dataset of 3DGS using the commonly used
ShapeNet and ModelNet datasets. Our dataset ShapeSplat consists of 65K objects
from 87 unique categories, whose labels are in accordance with the respective
datasets. The creation of this dataset utilized the compute equivalent of 2 GPU
years on a TITAN XP GPU.
  We utilize our dataset for unsupervised pretraining and supervised finetuning
for classification and segmentation tasks. To this end, we introduce
\textbf{\textit{Gaussian-MAE}}, which highlights the unique benefits of
representation learning from Gaussian parameters. Through exhaustive
experiments, we provide several valuable insights. In particular, we show that
(1) the distribution of the optimized GS centroids significantly differs from
the uniformly sampled point cloud (used for initialization) counterpart; (2)
this change in distribution results in degradation in classification but
improvement in segmentation tasks when using only the centroids; (3) to
leverage additional Gaussian parameters, we propose Gaussian feature grouping
in a normalized feature space, along with splats pooling layer, offering a
tailored solution to effectively group and embed similar Gaussians, which leads
to notable improvement in finetuning tasks.";Qi Ma<author:sep>Yue Li<author:sep>Bin Ren<author:sep>Nicu Sebe<author:sep>Ender Konukoglu<author:sep>Theo Gevers<author:sep>Luc Van Gool<author:sep>Danda Pani Paudel;http://arxiv.org/pdf/2408.10906v1;cs.CV;;gaussian splatting
2408.11085v1;http://arxiv.org/abs/2408.11085v1;2024-08-20;GSLoc: Efficient Camera Pose Refinement via 3D Gaussian Splatting;"We leverage 3D Gaussian Splatting (3DGS) as a scene representation and
propose a novel test-time camera pose refinement framework, GSLoc. This
framework enhances the localization accuracy of state-of-the-art absolute pose
regression and scene coordinate regression methods. The 3DGS model renders
high-quality synthetic images and depth maps to facilitate the establishment of
2D-3D correspondences. GSLoc obviates the need for training feature extractors
or descriptors by operating directly on RGB images, utilizing the 3D vision
foundation model, MASt3R, for precise 2D matching. To improve the robustness of
our model in challenging outdoor environments, we incorporate an
exposure-adaptive module within the 3DGS framework. Consequently, GSLoc enables
efficient pose refinement given a single RGB query and a coarse initial pose
estimation. Our proposed approach surpasses leading NeRF-based optimization
methods in both accuracy and runtime across indoor and outdoor visual
localization benchmarks, achieving state-of-the-art accuracy on two indoor
datasets.";Changkun Liu<author:sep>Shuai Chen<author:sep>Yash Bhalgat<author:sep>Siyan Hu<author:sep>Zirui Wang<author:sep>Ming Cheng<author:sep>Victor Adrian Prisacariu<author:sep>Tristan Braud;http://arxiv.org/pdf/2408.11085v1;cs.CV;The project page is available at https://gsloc.active.vision;gaussian splatting<tag:sep>nerf
2408.10789v1;http://arxiv.org/abs/2408.10789v1;2024-08-20;Learning Part-aware 3D Representations by Fusing 2D Gaussians and  Superquadrics;"Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D
Gaussians, are commonly used to represent 3D objects or scenes. However, humans
usually perceive 3D objects or scenes at a higher level as a composition of
parts or structures rather than points or voxels. Representing 3D as semantic
parts can benefit further understanding and applications. We aim to solve
part-aware 3D reconstruction, which parses objects or scenes into semantic
parts. In this paper, we introduce a hybrid representation of superquadrics and
2D Gaussians, trying to dig 3D structural clues from multi-view image inputs.
Accurate structured geometry reconstruction and high-quality rendering are
achieved at the same time. We incorporate parametric superquadrics in mesh
forms into 2D Gaussians by attaching Gaussian centers to faces in meshes.
During the training, superquadrics parameters are iteratively optimized, and
Gaussians are deformed accordingly, resulting in an efficient hybrid
representation. On the one hand, this hybrid representation inherits the
advantage of superquadrics to represent different shape primitives, supporting
flexible part decomposition of scenes. On the other hand, 2D Gaussians are
incorporated to model the complex texture and geometry details, ensuring
high-quality rendering and geometry reconstruction. The reconstruction is fully
unsupervised. We conduct extensive experiments on data from DTU and ShapeNet
datasets, in which the method decomposes scenes into reasonable parts,
outperforming existing state-of-the-art approaches.";Zhirui Gao<author:sep>Renjiao Yi<author:sep>Yuhang Huang<author:sep>Wei Chen<author:sep>Chenyang Zhu<author:sep>Kai Xu;http://arxiv.org/pdf/2408.10789v1;cs.CV;;nerf
2408.10739v1;http://arxiv.org/abs/2408.10739v1;2024-08-20;TrackNeRF: Bundle Adjusting NeRF from Sparse and Noisy Views via Feature  Tracks;"Neural radiance fields (NeRFs) generally require many images with accurate
poses for accurate novel view synthesis, which does not reflect realistic
setups where views can be sparse and poses can be noisy. Previous solutions for
learning NeRFs with sparse views and noisy poses only consider local geometry
consistency with pairs of views. Closely following \textit{bundle adjustment}
in Structure-from-Motion (SfM), we introduce TrackNeRF for more globally
consistent geometry reconstruction and more accurate pose optimization.
TrackNeRF introduces \textit{feature tracks}, \ie connected pixel trajectories
across \textit{all} visible views that correspond to the \textit{same} 3D
points. By enforcing reprojection consistency among feature tracks, TrackNeRF
encourages holistic 3D consistency explicitly. Through extensive experiments,
TrackNeRF sets a new benchmark in noisy and sparse view reconstruction. In
particular, TrackNeRF shows significant improvements over the state-of-the-art
BARF and SPARF by $\sim8$ and $\sim1$ in terms of PSNR on DTU under various
sparse and noisy view setups. The code is available at
\href{https://tracknerf.github.io/}.";Jinjie Mai<author:sep>Wenxuan Zhu<author:sep>Sara Rojas<author:sep>Jesus Zarzar<author:sep>Abdullah Hamdi<author:sep>Guocheng Qian<author:sep>Bing Li<author:sep>Silvio Giancola<author:sep>Bernard Ghanem;http://arxiv.org/pdf/2408.10739v1;cs.CV;ECCV 2024 (supplemental pages included);nerf
2408.09663v2;http://arxiv.org/abs/2408.09663v2;2024-08-19;CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian  Splatting and Contrastive Learning;"Recent advancements in human avatar synthesis have utilized radiance fields
to reconstruct photo-realistic animatable human avatars. However, both
NeRFs-based and 3DGS-based methods struggle with maintaining 3D consistency and
exhibit suboptimal detail reconstruction, especially with sparse inputs. To
address this challenge, we propose CHASE, which introduces supervision from
intrinsic 3D consistency across poses and 3D geometry contrastive learning,
achieving performance comparable with sparse inputs to that with full inputs.
Following previous work, we first integrate a skeleton-driven rigid deformation
and a non-rigid cloth dynamics deformation to coordinate the movements of
individual Gaussians during animation, reconstructing basic avatar with coarse
3D consistency. To improve 3D consistency under sparse inputs, we design
Dynamic Avatar Adjustment(DAA) to adjust deformed Gaussians based on a selected
similar pose/image from the dataset. Minimizing the difference between the
image rendered by adjusted Gaussians and the image with the similar pose serves
as an additional form of supervision for avatar. Furthermore, we propose a 3D
geometry contrastive learning strategy to maintain the 3D global consistency of
generated avatars. Though CHASE is designed for sparse inputs, it surprisingly
outperforms current SOTA methods \textbf{in both full and sparse settings} on
the ZJU-MoCap and H36M datasets, demonstrating that our CHASE successfully
maintains avatar's 3D consistency, hence improving rendering quality.";Haoyu Zhao<author:sep>Hao Wang<author:sep>Chen Yang<author:sep>Wei Shen;http://arxiv.org/pdf/2408.09663v2;cs.CV;13 pages, 6 figures;nerf
2408.09665v1;http://arxiv.org/abs/2408.09665v1;2024-08-19;SG-GS: Photo-realistic Animatable Human Avatars with Semantically-Guided  Gaussian Splatting;"Reconstructing photo-realistic animatable human avatars from monocular videos
remains challenging in computer vision and graphics. Recently, methods using 3D
Gaussians to represent the human body have emerged, offering faster
optimization and real-time rendering. However, due to ignoring the crucial role
of human body semantic information which represents the intrinsic structure and
connections within the human body, they fail to achieve fine-detail
reconstruction of dynamic human avatars. To address this issue, we propose
SG-GS, which uses semantics-embedded 3D Gaussians, skeleton-driven rigid
deformation, and non-rigid cloth dynamics deformation to create photo-realistic
animatable human avatars from monocular videos. We then design a Semantic
Human-Body Annotator (SHA) which utilizes SMPL's semantic prior for efficient
body part semantic labeling. The generated labels are used to guide the
optimization of Gaussian semantic attributes. To address the limited receptive
field of point-level MLPs for local features, we also propose a 3D network that
integrates geometric and semantic associations for human avatar deformation. We
further implement three key strategies to enhance the semantic accuracy of 3D
Gaussians and rendering quality: semantic projection with 2D regularization,
semantic-guided density regularization and semantic-aware regularization with
neighborhood consistency. Extensive experiments demonstrate that SG-GS achieves
state-of-the-art geometry and appearance reconstruction performance.";Haoyu Zhao<author:sep>Chen Yang<author:sep>Hao Wang<author:sep>Xingyue Zhao<author:sep>Wei Shen;http://arxiv.org/pdf/2408.09665v1;cs.CV;12 pages, 5 figures;gaussian splatting
2408.09928v1;http://arxiv.org/abs/2408.09928v1;2024-08-19;DiscoNeRF: Class-Agnostic Object Field for 3D Object Discovery;"Neural Radiance Fields (NeRFs) have become a powerful tool for modeling 3D
scenes from multiple images. However, NeRFs remain difficult to segment into
semantically meaningful regions. Previous approaches to 3D segmentation of
NeRFs either require user interaction to isolate a single object, or they rely
on 2D semantic masks with a limited number of classes for supervision. As a
consequence, they generalize poorly to class-agnostic masks automatically
generated in real scenes. This is attributable to the ambiguity arising from
zero-shot segmentation, yielding inconsistent masks across views. In contrast,
we propose a method that is robust to inconsistent segmentations and
successfully decomposes the scene into a set of objects of any class. By
introducing a limited number of competing object slots against which masks are
matched, a meaningful object representation emerges that best explains the 2D
supervision and minimizes an additional regularization term. Our experiments
demonstrate the ability of our method to generate 3D panoptic segmentations on
complex scenes, and extract high-quality 3D assets from NeRFs that can then be
used in virtual 3D environments.";Corentin Dumery<author:sep>Aoxiang Fan<author:sep>Ren Li<author:sep>Nicolas Talabot<author:sep>Pascal Fua;http://arxiv.org/pdf/2408.09928v1;cs.CV;;nerf
2408.10135v1;http://arxiv.org/abs/2408.10135v1;2024-08-19;$R^2$-Mesh: Reinforcement Learning Powered Mesh Reconstruction via  Geometry and Appearance Refinement;"Mesh reconstruction based on Neural Radiance Fields (NeRF) is popular in a
variety of applications such as computer graphics, virtual reality, and medical
imaging due to its efficiency in handling complex geometric structures and
facilitating real-time rendering. However, existing works often fail to capture
fine geometric details accurately and struggle with optimizing rendering
quality. To address these challenges, we propose a novel algorithm that
progressively generates and optimizes meshes from multi-view images. Our
approach initiates with the training of a NeRF model to establish an initial
Signed Distance Field (SDF) and a view-dependent appearance field.
Subsequently, we iteratively refine the SDF through a differentiable mesh
extraction method, continuously updating both the vertex positions and their
connectivity based on the loss from mesh differentiable rasterization, while
also optimizing the appearance representation. To further leverage
high-fidelity and detail-rich representations from NeRF, we propose an
online-learning strategy based on Upper Confidence Bound (UCB) to enhance
viewpoints by adaptively incorporating images rendered by the initial NeRF
model into the training dataset. Through extensive experiments, we demonstrate
that our method delivers highly competitive and robust performance in both mesh
rendering quality and geometric quality.";Haoyang Wang<author:sep>Liming Liu<author:sep>Quanlu Jia<author:sep>Jiangkai Wu<author:sep>Haodan Zhang<author:sep>Peiheng Wang<author:sep>Xinggong Zhang;http://arxiv.org/pdf/2408.10135v1;cs.CV;;nerf
2408.10154v2;http://arxiv.org/abs/2408.10154v2;2024-08-19;LoopSplat: Loop Closure by Registering 3D Gaussian Splats;"Simultaneous Localization and Mapping (SLAM) based on 3D Gaussian Splats
(3DGS) has recently shown promise towards more accurate, dense 3D scene maps.
However, existing 3DGS-based methods fail to address the global consistency of
the scene via loop closure and/or global bundle adjustment. To this end, we
propose LoopSplat, which takes RGB-D images as input and performs dense mapping
with 3DGS submaps and frame-to-model tracking. LoopSplat triggers loop closure
online and computes relative loop edge constraints between submaps directly via
3DGS registration, leading to improvements in efficiency and accuracy over
traditional global-to-local point cloud registration. It uses a robust pose
graph optimization formulation and rigidly aligns the submaps to achieve global
consistency. Evaluation on the synthetic Replica and real-world TUM-RGBD,
ScanNet, and ScanNet++ datasets demonstrates competitive or superior tracking,
mapping, and rendering compared to existing methods for dense RGB-D SLAM. Code
is available at loopsplat.github.io.";Liyuan Zhu<author:sep>Yue Li<author:sep>Erik Sandström<author:sep>Shengyu Huang<author:sep>Konrad Schindler<author:sep>Iro Armeni;http://arxiv.org/pdf/2408.10154v2;cs.CV;Project page: https://loopsplat.github.io/;
2408.10041v1;http://arxiv.org/abs/2408.10041v1;2024-08-19;Implicit Gaussian Splatting with Efficient Multi-Level Tri-Plane  Representation;"Recent advancements in photo-realistic novel view synthesis have been
significantly driven by Gaussian Splatting (3DGS). Nevertheless, the explicit
nature of 3DGS data entails considerable storage requirements, highlighting a
pressing need for more efficient data representations. To address this, we
present Implicit Gaussian Splatting (IGS), an innovative hybrid model that
integrates explicit point clouds with implicit feature embeddings through a
multi-level tri-plane architecture. This architecture features 2D feature grids
at various resolutions across different levels, facilitating continuous spatial
domain representation and enhancing spatial correlations among Gaussian
primitives. Building upon this foundation, we introduce a level-based
progressive training scheme, which incorporates explicit spatial
regularization. This method capitalizes on spatial correlations to enhance both
the rendering quality and the compactness of the IGS representation.
Furthermore, we propose a novel compression pipeline tailored for both point
clouds and 2D feature grids, considering the entropy variations across
different levels. Extensive experimental evaluations demonstrate that our
algorithm can deliver high-quality rendering using only a few MBs, effectively
balancing storage efficiency and rendering fidelity, and yielding results that
are competitive with the state-of-the-art.";Minye Wu<author:sep>Tinne Tuytelaars;http://arxiv.org/pdf/2408.10041v1;cs.CV;;gaussian splatting
2408.09347v1;http://arxiv.org/abs/2408.09347v1;2024-08-18;S^3D-NeRF: Single-Shot Speech-Driven Neural Radiance Field for High  Fidelity Talking Head Synthesis;"Talking head synthesis is a practical technique with wide applications.
Current Neural Radiance Field (NeRF) based approaches have shown their
superiority on driving one-shot talking heads with videos or signals regressed
from audio. However, most of them failed to take the audio as driven
information directly, unable to enjoy the flexibility and availability of
speech. Since mapping audio signals to face deformation is non-trivial, we
design a Single-Shot Speech-Driven Neural Radiance Field (S^3D-NeRF) method in
this paper to tackle the following three difficulties: learning a
representative appearance feature for each identity, modeling motion of
different face regions with audio, and keeping the temporal consistency of the
lip area. To this end, we introduce a Hierarchical Facial Appearance Encoder to
learn multi-scale representations for catching the appearance of different
speakers, and elaborate a Cross-modal Facial Deformation Field to perform
speech animation according to the relationship between the audio signal and
different face regions. Moreover, to enhance the temporal consistency of the
important lip area, we introduce a lip-sync discriminator to penalize the
out-of-sync audio-visual sequences. Extensive experiments have shown that our
S^3D-NeRF surpasses previous arts on both video fidelity and audio-lip
synchronization.";Dongze Li<author:sep>Kang Zhao<author:sep>Wei Wang<author:sep>Yifeng Ma<author:sep>Bo Peng<author:sep>Yingya Zhang<author:sep>Jing Dong;http://arxiv.org/pdf/2408.09347v1;cs.CV;ECCV 2024;nerf
2408.09144v1;http://arxiv.org/abs/2408.09144v1;2024-08-17;SSNeRF: Sparse View Semi-supervised Neural Radiance Fields with  Augmentation;"Sparse view NeRF is challenging because limited input images lead to an under
constrained optimization problem for volume rendering. Existing methods address
this issue by relying on supplementary information, such as depth maps.
However, generating this supplementary information accurately remains
problematic and often leads to NeRF producing images with undesired artifacts.
To address these artifacts and enhance robustness, we propose SSNeRF, a sparse
view semi supervised NeRF method based on a teacher student framework. Our key
idea is to challenge the NeRF module with progressively severe sparse view
degradation while providing high confidence pseudo labels. This approach helps
the NeRF model become aware of noise and incomplete information associated with
sparse views, thus improving its robustness. The novelty of SSNeRF lies in its
sparse view specific augmentations and semi supervised learning mechanism. In
this approach, the teacher NeRF generates novel views along with confidence
scores, while the student NeRF, perturbed by the augmented input, learns from
the high confidence pseudo labels. Our sparse view degradation augmentation
progressively injects noise into volume rendering weights, perturbs feature
maps in vulnerable layers, and simulates sparse view blurriness. These
augmentation strategies force the student NeRF to recognize degradation and
produce clearer rendered views. By transferring the student's parameters to the
teacher, the teacher gains increased robustness in subsequent training
iterations. Extensive experiments demonstrate the effectiveness of our SSNeRF
in generating novel views with less sparse view degradation. We will release
code upon acceptance.";Xiao Cao<author:sep>Beibei Lin<author:sep>Bo Wang<author:sep>Zhiyong Huang<author:sep>Robby T. Tan;http://arxiv.org/pdf/2408.09144v1;cs.CV;;nerf
2408.09130v2;http://arxiv.org/abs/2408.09130v2;2024-08-17;Gaussian in the Dark: Real-Time View Synthesis From Inconsistent Dark  Images Using Gaussian Splatting;"3D Gaussian Splatting has recently emerged as a powerful representation that
can synthesize remarkable novel views using consistent multi-view images as
input. However, we notice that images captured in dark environments where the
scenes are not fully illuminated can exhibit considerable brightness variations
and multi-view inconsistency, which poses great challenges to 3D Gaussian
Splatting and severely degrades its performance. To tackle this problem, we
propose Gaussian-DK. Observing that inconsistencies are mainly caused by camera
imaging, we represent a consistent radiance field of the physical world using a
set of anisotropic 3D Gaussians, and design a camera response module to
compensate for multi-view inconsistencies. We also introduce a step-based
gradient scaling strategy to constrain Gaussians near the camera, which turn
out to be floaters, from splitting and cloning. Experiments on our proposed
benchmark dataset demonstrate that Gaussian-DK produces high-quality renderings
without ghosting and floater artifacts and significantly outperforms existing
methods. Furthermore, we can also synthesize light-up images by controlling
exposure levels that clearly show details in shadow areas.";Sheng Ye<author:sep>Zhen-Hui Dong<author:sep>Yubin Hu<author:sep>Yu-Hui Wen<author:sep>Yong-Jin Liu;http://arxiv.org/pdf/2408.09130v2;cs.CV;accepted by PG 2024;gaussian splatting
2408.09104v1;http://arxiv.org/abs/2408.09104v1;2024-08-17;HybridOcc: NeRF Enhanced Transformer-based Multi-Camera 3D Occupancy  Prediction;"Vision-based 3D semantic scene completion (SSC) describes autonomous driving
scenes through 3D volume representations. However, the occlusion of invisible
voxels by scene surfaces poses challenges to current SSC methods in
hallucinating refined 3D geometry. This paper proposes HybridOcc, a hybrid 3D
volume query proposal method generated by Transformer framework and NeRF
representation and refined in a coarse-to-fine SSC prediction framework.
HybridOcc aggregates contextual features through the Transformer paradigm based
on hybrid query proposals while combining it with NeRF representation to obtain
depth supervision. The Transformer branch contains multiple scales and uses
spatial cross-attention for 2D to 3D transformation. The newly designed NeRF
branch implicitly infers scene occupancy through volume rendering, including
visible and invisible voxels, and explicitly captures scene depth rather than
generating RGB color. Furthermore, we present an innovative occupancy-aware ray
sampling method to orient the SSC task instead of focusing on the scene
surface, further improving the overall performance. Extensive experiments on
nuScenes and SemanticKITTI datasets demonstrate the effectiveness of our
HybridOcc on the SSC task.";Xiao Zhao<author:sep>Bo Chen<author:sep>Mingyang Sun<author:sep>Dingkang Yang<author:sep>Youxing Wang<author:sep>Xukun Zhang<author:sep>Mingcheng Li<author:sep>Dongliang Kou<author:sep>Xiaoyi Wei<author:sep>Lihua Zhang;http://arxiv.org/pdf/2408.09104v1;cs.CV;Accepted to IEEE RAL;nerf
2408.08524v1;http://arxiv.org/abs/2408.08524v1;2024-08-16;GS-ID: Illumination Decomposition on Gaussian Splatting via Diffusion  Prior and Parametric Light Source Optimization;"We present GS-ID, a novel framework for illumination decomposition on
Gaussian Splatting, achieving photorealistic novel view synthesis and intuitive
light editing. Illumination decomposition is an ill-posed problem facing three
main challenges: 1) priors for geometry and material are often lacking; 2)
complex illumination conditions involve multiple unknown light sources; and 3)
calculating surface shading with numerous light sources is computationally
expensive. To address these challenges, we first introduce intrinsic diffusion
priors to estimate the attributes for physically based rendering. Then we
divide the illumination into environmental and direct components for joint
optimization. Last, we employ deferred rendering to reduce the computational
load. Our framework uses a learnable environment map and Spherical Gaussians
(SGs) to represent light sources parametrically, therefore enabling
controllable and photorealistic relighting on Gaussian Splatting. Extensive
experiments and applications demonstrate that GS-ID produces state-of-the-art
illumination decomposition results while achieving better geometry
reconstruction and rendering performance.";Kang Du<author:sep>Zhihao Liang<author:sep>Zeyu Wang;http://arxiv.org/pdf/2408.08524v1;cs.CV;15 pages, 13 figures;gaussian splatting
2408.08723v1;http://arxiv.org/abs/2408.08723v1;2024-08-16;Correspondence-Guided SfM-Free 3D Gaussian Splatting for NVS;"Novel View Synthesis (NVS) without Structure-from-Motion (SfM) pre-processed
camera poses--referred to as SfM-free methods--is crucial for promoting rapid
response capabilities and enhancing robustness against variable operating
conditions. Recent SfM-free methods have integrated pose optimization,
designing end-to-end frameworks for joint camera pose estimation and NVS.
However, most existing works rely on per-pixel image loss functions, such as L2
loss. In SfM-free methods, inaccurate initial poses lead to misalignment issue,
which, under the constraints of per-pixel image loss functions, results in
excessive gradients, causing unstable optimization and poor convergence for
NVS. In this study, we propose a correspondence-guided SfM-free 3D Gaussian
splatting for NVS. We use correspondences between the target and the rendered
result to achieve better pixel alignment, facilitating the optimization of
relative poses between frames. We then apply the learned poses to optimize the
entire scene. Each 2D screen-space pixel is associated with its corresponding
3D Gaussians through approximated surface rendering to facilitate gradient back
propagation. Experimental results underline the superior performance and time
efficiency of the proposed approach compared to the state-of-the-art baselines.";Wei Sun<author:sep>Xiaosong Zhang<author:sep>Fang Wan<author:sep>Yanzhao Zhou<author:sep>Yuan Li<author:sep>Qixiang Ye<author:sep>Jianbin Jiao;http://arxiv.org/pdf/2408.08723v1;cs.CV;arXiv admin note: text overlap with arXiv:2312.07504 by other authors;gaussian splatting
2408.08766v1;http://arxiv.org/abs/2408.08766v1;2024-08-16;VF-NeRF: Learning Neural Vector Fields for Indoor Scene Reconstruction;"Implicit surfaces via neural radiance fields (NeRF) have shown surprising
accuracy in surface reconstruction. Despite their success in reconstructing
richly textured surfaces, existing methods struggle with planar regions with
weak textures, which account for the majority of indoor scenes. In this paper,
we address indoor dense surface reconstruction by revisiting key aspects of
NeRF in order to use the recently proposed Vector Field (VF) as the implicit
representation. VF is defined by the unit vector directed to the nearest
surface point. It therefore flips direction at the surface and equals to the
explicit surface normals. Except for this flip, VF remains constant along
planar surfaces and provides a strong inductive bias in representing planar
surfaces. Concretely, we develop a novel density-VF relationship and a training
scheme that allows us to learn VF via volume rendering By doing this, VF-NeRF
can model large planar surfaces and sharp corners accurately. We show that,
when depth cues are available, our method further improves and achieves
state-of-the-art results in reconstructing indoor scenes and rendering novel
views. We extensively evaluate VF-NeRF on indoor datasets and run ablations of
its components.";Albert Gassol Puigjaner<author:sep>Edoardo Mello Rella<author:sep>Erik Sandström<author:sep>Ajad Chhatkuli<author:sep>Luc Van Gool;http://arxiv.org/pdf/2408.08766v1;cs.CV;15 pages;nerf
2408.07967v2;http://arxiv.org/abs/2408.07967v2;2024-08-15;FlashGS: Efficient 3D Gaussian Splatting for Large-scale and  High-resolution Rendering;"This work introduces FlashGS, an open-source CUDA Python library, designed to
facilitate the efficient differentiable rasterization of 3D Gaussian Splatting
through algorithmic and kernel-level optimizations. FlashGS is developed based
on the observations from a comprehensive analysis of the rendering process to
enhance computational efficiency and bring the technique to wide adoption. The
paper includes a suite of optimization strategies, encompassing redundancy
elimination, efficient pipelining, refined control and scheduling mechanisms,
and memory access optimizations, all of which are meticulously integrated to
amplify the performance of the rasterization process. An extensive evaluation
of FlashGS' performance has been conducted across a diverse spectrum of
synthetic and real-world large-scale scenes, encompassing a variety of image
resolutions. The empirical findings demonstrate that FlashGS consistently
achieves an average 4x acceleration over mobile consumer GPUs, coupled with
reduced memory consumption. These results underscore the superior performance
and resource optimization capabilities of FlashGS, positioning it as a
formidable tool in the domain of 3D rendering.";Guofeng Feng<author:sep>Siyan Chen<author:sep>Rong Fu<author:sep>Zimu Liao<author:sep>Yi Wang<author:sep>Tao Liu<author:sep>Zhilin Pei<author:sep>Hengjie Li<author:sep>Xingcheng Zhang<author:sep>Bo Dai;http://arxiv.org/pdf/2408.07967v2;cs.CV;;gaussian splatting
2408.08206v1;http://arxiv.org/abs/2408.08206v1;2024-08-15;WaterSplatting: Fast Underwater 3D Scene Reconstruction Using Gaussian  Splatting;"The underwater 3D scene reconstruction is a challenging, yet interesting
problem with applications ranging from naval robots to VR experiences. The
problem was successfully tackled by fully volumetric NeRF-based methods which
can model both the geometry and the medium (water). Unfortunately, these
methods are slow to train and do not offer real-time rendering. More recently,
3D Gaussian Splatting (3DGS) method offered a fast alternative to NeRFs.
However, because it is an explicit method that renders only the geometry, it
cannot render the medium and is therefore unsuited for underwater
reconstruction. Therefore, we propose a novel approach that fuses volumetric
rendering with 3DGS to handle underwater data effectively. Our method employs
3DGS for explicit geometry representation and a separate volumetric field
(queried once per pixel) for capturing the scattering medium. This dual
representation further allows the restoration of the scenes by removing the
scattering medium. Our method outperforms state-of-the-art NeRF-based methods
in rendering quality on the underwater SeaThru-NeRF dataset. Furthermore, it
does so while offering real-time rendering performance, addressing the
efficiency limitations of existing methods. Web:
https://water-splatting.github.io";Huapeng Li<author:sep>Wenxuan Song<author:sep>Tianao Xu<author:sep>Alexandre Elsig<author:sep>Jonas Kulhanek;http://arxiv.org/pdf/2408.08206v1;cs.CV;Web: https://water-splatting.github.io;gaussian splatting<tag:sep>nerf
2408.07416v2;http://arxiv.org/abs/2408.07416v2;2024-08-14;Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space;"Understanding the 3D semantics of a scene is a fundamental problem for
various scenarios such as embodied agents. While NeRFs and 3DGS excel at
novel-view synthesis, previous methods for understanding their semantics have
been limited to incomplete 3D understanding: their segmentation results are 2D
masks and their supervision is anchored at 2D pixels. This paper revisits the
problem set to pursue a better 3D understanding of a scene modeled by NeRFs and
3DGS as follows. 1) We directly supervise the 3D points to train the language
embedding field. It achieves state-of-the-art accuracy without relying on
multi-scale language embeddings. 2) We transfer the pre-trained language field
to 3DGS, achieving the first real-time rendering speed without sacrificing
training time or accuracy. 3) We introduce a 3D querying and evaluation
protocol for assessing the reconstructed geometry and semantics together. Code,
checkpoints, and annotations will be available online. Project page:
https://hyunji12.github.io/Open3DRF";Hyunjee Lee<author:sep>Youngsik Yun<author:sep>Jeongmin Bae<author:sep>Seoha Kim<author:sep>Youngjung Uh;http://arxiv.org/pdf/2408.07416v2;cs.CV;Project page: https://hyunji12.github.io/Open3DRF;nerf
2408.07540v1;http://arxiv.org/abs/2408.07540v1;2024-08-14;3D Gaussian Editing with A Single Image;"The modeling and manipulation of 3D scenes captured from the real world are
pivotal in various applications, attracting growing research interest. While
previous works on editing have achieved interesting results through
manipulating 3D meshes, they often require accurately reconstructed meshes to
perform editing, which limits their application in 3D content generation. To
address this gap, we introduce a novel single-image-driven 3D scene editing
approach based on 3D Gaussian Splatting, enabling intuitive manipulation via
directly editing the content on a 2D image plane. Our method learns to optimize
the 3D Gaussians to align with an edited version of the image rendered from a
user-specified viewpoint of the original scene. To capture long-range object
deformation, we introduce positional loss into the optimization process of 3D
Gaussian Splatting and enable gradient propagation through reparameterization.
To handle occluded 3D Gaussians when rendering from the specified viewpoint, we
build an anchor-based structure and employ a coarse-to-fine optimization
strategy capable of handling long-range deformation while maintaining
structural stability. Furthermore, we design a novel masking strategy to
adaptively identify non-rigid deformation regions for fine-scale modeling.
Extensive experiments show the effectiveness of our method in handling
geometric details, long-range, and non-rigid deformation, demonstrating
superior editing flexibility and quality compared to previous approaches.";Guan Luo<author:sep>Tian-Xing Xu<author:sep>Ying-Tian Liu<author:sep>Xiao-Xiong Fan<author:sep>Fang-Lue Zhang<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2408.07540v1;cs.CV;10 pages, 12 figures;gaussian splatting
2408.07595v1;http://arxiv.org/abs/2408.07595v1;2024-08-14;Progressive Radiance Distillation for Inverse Rendering with Gaussian  Splatting;"We propose progressive radiance distillation, an inverse rendering method
that combines physically-based rendering with Gaussian-based radiance field
rendering using a distillation progress map. Taking multi-view images as input,
our method starts from a pre-trained radiance field guidance, and distills
physically-based light and material parameters from the radiance field using an
image-fitting process. The distillation progress map is initialized to a small
value, which favors radiance field rendering. During early iterations when
fitted light and material parameters are far from convergence, the radiance
field fallback ensures the sanity of image loss gradients and avoids local
minima that attracts under-fit states. As fitted parameters converge, the
physical model gradually takes over and the distillation progress increases
correspondingly. In presence of light paths unmodeled by the physical model,
the distillation progress never finishes on affected pixels and the learned
radiance field stays in the final rendering. With this designed tolerance for
physical model limitations, we prevent unmodeled color components from leaking
into light and material parameters, alleviating relighting artifacts.
Meanwhile, the remaining radiance field compensates for the limitations of the
physical model, guaranteeing high-quality novel views synthesis. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
techniques quality-wise in both novel view synthesis and relighting. The idea
of progressive radiance distillation is not limited to Gaussian splatting. We
show that it also has positive effects for prominently specular scenes when
adapted to a mesh-based inverse rendering method.";Keyang Ye<author:sep>Qiming Hou<author:sep>Kun Zhou;http://arxiv.org/pdf/2408.07595v1;cs.CV;;gaussian splatting
2408.06608v1;http://arxiv.org/abs/2408.06608v1;2024-08-13;Potamoi: Accelerating Neural Rendering via a Unified Streaming  Architecture;"Neural Radiance Field (NeRF) has emerged as a promising alternative for
photorealistic rendering. Despite recent algorithmic advancements, achieving
real-time performance on today's resource-constrained devices remains
challenging. In this paper, we identify the primary bottlenecks in current NeRF
algorithms and introduce a unified algorithm-architecture co-design, Potamoi,
designed to accommodate various NeRF algorithms. Specifically, we introduce a
runtime system featuring a plug-and-play algorithm, SpaRW, which significantly
reduces the per-frame computational workload and alleviates compute
inefficiencies. Furthermore, our unified streaming pipeline coupled with
customized hardware support effectively tames both SRAM and DRAM inefficiencies
by minimizing repetitive DRAM access and completely eliminating SRAM bank
conflicts. When evaluated against a baseline utilizing a dedicated DNN
accelerator, our framework demonstrates a speed-up and energy reduction of
53.1$\times$ and 67.7$\times$, respectively, all while maintaining high visual
quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.";Yu Feng<author:sep>Weikai Lin<author:sep>Zihan Liu<author:sep>Jingwen Leng<author:sep>Minyi Guo<author:sep>Han Zhao<author:sep>Xiaofeng Hou<author:sep>Jieru Zhao<author:sep>Yuhao Zhu;http://arxiv.org/pdf/2408.06608v1;cs.AR;arXiv admin note: substantial text overlap with arXiv:2404.11852;nerf
2408.06592v1;http://arxiv.org/abs/2408.06592v1;2024-08-13;ActiveNeRF: Learning Accurate 3D Geometry by Active Pattern Projection;"NeRFs have achieved incredible success in novel view synthesis. However, the
accuracy of the implicit geometry is unsatisfactory because the passive static
environmental illumination has low spatial frequency and cannot provide enough
information for accurate geometry reconstruction. In this work, we propose
ActiveNeRF, a 3D geometry reconstruction framework, which improves the geometry
quality of NeRF by actively projecting patterns of high spatial frequency onto
the scene using a projector which has a constant relative pose to the camera.
We design a learnable active pattern rendering pipeline which jointly learns
the scene geometry and the active pattern. We find that, by adding the active
pattern and imposing its consistency across different views, our proposed
method outperforms state of the art geometry reconstruction methods
qualitatively and quantitatively in both simulation and real experiments. Code
is avaliable at https://github.com/hcp16/active_nerf";Jianyu Tao<author:sep>Changping Hu<author:sep>Edward Yang<author:sep>Jing Xu<author:sep>Rui Chen;http://arxiv.org/pdf/2408.06592v1;cs.CV;18 pages, 10 figures;nerf
2408.06975v1;http://arxiv.org/abs/2408.06975v1;2024-08-13;SpectralGaussians: Semantic, spectral 3D Gaussian splatting for  multi-spectral scene representation, visualization and analysis;"We propose a novel cross-spectral rendering framework based on 3D Gaussian
Splatting (3DGS) that generates realistic and semantically meaningful splats
from registered multi-view spectrum and segmentation maps. This extension
enhances the representation of scenes with multiple spectra, providing insights
into the underlying materials and segmentation. We introduce an improved
physically-based rendering approach for Gaussian splats, estimating reflectance
and lights per spectra, thereby enhancing accuracy and realism. In a
comprehensive quantitative and qualitative evaluation, we demonstrate the
superior performance of our approach with respect to other recent
learning-based spectral scene representation approaches (i.e., XNeRF and
SpectralNeRF) as well as other non-spectral state-of-the-art learning-based
approaches. Our work also demonstrates the potential of spectral scene
understanding for precise scene editing techniques like style transfer,
inpainting, and removal. Thereby, our contributions address challenges in
multi-spectral scene representation, rendering, and editing, offering new
possibilities for diverse applications.";Saptarshi Neil Sinha<author:sep>Holger Graf<author:sep>Michael Weinmann;http://arxiv.org/pdf/2408.06975v1;cs.CV;;gaussian splatting<tag:sep>nerf
2408.06543v1;http://arxiv.org/abs/2408.06543v1;2024-08-13;HDRGS: High Dynamic Range Gaussian Splatting;"Recent years have witnessed substantial advancements in the field of 3D
reconstruction from 2D images, particularly following the introduction of the
neural radiance field (NeRF) technique. However, reconstructing a 3D high
dynamic range (HDR) radiance field, which aligns more closely with real-world
conditions, from 2D multi-exposure low dynamic range (LDR) images continues to
pose significant challenges. Approaches to this issue fall into two categories:
grid-based and implicit-based. Implicit methods, using multi-layer perceptrons
(MLP), face inefficiencies, limited solvability, and overfitting risks.
Conversely, grid-based methods require significant memory and struggle with
image quality and long training times. In this paper, we introduce Gaussian
Splatting-a recent, high-quality, real-time 3D reconstruction technique-into
this domain. We further develop the High Dynamic Range Gaussian Splatting
(HDR-GS) method, designed to address the aforementioned challenges. This method
enhances color dimensionality by including luminance and uses an asymmetric
grid for tone-mapping, swiftly and precisely converting pixel irradiance to
color. Our approach improves HDR scene recovery accuracy and integrates a novel
coarse-to-fine strategy to speed up model convergence, enhancing robustness
against sparse viewpoints and exposure extremes, and preventing local optima.
Extensive testing confirms that our method surpasses current state-of-the-art
techniques in both synthetic and real-world scenarios. Code will be released at
\url{https://github.com/WuJH2001/HDRGS}";Jiahao Wu<author:sep>Lu Xiao<author:sep>Chao Wang<author:sep>Rui Peng<author:sep>Kaiqiang Xiong<author:sep>Ronggang Wang;http://arxiv.org/pdf/2408.06543v1;cs.CV;;gaussian splatting<tag:sep>nerf
2408.10258v2;http://arxiv.org/abs/2408.10258v2;2024-08-13;NeRF-US: Removing Ultrasound Imaging Artifacts from Neural Radiance  Fields in the Wild;"Current methods for performing 3D reconstruction and novel view synthesis
(NVS) in ultrasound imaging data often face severe artifacts when training
NeRF-based approaches. The artifacts produced by current approaches differ from
NeRF floaters in general scenes because of the unique nature of ultrasound
capture. Furthermore, existing models fail to produce reasonable 3D
reconstructions when ultrasound data is captured or obtained casually in
uncontrolled environments, which is common in clinical settings. Consequently,
existing reconstruction and NVS methods struggle to handle ultrasound motion,
fail to capture intricate details, and cannot model transparent and reflective
surfaces. In this work, we introduced NeRF-US, which incorporates 3D-geometry
guidance for border probability and scattering density into NeRF training,
while also utilizing ultrasound-specific rendering over traditional volume
rendering. These 3D priors are learned through a diffusion model. Through
experiments conducted on our new ""Ultrasound in the Wild"" dataset, we observed
accurate, clinically plausible, artifact-free reconstructions.";Rishit Dagli<author:sep>Atsuhiro Hibi<author:sep>Rahul G. Krishnan<author:sep>Pascal N. Tyrrell;http://arxiv.org/pdf/2408.10258v2;cs.CV;;nerf
2408.06030v1;http://arxiv.org/abs/2408.06030v1;2024-08-12;Developing Smart MAVs for Autonomous Inspection in GPS-denied  Constructions;"Smart Micro Aerial Vehicles (MAVs) have transformed infrastructure inspection
by enabling efficient, high-resolution monitoring at various stages of
construction, including hard-to-reach areas. Traditional manual operation of
drones in GPS-denied environments, such as industrial facilities and
infrastructure, is labour-intensive, tedious and prone to error. This study
presents an innovative framework for smart MAV inspections in such complex and
GPS-denied indoor environments. The framework features a hierarchical
perception and planning system that identifies regions of interest and
optimises task paths. It also presents an advanced MAV system with enhanced
localisation and motion planning capabilities, integrated with Neural
Reconstruction technology for comprehensive 3D reconstruction of building
structures. The effectiveness of the framework was empirically validated in a
4,000 square meters indoor infrastructure facility with an interior length of
80 metres, a width of 50 metres and a height of 7 metres. The main structure
consists of columns and walls. Experimental results show that our MAV system
performs exceptionally well in autonomous inspection tasks, achieving a 100\%
success rate in generating and executing scan paths. Extensive experiments
validate the manoeuvrability of our developed MAV, achieving a 100\% success
rate in motion planning with a tracking error of less than 0.1 metres. In
addition, the enhanced reconstruction method using 3D Gaussian Splatting
technology enables the generation of high-fidelity rendering models from the
acquired data. Overall, our novel method represents a significant advancement
in the use of robotics for infrastructure inspection.";Paoqiang Pan<author:sep>Kewei Hu<author:sep>Xiao Huang<author:sep>Wei Ying<author:sep>Xiaoxuan Xie<author:sep>Yue Ma<author:sep>Naizhong Zhang<author:sep>Hanwen Kang;http://arxiv.org/pdf/2408.06030v1;cs.RO;;gaussian splatting
2408.06019v1;http://arxiv.org/abs/2408.06019v1;2024-08-12;HeadGAP: Few-shot 3D Head Avatar via Generalizable Gaussian Priors;"In this paper, we present a novel 3D head avatar creation approach capable of
generalizing from few-shot in-the-wild data with high-fidelity and animatable
robustness. Given the underconstrained nature of this problem, incorporating
prior knowledge is essential. Therefore, we propose a framework comprising
prior learning and avatar creation phases. The prior learning phase leverages
3D head priors derived from a large-scale multi-view dynamic dataset, and the
avatar creation phase applies these priors for few-shot personalization. Our
approach effectively captures these priors by utilizing a Gaussian
Splatting-based auto-decoder network with part-based dynamic modeling. Our
method employs identity-shared encoding with personalized latent codes for
individual identities to learn the attributes of Gaussian primitives. During
the avatar creation phase, we achieve fast head avatar personalization by
leveraging inversion and fine-tuning strategies. Extensive experiments
demonstrate that our model effectively exploits head priors and successfully
generalizes them to few-shot personalization, achieving photo-realistic
rendering quality, multi-view consistency, and stable animation.";Xiaozheng Zheng<author:sep>Chao Wen<author:sep>Zhaohu Li<author:sep>Weiyi Zhang<author:sep>Zhuo Su<author:sep>Xu Chang<author:sep>Yang Zhao<author:sep>Zheng Lv<author:sep>Xiaoyuan Zhang<author:sep>Yongjie Zhang<author:sep>Guidong Wang<author:sep>Lan Xu;http://arxiv.org/pdf/2408.06019v1;cs.CV;Project page: https://headgap.github.io/;
2408.06244v1;http://arxiv.org/abs/2408.06244v1;2024-08-12;3D Reconstruction of Protein Structures from Multi-view AFM Images using  Neural Radiance Fields (NeRFs);"Recent advancements in deep learning for predicting 3D protein structures
have shown promise, particularly when leveraging inputs like protein sequences
and Cryo-Electron microscopy (Cryo-EM) images. However, these techniques often
fall short when predicting the structures of protein complexes (PCs), which
involve multiple proteins. In our study, we investigate using atomic force
microscopy (AFM) combined with deep learning to predict the 3D structures of
PCs. AFM generates height maps that depict the PCs in various random
orientations, providing a rich information for training a neural network to
predict the 3D structures. We then employ the pre-trained UpFusion model (which
utilizes a conditional diffusion model for synthesizing novel views) to train
an instance-specific NeRF model for 3D reconstruction. The performance of
UpFusion is evaluated through zero-shot predictions of 3D protein structures
using AFM images. The challenge, however, lies in the time-intensive and
impractical nature of collecting actual AFM images. To address this, we use a
virtual AFM imaging process that transforms a `PDB' protein file into
multi-view 2D virtual AFM images via volume rendering techniques. We
extensively validate the UpFusion architecture using both virtual and actual
multi-view AFM images. Our results include a comparison of structures predicted
with varying numbers of views and different sets of views. This novel approach
holds significant potential for enhancing the accuracy of protein complex
structure predictions with further fine-tuning of the UpFusion network.";Jaydeep Rade<author:sep>Ethan Herron<author:sep>Soumik Sarkar<author:sep>Anwesha Sarkar<author:sep>Adarsh Krishnamurthy;http://arxiv.org/pdf/2408.06244v1;cs.CV;;nerf
2408.06286v1;http://arxiv.org/abs/2408.06286v1;2024-08-12;Mipmap-GS: Let Gaussians Deform with Scale-specific Mipmap for  Anti-aliasing Rendering;"3D Gaussian Splatting (3DGS) has attracted great attention in novel view
synthesis because of its superior rendering efficiency and high fidelity.
However, the trained Gaussians suffer from severe zooming degradation due to
non-adjustable representation derived from single-scale training. Though some
methods attempt to tackle this problem via post-processing techniques such as
selective rendering or filtering techniques towards primitives, the
scale-specific information is not involved in Gaussians. In this paper, we
propose a unified optimization method to make Gaussians adaptive for arbitrary
scales by self-adjusting the primitive properties (e.g., color, shape and size)
and distribution (e.g., position). Inspired by the mipmap technique, we design
pseudo ground-truth for the target scale and propose a scale-consistency
guidance loss to inject scale information into 3D Gaussians. Our method is a
plug-in module, applicable for any 3DGS models to solve the zoom-in and
zoom-out aliasing. Extensive experiments demonstrate the effectiveness of our
method. Notably, our method outperforms 3DGS in PSNR by an average of 9.25 dB
for zoom-in and 10.40 dB for zoom-out on the NeRF Synthetic dataset.";Jiameng Li<author:sep>Yue Shi<author:sep>Jiezhang Cao<author:sep>Bingbing Ni<author:sep>Wenjun Zhang<author:sep>Kai Zhang<author:sep>Luc Van Gool;http://arxiv.org/pdf/2408.06286v1;cs.CV;9 pages;gaussian splatting<tag:sep>nerf
2408.05533v1;http://arxiv.org/abs/2408.05533v1;2024-08-10;Radiance Field Learners As UAV First-Person Viewers;"First-Person-View (FPV) holds immense potential for revolutionizing the
trajectory of Unmanned Aerial Vehicles (UAVs), offering an exhilarating avenue
for navigating complex building structures. Yet, traditional Neural Radiance
Field (NeRF) methods face challenges such as sampling single points per
iteration and requiring an extensive array of views for supervision. UAV videos
exacerbate these issues with limited viewpoints and significant spatial scale
variations, resulting in inadequate detail rendering across diverse scales. In
response, we introduce FPV-NeRF, addressing these challenges through three key
facets: (1) Temporal consistency. Leveraging spatio-temporal continuity ensures
seamless coherence between frames; (2) Global structure. Incorporating various
global features during point sampling preserves space integrity; (3) Local
granularity. Employing a comprehensive framework and multi-resolution
supervision for multi-scale scene feature representation tackles the
intricacies of UAV video spatial scales. Additionally, due to the scarcity of
publicly available FPV videos, we introduce an innovative view synthesis method
using NeRF to generate FPV perspectives from UAV footage, enhancing spatial
perception for drones. Our novel dataset spans diverse trajectories, from
outdoor to indoor environments, in the UAV domain, differing significantly from
traditional NeRF scenarios. Through extensive experiments encompassing both
interior and exterior building structures, FPV-NeRF demonstrates a superior
understanding of the UAV flying space, outperforming state-of-the-art methods
in our curated UAV dataset. Explore our project page for further insights:
https://fpv-nerf.github.io/.";Liqi Yan<author:sep>Qifan Wang<author:sep>Junhan Zhao<author:sep>Qiang Guan<author:sep>Zheng Tang<author:sep>Jianhui Zhang<author:sep>Dongfang Liu;http://arxiv.org/pdf/2408.05533v1;cs.CV;Accepted to ECCV 2024;nerf
2408.05635v1;http://arxiv.org/abs/2408.05635v1;2024-08-10;Visual SLAM with 3D Gaussian Primitives and Depth Priors Enabling Novel  View Synthesis;"Conventional geometry-based SLAM systems lack dense 3D reconstruction
capabilities since their data association usually relies on feature
correspondences. Additionally, learning-based SLAM systems often fall short in
terms of real-time performance and accuracy. Balancing real-time performance
with dense 3D reconstruction capabilities is a challenging problem. In this
paper, we propose a real-time RGB-D SLAM system that incorporates a novel view
synthesis technique, 3D Gaussian Splatting, for 3D scene representation and
pose estimation. This technique leverages the real-time rendering performance
of 3D Gaussian Splatting with rasterization and allows for differentiable
optimization in real time through CUDA implementation. We also enable mesh
reconstruction from 3D Gaussians for explicit dense 3D reconstruction. To
estimate accurate camera poses, we utilize a rotation-translation decoupled
strategy with inverse optimization. This involves iteratively updating both in
several iterations through gradient-based optimization. This process includes
differentiably rendering RGB, depth, and silhouette maps and updating the
camera parameters to minimize a combined loss of photometric loss, depth
geometry loss, and visibility loss, given the existing 3D Gaussian map.
However, 3D Gaussian Splatting (3DGS) struggles to accurately represent
surfaces due to the multi-view inconsistency of 3D Gaussians, which can lead to
reduced accuracy in both camera pose estimation and scene reconstruction. To
address this, we utilize depth priors as additional regularization to enforce
geometric constraints, thereby improving the accuracy of both pose estimation
and 3D reconstruction. We also provide extensive experimental results on public
benchmark datasets to demonstrate the effectiveness of our proposed methods in
terms of pose accuracy, geometric accuracy, and rendering performance.";Zhongche Qu<author:sep>Zhi Zhang<author:sep>Cong Liu<author:sep>Jianhua Yin;http://arxiv.org/pdf/2408.05635v1;cs.CV;;gaussian splatting
2408.04831v2;http://arxiv.org/abs/2408.04831v2;2024-08-09;Self-augmented Gaussian Splatting with Structure-aware Masks for  Sparse-view 3D Reconstruction;"Sparse-view 3D reconstruction stands as a formidable challenge in computer
vision, aiming to build complete three-dimensional models from a limited array
of viewing perspectives. This task confronts several difficulties: 1) the
limited number of input images that lack consistent information; 2) dependence
on the quality of input images; and 3) the substantial size of model
parameters. To address these challenges, we propose a self-augmented
coarse-to-fine Gaussian splatting paradigm, enhanced with a structure-aware
mask, for sparse-view 3D reconstruction. In particular, our method initially
employs a coarse Gaussian model to obtain a basic 3D representation from
sparse-view inputs. Subsequently, we develop a fine Gaussian network to enhance
consistent and detailed representation of the output with both 3D geometry
augmentation and perceptual view augmentation. During training, we design a
structure-aware masking strategy to further improve the model's robustness
against sparse inputs and noise.Experimental results on the MipNeRF360 and
OmniObject3D datasets demonstrate that the proposed method achieves
state-of-the-art performances for sparse input views in both perceptual quality
and efficiency.";Lingbei Meng<author:sep>Bi'an Du<author:sep>Wei Hu;http://arxiv.org/pdf/2408.04831v2;cs.CV;;gaussian splatting<tag:sep>nerf
2408.05008v1;http://arxiv.org/abs/2408.05008v1;2024-08-09;DreamCouple: Exploring High Quality Text-to-3D Generation Via Rectified  Flow;"The Score Distillation Sampling (SDS), which exploits pretrained
text-to-image model diffusion models as priors to 3D model training, has
achieved significant success. Currently, the flow-based diffusion model has
become a new trend for generations. Yet, adapting SDS to flow-based diffusion
models in 3D generation remains unexplored. Our work is aimed to bridge this
gap. In this paper, we adapt SDS to rectified flow and re-examine the
over-smoothing issue under this novel framework. The issue can be explained
that the model learns an average of multiple ODE trajectories. Then we propose
DreamCouple, which instead of randomly sampling noise, uses a rectified flow
model to find the coupled noise. Its Unique Couple Matching (UCM) loss guides
the model to learn different trajectories and thus solves the over-smoothing
issue. We apply our method to both NeRF and 3D Gaussian splatting and achieve
state-of-the-art performances. We also identify some other interesting open
questions such as initialization issues for NeRF and faster training
convergence. Our code will be released soon.";Hangyu Li<author:sep>Xiangxiang Chu<author:sep>Dingyuan Shi;http://arxiv.org/pdf/2408.05008v1;cs.CV;Tech Report;gaussian splatting<tag:sep>nerf
2408.04803v1;http://arxiv.org/abs/2408.04803v1;2024-08-09;FewShotNeRF: Meta-Learning-based Novel View Synthesis for Rapid  Scene-Specific Adaptation;"In this paper, we address the challenge of generating novel views of
real-world objects with limited multi-view images through our proposed
approach, FewShotNeRF. Our method utilizes meta-learning to acquire optimal
initialization, facilitating rapid adaptation of a Neural Radiance Field (NeRF)
to specific scenes. The focus of our meta-learning process is on capturing
shared geometry and textures within a category, embedded in the weight
initialization. This approach expedites the learning process of NeRFs and
leverages recent advancements in positional encodings to reduce the time
required for fitting a NeRF to a scene, thereby accelerating the inner loop
optimization of meta-learning. Notably, our method enables meta-learning on a
large number of 3D scenes to establish a robust 3D prior for various
categories. Through extensive evaluations on the Common Objects in 3D open
source dataset, we empirically demonstrate the efficacy and potential of
meta-learning in generating high-quality novel views of objects.";Piraveen Sivakumar<author:sep>Paul Janson<author:sep>Jathushan Rajasegaran<author:sep>Thanuja Ambegoda;http://arxiv.org/pdf/2408.04803v1;cs.CV;;nerf
2408.04268v1;http://arxiv.org/abs/2408.04268v1;2024-08-08;Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs  Gaussian-Based Methods;"Exploring the capabilities of Neural Radiance Fields (NeRF) and
Gaussian-based methods in the context of 3D scene reconstruction, this study
contrasts these modern approaches with traditional Simultaneous Localization
and Mapping (SLAM) systems. Utilizing datasets such as Replica and ScanNet, we
assess performance based on tracking accuracy, mapping fidelity, and view
synthesis. Findings reveal that NeRF excels in view synthesis, offering unique
capabilities in generating new perspectives from existing data, albeit at
slower processing speeds. Conversely, Gaussian-based methods provide rapid
processing and significant expressiveness but lack comprehensive scene
completion. Enhanced by global optimization and loop closure techniques, newer
methods like NICE-SLAM and SplaTAM not only surpass older frameworks such as
ORB-SLAM2 in terms of robustness but also demonstrate superior performance in
dynamic and complex environments. This comparative analysis bridges theoretical
research with practical implications, shedding light on future developments in
robust 3D scene reconstruction across various real-world applications.";Yiming Zhou<author:sep>Zixuan Zeng<author:sep>Andi Chen<author:sep>Xiaofan Zhou<author:sep>Haowei Ni<author:sep>Shiyao Zhang<author:sep>Panfeng Li<author:sep>Liangxi Liu<author:sep>Mengyao Zheng<author:sep>Xupeng Chen;http://arxiv.org/pdf/2408.04268v1;cs.CV;"Accepted by 2024 6th International Conference on Data-driven
  Optimization of Complex Systems";nerf
2408.04426v1;http://arxiv.org/abs/2408.04426v1;2024-08-08;A Review of 3D Reconstruction Techniques for Deformable Tissues in  Robotic Surgery;"As a crucial and intricate task in robotic minimally invasive surgery,
reconstructing surgical scenes using stereo or monocular endoscopic video holds
immense potential for clinical applications. NeRF-based techniques have
recently garnered attention for the ability to reconstruct scenes implicitly.
On the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly
using 3D Gaussians and projects them onto a 2D plane as a replacement for the
complex volume rendering in NeRF. However, these methods face challenges
regarding surgical scene reconstruction, such as slow inference, dynamic
scenes, and surgical tool occlusion. This work explores and reviews
state-of-the-art (SOTA) approaches, discussing their innovations and
implementation principles. Furthermore, we replicate the models and conduct
testing and evaluation on two datasets. The test results demonstrate that with
advancements in these techniques, achieving real-time, high-quality
reconstructions becomes feasible.";Mengya Xu<author:sep>Ziqi Guo<author:sep>An Wang<author:sep>Long Bai<author:sep>Hongliang Ren;http://arxiv.org/pdf/2408.04426v1;cs.CV;"To appear in MICCAI 2024 EARTH Workshop. Code availability:
  https://github.com/Epsilon404/surgicalnerf";gaussian splatting<tag:sep>nerf
2408.04249v1;http://arxiv.org/abs/2408.04249v1;2024-08-08;InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian  Splatting;"We present InstantStyleGaussian, an innovative 3D style transfer method based
on the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target
style image, it quickly generates new 3D GS scenes. Our approach operates on
pre-reconstructed GS scenes, combining diffusion models with an improved
iterative dataset update strategy. It utilizes diffusion models to generate
target style images, adds these new images to the training dataset, and uses
this dataset to iteratively update and optimize the GS scenes. Extensive
experimental results demonstrate that our method ensures high-quality stylized
scenes while offering significant advantages in style transfer speed and
consistency.";Xin-Yi Yu<author:sep>Jun-Xin Yu<author:sep>Li-Bo Zhou<author:sep>Yan Wei<author:sep>Lin-Lin Ou;http://arxiv.org/pdf/2408.04249v1;cs.CV;;gaussian splatting
2408.03825v1;http://arxiv.org/abs/2408.03825v1;2024-08-07;Towards Real-Time Gaussian Splatting: Accelerating 3DGS through  Photometric SLAM;"Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous
Localization and Mapping (VSLAM) demonstrate the generation of high-quality
volumetric reconstructions from monocular video streams. However, despite these
promising advancements, current 3DGS integrations have reduced tracking
performance and lower operating speeds compared to traditional VSLAM. To
address these issues, we propose integrating 3DGS with Direct Sparse Odometry,
a monocular photometric SLAM system. We have done preliminary experiments
showing that using Direct Sparse Odometry point cloud outputs, as opposed to
standard structure-from-motion methods, significantly shortens the training
time needed to achieve high-quality renders. Reducing 3DGS training time
enables the development of 3DGS-integrated SLAM systems that operate in
real-time on mobile hardware. These promising initial findings suggest further
exploration is warranted in combining traditional VSLAM systems with 3DGS.";Yan Song Hu<author:sep>Dayou Mao<author:sep>Yuhao Chen<author:sep>John Zelek;http://arxiv.org/pdf/2408.03825v1;cs.RO;"This extended abstract has been submitted to be presented at an IEEE
  conference. It will be made available online by IEEE but will not be
  published in IEEE Xplore. Copyright may be transferred without notice, after
  which this version may no longer be accessible";gaussian splatting
2408.03753v1;http://arxiv.org/abs/2408.03753v1;2024-08-07;3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting;"The use of 3D Gaussians as representation of radiance fields has enabled high
quality novel view synthesis at real-time rendering speed. However, the choice
of optimising the outgoing radiance of each Gaussian independently as spherical
harmonics results in unsatisfactory view dependent effects. In response to
these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian
Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering
quality. Instead of optimising a single outgoing radiance parameter, 3iGS
enhances 3DGS view-dependent effects by expressing the outgoing radiance as a
function of a local illumination field and Bidirectional Reflectance
Distribution Function (BRDF) features. We optimise a continuous incident
illumination field through a Tensorial Factorisation representation, while
separately fine-tuning the BRDF features of each 3D Gaussian relative to this
illumination field. Our methodology significantly enhances the rendering
quality of specular view-dependent effects of 3DGS, while maintaining rapid
training and rendering speeds.";Zhe Jun Tang<author:sep>Tat-Jen Cham;http://arxiv.org/pdf/2408.03753v1;cs.CV;The 18th European Conference on Computer Vision ECCV 2024;gaussian splatting
2408.03538v1;http://arxiv.org/abs/2408.03538v1;2024-08-07;PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time  High-Quality Relighting;"We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a
real-time high-quality relighting method for Gaussian splats in low-frequency
lighting environments that captures soft shadows and interreflections by
precomputing 3D Gaussian splats' radiance transfer. Existing studies have
demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields'
efficiency for dynamic lighting scenarios. However, the current relighting
method based on 3DGS still struggles to compute high-quality shadow and
indirect illumination in real time for dynamic light, leading to unrealistic
rendering results. We solve this problem by precomputing the expensive
transport simulations required for complex transfer functions like shadowing,
the resulting transfer functions are represented as dense sets of vectors or
matrices for every Gaussian splat. We introduce distinct precomputing methods
tailored for training and rendering stages, along with unique ray tracing and
indirect lighting precomputation techniques for 3D Gaussian splats to
accelerate training speed and compute accurate indirect lighting related to
environment light. Experimental analyses demonstrate that our approach achieves
state-of-the-art visual quality while maintaining competitive training times
and allows high-quality real-time (30+ fps) relighting for dynamic light and
relatively complex scenes at 1080p resolution.";Yijia Guo<author:sep>Yuanxi Bai<author:sep>Liwen Hu<author:sep>Ziyi Guo<author:sep>Mianzhi Liu<author:sep>Yu Cai<author:sep>Tiejun Huang<author:sep>Lei Ma;http://arxiv.org/pdf/2408.03538v1;cs.CV;;gaussian splatting
2408.03646v1;http://arxiv.org/abs/2408.03646v1;2024-08-07;Goal-oriented Semantic Communication for the Metaverse Application;"With the emergence of the metaverse and its role in enabling real-time
simulation and analysis of real-world counterparts, an increasing number of
personalized metaverse scenarios are being created to influence entertainment
experiences and social behaviors. However, compared to traditional image and
video entertainment applications, the exact transmission of the vast amount of
metaverse-associated information significantly challenges the capacity of
existing bit-oriented communication networks. Moreover, the current metaverse
also witnesses a growing goal shift for transmitting the meaning behind
custom-designed content, such as user-designed buildings and avatars, rather
than exact copies of physical objects. To meet this growing goal shift and
bandwidth challenge, this paper proposes a goal-oriented semantic communication
framework for metaverse application (GSCM) to explore and define semantic
information through the goal levels. Specifically, we first analyze the
traditional image communication framework in metaverse construction and then
detail our proposed semantic information along with the end-to-end wireless
communication. We then describe the designed modules of the GSCM framework,
including goal-oriented semantic information extraction, base knowledge
definition, and neural radiance field (NeRF) based metaverse construction.
Finally, numerous experiments have been conducted to demonstrate that, compared
to image communication, our proposed GSCM framework decreases transmission
latency by up to 92.6% and enhances the virtual object operation accuracy and
metaverse construction clearance by up to 45.6% and 44.7%, respectively.";Zhe Wang<author:sep>Nan Li<author:sep>Yansha Deng;http://arxiv.org/pdf/2408.03646v1;eess.SY;;nerf
2408.03822v1;http://arxiv.org/abs/2408.03822v1;2024-08-07;Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields;"3D Gaussian splatting (3DGS) has recently emerged as an alternative
representation that leverages a 3D Gaussian-based representation and introduces
an approximated volumetric rendering, achieving very fast rendering speed and
promising image quality. Furthermore, subsequent studies have successfully
extended 3DGS to dynamic 3D scenes, demonstrating its wide range of
applications. However, a significant drawback arises as 3DGS and its following
methods entail a substantial number of Gaussians to maintain the high fidelity
of the rendered images, which requires a large amount of memory and storage. To
address this critical issue, we place a specific emphasis on two key
objectives: reducing the number of Gaussian points without sacrificing
performance and compressing the Gaussian attributes, such as view-dependent
color and covariance. To this end, we propose a learnable mask strategy that
significantly reduces the number of Gaussians while preserving high
performance. In addition, we propose a compact but effective representation of
view-dependent color by employing a grid-based neural field rather than relying
on spherical harmonics. Finally, we learn codebooks to compactly represent the
geometric and temporal attributes by residual vector quantization. With model
compression techniques such as quantization and entropy coding, we consistently
show over 25x reduced storage and enhanced rendering speed compared to 3DGS for
static scenes, while maintaining the quality of the scene representation. For
dynamic scenes, our approach achieves more than 12x storage efficiency and
retains a high-quality reconstruction compared to the existing state-of-the-art
methods. Our work provides a comprehensive framework for 3D scene
representation, achieving high performance, fast training, compactness, and
real-time rendering. Our project page is available at
https://maincold2.github.io/c3dgs/.";Joo Chan Lee<author:sep>Daniel Rho<author:sep>Xiangyu Sun<author:sep>Jong Hwan Ko<author:sep>Eunbyung Park;http://arxiv.org/pdf/2408.03822v1;cs.CV;Project page: https://maincold2.github.io/c3dgs/;gaussian splatting
2408.03356v1;http://arxiv.org/abs/2408.03356v1;2024-08-06;RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel  View Synthesis;"Differentiable volumetric rendering-based methods made significant progress
in novel view synthesis. On one hand, innovative methods have replaced the
Neural Radiance Fields (NeRF) network with locally parameterized structures,
enabling high-quality renderings in a reasonable time. On the other hand,
approaches have used differentiable splatting instead of NeRF's ray casting to
optimize radiance fields rapidly using Gaussian kernels, allowing for fine
adaptation to the scene. However, differentiable ray casting of irregularly
spaced kernels has been scarcely explored, while splatting, despite enabling
fast rendering times, is susceptible to clearly visible artifacts.
  Our work closes this gap by providing a physically consistent formulation of
the emitted radiance c and density {\sigma}, decomposed with Gaussian functions
associated with Spherical Gaussians/Harmonics for all-frequency colorimetric
representation. We also introduce a method enabling differentiable ray casting
of irregularly distributed Gaussians using an algorithm that integrates
radiance fields slab by slab and leverages a BVH structure. This allows our
approach to finely adapt to the scene while avoiding splatting artifacts. As a
result, we achieve superior rendering quality compared to the state-of-the-art
while maintaining reasonable training times and achieving inference speeds of
25 FPS on the Blender dataset. Project page with videos and code:
https://raygauss.github.io/";Hugo Blanc<author:sep>Jean-Emmanuel Deschaud<author:sep>Alexis Paljic;http://arxiv.org/pdf/2408.03356v1;cs.CV;Project page with videos and code: https://raygauss.github.io/;nerf
2408.04474v1;http://arxiv.org/abs/2408.04474v1;2024-08-06;LumiGauss: High-Fidelity Outdoor Relighting with 2D Gaussian Splatting;"Decoupling lighting from geometry using unconstrained photo collections is
notoriously challenging. Solving it would benefit many users, as creating
complex 3D assets takes days of manual labor. Many previous works have
attempted to address this issue, often at the expense of output fidelity, which
questions the practicality of such methods.
  We introduce LumiGauss, a technique that tackles 3D reconstruction of scenes
and environmental lighting through 2D Gaussian Splatting. Our approach yields
high-quality scene reconstructions and enables realistic lighting synthesis
under novel environment maps. We also propose a method for enhancing the
quality of shadows, common in outdoor scenes, by exploiting spherical harmonics
properties. Our approach facilitates seamless integration with game engines and
enables the use of fast precomputed radiance transfer.
  We validate our method on the NeRF-OSR dataset, demonstrating superior
performance over baseline methods. Moreover, LumiGauss can synthesize realistic
images when applying novel environment maps.";Joanna Kaleta<author:sep>Kacper Kania<author:sep>Tomasz Trzcinski<author:sep>Marek Kowalski;http://arxiv.org/pdf/2408.04474v1;cs.CV;Includes video files in src;gaussian splatting<tag:sep>nerf
2408.03060v1;http://arxiv.org/abs/2408.03060v1;2024-08-06;MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View  Images;"Over the last few decades, image-based building surface reconstruction has
garnered substantial research interest and has been applied across various
fields, such as heritage preservation, architectural planning, etc. Compared to
the traditional photogrammetric and NeRF-based solutions, recently, Gaussian
fields-based methods have exhibited significant potential in generating surface
meshes due to their time-efficient training and detailed 3D information
preservation. However, most gaussian fields-based methods are trained with all
image pixels, encompassing building and nonbuilding areas, which results in a
significant noise for building meshes and degeneration in time efficiency. This
paper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to
generate accurate surface reconstruction for building in a time-efficient way.
The framework first applies EfficientSAM and COLMAP to generate multi-level
masks of building and the corresponding masked point clouds. Subsequently, the
masked gaussian fields are trained by integrating two innovative losses: a
multi-level perceptual masked loss focused on constructing building regions and
a boundary loss aimed at enhancing the details of the boundaries between
different masks. Finally, we improve the tetrahedral surface mesh extraction
method based on the masked gaussian spheres. Comprehensive experiments on UAV
images demonstrate that, compared to the traditional method and several
NeRF-based and Gaussian-based SOTA solutions, our approach significantly
improves both the accuracy and efficiency of building surface reconstruction.
Notably, as a byproduct, there is an additional gain in the novel view
synthesis of building.";Tengfei Wang<author:sep>Zongqian Zhan<author:sep>Rui Xia<author:sep>Linxia Ji<author:sep>Xin Wang;http://arxiv.org/pdf/2408.03060v1;cs.CV;;nerf
2408.03193v1;http://arxiv.org/abs/2408.03193v1;2024-08-06;Efficient NeRF Optimization -- Not All Samples Remain Equally Hard;"We propose an application of online hard sample mining for efficient training
of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality
for many 3D reconstruction and rendering tasks but require substantial
computational resources. The encoding of the scene information within the NeRF
network parameters necessitates stochastic sampling. We observe that during the
training, a major part of the compute time and memory usage is spent on
processing already learnt samples, which no longer affect the model update
significantly. We identify the backward pass on the stochastic samples as the
computational bottleneck during the optimization. We thus perform the first
forward pass in inference mode as a relatively low-cost search for hard
samples. This is followed by building the computational graph and updating the
NeRF network parameters using only the hard samples. To demonstrate the
effectiveness of the proposed approach, we apply our method to Instant-NGP,
resulting in significant improvements of the view-synthesis quality over the
baseline (1 dB improvement on average per training time, or 2x speedup to reach
the same PSNR level) along with approx. 40% memory savings coming from using
only the hard samples to build the computational graph. As our method only
interfaces with the network module, we expect it to be widely applicable.";Juuso Korhonen<author:sep>Goutham Rangu<author:sep>Hamed R. Tavakoli<author:sep>Juho Kannala;http://arxiv.org/pdf/2408.03193v1;cs.CV;;nerf
2408.02053v1;http://arxiv.org/abs/2408.02053v1;2024-08-04;PanicleNeRF: low-cost, high-precision in-field phenotypingof rice  panicles with smartphone;"The rice panicle traits significantly influence grain yield, making them a
primary target for rice phenotyping studies. However, most existing techniques
are limited to controlled indoor environments and difficult to capture the rice
panicle traits under natural growth conditions. Here, we developed PanicleNeRF,
a novel method that enables high-precision and low-cost reconstruction of rice
panicle three-dimensional (3D) models in the field using smartphone. The
proposed method combined the large model Segment Anything Model (SAM) and the
small model You Only Look Once version 8 (YOLOv8) to achieve high-precision
segmentation of rice panicle images. The NeRF technique was then employed for
3D reconstruction using the images with 2D segmentation. Finally, the resulting
point clouds are processed to successfully extract panicle traits. The results
show that PanicleNeRF effectively addressed the 2D image segmentation task,
achieving a mean F1 Score of 86.9% and a mean Intersection over Union (IoU) of
79.8%, with nearly double the boundary overlap (BO) performance compared to
YOLOv8. As for point cloud quality, PanicleNeRF significantly outperformed
traditional SfM-MVS (structure-from-motion and multi-view stereo) methods, such
as COLMAP and Metashape. The panicle length was then accurately extracted with
the rRMSE of 2.94% for indica and 1.75% for japonica rice. The panicle volume
estimated from 3D point clouds strongly correlated with the grain number (R2 =
0.85 for indica and 0.82 for japonica) and grain mass (0.80 for indica and 0.76
for japonica). This method provides a low-cost solution for high-throughput
in-field phenotyping of rice panicles, accelerating the efficiency of rice
breeding.";Xin Yang<author:sep>Xuqi Lu<author:sep>Pengyao Xie<author:sep>Ziyue Guo<author:sep>Hui Fang<author:sep>Haowei Fu<author:sep>Xiaochun Hu<author:sep>Zhenbiao Sun<author:sep>Haiyan Cen;http://arxiv.org/pdf/2408.02053v1;cs.CV;;nerf
2408.01878v1;http://arxiv.org/abs/2408.01878v1;2024-08-03;FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and  Fisheye Neural Radiance Fields;"Previous studies aiming to optimize and bundle-adjust camera poses using
Neural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated
impressive capabilities in 3D scene reconstruction. However, these approaches
have been designed for pinhole-camera pose optimization and do not perform well
under radial image distortions such as those in fisheye cameras. Furthermore,
inaccurate depth initialization in DBARF results in erroneous geometric
information affecting the overall convergence and quality of results. In this
paper, we propose adaptive GRUs with a flexible bundle-adjustment method
adapted to radial distortions and incorporate feature-based recurrent neural
networks to generate continuous novel views from fisheye datasets. Other NeRF
methods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray
distance loss for distorted pose refinement, causing severe artifacts, long
rendering time, and are difficult to use in downstream tasks, where the dense
voxel representation generated by a NeRF method needs to be converted into a
mesh representation. We also address depth initialization issues by adding
MiDaS-based depth priors for pinhole images. Through extensive experiments, we
demonstrate the generalization capacity of FBINeRF and show high-fidelity
results for both pinhole-camera and fisheye-camera NeRFs.";Yifan Wu<author:sep>Tianyi Cheng<author:sep>Peixu Xin<author:sep>Janusz Konrad;http://arxiv.org/pdf/2408.01878v1;cs.CV;18 pages;nerf
2408.01840v1;http://arxiv.org/abs/2408.01840v1;2024-08-03;E$^3$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry  Images;"Neural Radiance Fields (NeRF) achieve impressive rendering performance by
learning volumetric 3D representation from several images of different views.
However, it is difficult to reconstruct a sharp NeRF from blurry input as it
often occurs in the wild. To solve this problem, we propose a novel Efficient
Event-Enhanced NeRF (E$^3$NeRF) by utilizing the combination of RGB images and
event streams. To effectively introduce event streams into the neural
volumetric representation learning process, we propose an event-enhanced blur
rendering loss and an event rendering loss, which guide the network via
modeling the real blur process and event generation process, respectively.
Specifically, we leverage spatial-temporal information from the event stream to
evenly distribute learning attention over temporal blur while simultaneously
focusing on blurry texture through the spatial attention. Moreover, a camera
pose estimation framework for real-world data is built with the guidance of the
events to generalize the method to practical applications. Compared to previous
image-based or event-based NeRF, our framework makes more profound use of the
internal relationship between events and images. Extensive experiments on both
synthetic data and real-world data demonstrate that E$^3$NeRF can effectively
learn a sharp NeRF from blurry images, especially in non-uniform motion and
low-light scenes.";Yunshan Qi<author:sep>Jia Li<author:sep>Yifan Zhao<author:sep>Yu Zhang<author:sep>Lin Zhu;http://arxiv.org/pdf/2408.01840v1;cs.CV;;nerf
2408.01126v2;http://arxiv.org/abs/2408.01126v2;2024-08-02;IG-SLAM: Instant Gaussian SLAM;"3D Gaussian Splatting has recently shown promising results as an alternative
scene representation in SLAM systems to neural implicit representations.
However, current methods either lack dense depth maps to supervise the mapping
process or detailed training designs that consider the scale of the
environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only
SLAM system that employs robust Dense-SLAM methods for tracking and combines
them with Gaussian Splatting. A 3D map of the environment is constructed using
accurate pose and dense depth provided by tracking. Additionally, we utilize
depth uncertainty in map optimization to improve 3D reconstruction. Our decay
strategy in map optimization enhances convergence and allows the system to run
at 10 fps in a single process. We demonstrate competitive performance with
state-of-the-art RGB-only SLAM systems while achieving faster operation speeds.
We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC
datasets. The system achieves photo-realistic 3D reconstruction in large-scale
sequences, particularly in the EuRoC dataset.";F. Aykut Sarikamis<author:sep>A. Aydin Alatan;http://arxiv.org/pdf/2408.01126v2;cs.CV;8 pages, 3 page ref, 5 figures;gaussian splatting
2408.01269v1;http://arxiv.org/abs/2408.01269v1;2024-08-02;A General Framework to Boost 3D GS Initialization for Text-to-3D  Generation by Lexical Richness;"Text-to-3D content creation has recently received much attention, especially
with the prevalence of 3D Gaussians Splatting. In general, GS-based methods
comprise two key stages: initialization and rendering optimization. To achieve
initialization, existing works directly apply random sphere initialization or
3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such
strategies suffer from two critical yet challenging problems: 1) the final
shapes are still similar to the initial ones even after training; 2) shapes can
be produced only from simple texts, e.g., ""a dog"", not for lexically richer
texts, e.g., ""a dog is sitting on the top of the airplane"". To address these
problems, this paper proposes a novel general framework to boost the 3D GS
Initialization for text-to-3D generation upon the lexical richness. Our key
idea is to aggregate 3D Gaussians into spatially uniform voxels to represent
complex shapes while enabling the spatial interaction among the 3D Gaussians
and semantic interaction between Gaussians and texts. Specifically, we first
construct a voxelized representation, where each voxel holds a 3D Gaussian with
its position, scale, and rotation fixed while setting opacity as the sole
factor to determine a position's occupancy. We then design an initialization
network mainly consisting of two novel components: 1) Global Information
Perception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design
enables each 3D Gaussian to assimilate the spatial information from other areas
and semantic information from texts. Extensive experiments show the superiority
of our framework of high-quality 3D GS initialization against the existing
methods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.
Also, our framework can be seamlessly plugged into SoTA training frameworks,
e.g., LucidDreamer, for semantically consistent text-to-3D generation.";Lutao Jiang<author:sep>Hangyu Li<author:sep>Lin Wang;http://arxiv.org/pdf/2408.01269v1;cs.CV;;
2408.01251v1;http://arxiv.org/abs/2408.01251v1;2024-08-02;NeRFoot: Robot-Footprint Estimation for Image-Based Visual Servoing;"This paper investigates the utility of Neural Radiance Fields (NeRF) models
in extending the regions of operation of a mobile robot, controlled by
Image-Based Visual Servoing (IBVS) via static CCTV cameras. Using NeRF as a
3D-representation prior, the robot's footprint may be extrapolated
geometrically and used to train a CNN-based network to extract it online from
the robot's appearance alone. The resulting footprint results in a tighter
bound than a robot-wide bounding box, allowing the robot's controller to
prescribe more optimal trajectories and expand its safe operational floor area.";Daoxin Zhong<author:sep>Luke Robinson<author:sep>Daniele De Martini;http://arxiv.org/pdf/2408.01251v1;cs.RO;;nerf
2408.01225v1;http://arxiv.org/abs/2408.01225v1;2024-08-02;Reality Fusion: Robust Real-time Immersive Mobile Robot Teleoperation  with Volumetric Visual Data Fusion;"We introduce Reality Fusion, a novel robot teleoperation system that
localizes, streams, projects, and merges a typical onboard depth sensor with a
photorealistic, high resolution, high framerate, and wide field of view (FoV)
rendering of the complex remote environment represented as 3D Gaussian splats
(3DGS). Our framework enables robust egocentric and exocentric robot
teleoperation in immersive VR, with the 3DGS effectively extending spatial
information of a depth sensor with limited FoV and balancing the trade-off
between data streaming costs and data visual quality. We evaluated our
framework through a user study with 24 participants, which revealed that
Reality Fusion leads to significantly better user performance, situation
awareness, and user preferences. To support further research and development,
we provide an open-source implementation with an easy-to-replicate custom-made
telepresence robot, a high-performance virtual reality 3DGS renderer, and an
immersive robot control package. (Source code:
https://github.com/uhhhci/RealityFusion)";Ke Li<author:sep>Reinhard Bacher<author:sep>Susanne Schmidt<author:sep>Wim Leemans<author:sep>Frank Steinicke;http://arxiv.org/pdf/2408.01225v1;cs.RO;Accepted, to appear at IROS 2024;
2408.00860v2;http://arxiv.org/abs/2408.00860v2;2024-08-01;UlRe-NeRF: 3D Ultrasound Imaging through Neural Rendering with  Ultrasound Reflection Direction Parameterization;"Three-dimensional ultrasound imaging is a critical technology widely used in
medical diagnostics. However, traditional 3D ultrasound imaging methods have
limitations such as fixed resolution, low storage efficiency, and insufficient
contextual connectivity, leading to poor performance in handling complex
artifacts and reflection characteristics. Recently, techniques based on NeRF
(Neural Radiance Fields) have made significant progress in view synthesis and
3D reconstruction, but there remains a research gap in high-quality ultrasound
imaging. To address these issues, we propose a new model, UlRe-NeRF, which
combines implicit neural networks and explicit ultrasound volume rendering into
an ultrasound neural rendering architecture. This model incorporates reflection
direction parameterization and harmonic encoding, using a directional MLP
module to generate view-dependent high-frequency reflection intensity
estimates, and a spatial MLP module to produce the medium's physical property
parameters. These parameters are used in the volume rendering process to
accurately reproduce the propagation and reflection behavior of ultrasound
waves in the medium. Experimental results demonstrate that the UlRe-NeRF model
significantly enhances the realism and accuracy of high-fidelity ultrasound
image reconstruction, especially in handling complex medium structures.";Ziwen Guo<author:sep>Zi Fang<author:sep>Zhuang Fu;http://arxiv.org/pdf/2408.00860v2;cs.AI;;nerf
2408.00254v1;http://arxiv.org/abs/2408.00254v1;2024-08-01;LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting;"Despite the photorealistic novel view synthesis (NVS) performance achieved by
the original 3D Gaussian splatting (3DGS), its rendering quality significantly
degrades with sparse input views. This performance drop is mainly caused by the
limited number of initial points generated from the sparse input, insufficient
supervision during the training process, and inadequate regularization of the
oversized Gaussian ellipsoids. To handle these issues, we propose the
LoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis
task. In specific, we propose a loop-based Progressive Gaussian Initialization
(PGI) strategy that could iteratively densify the initialized point cloud using
the rendered pseudo images during the training process. Then, the sparse and
reliable depth from the Structure from Motion, and the window-based dense
monocular depth are leveraged to provide precise geometric supervision via the
proposed Depth-alignment Regularization (DAR). Additionally, we introduce a
novel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian
ellipsoids leading to large pixel errors. Comprehensive experiments on four
datasets demonstrate that LoopSparseGS outperforms existing state-of-the-art
methods for sparse-input novel view synthesis, across indoor, outdoor, and
object-level scenes with various image resolutions.";Zhenyu Bao<author:sep>Guibiao Liao<author:sep>Kaichen Zhou<author:sep>Kanglin Liu<author:sep>Qing Li<author:sep>Guoping Qiu;http://arxiv.org/pdf/2408.00254v1;cs.CV;13 pages, 10 figures;gaussian splatting
2408.00150v1;http://arxiv.org/abs/2408.00150v1;2024-07-31;StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive  Volume Visualization;"In volume visualization, visualization synthesis has attracted much attention
due to its ability to generate novel visualizations without following the
conventional rendering pipeline. However, existing solutions based on
generative adversarial networks often require many training images and take
significant training time. Still, issues such as low quality, consistency, and
flexibility persist. This paper introduces StyleRF-VolVis, an innovative style
transfer framework for expressive volume visualization (VolVis) via neural
radiance field (NeRF). The expressiveness of StyleRF-VolVis is upheld by its
ability to accurately separate the underlying scene geometry (i.e., content)
and color appearance (i.e., style), conveniently modify color, opacity, and
lighting of the original rendering while maintaining visual content consistency
across the views, and effectively transfer arbitrary styles from reference
images to the reconstructed 3D scene. To achieve these, we design a base NeRF
model for scene geometry extraction, a palette color network to classify
regions of the radiance field for photorealistic editing, and an unrestricted
color network to lift the color palette constraint via knowledge distillation
for non-photorealistic editing. We demonstrate the superior quality,
consistency, and flexibility of StyleRF-VolVis by experimenting with various
volume rendering scenes and reference images and comparing StyleRF-VolVis
against other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF
and SNeRF) style rendering solutions.";Kaiyuan Tang<author:sep>Chaoli Wang;http://arxiv.org/pdf/2408.00150v1;cs.GR;Accepted by IEEE VIS 2024;nerf
2407.21686v1;http://arxiv.org/abs/2407.21686v1;2024-07-31;Expressive Whole-Body 3D Gaussian Avatar;"Facial expression and hand motions are necessary to express our emotions and
interact with the world. Nevertheless, most of the 3D human avatars modeled
from a casually captured video only support body motions without facial
expressions and hand motions.In this work, we present ExAvatar, an expressive
whole-body 3D human avatar learned from a short monocular video. We design
ExAvatar as a combination of the whole-body parametric mesh model (SMPL-X) and
3D Gaussian Splatting (3DGS). The main challenges are 1) a limited diversity of
facial expressions and poses in the video and 2) the absence of 3D
observations, such as 3D scans and RGBD images. The limited diversity in the
video makes animations with novel facial expressions and poses non-trivial. In
addition, the absence of 3D observations could cause significant ambiguity in
human parts that are not observed in the video, which can result in noticeable
artifacts under novel motions. To address them, we introduce our hybrid
representation of the mesh and 3D Gaussians. Our hybrid representation treats
each 3D Gaussian as a vertex on the surface with pre-defined connectivity
information (i.e., triangle faces) between them following the mesh topology of
SMPL-X. It makes our ExAvatar animatable with novel facial expressions by
driven by the facial expression space of SMPL-X. In addition, by using
connectivity-based regularizers, we significantly reduce artifacts in novel
facial expressions and poses.";Gyeongsik Moon<author:sep>Takaaki Shiratori<author:sep>Shunsuke Saito;http://arxiv.org/pdf/2407.21686v1;cs.CV;"Accepted to ECCV 2024. Project page:
  https://mks0601.github.io/ExAvatar/";gaussian splatting
2408.00083v1;http://arxiv.org/abs/2408.00083v1;2024-07-31;Localized Gaussian Splatting Editing with Contextual Awareness;"Recent text-guided generation of individual 3D object has achieved great
success using diffusion priors. However, these methods are not suitable for
object insertion and replacement tasks as they do not consider the background,
leading to illumination mismatches within the environment. To bridge the gap,
we introduce an illumination-aware 3D scene editing pipeline for 3D Gaussian
Splatting (3DGS) representation. Our key observation is that inpainting by the
state-of-the-art conditional 2D diffusion model is consistent with background
in lighting. To leverage the prior knowledge from the well-trained diffusion
models for 3D object generation, our approach employs a coarse-to-fine
objection optimization pipeline with inpainted views. In the first coarse step,
we achieve image-to-3D lifting given an ideal inpainted view. The process
employs 3D-aware diffusion prior from a view-conditioned diffusion model, which
preserves illumination present in the conditioning image. To acquire an ideal
inpainted image, we introduce an Anchor View Proposal (AVP) algorithm to find a
single view that best represents the scene illumination in target region. In
the second Texture Enhancement step, we introduce a novel Depth-guided
Inpainting Score Distillation Sampling (DI-SDS), which enhances geometry and
texture details with the inpainting diffusion prior, beyond the scope of the
3D-aware diffusion prior knowledge in the first coarse step. DI-SDS not only
provides fine-grained texture enhancement, but also urges optimization to
respect scene lighting. Our approach efficiently achieves local editing with
global illumination consistency without explicitly modeling light transport. We
demonstrate robustness of our method by evaluating editing in real scenes
containing explicit highlight and shadows, and compare against the
state-of-the-art text-to-3D editing methods.";Hanyuan Xiao<author:sep>Yingshu Chen<author:sep>Huajian Huang<author:sep>Haolin Xiong<author:sep>Jing Yang<author:sep>Pratusha Prasad<author:sep>Yajie Zhao;http://arxiv.org/pdf/2408.00083v1;cs.CV;;gaussian splatting
2407.20908v1;http://arxiv.org/abs/2407.20908v1;2024-07-30;Dynamic Scene Understanding through Object-Centric Voxelization and  Neural Rendering;"Learning object-centric representations from unsupervised videos is
challenging. Unlike most previous approaches that focus on decomposing 2D
images, we present a 3D generative model named DynaVol-S for dynamic scenes
that enables object-centric learning within a differentiable volume rendering
framework. The key idea is to perform object-centric voxelization to capture
the 3D nature of the scene, which infers per-object occupancy probabilities at
individual spatial locations. These voxel features evolve through a
canonical-space deformation function and are optimized in an inverse rendering
pipeline with a compositional NeRF. Additionally, our approach integrates 2D
semantic features to create 3D semantic grids, representing the scene through
multiple disentangled voxel grids. DynaVol-S significantly outperforms existing
models in both novel view synthesis and unsupervised decomposition tasks for
dynamic scenes. By jointly considering geometric structures and semantic
features, it effectively addresses challenging real-world scenarios involving
complex object interactions. Furthermore, once trained, the explicitly
meaningful voxel features enable additional capabilities that 2D scene
decomposition methods cannot achieve, such as novel scene generation through
editing geometric shapes or manipulating the motion trajectories of objects.";Yanpeng Zhao<author:sep>Yiwei Hao<author:sep>Siyu Gao<author:sep>Yunbo Wang<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2407.20908v1;cs.CV;;nerf
2407.20727v1;http://arxiv.org/abs/2407.20727v1;2024-07-30;SceneTeller: Language-to-3D Scene Generation;"Designing high-quality indoor 3D scenes is important in many practical
applications, such as room planning or game development. Conventionally, this
has been a time-consuming process which requires both artistic skill and
familiarity with professional software, making it hardly accessible for layman
users. However, recent advances in generative AI have established solid
foundation for democratizing 3D design. In this paper, we propose a pioneering
approach for text-based 3D room design. Given a prompt in natural language
describing the object placement in the room, our method produces a high-quality
3D scene corresponding to it. With an additional text prompt the users can
change the appearance of the entire scene or of individual objects in it. Built
using in-context learning, CAD model retrieval and 3D-Gaussian-Splatting-based
stylization, our turnkey pipeline produces state-of-the-art 3D scenes, while
being easy to use even for novices. Our project page is available at
https://sceneteller.github.io/.";Başak Melis Öcal<author:sep>Maxim Tatarchenko<author:sep>Sezer Karaoglu<author:sep>Theo Gevers;http://arxiv.org/pdf/2407.20727v1;cs.CV;ECCV'24 camera-ready version;
2407.20194v1;http://arxiv.org/abs/2407.20194v1;2024-07-29;Radiance Fields for Robotic Teleoperation;"Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian
Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their
ability to synthesize new viewpoints with photo-realistic quality, as well as
capture complex volumetric and specular scenes, makes them an ideal
visualization for robotic teleoperation setups. Direct camera teleoperation
provides high-fidelity operation at the cost of maneuverability, while
reconstruction-based approaches offer controllable scenes with lower fidelity.
With this in mind, we propose replacing the traditional
reconstruction-visualization components of the robotic teleoperation pipeline
with online Radiance Fields, offering highly maneuverable scenes with
photorealistic quality. As such, there are three main contributions to state of
the art: (1) online training of Radiance Fields using live data from multiple
cameras, (2) support for a variety of radiance methods including NeRF and 3DGS,
(3) visualization suite for these methods including a virtual reality scene. To
enable seamless integration with existing setups, these components were tested
with multiple robots in multiple configurations and were displayed using
traditional tools as well as the VR headset. The results across methods and
robots were compared quantitatively to a baseline of mesh reconstruction, and a
user study was conducted to compare the different visualization methods. For
videos and code, check out https://leggedrobotics.github.io/rffr.github.io/.";Maximum Wilder-Smith<author:sep>Vaishakh Patil<author:sep>Marco Hutter;http://arxiv.org/pdf/2407.20194v1;cs.RO;8 pages, 10 figures, Accepted to IROS 2024;nerf
2407.19774v1;http://arxiv.org/abs/2407.19774v1;2024-07-29;Garment Animation NeRF with Color Editing;"Generating high-fidelity garment animations through traditional workflows,
from modeling to rendering, is both tedious and expensive. These workflows
often require repetitive steps in response to updates in character motion,
rendering viewpoint changes, or appearance edits. Although recent neural
rendering offers an efficient solution for computationally intensive processes,
it struggles with rendering complex garment animations containing fine wrinkle
details and realistic garment-and-body occlusions, while maintaining structural
consistency across frames and dense view rendering. In this paper, we propose a
novel approach to directly synthesize garment animations from body motion
sequences without the need for an explicit garment proxy. Our approach infers
garment dynamic features from body motion, providing a preliminary overview of
garment structure. Simultaneously, we capture detailed features from
synthesized reference images of the garment's front and back, generated by a
pre-trained image model. These features are then used to construct a neural
radiance field that renders the garment animation video. Additionally, our
technique enables garment recoloring by decomposing its visual elements. We
demonstrate the generalizability of our method across unseen body motions and
camera views, ensuring detailed structural consistency. Furthermore, we
showcase its applicability to color editing on both real and synthetic garment
data. Compared to existing neural rendering techniques, our method exhibits
qualitative and quantitative improvements in garment dynamics and wrinkle
detail modeling. Code is available at
\url{https://github.com/wrk226/GarmentAnimationNeRF}.";Renke Wang<author:sep>Meng Zhang<author:sep>Jun Li<author:sep>Jian Yan;http://arxiv.org/pdf/2407.19774v1;cs.CV;;nerf
2407.20213v1;http://arxiv.org/abs/2407.20213v1;2024-07-29;Registering Neural 4D Gaussians for Endoscopic Surgery;"The recent advance in neural rendering has enabled the ability to reconstruct
high-quality 4D scenes using neural networks. Although 4D neural reconstruction
is popular, registration for such representations remains a challenging task,
especially for dynamic scene registration in surgical planning and simulation.
In this paper, we propose a novel strategy for dynamic surgical neural scene
registration. We first utilize 4D Gaussian Splatting to represent the surgical
scene and capture both static and dynamic scenes effectively. Then, a spatial
aware feature aggregation method, Spatially Weight Cluttering (SWC) is proposed
to accurately align the feature between surgical scenes, enabling precise and
realistic surgical simulations. Lastly, we present a novel strategy of
deformable scene registration to register two dynamic scenes. By incorporating
both spatial and temporal information for correspondence matching, our approach
achieves superior performance compared to existing registration methods for
implicit neural representation. The proposed method has the potential to
improve surgical planning and training, ultimately leading to better patient
outcomes.";Yiming Huang<author:sep>Beilei Cui<author:sep>Ikemura Kei<author:sep>Jiekai Zhang<author:sep>Long Bai<author:sep>Hongliang Ren;http://arxiv.org/pdf/2407.20213v1;cs.RO;;gaussian splatting
2407.19166v2;http://arxiv.org/abs/2407.19166v2;2024-07-27;Revisit Self-supervised Depth Estimation with Local  Structure-from-Motion;"Both self-supervised depth estimation and Structure-from-Motion (SfM) recover
scene depth from RGB videos. Despite sharing a similar objective, the two
approaches are disconnected. Prior works of self-supervision backpropagate
losses defined within immediate neighboring frames. Instead of
learning-through-loss, this work proposes an alternative scheme by performing
local SfM. First, with calibrated RGB or RGB-D images, we employ a depth and
correspondence estimator to infer depthmaps and pair-wise correspondence maps.
Then, a novel bundle-RANSAC-adjustment algorithm jointly optimizes camera poses
and one depth adjustment for each depthmap. Finally, we fix camera poses and
employ a NeRF, however, without a neural network, for dense triangulation and
geometric verification. Poses, depth adjustments, and triangulated sparse
depths are our outputs. For the first time, we show self-supervision within $5$
frames already benefits SoTA supervised depth and correspondence models. The
project page is held in the link (https://shngjz.github.io/SSfM.github.io/).";Shengjie Zhu<author:sep>Xiaoming Liu;http://arxiv.org/pdf/2407.19166v2;cs.CV;;nerf
2407.19035v1;http://arxiv.org/abs/2407.19035v1;2024-07-26;ScalingGaussian: Enhancing 3D Content Creation with Generative Gaussian  Splatting;"The creation of high-quality 3D assets is paramount for applications in
digital heritage preservation, entertainment, and robotics. Traditionally, this
process necessitates skilled professionals and specialized software for the
modeling, texturing, and rendering of 3D objects. However, the rising demand
for 3D assets in gaming and virtual reality (VR) has led to the creation of
accessible image-to-3D technologies, allowing non-professionals to produce 3D
content and decreasing dependence on expert input. Existing methods for 3D
content generation struggle to simultaneously achieve detailed textures and
strong geometric consistency. We introduce a novel 3D content creation
framework, ScalingGaussian, which combines 3D and 2D diffusion models to
achieve detailed textures and geometric consistency in generated 3D assets.
Initially, a 3D diffusion model generates point clouds, which are then
densified through a process of selecting local regions, introducing Gaussian
noise, followed by using local density-weighted selection. To refine the 3D
gaussians, we utilize a 2D diffusion model with Score Distillation Sampling
(SDS) loss, guiding the 3D Gaussians to clone and split. Finally, the 3D
Gaussians are converted into meshes, and the surface textures are optimized
using Mean Square Error(MSE) and Gradient Profile Prior(GPP) losses. Our method
addresses the common issue of sparse point clouds in 3D diffusion, resulting in
improved geometric structure and detailed textures. Experiments on image-to-3D
tasks demonstrate that our approach efficiently generates high-quality 3D
assets.";Shen Chen<author:sep>Jiale Zhou<author:sep>Zhongyu Jiang<author:sep>Tianfang Zhang<author:sep>Zongkai Wu<author:sep>Jenq-Neng Hwang<author:sep>Lei Li;http://arxiv.org/pdf/2407.19035v1;cs.CV;14 pages;
2407.18611v1;http://arxiv.org/abs/2407.18611v1;2024-07-26;IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs;"Urban-level three-dimensional reconstruction for modern applications demands
high rendering fidelity while minimizing computational costs. The advent of
Neural Radiance Fields (NeRF) has enhanced 3D reconstruction, yet it exhibits
artifacts under multiple viewpoints. In this paper, we propose a new NeRF
framework method to address these issues. Our method uses image content and
pose data to iteratively plan the next best view. A crucial aspect of this
method involves uncertainty estimation, guiding the selection of views with
maximum information gain from a candidate set. This iterative process enhances
rendering quality over time. Simultaneously, we introduce the Vonoroi diagram
and threshold sampling together with flight classifier to boost the efficiency,
while keep the original NeRF network intact. It can serve as a plug-in tool to
assist in better rendering, outperforming baselines and similar prior works.";Jingpeng Xie<author:sep>Shiyu Tan<author:sep>Yuanlei Wang<author:sep>Yizhen Lao;http://arxiv.org/pdf/2407.18611v1;cs.CV;;nerf
2407.18046v1;http://arxiv.org/abs/2407.18046v1;2024-07-25;GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale  Image Super-Resolution;"Implicit neural representations (INRs) have significantly advanced the field
of arbitrary-scale super-resolution (ASSR) of images. Most existing INR-based
ASSR networks first extract features from the given low-resolution image using
an encoder, and then render the super-resolved result via a multi-layer
perceptron decoder. Although these approaches have shown promising results,
their performance is constrained by the limited representation ability of
discrete latent codes in the encoded features. In this paper, we propose a
novel ASSR method named GaussianSR that overcomes this limitation through 2D
Gaussian Splatting (2DGS). Unlike traditional methods that treat pixels as
discrete points, GaussianSR represents each pixel as a continuous Gaussian
field. The encoded features are simultaneously refined and upsampled by
rendering the mutually stacked Gaussian fields. As a result, long-range
dependencies are established to enhance representation ability. In addition, a
classifier is developed to dynamically assign Gaussian kernels to all pixels to
further improve flexibility. All components of GaussianSR (i.e., encoder,
classifier, Gaussian kernels, and decoder) are jointly learned end-to-end.
Experiments demonstrate that GaussianSR achieves superior ASSR performance with
fewer parameters than existing methods while enjoying interpretable and
content-aware feature aggregations.";Jintong Hu<author:sep>Bin Xia<author:sep>Bin Chen<author:sep>Wenming Yang<author:sep>Lei Zhang;http://arxiv.org/pdf/2407.18046v1;cs.CV;13 pages, 12 figures;gaussian splatting
2407.17470v1;http://arxiv.org/abs/2407.17470v1;2024-07-24;SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View  Consistency;"We present Stable Video 4D (SV4D), a latent video diffusion model for
multi-frame and multi-view consistent dynamic 3D content generation. Unlike
previous methods that rely on separately trained generative models for video
generation and novel view synthesis, we design a unified diffusion model to
generate novel view videos of dynamic 3D objects. Specifically, given a
monocular reference video, SV4D generates novel views for each video frame that
are temporally consistent. We then use the generated novel view videos to
optimize an implicit 4D representation (dynamic NeRF) efficiently, without the
need for cumbersome SDS-based optimization used in most prior works. To train
our unified novel view video generation model, we curated a dynamic 3D object
dataset from the existing Objaverse dataset. Extensive experimental results on
multiple datasets and user studies demonstrate SV4D's state-of-the-art
performance on novel-view video synthesis as well as 4D generation compared to
prior works.";Yiming Xie<author:sep>Chun-Han Yao<author:sep>Vikram Voleti<author:sep>Huaizu Jiang<author:sep>Varun Jampani;http://arxiv.org/pdf/2407.17470v1;cs.CV;Project page: https://sv4d.github.io/;nerf
2407.17418v1;http://arxiv.org/abs/2407.17418v1;2024-07-24;3D Gaussian Splatting: Survey, Technologies, Challenges, and  Opportunities;"3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the
potential to become a mainstream method for 3D representations. It can
effectively transform multi-view images into explicit 3D Gaussian
representations through efficient training, and achieve real-time rendering of
novel views. This survey aims to analyze existing 3DGS-related works from
multiple intersecting perspectives, including related tasks, technologies,
challenges, and opportunities. The primary objective is to provide newcomers
with a rapid understanding of the field and to assist researchers in
methodically organizing existing technologies and challenges. Specifically, we
delve into the optimization, application, and extension of 3DGS, categorizing
them based on their focuses or motivations. Additionally, we summarize and
classify nine types of technical modules and corresponding improvements
identified in existing works. Based on these analyses, we further examine the
common challenges and technologies across various tasks, proposing potential
research opportunities.";Yanqi Bao<author:sep>Tianyu Ding<author:sep>Jing Huo<author:sep>Yaoli Liu<author:sep>Yuxin Li<author:sep>Wenbin Li<author:sep>Yang Gao<author:sep>Jiebo Luo;http://arxiv.org/pdf/2407.17418v1;cs.CV;;gaussian splatting
2407.16260v1;http://arxiv.org/abs/2407.16260v1;2024-07-23;DreamDissector: Learning Disentangled Text-to-3D Generation from 2D  Diffusion Priors;"Text-to-3D generation has recently seen significant progress. To enhance its
practicality in real-world applications, it is crucial to generate multiple
independent objects with interactions, similar to layer-compositing in 2D image
editing. However, existing text-to-3D methods struggle with this task, as they
are designed to generate either non-independent objects or independent objects
lacking spatially plausible interactions. Addressing this, we propose
DreamDissector, a text-to-3D method capable of generating multiple independent
objects with interactions. DreamDissector accepts a multi-object text-to-3D
NeRF as input and produces independent textured meshes. To achieve this, we
introduce the Neural Category Field (NeCF) for disentangling the input NeRF.
Additionally, we present the Category Score Distillation Sampling (CSDS),
facilitated by a Deep Concept Mining (DCM) module, to tackle the concept gap
issue in diffusion models. By leveraging NeCF and CSDS, we can effectively
derive sub-NeRFs from the original scene. Further refinement enhances geometry
and texture. Our experimental results validate the effectiveness of
DreamDissector, providing users with novel means to control 3D synthesis at the
object level and potentially opening avenues for various creative applications
in the future.";Zizheng Yan<author:sep>Jiapeng Zhou<author:sep>Fanpeng Meng<author:sep>Yushuang Wu<author:sep>Lingteng Qiu<author:sep>Zisheng Ye<author:sep>Shuguang Cui<author:sep>Guanying Chen<author:sep>Xiaoguang Han;http://arxiv.org/pdf/2407.16260v1;cs.CV;ECCV 2024. Project page: https://chester256.github.io/dreamdissector;nerf
2407.16503v1;http://arxiv.org/abs/2407.16503v1;2024-07-23;HDRSplat: Gaussian Splatting for High Dynamic Range 3D Scene  Reconstruction from Raw Images;"The recent advent of 3D Gaussian Splatting (3DGS) has revolutionized the 3D
scene reconstruction space enabling high-fidelity novel view synthesis in
real-time. However, with the exception of RawNeRF, all prior 3DGS and
NeRF-based methods rely on 8-bit tone-mapped Low Dynamic Range (LDR) images for
scene reconstruction. Such methods struggle to achieve accurate reconstructions
in scenes that require a higher dynamic range. Examples include scenes captured
in nighttime or poorly lit indoor spaces having a low signal-to-noise ratio, as
well as daylight scenes with shadow regions exhibiting extreme contrast. Our
proposed method HDRSplat tailors 3DGS to train directly on 14-bit linear raw
images in near darkness which preserves the scenes' full dynamic range and
content. Our key contributions are two-fold: Firstly, we propose a linear HDR
space-suited loss that effectively extracts scene information from noisy dark
regions and nearly saturated bright regions simultaneously, while also handling
view-dependent colors without increasing the degree of spherical harmonics.
Secondly, through careful rasterization tuning, we implicitly overcome the
heavy reliance and sensitivity of 3DGS on point cloud initialization. This is
critical for accurate reconstruction in regions of low texture, high depth of
field, and low illumination. HDRSplat is the fastest method to date that does
14-bit (HDR) 3D scene reconstruction in $\le$15 minutes/scene ($\sim$30x faster
than prior state-of-the-art RawNeRF). It also boasts the fastest inference
speed at $\ge$120fps. We further demonstrate the applicability of our HDR scene
reconstruction by showcasing various applications like synthetic defocus, dense
depth map extraction, and post-capture control of exposure, tone-mapping and
view-point.";Shreyas Singh<author:sep>Aryan Garg<author:sep>Kaushik Mitra;http://arxiv.org/pdf/2407.16503v1;cs.CV;;gaussian splatting<tag:sep>nerf
2407.16600v2;http://arxiv.org/abs/2407.16600v2;2024-07-23;DHGS: Decoupled Hybrid Gaussian Splatting for Driving Scene;"Existing Gaussian splatting methods often fall short in achieving
satisfactory novel view synthesis in driving scenes, primarily due to the
absence of crafty design and geometric constraints for the involved elements.
This paper introduces a novel neural rendering method termed Decoupled Hybrid
Gaussian Splatting (DHGS), targeting at promoting the rendering quality of
novel view synthesis for static driving scenes. The novelty of this work lies
in the decoupled and hybrid pixel-level blender for road and non-road layers,
without the conventional unified differentiable rendering logic for the entire
scene, while still maintaining consistent and continuous superimposition
through the proposed depth-ordered hybrid rendering strategy. Additionally, an
implicit road representation comprised of a Signed Distance Field (SDF) is
trained to supervise the road surface with subtle geometric attributes.
Accompanied by the use of auxiliary transmittance loss and consistency loss,
novel images with imperceptible boundary and elevated fidelity are ultimately
obtained. Substantial experiments on the Waymo dataset prove that DHGS
outperforms the state-of-the-art methods. The project page where more video
evidences are given is: https://ironbrotherstyle.github.io/dhgs_web.";Xi Shi<author:sep>Lingli Chen<author:sep>Peng Wei<author:sep>Xi Wu<author:sep>Tian Jiang<author:sep>Yonggang Luo<author:sep>Lecheng Xie;http://arxiv.org/pdf/2407.16600v2;cs.CV;13 pages, 14 figures, conference;gaussian splatting
2407.16173v1;http://arxiv.org/abs/2407.16173v1;2024-07-23;Integrating Meshes and 3D Gaussians for Indoor Scene Reconstruction with  SAM Mask Guidance;"We present a novel approach for 3D indoor scene reconstruction that combines
3D Gaussian Splatting (3DGS) with mesh representations. We use meshes for the
room layout of the indoor scene, such as walls, ceilings, and floors, while
employing 3D Gaussians for other objects. This hybrid approach leverages the
strengths of both representations, offering enhanced flexibility and ease of
editing. However, joint training of meshes and 3D Gaussians is challenging
because it is not clear which primitive should affect which part of the
rendered image. Objects close to the room layout often struggle during
training, particularly when the room layout is textureless, which can lead to
incorrect optimizations and unnecessary 3D Gaussians. To overcome these
challenges, we employ Segment Anything Model (SAM) to guide the selection of
primitives. The SAM mask loss enforces each instance to be represented by
either Gaussians or meshes, ensuring clear separation and stable training.
Furthermore, we introduce an additional densification stage without resetting
the opacity after the standard densification. This stage mitigates the
degradation of image quality caused by a limited number of 3D Gaussians after
the standard densification.";Jiyeop Kim<author:sep>Jongwoo Lim;http://arxiv.org/pdf/2407.16173v1;cs.CV;;gaussian splatting
2407.15435v1;http://arxiv.org/abs/2407.15435v1;2024-07-22;Enhancement of 3D Gaussian Splatting using Raw Mesh for Photorealistic  Recreation of Architectures;"The photorealistic reconstruction and rendering of architectural scenes have
extensive applications in industries such as film, games, and transportation.
It also plays an important role in urban planning, architectural design, and
the city's promotion, especially in protecting historical and cultural relics.
The 3D Gaussian Splatting, due to better performance over NeRF, has become a
mainstream technology in 3D reconstruction. Its only input is a set of images
but it relies heavily on geometric parameters computed by the SfM process. At
the same time, there is an existing abundance of raw 3D models, that could
inform the structural perception of certain buildings but cannot be applied. In
this paper, we propose a straightforward method to harness these raw 3D models
to guide 3D Gaussians in capturing the basic shape of the building and improve
the visual quality of textures and details when photos are captured
non-systematically. This exploration opens up new possibilities for improving
the effectiveness of 3D reconstruction techniques in the field of architectural
design.";Ruizhe Wang<author:sep>Chunliang Hua<author:sep>Tomakayev Shingys<author:sep>Mengyuan Niu<author:sep>Qingxin Yang<author:sep>Lizhong Gao<author:sep>Yi Zheng<author:sep>Junyan Yang<author:sep>Qiao Wang;http://arxiv.org/pdf/2407.15435v1;cs.CV;;gaussian splatting<tag:sep>nerf
2407.15848v1;http://arxiv.org/abs/2407.15848v1;2024-07-22;BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis  in Large-scale Scenes;"While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality,
their protracted training duration remains a limitation. Generalizable and
MVS-based NeRFs, although capable of mitigating training time, often incur
tradeoffs in quality. This paper presents a novel approach called BoostMVSNeRFs
to enhance the rendering quality of MVS-based NeRFs in large-scale scenes. We
first identify limitations in MVS-based NeRF methods, such as restricted
viewport coverage and artifacts due to limited input views. Then, we address
these limitations by proposing a new method that selects and combines multiple
cost volumes during volume rendering. Our method does not require training and
can adapt to any MVS-based NeRF methods in a feed-forward fashion to improve
rendering quality. Furthermore, our approach is also end-to-end trainable,
allowing fine-tuning on specific scenes. We demonstrate the effectiveness of
our method through experiments on large-scale datasets, showing significant
rendering quality improvements in large-scale scenes and unbounded outdoor
scenarios. We release the source code of BoostMVSNeRFs at
https://su-terry.github.io/BoostMVSNeRFs/.";Chih-Hai Su<author:sep>Chih-Yao Hu<author:sep>Shr-Ruei Tsai<author:sep>Jie-Ying Lee<author:sep>Chin-Yang Lin<author:sep>Yu-Lun Liu;http://arxiv.org/pdf/2407.15848v1;cs.CV;"SIGGRAPH 2024 Conference Papers. Project page:
  https://su-terry.github.io/BoostMVSNeRFs/";nerf
2407.15484v1;http://arxiv.org/abs/2407.15484v1;2024-07-22;6DGS: 6D Pose Estimation from a Single Image and a 3D Gaussian Splatting  Model;"We propose 6DGS to estimate the camera pose of a target RGB image given a 3D
Gaussian Splatting (3DGS) model representing the scene. 6DGS avoids the
iterative process typical of analysis-by-synthesis methods (e.g. iNeRF) that
also require an initialization of the camera pose in order to converge.
Instead, our method estimates a 6DoF pose by inverting the 3DGS rendering
process. Starting from the object surface, we define a radiant Ellicell that
uniformly generates rays departing from each ellipsoid that parameterize the
3DGS model. Each Ellicell ray is associated with the rendering parameters of
each ellipsoid, which in turn is used to obtain the best bindings between the
target image pixels and the cast rays. These pixel-ray bindings are then ranked
to select the best scoring bundle of rays, which their intersection provides
the camera center and, in turn, the camera rotation. The proposed solution
obviates the necessity of an ""a priori"" pose for initialization, and it solves
6DoF pose estimation in closed form, without the need for iterations. Moreover,
compared to the existing Novel View Synthesis (NVS) baselines for pose
estimation, 6DGS can improve the overall average rotational accuracy by 12% and
translation accuracy by 22% on real scenes, despite not requiring any
initialization pose. At the same time, our method operates near real-time,
reaching 15fps on consumer hardware.";Matteo Bortolon<author:sep>Theodore Tsesmelis<author:sep>Stuart James<author:sep>Fabio Poiesi<author:sep>Alessio Del Bue;http://arxiv.org/pdf/2407.15484v1;cs.CV;"Project page: https://mbortolon97.github.io/6dgs/ Accepted to ECCV
  2024";gaussian splatting<tag:sep>nerf
2407.21047v1;http://arxiv.org/abs/2407.21047v1;2024-07-22;PAV: Personalized Head Avatar from Unstructured Video Collection;"We propose PAV, Personalized Head Avatar for the synthesis of human faces
under arbitrary viewpoints and facial expressions. PAV introduces a method that
learns a dynamic deformable neural radiance field (NeRF), in particular from a
collection of monocular talking face videos of the same character under various
appearance and shape changes. Unlike existing head NeRF methods that are
limited to modeling such input videos on a per-appearance basis, our method
allows for learning multi-appearance NeRFs, introducing appearance embedding
for each input video via learnable latent neural features attached to the
underlying geometry. Furthermore, the proposed appearance-conditioned density
formulation facilitates the shape variation of the character, such as facial
hair and soft tissues, in the radiance field prediction. To the best of our
knowledge, our approach is the first dynamic deformable NeRF framework to model
appearance and shape variations in a single unified network for
multi-appearances of the same subject. We demonstrate experimentally that PAV
outperforms the baseline method in terms of visual rendering quality in our
quantitative and qualitative studies on various subjects.";Akin Caliskan<author:sep>Berkay Kicanaoglu<author:sep>Hyeongwoo Kim;http://arxiv.org/pdf/2407.21047v1;cs.CV;"Accepted to ECCV24. Project page:
  https://akincaliskan3d.github.io/PAV";nerf
2407.15187v1;http://arxiv.org/abs/2407.15187v1;2024-07-21;HoloDreamer: Holistic 3D Panoramic World Generation from Text  Descriptions;"3D scene generation is in high demand across various domains, including
virtual reality, gaming, and the film industry. Owing to the powerful
generative capabilities of text-to-image diffusion models that provide reliable
priors, the creation of 3D scenes using only text prompts has become viable,
thereby significantly advancing researches in text-driven 3D scene generation.
In order to obtain multiple-view supervision from 2D diffusion models,
prevailing methods typically employ the diffusion model to generate an initial
local image, followed by iteratively outpainting the local image using
diffusion models to gradually generate scenes. Nevertheless, these
outpainting-based approaches prone to produce global inconsistent scene
generation results without high degree of completeness, restricting their
broader applications. To tackle these problems, we introduce HoloDreamer, a
framework that first generates high-definition panorama as a holistic
initialization of the full 3D scene, then leverage 3D Gaussian Splatting
(3D-GS) to quickly reconstruct the 3D scene, thereby facilitating the creation
of view-consistent and fully enclosed 3D scenes. Specifically, we propose
Stylized Equirectangular Panorama Generation, a pipeline that combines multiple
diffusion models to enable stylized and detailed equirectangular panorama
generation from complex text prompts. Subsequently, Enhanced Two-Stage Panorama
Reconstruction is introduced, conducting a two-stage optimization of 3D-GS to
inpaint the missing region and enhance the integrity of the scene.
Comprehensive experiments demonstrated that our method outperforms prior works
in terms of overall visual consistency and harmony as well as reconstruction
quality and rendering robustness when generating fully enclosed scenes.";Haiyang Zhou<author:sep>Xinhua Cheng<author:sep>Wangbo Yu<author:sep>Yonghong Tian<author:sep>Li Yuan;http://arxiv.org/pdf/2407.15187v1;cs.CV;Homepage: https://zhouhyocean.github.io/holodreamer;gaussian splatting
2407.14846v1;http://arxiv.org/abs/2407.14846v1;2024-07-20;Realistic Surgical Image Dataset Generation Based On 3D Gaussian  Splatting;"Computer vision technologies markedly enhance the automation capabilities of
robotic-assisted minimally invasive surgery (RAMIS) through advanced tool
tracking, detection, and localization. However, the limited availability of
comprehensive surgical datasets for training represents a significant challenge
in this field. This research introduces a novel method that employs 3D Gaussian
Splatting to generate synthetic surgical datasets. We propose a method for
extracting and combining 3D Gaussian representations of surgical instruments
and background operating environments, transforming and combining them to
generate high-fidelity synthetic surgical scenarios. We developed a data
recording system capable of acquiring images alongside tool and camera poses in
a surgical scene. Using this pose data, we synthetically replicate the scene,
thereby enabling direct comparisons of the synthetic image quality (29.592
PSNR). As a further validation, we compared two YOLOv5 models trained on the
synthetic and real data, respectively, and assessed their performance in an
unseen real-world test dataset. Comparing the performances, we observe an
improvement in neural network performance, with the synthetic-trained model
outperforming the real-world trained model by 12%, testing both on real-world
data.";Tianle Zeng<author:sep>Gerardo Loza Galindo<author:sep>Junlei Hu<author:sep>Pietro Valdastri<author:sep>Dominic Jones;http://arxiv.org/pdf/2407.14846v1;cs.CV;"This paper has already been accepted by INTERNATIONAL CONFERENCE ON
  MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION (MICCAI 2024)";
2407.14108v1;http://arxiv.org/abs/2407.14108v1;2024-07-19;GaussianBeV: 3D Gaussian Representation meets Perception Models for BeV  Segmentation;"The Bird's-eye View (BeV) representation is widely used for 3D perception
from multi-view camera images. It allows to merge features from different
cameras into a common space, providing a unified representation of the 3D
scene. The key component is the view transformer, which transforms image views
into the BeV. However, actual view transformer methods based on geometry or
cross-attention do not provide a sufficiently detailed representation of the
scene, as they use a sub-sampling of the 3D space that is non-optimal for
modeling the fine structures of the environment. In this paper, we propose
GaussianBeV, a novel method for transforming image features to BeV by finely
representing the scene using a set of 3D gaussians located and oriented in 3D
space. This representation is then splattered to produce the BeV feature map by
adapting recent advances in 3D representation rendering based on gaussian
splatting. GaussianBeV is the first approach to use this 3D gaussian modeling
and 3D scene rendering process online, i.e. without optimizing it on a specific
scene and directly integrated into a single stage model for BeV scene
understanding. Experiments show that the proposed representation is highly
effective and place GaussianBeV as the new state-of-the-art on the BeV semantic
segmentation task on the nuScenes dataset.";Florian Chabot<author:sep>Nicolas Granger<author:sep>Guillaume Lapouge;http://arxiv.org/pdf/2407.14108v1;cs.CV;;
2407.14053v1;http://arxiv.org/abs/2407.14053v1;2024-07-19;DirectL: Efficient Radiance Fields Rendering for 3D Light Field Displays;"Autostereoscopic display, despite decades of development, has not achieved
extensive application, primarily due to the daunting challenge of 3D content
creation for non-specialists. The emergence of Radiance Field as an innovative
3D representation has markedly revolutionized the domains of 3D reconstruction
and generation. This technology greatly simplifies 3D content creation for
common users, broadening the applicability of Light Field Displays (LFDs).
However, the combination of these two fields remains largely unexplored. The
standard paradigm to create optimal content for parallax-based light field
displays demands rendering at least 45 slightly shifted views preferably at
high resolution per frame, a substantial hurdle for real-time rendering. We
introduce DirectL, a novel rendering paradigm for Radiance Fields on 3D
displays. We thoroughly analyze the interweaved mapping of spatial rays to
screen subpixels, precisely determine the light rays entering the human eye,
and propose subpixel repurposing to significantly reduce the pixel count
required for rendering. Tailored for the two predominant radiance
fields--Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS), we
propose corresponding optimized rendering pipelines that directly render the
light field images instead of multi-view images. Extensive experiments across
various displays and user study demonstrate that DirectL accelerates rendering
by up to 40 times compared to the standard paradigm without sacrificing visual
quality. Its rendering process-only modification allows seamless integration
into subsequent radiance field tasks. Finally, we integrate DirectL into
diverse applications, showcasing the stunning visual experiences and the
synergy between LFDs and Radiance Fields, which unveils tremendous potential
for commercialization applications. \href{direct-l.github.io}{\textbf{Project
Homepage}";Zongyuan Yang<author:sep>Baolin Liu<author:sep>Yingde Song<author:sep>Yongping Xiong<author:sep>Lan Yi<author:sep>Zhaohe Zhang<author:sep>Xunbo Yu;http://arxiv.org/pdf/2407.14053v1;cs.GR;;gaussian splatting<tag:sep>nerf
2407.14419v1;http://arxiv.org/abs/2407.14419v1;2024-07-19;HOTS3D: Hyper-Spherical Optimal Transport for Semantic Alignment of  Text-to-3D Generation;"Recent CLIP-guided 3D generation methods have achieved promising results but
struggle with generating faithful 3D shapes that conform with input text due to
the gap between text and image embeddings. To this end, this paper proposes
HOTS3D which makes the first attempt to effectively bridge this gap by aligning
text features to the image features with spherical optimal transport (SOT).
However, in high-dimensional situations, solving the SOT remains a challenge.
To obtain the SOT map for high-dimensional features obtained from CLIP encoding
of two modalities, we mathematically formulate and derive the solution based on
Villani's theorem, which can directly align two hyper-sphere distributions
without manifold exponential maps. Furthermore, we implement it by leveraging
input convex neural networks (ICNNs) for the optimal Kantorovich potential.
With the optimally mapped features, a diffusion-based generator and a
Nerf-based decoder are subsequently utilized to transform them into 3D shapes.
Extensive qualitative and qualitative comparisons with state-of-the-arts
demonstrate the superiority of the proposed HOTS3D for 3D shape generation,
especially on the consistency with text semantics.";Zezeng Li<author:sep>Weimin Wang<author:sep>WenHai Li<author:sep>Na Lei<author:sep>Xianfeng Gu;http://arxiv.org/pdf/2407.14419v1;cs.CV;;nerf
2407.14197v1;http://arxiv.org/abs/2407.14197v1;2024-07-19;A Benchmark for Gaussian Splatting Compression and Quality Assessment  Study;"To fill the gap of traditional GS compression method, in this paper, we first
propose a simple and effective GS data compression anchor called Graph-based GS
Compression (GGSC). GGSC is inspired by graph signal processing theory and uses
two branches to compress the primitive center and attributes. We split the
whole GS sample via KDTree and clip the high-frequency components after the
graph Fourier transform. Followed by quantization, G-PCC and adaptive
arithmetic coding are used to compress the primitive center and attribute
residual matrix to generate the bitrate file. GGSS is the first work to explore
traditional GS compression, with advantages that can reveal the GS distortion
characteristics corresponding to typical compression operation, such as
high-frequency clipping and quantization. Second, based on GGSC, we create a GS
Quality Assessment dataset (GSQA) with 120 samples. A subjective experiment is
conducted in a laboratory environment to collect subjective scores after
rendering GS into Processed Video Sequences (PVS). We analyze the
characteristics of different GS distortions based on Mean Opinion Scores (MOS),
demonstrating the sensitivity of different attributes distortion to visual
quality. The GGSC code and the dataset, including GS samples, MOS, and PVS, are
made publicly available at https://github.com/Qi-Yangsjtu/GGSC.";Qi Yang<author:sep>Kaifa Yang<author:sep>Yuke Xing<author:sep>Yiling Xu<author:sep>Zhu Li;http://arxiv.org/pdf/2407.14197v1;cs.CV;;gaussian splatting
2407.13992v1;http://arxiv.org/abs/2407.13992v1;2024-07-19;Semantic Communications for 3D Human Face Transmission with Neural  Radiance Fields;"This paper investigates the transmission of three-dimensional (3D) human face
content for immersive communication over a rate-constrained
transmitter-receiver link. We propose a new framework named NeRF-SeCom, which
leverages neural radiance fields (NeRF) and semantic communications to improve
the quality of 3D visualizations while minimizing the communication overhead.
In the NeRF-SeCom framework, we first train a NeRF face model based on the
NeRFBlendShape method, which is pre-shared between the transmitter and receiver
as the semantic knowledge base to facilitate the real-time transmission. Next,
with knowledge base, the transmitter extracts and sends only the essential
semantic features for the receiver to reconstruct 3D face in real time. To
optimize the transmission efficiency, we classify the expression features into
static and dynamic types. Over each video chunk, static features are
transmitted once for all frames, whereas dynamic features are transmitted over
a portion of frames to adhere to rate constraints. Additionally, we propose a
feature prediction mechanism, which allows the receiver to predict the dynamic
features for frames that are not transmitted. Experiments show that our
proposed NeRF-SeCom framework significantly outperforms benchmark methods in
delivering high-quality 3D visualizations of human faces.";Guanlin Wu<author:sep>Zhonghao Lyu<author:sep>Juyong Zhang<author:sep>Jie Xu;http://arxiv.org/pdf/2407.13992v1;eess.IV;"6 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:2405.12155";nerf
2407.13520v1;http://arxiv.org/abs/2407.13520v1;2024-07-18;EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian  Splatting;"3D deblurring reconstruction techniques have recently seen significant
advancements with the development of Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS). Although these techniques can recover relatively
clear 3D reconstructions from blurry image inputs, they still face limitations
in handling severe blurring and complex camera motion. To address these issues,
we propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting
(EaDeblur-GS), which integrates event camera data to enhance the robustness of
3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)
network to estimate Gaussian center deviations and using novel loss functions,
EaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating
performance comparable to state-of-the-art methods.";Yuchen Weng<author:sep>Zhengwen Shen<author:sep>Ruofan Chen<author:sep>Qi Wang<author:sep>Jun Wang;http://arxiv.org/pdf/2407.13520v1;cs.CV;;gaussian splatting<tag:sep>nerf
2407.13185v1;http://arxiv.org/abs/2407.13185v1;2024-07-18;KFD-NeRF: Rethinking Dynamic NeRF with Kalman Filter;"We introduce KFD-NeRF, a novel dynamic neural radiance field integrated with
an efficient and high-quality motion reconstruction framework based on Kalman
filtering. Our key idea is to model the dynamic radiance field as a dynamic
system whose temporally varying states are estimated based on two sources of
knowledge: observations and predictions. We introduce a novel plug-in Kalman
filter guided deformation field that enables accurate deformation estimation
from scene observations and predictions. We use a shallow Multi-Layer
Perceptron (MLP) for observations and model the motion as locally linear to
calculate predictions with motion equations. To further enhance the performance
of the observation MLP, we introduce regularization in the canonical space to
facilitate the network's ability to learn warping for different frames.
Additionally, we employ an efficient tri-plane representation for encoding the
canonical space, which has been experimentally demonstrated to converge quickly
with high quality. This enables us to use a shallower observation MLP,
consisting of just two layers in our implementation. We conduct experiments on
synthetic and real data and compare with past dynamic NeRF methods. Our
KFD-NeRF demonstrates similar or even superior rendering performance within
comparable computational time and achieves state-of-the-art view synthesis
performance with thorough training.";Yifan Zhan<author:sep>Zhuoxiao Li<author:sep>Muyao Niu<author:sep>Zhihang Zhong<author:sep>Shohei Nobuhara<author:sep>Ko Nishino<author:sep>Yinqiang Zheng;http://arxiv.org/pdf/2407.13185v1;cs.CV;accepted to eccv2024;nerf
2407.13390v1;http://arxiv.org/abs/2407.13390v1;2024-07-18;GeometrySticker: Enabling Ownership Claim of Recolorized Neural Radiance  Fields;"Remarkable advancements in the recolorization of Neural Radiance Fields
(NeRF) have simplified the process of modifying NeRF's color attributes. Yet,
with the potential of NeRF to serve as shareable digital assets, there's a
concern that malicious users might alter the color of NeRF models and falsely
claim the recolorized version as their own. To safeguard against such breaches
of ownership, enabling original NeRF creators to establish rights over
recolorized NeRF is crucial. While approaches like CopyRNeRF have been
introduced to embed binary messages into NeRF models as digital signatures for
copyright protection, the process of recolorization can remove these binary
messages. In our paper, we present GeometrySticker, a method for seamlessly
integrating binary messages into the geometry components of radiance fields,
akin to applying a sticker. GeometrySticker can embed binary messages into NeRF
models while preserving the effectiveness of these messages against
recolorization. Our comprehensive studies demonstrate that GeometrySticker is
adaptable to prevalent NeRF architectures and maintains a commendable level of
robustness against various distortions. Project page:
https://kevinhuangxf.github.io/GeometrySticker/.";Xiufeng Huang<author:sep>Ka Chun Cheung<author:sep>Simon See<author:sep>Renjie Wan;http://arxiv.org/pdf/2407.13390v1;cs.CV;;nerf
2407.13584v2;http://arxiv.org/abs/2407.13584v2;2024-07-18;Connecting Consistency Distillation to Score Distillation for Text-to-3D  Generation;"Although recent advancements in text-to-3D generation have significantly
improved generation quality, issues like limited level of detail and low
fidelity still persist, which requires further improvement. To understand the
essence of those issues, we thoroughly analyze current score distillation
methods by connecting theories of consistency distillation to score
distillation. Based on the insights acquired through analysis, we propose an
optimization framework, Guided Consistency Sampling (GCS), integrated with 3D
Gaussian Splatting (3DGS) to alleviate those issues. Additionally, we have
observed the persistent oversaturation in the rendered views of generated 3D
assets. From experiments, we find that it is caused by unwanted accumulated
brightness in 3DGS during optimization. To mitigate this issue, we introduce a
Brightness-Equalized Generation (BEG) scheme in 3DGS rendering. Experimental
results demonstrate that our approach generates 3D assets with more details and
higher fidelity than state-of-the-art methods. The codes are released at
https://github.com/LMozart/ECCV2024-GCS-BEG.";Zongrui Li<author:sep>Minghui Hu<author:sep>Qian Zheng<author:sep>Xudong Jiang;http://arxiv.org/pdf/2407.13584v2;cs.CV;Paper accepted by ECCV2024;gaussian splatting
2407.12667v1;http://arxiv.org/abs/2407.12667v1;2024-07-17;SG-NeRF: Neural Surface Reconstruction with Scene Graph Optimization;"3D surface reconstruction from images is essential for numerous applications.
Recently, Neural Radiance Fields (NeRFs) have emerged as a promising framework
for 3D modeling. However, NeRFs require accurate camera poses as input, and
existing methods struggle to handle significantly noisy pose estimates (i.e.,
outliers), which are commonly encountered in real-world scenarios. To tackle
this challenge, we present a novel approach that optimizes radiance fields with
scene graphs to mitigate the influence of outlier poses. Our method
incorporates an adaptive inlier-outlier confidence estimation scheme based on
scene graphs, emphasizing images of high compatibility with the neighborhood
and consistency in the rendering quality. We also introduce an effective
intersection-over-union (IoU) loss to optimize the camera pose and surface
geometry, together with a coarse-to-fine strategy to facilitate the training.
Furthermore, we propose a new dataset containing typical outlier poses for a
detailed evaluation. Experimental results on various datasets consistently
demonstrate the effectiveness and superiority of our method over existing
approaches, showcasing its robustness in handling outliers and producing
high-quality 3D reconstructions. Our code and data are available at:
\url{https://github.com/Iris-cyy/SG-NeRF}.";Yiyang Chen<author:sep>Siyan Dong<author:sep>Xulong Wang<author:sep>Lulu Cai<author:sep>Youyi Zheng<author:sep>Yanchao Yang;http://arxiv.org/pdf/2407.12667v1;cs.CV;ECCV 2024;nerf
2407.12661v1;http://arxiv.org/abs/2407.12661v1;2024-07-17;InfoNorm: Mutual Information Shaping of Normals for Sparse-View  Reconstruction;"3D surface reconstruction from multi-view images is essential for scene
understanding and interaction. However, complex indoor scenes pose challenges
such as ambiguity due to limited observations. Recent implicit surface
representations, such as Neural Radiance Fields (NeRFs) and signed distance
functions (SDFs), employ various geometric priors to resolve the lack of
observed information. Nevertheless, their performance heavily depends on the
quality of the pre-trained geometry estimation models. To ease such dependence,
we propose regularizing the geometric modeling by explicitly encouraging the
mutual information among surface normals of highly correlated scene points. In
this way, the geometry learning process is modulated by the second-order
correlations from noisy (first-order) geometric priors, thus eliminating the
bias due to poor generalization. Additionally, we introduce a simple yet
effective scheme that utilizes semantic and geometric features to identify
correlated points, enhancing their mutual information accordingly. The proposed
technique can serve as a plugin for SDF-based neural surface representations.
Our experiments demonstrate the effectiveness of the proposed in improving the
surface reconstruction quality of major states of the arts. Our code is
available at: \url{https://github.com/Muliphein/InfoNorm}.";Xulong Wang<author:sep>Siyan Dong<author:sep>Youyi Zheng<author:sep>Yanchao Yang;http://arxiv.org/pdf/2407.12661v1;cs.CV;ECCV 2024;nerf
2407.12354v1;http://arxiv.org/abs/2407.12354v1;2024-07-17;Invertible Neural Warp for NeRF;"This paper tackles the simultaneous optimization of pose and Neural Radiance
Fields (NeRF). Departing from the conventional practice of using explicit
global representations for camera pose, we propose a novel overparameterized
representation that models camera poses as learnable rigid warp functions. We
establish that modeling the rigid warps must be tightly coupled with
constraints and regularization imposed. Specifically, we highlight the critical
importance of enforcing invertibility when learning rigid warp functions via
neural network and propose the use of an Invertible Neural Network (INN)
coupled with a geometry-informed constraint for this purpose. We present
results on synthetic and real-world datasets, and demonstrate that our approach
outperforms existing baselines in terms of pose estimation and high-fidelity
reconstruction due to enhanced optimization convergence.";Shin-Fang Chng<author:sep>Ravi Garg<author:sep>Hemanth Saratchandran<author:sep>Simon Lucey;http://arxiv.org/pdf/2407.12354v1;cs.CV;"Accepted to ECCV 2024. Project page:
  https://sfchng.github.io/ineurowarping-github.io/";nerf
2407.12777v1;http://arxiv.org/abs/2407.12777v1;2024-07-17;Generalizable Human Gaussians for Sparse View Synthesis;"Recent progress in neural rendering has brought forth pioneering methods,
such as NeRF and Gaussian Splatting, which revolutionize view rendering across
various domains like AR/VR, gaming, and content creation. While these methods
excel at interpolating {\em within the training data}, the challenge of
generalizing to new scenes and objects from very sparse views persists.
Specifically, modeling 3D humans from sparse views presents formidable hurdles
due to the inherent complexity of human geometry, resulting in inaccurate
reconstructions of geometry and textures. To tackle this challenge, this paper
leverages recent advancements in Gaussian Splatting and introduces a new method
to learn generalizable human Gaussians that allows photorealistic and accurate
view-rendering of a new human subject from a limited set of sparse views in a
feed-forward manner. A pivotal innovation of our approach involves
reformulating the learning of 3D Gaussian parameters into a regression process
defined on the 2D UV space of a human template, which allows leveraging the
strong geometry prior and the advantages of 2D convolutions. In addition, a
multi-scaffold is proposed to effectively represent the offset details. Our
method outperforms recent methods on both within-dataset generalization as well
as cross-dataset generalization settings.";Youngjoong Kwon<author:sep>Baole Fang<author:sep>Yixing Lu<author:sep>Haoye Dong<author:sep>Cheng Zhang<author:sep>Francisco Vicente Carrasco<author:sep>Albert Mosella-Montoro<author:sep>Jianjin Xu<author:sep>Shingo Takagi<author:sep>Daeil Kim<author:sep>Aayush Prakash<author:sep>Fernando De la Torre;http://arxiv.org/pdf/2407.12777v1;cs.CV;;gaussian splatting<tag:sep>nerf
2407.12306v1;http://arxiv.org/abs/2407.12306v1;2024-07-17;Splatfacto-W: A Nerfstudio Implementation of Gaussian Splatting for  Unconstrained Photo Collections;"Novel view synthesis from unconstrained in-the-wild image collections remains
a significant yet challenging task due to photometric variations and transient
occluders that complicate accurate scene reconstruction. Previous methods have
approached these issues by integrating per-image appearance features embeddings
in Neural Radiance Fields (NeRFs). Although 3D Gaussian Splatting (3DGS) offers
faster training and real-time rendering, adapting it for unconstrained image
collections is non-trivial due to the substantially different architecture. In
this paper, we introduce Splatfacto-W, an approach that integrates per-Gaussian
neural color features and per-image appearance embeddings into the
rasterization process, along with a spherical harmonics-based background model
to represent varying photometric appearances and better depict backgrounds. Our
key contributions include latent appearance modeling, efficient transient
object handling, and precise background modeling. Splatfacto-W delivers
high-quality, real-time novel view synthesis with improved scene consistency in
in-the-wild scenarios. Our method improves the Peak Signal-to-Noise Ratio
(PSNR) by an average of 5.3 dB compared to 3DGS, enhances training speed by 150
times compared to NeRF-based methods, and achieves a similar rendering speed to
3DGS. Additional video results and code integrated into Nerfstudio are
available at https://kevinxu02.github.io/splatfactow/.";Congrong Xu<author:sep>Justin Kerr<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2407.12306v1;cs.CV;9 pages;gaussian splatting<tag:sep>nerf
2407.11781v1;http://arxiv.org/abs/2407.11781v1;2024-07-16;SlingBAG: Sliding ball adaptive growth algorithm with differentiable  radiation enables super-efficient iterative 3D photoacoustic image  reconstruction;"High-quality 3D photoacoustic imaging (PAI) reconstruction under sparse view
or limited view has long been challenging. Traditional 3D iterative-based
reconstruction methods suffer from both slow speed and high memory consumption.
Recently, in computer graphics, the differentiable rendering has made
significant progress, particularly with the rise of 3D Gaussian Splatting.
Inspired by these, we introduce differentiable radiation into PAI, developing a
novel reconstruction algorithm: the Sliding Ball Adaptive Growth algorithm
(SlingBAG) for 3D PAI, which shows ability in high-quality 3D PAI
reconstruction both under extremely sparse view and limited view.
  We established the point cloud dataset in PAI, and used unique differentiable
rapid radiator based on the spherical decomposition strategy and the randomly
initialized point cloud adaptively optimized according to sparse sensor data.
Each point undergoes updates in 3D coordinates, initial pressure, and
resolution (denoted by the radius of ball). Points undergo adaptive growth
during iterative process, including point destroying, splitting and duplicating
along the gradient of their positions, manifesting the sliding ball effect.
  Finally, our point cloud to voxel grid shader renders the final
reconstruction results. Simulation and in vivo experiments demonstrate that our
SlingBAG reconstruction result's SNR can be more than 40 dB under extremely
sparse view, while the SNR of traditional back-projection algorithm's result is
less than 20 dB. Moreover, the result of SlingBAG's structural similarity to
the ground truth is significantly higher, with an SSIM value of 95.6%.
  Notably, our differentiable rapid radiator can conduct forward PA simulation
in homogeneous, non-viscous media substantially faster than current methods
that numerically simulate the wave propagation, such as k-Wave. The dataset and
all code will be open source.";Shuang Li<author:sep>Yibing Wang<author:sep>Jian Gao<author:sep>Chulhong Kim<author:sep>Seongwook Choi<author:sep>Yu Zhang<author:sep>Qian Chen<author:sep>Yao Yao<author:sep>Changhui Li;http://arxiv.org/pdf/2407.11781v1;cs.CV;;gaussian splatting
2407.11840v1;http://arxiv.org/abs/2407.11840v1;2024-07-16;MVG-Splatting: Multi-View Guided Gaussian Splatting with Adaptive  Quantile-Based Geometric Consistency Densification;"In the rapidly evolving field of 3D reconstruction, 3D Gaussian Splatting
(3DGS) and 2D Gaussian Splatting (2DGS) represent significant advancements.
Although 2DGS compresses 3D Gaussian primitives into 2D Gaussian surfels to
effectively enhance mesh extraction quality, this compression can potentially
lead to a decrease in rendering quality. Additionally, unreliable densification
processes and the calculation of depth through the accumulation of opacity can
compromise the detail of mesh extraction. To address this issue, we introduce
MVG-Splatting, a solution guided by Multi-View considerations. Specifically, we
integrate an optimized method for calculating normals, which, combined with
image gradients, helps rectify inconsistencies in the original depth
computations. Additionally, utilizing projection strategies akin to those in
Multi-View Stereo (MVS), we propose an adaptive quantile-based method that
dynamically determines the level of additional densification guided by depth
maps, from coarse to fine detail. Experimental evidence demonstrates that our
method not only resolves the issues of rendering quality degradation caused by
depth discrepancies but also facilitates direct mesh extraction from dense
Gaussian point clouds using the Marching Cubes algorithm. This approach
significantly enhances the overall fidelity and accuracy of the 3D
reconstruction process, ensuring that both the geometric details and visual
quality.";Zhuoxiao Li<author:sep>Shanliang Yao<author:sep>Yijie Chu<author:sep>Angel F. Garcia-Fernandez<author:sep>Yong Yue<author:sep>Eng Gee Lim<author:sep>Xiaohui Zhu;http://arxiv.org/pdf/2407.11840v1;cs.CV;https://mvgsplatting.github.io;gaussian splatting
2407.11343v1;http://arxiv.org/abs/2407.11343v1;2024-07-16;Ev-GS: Event-based Gaussian splatting for Efficient and Accurate  Radiance Field Rendering;"Computational neuromorphic imaging (CNI) with event cameras offers advantages
such as minimal motion blur and enhanced dynamic range, compared to
conventional frame-based methods. Existing event-based radiance field rendering
methods are built on neural radiance field, which is computationally heavy and
slow in reconstruction speed. Motivated by the two aspects, we introduce Ev-GS,
the first CNI-informed scheme to infer 3D Gaussian splatting from a monocular
event camera, enabling efficient novel view synthesis. Leveraging 3D Gaussians
with pure event-based supervision, Ev-GS overcomes challenges such as the
detection of fast-moving objects and insufficient lighting. Experimental
results show that Ev-GS outperforms the method that takes frame-based signals
as input by rendering realistic views with reduced blurring and improved visual
quality. Moreover, it demonstrates competitive reconstruction quality and
reduced computing occupancy compared to existing methods, which paves the way
to a highly efficient CNI approach for signal processing.";Jingqian Wu<author:sep>Shuo Zhu<author:sep>Chutian Wang<author:sep>Edmund Y. Lam;http://arxiv.org/pdf/2407.11343v1;cs.CV;;gaussian splatting
2407.11347v1;http://arxiv.org/abs/2407.11347v1;2024-07-16;I$^2$-SLAM: Inverting Imaging Process for Robust Photorealistic Dense  SLAM;"We present an inverse image-formation module that can enhance the robustness
of existing visual SLAM pipelines for casually captured scenarios. Casual video
captures often suffer from motion blur and varying appearances, which degrade
the final quality of coherent 3D visual representation. We propose integrating
the physical imaging into the SLAM system, which employs linear HDR radiance
maps to collect measurements. Specifically, individual frames aggregate images
of multiple poses along the camera trajectory to explain prevalent motion blur
in hand-held videos. Additionally, we accommodate per-frame appearance
variation by dedicating explicit variables for image formation steps, namely
white balance, exposure time, and camera response function. Through joint
optimization of additional variables, the SLAM pipeline produces high-quality
images with more accurate trajectories. Extensive experiments demonstrate that
our approach can be incorporated into recent visual SLAM pipelines using
various scene representations, such as neural radiance fields or Gaussian
splatting.";Gwangtak Bae<author:sep>Changwoon Choi<author:sep>Hyeongjun Heo<author:sep>Sang Min Kim<author:sep>Young Min Kim;http://arxiv.org/pdf/2407.11347v1;cs.CV;ECCV 2024;
2407.11921v2;http://arxiv.org/abs/2407.11921v2;2024-07-16;IPA-NeRF: Illusory Poisoning Attack Against Neural Radiance Fields;"Neural Radiance Field (NeRF) represents a significant advancement in computer
vision, offering implicit neural network-based scene representation and novel
view synthesis capabilities. Its applications span diverse fields including
robotics, urban mapping, autonomous navigation, virtual reality/augmented
reality, etc., some of which are considered high-risk AI applications. However,
despite its widespread adoption, the robustness and security of NeRF remain
largely unexplored. In this study, we contribute to this area by introducing
the Illusory Poisoning Attack against Neural Radiance Fields (IPA-NeRF). This
attack involves embedding a hidden backdoor view into NeRF, allowing it to
produce predetermined outputs, i.e. illusory, when presented with the specified
backdoor view while maintaining normal performance with standard inputs. Our
attack is specifically designed to deceive users or downstream models at a
particular position while ensuring that any abnormalities in NeRF remain
undetectable from other viewpoints. Experimental results demonstrate the
effectiveness of our Illusory Poisoning Attack, successfully presenting the
desired illusory on the specified viewpoint without impacting other views.
Notably, we achieve this attack by introducing small perturbations solely to
the training set. The code can be found at
https://github.com/jiang-wenxiang/IPA-NeRF.";Wenxiang Jiang<author:sep>Hanwei Zhang<author:sep>Shuo Zhao<author:sep>Zhongwen Guo<author:sep>Hao Wang;http://arxiv.org/pdf/2407.11921v2;cs.CV;;nerf
2407.11394v1;http://arxiv.org/abs/2407.11394v1;2024-07-16;DreamCatalyst: Fast and High-Quality 3D Editing via Controlling  Editability and Identity Preservation;"Score distillation sampling (SDS) has emerged as an effective framework in
text-driven 3D editing tasks due to its inherent 3D consistency. However,
existing SDS-based 3D editing methods suffer from extensive training time and
lead to low-quality results, primarily because these methods deviate from the
sampling dynamics of diffusion models. In this paper, we propose DreamCatalyst,
a novel framework that interprets SDS-based editing as a diffusion reverse
process. Our objective function considers the sampling dynamics, thereby making
the optimization process of DreamCatalyst an approximation of the diffusion
reverse process in editing tasks. DreamCatalyst aims to reduce training time
and improve editing quality. DreamCatalyst presents two modes: (1) a faster
mode, which edits the NeRF scene in only about 25 minutes, and (2) a
high-quality mode, which produces superior results in less than 70 minutes.
Specifically, our high-quality mode outperforms current state-of-the-art NeRF
editing methods both in terms of speed and quality. See more extensive results
on our project page: https://dream-catalyst.github.io.";Jiwook Kim<author:sep>Seonho Lee<author:sep>Jaeyo Shin<author:sep>Jiho Choi<author:sep>Hyunjung Shim;http://arxiv.org/pdf/2407.11394v1;cs.CV;;nerf
2407.11309v1;http://arxiv.org/abs/2407.11309v1;2024-07-16;Gaussian Splatting LK;"Reconstructing dynamic 3D scenes from 2D images and generating diverse views
over time presents a significant challenge due to the inherent complexity and
temporal dynamics involved. While recent advancements in neural implicit models
and dynamic Gaussian Splatting have shown promise, limitations persist,
particularly in accurately capturing the underlying geometry of highly dynamic
scenes. Some approaches address this by incorporating strong semantic and
geometric priors through diffusion models. However, we explore a different
avenue by investigating the potential of regularizing the native warp field
within the dynamic Gaussian Splatting framework. Our method is grounded on the
key intuition that an accurate warp field should produce continuous space-time
motions. While enforcing the motion constraints on warp fields is non-trivial,
we show that we can exploit knowledge innate to the forward warp field network
to derive an analytical velocity field, then time integrate for scene flows to
effectively constrain both the 2D motion and 3D positions of the Gaussians.
This derived Lucas-Kanade style analytical regularization enables our method to
achieve superior performance in reconstructing highly dynamic scenes, even
under minimal camera movement, extending the boundaries of what existing
dynamic Gaussian Splatting frameworks can achieve.";Liuyue Xie<author:sep>Joel Julin<author:sep>Koichiro Niinuma<author:sep>Laszlo A. Jeni;http://arxiv.org/pdf/2407.11309v1;cs.CV;15 pages, 10 figures;gaussian splatting
2407.11962v2;http://arxiv.org/abs/2407.11962v2;2024-07-16;Motion-Oriented Compositional Neural Radiance Fields for Monocular  Dynamic Human Modeling;"This paper introduces Motion-oriented Compositional Neural Radiance Fields
(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of
monocular human videos via novel non-rigid motion modeling approach. In the
context of dynamic clothed humans, complex cloth dynamics generate non-rigid
motions that are intrinsically distinct from skeletal articulations and
critically important for the rendering quality. The conventional approach
models non-rigid motions as spatial (3D) deviations in addition to skeletal
transformations. However, it is either time-consuming or challenging to achieve
optimal quality due to its high learning complexity without a direct
supervision. To target this problem, we propose a novel approach of modeling
non-rigid motions as radiance residual fields to benefit from more direct color
supervision in the rendering and utilize the rigid radiance fields as a prior
to reduce the complexity of the learning process. Our approach utilizes a
single multiresolution hash encoding (MHE) to concurrently learn the canonical
T-pose representation from rigid skeletal motions and the radiance residual
field for non-rigid motions. Additionally, to further improve both training
efficiency and usability, we extend MoCo-NeRF to support simultaneous training
of multiple subjects within a single framework, thanks to our effective design
for modeling non-rigid motions. This scalability is achieved through the
integration of a global MHE and learnable identity codes in addition to
multiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,
clearly demonstrating state-of-the-art performance in both single- and
multi-subject settings. The code and model will be made publicly available at
the project page: https://stevejaehyeok.github.io/publications/moco-nerf.";Jaehyeok Kim<author:sep>Dongyoon Wee<author:sep>Dan Xu;http://arxiv.org/pdf/2407.11962v2;cs.CV;Accepted by ECCV2024;nerf
2407.11793v1;http://arxiv.org/abs/2407.11793v1;2024-07-16;Click-Gaussian: Interactive Segmentation to Any 3D Gaussians;"Interactive segmentation of 3D Gaussians opens a great opportunity for
real-time manipulation of 3D scenes thanks to the real-time rendering
capability of 3D Gaussian Splatting. However, the current methods suffer from
time-consuming post-processing to deal with noisy segmentation output. Also,
they struggle to provide detailed segmentation, which is important for
fine-grained manipulation of 3D scenes. In this study, we propose
Click-Gaussian, which learns distinguishable feature fields of two-level
granularity, facilitating segmentation without time-consuming post-processing.
We delve into challenges stemming from inconsistently learned feature fields
resulting from 2D segmentation obtained independently from a 3D scene. 3D
segmentation accuracy deteriorates when 2D segmentation results across the
views, primary cues for 3D segmentation, are in conflict. To overcome these
issues, we propose Global Feature-guided Learning (GFL). GFL constructs the
clusters of global feature candidates from noisy 2D segments across the views,
which smooths out noises when training the features of 3D Gaussians. Our method
runs in 10 ms per click, 15 to 130 times as fast as the previous methods, while
also significantly improving segmentation accuracy. Our project page is
available at https://seokhunchoi.github.io/Click-Gaussian";Seokhun Choi<author:sep>Hyeonseop Song<author:sep>Jaechul Kim<author:sep>Taehyeong Kim<author:sep>Hoseok Do;http://arxiv.org/pdf/2407.11793v1;cs.CV;"Accepted to ECCV 2024. The first two authors contributed equally to
  this work";gaussian splatting
2407.11238v1;http://arxiv.org/abs/2407.11238v1;2024-07-15;Evaluating geometric accuracy of NeRF reconstructions compared to SLAM  method;"As Neural Radiance Field (NeRF) implementations become faster, more efficient
and accurate, their applicability to real world mapping tasks becomes more
accessible. Traditionally, 3D mapping, or scene reconstruction, has relied on
expensive LiDAR sensing. Photogrammetry can perform image-based 3D
reconstruction but is computationally expensive and requires extremely dense
image representation to recover complex geometry and photorealism. NeRFs
perform 3D scene reconstruction by training a neural network on sparse image
and pose data, achieving superior results to photogrammetry with less input
data. This paper presents an evaluation of two NeRF scene reconstructions for
the purpose of estimating the diameter of a vertical PVC cylinder. One of these
are trained on commodity iPhone data and the other is trained on robot-sourced
imagery and poses. This neural-geometry is compared to state-of-the-art
lidar-inertial SLAM in terms of scene noise and metric-accuracy.";Adam Korycki<author:sep>Colleen Josephson<author:sep>Steve McGuire;http://arxiv.org/pdf/2407.11238v1;cs.CV;;nerf
2407.10707v1;http://arxiv.org/abs/2407.10707v1;2024-07-15;Interactive Rendering of Relightable and Animatable Gaussian Avatars;"Creating relightable and animatable avatars from multi-view or monocular
videos is a challenging task for digital human creation and virtual reality
applications. Previous methods rely on neural radiance fields or ray tracing,
resulting in slow training and rendering processes. By utilizing Gaussian
Splatting, we propose a simple and efficient method to decouple body materials
and lighting from sparse-view or monocular avatar videos, so that the avatar
can be rendered simultaneously under novel viewpoints, poses, and lightings at
interactive frame rates (6.9 fps). Specifically, we first obtain the canonical
body mesh using a signed distance function and assign attributes to each mesh
vertex. The Gaussians in the canonical space then interpolate from nearby body
mesh vertices to obtain the attributes. We subsequently deform the Gaussians to
the posed space using forward skinning, and combine the learnable environment
light with the Gaussian attributes for shading computation. To achieve fast
shadow modeling, we rasterize the posed body mesh from dense viewpoints to
obtain the visibility. Our approach is not only simple but also fast enough to
allow interactive rendering of avatar animation under environmental light
changes. Experiments demonstrate that, compared to previous works, our method
can render higher quality results at a faster speed on both synthetic and real
datasets.";Youyi Zhan<author:sep>Tianjia Shao<author:sep>He Wang<author:sep>Yin Yang<author:sep>Kun Zhou;http://arxiv.org/pdf/2407.10707v1;cs.CV;;
2407.10865v1;http://arxiv.org/abs/2407.10865v1;2024-07-15;AirNeRF: 3D Reconstruction of Human with Drone and NeRF for Future  Communication Systems;"In the rapidly evolving landscape of digital content creation, the demand for
fast, convenient, and autonomous methods of crafting detailed 3D
reconstructions of humans has grown significantly. Addressing this pressing
need, our AirNeRF system presents an innovative pathway to the creation of a
realistic 3D human avatar. Our approach leverages Neural Radiance Fields (NeRF)
with an automated drone-based video capturing method. The acquired data
provides a swift and precise way to create high-quality human body
reconstructions following several stages of our system. The rigged mesh derived
from our system proves to be an excellent foundation for free-view synthesis of
dynamic humans, particularly well-suited for the immersive experiences within
gaming and virtual reality.";Alexey Kotcov<author:sep>Maria Dronova<author:sep>Vladislav Cheremnykh<author:sep>Sausar Karaf<author:sep>Dzmitry Tsetserukou;http://arxiv.org/pdf/2407.10865v1;cs.RO;;nerf
2407.10389v1;http://arxiv.org/abs/2407.10389v1;2024-07-15;Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High  Quality and Efficient Rendering;"Since the introduction of NeRFs, considerable attention has been focused on
improving their training and inference times, leading to the development of
Fast-NeRFs models. Despite demonstrating impressive rendering speed and
quality, the rapid convergence of such models poses challenges for further
improving reconstruction quality. Common strategies to improve rendering
quality involves augmenting model parameters or increasing the number of
sampled points. However, these computationally intensive approaches encounter
limitations in achieving significant quality enhancements. This study
introduces a model-agnostic framework inspired by Sparsely-Gated Mixture of
Experts to enhance rendering quality without escalating computational
complexity. Our approach enables specialization in rendering different scene
components by employing a mixture of experts with varying resolutions. We
present a novel gate formulation designed to maximize expert capabilities and
propose a resolution-based routing technique to effectively induce sparsity and
decompose scenes. Our work significantly improves reconstruction quality while
maintaining competitive performance.";Francesco Di Sario<author:sep>Riccardo Renzulli<author:sep>Enzo Tartaglione<author:sep>Marco Grangetto;http://arxiv.org/pdf/2407.10389v1;cs.CV;;nerf
2407.11174v1;http://arxiv.org/abs/2407.11174v1;2024-07-15;iHuman: Instant Animatable Digital Humans From Monocular Videos;"Personalized 3D avatars require an animatable representation of digital
humans. Doing so instantly from monocular videos offers scalability to broad
class of users and wide-scale applications. In this paper, we present a fast,
simple, yet effective method for creating animatable 3D digital humans from
monocular videos. Our method utilizes the efficiency of Gaussian splatting to
model both 3D geometry and appearance. However, we observed that naively
optimizing Gaussian splats results in inaccurate geometry, thereby leading to
poor animations. This work achieves and illustrates the need of accurate 3D
mesh-type modelling of the human body for animatable digitization through
Gaussian splats. This is achieved by developing a novel pipeline that benefits
from three key aspects: (a) implicit modelling of surface's displacements and
the color's spherical harmonics; (b) binding of 3D Gaussians to the respective
triangular faces of the body template; (c) a novel technique to render normals
followed by their auxiliary supervision. Our exhaustive experiments on three
different benchmark datasets demonstrates the state-of-the-art results of our
method, in limited time settings. In fact, our method is faster by an order of
magnitude (in terms of training time) than its closest competitor. At the same
time, we achieve superior rendering and 3D reconstruction performance under the
change of poses.";Pramish Paudel<author:sep>Anubhav Khanal<author:sep>Ajad Chhatkuli<author:sep>Danda Pani Paudel<author:sep>Jyoti Tandukar;http://arxiv.org/pdf/2407.11174v1;cs.CV;15 pages, eccv, 2024;gaussian splatting
2407.10762v1;http://arxiv.org/abs/2407.10762v1;2024-07-15;Domain Generalization for 6D Pose Estimation Through NeRF-based Image  Synthesis;"This work introduces a novel augmentation method that increases the diversity
of a train set to improve the generalization abilities of a 6D pose estimation
network. For this purpose, a Neural Radiance Field is trained from synthetic
images and exploited to generate an augmented set. Our method enriches the
initial set by enabling the synthesis of images with (i) unseen viewpoints,
(ii) rich illumination conditions through appearance extrapolation, and (iii)
randomized textures. We validate our augmentation method on the challenging
use-case of spacecraft pose estimation and show that it significantly improves
the pose estimation generalization capabilities. On the SPEED+ dataset, our
method reduces the error on the pose by 50% on both target domains.";Antoine Legrand<author:sep>Renaud Detry<author:sep>Christophe De Vleeschouwer;http://arxiv.org/pdf/2407.10762v1;cs.CV;;nerf
2407.10482v1;http://arxiv.org/abs/2407.10482v1;2024-07-15;NGP-RT: Fusing Multi-Level Hash Features with Lightweight Attention for  Real-Time Novel View Synthesis;"This paper presents NGP-RT, a novel approach for enhancing the rendering
speed of Instant-NGP to achieve real-time novel view synthesis. As a classic
NeRF-based method, Instant-NGP stores implicit features in multi-level grids or
hash tables and applies a shallow MLP to convert the implicit features into
explicit colors and densities. Although it achieves fast training speed, there
is still a lot of room for improvement in its rendering speed due to the
per-point MLP executions for implicit multi-level feature aggregation,
especially for real-time applications. To address this challenge, our proposed
NGP-RT explicitly stores colors and densities as hash features, and leverages a
lightweight attention mechanism to disambiguate the hash collisions instead of
using computationally intensive MLP. At the rendering stage, NGP-RT
incorporates a pre-computed occupancy distance grid into the ray marching
strategy to inform the distance to the nearest occupied voxel, thereby reducing
the number of marching points and global memory access. Experimental results
show that on the challenging Mip-NeRF360 dataset, NGP-RT achieves better
rendering quality than previous NeRF-based methods, achieving 108 fps at 1080p
resolution on a single Nvidia RTX 3090 GPU. Our approach is promising for
NeRF-based real-time applications that require efficient and high-quality
rendering.";Yubin Hu<author:sep>Xiaoyang Guo<author:sep>Yang Xiao<author:sep>Jingwei Huang<author:sep>Yong-Jin Liu;http://arxiv.org/pdf/2407.10482v1;cs.CV;ECCV 2024;nerf
2407.10743v1;http://arxiv.org/abs/2407.10743v1;2024-07-15;Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using  Datagraphs;"This paper addresses the challenge of scaling Large Multimodal Models (LMMs)
to expansive 3D environments. Solving this open problem is especially relevant
for robot deployment in many first-responder scenarios, such as
search-and-rescue missions that cover vast spaces. The use of LMMs in these
settings is currently hampered by the strict context windows that limit the
LMM's input size. We therefore introduce a novel approach that utilizes a
datagraph structure, which allows the LMM to iteratively query smaller sections
of a large environment. Using the datagraph in conjunction with graph traversal
algorithms, we can prioritize the most relevant locations to the query, thereby
improving the scalability of 3D scene language tasks. We illustrate the
datagraph using 3D scenes, but these can be easily substituted by other dense
modalities that represent the environment, such as pointclouds or Gaussian
splats. We demonstrate the potential to use the datagraph for two 3D scene
language task use cases, in a search-and-rescue mission example.";W. J. Meijer<author:sep>A. C. Kemmeren<author:sep>E. H. J. Riemens<author:sep>J. E. Fransman<author:sep>M. van Bekkum<author:sep>G. J. Burghouts<author:sep>J. D. van Mil;http://arxiv.org/pdf/2407.10743v1;cs.RO;"Accepted to the RSS Workshop on Semantics for Robotics: From
  Environment Understanding and Reasoning to Safe Interaction 2024";
2407.10695v1;http://arxiv.org/abs/2407.10695v1;2024-07-15;IE-NeRF: Inpainting Enhanced Neural Radiance Fields in the Wild;"We present a novel approach for synthesizing realistic novel views using
Neural Radiance Fields (NeRF) with uncontrolled photos in the wild. While NeRF
has shown impressive results in controlled settings, it struggles with
transient objects commonly found in dynamic and time-varying scenes. Our
framework called \textit{Inpainting Enhanced NeRF}, or \ours, enhances the
conventional NeRF by drawing inspiration from the technique of image
inpainting. Specifically, our approach extends the Multi-Layer Perceptrons
(MLP) of NeRF, enabling it to simultaneously generate intrinsic properties
(static color, density) and extrinsic transient masks. We introduce an
inpainting module that leverages the transient masks to effectively exclude
occlusions, resulting in improved volume rendering quality. Additionally, we
propose a new training strategy with frequency regularization to address the
sparsity issue of low-frequency transient components. We evaluate our approach
on internet photo collections of landmarks, demonstrating its ability to
generate high-quality novel views and achieve state-of-the-art performance.";Shuaixian Wang<author:sep>Haoran Xu<author:sep>Yaokun Li<author:sep>Jiwei Chen<author:sep>Guang Tan;http://arxiv.org/pdf/2407.10695v1;cs.CV;;nerf
2407.10318v2;http://arxiv.org/abs/2407.10318v2;2024-07-14;RecGS: Removing Water Caustic with Recurrent Gaussian Splatting;"Water caustics are commonly observed in seafloor imaging data from
shallow-water areas. Traditional methods that remove caustic patterns from
images often rely on 2D filtering or pre-training on an annotated dataset,
hindering the performance when generalizing to real-world seafloor data with 3D
structures. In this paper, we present a novel method Recurrent Gaussian
Splatting (RecGS), which takes advantage of today's photorealistic 3D
reconstruction technology, 3DGS, to separate caustics from seafloor imagery.
With a sequence of images taken by an underwater robot, we build 3DGS
recurrently and decompose the caustic with low-pass filtering in each
iteration. In the experiments, we analyze and compare with different methods,
including joint optimization, 2D filtering, and deep learning approaches. The
results show that our method can effectively separate the caustic from the
seafloor, improving the visual appearance, and can be potentially applied on
more problems with inconsistent illumination.";Tianyi Zhang<author:sep>Weiming Zhi<author:sep>Kaining Huang<author:sep>Joshua Mangelson<author:sep>Corina Barbalata<author:sep>Matthew Johnson-Roberson;http://arxiv.org/pdf/2407.10318v2;cs.CV;8 pages, 9 figures;gaussian splatting
2407.10267v1;http://arxiv.org/abs/2407.10267v1;2024-07-14;RS-NeRF: Neural Radiance Fields from Rolling Shutter Images;"Neural Radiance Fields (NeRFs) have become increasingly popular because of
their impressive ability for novel view synthesis. However, their effectiveness
is hindered by the Rolling Shutter (RS) effects commonly found in most camera
systems. To solve this, we present RS-NeRF, a method designed to synthesize
normal images from novel views using input with RS distortions. This involves a
physical model that replicates the image formation process under RS conditions
and jointly optimizes NeRF parameters and camera extrinsic for each image row.
We further address the inherent shortcomings of the basic RS-NeRF model by
delving into the RS characteristics and developing algorithms to enhance its
functionality. First, we impose a smoothness regularization to better estimate
trajectories and improve the synthesis quality, in line with the camera
movement prior. We also identify and address a fundamental flaw in the vanilla
RS model by introducing a multi-sampling algorithm. This new approach improves
the model's performance by comprehensively exploiting the RGB data across
different rows for each intermediate camera pose. Through rigorous
experimentation, we demonstrate that RS-NeRF surpasses previous methods in both
synthetic and real-world scenarios, proving its ability to correct RS-related
distortions effectively. Codes and data available:
https://github.com/MyNiuuu/RS-NeRF";Muyao Niu<author:sep>Tong Chen<author:sep>Yifan Zhan<author:sep>Zhuoxiao Li<author:sep>Xiang Ji<author:sep>Yinqiang Zheng;http://arxiv.org/pdf/2407.10267v1;cs.CV;"ECCV 2024 ; Codes and data: https://github.com/MyNiuuu/RS-NeRF";nerf
2407.10062v1;http://arxiv.org/abs/2407.10062v1;2024-07-14;SpikeGS: 3D Gaussian Splatting from Spike Streams with High-Speed Camera  Motion;"Novel View Synthesis plays a crucial role by generating new 2D renderings
from multi-view images of 3D scenes. However, capturing high-speed scenes with
conventional cameras often leads to motion blur, hindering the effectiveness of
3D reconstruction. To address this challenge, high-frame-rate dense 3D
reconstruction emerges as a vital technique, enabling detailed and accurate
modeling of real-world objects or scenes in various fields, including Virtual
Reality or embodied AI. Spike cameras, a novel type of neuromorphic sensor,
continuously record scenes with an ultra-high temporal resolution, showing
potential for accurate 3D reconstruction. Despite their promise, existing
approaches, such as applying Neural Radiance Fields (NeRF) to spike cameras,
encounter challenges due to the time-consuming rendering process. To address
this issue, we make the first attempt to introduce the 3D Gaussian Splatting
(3DGS) into spike cameras in high-speed capture, providing 3DGS as dense and
continuous clues of views, then constructing SpikeGS. Specifically, to train
SpikeGS, we establish computational equations between the rendering process of
3DGS and the processes of instantaneous imaging and exposing-like imaging of
the continuous spike stream. Besides, we build a very lightweight but effective
mapping process from spikes to instant images to support training. Furthermore,
we introduced a new spike-based 3D rendering dataset for validation. Extensive
experiments have demonstrated our method possesses the high quality of novel
view rendering, proving the tremendous potential of spike cameras in modeling
3D scenes.";Jiyuan Zhang<author:sep>Kang Chen<author:sep>Shiyan Chen<author:sep>Yajing Zheng<author:sep>Tiejun Huang<author:sep>Zhaofei Yu;http://arxiv.org/pdf/2407.10062v1;cs.CV;;gaussian splatting<tag:sep>nerf
2407.10102v1;http://arxiv.org/abs/2407.10102v1;2024-07-14;3DEgo: 3D Editing on the Go!;"We introduce 3DEgo to address a novel problem of directly synthesizing
photorealistic 3D scenes from monocular videos guided by textual prompts.
Conventional methods construct a text-conditioned 3D scene through a
three-stage process, involving pose estimation using Structure-from-Motion
(SfM) libraries like COLMAP, initializing the 3D model with unedited images,
and iteratively updating the dataset with edited images to achieve a 3D scene
with text fidelity. Our framework streamlines the conventional multi-stage 3D
editing process into a single-stage workflow by overcoming the reliance on
COLMAP and eliminating the cost of model initialization. We apply a diffusion
model to edit video frames prior to 3D scene creation by incorporating our
designed noise blender module for enhancing multi-view editing consistency, a
step that does not require additional training or fine-tuning of T2I diffusion
models. 3DEgo utilizes 3D Gaussian Splatting to create 3D scenes from the
multi-view consistent edited frames, capitalizing on the inherent temporal
continuity and explicit point cloud data. 3DEgo demonstrates remarkable editing
precision, speed, and adaptability across a variety of video sources, as
validated by extensive evaluations on six datasets, including our own prepared
GS25 dataset. Project Page: https://3dego.github.io/";Umar Khalid<author:sep>Hasan Iqbal<author:sep>Azib Farooq<author:sep>Jing Hua<author:sep>Chen Chen;http://arxiv.org/pdf/2407.10102v1;cs.CV;ECCV 2024 Accepted Paper;gaussian splatting
2407.09733v1;http://arxiv.org/abs/2407.09733v1;2024-07-13;Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity;"In this paper, we introduce Textured-GS, an innovative method for rendering
Gaussian splatting that incorporates spatially defined color and opacity
variations using Spherical Harmonics (SH). This approach enables each Gaussian
to exhibit a richer representation by accommodating varying colors and
opacities across its surface, significantly enhancing rendering quality
compared to traditional methods. To demonstrate the merits of our approach, we
have adapted the Mini-Splatting architecture to integrate textured Gaussians
without increasing the number of Gaussians. Our experiments across multiple
real-world datasets show that Textured-GS consistently outperforms both the
baseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The
results highlight the potential of Textured-GS to advance Gaussian-based
rendering technologies, promising more efficient and high-quality scene
reconstructions.";Zhentao Huang<author:sep>Minglun Gong;http://arxiv.org/pdf/2407.09733v1;cs.CV;9 pages;gaussian splatting
2407.09473v1;http://arxiv.org/abs/2407.09473v1;2024-07-12;StyleSplat: 3D Object Style Transfer with Gaussian Splatting;"Recent advancements in radiance fields have opened new avenues for creating
high-quality 3D assets and scenes. Style transfer can enhance these 3D assets
with diverse artistic styles, transforming creative expression. However,
existing techniques are often slow or unable to localize style transfer to
specific objects. We introduce StyleSplat, a lightweight method for stylizing
3D objects in scenes represented by 3D Gaussians from reference style images.
Our approach first learns a photorealistic representation of the scene using 3D
Gaussian splatting while jointly segmenting individual 3D objects. We then use
a nearest-neighbor feature matching loss to finetune the Gaussians of the
selected objects, aligning their spherical harmonic coefficients with the style
image to ensure consistency and visual appeal. StyleSplat allows for quick,
customizable style transfer and localized stylization of multiple objects
within a scene, each with a different style. We demonstrate its effectiveness
across various 3D scenes and styles, showcasing enhanced control and
customization in 3D creation.";Sahil Jain<author:sep>Avik Kuthiala<author:sep>Prabhdeep Singh Sethi<author:sep>Prakanshul Saxena;http://arxiv.org/pdf/2407.09473v1;cs.CV;for code and results, see http://bernard0047.github.io/stylesplat;gaussian splatting
2407.09386v1;http://arxiv.org/abs/2407.09386v1;2024-07-12;Radiance Fields from Photons;"Neural radiance fields, or NeRFs, have become the de facto approach for
high-quality view synthesis from a collection of images captured from multiple
viewpoints. However, many issues remain when capturing images in-the-wild under
challenging conditions, such as low light, high dynamic range, or rapid motion
leading to smeared reconstructions with noticeable artifacts. In this work, we
introduce quanta radiance fields, a novel class of neural radiance fields that
are trained at the granularity of individual photons using single-photon
cameras (SPCs). We develop theory and practical computational techniques for
building radiance fields and estimating dense camera poses from unconventional,
stochastic, and high-speed binary frame sequences captured by SPCs. We
demonstrate, both via simulations and a SPC hardware prototype, high-fidelity
reconstructions under high-speed motion, in low light, and for extreme dynamic
range settings.";Sacha Jungerman<author:sep>Mohit Gupta;http://arxiv.org/pdf/2407.09386v1;cs.CV;;nerf
2407.09026v1;http://arxiv.org/abs/2407.09026v1;2024-07-12;HPC: Hierarchical Progressive Coding Framework for Volumetric Video;"Volumetric video based on Neural Radiance Field (NeRF) holds vast potential
for various 3D applications, but its substantial data volume poses significant
challenges for compression and transmission. Current NeRF compression lacks the
flexibility to adjust video quality and bitrate within a single model for
various network and device capacities. To address these issues, we propose HPC,
a novel hierarchical progressive volumetric video coding framework achieving
variable bitrate using a single model. Specifically, HPC introduces a
hierarchical representation with a multi-resolution residual radiance field to
reduce temporal redundancy in long-duration sequences while simultaneously
generating various levels of detail. Then, we propose an end-to-end progressive
learning approach with a multi-rate-distortion loss function to jointly
optimize both hierarchical representation and compression. Our HPC trained only
once can realize multiple compression levels, while the current methods need to
train multiple fixed-bitrate models for different rate-distortion (RD)
tradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality
levels with variable bitrate by a single model and exhibits competitive RD
performance, even outperforming fixed-bitrate models across various datasets.";Zihan Zheng<author:sep>Houqiang Zhong<author:sep>Qiang Hu<author:sep>Xiaoyun Zhang<author:sep>Li Song<author:sep>Ya Zhang<author:sep>Yanfeng Wang;http://arxiv.org/pdf/2407.09026v1;cs.CV;11 pages, 7 figures;nerf
2407.09679v1;http://arxiv.org/abs/2407.09679v1;2024-07-12;Physics-Informed Learning of Characteristic Trajectories for Smoke  Reconstruction;"We delve into the physics-informed neural reconstruction of smoke and
obstacles through sparse-view RGB videos, tackling challenges arising from
limited observation of complex dynamics. Existing physics-informed neural
networks often emphasize short-term physics constraints, leaving the proper
preservation of long-term conservation less explored. We introduce Neural
Characteristic Trajectory Fields, a novel representation utilizing Eulerian
neural fields to implicitly model Lagrangian fluid trajectories. This
topology-free, auto-differentiable representation facilitates efficient flow
map calculations between arbitrary frames as well as efficient velocity
extraction via auto-differentiation. Consequently, it enables end-to-end
supervision covering long-term conservation and short-term physics priors.
Building on the representation, we propose physics-informed trajectory learning
and integration into NeRF-based scene reconstruction. We enable advanced
obstacle handling through self-supervised scene decomposition and seamless
integrated boundary constraints. Our results showcase the ability to overcome
challenges like occlusion uncertainty, density-color ambiguity, and
static-dynamic entanglements. Code and sample tests are at
\url{https://github.com/19reborn/PICT_smoke}.";Yiming Wang<author:sep>Siyu Tang<author:sep>Mengyu Chu;http://arxiv.org/pdf/2407.09679v1;cs.CV;"SIGGRAPH 2024 (conference track), Project Website:
  \url{https://19reborn.github.io/PICT_Smoke.github.io/}";nerf
2407.08154v1;http://arxiv.org/abs/2407.08154v1;2024-07-11;Bayesian uncertainty analysis for underwater 3D reconstruction with  neural radiance fields;"Neural radiance fields (NeRFs) are a deep learning technique that can
generate novel views of 3D scenes using sparse 2D images from different viewing
directions and camera poses. As an extension of conventional NeRFs in
underwater environment, where light can get absorbed and scattered by water,
SeaThru-NeRF was proposed to separate the clean appearance and geometric
structure of underwater scene from the effects of the scattering medium. Since
the quality of the appearance and structure of underwater scenes is crucial for
downstream tasks such as underwater infrastructure inspection, the reliability
of the 3D reconstruction model should be considered and evaluated. Nonetheless,
owing to the lack of ability to quantify uncertainty in 3D reconstruction of
underwater scenes under natural ambient illumination, the practical deployment
of NeRFs in unmanned autonomous underwater navigation is limited. To address
this issue, we introduce a spatial perturbation field D_omega based on Bayes'
rays in SeaThru-NeRF and perform Laplace approximation to obtain a Gaussian
distribution N(0,Sigma) of the parameters omega, where the diagonal elements of
Sigma correspond to the uncertainty at each spatial location. We also employ a
simple thresholding method to remove artifacts from the rendered results of
underwater scenes. Numerical experiments are provided to demonstrate the
effectiveness of this approach.";Haojie Lian<author:sep>Xinhao Li<author:sep>Yilin Qu<author:sep>Jing Du<author:sep>Zhuxuan Meng<author:sep>Jie Liu<author:sep>Leilei Chen;http://arxiv.org/pdf/2407.08154v1;cs.CE;;nerf
2407.08137v1;http://arxiv.org/abs/2407.08137v1;2024-07-11;Survey on Fundamental Deep Learning 3D Reconstruction Techniques;"This survey aims to investigate fundamental deep learning (DL) based 3D
reconstruction techniques that produce photo-realistic 3D models and scenes,
highlighting Neural Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and
3D Gaussian Splatting. We dissect the underlying algorithms, evaluate their
strengths and tradeoffs, and project future research trajectories in this
rapidly evolving field. We provide a comprehensive overview of the fundamental
in DL-driven 3D scene reconstruction, offering insights into their potential
applications and limitations.";Yonge Bai<author:sep>LikHang Wong<author:sep>TszYin Twan;http://arxiv.org/pdf/2407.08137v1;cs.CV;;gaussian splatting<tag:sep>nerf
2407.08414v1;http://arxiv.org/abs/2407.08414v1;2024-07-11;MeshAvatar: Learning High-quality Triangular Human Avatars from  Multi-view Videos;"We present a novel pipeline for learning high-quality triangular human
avatars from multi-view videos. Recent methods for avatar learning are
typically based on neural radiance fields (NeRF), which is not compatible with
traditional graphics pipeline and poses great challenges for operations like
editing or synthesizing under different environments. To overcome these
limitations, our method represents the avatar with an explicit triangular mesh
extracted from an implicit SDF field, complemented by an implicit material
field conditioned on given poses. Leveraging this triangular avatar
representation, we incorporate physics-based rendering to accurately decompose
geometry and texture. To enhance both the geometric and appearance details, we
further employ a 2D UNet as the network backbone and introduce pseudo normal
ground-truth as additional supervision. Experiments show that our method can
learn triangular avatars with high-quality geometry reconstruction and
plausible material decomposition, inherently supporting editing, manipulation
or relighting operations.";Yushuo Chen<author:sep>Zerong Zheng<author:sep>Zhe Li<author:sep>Chao Xu<author:sep>Yebin Liu;http://arxiv.org/pdf/2407.08414v1;cs.CV;Project Page: https://shad0wta9.github.io/meshavatar-page/;nerf
2407.08795v1;http://arxiv.org/abs/2407.08795v1;2024-07-11;Feasibility of Neural Radiance Fields for Crime Scene Video  Reconstruction;"This paper aims to review and determine the feasibility of using variations
of NeRF models in order to reconstruct crime scenes given input videos of the
scene. We focus on three main innovations of NeRF when it comes to
reconstructing crime scenes: Multi-object Synthesis, Deformable Synthesis, and
Lighting. From there, we analyse its innovation progress against the
requirements to be met in order to be able to reconstruct crime scenes with
given videos of such scenes.";Shariq Nadeem Malik<author:sep>Min Hao Chee<author:sep>Dayan Mario Anthony Perera<author:sep>Chern Hong Lim;http://arxiv.org/pdf/2407.08795v1;cs.CV;4 pages, 1 table;nerf
2407.08447v1;http://arxiv.org/abs/2407.08447v1;2024-07-11;WildGaussians: 3D Gaussian Splatting in the Wild;"While the field of 3D scene reconstruction is dominated by NeRFs due to their
photorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,
offering similar quality with real-time rendering speeds. However, both methods
primarily excel with well-controlled 3D scenes, while in-the-wild data -
characterized by occlusions, dynamic objects, and varying illumination -
remains challenging. NeRFs can adapt to such conditions easily through
per-image embedding vectors, but 3DGS struggles due to its explicit
representation and lack of shared parameters. To address this, we introduce
WildGaussians, a novel approach to handle occlusions and appearance changes
with 3DGS. By leveraging robust DINO features and integrating an appearance
modeling module within 3DGS, our method achieves state-of-the-art results. We
demonstrate that WildGaussians matches the real-time rendering speed of 3DGS
while surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all
within a simple architectural framework.";Jonas Kulhanek<author:sep>Songyou Peng<author:sep>Zuzana Kukelova<author:sep>Marc Pollefeys<author:sep>Torsten Sattler;http://arxiv.org/pdf/2407.08447v1;cs.CV;https://wild-gaussians.github.io/;gaussian splatting<tag:sep>nerf
2407.08165v1;http://arxiv.org/abs/2407.08165v1;2024-07-11;Explicit_NeRF_QA: A Quality Assessment Database for Explicit NeRF Model  Compression;"In recent years, Neural Radiance Fields (NeRF) have demonstrated significant
advantages in representing and synthesizing 3D scenes. Explicit NeRF models
facilitate the practical NeRF applications with faster rendering speed, and
also attract considerable attention in NeRF compression due to its huge storage
cost. To address the challenge of the NeRF compression study, in this paper, we
construct a new dataset, called Explicit_NeRF_QA. We use 22 3D objects with
diverse geometries, textures, and material complexities to train four typical
explicit NeRF models across five parameter levels. Lossy compression is
introduced during the model generation, pivoting the selection of key
parameters such as hash table size for InstantNGP and voxel grid resolution for
Plenoxels. By rendering NeRF samples to processed video sequences (PVS), a
large scale subjective experiment with lab environment is conducted to collect
subjective scores from 21 viewers. The diversity of content, accuracy of mean
opinion scores (MOS), and characteristics of NeRF distortion are
comprehensively presented, establishing the heterogeneity of the proposed
dataset. The state-of-the-art objective metrics are tested in the new dataset.
Best Person correlation, which is around 0.85, is collected from the
full-reference objective metric. All tested no-reference metrics report very
poor results with 0.4 to 0.6 correlations, demonstrating the need for further
development of more robust no-reference metrics. The dataset, including NeRF
samples, source 3D objects, multiview images for NeRF generation, PVSs, MOS, is
made publicly available at the following location:
https://github.com/LittlericeChloe/Explicit_NeRF_QA.";Yuke Xing<author:sep>Qi Yang<author:sep>Kaifa Yang<author:sep>Yilin Xu<author:sep>Zhu Li;http://arxiv.org/pdf/2407.08165v1;eess.IV;5 pages, 4 figures, 2 tables, conference;nerf
2407.07735v1;http://arxiv.org/abs/2407.07735v1;2024-07-10;Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model;"Neural Radiance Fields (NeRFs) have become a key method for 3D scene
representation. With the rising prominence and influence of NeRF, safeguarding
its intellectual property has become increasingly important. In this paper, we
propose \textbf{NeRFProtector}, which adopts a plug-and-play strategy to
protect NeRF's copyright during its creation. NeRFProtector utilizes a
pre-trained watermarking base model, enabling NeRF creators to embed binary
messages directly while creating their NeRF. Our plug-and-play property ensures
NeRF creators can flexibly choose NeRF variants without excessive
modifications. Leveraging our newly designed progressive distillation, we
demonstrate performance on par with several leading-edge neural rendering
methods. Our project is available at:
\url{https://qsong2001.github.io/NeRFProtector}.";Qi Song<author:sep>Ziyuan Luo<author:sep>Ka Chun Cheung<author:sep>Simon See<author:sep>Renjie Wan;http://arxiv.org/pdf/2407.07735v1;cs.CV;Accepted by ECCV2024;nerf
2407.07461v1;http://arxiv.org/abs/2407.07461v1;2024-07-10;Drantal-NeRF: Diffusion-Based Restoration for Anti-aliasing Neural  Radiance Field;"Aliasing artifacts in renderings produced by Neural Radiance Field (NeRF) is
a long-standing but complex issue in the field of 3D implicit representation,
which arises from a multitude of intricate causes and was mitigated by
designing more advanced but complex scene parameterization methods before. In
this paper, we present a Diffusion-based restoration method for anti-aliasing
Neural Radiance Field (Drantal-NeRF). We consider the anti-aliasing issue from
a low-level restoration perspective by viewing aliasing artifacts as a kind of
degradation model added to clean ground truths. By leveraging the powerful
prior knowledge encapsulated in diffusion model, we could restore the
high-realism anti-aliasing renderings conditioned on aliased low-quality
counterparts. We further employ a feature-wrapping operation to ensure
multi-view restoration consistency and finetune the VAE decoder to better adapt
to the scene-specific data distribution. Our proposed method is easy to
implement and agnostic to various NeRF backbones. We conduct extensive
experiments on challenging large-scale urban scenes as well as unbounded
360-degree scenes and achieve substantial qualitative and quantitative
improvements.";Ganlin Yang<author:sep>Kaidong Zhang<author:sep>Jingjing Fu<author:sep>Dong Liu;http://arxiv.org/pdf/2407.07461v1;cs.CV;;nerf
2407.07284v1;http://arxiv.org/abs/2407.07284v1;2024-07-10;MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition;"We introduce MIGS (Multi-Identity Gaussian Splatting), a novel method that
learns a single neural representation for multiple identities, using only
monocular videos. Recent 3D Gaussian Splatting (3DGS) approaches for human
avatars require per-identity optimization. However, learning a multi-identity
representation presents advantages in robustly animating humans under arbitrary
poses. We propose to construct a high-order tensor that combines all the
learnable 3DGS parameters for all the training identities. By assuming a
low-rank structure and factorizing the tensor, we model the complex rigid and
non-rigid deformations of multiple subjects in a unified network, significantly
reducing the total number of parameters. Our proposed approach leverages
information from all the training identities, enabling robust animation under
challenging unseen poses, outperforming existing approaches. We also
demonstrate how it can be extended to learn unseen identities.";Aggelina Chatziagapi<author:sep>Grigorios G. Chrysos<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2407.07284v1;cs.CV;"Accepted by ECCV 2024. Project page:
  https://aggelinacha.github.io/MIGS/";gaussian splatting
2407.06613v1;http://arxiv.org/abs/2407.06613v1;2024-07-09;Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View;"Recent studies construct deblurred neural radiance fields (DeRF) using dozens
of blurry images, which are not practical scenarios if only a limited number of
blurry images are available. This paper focuses on constructing DeRF from
sparse-view for more pragmatic real-world scenarios. As observed in our
experiments, establishing DeRF from sparse views proves to be a more
challenging problem due to the inherent complexity arising from the
simultaneous optimization of blur kernels and NeRF from sparse view.
Sparse-DeRF successfully regularizes the complicated joint optimization,
presenting alleviated overfitting artifacts and enhanced quality on radiance
fields. The regularization consists of three key components: Surface
smoothness, helps the model accurately predict the scene structure utilizing
unseen and additional hidden rays derived from the blur kernel based on
statistical tendencies of real-world; Modulated gradient scaling, helps the
model adjust the amount of the backpropagated gradient according to the
arrangements of scene objects; Perceptual distillation improves the perceptual
quality by overcoming the ill-posed multi-view inconsistency of image
deblurring and distilling the pre-filtered information, compensating for the
lack of clean information in blurry images. We demonstrate the effectiveness of
the Sparse-DeRF with extensive quantitative and qualitative experimental
results by training DeRF from 2-view, 4-view, and 6-view blurry images.";Dogyoon Lee<author:sep>Donghyeong Kim<author:sep>Jungho Lee<author:sep>Minhyeok Lee<author:sep>Seunghoon Lee<author:sep>Sangyoun Lee;http://arxiv.org/pdf/2407.06613v1;cs.CV;Project page: https://dogyoonlee.github.io/sparsederf/;nerf
2407.07220v1;http://arxiv.org/abs/2407.07220v1;2024-07-09;Reference-based Controllable Scene Stylization with Gaussian Splatting;"Referenced-based scene stylization that edits the appearance based on a
content-aligned reference image is an emerging research area. Starting with a
pretrained neural radiance field (NeRF), existing methods typically learn a
novel appearance that matches the given style. Despite their effectiveness,
they inherently suffer from time-consuming volume rendering, and thus are
impractical for many real-time applications. In this work, we propose ReGS,
which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to
enable real-time stylized view synthesis. Editing the appearance of a
pretrained 3DGS is challenging as it uses discrete Gaussians as 3D
representation, which tightly bind appearance with geometry. Simply optimizing
the appearance as prior methods do is often insufficient for modeling
continuous textures in the given reference image. To address this challenge, we
propose a novel texture-guided control mechanism that adaptively adjusts local
responsible Gaussians to a new geometric arrangement, serving for desired
texture details. The proposed process is guided by texture clues for effective
appearance editing, and regularized by scene depth for preserving original
geometric structure. With these novel designs, we show ReGs can produce
state-of-the-art stylization results that respect the reference texture while
embracing real-time rendering speed for free-view navigation.";Yiqun Mei<author:sep>Jiacong Xu<author:sep>Vishal M. Patel;http://arxiv.org/pdf/2407.07220v1;cs.CV;;gaussian splatting<tag:sep>nerf
2407.07090v2;http://arxiv.org/abs/2407.07090v2;2024-07-09;3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes;"Particle-based representations of radiance fields such as 3D Gaussian
Splatting have found great success for reconstructing and re-rendering of
complex scenes. Most existing methods render particles via rasterization,
projecting them to screen space tiles for processing in a sorted order. This
work instead considers ray tracing the particles, building a bounding volume
hierarchy and casting a ray for each pixel using high-performance GPU ray
tracing hardware. To efficiently handle large numbers of semi-transparent
particles, we describe a specialized rendering algorithm which encapsulates
particles with bounding meshes to leverage fast ray-triangle intersections, and
shades batches of intersections in depth-order. The benefits of ray tracing are
well-known in computer graphics: processing incoherent rays for secondary
lighting effects such as shadows and reflections, rendering from
highly-distorted cameras common in robotics, stochastically sampling rays, and
more. With our renderer, this flexibility comes at little cost compared to
rasterization. Experiments demonstrate the speed and accuracy of our approach,
as well as several applications in computer graphics and vision. We further
propose related improvements to the basic Gaussian representation, including a
simple use of generalized kernel functions which significantly reduces particle
hit counts.";Nicolas Moenne-Loccoz<author:sep>Ashkan Mirzaei<author:sep>Or Perel<author:sep>Riccardo de Lutio<author:sep>Janick Martinez Esturo<author:sep>Gavriel State<author:sep>Sanja Fidler<author:sep>Nicholas Sharp<author:sep>Zan Gojcic;http://arxiv.org/pdf/2407.07090v2;cs.GR;Project page: https://gaussiantracer.github.io/;
2407.06397v1;http://arxiv.org/abs/2407.06397v1;2024-07-08;RRM: Relightable assets using Radiance guided Material extraction;"Synthesizing NeRFs under arbitrary lighting has become a seminal problem in
the last few years. Recent efforts tackle the problem via the extraction of
physically-based parameters that can then be rendered under arbitrary lighting,
but they are limited in the range of scenes they can handle, usually
mishandling glossy scenes. We propose RRM, a method that can extract the
materials, geometry, and environment lighting of a scene even in the presence
of highly reflective objects. Our method consists of a physically-aware
radiance field representation that informs physically-based parameters, and an
expressive environment light structure based on a Laplacian Pyramid. We
demonstrate that our contributions outperform the state-of-the-art on parameter
retrieval tasks, leading to high-fidelity relighting and novel view synthesis
on surfacic scenes.";Diego Gomez<author:sep>Julien Philip<author:sep>Adrien Kaiser<author:sep>Élie Michel;http://arxiv.org/pdf/2407.06397v1;cs.CV;Paper accepted and presented at CGI 2024;nerf
2407.05666v1;http://arxiv.org/abs/2407.05666v1;2024-07-08;Enhancing Neural Radiance Fields with Depth and Normal Completion Priors  from Sparse Views;"Neural Radiance Fields (NeRF) are an advanced technology that creates highly
realistic images by learning about scenes through a neural network model.
However, NeRF often encounters issues when there are not enough images to work
with, leading to problems in accurately rendering views. The main issue is that
NeRF lacks sufficient structural details to guide the rendering process
accurately. To address this, we proposed a Depth and Normal Dense Completion
Priors for NeRF (CP\_NeRF) framework. This framework enhances view rendering by
adding depth and normal dense completion priors to the NeRF optimization
process. Before optimizing NeRF, we obtain sparse depth maps using the
Structure from Motion (SfM) technique used to get camera poses. Based on the
sparse depth maps and a normal estimator, we generate sparse normal maps for
training a normal completion prior with precise standard deviations. During
optimization, we apply depth and normal completion priors to transform sparse
data into dense depth and normal maps with their standard deviations. We use
these dense maps to guide ray sampling, assist distance sampling and construct
a normal loss function for better training accuracy. To improve the rendering
of NeRF's normal outputs, we incorporate an optical centre position embedder
that helps synthesize more accurate normals through volume rendering.
Additionally, we employ a normal patch matching technique to choose accurate
rendered normal maps, ensuring more precise supervision for the model. Our
method is superior to leading techniques in rendering detailed indoor scenes,
even with limited input views.";Jiawei Guo<author:sep>HungChyun Chou<author:sep>Ning Ding;http://arxiv.org/pdf/2407.05666v1;cs.CV;;nerf
2407.05586v1;http://arxiv.org/abs/2407.05586v1;2024-07-08;Dynamic Neural Radiance Field From Defocused Monocular Video;"Dynamic Neural Radiance Field (NeRF) from monocular videos has recently been
explored for space-time novel view synthesis and achieved excellent results.
However, defocus blur caused by depth variation often occurs in video capture,
compromising the quality of dynamic reconstruction because the lack of sharp
details interferes with modeling temporal consistency between input views. To
tackle this issue, we propose D2RF, the first dynamic NeRF method designed to
restore sharp novel views from defocused monocular videos. We introduce layered
Depth-of-Field (DoF) volume rendering to model the defocus blur and reconstruct
a sharp NeRF supervised by defocused views. The blur model is inspired by the
connection between DoF rendering and volume rendering. The opacity in volume
rendering aligns with the layer visibility in DoF rendering.To execute the
blurring, we modify the layered blur kernel to the ray-based kernel and employ
an optimized sparse kernel to gather the input rays efficiently and render the
optimized rays with our layered DoF volume rendering. We synthesize a dataset
with defocused dynamic scenes for our task, and extensive experiments on our
dataset show that our method outperforms existing approaches in synthesizing
all-in-focus novel views from defocus blur while maintaining spatial-temporal
consistency in the scene.";Xianrui Luo<author:sep>Huiqiang Sun<author:sep>Juewen Peng<author:sep>Zhiguo Cao;http://arxiv.org/pdf/2407.05586v1;cs.CV;Accepted by ECCV 2024;nerf
2407.05597v1;http://arxiv.org/abs/2407.05597v1;2024-07-08;GeoNLF: Geometry guided Pose-Free Neural LiDAR Fields;"Although recent efforts have extended Neural Radiance Fields (NeRF) into
LiDAR point cloud synthesis, the majority of existing works exhibit a strong
dependence on precomputed poses. However, point cloud registration methods
struggle to achieve precise global pose estimation, whereas previous pose-free
NeRFs overlook geometric consistency in global reconstruction. In light of
this, we explore the geometric insights of point clouds, which provide explicit
registration priors for reconstruction. Based on this, we propose Geometry
guided Neural LiDAR Fields(GeoNLF), a hybrid framework performing alternately
global neural reconstruction and pure geometric pose optimization. Furthermore,
NeRFs tend to overfit individual frames and easily get stuck in local minima
under sparse-view inputs. To tackle this issue, we develop a
selective-reweighting strategy and introduce geometric constraints for robust
optimization. Extensive experiments on NuScenes and KITTI-360 datasets
demonstrate the superiority of GeoNLF in both novel view synthesis and
multi-view registration of low-frequency large-scale point clouds.";Weiyi Xue<author:sep>Zehan Zheng<author:sep>Fan Lu<author:sep>Haiyun Wei<author:sep>Guang Chen<author:sep>Changjun Jiang;http://arxiv.org/pdf/2407.05597v1;cs.CV;;nerf
2407.06150v1;http://arxiv.org/abs/2407.06150v1;2024-07-08;PanDORA: Casual HDR Radiance Acquisition for Indoor Scenes;"Most novel view synthesis methods such as NeRF are unable to capture the true
high dynamic range (HDR) radiance of scenes since they are typically trained on
photos captured with standard low dynamic range (LDR) cameras. While the
traditional exposure bracketing approach which captures several images at
different exposures has recently been adapted to the multi-view case, we find
such methods to fall short of capturing the full dynamic range of indoor
scenes, which includes very bright light sources. In this paper, we present
PanDORA: a PANoramic Dual-Observer Radiance Acquisition system for the casual
capture of indoor scenes in high dynamic range. Our proposed system comprises
two 360{\deg} cameras rigidly attached to a portable tripod. The cameras
simultaneously acquire two 360{\deg} videos: one at a regular exposure and the
other at a very fast exposure, allowing a user to simply wave the apparatus
casually around the scene in a matter of minutes. The resulting images are fed
to a NeRF-based algorithm that reconstructs the scene's full high dynamic
range. Compared to HDR baselines from previous work, our approach reconstructs
the full HDR radiance of indoor scenes without sacrificing the visual quality
while retaining the ease of capture from recent NeRF-like approaches.";Mohammad Reza Karimi Dastjerdi<author:sep>Frédéric Fortier-Chouinard<author:sep>Yannick Hold-Geoffroy<author:sep>Marc Hébert<author:sep>Claude Demers<author:sep>Nima Kalantari<author:sep>Jean-François Lalonde;http://arxiv.org/pdf/2407.06150v1;cs.CV;10 pages, 8 figures;nerf
2407.05254v1;http://arxiv.org/abs/2407.05254v1;2024-07-07;GaussReg: Fast 3D Registration with Gaussian Splatting;"Point cloud registration is a fundamental problem for large-scale 3D scene
scanning and reconstruction. With the help of deep learning, registration
methods have evolved significantly, reaching a nearly-mature stage. As the
introduction of Neural Radiance Fields (NeRF), it has become the most popular
3D scene representation as its powerful view synthesis capabilities. Regarding
NeRF representation, its registration is also required for large-scale scene
reconstruction. However, this topic extremly lacks exploration. This is due to
the inherent challenge to model the geometric relationship among two scenes
with implicit representations. The existing methods usually convert the
implicit representation to explicit representation for further registration.
Most recently, Gaussian Splatting (GS) is introduced, employing explicit 3D
Gaussian. This method significantly enhances rendering speed while maintaining
high rendering quality. Given two scenes with explicit GS representations, in
this work, we explore the 3D registration task between them. To this end, we
propose GaussReg, a novel coarse-to-fine framework, both fast and accurate. The
coarse stage follows existing point cloud registration methods and estimates a
rough alignment for point clouds from GS. We further newly present an
image-guided fine registration approach, which renders images from GS to
provide more detailed geometric information for precise alignment. To support
comprehensive evaluation, we carefully build a scene-level dataset called
ScanNet-GSReg with 1379 scenes obtained from the ScanNet dataset and collect an
in-the-wild dataset called GSReg. Experimental results demonstrate our method
achieves state-of-the-art performance on multiple datasets. Our GaussReg is 44
times faster than HLoc (SuperPoint as the feature extractor and SuperGlue as
the matcher) with comparable accuracy.";Jiahao Chang<author:sep>Yinglin Xu<author:sep>Yihao Li<author:sep>Yuantao Chen<author:sep>Xiaoguang Han;http://arxiv.org/pdf/2407.05254v1;cs.CV;ECCV 2024;gaussian splatting<tag:sep>nerf
2407.05324v1;http://arxiv.org/abs/2407.05324v1;2024-07-07;PICA: Physics-Integrated Clothed Avatar;"We introduce PICA, a novel representation for high-fidelity animatable
clothed human avatars with physics-accurate dynamics, even for loose clothing.
Previous neural rendering-based representations of animatable clothed humans
typically employ a single model to represent both the clothing and the
underlying body. While efficient, these approaches often fail to accurately
represent complex garment dynamics, leading to incorrect deformations and
noticeable rendering artifacts, especially for sliding or loose garments.
Furthermore, previous works represent garment dynamics as pose-dependent
deformations and facilitate novel pose animations in a data-driven manner. This
often results in outcomes that do not faithfully represent the mechanics of
motion and are prone to generating artifacts in out-of-distribution poses. To
address these issues, we adopt two individual 3D Gaussian Splatting (3DGS)
models with different deformation characteristics, modeling the human body and
clothing separately. This distinction allows for better handling of their
respective motion characteristics. With this representation, we integrate a
graph neural network (GNN)-based clothed body physics simulation module to
ensure an accurate representation of clothing dynamics. Our method, through its
carefully designed features, achieves high-fidelity rendering of clothed human
bodies in complex and novel driving poses, significantly outperforming previous
methods under the same settings.";Bo Peng<author:sep>Yunfan Tao<author:sep>Haoyu Zhan<author:sep>Yudong Guo<author:sep>Juyong Zhang;http://arxiv.org/pdf/2407.05324v1;cs.CV;Project page: https://ustc3dv.github.io/PICA/;gaussian splatting
2407.05023v1;http://arxiv.org/abs/2407.05023v1;2024-07-06;SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical  Scene Reconstruction;"Dynamic reconstruction of deformable tissues in endoscopic video is a key
technology for robot-assisted surgery. Recent reconstruction methods based on
neural radiance fields (NeRFs) have achieved remarkable results in the
reconstruction of surgical scenes. However, based on implicit representation,
NeRFs struggle to capture the intricate details of objects in the scene and
cannot achieve real-time rendering. In addition, restricted single view
perception and occluded instruments also propose special challenges in surgical
scene reconstruction. To address these issues, we develop SurgicalGaussian, a
deformable 3D Gaussian Splatting method to model dynamic surgical scenes. Our
approach models the spatio-temporal features of soft tissues at each time stamp
via a forward-mapping deformation MLP and regularization to constrain local 3D
Gaussians to comply with consistent movement. With the depth initialization
strategy and tool mask-guided training, our method can remove surgical
instruments and reconstruct high-fidelity surgical scenes. Through experiments
on various surgical videos, our network outperforms existing method on many
aspects, including rendering quality, rendering speed and GPU usage. The
project page can be found at https://surgicalgaussian.github.io.";Weixing Xie<author:sep>Junfeng Yao<author:sep>Xianpeng Cao<author:sep>Qiqin Lin<author:sep>Zerui Tang<author:sep>Xiao Dong<author:sep>Xiaohu Guo;http://arxiv.org/pdf/2407.05023v1;cs.CV;;gaussian splatting<tag:sep>nerf
2407.04545v1;http://arxiv.org/abs/2407.04545v1;2024-07-05;Gaussian Eigen Models for Human Heads;"We present personalized Gaussian Eigen Models (GEMs) for human heads, a novel
method that compresses dynamic 3D Gaussians into low-dimensional linear spaces.
Our approach is inspired by the seminal work of Blanz and Vetter, where a
mesh-based 3D morphable model (3DMM) is constructed from registered meshes.
Based on dynamic 3D Gaussians, we create a lower-dimensional representation of
primitives that applies to most 3DGS head avatars. Specifically, we propose a
universal method to distill the appearance of a mesh-controlled UNet Gaussian
avatar using an ensemble of linear eigenbasis. We replace heavy CNN-based
architectures with a single linear layer improving speed and enabling a range
of real-time downstream applications. To create a particular facial expression,
one simply needs to perform a dot product between the eigen coefficients and
the distilled basis. This efficient method removes the requirement for an input
mesh during testing, enhancing simplicity and speed in expression generation.
This process is highly efficient and supports real-time rendering on everyday
devices, leveraging the effectiveness of standard Gaussian Splatting. In
addition, we demonstrate how the GEM can be controlled using a ResNet-based
regression architecture. We show and compare self-reenactment and cross-person
reenactment to state-of-the-art 3D avatar methods, demonstrating higher quality
and better control. A real-time demo showcases the applicability of the GEM
representation.";Wojciech Zielonka<author:sep>Timo Bolkart<author:sep>Thabo Beeler<author:sep>Justus Thies;http://arxiv.org/pdf/2407.04545v1;cs.CV;https://zielon.github.io/gem/;gaussian splatting
2407.04237v2;http://arxiv.org/abs/2407.04237v2;2024-07-05;GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction;"We present GSD, a diffusion model approach based on Gaussian Splatting (GS)
representation for 3D object reconstruction from a single view. Prior works
suffer from inconsistent 3D geometry or mediocre rendering quality due to
improper representations. We take a step towards resolving these shortcomings
by utilizing the recent state-of-the-art 3D explicit representation, Gaussian
Splatting, and an unconditional diffusion model. This model learns to generate
3D objects represented by sets of GS ellipsoids. With these strong generative
3D priors, though learning unconditionally, the diffusion model is ready for
view-guided reconstruction without further model fine-tuning. This is achieved
by propagating fine-grained 2D features through the efficient yet flexible
splatting function and the guided denoising sampling process. In addition, a 2D
diffusion model is further employed to enhance rendering fidelity, and improve
reconstructed GS quality by polishing and re-using the rendered images. The
final reconstructed objects explicitly come with high-quality 3D structure and
texture, and can be efficiently rendered in arbitrary views. Experiments on the
challenging real-world CO3D dataset demonstrate the superiority of our
approach. Project page: $\href{https://yxmu.foo/GSD/}{\text{this https URL}}$";Yuxuan Mu<author:sep>Xinxin Zuo<author:sep>Chuan Guo<author:sep>Yilin Wang<author:sep>Juwei Lu<author:sep>Xiaofeng Wu<author:sep>Songcen Xu<author:sep>Peng Dai<author:sep>Youliang Yan<author:sep>Li Cheng;http://arxiv.org/pdf/2407.04237v2;cs.CV;Accepted for ECCV 2024;gaussian splatting
2407.04504v2;http://arxiv.org/abs/2407.04504v2;2024-07-05;Segment Any 4D Gaussians;"Modeling, understanding, and reconstructing the real world are crucial in
XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable
success in modeling and understanding 3D scenes. Similarly, various 4D
representations have demonstrated the ability to capture the dynamics of the 4D
world. However, there is a dearth of research focusing on segmentation within
4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D),
one of the first frameworks to segment anything in the 4D digital world based
on 4D Gaussians. In SA4D, an efficient temporal identity feature field is
introduced to handle Gaussian drifting, with the potential to learn precise
identity features from noisy and sparse input. Additionally, a 4D segmentation
refinement process is proposed to remove artifacts. Our SA4D achieves precise,
high-quality segmentation within seconds in 4D Gaussians and shows the ability
to remove, recolor, compose, and render high-quality anything masks. More demos
are available at: https://jsxzs.github.io/sa4d/.";Shengxiang Ji<author:sep>Guanjun Wu<author:sep>Jiemin Fang<author:sep>Jiazhong Cen<author:sep>Taoran Yi<author:sep>Wenyu Liu<author:sep>Qi Tian<author:sep>Xinggang Wang;http://arxiv.org/pdf/2407.04504v2;cs.CV;22 pages;gaussian splatting
2407.03771v1;http://arxiv.org/abs/2407.03771v1;2024-07-04;SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors;"3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance
in 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.
Fulfilling this requirement can be challenging in real-world scenarios
especially when the camera moves fast, which severely limits the application of
3DGS. To address these challenges, we proposed Spike Gausian Splatting
(SpikeGS), the first framework that integrates the spike streams into 3DGS
pipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With
accumulation rasterization, interval supervision, and a specially designed
pipeline, SpikeGS extracts detailed geometry and texture from high temporal
resolution but texture lacking spike stream, reconstructs 3D scenes captured in
1 second. Extensive experiments on multiple synthetic and real-world datasets
demonstrate the superiority of SpikeGS compared with existing spike-based and
deblur 3D scene reconstruction methods. Codes and data will be released soon.";Yijia Guo<author:sep>Liwen Hu<author:sep>Lei Ma<author:sep>Tiejun Huang;http://arxiv.org/pdf/2407.03771v1;cs.CV;;gaussian splatting
2407.03923v1;http://arxiv.org/abs/2407.03923v1;2024-07-04;CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion  Blur Images;"Neural radiance fields (NeRFs) have received significant attention due to
their high-quality novel view rendering ability, prompting research to address
various real-world cases. One critical challenge is the camera motion blur
caused by camera movement during exposure time, which prevents accurate 3D
scene reconstruction. In this study, we propose continuous rigid motion-aware
gaussian splatting (CRiM-GS) to reconstruct accurate 3D scene from blurry
images with real-time rendering speed. Considering the actual camera motion
blurring process, which consists of complex motion patterns, we predict the
continuous movement of the camera based on neural ordinary differential
equations (ODEs). Specifically, we leverage rigid body transformations to model
the camera motion with proper regularization, preserving the shape and size of
the object. Furthermore, we introduce a continuous deformable 3D transformation
in the \textit{SE(3)} field to adapt the rigid body transformation to
real-world problems by ensuring a higher degree of freedom. By revisiting
fundamental camera theory and employing advanced neural network training
techniques, we achieve accurate modeling of continuous camera trajectories. We
conduct extensive experiments, demonstrating state-of-the-art performance both
quantitatively and qualitatively on benchmark datasets.";Junghe Lee<author:sep>Donghyeong Kim<author:sep>Dogyoon Lee<author:sep>Suhwan Cho<author:sep>Sangyoun Lee;http://arxiv.org/pdf/2407.03923v1;cs.CV;Project Page : https://jho-yonsei.github.io/CRiM-Gaussian/;gaussian splatting<tag:sep>nerf
2407.03857v1;http://arxiv.org/abs/2407.03857v1;2024-07-04;PFGS: High Fidelity Point Cloud Rendering via Feature Splatting;"Rendering high-fidelity images from sparse point clouds is still challenging.
Existing learning-based approaches suffer from either hole artifacts, missing
details, or expensive computations. In this paper, we propose a novel framework
to render high-quality images from sparse points. This method first attempts to
bridge the 3D Gaussian Splatting and point cloud rendering, which includes
several cascaded modules. We first use a regressor to estimate Gaussian
properties in a point-wise manner, the estimated properties are used to
rasterize neural feature descriptors into 2D planes which are extracted from a
multiscale extractor. The projected feature volume is gradually decoded toward
the final prediction via a multiscale and progressive decoder. The whole
pipeline experiences a two-stage training and is driven by our well-designed
progressive and multiscale reconstruction loss. Experiments on different
benchmarks show the superiority of our method in terms of rendering qualities
and the necessities of our main components.";Jiaxu Wang<author:sep>Ziyi Zhang<author:sep>Junhao He<author:sep>Renjing Xu;http://arxiv.org/pdf/2407.03857v1;cs.CV;;gaussian splatting
2407.02945v2;http://arxiv.org/abs/2407.02945v2;2024-07-03;VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using  Learned Priors;"Neural rendering-based urban scene reconstruction methods commonly rely on
images collected from driving vehicles with cameras facing and moving forward.
Although these methods can successfully synthesize from views similar to
training camera trajectory, directing the novel view outside the training
camera distribution does not guarantee on-par performance. In this paper, we
tackle the Extrapolated View Synthesis (EVS) problem by evaluating the
reconstructions on views such as looking left, right or downwards with respect
to training camera distributions. To improve rendering quality for EVS, we
initialize our model by constructing dense LiDAR map, and propose to leverage
prior scene knowledge such as surface normal estimator and large-scale
diffusion model. Qualitative and quantitative comparisons demonstrate the
effectiveness of our methods on EVS. To the best of our knowledge, we are the
first to address the EVS problem in urban scene reconstruction. Link to our
project page: https://vegs3d.github.io/.";Sungwon Hwang<author:sep>Min-Jung Kim<author:sep>Taewoong Kang<author:sep>Jayeon Kang<author:sep>Jaegul Choo;http://arxiv.org/pdf/2407.02945v2;cs.CV;"The first two authors contributed equally. Project Page:
  https://vegs3d.github.io/";gaussian splatting
2407.02918v1;http://arxiv.org/abs/2407.02918v1;2024-07-03;Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene  Reconstruction;"Real-time 3D reconstruction of surgical scenes plays a vital role in
computer-assisted surgery, holding a promise to enhance surgeons' visibility.
Recent advancements in 3D Gaussian Splatting (3DGS) have shown great potential
for real-time novel view synthesis of general scenes, which relies on accurate
poses and point clouds generated by Structure-from-Motion (SfM) for
initialization. However, 3DGS with SfM fails to recover accurate camera poses
and geometry in surgical scenes due to the challenges of minimal textures and
photometric inconsistencies. To tackle this problem, in this paper, we propose
the first SfM-free 3DGS-based method for surgical scene reconstruction by
jointly optimizing the camera poses and scene representation. Based on the
video continuity, the key of our method is to exploit the immediate optical
flow priors to guide the projection flow derived from 3D Gaussians. Unlike most
previous methods relying on photometric loss only, we formulate the pose
estimation problem as minimizing the flow loss between the projection flow and
optical flow. A consistency check is further introduced to filter the flow
outliers by detecting the rigid and reliable points that satisfy the epipolar
geometry. During 3D Gaussian optimization, we randomly sample frames to
optimize the scene representations to grow the 3D Gaussian progressively.
Experiments on the SCARED dataset demonstrate our superior performance over
existing methods in novel view synthesis and pose estimation with high
efficiency. Code is available at https://github.com/wrld/Free-SurGS.";Jiaxin Guo<author:sep>Jiangliu Wang<author:sep>Di Kang<author:sep>Wenzhen Dong<author:sep>Wenting Wang<author:sep>Yun-hui Liu;http://arxiv.org/pdf/2407.02918v1;cs.CV;Accepted to MICCAI 2024;gaussian splatting
2407.02174v2;http://arxiv.org/abs/2407.02174v2;2024-07-02;BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event  Stream;"Neural implicit representation of visual scenes has attracted a lot of
attention in recent research of computer vision and graphics. Most prior
methods focus on how to reconstruct 3D scene representation from a set of
images. In this work, we demonstrate the possibility to recover the neural
radiance fields (NeRF) from a single blurry image and its corresponding event
stream. We model the camera motion with a cubic B-Spline in SE(3) space. Both
the blurry image and the brightness change within a time interval, can then be
synthesized from the 3D scene representation given the 6-DoF poses interpolated
from the cubic B-Spline. Our method can jointly learn both the implicit neural
scene representation and recover the camera motion by minimizing the
differences between the synthesized data and the real measurements without
pre-computed camera poses from COLMAP. We evaluate the proposed method with
both synthetic and real datasets. The experimental results demonstrate that we
are able to render view-consistent latent sharp images from the learned NeRF
and bring a blurry image alive in high quality. Code and data are available at
https://github.com/WU-CVGL/BeNeRF.";Wenpu Li<author:sep>Pian Wan<author:sep>Peng Wang<author:sep>Jinghang Li<author:sep>Yi Zhou<author:sep>Peidong Liu;http://arxiv.org/pdf/2407.02174v2;cs.CV;Accepted to ECCV 2024;nerf
2407.02668v1;http://arxiv.org/abs/2407.02668v1;2024-07-02;MomentsNeRF: Leveraging Orthogonal Moments for Few-Shot Neural Rendering;"We propose MomentsNeRF, a novel framework for one- and few-shot neural
rendering that predicts a neural representation of a 3D scene using Orthogonal
Moments. Our architecture offers a new transfer learning method to train on
multi-scenes and incorporate a per-scene optimization using one or a few images
at test time. Our approach is the first to successfully harness features
extracted from Gabor and Zernike moments, seamlessly integrating them into the
NeRF architecture. We show that MomentsNeRF performs better in synthesizing
images with complex textures and shapes, achieving a significant noise
reduction, artifact elimination, and completing the missing parts compared to
the recent one- and few-shot neural rendering frameworks. Extensive experiments
on the DTU and Shapenet datasets show that MomentsNeRF improves the
state-of-the-art by {3.39\;dB\;PSNR}, 11.1% SSIM, 17.9% LPIPS, and 8.3% DISTS
metrics. Moreover, it outperforms state-of-the-art performance for both novel
view synthesis and single-image 3D view reconstruction. The source code is
accessible at: https://amughrabi.github.io/momentsnerf/.";Ahmad AlMughrabi<author:sep>Ricardo Marques<author:sep>Petia Radeva;http://arxiv.org/pdf/2407.02668v1;cs.CV;;nerf
2407.02034v1;http://arxiv.org/abs/2407.02034v1;2024-07-02;TrAME: Trajectory-Anchored Multi-View Editing for Text-Guided 3D  Gaussian Splatting Manipulation;"Despite significant strides in the field of 3D scene editing, current methods
encounter substantial challenge, particularly in preserving 3D consistency in
multi-view editing process. To tackle this challenge, we propose a progressive
3D editing strategy that ensures multi-view consistency via a
Trajectory-Anchored Scheme (TAS) with a dual-branch editing mechanism.
Specifically, TAS facilitates a tightly coupled iterative process between 2D
view editing and 3D updating, preventing error accumulation yielded from
text-to-image process. Additionally, we explore the relationship between
optimization-based methods and reconstruction-based methods, offering a unified
perspective for selecting superior design choice, supporting the rationale
behind the designed TAS. We further present a tuning-free View-Consistent
Attention Control (VCAC) module that leverages cross-view semantic and
geometric reference from the source branch to yield aligned views from the
target branch during the editing of 2D views. To validate the effectiveness of
our method, we analyze 2D examples to demonstrate the improved consistency with
the VCAC module. Further extensive quantitative and qualitative results in
text-guided 3D scene editing indicate that our method achieves superior editing
quality compared to state-of-the-art methods. We will make the complete
codebase publicly available following the conclusion of the double-blind review
process.";Chaofan Luo<author:sep>Donglin Di<author:sep>Yongjia Ma<author:sep>Zhou Xue<author:sep>Chen Wei<author:sep>Xun Yang<author:sep>Yebin Liu;http://arxiv.org/pdf/2407.02034v1;cs.CV;;gaussian splatting
2407.02598v2;http://arxiv.org/abs/2407.02598v2;2024-07-02;AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene  Reconstruction;"Realistic scene reconstruction and view synthesis are essential for advancing
autonomous driving systems by simulating safety-critical scenarios. 3D Gaussian
Splatting excels in real-time rendering and static scene reconstructions but
struggles with modeling driving scenarios due to complex backgrounds, dynamic
objects, and sparse views. We propose AutoSplat, a framework employing Gaussian
splatting to achieve highly realistic reconstructions of autonomous driving
scenes. By imposing geometric constraints on Gaussians representing the road
and sky regions, our method enables multi-view consistent simulation of
challenging scenarios including lane changes. Leveraging 3D templates, we
introduce a reflected Gaussian consistency constraint to supervise both the
visible and unseen side of foreground objects. Moreover, to model the dynamic
appearance of foreground objects, we estimate residual spherical harmonics for
each foreground Gaussian. Extensive experiments on Pandaset and KITTI
demonstrate that AutoSplat outperforms state-of-the-art methods in scene
reconstruction and novel view synthesis across diverse driving scenarios. Visit
our project page at https://autosplat.github.io/.";Mustafa Khan<author:sep>Hamidreza Fazlali<author:sep>Dhruv Sharma<author:sep>Tongtong Cao<author:sep>Dongfeng Bai<author:sep>Yuan Ren<author:sep>Bingbing Liu;http://arxiv.org/pdf/2407.02598v2;cs.CV;;gaussian splatting
2407.01301v1;http://arxiv.org/abs/2407.01301v1;2024-07-01;GaussianStego: A Generalizable Stenography Pipeline for Generative 3D  Gaussians Splatting;"Recent advancements in large generative models and real-time neural rendering
using point-based techniques pave the way for a future of widespread visual
data distribution through sharing synthesized 3D assets. However, while
standardized methods for embedding proprietary or copyright information, either
overtly or subtly, exist for conventional visual content such as images and
videos, this issue remains unexplored for emerging generative 3D formats like
Gaussian Splatting. We present GaussianStego, a method for embedding
steganographic information in the rendering of generated 3D assets. Our
approach employs an optimization framework that enables the accurate extraction
of hidden information from images rendered using Gaussian assets derived from
large models, while maintaining their original visual quality. We conduct
preliminary evaluations of our method across several potential deployment
scenarios and discuss issues identified through analysis. GaussianStego
represents an initial exploration into the novel challenge of embedding
customizable, imperceptible, and recoverable information within the renders
produced by current 3D generative models, while ensuring minimal impact on the
rendered content's quality.";Chenxin Li<author:sep>Hengyu Liu<author:sep>Zhiwen Fan<author:sep>Wuyang Li<author:sep>Yifan Liu<author:sep>Panwang Pan<author:sep>Yixuan Yuan;http://arxiv.org/pdf/2407.01301v1;cs.CV;Project website: https://gaussian-stego.github.io/;gaussian splatting
2407.01220v1;http://arxiv.org/abs/2407.01220v1;2024-07-01;Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation;"Understanding 3D scenes is a crucial challenge in computer vision research
with applications spanning multiple domains. Recent advancements in distilling
2D vision-language foundation models into neural fields, like NeRF and 3DGS,
enables open-vocabulary segmentation of 3D scenes from 2D multi-view images
without the need for precise 3D annotations. While effective, however, the
per-pixel distillation of high-dimensional CLIP features introduces ambiguity
and necessitates complex regularization strategies, adding inefficiencies
during training. This paper presents MaskField, which enables fast and
efficient 3D open-vocabulary segmentation with neural fields under weak
supervision. Unlike previous methods, MaskField distills masks rather than
dense high-dimensional CLIP features. MaskFields employ neural fields as binary
mask generators and supervise them with masks generated by SAM and classified
by coarse CLIP features. MaskField overcomes the ambiguous object boundaries by
naturally introducing SAM segmented object shapes without extra regularization
during training. By circumventing the direct handling of high-dimensional CLIP
features during training, MaskField is particularly compatible with explicit
scene representations like 3DGS. Our extensive experiments show that MaskField
not only surpasses prior state-of-the-art methods but also achieves remarkably
fast convergence, outperforming previous methods with just 5 minutes of
training. We hope that MaskField will inspire further exploration into how
neural fields can be trained to comprehend 3D scenes from 2D models.";Zihan Gao<author:sep>Lingling Li<author:sep>Licheng Jiao<author:sep>Fang Liu<author:sep>Xu Liu<author:sep>Wenping Ma<author:sep>Yuwei Guo<author:sep>Shuyuan Yang;http://arxiv.org/pdf/2407.01220v1;cs.CV;16 pages, 7 figures;nerf
2407.01029v1;http://arxiv.org/abs/2407.01029v1;2024-07-01;EndoSparse: Real-Time Sparse View Synthesis of Endoscopic Scenes using  Gaussian Splatting;"3D reconstruction of biological tissues from a collection of endoscopic
images is a key to unlock various important downstream surgical applications
with 3D capabilities. Existing methods employ various advanced neural rendering
techniques for photorealistic view synthesis, but they often struggle to
recover accurate 3D representations when only sparse observations are
available, which is usually the case in real-world clinical scenarios. To
tackle this {sparsity} challenge, we propose a framework leveraging the prior
knowledge from multiple foundation models during the reconstruction process,
dubbed as \textit{EndoSparse}. Experimental results indicate that our proposed
strategy significantly improves the geometric and appearance quality under
challenging sparse-view conditions, including using only three views. In
rigorous benchmarking experiments against state-of-the-art methods,
\textit{EndoSparse} achieves superior results in terms of accurate geometry,
realistic appearance, and rendering efficiency, confirming the robustness to
sparse-view limitations in endoscopic reconstruction. \textit{EndoSparse}
signifies a steady step towards the practical deployment of neural 3D
reconstruction in real-world clinical scenarios. Project page:
https://endo-sparse.github.io/.";Chenxin Li<author:sep>Brandon Y. Feng<author:sep>Yifan Liu<author:sep>Hengyu Liu<author:sep>Cheng Wang<author:sep>Weihao Yu<author:sep>Yixuan Yuan;http://arxiv.org/pdf/2407.01029v1;cs.CV;Accpeted by MICCAI2024;gaussian splatting
2407.01811v1;http://arxiv.org/abs/2407.01811v1;2024-07-01;Active Human Pose Estimation via an Autonomous UAV Agent;"One of the core activities of an active observer involves moving to secure a
""better"" view of the scene, where the definition of ""better"" is task-dependent.
This paper focuses on the task of human pose estimation from videos capturing a
person's activity. Self-occlusions within the scene can complicate or even
prevent accurate human pose estimation. To address this, relocating the camera
to a new vantage point is necessary to clarify the view, thereby improving 2D
human pose estimation. This paper formalizes the process of achieving an
improved viewpoint. Our proposed solution to this challenge comprises three
main components: a NeRF-based Drone-View Data Generation Framework, an On-Drone
Network for Camera View Error Estimation, and a Combined Planner for devising a
feasible motion plan to reposition the camera based on the predicted errors for
camera views. The Data Generation Framework utilizes NeRF-based methods to
generate a comprehensive dataset of human poses and activities, enhancing the
drone's adaptability in various scenarios. The Camera View Error Estimation
Network is designed to evaluate the current human pose and identify the most
promising next viewing angles for the drone, ensuring a reliable and precise
pose estimation from those angles. Finally, the combined planner incorporates
these angles while considering the drone's physical and environmental
limitations, employing efficient algorithms to navigate safe and effective
flight paths. This system represents a significant advancement in active 2D
human pose estimation for an autonomous UAV agent, offering substantial
potential for applications in aerial cinematography by improving the
performance of autonomous human pose estimation and maintaining the operational
safety and efficiency of UAVs.";Jingxi Chen<author:sep>Botao He<author:sep>Chahat Deep Singh<author:sep>Cornelia Fermuller<author:sep>Yiannis Aloimonos;http://arxiv.org/pdf/2407.01811v1;cs.RO;;nerf
2407.01761v1;http://arxiv.org/abs/2407.01761v1;2024-07-01;DRAGON: Drone and Ground Gaussian Splatting for 3D Building  Reconstruction;"3D building reconstruction from imaging data is an important task for many
applications ranging from urban planning to reconnaissance. Modern Novel View
synthesis (NVS) methods like NeRF and Gaussian Splatting offer powerful
techniques for developing 3D models from natural 2D imagery in an unsupervised
fashion. These algorithms generally require input training views surrounding
the scene of interest, which, in the case of large buildings, is typically not
available across all camera elevations. In particular, the most readily
available camera viewpoints at scale across most buildings are at near-ground
(e.g., with mobile phones) and aerial (drones) elevations. However, due to the
significant difference in viewpoint between drone and ground image sets, camera
registration - a necessary step for NVS algorithms - fails. In this work we
propose a method, DRAGON, that can take drone and ground building imagery as
input and produce a 3D NVS model. The key insight of DRAGON is that
intermediate elevation imagery may be extrapolated by an NVS algorithm itself
in an iterative procedure with perceptual regularization, thereby bridging the
visual feature gap between the two elevations and enabling registration. We
compiled a semi-synthetic dataset of 9 large building scenes using Google Earth
Studio, and quantitatively and qualitatively demonstrate that DRAGON can
generate compelling renderings on this dataset compared to baseline strategies.";Yujin Ham<author:sep>Mateusz Michalkiewicz<author:sep>Guha Balakrishnan;http://arxiv.org/pdf/2407.01761v1;cs.CV;12 pages, 9 figures, accepted to ICCP 2024;gaussian splatting<tag:sep>nerf
2407.00500v1;http://arxiv.org/abs/2407.00500v1;2024-06-29;Intrinsic PAPR for Point-level 3D Scene Albedo and Shading Editing;"Recent advancements in neural rendering have excelled at novel view synthesis
from multi-view RGB images. However, they often lack the capability to edit the
shading or colour of the scene at a detailed point-level, while ensuring
consistency across different viewpoints. In this work, we address the challenge
of point-level 3D scene albedo and shading editing from multi-view RGB images,
focusing on detailed editing at the point-level rather than at a part or global
level. While prior works based on volumetric representation such as NeRF
struggle with achieving 3D consistent editing at the point level, recent
advancements in point-based neural rendering show promise in overcoming this
challenge. We introduce ``Intrinsic PAPR'', a novel method based on the recent
point-based neural rendering technique Proximity Attention Point Rendering
(PAPR). Unlike other point-based methods that model the intrinsic decomposition
of the scene, our approach does not rely on complicated shading models or
simplistic priors that may not universally apply. Instead, we directly model
scene decomposition into albedo and shading components, leading to better
estimation accuracy. Comparative evaluations against the latest point-based
inverse rendering methods demonstrate that Intrinsic PAPR achieves
higher-quality novel view rendering and superior point-level albedo and shading
editing.";Alireza Moazeni<author:sep>Shichong Peng<author:sep>Ke Li;http://arxiv.org/pdf/2407.00500v1;cs.CV;;nerf
2407.00435v2;http://arxiv.org/abs/2407.00435v2;2024-06-29;RTGS: Enabling Real-Time Gaussian Splatting on Mobile Devices Using  Efficiency-Guided Pruning and Foveated Rendering;"Point-Based Neural Rendering (PBNR), i.e., the 3D Gaussian Splatting-family
algorithms, emerges as a promising class of rendering techniques, which are
permeating all aspects of society, driven by a growing demand for real-time,
photorealistic rendering in AR/VR and digital twins. Achieving real-time PBNR
on mobile devices is challenging.
  This paper proposes RTGS, a PBNR system that for the first time delivers
real-time neural rendering on mobile devices while maintaining human visual
quality. RTGS combines two techniques. First, we present an efficiency-aware
pruning technique to optimize rendering speed. Second, we introduce a Foveated
Rendering (FR) method for PBNR, leveraging humans' low visual acuity in
peripheral regions to relax rendering quality and improve rendering speed. Our
system executes in real-time (above 100 FPS) on Nvidia Jetson Xavier board
without sacrificing subjective visual quality, as confirmed by a user study.
The code is open-sourced at [https://github.com/horizon-research/Fov-3DGS].";Weikai Lin<author:sep>Yu Feng<author:sep>Yuhao Zhu;http://arxiv.org/pdf/2407.00435v2;cs.GR;9 pages;gaussian splatting
2407.00316v1;http://arxiv.org/abs/2407.00316v1;2024-06-29;OccFusion: Rendering Occluded Humans with Generative Diffusion Priors;"Most existing human rendering methods require every part of the human to be
fully visible throughout the input video. However, this assumption does not
hold in real-life settings where obstructions are common, resulting in only
partial visibility of the human. Considering this, we present OccFusion, an
approach that utilizes efficient 3D Gaussian splatting supervised by pretrained
2D diffusion models for efficient and high-fidelity human rendering. We propose
a pipeline consisting of three stages. In the Initialization stage, complete
human masks are generated from partial visibility masks. In the Optimization
stage, 3D human Gaussians are optimized with additional supervision by
Score-Distillation Sampling (SDS) to create a complete geometry of the human.
Finally, in the Refinement stage, in-context inpainting is designed to further
improve rendering quality on the less observed human body parts. We evaluate
OccFusion on ZJU-MoCap and challenging OcMotion sequences and find that it
achieves state-of-the-art performance in the rendering of occluded humans.";Adam Sun<author:sep>Tiange Xiang<author:sep>Scott Delp<author:sep>Li Fei-Fei<author:sep>Ehsan Adeli;http://arxiv.org/pdf/2407.00316v1;cs.CV;;gaussian splatting
2406.20055v1;http://arxiv.org/abs/2406.20055v1;2024-06-28;SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting;"3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction,
offering efficient training and rendering speeds, making it suitable for
real-time applications.However, current methods require highly controlled
environments (no moving people or wind-blown elements, and consistent lighting)
to meet the inter-view consistency assumption of 3DGS. This makes
reconstruction of real-world captures problematic. We present SpotlessSplats,
an approach that leverages pre-trained and general-purpose features coupled
with robust optimization to effectively ignore transient distractors. Our
method achieves state-of-the-art reconstruction quality both visually and
quantitatively, on casual captures.";Sara Sabour<author:sep>Lily Goli<author:sep>George Kopanas<author:sep>Mark Matthews<author:sep>Dmitry Lagun<author:sep>Leonidas Guibas<author:sep>Alec Jacobson<author:sep>David J. Fleet<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2406.20055v1;cs.CV;;gaussian splatting
2406.20066v1;http://arxiv.org/abs/2406.20066v1;2024-06-28;ASSR-NeRF: Arbitrary-Scale Super-Resolution on Voxel Grid for  High-Quality Radiance Fields Reconstruction;"NeRF-based methods reconstruct 3D scenes by building a radiance field with
implicit or explicit representations. While NeRF-based methods can perform
novel view synthesis (NVS) at arbitrary scale, the performance in
high-resolution novel view synthesis (HRNVS) with low-resolution (LR)
optimization often results in oversmoothing. On the other hand, single-image
super-resolution (SR) aims to enhance LR images to HR counterparts but lacks
multi-view consistency. To address these challenges, we propose Arbitrary-Scale
Super-Resolution NeRF (ASSR-NeRF), a novel framework for super-resolution novel
view synthesis (SRNVS). We propose an attention-based VoxelGridSR model to
directly perform 3D super-resolution (SR) on the optimized volume. Our model is
trained on diverse scenes to ensure generalizability. For unseen scenes trained
with LR views, we then can directly apply our VoxelGridSR to further refine the
volume and achieve multi-view consistent SR. We demonstrate quantitative and
qualitatively that the proposed method achieves significant performance in
SRNVS.";Ding-Jiun Huang<author:sep>Zi-Ting Chou<author:sep>Yu-Chiang Frank Wang<author:sep>Cheng Sun;http://arxiv.org/pdf/2406.20066v1;cs.CV;;nerf
2406.19811v1;http://arxiv.org/abs/2406.19811v1;2024-06-28;EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D  Gaussian Splatting;"Human activities are inherently complex, and even simple household tasks
involve numerous object interactions. To better understand these activities and
behaviors, it is crucial to model their dynamic interactions with the
environment. The recent availability of affordable head-mounted cameras and
egocentric data offers a more accessible and efficient means to understand
dynamic human-object interactions in 3D environments. However, most existing
methods for human activity modeling either focus on reconstructing 3D models of
hand-object or human-scene interactions or on mapping 3D scenes, neglecting
dynamic interactions with objects. The few existing solutions often require
inputs from multiple sources, including multi-camera setups, depth-sensing
cameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the
first method capable of simultaneously reconstructing 3D scenes and dynamically
tracking 3D object motion from RGB egocentric input alone. We leverage the
uniquely discrete nature of Gaussian Splatting and segment dynamic interactions
from the background. Our approach employs a clip-level online learning pipeline
that leverages the dynamic nature of human activities, allowing us to
reconstruct the temporal evolution of the scene in chronological order and
track rigid object motion. Additionally, our method automatically segments
object and background Gaussians, providing 3D representations for both static
scenes and dynamic objects. EgoGaussian outperforms previous NeRF and Dynamic
Gaussian methods in challenging in-the-wild videos and we also qualitatively
demonstrate the high quality of the reconstructed models.";Daiwei Zhang<author:sep>Gengyan Li<author:sep>Jiajie Li<author:sep>Mickaël Bressieux<author:sep>Otmar Hilliges<author:sep>Marc Pollefeys<author:sep>Luc Van Gool<author:sep>Xi Wang;http://arxiv.org/pdf/2406.19811v1;cs.CV;;gaussian splatting<tag:sep>nerf
2406.18840v1;http://arxiv.org/abs/2406.18840v1;2024-06-27;Shorter SPECT Scans Using Self-supervised Coordinate Learning to  Synthesize Skipped Projection Views;"Purpose: This study addresses the challenge of extended SPECT imaging
duration under low-count conditions, as encountered in Lu-177 SPECT imaging, by
developing a self-supervised learning approach to synthesize skipped SPECT
projection views, thus shortening scan times in clinical settings. Methods: We
employed a self-supervised coordinate-based learning technique, adapting the
neural radiance field (NeRF) concept in computer vision to synthesize
under-sampled SPECT projection views. For each single scan, we used
self-supervised coordinate learning to estimate skipped SPECT projection views.
The method was tested with various down-sampling factors (DFs=2, 4, 8) on both
Lu-177 phantom SPECT/CT measurements and clinical SPECT/CT datasets, from 11
patients undergoing Lu-177 DOTATATE and 6 patients undergoing Lu-177 PSMA-617
radiopharmaceutical therapy. Results: For SPECT reconstructions, our method
outperformed the use of linearly interpolated projections and partial
projection views in relative contrast-to-noise-ratios (RCNR) averaged across
different downsampling factors: 1) DOTATATE: 83% vs. 65% vs. 67% for lesions
and 86% vs. 70% vs. 67% for kidney, 2) PSMA: 76% vs. 69% vs. 68% for lesions
and 75% vs. 55% vs. 66% for organs, including kidneys, lacrimal glands, parotid
glands, and submandibular glands. Conclusion: The proposed method enables
reduction in acquisition time (by factors of 2, 4, or 8) while maintaining
quantitative accuracy in clinical SPECT protocols by allowing for the
collection of fewer projections. Importantly, the self-supervised nature of
this NeRF-based approach eliminates the need for extensive training data,
instead learning from each patient's projection data alone. The reduction in
acquisition time is particularly relevant for imaging under low-count
conditions and for protocols that require multiple-bed positions such as
whole-body imaging.";Zongyu Li<author:sep>Yixuan Jia<author:sep>Xiaojian Xu<author:sep>Jason Hu<author:sep>Jeffrey A. Fessler<author:sep>Yuni K. Dewaraja;http://arxiv.org/pdf/2406.18840v1;eess.IV;25 pages, 5568 words;nerf
2406.19434v1;http://arxiv.org/abs/2406.19434v1;2024-06-27;Lightweight Predictive 3D Gaussian Splats;"Recent approaches representing 3D objects and scenes using Gaussian splats
show increased rendering speed across a variety of platforms and devices. While
rendering such representations is indeed extremely efficient, storing and
transmitting them is often prohibitively expensive. To represent large-scale
scenes, one often needs to store millions of 3D Gaussians, occupying gigabytes
of disk space. This poses a very practical limitation, prohibiting widespread
adoption.Several solutions have been proposed to strike a balance between disk
size and rendering quality, noticeably reducing the visual quality. In this
work, we propose a new representation that dramatically reduces the hard drive
footprint while featuring similar or improved quality when compared to the
standard 3D Gaussian splats. When compared to other compact solutions, ours
offers higher quality renderings with significantly reduced storage, being able
to efficiently run on a mobile device in real-time. Our key observation is that
nearby points in the scene can share similar representations. Hence, only a
small ratio of 3D points needs to be stored. We introduce an approach to
identify such points which are called parent points. The discarded points
called children points along with attributes can be efficiently predicted by
tiny MLPs.";Junli Cao<author:sep>Vidit Goel<author:sep>Chaoyang Wang<author:sep>Anil Kag<author:sep>Ju Hu<author:sep>Sergei Korolev<author:sep>Chenfanfu Jiang<author:sep>Sergey Tulyakov<author:sep>Jian Ren;http://arxiv.org/pdf/2406.19434v1;cs.GR;Project Page: https://plumpuddings.github.io/LPGS//;
2406.18214v1;http://arxiv.org/abs/2406.18214v1;2024-06-26;Trimming the Fat: Efficient Compression of 3D Gaussian Splats through  Pruning;"In recent times, the utilization of 3D models has gained traction, owing to
the capacity for end-to-end training initially offered by Neural Radiance
Fields and more recently by 3D Gaussian Splatting (3DGS) models. The latter
holds a significant advantage by inherently easing rapid convergence during
training and offering extensive editability. However, despite rapid
advancements, the literature still lives in its infancy regarding the
scalability of these models. In this study, we take some initial steps in
addressing this gap, showing an approach that enables both the memory and
computational scalability of such models. Specifically, we propose ""Trimming
the fat"", a post-hoc gradient-informed iterative pruning technique to eliminate
redundant information encoded in the model. Our experimental findings on widely
acknowledged benchmarks attest to the effectiveness of our approach, revealing
that up to 75% of the Gaussians can be removed while maintaining or even
improving upon baseline performance. Our approach achieves around 50$\times$
compression while preserving performance similar to the baseline model, and is
able to speed-up computation up to 600~FPS.";Muhammad Salman Ali<author:sep>Maryam Qamar<author:sep>Sung-Ho Bae<author:sep>Enzo Tartaglione;http://arxiv.org/pdf/2406.18214v1;cs.CV;;gaussian splatting
2406.18198v1;http://arxiv.org/abs/2406.18198v1;2024-06-26;VDG: Vision-Only Dynamic Gaussian for Driving Simulation;"Dynamic Gaussian splatting has led to impressive scene reconstruction and
image synthesis advances in novel views. Existing methods, however, heavily
rely on pre-computed poses and Gaussian initialization by Structure from Motion
(SfM) algorithms or expensive sensors. For the first time, this paper addresses
this issue by integrating self-supervised VO into our pose-free dynamic
Gaussian method (VDG) to boost pose and depth initialization and static-dynamic
decomposition. Moreover, VDG can work with only RGB image input and construct
dynamic scenes at a faster speed and larger scenes compared with the pose-free
dynamic view-synthesis method. We demonstrate the robustness of our approach
via extensive quantitative and qualitative experiments. Our results show
favorable performance over the state-of-the-art dynamic view synthesis methods.
Additional video and source code will be posted on our project page at
https://3d-aigc.github.io/VDG.";Hao Li<author:sep>Jingfeng Li<author:sep>Dingwen Zhang<author:sep>Chenming Wu<author:sep>Jieqi Shi<author:sep>Chen Zhao<author:sep>Haocheng Feng<author:sep>Errui Ding<author:sep>Jingdong Wang<author:sep>Junwei Han;http://arxiv.org/pdf/2406.18198v1;cs.CV;;gaussian splatting
2406.18199v1;http://arxiv.org/abs/2406.18199v1;2024-06-26;GS-Octree: Octree-based 3D Gaussian Splatting for Robust Object-level 3D  Reconstruction Under Strong Lighting;"The 3D Gaussian Splatting technique has significantly advanced the
construction of radiance fields from multi-view images, enabling real-time
rendering. While point-based rasterization effectively reduces computational
demands for rendering, it often struggles to accurately reconstruct the
geometry of the target object, especially under strong lighting. To address
this challenge, we introduce a novel approach that combines octree-based
implicit surface representations with Gaussian splatting. Our method consists
of four stages. Initially, it reconstructs a signed distance field (SDF) and a
radiance field through volume rendering, encoding them in a low-resolution
octree. The initial SDF represents the coarse geometry of the target object.
Subsequently, it introduces 3D Gaussians as additional degrees of freedom,
which are guided by the SDF. In the third stage, the optimized Gaussians
further improve the accuracy of the SDF, allowing it to recover finer geometric
details compared to the initial SDF obtained in the first stage. Finally, it
adopts the refined SDF to further optimize the 3D Gaussians via splatting,
eliminating those that contribute little to visual appearance. Experimental
results show that our method, which leverages the distribution of 3D Gaussians
with SDFs, reconstructs more accurate geometry, particularly in images with
specular highlights caused by strong lighting.";Jiaze Li<author:sep>Zhengyu Wen<author:sep>Luo Zhang<author:sep>Jiangbei Hu<author:sep>Fei Hou<author:sep>Zhebin Zhang<author:sep>Ying He;http://arxiv.org/pdf/2406.18199v1;cs.CV;;gaussian splatting
2406.18717v1;http://arxiv.org/abs/2406.18717v1;2024-06-26;Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular  Videos;"Gaussian splatting has become a popular representation for novel-view
synthesis, exhibiting clear strengths in efficiency, photometric quality, and
compositional edibility. Following its success, many works have extended
Gaussians to 4D, showing that dynamic Gaussians maintain these benefits while
also tracking scene geometry far better than alternative representations. Yet,
these methods assume dense multi-view videos as supervision, constraining their
use to controlled capture settings. In this work, we extend the capability of
Gaussian scene representations to casually captured monocular videos. We show
that existing 4D Gaussian methods dramatically fail in this setup because the
monocular setting is underconstrained. Building off this finding, we propose
Dynamic Gaussian Marbles (DGMarbles), consisting of three core modifications
that target the difficulties of the monocular setting. First, DGMarbles uses
isotropic Gaussian ""marbles"", reducing the degrees of freedom of each Gaussian,
and constraining the optimization to focus on motion and appearance over local
shape. Second, DGMarbles employs a hierarchical divide-and-conquer learning
strategy to guide the optimization towards solutions with coherent motion.
Finally, DGMarbles adds image-level and geometry-level priors into the
optimization, including a tracking loss that takes advantage of recent progress
in point tracking. By constraining the optimization in these ways, DGMarbles
learns Gaussian trajectories that enable novel-view rendering and accurately
capture the 3D motion of the scene elements. We evaluate on the (monocular)
Nvidia Dynamic Scenes dataset and the Dycheck iPhone dataset, and show that
DGMarbles significantly outperforms other Gaussian baselines in quality, and is
on-par with non-Gaussian representations, all while maintaining the efficiency,
compositionality, editability, and tracking benefits of Gaussians.";Colton Stearns<author:sep>Adam Harley<author:sep>Mikaela Uy<author:sep>Florian Dubost<author:sep>Federico Tombari<author:sep>Gordon Wetzstein<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2406.18717v1;cs.CV;;gaussian splatting
2406.18533v1;http://arxiv.org/abs/2406.18533v1;2024-06-26;On Scaling Up 3D Gaussian Splatting Training;"3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction
due to its superior visual quality and rendering speed. However, 3DGS training
currently occurs on a single GPU, limiting its ability to handle
high-resolution and large-scale 3D reconstruction tasks due to memory
constraints. We introduce Grendel, a distributed system designed to partition
3DGS parameters and parallelize computation across multiple GPUs. As each
Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs
sparse all-to-all communication to transfer the necessary Gaussians to pixel
partitions and performs dynamic load balancing. Unlike existing 3DGS systems
that train using one camera view image at a time, Grendel supports batched
training with multiple views. We explore various optimization hyperparameter
scaling strategies and find that a simple sqrt(batch size) scaling rule is
highly effective. Evaluations using large-scale, high-resolution scenes show
that Grendel enhances rendering quality by scaling up 3DGS parameters across
multiple GPUs. On the Rubble dataset, we achieve a test PSNR of 27.28 by
distributing 40.4 million Gaussians across 16 GPUs, compared to a PSNR of 26.28
using 11.2 million Gaussians on a single GPU. Grendel is an open-source project
available at: https://github.com/nyu-systems/Grendel-GS";Hexu Zhao<author:sep>Haoyang Weng<author:sep>Daohan Lu<author:sep>Ang Li<author:sep>Jinyang Li<author:sep>Aurojit Panda<author:sep>Saining Xie;http://arxiv.org/pdf/2406.18533v1;cs.CV;"Code: https://github.com/nyu-systems/Grendel-GS ; Project page:
  https://daohanlu.github.io/scaling-up-3dgs";gaussian splatting
2406.18462v1;http://arxiv.org/abs/2406.18462v1;2024-06-26;GaussianDreamerPro: Text to Manipulable 3D Gaussians with Highly  Enhanced Quality;"Recently, 3D Gaussian splatting (3D-GS) has achieved great success in
reconstructing and rendering real-world scenes. To transfer the high rendering
quality to generation tasks, a series of research works attempt to generate
3D-Gaussian assets from text. However, the generated assets have not achieved
the same quality as those in reconstruction tasks. We observe that Gaussians
tend to grow without control as the generation process may cause indeterminacy.
Aiming at highly enhancing the generation quality, we propose a novel framework
named GaussianDreamerPro. The main idea is to bind Gaussians to reasonable
geometry, which evolves over the whole generation process. Along different
stages of our framework, both the geometry and appearance can be enriched
progressively. The final output asset is constructed with 3D Gaussians bound to
mesh, which shows significantly enhanced details and quality compared with
previous methods. Notably, the generated asset can also be seamlessly
integrated into downstream manipulation pipelines, e.g. animation, composition,
and simulation etc., greatly promoting its potential in wide applications.
Demos are available at https://taoranyi.com/gaussiandreamerpro/.";Taoran Yi<author:sep>Jiemin Fang<author:sep>Zanwei Zhou<author:sep>Junjie Wang<author:sep>Guanjun Wu<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wenyu Liu<author:sep>Xinggang Wang<author:sep>Qi Tian;http://arxiv.org/pdf/2406.18462v1;cs.CV;Project page: https://taoranyi.com/gaussiandreamerpro/;gaussian splatting
2406.17438v1;http://arxiv.org/abs/2406.17438v1;2024-06-25;Implicit-Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D  Images and 3D Scenes;"Neural implicit functions have demonstrated significant importance in various
areas such as computer vision, graphics. Their advantages include the ability
to represent complex shapes and scenes with high fidelity, smooth interpolation
capabilities, and continuous representations. Despite these benefits, the
development and analysis of implicit functions have been limited by the lack of
comprehensive datasets and the substantial computational resources required for
their implementation and evaluation. To address these challenges, we introduce
""Implicit-Zoo"": a large-scale dataset requiring thousands of GPU training days
designed to facilitate research and development in this field. Our dataset
includes diverse 2D and 3D scenes, such as CIFAR-10, ImageNet-1K, and
Cityscapes for 2D image tasks, and the OmniObject3D dataset for 3D vision
tasks. We ensure high quality through strict checks, refining or filtering out
low-quality data. Using Implicit-Zoo, we showcase two immediate benefits as it
enables to: (1) learn token locations for transformer models; (2) directly
regress 3D cameras poses of 2D images with respect to NeRF models. This in turn
leads to an improved performance in all three task of image classification,
semantic segmentation, and 3D pose regression, thereby unlocking new avenues
for research.";Qi Ma<author:sep>Danda Pani Paudel<author:sep>Ender Konukoglu<author:sep>Luc Van Gool;http://arxiv.org/pdf/2406.17438v1;cs.CV;;nerf
2406.17345v1;http://arxiv.org/abs/2406.17345v1;2024-06-25;NerfBaselines: Consistent and Reproducible Evaluation of Novel View  Synthesis Methods;"Novel view synthesis is an important problem with many applications,
including AR/VR, gaming, and simulations for robotics. With the recent rapid
development of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS)
methods, it is becoming difficult to keep track of the current state of the art
(SoTA) due to methods using different evaluation protocols, codebases being
difficult to install and use, and methods not generalizing well to novel 3D
scenes. Our experiments support this claim by showing that tiny differences in
evaluation protocols of various methods can lead to inconsistent reported
metrics. To address these issues, we propose a framework called NerfBaselines,
which simplifies the installation of various methods, provides consistent
benchmarking tools, and ensures reproducibility. We validate our implementation
experimentally by reproducing numbers reported in the original papers. To
further improve the accessibility, we release a web platform where commonly
used methods are compared on standard benchmarks. Web:
https://jkulhanek.com/nerfbaselines";Jonas Kulhanek<author:sep>Torsten Sattler;http://arxiv.org/pdf/2406.17345v1;cs.CV;Web: https://jkulhanek.com/nerfbaselines;gaussian splatting<tag:sep>nerf
2406.16850v1;http://arxiv.org/abs/2406.16850v1;2024-06-24;From Perfect to Noisy World Simulation: Customizable Embodied  Multi-modal Perturbations for SLAM Robustness Benchmarking;"Embodied agents require robust navigation systems to operate in unstructured
environments, making the robustness of Simultaneous Localization and Mapping
(SLAM) models critical to embodied agent autonomy. While real-world datasets
are invaluable, simulation-based benchmarks offer a scalable approach for
robustness evaluations. However, the creation of a challenging and controllable
noisy world with diverse perturbations remains under-explored. To this end, we
propose a novel, customizable pipeline for noisy data synthesis, aimed at
assessing the resilience of multi-modal SLAM models against various
perturbations. The pipeline comprises a comprehensive taxonomy of sensor and
motion perturbations for embodied multi-modal (specifically RGB-D) sensing,
categorized by their sources and propagation order, allowing for procedural
composition. We also provide a toolbox for synthesizing these perturbations,
enabling the transformation of clean environments into challenging noisy
simulations. Utilizing the pipeline, we instantiate the large-scale
Noisy-Replica benchmark, which includes diverse perturbation types, to evaluate
the risk tolerance of existing advanced RGB-D SLAM models. Our extensive
analysis uncovers the susceptibilities of both neural (NeRF and Gaussian
Splatting -based) and non-neural SLAM models to disturbances, despite their
demonstrated accuracy in standard benchmarks. Our code is publicly available at
https://github.com/Xiaohao-Xu/SLAM-under-Perturbation.";Xiaohao Xu<author:sep>Tianyi Zhang<author:sep>Sibo Wang<author:sep>Xiang Li<author:sep>Yongqi Chen<author:sep>Ye Li<author:sep>Bhiksha Raj<author:sep>Matthew Johnson-Roberson<author:sep>Xiaonan Huang;http://arxiv.org/pdf/2406.16850v1;cs.CV;"50 pages. arXiv admin note: substantial text overlap with
  arXiv:2402.08125";nerf
2406.16815v1;http://arxiv.org/abs/2406.16815v1;2024-06-24;ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians;"High-fidelity 3D garment synthesis from text is desirable yet challenging for
digital avatar creation. Recent diffusion-based approaches via Score
Distillation Sampling (SDS) have enabled new possibilities but either
intricately couple with human body or struggle to reuse. We introduce
ClotheDreamer, a 3D Gaussian-based method for generating wearable,
production-ready 3D garment assets from text prompts. We propose a novel
representation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate
optimization. DCGS represents clothed avatar as one Gaussian model but freezes
body Gaussian splats. To enhance quality and completeness, we incorporate
bidirectional SDS to supervise clothed avatar and garment RGBD renderings
respectively with pose conditions and propose a new pruning strategy for loose
clothing. Our approach can also support custom clothing templates as input.
Benefiting from our design, the synthetic 3D garment can be easily applied to
virtual try-on and support physically accurate animation. Extensive experiments
showcase our method's superior and competitive performance. Our project page is
at https://ggxxii.github.io/clothedreamer.";Yufei Liu<author:sep>Junshu Tang<author:sep>Chu Zheng<author:sep>Shijie Zhang<author:sep>Jinkun Hao<author:sep>Junwei Zhu<author:sep>Dongjin Huang;http://arxiv.org/pdf/2406.16815v1;cs.CV;Project Page: https://ggxxii.github.io/clothedreamer;gaussian splatting
2406.17074v1;http://arxiv.org/abs/2406.17074v1;2024-06-24;Reducing the Memory Footprint of 3D Gaussian Splatting;"3D Gaussian splatting provides excellent visual quality for novel view
synthesis, with fast training and real-time rendering; unfortunately, the
memory requirements of this method for storing and transmission are
unreasonably high. We first analyze the reasons for this, identifying three
main areas where storage can be reduced: the number of 3D Gaussian primitives
used to represent a scene, the number of coefficients for the spherical
harmonics used to represent directional radiance, and the precision required to
store Gaussian primitive attributes. We present a solution to each of these
issues. First, we propose an efficient, resolution-aware primitive pruning
approach, reducing the primitive count by half. Second, we introduce an
adaptive adjustment method to choose the number of coefficients used to
represent directional radiance for each Gaussian primitive, and finally a
codebook-based quantization method, together with a half-float representation
for further memory reduction. Taken together, these three components result in
a 27 reduction in overall size on disk on the standard datasets we tested,
along with a 1.7 speedup in rendering speed. We demonstrate our method on
standard datasets and show how our solution results in significantly reduced
download times when using the method on a mobile device.";Panagiotis Papantonakis<author:sep>Georgios Kopanas<author:sep>Bernhard Kerbl<author:sep>Alexandre Lanvin<author:sep>George Drettakis;http://arxiv.org/pdf/2406.17074v1;cs.CV;Project website: https://repo-sam.inria.fr/fungraph/reduced_3dgs/;gaussian splatting
2406.08953v1;http://arxiv.org/abs/2406.08953v1;2024-06-13;Preserving Identity with Variational Score for General-purpose 3D  Editing;"We present Piva (Preserving Identity with Variational Score Distillation), a
novel optimization-based method for editing images and 3D models based on
diffusion models. Specifically, our approach is inspired by the recently
proposed method for 2D image editing - Delta Denoising Score (DDS). We pinpoint
the limitations in DDS for 2D and 3D editing, which causes detail loss and
over-saturation. To address this, we propose an additional score distillation
term that enforces identity preservation. This results in a more stable editing
process, gradually optimizing NeRF models to match target prompts while
retaining crucial input characteristics. We demonstrate the effectiveness of
our approach in zero-shot image and neural field editing. Our method
successfully alters visual attributes, adds both subtle and substantial
structural elements, translates shapes, and achieves competitive results on
standard 2D and 3D editing benchmarks. Additionally, our method imposes no
constraints like masking or pre-training, making it compatible with a wide
range of pre-trained diffusion models. This allows for versatile editing
without needing neural field-to-mesh conversion, offering a more user-friendly
experience.";Duong H. Le<author:sep>Tuan Pham<author:sep>Aniruddha Kembhavi<author:sep>Stephan Mandt<author:sep>Wei-Chiu Ma<author:sep>Jiasen Lu;http://arxiv.org/pdf/2406.08953v1;cs.CV;22 pages, 14 figures;nerf
2406.08839v1;http://arxiv.org/abs/2406.08839v1;2024-06-13;NeRF Director: Revisiting View Selection in Neural Volume Rendering;"Neural Rendering representations have significantly contributed to the field
of 3D computer vision. Given their potential, considerable efforts have been
invested to improve their performance. Nonetheless, the essential question of
selecting training views is yet to be thoroughly investigated. This key aspect
plays a vital role in achieving high-quality results and aligns with the
well-known tenet of deep learning: ""garbage in, garbage out"". In this paper, we
first illustrate the importance of view selection by demonstrating how a simple
rotation of the test views within the most pervasive NeRF dataset can lead to
consequential shifts in the performance rankings of state-of-the-art
techniques. To address this challenge, we introduce a unified framework for
view selection methods and devise a thorough benchmark to assess its impact.
Significant improvements can be achieved without leveraging error or
uncertainty estimation but focusing on uniform view coverage of the
reconstructed object, resulting in a training-free approach. Using this
technique, we show that high-quality renderings can be achieved faster by using
fewer views. We conduct extensive experiments on both synthetic datasets and
realistic data to demonstrate the effectiveness of our proposed method compared
with random, conventional error-based, and uncertainty-guided view selection.";Wenhui Xiao<author:sep>Rodrigo Santa Cruz<author:sep>David Ahmedt-Aristizabal<author:sep>Olivier Salvado<author:sep>Clinton Fookes<author:sep>Leo Lebrat;http://arxiv.org/pdf/2406.08839v1;cs.CV;CVPR2024;nerf
2406.09377v1;http://arxiv.org/abs/2406.09377v1;2024-06-13;GGHead: Fast and Generalizable 3D Gaussian Heads;"Learning 3D head priors from large 2D image collections is an important step
towards high-quality 3D-aware human modeling. A core requirement is an
efficient architecture that scales well to large-scale datasets and large image
resolutions. Unfortunately, existing 3D GANs struggle to scale to generate
samples at high resolutions due to their relatively slow train and render
speeds, and typically have to rely on 2D superresolution networks at the
expense of global 3D consistency. To address these challenges, we propose
Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian
Splatting representation within a 3D GAN framework. To generate a 3D
representation, we employ a powerful 2D CNN generator to predict Gaussian
attributes in the UV space of a template head mesh. This way, GGHead exploits
the regularity of the template's UV layout, substantially facilitating the
challenging task of predicting an unstructured set of 3D Gaussians. We further
improve the geometric fidelity of the generated 3D representations with a novel
total variation loss on rendered UV coordinates. Intuitively, this
regularization encourages that neighboring rendered pixels should stem from
neighboring Gaussians in the template's UV space. Taken together, our pipeline
can efficiently generate 3D heads trained only from single-view 2D image
observations. Our proposed framework matches the quality of existing 3D head
GANs on FFHQ while being both substantially faster and fully 3D consistent. As
a result, we demonstrate real-time generation and rendering of high-quality
3D-consistent heads at $1024^2$ resolution for the first time.";Tobias Kirschstein<author:sep>Simon Giebenhain<author:sep>Jiapeng Tang<author:sep>Markos Georgopoulos<author:sep>Matthias Nießner;http://arxiv.org/pdf/2406.09377v1;cs.CV;"Project Page: https://tobias-kirschstein.github.io/gghead/ ; YouTube
  Video: https://www.youtube.com/watch?v=1iyC74neQXc";
2406.09417v1;http://arxiv.org/abs/2406.09417v1;2024-06-13;Rethinking Score Distillation as a Bridge Between Image Distributions;"Score distillation sampling (SDS) has proven to be an important tool,
enabling the use of large-scale diffusion priors for tasks operating in
data-poor domains. Unfortunately, SDS has a number of characteristic artifacts
that limit its usefulness in general-purpose applications. In this paper, we
make progress toward understanding the behavior of SDS and its variants by
viewing them as solving an optimal-cost transport path from a source
distribution to a target distribution. Under this new interpretation, these
methods seek to transport corrupted images (source) to the natural image
distribution (target). We argue that current methods' characteristic artifacts
are caused by (1) linear approximation of the optimal path and (2) poor
estimates of the source distribution. We show that calibrating the text
conditioning of the source distribution can produce high-quality generation and
translation results with little extra overhead. Our method can be easily
applied across many domains, matching or beating the performance of specialized
methods. We demonstrate its utility in text-to-2D, text-based NeRF
optimization, translating paintings to real images, optical illusion
generation, and 3D sketch-to-real. We compare our method to existing approaches
for score distillation sampling and show that it can produce high-frequency
details with realistic colors.";David McAllister<author:sep>Songwei Ge<author:sep>Jia-Bin Huang<author:sep>David W. Jacobs<author:sep>Alexei A. Efros<author:sep>Aleksander Holynski<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2406.09417v1;cs.CV;Project webpage: https://sds-bridge.github.io/;nerf
2406.09395v1;http://arxiv.org/abs/2406.09395v1;2024-06-13;Modeling Ambient Scene Dynamics for Free-view Synthesis;"We introduce a novel method for dynamic free-view synthesis of an ambient
scenes from a monocular capture bringing a immersive quality to the viewing
experience. Our method builds upon the recent advancements in 3D Gaussian
Splatting (3DGS) that can faithfully reconstruct complex static scenes.
Previous attempts to extend 3DGS to represent dynamics have been confined to
bounded scenes or require multi-camera captures, and often fail to generalize
to unseen motions, limiting their practical application. Our approach overcomes
these constraints by leveraging the periodicity of ambient motions to learn the
motion trajectory model, coupled with careful regularization. We also propose
important practical strategies to improve the visual quality of the baseline
3DGS static reconstructions and to improve memory efficiency critical for
GPU-memory intensive learning. We demonstrate high-quality photorealistic novel
view synthesis of several ambient natural scenes with intricate textures and
fine structural elements.";Meng-Li Shih<author:sep>Jia-Bin Huang<author:sep>Changil Kim<author:sep>Rajvi Shah<author:sep>Johannes Kopf<author:sep>Chen Gao;http://arxiv.org/pdf/2406.09395v1;cs.CV;;
2406.08759v1;http://arxiv.org/abs/2406.08759v1;2024-06-13;Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for  Compressed Scene Modeling;"The field of novel-view synthesis has recently witnessed the emergence of 3D
Gaussian Splatting, which represents scenes in a point-based manner and renders
through rasterization. This methodology, in contrast to Radiance Fields that
rely on ray tracing, demonstrates superior rendering quality and speed.
However, the explicit and unstructured nature of 3D Gaussians poses a
significant storage challenge, impeding its broader application. To address
this challenge, we introduce the Gaussian-Forest modeling framework, which
hierarchically represents a scene as a forest of hybrid 3D Gaussians. Each
hybrid Gaussian retains its unique explicit attributes while sharing implicit
ones with its sibling Gaussians, thus optimizing parameterization with
significantly fewer variables. Moreover, adaptive growth and pruning strategies
are designed, ensuring detailed representation in complex regions and a notable
reduction in the number of required Gaussians. Extensive experiments
demonstrate that Gaussian-Forest not only maintains comparable speed and
quality but also achieves a compression rate surpassing 10 times, marking a
significant advancement in efficient scene modeling. Codes are available at
https://github.com/Xian-Bei/GaussianForest.";Fengyi Zhang<author:sep>Tianjun Zhang<author:sep>Lin Zhang<author:sep>Helen Huang<author:sep>Yadan Luo;http://arxiv.org/pdf/2406.08759v1;cs.CV;;gaussian splatting
2406.08920v1;http://arxiv.org/abs/2406.08920v1;2024-06-13;AV-GS: Learning Material and Geometry Aware Priors for Novel View  Acoustic Synthesis;"Novel view acoustic synthesis (NVAS) aims to render binaural audio at any
target viewpoint, given a mono audio emitted by a sound source at a 3D scene.
Existing methods have proposed NeRF-based implicit models to exploit visual
cues as a condition for synthesizing binaural audio. However, in addition to
low efficiency originating from heavy NeRF rendering, these methods all have a
limited ability of characterizing the entire scene environment such as room
geometry, material properties, and the spatial relation between the listener
and sound source. To address these issues, we propose a novel Audio-Visual
Gaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware
condition for audio synthesis, we learn an explicit point-based scene
representation with an audio-guidance parameter on locally initialized Gaussian
points, taking into account the space relation from the listener and sound
source. To make the visual scene model audio adaptive, we propose a point
densification and pruning strategy to optimally distribute the Gaussian points,
with the per-point contribution in sound propagation (e.g., more points needed
for texture-less wall surfaces as they affect sound path diversion). Extensive
experiments validate the superiority of our AV-GS over existing alternatives on
the real-world RWAS and simulation-based SoundSpaces datasets.";Swapnil Bhosale<author:sep>Haosen Yang<author:sep>Diptesh Kanojia<author:sep>Jiankang Deng<author:sep>Xiatian Zhu;http://arxiv.org/pdf/2406.08920v1;cs.SD;;gaussian splatting<tag:sep>nerf
2406.08943v1;http://arxiv.org/abs/2406.08943v1;2024-06-13;Neural NeRF Compression;"Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing
detailed 3D scenes through continuous volumetric representations. Recent NeRFs
utilize feature grids to improve rendering quality and speed; however, these
representations introduce significant storage overhead. This paper presents a
novel method for efficiently compressing a grid-based NeRF model, addressing
the storage overhead concern. Our approach is based on the non-linear transform
coding paradigm, employing neural compression for compressing the model's
feature grids. Due to the lack of training data involving many i.i.d scenes, we
design an encoder-free, end-to-end optimized approach for individual scenes,
using lightweight decoders. To leverage the spatial inhomogeneity of the latent
feature grids, we introduce an importance-weighted rate-distortion objective
and a sparse entropy model employing a masking mechanism. Our experimental
results validate that our proposed method surpasses existing works in terms of
grid-based NeRF compression efficacy and reconstruction quality.";Tuan Pham<author:sep>Stephan Mandt;http://arxiv.org/pdf/2406.08943v1;cs.CV;Accepted to ICML 2024;nerf
2406.08475v1;http://arxiv.org/abs/2406.08475v1;2024-06-12;Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent  Diffusion Models;"Creating realistic avatars from a single RGB image is an attractive yet
challenging problem. Due to its ill-posed nature, recent works leverage
powerful prior from 2D diffusion models pretrained on large datasets. Although
2D diffusion models demonstrate strong generalization capability, they cannot
provide multi-view shape priors with guaranteed 3D consistency. We propose
Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent
Diffusion. Our key insight is that 2D multi-view diffusion and 3D
reconstruction models provide complementary information for each other, and by
coupling them in a tight manner, we can fully leverage the potential of both
models. We introduce a novel image-conditioned generative 3D Gaussian Splats
reconstruction model that leverages the priors from 2D multi-view diffusion
models, and provides an explicit 3D representation, which further guides the 2D
reverse sampling process to have better 3D consistency. Experiments show that
our proposed framework outperforms state-of-the-art methods and enables the
creation of realistic avatars from a single RGB image, achieving high-fidelity
in both geometry and appearance. Extensive ablations also validate the efficacy
of our design, (1) multi-view 2D priors conditioning in generative 3D
reconstruction and (2) consistency refinement of sampling trajectory via the
explicit 3D representation. Our code and models will be released on
https://yuxuan-xue.com/human-3diffusion.";Yuxuan Xue<author:sep>Xianghui Xie<author:sep>Riccardo Marin<author:sep>Gerard Pons-Moll;http://arxiv.org/pdf/2406.08475v1;cs.CV;Project Page: https://yuxuan-xue.com/human-3diffusion;
2406.08300v1;http://arxiv.org/abs/2406.08300v1;2024-06-12;From Chaos to Clarity: 3DGS in the Dark;"Novel view synthesis from raw images provides superior high dynamic range
(HDR) information compared to reconstructions from low dynamic range RGB
images. However, the inherent noise in unprocessed raw images compromises the
accuracy of 3D scene representation. Our study reveals that 3D Gaussian
Splatting (3DGS) is particularly susceptible to this noise, leading to numerous
elongated Gaussian shapes that overfit the noise, thereby significantly
degrading reconstruction quality and reducing inference speed, especially in
scenarios with limited views. To address these issues, we introduce a novel
self-supervised learning framework designed to reconstruct HDR 3DGS from a
limited number of noisy raw images. This framework enhances 3DGS by integrating
a noise extractor and employing a noise-robust reconstruction loss that
leverages a noise distribution prior. Experimental results show that our method
outperforms LDR/HDR 3DGS and previous state-of-the-art (SOTA) self-supervised
and supervised pre-trained models in both reconstruction quality and inference
speed on the RawNeRF dataset across a broad range of training views. Code can
be found in \url{https://lizhihao6.github.io/Raw3DGS}.";Zhihao Li<author:sep>Yufei Wang<author:sep>Alex Kot<author:sep>Bihan Wen;http://arxiv.org/pdf/2406.08300v1;eess.IV;;nerf
2406.07828v1;http://arxiv.org/abs/2406.07828v1;2024-06-12;Spatial Annealing Smoothing for Efficient Few-shot Neural Rendering;"Neural Radiance Fields (NeRF) with hybrid representations have shown
impressive capabilities in reconstructing scenes for view synthesis, delivering
high efficiency. Nonetheless, their performance significantly drops with sparse
view inputs, due to the issue of overfitting. While various regularization
strategies have been devised to address these challenges, they often depend on
inefficient assumptions or are not compatible with hybrid models. There is a
clear need for a method that maintains efficiency and improves resilience to
sparse views within a hybrid framework. In this paper, we introduce an accurate
and efficient few-shot neural rendering method named Spatial Annealing
smoothing regularized NeRF (SANeRF), which is specifically designed for a
pre-filtering-driven hybrid representation architecture. We implement an
exponential reduction of the sample space size from an initially large value.
This methodology is crucial for stabilizing the early stages of the training
phase and significantly contributes to the enhancement of the subsequent
process of detail refinement. Our extensive experiments reveal that, by adding
merely one line of code, SANeRF delivers superior rendering quality and much
faster reconstruction speed compared to current few-shot NeRF methods. Notably,
SANeRF outperforms FreeNeRF by 0.3 dB in PSNR on the Blender dataset, while
achieving 700x faster reconstruction speed.";Yuru Xiao<author:sep>Xianming Liu<author:sep>Deming Zhai<author:sep>Kui Jiang<author:sep>Junjun Jiang<author:sep>Xiangyang Ji;http://arxiv.org/pdf/2406.07828v1;cs.CV;;nerf
2406.08488v1;http://arxiv.org/abs/2406.08488v1;2024-06-12;ICE-G: Image Conditional Editing of 3D Gaussian Splats;"Recently many techniques have emerged to create high quality 3D assets and
scenes. When it comes to editing of these objects, however, existing approaches
are either slow, compromise on quality, or do not provide enough customization.
We introduce a novel approach to quickly edit a 3D model from a single
reference view. Our technique first segments the edit image, and then matches
semantically corresponding regions across chosen segmented dataset views using
DINO features. A color or texture change from a particular region of the edit
image can then be applied to other views automatically in a semantically
sensible manner. These edited views act as an updated dataset to further train
and re-style the 3D scene. The end-result is therefore an edited 3D model. Our
framework enables a wide variety of editing tasks such as manual local edits,
correspondence based style transfer from any example image, and a combination
of different styles from multiple example images. We use Gaussian Splats as our
primary 3D representation due to their speed and ease of local editing, but our
technique works for other methods such as NeRFs as well. We show through
multiple examples that our method produces higher quality results while
offering fine-grained control of editing. Project page: ice-gaussian.github.io";Vishnu Jaganathan<author:sep>Hannah Hanyun Huang<author:sep>Muhammad Zubair Irshad<author:sep>Varun Jampani<author:sep>Amit Raj<author:sep>Zsolt Kira;http://arxiv.org/pdf/2406.08488v1;cs.CV;"Accepted to CVPR AI4CC Workshop 2024. Project page:
  https://ice-gaussian.github.io";nerf
2406.08009v1;http://arxiv.org/abs/2406.08009v1;2024-06-12;OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with  Fine-Grained Understanding;"In recent years, there has been a surge of interest in open-vocabulary 3D
scene reconstruction facilitated by visual language models (VLMs), which
showcase remarkable capabilities in open-set retrieval. However, existing
methods face some limitations: they either focus on learning point-wise
features, resulting in blurry semantic understanding, or solely tackle
object-level reconstruction, thereby overlooking the intricate details of the
object's interior. To address these challenges, we introduce OpenObj, an
innovative approach to build open-vocabulary object-level Neural Radiance
Fields (NeRF) with fine-grained understanding. In essence, OpenObj establishes
a robust framework for efficient and watertight scene modeling and
comprehension at the object-level. Moreover, we incorporate part-level features
into the neural fields, enabling a nuanced representation of object interiors.
This approach captures object-level instances while maintaining a fine-grained
understanding. The results on multiple datasets demonstrate that OpenObj
achieves superior performance in zero-shot semantic segmentation and retrieval
tasks. Additionally, OpenObj supports real-world robotics tasks at multiple
scales, including global movement and local manipulation.";Yinan Deng<author:sep>Jiahui Wang<author:sep>Jingyu Zhao<author:sep>Jianyu Dou<author:sep>Yi Yang<author:sep>Yufeng Yue;http://arxiv.org/pdf/2406.08009v1;cs.CV;8 pages, 7figures. Project Url: https://openobj.github.io/;nerf
2406.07499v1;http://arxiv.org/abs/2406.07499v1;2024-06-11;Trim 3D Gaussian Splatting for Accurate Geometry Representation;"In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to
reconstruct accurate 3D geometry from images. Previous arts for geometry
reconstruction from 3D Gaussians mainly focus on exploring strong geometry
regularization. Instead, from a fresh perspective, we propose to obtain
accurate 3D geometry of a scene by Gaussian trimming, which selectively removes
the inaccurate geometry while preserving accurate structures. To achieve this,
we analyze the contributions of individual 3D Gaussians and propose a
contribution-based trimming strategy to remove the redundant or inaccurate
Gaussians. Furthermore, our experimental and theoretical analyses reveal that a
relatively small Gaussian scale is a non-negligible factor in representing and
optimizing the intricate details. Therefore the proposed TrimGS maintains
relatively small Gaussian scales. In addition, TrimGS is also compatible with
the effective geometry regularization strategies in previous arts. When
combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS
consistently yields more accurate geometry and higher perceptual quality. Our
project page is https://trimgs.github.io";Lue Fan<author:sep>Yuxue Yang<author:sep>Minxing Li<author:sep>Hongsheng Li<author:sep>Zhaoxiang Zhang;http://arxiv.org/pdf/2406.07499v1;cs.CV;Project page: https://trimgs.github.io/;gaussian splatting
2406.07648v1;http://arxiv.org/abs/2406.07648v1;2024-06-11;M-LRM: Multi-view Large Reconstruction Model;"Despite recent advancements in the Large Reconstruction Model (LRM)
demonstrating impressive results, when extending its input from single image to
multiple images, it exhibits inefficiencies, subpar geometric and texture
quality, as well as slower convergence speed than expected.
  It is attributed to that, LRM formulates 3D reconstruction as a naive
images-to-3D translation problem, ignoring the strong 3D coherence among the
input images. In this paper, we propose a Multi-view Large Reconstruction Model
(M-LRM) designed to efficiently reconstruct high-quality 3D shapes from
multi-views in a 3D-aware manner. Specifically, we introduce a multi-view
consistent cross-attention scheme to enable M-LRM to accurately query
information from the input images. Moreover, we employ the 3D priors of the
input multi-view images to initialize the tri-plane tokens. Compared to LRM,
the proposed M-LRM can produce a tri-plane NeRF with $128 \times 128$
resolution and generate 3D shapes of high fidelity. Experimental studies
demonstrate that our model achieves a significant performance gain and faster
training convergence than LRM. Project page: https://murphylmf.github.io/M-LRM/";Mengfei Li<author:sep>Xiaoxiao Long<author:sep>Yixun Liang<author:sep>Weiyu Li<author:sep>Yuan Liu<author:sep>Peng Li<author:sep>Xiaowei Chi<author:sep>Xingqun Qi<author:sep>Wei Xue<author:sep>Wenhan Luo<author:sep>Qifeng Liu<author:sep>Yike Guo;http://arxiv.org/pdf/2406.07648v1;cs.CV;;nerf
2406.07742v1;http://arxiv.org/abs/2406.07742v1;2024-06-11;C3DAG: Controlled 3D Animal Generation using 3D pose guidance;"Recent advancements in text-to-3D generation have demonstrated the ability to
generate high quality 3D assets. However while generating animals these methods
underperform, often portraying inaccurate anatomy and geometry. Towards
ameliorating this defect, we present C3DAG, a novel pose-Controlled text-to-3D
Animal Generation framework which generates a high quality 3D animal consistent
with a given pose. We also introduce an automatic 3D shape creator tool, that
allows dynamic pose generation and modification via a web-based tool, and that
generates a 3D balloon animal using simple geometries. A NeRF is then
initialized using this 3D shape using depth-controlled SDS. In the next stage,
the pre-trained NeRF is fine-tuned using quadruped-pose-controlled SDS. The
pipeline that we have developed not only produces geometrically and
anatomically consistent results, but also renders highly controlled 3D animals,
unlike prior methods which do not allow fine-grained pose control.";Sandeep Mishra<author:sep>Oindrila Saha<author:sep>Alan C. Bovik;http://arxiv.org/pdf/2406.07742v1;cs.CV;;nerf
2406.06972v1;http://arxiv.org/abs/2406.06972v1;2024-06-11;Generative Lifting of Multiview to 3D from Unknown Pose: Wrapping NeRF  inside Diffusion;"We cast multiview reconstruction from unknown pose as a generative modeling
problem. From a collection of unannotated 2D images of a scene, our approach
simultaneously learns both a network to predict camera pose from 2D image
input, as well as the parameters of a Neural Radiance Field (NeRF) for the 3D
scene. To drive learning, we wrap both the pose prediction network and NeRF
inside a Denoising Diffusion Probabilistic Model (DDPM) and train the system
via the standard denoising objective. Our framework requires the system
accomplish the task of denoising an input 2D image by predicting its pose and
rendering the NeRF from that pose. Learning to denoise thus forces the system
to concurrently learn the underlying 3D NeRF representation and a mapping from
images to camera extrinsic parameters. To facilitate the latter, we design a
custom network architecture to represent pose as a distribution, granting
implicit capacity for discovering view correspondences when trained end-to-end
for denoising alone. This technique allows our system to successfully build
NeRFs, without pose knowledge, for challenging scenes where competing methods
fail. At the conclusion of training, our learned NeRF can be extracted and used
as a 3D scene model; our full system can be used to sample novel camera poses
and generate novel-view images.";Xin Yuan<author:sep>Rana Hanocka<author:sep>Michael Maire;http://arxiv.org/pdf/2406.06972v1;cs.CV;;nerf
2406.07329v1;http://arxiv.org/abs/2406.07329v1;2024-06-11;Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field;"Radiance field methods represent the state of the art in reconstructing
complex scenes from multi-view photos. However, these reconstructions often
suffer from one or both of the following limitations: First, they typically
represent scenes in low dynamic range (LDR), which restricts their use to
evenly lit environments and hinders immersive viewing experiences. Secondly,
their reliance on a pinhole camera model, assuming all scene elements are in
focus in the input images, presents practical challenges and complicates
refocusing during novel-view synthesis. Addressing these limitations, we
present a lightweight method based on 3D Gaussian Splatting that utilizes
multi-view LDR images of a scene with varying exposure times, apertures, and
focus distances as input to reconstruct a high-dynamic-range (HDR) radiance
field. By incorporating analytical convolutions of Gaussians based on a
thin-lens camera model as well as a tonemapping module, our reconstructions
enable the rendering of HDR content with flexible refocusing capabilities. We
demonstrate that our combined treatment of HDR and depth of field facilitates
real-time cinematic rendering, outperforming the state of the art.";Chao Wang<author:sep>Krzysztof Wolski<author:sep>Bernhard Kerbl<author:sep>Ana Serrano<author:sep>Mojtaba Bemana<author:sep>Hans-Peter Seidel<author:sep>Karol Myszkowski<author:sep>Thomas Leimkühler;http://arxiv.org/pdf/2406.07329v1;cs.CV;;gaussian splatting
2406.06948v1;http://arxiv.org/abs/2406.06948v1;2024-06-11;Neural Visibility Field for Uncertainty-Driven Active Mapping;"This paper presents Neural Visibility Field (NVF), a novel uncertainty
quantification method for Neural Radiance Fields (NeRF) applied to active
mapping. Our key insight is that regions not visible in the training views lead
to inherently unreliable color predictions by NeRF at this region, resulting in
increased uncertainty in the synthesized views. To address this, we propose to
use Bayesian Networks to composite position-based field uncertainty into
ray-based uncertainty in camera observations. Consequently, NVF naturally
assigns higher uncertainty to unobserved regions, aiding robots to select the
most informative next viewpoints. Extensive evaluations show that NVF excels
not only in uncertainty quantification but also in scene reconstruction for
active mapping, outperforming existing methods.";Shangjie Xue<author:sep>Jesse Dill<author:sep>Pranay Mathur<author:sep>Frank Dellaert<author:sep>Panagiotis Tsiotra<author:sep>Danfei Xu;http://arxiv.org/pdf/2406.06948v1;cs.CV;"Accepted to CVPR 2024. More details can be found at
  https://sites.google.com/view/nvf-cvpr24/";nerf
2406.07431v1;http://arxiv.org/abs/2406.07431v1;2024-06-11;Active Scout: Multi-Target Tracking Using Neural Radiance Fields in  Dense Urban Environments;"We study pursuit-evasion games in highly occluded urban environments, e.g.
tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic
targets on the ground. We show that we can build a neural radiance field (NeRF)
representation of the city -- online -- using RGB and depth images from
different vantage points. This representation is used to calculate the
information gain to both explore unknown parts of the city and track the
targets -- thereby giving a completely first-principles approach to actively
tracking dynamic targets. We demonstrate, using a custom-built simulator using
Open Street Maps data of Philadelphia and New York City, that we can explore
and locate 20 stationary targets within 300 steps. This is slower than a greedy
baseline which which does not use active perception. But for dynamic targets
that actively hide behind occlusions, we show that our approach maintains, at
worst, a tracking error of 200m; the greedy baseline can have a tracking error
as large as 600m. We observe a number of interesting properties in the scout's
policies, e.g., it switches its attention to track a different target
periodically, as the quality of the NeRF representation improves over time, the
scout also becomes better in terms of target tracking.";Christopher D. Hsu<author:sep>Pratik Chaudhari;http://arxiv.org/pdf/2406.07431v1;cs.MA;8 pages, 8 figures, 1 table;nerf
2406.06133v1;http://arxiv.org/abs/2406.06133v1;2024-06-10;ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields  with Diffusion Models;"We propose ExtraNeRF, a novel method for extrapolating the range of views
handled by a Neural Radiance Field (NeRF). Our main idea is to leverage NeRFs
to model scene-specific, fine-grained details, while capitalizing on diffusion
models to extrapolate beyond our observed data. A key ingredient is to track
visibility to determine what portions of the scene have not been observed, and
focus on reconstructing those regions consistently with diffusion models. Our
primary contributions include a visibility-aware diffusion-based inpainting
module that is fine-tuned on the input imagery, yielding an initial NeRF with
moderate quality (often blurry) inpainted regions, followed by a second
diffusion model trained on the input imagery to consistently enhance, notably
sharpen, the inpainted imagery from the first pass. We demonstrate high-quality
results, extrapolating beyond a small number of (typically six or fewer) input
views, effectively outpainting the NeRF as well as inpainting newly disoccluded
regions inside the original viewing volume. We compare with related work both
quantitatively and qualitatively and show significant gains over prior art.";Meng-Li Shih<author:sep>Wei-Chiu Ma<author:sep>Aleksander Holynski<author:sep>Forrester Cole<author:sep>Brian L. Curless<author:sep>Janne Kontkanen;http://arxiv.org/pdf/2406.06133v1;cs.CV;8 pages, 8 figures, CVPR2024;nerf
2406.06367v1;http://arxiv.org/abs/2406.06367v1;2024-06-10;MVGamba: Unify 3D Content Generation as State Space Sequence Modeling;"Recent 3D large reconstruction models (LRMs) can generate high-quality 3D
content in sub-seconds by integrating multi-view diffusion models with scalable
multi-view reconstructors. Current works further leverage 3D Gaussian Splatting
as 3D representation for improved visual quality and rendering efficiency.
However, we observe that existing Gaussian reconstruction models often suffer
from multi-view inconsistency and blurred textures. We attribute this to the
compromise of multi-view information propagation in favor of adopting powerful
yet computationally intensive architectures (\eg, Transformers). To address
this issue, we introduce MVGamba, a general and lightweight Gaussian
reconstruction model featuring a multi-view Gaussian reconstructor based on the
RNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal
context containing multi-view information for cross-view self-refinement while
generating a long sequence of Gaussians for fine-detail modeling with linear
complexity. With off-the-shelf multi-view diffusion models integrated, MVGamba
unifies 3D generation tasks from a single image, sparse images, or text
prompts. Extensive experiments demonstrate that MVGamba outperforms
state-of-the-art baselines in all 3D content generation scenarios with
approximately only $0.1\times$ of the model size.";Xuanyu Yi<author:sep>Zike Wu<author:sep>Qiuhong Shen<author:sep>Qingshan Xu<author:sep>Pan Zhou<author:sep>Joo-Hwee Lim<author:sep>Shuicheng Yan<author:sep>Xinchao Wang<author:sep>Hanwang Zhang;http://arxiv.org/pdf/2406.06367v1;cs.CV;;gaussian splatting
2406.06216v1;http://arxiv.org/abs/2406.06216v1;2024-06-10;Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering  for HDR View Synthesis;"Volumetric rendering based methods, like NeRF, excel in HDR view synthesis
from RAWimages, especially for nighttime scenes. While, they suffer from long
training times and cannot perform real-time rendering due to dense sampling
requirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time
rendering and faster training. However, implementing RAW image-based view
synthesis directly using 3DGS is challenging due to its inherent drawbacks: 1)
in nighttime scenes, extremely low SNR leads to poor structure-from-motion
(SfM) estimation in distant views; 2) the limited representation capacity of
spherical harmonics (SH) function is unsuitable for RAW linear color space; and
3) inaccurate scene structure hampers downstream tasks such as refocusing. To
address these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our
method proposes Cone Scatter Initialization to enrich the estimation of SfM,
and replaces SH with a Color MLP to represent the RAW linear color space.
Additionally, we introduce depth distortion and near-far regularizations to
improve the accuracy of scene structure for downstream tasks. These designs
enable LE3D to perform real-time novel view synthesis, HDR rendering,
refocusing, and tone-mapping changes. Compared to previous volumetric rendering
based methods, LE3D reduces training time to 1% and improves rendering speed by
up to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can
be found in https://github.com/Srameo/LE3D .";Xin Jin<author:sep>Pengyi Jiao<author:sep>Zheng-Peng Duan<author:sep>Xingchao Yang<author:sep>Chun-Le Guo<author:sep>Bo Ren<author:sep>Chongyi Li;http://arxiv.org/pdf/2406.06216v1;cs.CV;;gaussian splatting<tag:sep>nerf
2406.06526v1;http://arxiv.org/abs/2406.06526v1;2024-06-10;GaussianCity: Generative Gaussian Splatting for Unbounded 3D City  Generation;"3D city generation with NeRF-based methods shows promising generation results
but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has
emerged as a highly efficient alternative for object-level 3D generation.
However, adapting 3D-GS from finite-scale 3D objects and humans to
infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails
significant storage overhead (out-of-memory issues), arising from the need to
expand points to billions, often demanding hundreds of Gigabytes of VRAM for a
city scene spanning 10km^2. In this paper, we propose GaussianCity, a
generative Gaussian Splatting framework dedicated to efficiently synthesizing
unbounded 3D cities with a single feed-forward pass. Our key insights are
two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a
highly compact intermediate representation, ensuring that the growth in VRAM
usage for unbounded scenes remains constant, thus enabling unbounded city
generation. 2) Spatial-aware Gaussian Attribute Decoder: We present
spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which
leverages Point Serializer to integrate the structural and contextual
characteristics of BEV points. Extensive experiments demonstrate that
GaussianCity achieves state-of-the-art results in both drone-view and
street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity
exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18
FPS).";Haozhe Xie<author:sep>Zhaoxi Chen<author:sep>Fangzhou Hong<author:sep>Ziwei Liu;http://arxiv.org/pdf/2406.06526v1;cs.CV;;gaussian splatting<tag:sep>nerf
2406.06521v1;http://arxiv.org/abs/2406.06521v1;2024-06-10;PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity  Surface Reconstruction;"Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due
to its high-quality rendering, and ultra-fast training and rendering speed.
However, due to the unstructured and irregular nature of Gaussian point clouds,
it is difficult to guarantee geometric reconstruction accuracy and multi-view
consistency simply by relying on image reconstruction loss. Although many
studies on surface reconstruction based on 3DGS have emerged recently, the
quality of their meshes is generally unsatisfactory. To address this problem,
we propose a fast planar-based Gaussian splatting reconstruction representation
(PGSR) to achieve high-fidelity surface reconstruction while ensuring
high-quality rendering. Specifically, we first introduce an unbiased depth
rendering method, which directly renders the distance from the camera origin to
the Gaussian plane and the corresponding normal map based on the Gaussian
distribution of the point cloud, and divides the two to obtain the unbiased
depth. We then introduce single-view geometric, multi-view photometric, and
geometric regularization to preserve global geometric accuracy. We also propose
a camera exposure compensation model to cope with scenes with large
illumination variations. Experiments on indoor and outdoor scenes show that our
method achieves fast training and rendering while maintaining high-fidelity
rendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based
methods.";Danpeng Chen<author:sep>Hai Li<author:sep>Weicai Ye<author:sep>Yifan Wang<author:sep>Weijian Xie<author:sep>Shangjin Zhai<author:sep>Nan Wang<author:sep>Haomin Liu<author:sep>Hujun Bao<author:sep>Guofeng Zhang;http://arxiv.org/pdf/2406.06521v1;cs.CV;project page: https://zju3dv.github.io/pgsr/;gaussian splatting<tag:sep>nerf
2406.06527v1;http://arxiv.org/abs/2406.06527v1;2024-06-10;IllumiNeRF: 3D Relighting without Inverse Rendering;"Existing methods for relightable view synthesis -- using a set of images of
an object under unknown lighting to recover a 3D representation that can be
rendered from novel viewpoints under a target illumination -- are based on
inverse rendering, and attempt to disentangle the object geometry, materials,
and lighting that explain the input images. Furthermore, this typically
involves optimization through differentiable Monte Carlo rendering, which is
brittle and computationally-expensive. In this work, we propose a simpler
approach: we first relight each input image using an image diffusion model
conditioned on lighting and then reconstruct a Neural Radiance Field (NeRF)
with these relit images, from which we render novel views under the target
lighting. We demonstrate that this strategy is surprisingly competitive and
achieves state-of-the-art results on multiple relighting benchmarks. Please see
our project page at https://illuminerf.github.io/.";Xiaoming Zhao<author:sep>Pratul P. Srinivasan<author:sep>Dor Verbin<author:sep>Keunhong Park<author:sep>Ricardo Martin Brualla<author:sep>Philipp Henzler;http://arxiv.org/pdf/2406.06527v1;cs.CV;Project page: https://illuminerf.github.io/;nerf
2406.05774v1;http://arxiv.org/abs/2406.05774v1;2024-06-09;VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface  Reconstruction;"Although 3D Gaussian Splatting has been widely studied because of its
realistic and efficient novel-view synthesis, it is still challenging to
extract a high-quality surface from the point-based representation. Previous
works improve the surface by incorporating geometric priors from the
off-the-shelf normal estimator. However, there are two main limitations: 1)
Supervising normal rendered from 3D Gaussians updates only the rotation
parameter while neglecting other geometric parameters; 2) The inconsistency of
predicted normal maps across multiple views may lead to severe reconstruction
artifacts. In this paper, we propose a Depth-Normal regularizer that directly
couples normal with other geometric parameters, leading to full updates of the
geometric parameters from normal regularization. We further propose a
confidence term to mitigate inconsistencies of normal predictions across
multiple views. Moreover, we also introduce a densification and splitting
strategy to regularize the size and distribution of 3D Gaussians for more
accurate surface modeling. Compared with Gaussian-based baselines, experiments
show that our approach obtains better reconstruction quality and maintains
competitive appearance quality at faster training speed and 100+ FPS rendering.
Our code will be made open-source upon paper acceptance.";Hanlin Chen<author:sep>Fangyin Wei<author:sep>Chen Li<author:sep>Tianxin Huang<author:sep>Yunsong Wang<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2406.05774v1;cs.CV;;gaussian splatting
2406.05852v1;http://arxiv.org/abs/2406.05852v1;2024-06-09;RefGaussian: Disentangling Reflections from 3D Gaussian Splatting for  Realistic Rendering;"3D Gaussian Splatting (3D-GS) has made a notable advancement in the field of
neural rendering, 3D scene reconstruction, and novel view synthesis.
Nevertheless, 3D-GS encounters the main challenge when it comes to accurately
representing physical reflections, especially in the case of total reflection
and semi-reflection that are commonly found in real-world scenes. This
limitation causes reflections to be mistakenly treated as independent elements
with physical presence, leading to imprecise reconstructions. Herein, to tackle
this challenge, we propose RefGaussian to disentangle reflections from 3D-GS
for realistically modeling reflections. Specifically, we propose to split a
scene into transmitted and reflected components and represent these components
using two Spherical Harmonics (SH). Given that this decomposition is not fully
determined, we employ local regularization techniques to ensure local
smoothness for both the transmitted and reflected components, thereby achieving
more plausible decomposition outcomes than 3D-GS. Experimental results
demonstrate that our approach achieves superior novel view synthesis and
accurate depth estimation outcomes. Furthermore, it enables the utilization of
scene editing applications, ensuring both high-quality results and physical
coherence.";Rui Zhang<author:sep>Tianyue Luo<author:sep>Weidong Yang<author:sep>Ben Fei<author:sep>Jingyi Xu<author:sep>Qingyuan Zhou<author:sep>Keyi Liu<author:sep>Ying He;http://arxiv.org/pdf/2406.05852v1;cs.CV;;gaussian splatting
2406.05649v1;http://arxiv.org/abs/2406.05649v1;2024-06-09;GTR: Improving Large 3D Reconstruction Models through Geometry and  Texture Refinement;"We propose a novel approach for 3D mesh reconstruction from multi-view
images. Our method takes inspiration from large reconstruction models like LRM
that use a transformer-based triplane generator and a Neural Radiance Field
(NeRF) model trained on multi-view images. However, in our method, we introduce
several important modifications that allow us to significantly enhance 3D
reconstruction quality. First of all, we examine the original LRM architecture
and find several shortcomings. Subsequently, we introduce respective
modifications to the LRM architecture, which lead to improved multi-view image
representation and more computationally efficient training. Second, in order to
improve geometry reconstruction and enable supervision at full image
resolution, we extract meshes from the NeRF field in a differentiable manner
and fine-tune the NeRF model through mesh rendering. These modifications allow
us to achieve state-of-the-art performance on both 2D and 3D evaluation
metrics, such as a PSNR of 28.67 on Google Scanned Objects (GSO) dataset.
Despite these superior results, our feed-forward model still struggles to
reconstruct complex textures, such as text and portraits on assets. To address
this, we introduce a lightweight per-instance texture refinement procedure.
This procedure fine-tunes the triplane representation and the NeRF color
estimation model on the mesh surface using the input multi-view images in just
4 seconds. This refinement improves the PSNR to 29.79 and achieves faithful
reconstruction of complex textures, such as text. Additionally, our approach
enables various downstream applications, including text- or image-to-3D
generation.";Peiye Zhuang<author:sep>Songfang Han<author:sep>Chaoyang Wang<author:sep>Aliaksandr Siarohin<author:sep>Jiaxu Zou<author:sep>Michael Vasilkovsky<author:sep>Vladislav Shakhrai<author:sep>Sergey Korolev<author:sep>Sergey Tulyakov<author:sep>Hsin-Ying Lee;http://arxiv.org/pdf/2406.05649v1;cs.CV;19 pages, 17 figures. Project page: https://payeah.net/projects/GTR/;nerf
2406.04961v1;http://arxiv.org/abs/2406.04961v1;2024-06-07;Multiplane Prior Guided Few-Shot Aerial Scene Rendering;"Neural Radiance Fields (NeRF) have been successfully applied in various
aerial scenes, yet they face challenges with sparse views due to limited
supervision. The acquisition of dense aerial views is often prohibitive, as
unmanned aerial vehicles (UAVs) may encounter constraints in perspective range
and energy constraints. In this work, we introduce Multiplane Prior guided NeRF
(MPNeRF), a novel approach tailored for few-shot aerial scene rendering-marking
a pioneering effort in this domain. Our key insight is that the intrinsic
geometric regularities specific to aerial imagery could be leveraged to enhance
NeRF in sparse aerial scenes. By investigating NeRF's and Multiplane Image
(MPI)'s behavior, we propose to guide the training process of NeRF with a
Multiplane Prior. The proposed Multiplane Prior draws upon MPI's benefits and
incorporates advanced image comprehension through a SwinV2 Transformer,
pre-trained via SimMIM. Our extensive experiments demonstrate that MPNeRF
outperforms existing state-of-the-art methods applied in non-aerial contexts,
by tripling the performance in SSIM and LPIPS even with three views available.
We hope our work offers insights into the development of NeRF-based
applications in aerial scenes with limited data.";Zihan Gao<author:sep>Licheng Jiao<author:sep>Lingling Li<author:sep>Xu Liu<author:sep>Fang Liu<author:sep>Puhua Chen<author:sep>Yuwei Guo;http://arxiv.org/pdf/2406.04961v1;cs.CV;17 pages, 8 figures, accepted at CVPR 2024;nerf
2406.04960v1;http://arxiv.org/abs/2406.04960v1;2024-06-07;Multi-style Neural Radiance Field with AdaIN;"In this work, we propose a novel pipeline that combines AdaIN and NeRF for
the task of stylized Novel View Synthesis. Compared to previous works, we make
the following contributions: 1) We simplify the pipeline. 2) We extend the
capabilities of model to handle the multi-style task. 3) We modify the model
architecture to perform well on styles with strong brush strokes. 4) We
implement style interpolation on the multi-style model, allowing us to control
the style between any two styles and the style intensity between the stylized
output and the original scene, providing better control over the stylization
strength.";Yu-Wen Pao<author:sep>An-Jie Li;http://arxiv.org/pdf/2406.04960v1;cs.CV;;nerf
2406.04343v1;http://arxiv.org/abs/2406.04343v1;2024-06-06;Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a  Single Image;"In this paper, we propose Flash3D, a method for scene reconstruction and
novel view synthesis from a single image which is both very generalisable and
efficient. For generalisability, we start from a ""foundation"" model for
monocular depth estimation and extend it to a full 3D shape and appearance
reconstructor. For efficiency, we base this extension on feed-forward Gaussian
Splatting. Specifically, we predict a first layer of 3D Gaussians at the
predicted depth, and then add additional layers of Gaussians that are offset in
space, allowing the model to complete the reconstruction behind occlusions and
truncations. Flash3D is very efficient, trainable on a single GPU in a day, and
thus accessible to most researchers. It achieves state-of-the-art results when
trained and tested on RealEstate10k. When transferred to unseen datasets like
NYU it outperforms competitors by a large margin. More impressively, when
transferred to KITTI, Flash3D achieves better PSNR than methods trained
specifically on that dataset. In some instances, it even outperforms recent
methods that use multiple views as input. Code, models, demo, and more results
are available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.";Stanislaw Szymanowicz<author:sep>Eldar Insafutdinov<author:sep>Chuanxia Zheng<author:sep>Dylan Campbell<author:sep>João F. Henriques<author:sep>Christian Rupprecht<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2406.04343v1;cs.CV;Project page: https://www.robots.ox.ac.uk/~vgg/research/flash3d/;
2406.04101v1;http://arxiv.org/abs/2406.04101v1;2024-06-06;How Far Can We Compress Instant-NGP-Based NeRF?;"In recent years, Neural Radiance Field (NeRF) has demonstrated remarkable
capabilities in representing 3D scenes. To expedite the rendering process,
learnable explicit representations have been introduced for combination with
implicit NeRF representation, which however results in a large storage space
requirement. In this paper, we introduce the Context-based NeRF Compression
(CNC) framework, which leverages highly efficient context models to provide a
storage-friendly NeRF representation. Specifically, we excavate both level-wise
and dimension-wise context dependencies to enable probability prediction for
information entropy reduction. Additionally, we exploit hash collision and
occupancy grids as strong prior knowledge for better context modeling. To the
best of our knowledge, we are the first to construct and exploit context models
for NeRF compression. We achieve a size reduction of 100$\times$ and 70$\times$
with improved fidelity against the baseline Instant-NGP on Synthesic-NeRF and
Tanks and Temples datasets, respectively. Additionally, we attain 86.7\% and
82.3\% storage size reduction against the SOTA NeRF compression method BiRF.
Our code is available here: https://github.com/YihangChen-ee/CNC.";Yihang Chen<author:sep>Qianyi Wu<author:sep>Mehrtash Harandi<author:sep>Jianfei Cai;http://arxiv.org/pdf/2406.04101v1;cs.CV;"Project Page: https://yihangchen-ee.github.io/project_cnc/ Code:
  https://github.com/yihangchen-ee/cnc/. We further propose a 3DGS compression
  method HAC, which is based on CNC:
  https://yihangchen-ee.github.io/project_hac/";nerf
2406.03697v1;http://arxiv.org/abs/2406.03697v1;2024-06-06;Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene  Reconstruction;"Rendering novel view images in dynamic scenes is a crucial yet challenging
task. Current methods mainly utilize NeRF-based methods to represent the static
scene and an additional time-variant MLP to model scene deformations, resulting
in relatively low rendering quality as well as slow inference speed. To tackle
these challenges, we propose a novel framework named Superpoint Gaussian
Splatting (SP-GS). Specifically, our framework first employs explicit 3D
Gaussians to reconstruct the scene and then clusters Gaussians with similar
properties (e.g., rotation, translation, and location) into superpoints.
Empowered by these superpoints, our method manages to extend 3D Gaussian
splatting to dynamic scenes with only a slight increase in computational
expense. Apart from achieving state-of-the-art visual quality and real-time
rendering under high resolutions, the superpoint representation provides a
stronger manipulation capability. Extensive experiments demonstrate the
practicality and effectiveness of our approach on both synthetic and real-world
datasets. Please see our project page at
https://dnvtmf.github.io/SP_GS.github.io.";Diwen Wan<author:sep>Ruijie Lu<author:sep>Gang Zeng;http://arxiv.org/pdf/2406.03697v1;cs.CV;Accepted by ICML 2024;gaussian splatting<tag:sep>nerf
2406.03723v1;http://arxiv.org/abs/2406.03723v1;2024-06-06;Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware  Spatio-Temporal Sampling;"Extensions of Neural Radiance Fields (NeRFs) to model dynamic scenes have
enabled their near photo-realistic, free-viewpoint rendering. Although these
methods have shown some potential in creating immersive experiences, two
drawbacks limit their ubiquity: (i) a significant reduction in reconstruction
quality when the computing budget is limited, and (ii) a lack of semantic
understanding of the underlying scenes. To address these issues, we introduce
Gear-NeRF, which leverages semantic information from powerful image
segmentation models. Our approach presents a principled way for learning a
spatio-temporal (4D) semantic embedding, based on which we introduce the
concept of gears to allow for stratified modeling of dynamic regions of the
scene based on the extent of their motion. Such differentiation allows us to
adjust the spatio-temporal sampling resolution for each region in proportion to
its motion scale, achieving more photo-realistic dynamic novel view synthesis.
At the same time, almost for free, our approach enables free-viewpoint tracking
of objects of interest - a functionality not yet achieved by existing
NeRF-based methods. Empirical studies validate the effectiveness of our method,
where we achieve state-of-the-art rendering and tracking performance on
multiple challenging datasets.";Xinhang Liu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang<author:sep>Pedro Miraldo<author:sep>Suhas Lohit<author:sep>Moitreya Chatterjee;http://arxiv.org/pdf/2406.03723v1;cs.CV;"Paper accepted to IEEE/CVF CVPR 2024 (Spotlight). Work done when XL
  was an intern at MERL. Project Page Link:
  https://merl.com/research/highlights/gear-nerf";nerf
2406.04251v1;http://arxiv.org/abs/2406.04251v1;2024-06-06;Localized Gaussian Point Management;"Point management is a critical component in optimizing 3D Gaussian Splatting
(3DGS) models, as the point initiation (e.g., via structure from motion) is
distributionally inappropriate. Typically, the Adaptive Density Control (ADC)
algorithm is applied, leveraging view-averaged gradient magnitude thresholding
for point densification, opacity thresholding for pruning, and regular
all-points opacity reset. However, we reveal that this strategy is limited in
tackling intricate/special image regions (e.g., transparent) as it is unable to
identify all the 3D zones that require point densification, and lacking an
appropriate mechanism to handle the ill-conditioned points with negative
impacts (occlusion due to false high opacity). To address these limitations, we
propose a Localized Point Management (LPM) strategy, capable of identifying
those error-contributing zones in the highest demand for both point addition
and geometry calibration. Zone identification is achieved by leveraging the
underlying multiview geometry constraints, with the guidance of image rendering
errors. We apply point densification in the identified zone, whilst resetting
the opacity of those points residing in front of these regions so that a new
opportunity is created to correct ill-conditioned points. Serving as a
versatile plugin, LPM can be seamlessly integrated into existing 3D Gaussian
Splatting models. Experimental evaluation across both static 3D and dynamic 4D
scenes validate the efficacy of our LPM strategy in boosting a variety of
existing 3DGS models both quantitatively and qualitatively. Notably, LPM
improves both vanilla 3DGS and SpaceTimeGS to achieve state-of-the-art
rendering quality while retaining real-time speeds, outperforming on
challenging datasets such as Tanks & Temples and the Neural 3D Video Dataset.";Haosen Yang<author:sep>Chenhao Zhang<author:sep>Wenqing Wang<author:sep>Marco Volino<author:sep>Adrian Hilton<author:sep>Li Zhang<author:sep>Xiatian Zhu;http://arxiv.org/pdf/2406.04251v1;cs.CV;;gaussian splatting
2406.04155v1;http://arxiv.org/abs/2406.04155v1;2024-06-06;Improving Physics-Augmented Continuum Neural Radiance Field-Based  Geometry-Agnostic System Identification with Lagrangian Particle Optimization;"Geometry-agnostic system identification is a technique for identifying the
geometry and physical properties of an object from video sequences without any
geometric assumptions. Recently, physics-augmented continuum neural radiance
fields (PAC-NeRF) has demonstrated promising results for this technique by
utilizing a hybrid Eulerian-Lagrangian representation, in which the geometry is
represented by the Eulerian grid representations of NeRF, the physics is
described by a material point method (MPM), and they are connected via
Lagrangian particles. However, a notable limitation of PAC-NeRF is that its
performance is sensitive to the learning of the geometry from the first frames
owing to its two-step optimization. First, the grid representations are
optimized with the first frames of video sequences, and then the physical
properties are optimized through video sequences utilizing the fixed
first-frame grid representations. This limitation can be critical when learning
of the geometric structure is difficult, for example, in a few-shot (sparse
view) setting. To overcome this limitation, we propose Lagrangian particle
optimization (LPO), in which the positions and features of particles are
optimized through video sequences in Lagrangian space. This method allows for
the optimization of the geometric structure across the entire video sequence
within the physical constraints imposed by the MPM. The experimental results
demonstrate that the LPO is useful for geometric correction and physical
identification in sparse-view settings.";Takuhiro Kaneko;http://arxiv.org/pdf/2406.04155v1;cs.CV;"Accepted to CVPR 2024. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/lpo/";nerf
2406.04253v1;http://arxiv.org/abs/2406.04253v1;2024-06-06;A Survey on 3D Human Avatar Modeling -- From Reconstruction to  Generation;"3D modeling has long been an important area in computer vision and computer
graphics. Recently, thanks to the breakthroughs in neural representations and
generative models, we witnessed a rapid development of 3D modeling. 3D human
modeling, lying at the core of many real-world applications, such as gaming and
animation, has attracted significant attention. Over the past few years, a
large body of work on creating 3D human avatars has been introduced, forming a
new and abundant knowledge base for 3D human modeling. The scale of the
literature makes it difficult for individuals to keep track of all the works.
This survey aims to provide a comprehensive overview of these emerging
techniques for 3D human avatar modeling, from both reconstruction and
generation perspectives. Firstly, we review representative methods for 3D human
reconstruction, including methods based on pixel-aligned implicit function,
neural radiance field, and 3D Gaussian Splatting, etc. We then summarize
representative methods for 3D human generation, especially those using large
language models like CLIP, diffusion models, and various 3D representations,
which demonstrate state-of-the-art performance. Finally, we discuss our
reflection on existing methods and open challenges for 3D human avatar
modeling, shedding light on future research.";Ruihe Wang<author:sep>Yukang Cao<author:sep>Kai Han<author:sep>Kwan-Yee K. Wong;http://arxiv.org/pdf/2406.04253v1;cs.CV;30 pages, 21 figures;gaussian splatting
2406.02968v1;http://arxiv.org/abs/2406.02968v1;2024-06-05;Adversarial Generation of Hierarchical Gaussians for 3D Generative Model;"Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend
on ray casting-based volume rendering, which incurs demanding rendering costs.
One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS),
providing a much faster rendering speed and explicit 3D representation. In this
paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its
efficient and explicit characteristics. However, in an adversarial framework,
we observe that a na\""ive generator architecture suffers from training
instability and lacks the capability to adjust the scale of Gaussians. This
leads to model divergence and visual artifacts due to the absence of proper
guidance for initialized positions of Gaussians and densification to manage
their scales adaptively. To address these issues, we introduce a generator
architecture with a hierarchical multi-scale Gaussian representation that
effectively regularizes the position and scale of generated Gaussians.
Specifically, we design a hierarchy of Gaussians where finer-level Gaussians
are parameterized by their coarser-level counterparts; the position of
finer-level Gaussians would be located near their coarser-level counterparts,
and the scale would monotonically decrease as the level becomes finer, modeling
both coarse and fine details of the 3D scene. Experimental results demonstrate
that ours achieves a significantly faster rendering speed (x100) compared to
state-of-the-art 3D consistent GANs with comparable 3D generation capability.
Project page: https://hse1032.github.io/gsgan.";Sangeek Hyun<author:sep>Jae-Pil Heo;http://arxiv.org/pdf/2406.02968v1;cs.CV;Project page: https://hse1032.github.io/gsgan;gaussian splatting
2406.02972v2;http://arxiv.org/abs/2406.02972v2;2024-06-05;Event3DGS: Event-Based 3D Gaussian Splatting for High-Speed Robot  Egomotion;"By combining differentiable rendering with explicit point-based scene
representations, 3D Gaussian Splatting (3DGS) has demonstrated breakthrough 3D
reconstruction capabilities. However, to date 3DGS has had limited impact on
robotics, where high-speed egomotion is pervasive: Egomotion introduces motion
blur and leads to artifacts in existing frame-based 3DGS reconstruction
methods. To address this challenge, we introduce Event3DGS, an {\em
event-based} 3DGS framework. By exploiting the exceptional temporal resolution
of event cameras, Event3GDS can reconstruct high-fidelity 3D structure and
appearance under high-speed egomotion. Extensive experiments on multiple
synthetic and real-world datasets demonstrate the superiority of Event3DGS
compared with existing event-based dense 3D scene reconstruction frameworks;
Event3DGS substantially improves reconstruction quality (+3dB) while reducing
computational costs by 95\%. Our framework also allows one to incorporate a few
motion-blurred frame-based measurements into the reconstruction process to
further improve appearance fidelity without loss of structural accuracy.";Tianyi Xiong<author:sep>Jiayi Wu<author:sep>Botao He<author:sep>Cornelia Fermuller<author:sep>Yiannis Aloimonos<author:sep>Heng Huang<author:sep>Christopher A. Metzler;http://arxiv.org/pdf/2406.02972v2;cs.CV;;gaussian splatting
2406.01916v1;http://arxiv.org/abs/2406.01916v1;2024-06-04;FastLGS: Speeding up Language Embedded Gaussians with Feature Grid  Mapping;"The semantically interactive radiance field has always been an appealing task
for its potential to facilitate user-friendly and automated real-world 3D scene
understanding applications. However, it is a challenging task to achieve high
quality, efficiency and zero-shot ability at the same time with semantics in
radiance fields. In this work, we present FastLGS, an approach that supports
real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high
resolution. We propose the semantic feature grid to save multi-view CLIP
features which are extracted based on Segment Anything Model (SAM) masks, and
map the grids to low dimensional features for semantic field training through
3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through
feature grids from rendered features for open-vocabulary queries. Comparisons
with other state-of-the-art methods prove that FastLGS can achieve the first
place performance concerning both speed and accuracy, where FastLGS is 98x
faster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that
FastLGS is adaptive and compatible with many downstream tasks, such as 3D
segmentation and 3D object inpainting, which can be easily applied to other 3D
manipulation systems.";Yuzhou Ji<author:sep>He Zhu<author:sep>Junshu Tang<author:sep>Wuyi Liu<author:sep>Zhizhong Zhang<author:sep>Yuan Xie<author:sep>Lizhuang Ma<author:sep>Xin Tan;http://arxiv.org/pdf/2406.01916v1;cs.CV;;gaussian splatting
2406.02058v1;http://arxiv.org/abs/2406.02058v1;2024-06-04;OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary  Understanding;"This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting
(3DGS) capable of 3D point-level open vocabulary understanding. Our primary
motivation stems from observing that existing 3DGS-based open vocabulary
methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D
point-level tasks due to weak feature expressiveness and inaccurate 2D-3D
feature associations. To ensure robust feature presentation and 3D point-level
understanding, we first employ SAM masks without cross-frame associations to
train instance features with 3D consistency. These features exhibit both
intra-object consistency and inter-object distinction. Then, we propose a
two-stage codebook to discretize these features from coarse to fine levels. At
the coarse level, we consider the positional information of 3D points to
achieve location-based clustering, which is then refined at the fine level.
Finally, we introduce an instance-level 3D-2D feature association method that
links 3D points to 2D masks, which are further associated with 2D CLIP
features. Extensive experiments, including open vocabulary-based 3D object
selection, 3D point cloud understanding, click-based 3D object selection, and
ablation studies, demonstrate the effectiveness of our proposed method. Project
page: https://3d-aigc.github.io/OpenGaussian";Yanmin Wu<author:sep>Jiarui Meng<author:sep>Haijie Li<author:sep>Chenming Wu<author:sep>Yahao Shi<author:sep>Xinhua Cheng<author:sep>Chen Zhao<author:sep>Haocheng Feng<author:sep>Errui Ding<author:sep>Jingdong Wang<author:sep>Jian Zhang;http://arxiv.org/pdf/2406.02058v1;cs.CV;technical report, 15 pages;gaussian splatting
2406.02720v1;http://arxiv.org/abs/2406.02720v1;2024-06-04;3D-HGS: 3D Half-Gaussian Splatting;"Photo-realistic 3D Reconstruction is a fundamental problem in 3D computer
vision. This domain has seen considerable advancements owing to the advent of
recent neural rendering techniques. These techniques predominantly aim to focus
on learning volumetric representations of 3D scenes and refining these
representations via loss functions derived from rendering. Among these, 3D
Gaussian Splatting (3D-GS) has emerged as a significant method, surpassing
Neural Radiance Fields (NeRFs). 3D-GS uses parameterized 3D Gaussians for
modeling both spatial locations and color information, combined with a
tile-based fast rendering technique. Despite its superior rendering performance
and speed, the use of 3D Gaussian kernels has inherent limitations in
accurately representing discontinuous functions, notably at edges and corners
for shape discontinuities, and across varying textures for color
discontinuities. To address this problem, we propose to employ 3D Half-Gaussian
(3D-HGS) kernels, which can be used as a plug-and-play kernel. Our experiments
demonstrate their capability to improve the performance of current 3D-GS
related methods and achieve state-of-the-art rendering performance on various
datasets without compromising rendering speed.";Haolin Li<author:sep>Jinyang Liu<author:sep>Mario Sznaier<author:sep>Octavia Camps;http://arxiv.org/pdf/2406.02720v1;cs.CV;9 pages, 6 figures;gaussian splatting<tag:sep>nerf
2406.02518v1;http://arxiv.org/abs/2406.02518v1;2024-06-04;DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume  Rendering;"Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images
generated from 3D CT volumes, widely used in preoperative settings but limited
in intraoperative applications due to computational bottlenecks, especially for
accurate but heavy physics-based Monte Carlo methods. While analytical DRR
renderers offer greater efficiency, they overlook anisotropic X-ray image
formation phenomena, such as Compton scattering. We present a novel approach
that marries realistic physics-inspired X-ray simulation with efficient,
differentiable DRR generation using 3D Gaussian splatting (3DGS). Our
direction-disentangled 3DGS (DDGS) method separates the radiosity contribution
into isotropic and direction-dependent components, approximating complex
anisotropic interactions without intricate runtime simulations. Additionally,
we adapt the 3DGS initialization to account for tomography data properties,
enhancing accuracy and efficiency. Our method outperforms state-of-the-art
techniques in image accuracy. Furthermore, our DDGS shows promise for
intraoperative applications and inverse problems such as pose registration,
delivering superior registration accuracy and runtime performance compared to
analytical DRR methods.";Zhongpai Gao<author:sep>Benjamin Planche<author:sep>Meng Zheng<author:sep>Xiao Chen<author:sep>Terrence Chen<author:sep>Ziyan Wu;http://arxiv.org/pdf/2406.02518v1;cs.CV;;gaussian splatting
2406.02541v3;http://arxiv.org/abs/2406.02541v3;2024-06-04;Enhancing Temporal Consistency in Video Editing by Reconstructing Videos  with 3D Gaussian Splatting;"Recent advancements in zero-shot video diffusion models have shown promise
for text-driven video editing, but challenges remain in achieving high temporal
consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting
(3DGS)-based video refiner designed to enhance temporal consistency in
zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian
optimizing process tailored for editing dynamic monocular videos. In the first
stage, Video-3DGS employs an improved version of COLMAP, referred to as
MC-COLMAP, which processes original videos using a Masked and Clipped approach.
For each video clip, MC-COLMAP generates the point clouds for dynamic
foreground objects and complex backgrounds. These point clouds are utilized to
initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent
foreground and background views. Both foreground and background views are then
merged with a 2D learnable parameter map to reconstruct full views. In the
second stage, we leverage the reconstruction ability developed in the first
stage to impose the temporal constraints on the video diffusion model. To
demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive
experiments across two related tasks: Video Reconstruction and Video Editing.
Video-3DGS trained with 3k iterations significantly improves video
reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency
(x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods
on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring
temporal consistency across 58 dynamic monocular videos.";Inkyu Shin<author:sep>Qihang Yu<author:sep>Xiaohui Shen<author:sep>In So Kweon<author:sep>Kuk-Jin Yoon<author:sep>Liang-Chieh Chen;http://arxiv.org/pdf/2406.02541v3;cs.CV;Project page at https://video-3dgs-project.github.io/;gaussian splatting<tag:sep>nerf
2406.02533v1;http://arxiv.org/abs/2406.02533v1;2024-06-04;SatSplatYOLO: 3D Gaussian Splatting-based Virtual Object Detection  Ensembles for Satellite Feature Recognition;"On-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possibly unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. In this article, we present an approach for
mapping geometries and high-confidence detection of components of unknown,
non-cooperative satellites on orbit. We implement accelerated 3D Gaussian
splatting to learn a 3D representation of the satellite, render virtual views
of the target, and ensemble the YOLOv5 object detector over the virtual views,
resulting in reliable, accurate, and precise satellite component detections.
The full pipeline capable of running on-board and stand to enable downstream
machine intelligence tasks necessary for autonomous guidance, navigation, and
control tasks.";Van Minh Nguyen<author:sep>Emma Sandidge<author:sep>Trupti Mahendrakar<author:sep>Ryan T. White;http://arxiv.org/pdf/2406.02533v1;cs.CV;;gaussian splatting
2406.02407v1;http://arxiv.org/abs/2406.02407v1;2024-06-04;WE-GS: An In-the-wild Efficient 3D Gaussian Representation for  Unconstrained Photo Collections;"Novel View Synthesis (NVS) from unconstrained photo collections is
challenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) has
shown promise for photorealistic and real-time NVS of static scenes. Building
on 3DGS, we propose an efficient point-based differentiable rendering framework
for scene reconstruction from photo collections. Our key innovation is a
residual-based spherical harmonic coefficients transfer module that adapts 3DGS
to varying lighting conditions and photometric post-processing. This
lightweight module can be pre-computed and ensures efficient gradient
propagation from rendered images to 3D Gaussian attributes. Additionally, we
observe that the appearance encoder and the transient mask predictor, the two
most critical parts of NVS from unconstrained photo collections, can be
mutually beneficial. We introduce a plug-and-play lightweight spatial attention
module to simultaneously predict transient occluders and latent appearance
representation for each image. After training and preprocessing, our method
aligns with the standard 3DGS format and rendering pipeline, facilitating
seamlessly integration into various 3DGS applications. Extensive experiments on
diverse datasets show our approach outperforms existing approaches on the
rendering quality of novel view and appearance synthesis with high converge and
rendering speed.";Yuze Wang<author:sep>Junyi Wang<author:sep>Yue Qi;http://arxiv.org/pdf/2406.02407v1;cs.CV;"Our project page is available at
  https://yuzewang1998.github.io/we-gs.github.io/";gaussian splatting
2406.02370v2;http://arxiv.org/abs/2406.02370v2;2024-06-04;Query-based Semantic Gaussian Field for Scene Representation in  Reinforcement Learning;"Latent scene representation plays a significant role in training
reinforcement learning (RL) agents. To obtain good latent vectors describing
the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF
pipeline into scene representation learning. However, these NeRF-related
methods struggle to perceive 3D structural information due to the inefficient
dense sampling in volumetric rendering. Moreover, they lack fine-grained
semantic information included in their scene representation vectors because
they evenly consider free and occupied spaces. Both of them can destroy the
performance of downstream RL tasks. To address the above challenges, we propose
a novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to
learn 3D scene representation for the first time. In brief, we present the
Query-based Generalizable 3DGS to bridge the 3DGS technique and scene
representations with more geometrical awareness than those in NeRFs. Moreover,
we present the Hierarchical Semantics Encoding to ground the fine-grained
semantic features to 3D Gaussians and further distilled to the scene
representation vectors. We conduct extensive experiments on two RL platforms
including Maniskill2 and Robomimic across 10 different tasks. The results show
that our method outperforms the other 5 baselines by a large margin. We achieve
the best success rates on 8 tasks and the second-best on the other two tasks.";Jiaxu Wang<author:sep>Ziyi Zhang<author:sep>Qiang Zhang<author:sep>Jia Li<author:sep>Jingkai Sun<author:sep>Mingyuan Sun<author:sep>Junhao He<author:sep>Renjing Xu;http://arxiv.org/pdf/2406.02370v2;cs.RO;;gaussian splatting<tag:sep>nerf
2406.01042v1;http://arxiv.org/abs/2406.01042v1;2024-06-03;Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using  Gaussian Splatting;"Gaussian Splatting (GS) has significantly elevated scene reconstruction
efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance
Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS
methods, whether based on GS or NeRF, primarily rely on camera parameters
provided by COLMAP and even utilize sparse point clouds generated by COLMAP for
initialization, which lack accuracy as well are time-consuming. This sometimes
results in poor dynamic scene representation, especially in scenes with large
object movements, or extreme camera conditions e.g. small translations combined
with large rotations. Some studies simultaneously optimize the estimation of
camera parameters and scenes, supervised by additional information like depth,
optical flow, etc. obtained from off-the-shelf models. Using this unverified
information as ground truth can reduce robustness and accuracy, which does
frequently occur for long monocular videos (with e.g. > hundreds of frames). We
propose a novel approach that learns a high-fidelity 4D GS scene representation
with self-calibration of camera parameters. It includes the extraction of 2D
point features that robustly represent 3D structure, and their use for
subsequent joint optimization of camera parameters and 3D structure towards
overall 4D scene optimization. We demonstrate the accuracy and time efficiency
of our method through extensive quantitative and qualitative experimental
results on several standard benchmarks. The results show significant
improvements over state-of-the-art methods for 4D novel view synthesis. The
source code will be released soon at https://github.com/fangli333/SC-4DGS.";Fang Li<author:sep>Hao Zhang<author:sep>Narendra Ahuja;http://arxiv.org/pdf/2406.01042v1;cs.CV;GitHub Page: https://github.com/fangli333/SC-4DGS;gaussian splatting<tag:sep>nerf
2406.01593v1;http://arxiv.org/abs/2406.01593v1;2024-06-03;Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed  Gaussian Splatting;"3D reconstruction and simulation, while interrelated, have distinct
objectives: reconstruction demands a flexible 3D representation adaptable to
diverse scenes, whereas simulation requires a structured representation to
model motion principles effectively. This paper introduces the Mesh-adsorbed
Gaussian Splatting (MaGS) method to resolve such a dilemma. MaGS constrains 3D
Gaussians to hover on the mesh surface, creating a mutual-adsorbed
mesh-Gaussian 3D representation that combines the rendering flexibility of 3D
Gaussians with the spatial coherence of meshes. Leveraging this representation,
we introduce a learnable Relative Deformation Field (RDF) to model the relative
displacement between the mesh and 3D Gaussians, extending traditional
mesh-driven deformation paradigms that only rely on ARAP prior, thus capturing
the motion of each 3D Gaussian more precisely. By joint optimizing meshes, 3D
Gaussians, and RDF, MaGS achieves both high rendering accuracy and realistic
deformation. Extensive experiments on the D-NeRF and NeRF-DS datasets
demonstrate that MaGS can generate competitive results in both reconstruction
and simulation.";Shaojie Ma<author:sep>Yawei Luo<author:sep>Yi Yang;http://arxiv.org/pdf/2406.01593v1;cs.CV;Project Page: see https://wcwac.github.io/MaGS-page/;gaussian splatting<tag:sep>nerf
2406.01467v1;http://arxiv.org/abs/2406.01467v1;2024-06-03;RaDe-GS: Rasterizing Depth in Gaussian Splatting;"Gaussian Splatting (GS) has proven to be highly effective in novel view
synthesis, achieving high-quality and real-time rendering. However, its
potential for reconstructing detailed 3D shapes has not been fully explored.
Existing methods often suffer from limited shape accuracy due to the discrete
and unstructured nature of Gaussian splats, which complicates the shape
extraction. While recent techniques like 2D GS have attempted to improve shape
reconstruction, they often reformulate the Gaussian primitives in ways that
reduce both rendering quality and computational efficiency. To address these
problems, our work introduces a rasterized approach to render the depth maps
and surface normal maps of general 3D Gaussian splats. Our method not only
significantly enhances shape reconstruction accuracy but also maintains the
computational efficiency intrinsic to Gaussian Splatting. Our approach achieves
a Chamfer distance error comparable to NeuraLangelo on the DTU dataset and
similar training and rendering time as traditional Gaussian Splatting on the
Tanks & Temples dataset. Our method is a significant advancement in Gaussian
Splatting and can be directly integrated into existing Gaussian Splatting-based
methods.";Baowen Zhang<author:sep>Chuan Fang<author:sep>Rakesh Shrestha<author:sep>Yixun Liang<author:sep>Xiaoxiao Long<author:sep>Ping Tan;http://arxiv.org/pdf/2406.01467v1;cs.GR;;gaussian splatting
2406.01579v1;http://arxiv.org/abs/2406.01579v1;2024-06-03;Tetrahedron Splatting for 3D Generation;"3D representation is essential to the significant advance of 3D generation
with 2D diffusion priors. As a flexible representation, NeRF has been first
adopted for 3D representation. With density-based volumetric rendering, it
however suffers both intensive computational overhead and inaccurate mesh
extraction. Using a signed distance field and Marching Tetrahedra, DMTet allows
for precise mesh extraction and real-time rendering but is limited in handling
large topological changes in meshes, leading to optimization challenges.
Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and
rendering efficiency while falling short in mesh extraction. In this work, we
introduce a novel 3D representation, Tetrahedron Splatting (TeT-Splatting),
that supports easy convergence during optimization, precise mesh extraction,
and real-time rendering simultaneously. This is achieved by integrating
surface-based volumetric rendering within a structured tetrahedral grid while
preserving the desired ability of precise mesh extraction, and a tile-based
differentiable tetrahedron rasterizer. Furthermore, we incorporate eikonal and
normal consistency regularization terms for the signed distance field to
improve generation quality and stability. Critically, our representation can be
trained without mesh extraction, making the optimization process easier to
converge. Our TeT-Splatting can be readily integrated in existing 3D generation
pipelines, along with polygonal mesh for texture optimization. Extensive
experiments show that our TeT-Splatting strikes a superior tradeoff among
convergence speed, render efficiency, and mesh quality as compared to previous
alternatives under varying 3D generation settings.";Chun Gu<author:sep>Zeyu Yang<author:sep>Zijie Pan<author:sep>Xiatian Zhu<author:sep>Li Zhang;http://arxiv.org/pdf/2406.01579v1;cs.CV;Code: https://github.com/fudan-zvg/tet-splatting;gaussian splatting<tag:sep>nerf
2406.01476v1;http://arxiv.org/abs/2406.01476v1;2024-06-03;DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with  Video Diffusion Priors;"Dynamic 3D interaction has witnessed great interest in recent works, while
creating such 4D content remains challenging. One solution is to animate 3D
scenes with physics-based simulation, and the other is to learn the deformation
of static 3D objects with the distillation of video generative models. The
former one requires assigning precise physical properties to the target object,
otherwise the simulated results would become unnatural. The latter tends to
formulate the video with minor motions and discontinuous frames, due to the
absence of physical constraints in deformation learning. We think that video
generative models are trained with real-world captured data, capable of judging
physical phenomenon in simulation environments. To this end, we propose
DreamPhysics in this work, which estimates physical properties of 3D Gaussian
Splatting with video diffusion priors. DreamPhysics supports both image- and
text-conditioned guidance, optimizing physical parameters via score
distillation sampling with frame interpolation and log gradient. Based on a
material point method simulator with proper physical parameters, our method can
generate 4D content with realistic motions. Experimental results demonstrate
that, by distilling the prior knowledge of video diffusion models, inaccurate
physical properties can be gradually refined for high-quality simulation. Codes
are released at: https://github.com/tyhuang0428/DreamPhysics.";Tianyu Huang<author:sep>Yihan Zeng<author:sep>Hui Li<author:sep>Wangmeng Zuo<author:sep>Rynson W. H. Lau;http://arxiv.org/pdf/2406.01476v1;cs.CV;"Technical report. Codes are released at:
  https://github.com/tyhuang0428/DreamPhysics";
2406.00598v1;http://arxiv.org/abs/2406.00598v1;2024-06-02;Efficient Neural Light Fields (ENeLF) for Mobile Devices;"Novel view synthesis (NVS) is a challenge in computer vision and graphics,
focusing on generating realistic images of a scene from unobserved camera
poses, given a limited set of authentic input images. Neural radiance fields
(NeRF) achieved impressive results in rendering quality by utilizing volumetric
rendering. However, NeRF and its variants are unsuitable for mobile devices due
to the high computational cost of volumetric rendering. Emerging research in
neural light fields (NeLF) eliminates the need for volumetric rendering by
directly learning a mapping from ray representation to pixel color. NeLF has
demonstrated its capability to achieve results similar to NeRF but requires a
more extensive, computationally intensive network that is not mobile-friendly.
Unlike existing works, this research builds upon the novel network architecture
introduced by MobileR2L and aggressively applies a compression technique
(channel-wise structure pruning) to produce a model that runs efficiently on
mobile devices with lower latency and smaller sizes, with a slight decrease in
performance.";Austin Peng;http://arxiv.org/pdf/2406.00598v1;cs.CV;;nerf
2406.00798v1;http://arxiv.org/abs/2406.00798v1;2024-06-02;PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency;"Neural Radiance Fields (NeRF) have shown remarkable performance in learning
3D scenes. However, NeRF exhibits vulnerability when confronted with
distractors in the training images -- unexpected objects are present only
within specific views, such as moving entities like pedestrians or birds.
Excluding distractors during dataset construction is a straightforward
solution, but without prior knowledge of their types and quantities, it becomes
prohibitively expensive. In this paper, we propose PruNeRF, a segment-centric
dataset pruning framework via 3D spatial consistency, that effectively
identifies and prunes the distractors. We first examine existing metrics for
measuring pixel-wise distraction and introduce Influence Functions for more
accurate measurements. Then, we assess 3D spatial consistency using a
depth-based reprojection technique to obtain 3D-aware distraction. Furthermore,
we incorporate segmentation for pixel-to-segment refinement, enabling more
precise identification. Our experiments on benchmark datasets demonstrate that
PruNeRF consistently outperforms state-of-the-art methods in robustness against
distractors.";Yeonsung Jung<author:sep>Heecheol Yun<author:sep>Joonhyung Park<author:sep>Jin-Hwa Kim<author:sep>Eunho Yang;http://arxiv.org/pdf/2406.00798v1;cs.CV;;nerf
2406.00637v1;http://arxiv.org/abs/2406.00637v1;2024-06-02;Representing Animatable Avatar via Factorized Neural Fields;"For reconstructing high-fidelity human 3D models from monocular videos, it is
crucial to maintain consistent large-scale body shapes along with finely
matched subtle wrinkles. This paper explores the observation that the per-frame
rendering results can be factorized into a pose-independent component and a
corresponding pose-dependent equivalent to facilitate frame consistency. Pose
adaptive textures can be further improved by restricting frequency bands of
these two components. In detail, pose-independent outputs are expected to be
low-frequency, while highfrequency information is linked to pose-dependent
factors. We achieve a coherent preservation of both coarse body contours across
the entire input video and finegrained texture features that are time variant
with a dual-branch network with distinct frequency components. The first branch
takes coordinates in canonical space as input, while the second branch
additionally considers features outputted by the first branch and pose
information of each frame. Our network integrates the information predicted by
both branches and utilizes volume rendering to generate photo-realistic 3D
human images. Through experiments, we demonstrate that our network surpasses
the neural radiance fields (NeRF) based state-of-the-art methods in preserving
high-frequency details and ensuring consistent body contours.";Chunjin Song<author:sep>Zhijie Wu<author:sep>Bastian Wandt<author:sep>Leonid Sigal<author:sep>Helge Rhodin;http://arxiv.org/pdf/2406.00637v1;cs.CV;;nerf
2406.00609v2;http://arxiv.org/abs/2406.00609v2;2024-06-02;SuperGaussian: Repurposing Video Models for 3D Super Resolution;"We present a simple, modular, and generic method that upsamples coarse 3D
models by adding geometric and appearance details. While generative 3D models
now exist, they do not yet match the quality of their counterparts in image and
video domains. We demonstrate that it is possible to directly repurpose
existing (pretrained) video models for 3D super-resolution and thus sidestep
the problem of the shortage of large repositories of high-quality 3D training
models. We describe how to repurpose video upsampling models, which are not 3D
consistent, and combine them with 3D consolidation to produce 3D-consistent
results. As output, we produce high quality Gaussian Splat models, which are
object centric and effective. Our method is category agnostic and can be easily
incorporated into existing 3D workflows. We evaluate our proposed SuperGaussian
on a variety of 3D inputs, which are diverse both in terms of complexity and
representation (e.g., Gaussian Splats or NeRFs), and demonstrate that our
simple method significantly improves the fidelity of the final 3D models. Check
our project website for details: supergaussian.github.io";Yuan Shen<author:sep>Duygu Ceylan<author:sep>Paul Guerrero<author:sep>Zexiang Xu<author:sep>Niloy J. Mitra<author:sep>Shenlong Wang<author:sep>Anna Frühstück;http://arxiv.org/pdf/2406.00609v2;cs.CV;"Check our project website for details:
  https://supergaussian.github.io";nerf
2406.00434v1;http://arxiv.org/abs/2406.00434v1;2024-06-01;MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular  Videos;"In this paper, we propose MoDGS, a new pipeline to render novel-view images
in dynamic scenes using only casually captured monocular videos. Previous
monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid
movement of input cameras to construct multiview consistency but fail to
reconstruct dynamic scenes on casually captured input videos whose cameras are
static or move slowly. To address this challenging task, MoDGS adopts recent
single-view depth estimation methods to guide the learning of the dynamic
scene. Then, a novel 3D-aware initialization method is proposed to learn a
reasonable deformation field and a new robust depth loss is proposed to guide
the learning of dynamic scene geometry. Comprehensive experiments demonstrate
that MoDGS is able to render high-quality novel view images of dynamic scenes
from just a casually captured monocular video, which outperforms baseline
methods by a significant margin.";Qingming Liu<author:sep>Yuan Liu<author:sep>Jiepeng Wang<author:sep>Xianqiang Lv<author:sep>Peng Wang<author:sep>Wenping Wang<author:sep>Junhui Hou;http://arxiv.org/pdf/2406.00434v1;cs.CV;;gaussian splatting<tag:sep>nerf
2406.00440v1;http://arxiv.org/abs/2406.00440v1;2024-06-01;Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head  Capture;"4D head capture aims to generate dynamic topological meshes and corresponding
texture maps from videos, which is widely utilized in movies and games for its
ability to simulate facial muscle movements and recover dynamic textures in
pore-squeezing. The industry often adopts the method involving multi-view
stereo and non-rigid alignment. However, this approach is prone to errors and
heavily reliant on time-consuming manual processing by artists. To simplify
this process, we propose Topo4D, a novel framework for automatic geometry and
texture generation, which optimizes densely aligned 4D heads and 8K texture
maps directly from calibrated multi-view time-series images. Specifically, we
first represent the time-series faces as a set of dynamic 3D Gaussians with
fixed topology in which the Gaussian centers are bound to the mesh vertices.
Afterward, we perform alternative geometry and texture optimization
frame-by-frame for high-quality geometry and texture learning while maintaining
temporal topology stability. Finally, we can extract dynamic facial meshes in
regular wiring arrangement and high-fidelity textures with pore-level details
from the learned Gaussians. Extensive experiments show that our method achieves
superior results than the current SOTA face reconstruction methods both in the
quality of meshes and textures. Project page:
https://xuanchenli.github.io/Topo4D/.";X. Li<author:sep>Y. Cheng<author:sep>X. Ren<author:sep>H. Jia<author:sep>D. Xu<author:sep>W. Zhu<author:sep>Y. Yan;http://arxiv.org/pdf/2406.00440v1;cs.CV;;gaussian splatting
2406.00448v1;http://arxiv.org/abs/2406.00448v1;2024-06-01;Bilateral Guided Radiance Field Processing;"Neural Radiance Fields (NeRF) achieves unprecedented performance in
synthesizing novel view synthesis, utilizing multi-view consistency. When
capturing multiple inputs, image signal processing (ISP) in modern cameras will
independently enhance them, including exposure adjustment, color correction,
local tone mapping, etc. While these processings greatly improve image quality,
they often break the multi-view consistency assumption, leading to ""floaters""
in the reconstructed radiance fields. To address this concern without
compromising visual aesthetics, we aim to first disentangle the enhancement by
ISP at the NeRF training stage and re-apply user-desired enhancements to the
reconstructed radiance fields at the finishing stage. Furthermore, to make the
re-applied enhancements consistent between novel views, we need to perform
imaging signal processing in 3D space (i.e. ""3D ISP""). For this goal, we adopt
the bilateral grid, a locally-affine model, as a generalized representation of
ISP processing. Specifically, we optimize per-view 3D bilateral grids with
radiance fields to approximate the effects of camera pipelines for each input
view. To achieve user-adjustable 3D finishing, we propose to learn a low-rank
4D bilateral grid from a given single view edit, lifting photo enhancements to
the whole 3D scene. We demonstrate our approach can boost the visual quality of
novel view synthesis by effectively removing floaters and performing
enhancements from user retouching. The source code and our data are available
at: https://bilarfpro.github.io.";Yuehao Wang<author:sep>Chaoyi Wang<author:sep>Bingchen Gong<author:sep>Tianfan Xue;http://arxiv.org/pdf/2406.00448v1;cs.CV;SIGGRAPH (ACM TOG), 2024. Project page: https://bilarfpro.github.io;nerf
2405.20674v1;http://arxiv.org/abs/2405.20674v1;2024-05-31;4Diffusion: Multi-view Video Diffusion Model for 4D Generation;"Current 4D generation methods have achieved noteworthy efficacy with the aid
of advanced diffusion generative models. However, these methods lack multi-view
spatial-temporal modeling and encounter challenges in integrating diverse prior
knowledge from multiple diffusion models, resulting in inconsistent temporal
appearance and flickers. In this paper, we propose a novel 4D generation
pipeline, namely 4Diffusion aimed at generating spatial-temporally consistent
4D content from a monocular video. We first design a unified diffusion model
tailored for multi-view video generation by incorporating a learnable motion
module into a frozen 3D-aware diffusion model to capture multi-view
spatial-temporal correlations. After training on a curated dataset, our
diffusion model acquires reasonable temporal consistency and inherently
preserves the generalizability and spatial consistency of the 3D-aware
diffusion model. Subsequently, we propose 4D-aware Score Distillation Sampling
loss, which is based on our multi-view video diffusion model, to optimize 4D
representation parameterized by dynamic NeRF. This aims to eliminate
discrepancies arising from multiple diffusion models, allowing for generating
spatial-temporally consistent 4D content. Moreover, we devise an anchor loss to
enhance the appearance details and facilitate the learning of dynamic NeRF.
Extensive qualitative and quantitative experiments demonstrate that our method
achieves superior performance compared to previous methods.";Haiyu Zhang<author:sep>Xinyuan Chen<author:sep>Yaohui Wang<author:sep>Xihui Liu<author:sep>Yunhong Wang<author:sep>Yu Qiao;http://arxiv.org/pdf/2405.20674v1;cs.CV;Project Page: https://aejion.github.io/4diffusion/;nerf
2405.20721v1;http://arxiv.org/abs/2405.20721v1;2024-05-31;ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model;"Recently, 3D Gaussian Splatting (3DGS) has become a promising framework for
novel view synthesis, offering fast rendering speeds and high fidelity.
However, the large number of Gaussians and their associated attributes require
effective compression techniques. Existing methods primarily compress neural
Gaussians individually and independently, i.e., coding all the neural Gaussians
at the same time, with little design for their interactions and spatial
dependence. Inspired by the effectiveness of the context model in image
compression, we propose the first autoregressive model at the anchor level for
3DGS compression in this work. We divide anchors into different levels and the
anchors that are not coded yet can be predicted based on the already coded ones
in all the coarser levels, leading to more accurate modeling and higher coding
efficiency. To further improve the efficiency of entropy coding, e.g., to code
the coarsest level with no already coded anchors, we propose to introduce a
low-dimensional quantized feature as the hyperprior for each anchor, which can
be effectively compressed. Our work pioneers the context model in the anchor
level for 3DGS representation, yielding an impressive size reduction of over
100 times compared to vanilla 3DGS and 15 times compared to the most recent
state-of-the-art work Scaffold-GS, while achieving comparable or even higher
rendering quality.";Yufei Wang<author:sep>Zhihao Li<author:sep>Lanqing Guo<author:sep>Wenhan Yang<author:sep>Alex C. Kot<author:sep>Bihan Wen;http://arxiv.org/pdf/2405.20721v1;cs.CV;;gaussian splatting
2405.20693v1;http://arxiv.org/abs/2405.20693v1;2024-05-31;R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic  Reconstruction;"3D Gaussian splatting (3DGS) has shown promising results in image rendering
and surface reconstruction. However, its potential in volumetric reconstruction
tasks, such as X-ray computed tomography, remains under-explored. This paper
introduces R2-Gaussian, the first 3DGS-based framework for sparse-view
tomographic reconstruction. By carefully deriving X-ray rasterization
functions, we discover a previously unknown integration bias in the standard
3DGS formulation, which hampers accurate volume retrieval. To address this
issue, we propose a novel rectification technique via refactoring the
projection from 3D to 2D Gaussians. Our new method presents three key
innovations: (1) introducing tailored Gaussian kernels, (2) extending
rasterization to X-ray imaging, and (3) developing a CUDA-based differentiable
voxelizer. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches by 0.93 dB in PSNR and 0.014 in SSIM. Crucially, it
delivers high-quality results in 3 minutes, which is 12x faster than NeRF-based
methods and on par with traditional algorithms. The superior performance and
rapid convergence of our method highlight its practical value.";Ruyi Zha<author:sep>Tao Jun Lin<author:sep>Yuanhao Cai<author:sep>Jiwen Cao<author:sep>Yanhao Zhang<author:sep>Hongdong Li;http://arxiv.org/pdf/2405.20693v1;eess.IV;;gaussian splatting<tag:sep>nerf
2405.20323v1;http://arxiv.org/abs/2405.20323v1;2024-05-30;$\textit{S}^3$Gaussian: Self-Supervised Street Gaussians for Autonomous  Driving;"Photorealistic 3D reconstruction of street scenes is a critical technique for
developing real-world simulators for autonomous driving. Despite the efficacy
of Neural Radiance Fields (NeRF) for driving scenes, 3D Gaussian Splatting
(3DGS) emerges as a promising direction due to its faster speed and more
explicit representation. However, most existing street 3DGS methods require
tracked 3D vehicle bounding boxes to decompose the static and dynamic elements
for effective reconstruction, limiting their applications for in-the-wild
scenarios. To facilitate efficient 3D scene reconstruction without costly
annotations, we propose a self-supervised street Gaussian
($\textit{S}^3$Gaussian) method to decompose dynamic and static elements from
4D consistency. We represent each scene with 3D Gaussians to preserve the
explicitness and further accompany them with a spatial-temporal field network
to compactly model the 4D dynamics. We conduct extensive experiments on the
challenging Waymo-Open dataset to evaluate the effectiveness of our method. Our
$\textit{S}^3$Gaussian demonstrates the ability to decompose static and dynamic
scenes and achieves the best performance without using 3D annotations. Code is
available at: https://github.com/nnanhuang/S3Gaussian/.";Nan Huang<author:sep>Xiaobao Wei<author:sep>Wenzhao Zheng<author:sep>Pengju An<author:sep>Ming Lu<author:sep>Wei Zhan<author:sep>Masayoshi Tomizuka<author:sep>Kurt Keutzer<author:sep>Shanghang Zhang;http://arxiv.org/pdf/2405.20323v1;cs.CV;Code is available at: https://github.com/nnanhuang/S3Gaussian/;gaussian splatting<tag:sep>nerf
2405.20104v1;http://arxiv.org/abs/2405.20104v1;2024-05-30;Object-centric Reconstruction and Tracking of Dynamic Unknown Objects  using 3D Gaussian Splatting;"Generalizable perception is one of the pillars of high-level autonomy in
space robotics. Estimating the structure and motion of unknown objects in
dynamic environments is fundamental for such autonomous systems. Traditionally,
the solutions have relied on prior knowledge of target objects, multiple
disparate representations, or low-fidelity outputs unsuitable for robotic
operations. This work proposes a novel approach to incrementally reconstruct
and track a dynamic unknown object using a unified representation -- a set of
3D Gaussian blobs that describe its geometry and appearance. The differentiable
3D Gaussian Splatting framework is adapted to a dynamic object-centric setting.
The input to the pipeline is a sequential set of RGB-D images. 3D
reconstruction and 6-DoF pose tracking tasks are tackled using first-order
gradient-based optimization. The formulation is simple, requires no
pre-training, assumes no prior knowledge of the object or its motion, and is
suitable for online applications. The proposed approach is validated on a
dataset of 10 unknown spacecraft of diverse geometry and texture under
arbitrary relative motion. The experiments demonstrate successful 3D
reconstruction and accurate 6-DoF tracking of the target object in proximity
operations over a short to medium duration. The causes of tracking drift are
discussed and potential solutions are outlined.";Kuldeep R Barad<author:sep>Antoine Richard<author:sep>Jan Dentler<author:sep>Miguel Olivares-Mendez<author:sep>Carol Martinez;http://arxiv.org/pdf/2405.20104v1;cs.RO;Accepted at IEEE International Conference on Space Robotics 2024;gaussian splatting
2405.20283v1;http://arxiv.org/abs/2405.20283v1;2024-05-30;TetSphere Splatting: Representing High-Quality Geometry with Lagrangian  Volumetric Meshes;"We present TetSphere splatting, an explicit, Lagrangian representation for
reconstructing 3D shapes with high-quality geometry. In contrast to
conventional object reconstruction methods which predominantly use Eulerian
representations, including both neural implicit (e.g., NeRF, NeuS) and explicit
representations (e.g., DMTet), and often struggle with high computational
demands and suboptimal mesh quality, TetSphere splatting utilizes an underused
but highly effective geometric primitive -- tetrahedral meshes. This approach
directly yields superior mesh quality without relying on neural networks or
post-processing. It deforms multiple initial tetrahedral spheres to accurately
reconstruct the 3D shape through a combination of differentiable rendering and
geometric energy optimization, resulting in significant computational
efficiency. Serving as a robust and versatile geometry representation,
Tet-Sphere splatting seamlessly integrates into diverse applications, including
single-view 3D reconstruction, image-/text-to-3D content generation.
Experimental results demonstrate that TetSphere splatting outperforms existing
representations, delivering faster optimization speed, enhanced mesh quality,
and reliable preservation of thin structures.";Minghao Guo<author:sep>Bohan Wang<author:sep>Kaiming He<author:sep>Wojciech Matusik;http://arxiv.org/pdf/2405.20283v1;cs.CV;;nerf
2405.20310v3;http://arxiv.org/abs/2405.20310v3;2024-05-30;A Pixel Is Worth More Than One 3D Gaussians in Single-View 3D  Reconstruction;"Learning 3D scene representation from a single-view image is a long-standing
fundamental problem in computer vision, with the inherent ambiguity in
predicting contents unseen from the input view. Built on the recently proposed
3D Gaussian Splatting (3DGS), the Splatter Image method has made promising
progress on fast single-image novel view synthesis via learning a single 3D
Gaussian for each pixel based on the U-Net feature map of an input image.
However, it has limited expressive power to represent occluded components that
are not observable in the input view. To address this problem, this paper
presents a Hierarchical Splatter Image method in which a pixel is worth more
than one 3D Gaussians. Specifically, each pixel is represented by a parent 3D
Gaussian and a small number of child 3D Gaussians. Parent 3D Gaussians are
learned as done in the vanilla Splatter Image. Child 3D Gaussians are learned
via a lightweight Multi-Layer Perceptron (MLP) which takes as input the
projected image features of a parent 3D Gaussian and the embedding of a target
camera view. Both parent and child 3D Gaussians are learned end-to-end in a
stage-wise way. The joint condition of input image features from eyes of the
parent Gaussians and the target camera position facilitates learning to
allocate child Gaussians to ``see the unseen'', recovering the occluded details
that are often missed by parent Gaussians.
  In experiments, the proposed method is tested on the ShapeNet-SRN and CO3D
datasets with state-of-the-art performance obtained, especially showing
promising capabilities of reconstructing occluded contents in the input view.";Jianghao Shen<author:sep>Nan Xue<author:sep>Tianfu Wu;http://arxiv.org/pdf/2405.20310v3;cs.CV;preprint, under review;gaussian splatting
2405.19678v1;http://arxiv.org/abs/2405.19678v1;2024-05-30;View-Consistent Hierarchical 3D SegmentationUsing Ultrametric Feature  Fields;"Large-scale vision foundation models such as Segment Anything (SAM)
demonstrate impressive performance in zero-shot image segmentation at multiple
levels of granularity. However, these zero-shot predictions are rarely
3D-consistent. As the camera viewpoint changes in a scene, so do the
segmentation predictions, as well as the characterizations of ``coarse"" or
``fine"" granularity. In this work, we address the challenging task of lifting
multi-granular and view-inconsistent image segmentations into a hierarchical
and 3D-consistent representation. We learn a novel feature field within a
Neural Radiance Field (NeRF) representing a 3D scene, whose segmentation
structure can be revealed at different scales by simply using different
thresholds on feature distance. Our key idea is to learn an ultrametric feature
space, which unlike a Euclidean space, exhibits transitivity in distance-based
grouping, naturally leading to a hierarchical clustering. Put together, our
method takes view-inconsistent multi-granularity 2D segmentations as input and
produces a hierarchy of 3D-consistent segmentations as output. We evaluate our
method and several baselines on synthetic datasets with multi-view images and
multi-granular segmentation, showcasing improved accuracy and
viewpoint-consistency. We additionally provide qualitative examples of our
model's 3D hierarchical segmentations in real world scenes.\footnote{The code
and dataset are available at:";Haodi He<author:sep>Colton Stearns<author:sep>Adam W. Harley<author:sep>Leonidas J. Guibas;http://arxiv.org/pdf/2405.19678v1;cs.CV;;nerf
2405.19957v2;http://arxiv.org/abs/2405.19957v2;2024-05-30;PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting;"As text-conditioned diffusion models (DMs) achieve breakthroughs in image,
video, and 3D generation, the research community's focus has shifted to the
more challenging task of text-to-4D synthesis, which introduces a temporal
dimension to generate dynamic 3D objects. In this context, we identify Score
Distillation Sampling (SDS), a widely used technique for text-to-3D synthesis,
as a significant hindrance to text-to-4D performance due to its Janus-faced and
texture-unrealistic problems coupled with high computational costs. In this
paper, we propose \textbf{P}ixel-\textbf{L}evel \textbf{A}lignments for
Text-to-\textbf{4D} Gaussian Splatting (\textbf{PLA4D}), a novel method that
utilizes text-to-video frames as explicit pixel alignment targets to generate
static 3D objects and inject motion into them. Specifically, we introduce Focal
Alignment to calibrate camera poses for rendering and GS-Mesh Contrastive
Learning to distill geometry priors from rendered image contrasts at the pixel
level. Additionally, we develop Motion Alignment using a deformation network to
drive changes in Gaussians and implement Reference Refinement for smooth 4D
object surfaces. These techniques enable 4D Gaussian Splatting to align
geometry, texture, and motion with generated videos at the pixel level.
Compared to previous methods, PLA4D produces synthesized outputs with better
texture details in less time and effectively mitigates the Janus-faced problem.
PLA4D is fully implemented using open-source models, offering an accessible,
user-friendly, and promising direction for 4D digital content creation. Our
project page: https://github.com/MiaoQiaowei/PLA4D.github.io.";Qiaowei Miao<author:sep>Yawei Luo<author:sep>Yi Yang;http://arxiv.org/pdf/2405.19957v2;cs.CV;;gaussian splatting
2405.19671v1;http://arxiv.org/abs/2405.19671v1;2024-05-30;GaussianRoom: Improving 3D Gaussian Splatting with SDF Guidance and  Monocular Cues for Indoor Scene Reconstruction;"Recently, 3D Gaussian Splatting(3DGS) has revolutionized neural rendering
with its high-quality rendering and real-time speed. However, when it comes to
indoor scenes with a significant number of textureless areas, 3DGS yields
incomplete and noisy reconstruction results due to the poor initialization of
the point cloud and under-constrained optimization. Inspired by the continuity
of signed distance field (SDF), which naturally has advantages in modeling
surfaces, we present a unified optimizing framework integrating neural SDF with
3DGS. This framework incorporates a learnable neural SDF field to guide the
densification and pruning of Gaussians, enabling Gaussians to accurately model
scenes even with poor initialized point clouds. At the same time, the geometry
represented by Gaussians improves the efficiency of the SDF field by piloting
its point sampling. Additionally, we regularize the optimization with normal
and edge priors to eliminate geometry ambiguity in textureless areas and
improve the details. Extensive experiments in ScanNet and ScanNet++ show that
our method achieves state-of-the-art performance in both surface reconstruction
and novel view synthesis.";Haodong Xiang<author:sep>Xinghui Li<author:sep>Xiansong Lai<author:sep>Wanting Zhang<author:sep>Zhichao Liao<author:sep>Kai Cheng<author:sep>Xueping Liu;http://arxiv.org/pdf/2405.19671v1;cs.CV;;gaussian splatting
2405.19657v1;http://arxiv.org/abs/2405.19657v1;2024-05-30;Uncertainty-guided Optimal Transport in Depth Supervised Sparse-View 3D  Gaussian;"3D Gaussian splatting has demonstrated impressive performance in real-time
novel view synthesis. However, achieving successful reconstruction from RGB
images generally requires multiple input views captured under static
conditions. To address the challenge of sparse input views, previous approaches
have incorporated depth supervision into the training of 3D Gaussians to
mitigate overfitting, using dense predictions from pretrained depth networks as
pseudo-ground truth. Nevertheless, depth predictions from monocular depth
estimation models inherently exhibit significant uncertainty in specific areas.
Relying solely on pixel-wise L2 loss may inadvertently incorporate detrimental
noise from these uncertain areas. In this work, we introduce a novel method to
supervise the depth distribution of 3D Gaussians, utilizing depth priors with
integrated uncertainty estimates. To address these localized errors in depth
predictions, we integrate a patch-wise optimal transport strategy to complement
traditional L2 loss in depth supervision. Extensive experiments conducted on
the LLFF, DTU, and Blender datasets demonstrate that our approach, UGOT,
achieves superior novel view synthesis and consistently outperforms
state-of-the-art methods.";Wei Sun<author:sep>Qi Zhang<author:sep>Yanzhao Zhou<author:sep>Qixiang Ye<author:sep>Jianbin Jiao<author:sep>Yuan Li;http://arxiv.org/pdf/2405.19657v1;cs.CV;10pages;gaussian splatting
2405.20078v2;http://arxiv.org/abs/2405.20078v2;2024-05-30;NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics  Evaluation;"Neural radiance fields (NeRF) are a groundbreaking computer vision technology
that enables the generation of high-quality, immersive visual content from
multiple viewpoints. This capability holds significant advantages for
applications such as virtual/augmented reality, 3D modelling and content
creation for the film and entertainment industry. However, the evaluation of
NeRF methods poses several challenges, including a lack of comprehensive
datasets, reliable assessment methodologies, and objective quality metrics.
This paper addresses the problem of NeRF quality assessment thoroughly, by
conducting a rigorous subjective quality assessment test that considers several
scene classes and recently proposed NeRF view synthesis methods. Additionally,
the performance of a wide range of state-of-the-art conventional and
learning-based full-reference 2D image and video quality assessment metrics is
evaluated against the subjective scores of the subjective study. The
experimental results are analyzed in depth, providing a comparative evaluation
of several NeRF methods and objective quality metrics, across different classes
of visual scenes, including real and synthetic content for front-face and
360-degree camera trajectories.";Pedro Martin<author:sep>Antonio Rodrigues<author:sep>Joao Ascenso<author:sep>Maria Paula Queluz;http://arxiv.org/pdf/2405.20078v2;cs.MM;;nerf
2405.19712v1;http://arxiv.org/abs/2405.19712v1;2024-05-30;HINT: Learning Complete Human Neural Representations from Limited  Viewpoints;"No augmented application is possible without animated humanoid avatars. At
the same time, generating human replicas from real-world monocular hand-held or
robotic sensor setups is challenging due to the limited availability of views.
Previous work showed the feasibility of virtual avatars but required the
presence of 360 degree views of the targeted subject. To address this issue, we
propose HINT, a NeRF-based algorithm able to learn a detailed and complete
human model from limited viewing angles. We achieve this by introducing a
symmetry prior, regularization constraints, and training cues from large human
datasets. In particular, we introduce a sagittal plane symmetry prior to the
appearance of the human, directly supervise the density function of the human
model using explicit 3D body modeling, and leverage a co-learned human
digitization network as additional supervision for the unseen angles. As a
result, our method can reconstruct complete humans even from a few viewing
angles, increasing performance by more than 15% PSNR compared to previous
state-of-the-art algorithms.";Alessandro Sanvito<author:sep>Andrea Ramazzina<author:sep>Stefanie Walz<author:sep>Mario Bijelic<author:sep>Felix Heide;http://arxiv.org/pdf/2405.19712v1;cs.CV;;nerf
2405.19614v1;http://arxiv.org/abs/2405.19614v1;2024-05-30;TAMBRIDGE: Bridging Frame-Centered Tracking and 3D Gaussian Splatting  for Enhanced SLAM;"The limited robustness of 3D Gaussian Splatting (3DGS) to motion blur and
camera noise, along with its poor real-time performance, restricts its
application in robotic SLAM tasks. Upon analysis, the primary causes of these
issues are the density of views with motion blur and the cumulative errors in
dense pose estimation from calculating losses based on noisy original images
and rendering results, which increase the difficulty of 3DGS rendering
convergence. Thus, a cutting-edge 3DGS-based SLAM system is introduced,
leveraging the efficiency and flexibility of 3DGS to achieve real-time
performance while remaining robust against sensor noise, motion blur, and the
challenges posed by long-session SLAM. Central to this approach is the Fusion
Bridge module, which seamlessly integrates tracking-centered ORB Visual
Odometry with mapping-centered online 3DGS. Precise pose initialization is
enabled by this module through joint optimization of re-projection and
rendering loss, as well as strategic view selection, enhancing rendering
convergence in large-scale scenes. Extensive experiments demonstrate
state-of-the-art rendering quality and localization accuracy, positioning this
system as a promising solution for real-world robotics applications that
require stable, near-real-time performance. Our project is available at
https://ZeldaFromHeaven.github.io/TAMBRIDGE/";Peifeng Jiang<author:sep>Hong Liu<author:sep>Xia Li<author:sep>Ti Wang<author:sep>Fabian Zhang<author:sep>Joachim M. Buhmann;http://arxiv.org/pdf/2405.19614v1;cs.RO;;gaussian splatting
2405.19876v1;http://arxiv.org/abs/2405.19876v1;2024-05-30;IReNe: Instant Recoloring in Neural Radiance Fields;"Advances in NERFs have allowed for 3D scene reconstructions and novel view
synthesis. Yet, efficiently editing these representations while retaining
photorealism is an emerging challenge. Recent methods face three primary
limitations: they're slow for interactive use, lack precision at object
boundaries, and struggle to ensure multi-view consistency. We introduce IReNe
to address these limitations, enabling swift, near real-time color editing in
NeRF. Leveraging a pre-trained NeRF model and a single training image with
user-applied color edits, IReNe swiftly adjusts network parameters in seconds.
This adjustment allows the model to generate new scene views, accurately
representing the color changes from the training image while also controlling
object boundaries and view-specific effects. Object boundary control is
achieved by integrating a trainable segmentation module into the model. The
process gains efficiency by retraining only the weights of the last network
layer. We observed that neurons in this layer can be classified into those
responsible for view-dependent appearance and those contributing to diffuse
appearance. We introduce an automated classification approach to identify these
neuron types and exclusively fine-tune the weights of the diffuse neurons. This
further accelerates training and ensures consistent color edits across
different views. A thorough validation on a new dataset, with edited object
colors, shows significant quantitative and qualitative advancements over
competitors, accelerating speeds by 5x to 500x.";Alessio Mazzucchelli<author:sep>Adrian Garcia-Garcia<author:sep>Elena Garces<author:sep>Fernando Rivas-Manzaneque<author:sep>Francesc Moreno-Noguer<author:sep>Adrian Penate-Sanchez;http://arxiv.org/pdf/2405.19876v1;cs.CV;;nerf
2405.19331v1;http://arxiv.org/abs/2405.19331v1;2024-05-29;NPGA: Neural Parametric Gaussian Avatars;"The creation of high-fidelity, digital versions of human heads is an
important stepping stone in the process of further integrating virtual
components into our everyday lives. Constructing such avatars is a challenging
research problem, due to a high demand for photo-realism and real-time
rendering performance. In this work, we propose Neural Parametric Gaussian
Avatars (NPGA), a data-driven approach to create high-fidelity, controllable
avatars from multi-view video recordings. We build our method around 3D
Gaussian Splatting for its highly efficient rendering and to inherit the
topological flexibility of point clouds. In contrast to previous work, we
condition our avatars' dynamics on the rich expression space of neural
parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we
distill the backward deformation field of our underlying NPHM into forward
deformations which are compatible with rasterization-based rendering. All
remaining fine-scale, expression-dependent details are learned from the
multi-view videos. To increase the representational capacity of our avatars, we
augment the canonical Gaussian point cloud using per-primitive latent features
which govern its dynamic behavior. To regularize this increased dynamic
expressivity, we propose Laplacian terms on the latent features and predicted
dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating
that NPGA significantly outperforms the previous state-of-the-art avatars on
the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate
animation capabilities from real-world monocular videos.";Simon Giebenhain<author:sep>Tobias Kirschstein<author:sep>Martin Rünz<author:sep>Lourdes Agapito<author:sep>Matthias Nießner;http://arxiv.org/pdf/2405.19331v1;cs.CV;"Project Page: see https://simongiebenhain.github.io/NPGA/ ; Youtube
  Video: see https://www.youtube.com/watch?v=NGRxAYbIkus";gaussian splatting
2405.18784v1;http://arxiv.org/abs/2405.18784v1;2024-05-29;LP-3DGS: Learning to Prune 3D Gaussian Splatting;"Recently, 3D Gaussian Splatting (3DGS) has become one of the mainstream
methodologies for novel view synthesis (NVS) due to its high quality and fast
rendering speed. However, as a point-based scene representation, 3DGS
potentially generates a large number of Gaussians to fit the scene, leading to
high memory usage. Improvements that have been proposed require either an
empirical and preset pruning ratio or importance score threshold to prune the
point cloud. Such hyperparamter requires multiple rounds of training to
optimize and achieve the maximum pruning ratio, while maintaining the rendering
quality for each scene. In this work, we propose learning-to-prune 3DGS
(LP-3DGS), where a trainable binary mask is applied to the importance score
that can find optimal pruning ratio automatically. Instead of using the
traditional straight-through estimator (STE) method to approximate the binary
mask gradient, we redesign the masking function to leverage the Gumbel-Sigmoid
method, making it differentiable and compatible with the existing training
process of 3DGS. Extensive experiments have shown that LP-3DGS consistently
produces a good balance that is both efficient and high quality.";Zhaoliang Zhang<author:sep>Tianchen Song<author:sep>Yongjae Lee<author:sep>Li Yang<author:sep>Cheng Peng<author:sep>Rama Chellappa<author:sep>Deliang Fan;http://arxiv.org/pdf/2405.18784v1;cs.CV;;gaussian splatting
2405.18715v2;http://arxiv.org/abs/2405.18715v2;2024-05-29;NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the  Wild;"Neural Radiance Fields (NeRFs) have shown remarkable success in synthesizing
photorealistic views from multi-view images of static scenes, but face
challenges in dynamic, real-world environments with distractors like moving
objects, shadows, and lighting changes. Existing methods manage controlled
environments and low occlusion ratios but fall short in render quality,
especially under high occlusion scenarios. In this paper, we introduce NeRF
On-the-go, a simple yet effective approach that enables the robust synthesis of
novel views in complex, in-the-wild scenes from only casually captured image
sequences. Delving into uncertainty, our method not only efficiently eliminates
distractors, even when they are predominant in captures, but also achieves a
notably faster convergence speed. Through comprehensive experiments on various
scenes, our method demonstrates a significant improvement over state-of-the-art
techniques. This advancement opens new avenues for NeRF in diverse and dynamic
real-world applications.";Weining Ren<author:sep>Zihan Zhu<author:sep>Boyang Sun<author:sep>Jiaqi Chen<author:sep>Marc Pollefeys<author:sep>Songyou Peng;http://arxiv.org/pdf/2405.18715v2;cs.CV;"CVPR 2024, first two authors contributed equally. Project Page:
  https://rwn17.github.io/nerf-on-the-go/";nerf
2405.18863v1;http://arxiv.org/abs/2405.18863v1;2024-05-29;Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy;"Enabling the synthesis of arbitrarily novel viewpoint images within a
patient's stomach from pre-captured monocular gastroscopic images is a
promising topic in stomach diagnosis. Typical methods to achieve this objective
integrate traditional 3D reconstruction techniques, including
structure-from-motion (SfM) and Poisson surface reconstruction. These methods
produce explicit 3D representations, such as point clouds and meshes, thereby
enabling the rendering of the images from novel viewpoints. However, the
existence of low-texture and non-Lambertian regions within the stomach often
results in noisy and incomplete reconstructions of point clouds and meshes,
hindering the attainment of high-quality image rendering. In this paper, we
apply the emerging technique of neural radiance fields (NeRF) to monocular
gastroscopic data for synthesizing photo-realistic images for novel viewpoints.
To address the performance degradation due to view sparsity in local regions of
monocular gastroscopy, we incorporate geometry priors from a pre-reconstructed
point cloud into the training of NeRF, which introduces a novel geometry-based
loss to both pre-captured observed views and generated unobserved views.
Compared to other recent NeRF methods, our approach showcases high-fidelity
image renderings from novel viewpoints within the stomach both qualitatively
and quantitatively.";Zijie Jiang<author:sep>Yusuke Monno<author:sep>Masatoshi Okutomi<author:sep>Sho Suzuki<author:sep>Kenji Miki;http://arxiv.org/pdf/2405.18863v1;cs.CV;Accepted for EMBC 2024;nerf
2405.20224v1;http://arxiv.org/abs/2405.20224v1;2024-05-29;EvaGaussians: Event Stream Assisted Gaussian Splatting from Blurry  Images;"3D Gaussian Splatting (3D-GS) has demonstrated exceptional capabilities in 3D
scene reconstruction and novel view synthesis. However, its training heavily
depends on high-quality, sharp images and accurate camera poses. Fulfilling
these requirements can be challenging in non-ideal real-world scenarios, where
motion-blurred images are commonly encountered in high-speed moving cameras or
low-light environments that require long exposure times. To address these
challenges, we introduce Event Stream Assisted Gaussian Splatting
(EvaGaussians), a novel approach that integrates event streams captured by an
event camera to assist in reconstructing high-quality 3D-GS from blurry images.
Capitalizing on the high temporal resolution and dynamic range offered by the
event camera, we leverage the event streams to explicitly model the formation
process of motion-blurred images and guide the deblurring reconstruction of
3D-GS. By jointly optimizing the 3D-GS parameters and recovering camera motion
trajectories during the exposure time, our method can robustly facilitate the
acquisition of high-fidelity novel views with intricate texture details. We
comprehensively evaluated our method and compared it with previous
state-of-the-art deblurring rendering methods. Both qualitative and
quantitative comparisons demonstrate that our method surpasses existing
techniques in restoring fine details from blurry images and producing
high-fidelity novel views.";Wangbo Yu<author:sep>Chaoran Feng<author:sep>Jiye Tang<author:sep>Xu Jia<author:sep>Li Yuan<author:sep>Yonghong Tian;http://arxiv.org/pdf/2405.20224v1;cs.CV;Project Page: https://drexubery.github.io/EvaGaussians/;gaussian splatting
2405.17811v1;http://arxiv.org/abs/2405.17811v1;2024-05-28;Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh;"Neural 3D representations such as Neural Radiance Fields (NeRF), excel at
producing photo-realistic rendering results but lack the flexibility for
manipulation and editing which is crucial for content creation. Previous works
have attempted to address this issue by deforming a NeRF in canonical space or
manipulating the radiance field based on an explicit mesh. However,
manipulating NeRF is not highly controllable and requires a long training and
inference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely
high-fidelity novel view synthesis can be achieved using an explicit
point-based 3D representation with much faster training and rendering speed.
However, there is still a lack of effective means to manipulate 3DGS freely
while maintaining rendering quality. In this work, we aim to tackle the
challenge of achieving manipulable photo-realistic rendering. We propose to
utilize a triangular mesh to manipulate 3DGS directly with self-adaptation.
This approach reduces the need to design various algorithms for different types
of Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding
and adapting method, we can achieve 3DGS manipulation and preserve
high-fidelity rendering after manipulation. Our approach is capable of handling
large deformations, local manipulations, and soft body simulations while
keeping high-quality rendering. Furthermore, we demonstrate that our method is
also effective with inaccurate meshes extracted from 3DGS. Experiments
conducted demonstrate the effectiveness of our method and its superiority over
baseline approaches.";Xiangjun Gao<author:sep>Xiaoyu Li<author:sep>Yiyu Zhuang<author:sep>Qi Zhang<author:sep>Wenbo Hu<author:sep>Chaopeng Zhang<author:sep>Yao Yao<author:sep>Ying Shan<author:sep>Long Quan;http://arxiv.org/pdf/2405.17811v1;cs.GR;Project page here: https://gaoxiangjun.github.io/mani_gs/;gaussian splatting<tag:sep>nerf
2405.18033v1;http://arxiv.org/abs/2405.18033v1;2024-05-28;RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian  Representations of Radiance Fields;"Gaussian Splatting has revolutionized the world of novel view synthesis by
achieving high rendering performance in real-time. Recently, studies have
focused on enriching these 3D representations with semantic information for
downstream tasks. In this paper, we introduce RT-GS2, the first generalizable
semantic segmentation method employing Gaussian Splatting. While existing
Gaussian Splatting-based approaches rely on scene-specific training, RT-GS2
demonstrates the ability to generalize to unseen scenes. Our method adopts a
new approach by first extracting view-independent 3D Gaussian features in a
self-supervised manner, followed by a novel View-Dependent / View-Independent
(VDVI) feature fusion to enhance semantic consistency over different views.
Extensive experimentation on three different datasets showcases RT-GS2's
superiority over the state-of-the-art methods in semantic segmentation quality,
exemplified by a 8.01% increase in mIoU on the Replica dataset. Moreover, our
method achieves real-time performance of 27.03 FPS, marking an astonishing 901
times speedup compared to existing approaches. This work represents a
significant advancement in the field by introducing, to the best of our
knowledge, the first real-time generalizable semantic segmentation method for
3D Gaussian representations of radiance fields.";Mihnea-Bogdan Jurca<author:sep>Remco Royen<author:sep>Ion Giosan<author:sep>Adrian Munteanu;http://arxiv.org/pdf/2405.18033v1;cs.CV;;gaussian splatting
2405.17958v1;http://arxiv.org/abs/2405.17958v1;2024-05-28;FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View  Synthesis of Indoor Scenes;"Empowering 3D Gaussian Splatting with generalization ability is appealing.
However, existing generalizable 3D Gaussian Splatting methods are largely
confined to narrow-range interpolation between stereo images due to their heavy
backbones, thus lacking the ability to accurately localize 3D Gaussian and
support free-view synthesis across wide view range. In this paper, we present a
novel framework FreeSplat that is capable of reconstructing geometrically
consistent 3D scenes from long sequence input towards free-view
synthesis.Specifically, we firstly introduce Low-cost Cross-View Aggregation
achieved by constructing adaptive cost volumes among nearby views and
aggregating features using a multi-scale structure. Subsequently, we present
the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in
overlapping view regions and to aggregate features observed across multiple
views. Additionally, we propose a simple but effective free-view training
strategy that ensures robust view synthesis across broader view range
regardless of the number of views. Our empirical results demonstrate
state-of-the-art novel view synthesis peformances in both novel view rendered
color maps quality and depth maps accuracy across different numbers of input
views. We also show that FreeSplat performs inference more efficiently and can
effectively reduce redundant Gaussians, offering the possibility of
feed-forward large scene reconstruction without depth priors.";Yunsong Wang<author:sep>Tianxin Huang<author:sep>Hanlin Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2405.17958v1;cs.CV;;gaussian splatting
2405.18416v2;http://arxiv.org/abs/2405.18416v2;2024-05-28;3D StreetUnveiler with Semantic-Aware 2DGS;"Unveiling an empty street from crowded observations captured by in-car
cameras is crucial for autonomous driving. However, removing all temporarily
static objects, such as stopped vehicles and standing pedestrians, presents a
significant challenge. Unlike object-centric 3D inpainting, which relies on
thorough observation in a small scene, street scene cases involve long
trajectories that differ from previous 3D inpainting tasks. The camera-centric
moving environment of captured videos further complicates the task due to the
limited degree and time duration of object observation. To address these
obstacles, we introduce StreetUnveiler to reconstruct an empty street.
StreetUnveiler learns a 3D representation of the empty street from crowded
observations. Our representation is based on the hard-label semantic 2D
Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians
to be removed. We inpaint rendered image after removing unwanted Gaussians to
provide pseudo-labels and subsequently re-optimize the 2DGS. Given its temporal
continuous movement, we divide the empty street scene into observed,
partial-observed, and unobserved regions, which we propose to locate through a
rendered alpha map. This decomposition helps us to minimize the regions that
need to be inpainted. To enhance the temporal consistency of the inpainting, we
introduce a novel time-reversal framework to inpaint frames in reverse order
and use later frames as references for earlier frames to fully utilize the
long-trajectory observations. Our experiments conducted on the street scene
dataset successfully reconstructed a 3D representation of the empty street. The
mesh representation of the empty street can be extracted for further
applications. The project page and more visualizations can be found at:
https://streetunveiler.github.io";Jingwei Xu<author:sep>Yikai Wang<author:sep>Yiqun Zhao<author:sep>Yanwei Fu<author:sep>Shenghua Gao;http://arxiv.org/pdf/2405.18416v2;cs.CV;Project page: https://streetunveiler.github.io;gaussian splatting
2405.18133v1;http://arxiv.org/abs/2405.18133v1;2024-05-28;A Grid-Free Fluid Solver based on Gaussian Spatial Representation;"We present a grid-free fluid solver featuring a novel Gaussian
representation. Drawing inspiration from the expressive capabilities of 3D
Gaussian Splatting in multi-view image reconstruction, we model the continuous
flow velocity as a weighted sum of multiple Gaussian functions. Leveraging this
representation, we derive differential operators for the field and implement a
time-dependent PDE solver using the traditional operator splitting method.
Compared to implicit neural representations as another continuous spatial
representation with increasing attention, our method with flexible 3D Gaussians
presents enhanced accuracy on vorticity preservation. Moreover, we apply
physics-driven strategies to accelerate the optimization-based time integration
of Gaussian functions. This temporal evolution surpasses previous work based on
implicit neural representation with reduced computational time and memory.
Although not surpassing the quality of state-of-the-art Eulerian methods in
fluid simulation, experiments and ablation studies indicate the potential of
our memory-efficient representation. With enriched spatial information, our
method exhibits a distinctive perspective combining the advantages of Eulerian
and Lagrangian approaches.";Jingrui Xing<author:sep>Bin Wang<author:sep>Mengyu Chu<author:sep>Baoquan Chen;http://arxiv.org/pdf/2405.18133v1;cs.GR;;gaussian splatting
2405.17872v2;http://arxiv.org/abs/2405.17872v2;2024-05-28;HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal  High-Frequency Components for Endoscopic Scene Reconstruction;"Robot-assisted minimally invasive surgery benefits from enhancing dynamic
scene reconstruction, as it improves surgical outcomes. While Neural Radiance
Fields (NeRF) have been effective in scene reconstruction, their slow inference
speeds and lengthy training durations limit their applicability. To overcome
these limitations, 3D Gaussian Splatting (3D-GS) based methods have emerged as
a recent trend, offering rapid inference capabilities and superior 3D quality.
However, these methods still struggle with under-reconstruction in both static
and dynamic scenes. In this paper, we propose HFGS, a novel approach for
deformable endoscopic reconstruction that addresses these challenges from
spatial and temporal frequency perspectives. Our approach incorporates
deformation fields to better handle dynamic scenes and introduces Spatial
High-Frequency Emphasis Reconstruction (SHF) to minimize discrepancies in
spatial frequency spectra between the rendered image and its ground truth.
Additionally, we introduce Temporal High-Frequency Emphasis Reconstruction
(THF) to enhance dynamic awareness in neural rendering by leveraging flow
priors, focusing optimization on motion-intensive parts. Extensive experiments
on two widely used benchmarks demonstrate that HFGS achieves superior rendering
quality. Our code will be available.";Haoyu Zhao<author:sep>Xingyue Zhao<author:sep>Lingting Zhu<author:sep>Weixi Zheng<author:sep>Yongchao Xu;http://arxiv.org/pdf/2405.17872v2;cs.CV;13 pages, 4 figures;gaussian splatting<tag:sep>nerf
2405.18132v1;http://arxiv.org/abs/2405.18132v1;2024-05-28;EG4D: Explicit Generation of 4D Object without Score Distillation;"In recent years, the increasing demand for dynamic 3D assets in design and
gaming applications has given rise to powerful generative pipelines capable of
synthesizing high-quality 4D objects. Previous methods generally rely on score
distillation sampling (SDS) algorithm to infer the unseen views and motion of
4D objects, thus leading to unsatisfactory results with defects like
over-saturation and Janus problem. Therefore, inspired by recent progress of
video diffusion models, we propose to optimize a 4D representation by
explicitly generating multi-view videos from one input image. However, it is
far from trivial to handle practical challenges faced by such a pipeline,
including dramatic temporal inconsistency, inter-frame geometry and texture
diversity, and semantic defects brought by video generation results. To address
these issues, we propose DG4D, a novel multi-stage framework that generates
high-quality and consistent 4D assets without score distillation. Specifically,
collaborative techniques and solutions are developed, including an attention
injection strategy to synthesize temporal-consistent multi-view videos, a
robust and efficient dynamic reconstruction method based on Gaussian Splatting,
and a refinement stage with diffusion prior for semantic restoration. The
qualitative results and user preference study demonstrate that our framework
outperforms the baselines in generation quality by a considerable margin. Code
will be released at \url{https://github.com/jasongzy/EG4D}.";Qi Sun<author:sep>Zhiyang Guo<author:sep>Ziyu Wan<author:sep>Jing Nathan Yan<author:sep>Shengming Yin<author:sep>Wengang Zhou<author:sep>Jing Liao<author:sep>Houqiang Li;http://arxiv.org/pdf/2405.18132v1;cs.CV;;gaussian splatting
2405.17942v1;http://arxiv.org/abs/2405.17942v1;2024-05-28;Self-supervised Pre-training for Transferable Multi-modal Perception;"In autonomous driving, multi-modal perception models leveraging inputs from
multiple sensors exhibit strong robustness in degraded environments. However,
these models face challenges in efficiently and effectively transferring
learned representations across different modalities and tasks. This paper
presents NeRF-Supervised Masked Auto Encoder (NS-MAE), a self-supervised
pre-training paradigm for transferable multi-modal representation learning.
NS-MAE is designed to provide pre-trained model initializations for efficient
and high-performance fine-tuning. Our approach uses masked multi-modal
reconstruction in neural radiance fields (NeRF), training the model to
reconstruct missing or corrupted input data across multiple modalities.
Specifically, multi-modal embeddings are extracted from corrupted LiDAR point
clouds and images, conditioned on specific view directions and locations. These
embeddings are then rendered into projected multi-modal feature maps using
neural rendering techniques. The original multi-modal signals serve as
reconstruction targets for the rendered feature maps, facilitating
self-supervised representation learning. Extensive experiments demonstrate the
promising transferability of NS-MAE representations across diverse multi-modal
and single-modal perception models. This transferability is evaluated on
various 3D perception downstream tasks, such as 3D object detection and BEV map
segmentation, using different amounts of fine-tuning labeled data. Our code
will be released to support the community.";Xiaohao Xu<author:sep>Tianyi Zhang<author:sep>Jinrong Yang<author:sep>Matthew Johnson-Roberson<author:sep>Xiaonan Huang;http://arxiv.org/pdf/2405.17942v1;cs.CV;"8 pages. arXiv admin note: substantial text overlap with
  arXiv:2311.13750";nerf
2405.17891v1;http://arxiv.org/abs/2405.17891v1;2024-05-28;A Refined 3D Gaussian Representation for High-Quality Dynamic Scene  Reconstruction;"In recent years, Neural Radiance Fields (NeRF) has revolutionized
three-dimensional (3D) reconstruction with its implicit representation.
Building upon NeRF, 3D Gaussian Splatting (3D-GS) has departed from the
implicit representation of neural networks and instead directly represents
scenes as point clouds with Gaussian-shaped distributions. While this shift has
notably elevated the rendering quality and speed of radiance fields but
inevitably led to a significant increase in memory usage. Additionally,
effectively rendering dynamic scenes in 3D-GS has emerged as a pressing
challenge. To address these concerns, this paper purposes a refined 3D Gaussian
representation for high-quality dynamic scene reconstruction. Firstly, we use a
deformable multi-layer perceptron (MLP) network to capture the dynamic offset
of Gaussian points and express the color features of points through hash
encoding and a tiny MLP to reduce storage requirements. Subsequently, we
introduce a learnable denoising mask coupled with denoising loss to eliminate
noise points from the scene, thereby further compressing 3D Gaussian model.
Finally, motion noise of points is mitigated through static constraints and
motion consistency constraints. Experimental results demonstrate that our
method surpasses existing approaches in rendering quality and speed, while
significantly reducing the memory usage associated with 3D-GS, making it highly
suitable for various tasks such as novel view synthesis, and dynamic mapping.";Bin Zhang<author:sep>Bi Zeng<author:sep>Zexin Peng;http://arxiv.org/pdf/2405.17891v1;cs.CV;;gaussian splatting<tag:sep>nerf
2405.17793v1;http://arxiv.org/abs/2405.17793v1;2024-05-28;SafeguardGS: 3D Gaussian Primitive Pruning While Avoiding Catastrophic  Scene Destruction;"3D Gaussian Splatting (3DGS) has made a significant stride in novel view
synthesis, demonstrating top-notch rendering quality while achieving real-time
rendering speed. However, the excessively large number of Gaussian primitives
resulting from 3DGS' suboptimal densification process poses a major challenge,
slowing down frame-per-second (FPS) and demanding considerable memory cost,
making it unfavorable for low-end devices. To cope with this issue, many
follow-up studies have suggested various pruning techniques, often in
combination with different score functions, to optimize rendering performance.
Nonetheless, a comprehensive discussion regarding their effectiveness and
implications across all techniques is missing. In this paper, we first
categorize 3DGS pruning techniques into two types: Cross-view pruning and
pixel-wise pruning, which differ in their approaches to rank primitives. Our
subsequent experiments reveal that while cross-view pruning leads to disastrous
quality drops under extreme Gaussian primitives decimation, the pixel-wise
pruning technique not only sustains relatively high rendering quality with
minuscule performance degradation but also provides a reasonable minimum
boundary for pruning. Building on this observation, we further propose multiple
variations of score functions and empirically discover that the color-weighted
score function outperforms others for discriminating insignificant primitives
for rendering. We believe our research provides valuable insights for
optimizing 3DGS pruning strategies for future works.";Yongjae Lee<author:sep>Zhaoliang Zhang<author:sep>Deliang Fan;http://arxiv.org/pdf/2405.17793v1;cs.CV;Comprehensive experiments are in progress;gaussian splatting
2405.18426v1;http://arxiv.org/abs/2405.18426v1;2024-05-28;GFlow: Recovering 4D World from Monocular Video;"Reconstructing 4D scenes from video inputs is a crucial yet challenging task.
Conventional methods usually rely on the assumptions of multi-view video
inputs, known camera parameters, or static scenes, all of which are typically
absent under in-the-wild scenarios. In this paper, we relax all these
constraints and tackle a highly ambitious but practical task, which we termed
as AnyV4D: we assume only one monocular video is available without any camera
parameters as input, and we aim to recover the dynamic 4D world alongside the
camera poses. To this end, we introduce GFlow, a new framework that utilizes
only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit
representation, entailing a flow of Gaussian splatting through space and time.
GFlow first clusters the scene into still and moving parts, then applies a
sequential optimization process that optimizes camera poses and the dynamics of
3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity
among neighboring points and smooth movement across frames. Since dynamic
scenes always introduce new content, we also propose a new pixel-wise
densification strategy for Gaussian points to integrate new visual content.
Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also
enables tracking of any points across frames without the need for prior
training and segments moving objects from the scene in an unsupervised way.
Additionally, the camera poses of each frame can be derived from GFlow,
allowing for rendering novel views of a video scene through changing camera
pose. By employing the explicit representation, we may readily conduct
scene-level or object-level editing as desired, underscoring its versatility
and power. Visit our project website at: https://littlepure2333.github.io/GFlow";Shizun Wang<author:sep>Xingyi Yang<author:sep>Qiuhong Shen<author:sep>Zhenxiang Jiang<author:sep>Xinchao Wang;http://arxiv.org/pdf/2405.18426v1;cs.CV;Project page: https://littlepure2333.github.io/GFlow;gaussian splatting
2405.18163v2;http://arxiv.org/abs/2405.18163v2;2024-05-28;NegGS: Negative Gaussian Splatting;"One of the key advantages of 3D rendering is its ability to simulate
intricate scenes accurately. One of the most widely used methods for this
purpose is Gaussian Splatting, a novel approach that is known for its rapid
training and inference capabilities. In essence, Gaussian Splatting involves
incorporating data about the 3D objects of interest into a series of Gaussian
distributions, each of which can then be depicted in 3D in a manner analogous
to traditional meshes. It is regrettable that the use of Gaussians in Gaussian
Splatting is currently somewhat restrictive due to their perceived linear
nature. In practice, 3D objects are often composed of complex curves and highly
nonlinear structures. This issue can to some extent be alleviated by employing
a multitude of Gaussian components to reflect the complex, nonlinear structures
accurately. However, this approach results in a considerable increase in time
complexity. This paper introduces the concept of negative Gaussians, which are
interpreted as items with negative colors. The rationale behind this approach
is based on the density distribution created by dividing the probability
density functions (PDFs) of two Gaussians, which we refer to as Diff-Gaussian.
Such a distribution can be used to approximate structures such as donut and
moon-shaped datasets. Experimental findings indicate that the application of
these techniques enhances the modeling of high-frequency elements with rapid
color transitions. Additionally, it improves the representation of shadows. To
the best of our knowledge, this is the first paper to extend the simple
elipsoid shapes of Gaussian Splatting to more complex nonlinear structures.";Artur Kasymov<author:sep>Bartosz Czekaj<author:sep>Marcin Mazur<author:sep>Jacek Tabor<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2405.18163v2;cs.GR;;gaussian splatting
2405.18424v1;http://arxiv.org/abs/2405.18424v1;2024-05-28;3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian  Splatting;"Scene image editing is crucial for entertainment, photography, and
advertising design. Existing methods solely focus on either 2D individual
object or 3D global scene editing. This results in a lack of a unified approach
to effectively control and manipulate scenes at the 3D level with different
levels of granularity. In this work, we propose 3DitScene, a novel and unified
scene editing framework leveraging language-guided disentangled Gaussian
Splatting that enables seamless editing from 2D to 3D, allowing precise control
over scene composition and individual objects. We first incorporate 3D
Gaussians that are refined through generative priors and optimization
techniques. Language features from CLIP then introduce semantics into 3D
geometry for object disentanglement. With the disentangled Gaussians, 3DitScene
allows for manipulation at both the global and individual levels,
revolutionizing creative expression and empowering control over scenes and
objects. Experimental results demonstrate the effectiveness and versatility of
3DitScene in scene image editing. Code and online demo can be found at our
project homepage: https://zqh0253.github.io/3DitScene/.";Qihang Zhang<author:sep>Yinghao Xu<author:sep>Chaoyang Wang<author:sep>Hsin-Ying Lee<author:sep>Gordon Wetzstein<author:sep>Bolei Zhou<author:sep>Ceyuan Yang;http://arxiv.org/pdf/2405.18424v1;cs.CV;;
2405.17835v3;http://arxiv.org/abs/2405.17835v3;2024-05-28;Deform3DGS: Flexible Deformation for Fast Surgical Scene Reconstruction  with Gaussian Splatting;"Tissue deformation poses a key challenge for accurate surgical scene
reconstruction. Despite yielding high reconstruction quality, existing methods
suffer from slow rendering speeds and long training times, limiting their
intraoperative applicability. Motivated by recent progress in 3D Gaussian
Splatting, an emerging technology in real-time 3D rendering, this work presents
a novel fast reconstruction framework, termed Deform3DGS, for deformable
tissues during endoscopic surgery. Specifically, we introduce 3D GS into
surgical scenes by integrating a point cloud initialization to improve
reconstruction. Furthermore, we propose a novel flexible deformation modeling
scheme (FDM) to learn tissue deformation dynamics at the level of individual
Gaussians. Our FDM can model the surface deformation with efficient
representations, allowing for real-time rendering performance. More
importantly, FDM significantly accelerates surgical scene reconstruction,
demonstrating considerable clinical values, particularly in intraoperative
settings where time efficiency is crucial. Experiments on DaVinci robotic
surgery videos indicate the efficacy of our approach, showcasing superior
reconstruction fidelity PSNR: (37.90) and rendering speed (338.8 FPS) while
substantially reducing training time to only 1 minute/scene. Our code is
available at https://github.com/jinlab-imvr/Deform3DGS.";Shuojue Yang<author:sep>Qian Li<author:sep>Daiyun Shen<author:sep>Bingchen Gong<author:sep>Qi Dou<author:sep>Yueming Jin;http://arxiv.org/pdf/2405.17835v3;cs.CV;Early accepted at MICCAI 2024, 10 pages, 2 figures;gaussian splatting
2405.17421v1;http://arxiv.org/abs/2405.17421v1;2024-05-27;MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion  Scaffolds;"We introduce 4D Motion Scaffolds (MoSca), a neural information processing
system designed to reconstruct and synthesize novel views of dynamic scenes
from monocular videos captured casually in the wild. To address such a
challenging and ill-posed inverse problem, we leverage prior knowledge from
foundational vision models, lift the video data to a novel Motion Scaffold
(MoSca) representation, which compactly and smoothly encodes the underlying
motions / deformations. The scene geometry and appearance are then disentangled
from the deformation field, and are encoded by globally fusing the Gaussians
anchored onto the MoSca and optimized via Gaussian Splatting. Additionally,
camera poses can be seamlessly initialized and refined during the dynamic
rendering process, without the need for other pose estimation tools.
Experiments demonstrate state-of-the-art performance on dynamic rendering
benchmarks.";Jiahui Lei<author:sep>Yijia Weng<author:sep>Adam Harley<author:sep>Leonidas Guibas<author:sep>Kostas Daniilidis;http://arxiv.org/pdf/2405.17421v1;cs.CV;project page: https://www.cis.upenn.edu/~leijh/projects/mosca;gaussian splatting
2405.17705v2;http://arxiv.org/abs/2405.17705v2;2024-05-27;DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam  Videos;"We present DC-Gaussian, a new method for generating novel views from
in-vehicle dash cam videos. While neural rendering techniques have made
significant strides in driving scenarios, existing methods are primarily
designed for videos collected by autonomous vehicles. However, these videos are
limited in both quantity and diversity compared to dash cam videos, which are
more widely used across various types of vehicles and capture a broader range
of scenarios. Dash cam videos often suffer from severe obstructions such as
reflections and occlusions on the windshields, which significantly impede the
application of neural rendering techniques. To address this challenge, we
develop DC-Gaussian based on the recent real-time neural rendering technique 3D
Gaussian Splatting (3DGS). Our approach includes an adaptive image
decomposition module to model reflections and occlusions in a unified manner.
Additionally, we introduce illumination-aware obstruction modeling to manage
reflections and occlusions under varying lighting conditions. Lastly, we employ
a geometry-guided Gaussian enhancement strategy to improve rendering details by
incorporating additional geometry priors. Experiments on self-captured and
public dash cam videos show that our method not only achieves state-of-the-art
performance in novel view synthesis, but also accurately reconstructing
captured scenes getting rid of obstructions.";Linhan Wang<author:sep>Kai Cheng<author:sep>Shuo Lei<author:sep>Shengkun Wang<author:sep>Wei Yin<author:sep>Chenyang Lei<author:sep>Xiaoxiao Long<author:sep>Chang-Tien Lu;http://arxiv.org/pdf/2405.17705v2;cs.CV;"9 pages,7 figures;project page:
  https://linhanwang.github.io/dcgaussian/";gaussian splatting
2405.16829v3;http://arxiv.org/abs/2405.16829v3;2024-05-27;PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian  Splatting;"Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in
synthesizing photorealistic images of large-scale scenes. However, they are
often plagued by a loss of fine details and long rendering durations. 3D
Gaussian Splatting has recently been introduced as a potent alternative,
achieving both high-fidelity visual results and accelerated rendering
performance. Nonetheless, scaling 3D Gaussian Splatting is fraught with
challenges. Specifically, large-scale scenes grapples with the integration of
objects across multiple scales and disparate viewpoints, which often leads to
compromised efficacy as the Gaussians need to balance between detail levels.
Furthermore, the generation of initialization points via COLMAP from
large-scale dataset is both computationally demanding and prone to incomplete
reconstructions. To address these challenges, we present Pyramidal 3D Gaussian
Splatting (PyGS) with NeRF Initialization. Our approach represent the scene
with a hierarchical assembly of Gaussians arranged in a pyramidal fashion. The
top level of the pyramid is composed of a few large Gaussians, while each
subsequent layer accommodates a denser collection of smaller Gaussians. We
effectively initialize these pyramidal Gaussians through sampling a rapidly
trained grid-based NeRF at various frequencies. We group these pyramidal
Gaussians into clusters and use a compact weighting network to dynamically
determine the influence of each pyramid level of each cluster considering
camera viewpoint during rendering. Our method achieves a significant
performance leap across multiple large-scale datasets and attains a rendering
time that is over 400 times faster than current state-of-the-art approaches.";Zipeng Wang<author:sep>Dan Xu;http://arxiv.org/pdf/2405.16829v3;cs.CV;;gaussian splatting<tag:sep>nerf
2405.16923v2;http://arxiv.org/abs/2405.16923v2;2024-05-27;SA-GS: Semantic-Aware Gaussian Splatting for Large Scene Reconstruction  with Geometry Constrain;"With the emergence of Gaussian Splats, recent efforts have focused on
large-scale scene geometric reconstruction. However, most of these efforts
either concentrate on memory reduction or spatial space division, neglecting
information in the semantic space. In this paper, we propose a novel method,
named SA-GS, for fine-grained 3D geometry reconstruction using semantic-aware
3D Gaussian Splats. Specifically, we leverage prior information stored in large
vision models such as SAM and DINO to generate semantic masks. We then
introduce a geometric complexity measurement function to serve as soft
regularization, guiding the shape of each Gaussian Splat within specific
semantic areas. Additionally, we present a method that estimates the expected
number of Gaussian Splats in different semantic areas, effectively providing a
lower bound for Gaussian Splats in these areas. Subsequently, we extract the
point cloud using a novel probability density-based extraction method,
transforming Gaussian Splats into a point cloud crucial for downstream tasks.
Our method also offers the potential for detailed semantic inquiries while
maintaining high image-based reconstruction results. We provide extensive
experiments on publicly available large-scale scene reconstruction datasets
with highly accurate point clouds as ground truth and our novel dataset. Our
results demonstrate the superiority of our method over current state-of-the-art
Gaussian Splats reconstruction methods by a significant margin in terms of
geometric-based measurement metrics. Code and additional results will soon be
available on our project page.";Butian Xiong<author:sep>Xiaoyu Ye<author:sep>Tze Ho Elden Tse<author:sep>Kai Han<author:sep>Shuguang Cui<author:sep>Zhen Li;http://arxiv.org/pdf/2405.16923v2;cs.CV;Might need more comparison, will be add later;gaussian splatting
2405.17351v1;http://arxiv.org/abs/2405.17351v1;2024-05-27;DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for  Refocusing,Defocus Rendering and Blur Removal;"3D Gaussian Splatting-based techniques have recently advanced 3D scene
reconstruction and novel view synthesis, achieving high-quality real-time
rendering. However, these approaches are inherently limited by the underlying
pinhole camera assumption in modeling the images and hence only work for
All-in-Focus (AiF) sharp image inputs. This severely affects their
applicability in real-world scenarios where images often exhibit defocus blur
due to the limited depth-of-field (DOF) of imaging devices. Additionally,
existing 3D Gaussian Splatting (3DGS) methods also do not support rendering of
DOF effects.
  To address these challenges, we introduce DOF-GS that allows for rendering
adjustable DOF effects, removing defocus blur as well as refocusing of 3D
scenes, all from multi-view images degraded by defocus blur. To this end, we
re-imagine the traditional Gaussian Splatting pipeline by employing a finite
aperture camera model coupled with explicit, differentiable defocus rendering
guided by the Circle-of-Confusion (CoC). The proposed framework provides for
dynamic adjustment of DOF effects by changing the aperture and focal distance
of the underlying camera model on-demand. It also enables rendering varying DOF
effects of 3D scenes post-optimization, and generating AiF images from
defocused training images. Furthermore, we devise a joint optimization strategy
to further enhance details in the reconstructed scenes by jointly optimizing
rendered defocused and AiF images. Our experimental results indicate that
DOF-GS produces high-quality sharp all-in-focus renderings conditioned on
inputs compromised by defocus blur, with the training process incurring only a
modest increase in GPU memory consumption. We further demonstrate the
applications of the proposed method for adjustable defocus rendering and
refocusing of the 3D scene from input images degraded by defocus blur.";Yujie Wang<author:sep>Praneeth Chakravarthula<author:sep>Baoquan Chen;http://arxiv.org/pdf/2405.17351v1;cs.CV;;gaussian splatting
2405.17083v2;http://arxiv.org/abs/2405.17083v2;2024-05-27;F-3DGS: Factorized Coordinates and Representations for 3D Gaussian  Splatting;"The neural radiance field (NeRF) has made significant strides in representing
3D scenes and synthesizing novel views. Despite its advancements, the high
computational costs of NeRF have posed challenges for its deployment in
resource-constrained environments and real-time applications. As an alternative
to NeRF-like neural rendering methods, 3D Gaussian Splatting (3DGS) offers
rapid rendering speeds while maintaining excellent image quality. However, as
it represents objects and scenes using a myriad of Gaussians, it requires
substantial storage to achieve high-quality representation. To mitigate the
storage overhead, we propose Factorized 3D Gaussian Splatting (F-3DGS), a novel
approach that drastically reduces storage requirements while preserving image
quality. Inspired by classical matrix and tensor factorization techniques, our
method represents and approximates dense clusters of Gaussians with
significantly fewer Gaussians through efficient factorization. We aim to
efficiently represent dense 3D Gaussians by approximating them with a limited
amount of information for each axis and their combinations. This method allows
us to encode a substantially large number of Gaussians along with their
essential attributes -- such as color, scale, and rotation -- necessary for
rendering using a relatively small number of elements. Extensive experimental
results demonstrate that F-3DGS achieves a significant reduction in storage
costs while maintaining comparable quality in rendered images.";Xiangyu Sun<author:sep>Joo Chan Lee<author:sep>Daniel Rho<author:sep>Jong Hwan Ko<author:sep>Usman Ali<author:sep>Eunbyung Park;http://arxiv.org/pdf/2405.17083v2;cs.CV;"Our project page including code is available at
  https://xiangyu1sun.github.io/Factorize-3DGS/";gaussian splatting<tag:sep>nerf
2405.17596v1;http://arxiv.org/abs/2405.17596v1;2024-05-27;GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary  Semantic-space Hyperplane;"3D open-vocabulary scene understanding, crucial for advancing augmented
reality and robotic applications, involves interpreting and locating specific
regions within a 3D space as directed by natural language instructions. To this
end, we introduce GOI, a framework that integrates semantic features from 2D
vision-language foundation models into 3D Gaussian Splatting (3DGS) and
identifies 3D Gaussians of Interest using an Optimizable Semantic-space
Hyperplane. Our approach includes an efficient compression method that utilizes
scene priors to condense noisy high-dimensional semantic features into compact
low-dimensional vectors, which are subsequently embedded in 3DGS. During the
open-vocabulary querying process, we adopt a distinct approach compared to
existing methods, which depend on a manually set fixed empirical threshold to
select regions based on their semantic feature distance to the query text
embedding. This traditional approach often lacks universal accuracy, leading to
challenges in precisely identifying specific target areas. Instead, our method
treats the feature selection process as a hyperplane division within the
feature space, retaining only those features that are highly relevant to the
query. We leverage off-the-shelf 2D Referring Expression Segmentation (RES)
models to fine-tune the semantic-space hyperplane, enabling a more precise
distinction between target regions and others. This fine-tuning substantially
improves the accuracy of open-vocabulary queries, ensuring the precise
localization of pertinent 3D Gaussians. Extensive experiments demonstrate GOI's
superiority over previous state-of-the-art methods. Our project page is
available at https://goi-hyperplane.github.io/ .";Yansong Qu<author:sep>Shaohui Dai<author:sep>Xinyang Li<author:sep>Jianghang Lin<author:sep>Liujuan Cao<author:sep>Shengchuan Zhang<author:sep>Rongrong Ji;http://arxiv.org/pdf/2405.17596v1;cs.CV;Our project page is available at https://goi-hyperplane.github.io/;gaussian splatting
2405.17187v1;http://arxiv.org/abs/2405.17187v1;2024-05-27;Memorize What Matters: Emergent Scene Decomposition from Multitraverse;"Humans naturally retain memories of permanent elements, while ephemeral
moments often slip through the cracks of memory. This selective retention is
crucial for robotic perception, localization, and mapping. To endow robots with
this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised,
camera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM
converts multitraverse RGB videos from the same region into a Gaussian-based
environmental map while concurrently performing 2D ephemeral object
segmentation. Our key observation is that the environment remains consistent
across traversals, while objects frequently change. This allows us to exploit
self-supervision from repeated traversals to achieve environment-object
decomposition. More specifically, 3DGM formulates multitraverse environmental
mapping as a robust differentiable rendering problem, treating pixels of the
environment and objects as inliers and outliers, respectively. Using robust
feature distillation, feature residuals mining, and robust optimization, 3DGM
jointly performs 3D mapping and 2D segmentation without human intervention. We
build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets,
to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and
neural rendering. Extensive results verify the effectiveness and potential of
our method for self-driving and robotics.";Yiming Li<author:sep>Zehong Wang<author:sep>Yue Wang<author:sep>Zhiding Yu<author:sep>Zan Gojcic<author:sep>Marco Pavone<author:sep>Chen Feng<author:sep>Jose M. Alvarez;http://arxiv.org/pdf/2405.17187v1;cs.CV;Project page: https://3d-gaussian-mapping.github.io;gaussian splatting
2405.16544v1;http://arxiv.org/abs/2405.16544v1;2024-05-26;Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians;"3D Gaussian Splatting has emerged as a powerful representation of geometry
and appearance for RGB-only dense Simultaneous Localization and Mapping (SLAM),
as it provides a compact dense map representation while enabling efficient and
high-quality map rendering. However, existing methods show significantly worse
reconstruction quality than competing methods using other 3D representations,
e.g. neural points clouds, since they either do not employ global map and pose
optimization or make use of monocular depth. In response, we propose the first
RGB-only SLAM system with a dense 3D Gaussian map representation that utilizes
all benefits of globally optimized tracking by adapting dynamically to keyframe
pose and depth updates by actively deforming the 3D Gaussian map. Moreover, we
find that refining the depth updates in inaccurate areas with a monocular depth
estimator further improves the accuracy of the 3D reconstruction. Our
experiments on the Replica, TUM-RGBD, and ScanNet datasets indicate the
effectiveness of globally optimized 3D Gaussians, as the approach achieves
superior or on par performance with existing RGB-only SLAM methods methods in
tracking, mapping and rendering accuracy while yielding small map sizes and
fast runtimes. The source code is available at
https://github.com/eriksandstroem/Splat-SLAM.";Erik Sandström<author:sep>Keisuke Tateno<author:sep>Michael Oechsle<author:sep>Michael Niemeyer<author:sep>Luc Van Gool<author:sep>Martin R. Oswald<author:sep>Federico Tombari;http://arxiv.org/pdf/2405.16544v1;cs.CV;21 pages;gaussian splatting
2405.16517v1;http://arxiv.org/abs/2405.16517v1;2024-05-26;Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion  Priors;"We aim to tackle sparse-view reconstruction of a 360 3D scene using priors
from latent diffusion models (LDM). The sparse-view setting is ill-posed and
underconstrained, especially for scenes where the camera rotates 360 degrees
around a point, as no visual information is available beyond some frontal views
focused on the central object(s) of interest. In this work, we show that
pretrained 2D diffusion models can strongly improve the reconstruction of a
scene with low-cost fine-tuning. Specifically, we present SparseSplat360
(Sp2360), a method that employs a cascade of in-painting and artifact removal
models to fill in missing details and clean novel views. Due to superior
training and rendering speeds, we use an explicit scene representation in the
form of 3D Gaussians over NeRF-based implicit representations. We propose an
iterative update strategy to fuse generated pseudo novel views with existing 3D
Gaussians fitted to the initial sparse inputs. As a result, we obtain a
multi-view consistent scene representation with details coherent with the
observed inputs. Our evaluation on the challenging Mip-NeRF360 dataset shows
that our proposed 2D to 3D distillation algorithm considerably improves the
performance of a regularized version of 3DGS adapted to a sparse-view setting
and outperforms existing sparse-view reconstruction methods in 360 scene
reconstruction. Qualitatively, our method generates entire 360 scenes from as
few as 9 input views, with a high degree of foreground and background detail.";Soumava Paul<author:sep>Christopher Wewer<author:sep>Bernt Schiele<author:sep>Jan Eric Lenssen;http://arxiv.org/pdf/2405.16517v1;cs.CV;18 pages, 10 figures, 4 tables;nerf
2405.16645v1;http://arxiv.org/abs/2405.16645v1;2024-05-26;Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video  Diffusion Models;"The availability of large-scale multimodal datasets and advancements in
diffusion models have significantly accelerated progress in 4D content
generation. Most prior approaches rely on multiple image or video diffusion
models, utilizing score distillation sampling for optimization or generating
pseudo novel views for direct supervision. However, these methods are hindered
by slow optimization speeds and multi-view inconsistency issues. Spatial and
temporal consistency in 4D geometry has been extensively explored respectively
in 3D-aware diffusion models and traditional monocular video diffusion models.
Building on this foundation, we propose a strategy to migrate the temporal
consistency in video diffusion models to the spatial-temporal consistency
required for 4D generation. Specifically, we present a novel framework,
\textbf{Diffusion4D}, for efficient and scalable 4D content generation.
Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware
video diffusion model capable of synthesizing orbital views of dynamic 3D
assets. To control the dynamic strength of these assets, we introduce a
3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel
motion magnitude reconstruction loss and 3D-aware classifier-free guidance to
refine the learning and generation of motion dynamics. After obtaining orbital
views of the 4D asset, we perform explicit 4D construction with Gaussian
splatting in a coarse-to-fine manner. The synthesized multi-view consistent 4D
image set enables us to swiftly generate high-fidelity and diverse 4D assets
within just several minutes. Extensive experiments demonstrate that our method
surpasses prior state-of-the-art techniques in terms of generation efficiency
and 4D geometry consistency across various prompt modalities.";Hanwen Liang<author:sep>Yuyang Yin<author:sep>Dejia Xu<author:sep>Hanxue Liang<author:sep>Zhangyang Wang<author:sep>Konstantinos N. Plataniotis<author:sep>Yao Zhao<author:sep>Yunchao Wei;http://arxiv.org/pdf/2405.16645v1;cs.CV;Project page: https://vita-group.github.io/Diffusion4D;
2405.15196v1;http://arxiv.org/abs/2405.15196v1;2024-05-24;DisC-GS: Discontinuity-aware Gaussian Splatting;"Recently, Gaussian Splatting, a method that represents a 3D scene as a
collection of Gaussian distributions, has gained significant attention in
addressing the task of novel view synthesis. In this paper, we highlight a
fundamental limitation of Gaussian Splatting: its inability to accurately
render discontinuities and boundaries in images due to the continuous nature of
Gaussian distributions. To address this issue, we propose a novel framework
enabling Gaussian Splatting to perform discontinuity-aware image rendering.
Additionally, we introduce a B\'ezier-boundary gradient approximation strategy
within our framework to keep the ``differentiability'' of the proposed
discontinuity-aware rendering process. Extensive experiments demonstrate the
efficacy of our framework.";Haoxuan Qu<author:sep>Zhuoling Li<author:sep>Hossein Rahmani<author:sep>Yujun Cai<author:sep>Jun Liu;http://arxiv.org/pdf/2405.15196v1;cs.CV;;gaussian splatting
2405.15491v1;http://arxiv.org/abs/2405.15491v1;2024-05-24;GSDeformer: Direct Cage-based Deformation for 3D Gaussian Splatting;"We present GSDeformer, a method that achieves free-form deformation on 3D
Gaussian Splatting(3DGS) without requiring any architectural changes. Our
method extends cage-based deformation, a traditional mesh deformation method,
to 3DGS. This is done by converting 3DGS into a novel proxy point cloud
representation, where its deformation can be used to infer the transformations
to apply on the 3D gaussians making up 3DGS. We also propose an automatic cage
construction algorithm for 3DGS to minimize manual work. Our method does not
modify the underlying architecture of 3DGS. Therefore, any existing trained
vanilla 3DGS can be easily edited by our method. We compare the deformation
capability of our method against other existing methods, demonstrating the ease
of use and comparable quality of our method, despite being more direct and thus
easier to integrate with other concurrent developments on 3DGS.";Jiajun Huang<author:sep>Hongchuan Yu;http://arxiv.org/pdf/2405.15491v1;cs.CV;For project page, see https://jhuangbu.github.io/gsdeformer;gaussian splatting
2405.15125v2;http://arxiv.org/abs/2405.15125v2;2024-05-24;HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed  via Gaussian Splatting;"High dynamic range (HDR) novel view synthesis (NVS) aims to create
photorealistic images from novel viewpoints using HDR imaging techniques. The
rendered HDR images capture a wider range of brightness levels containing more
details of the scene than normal low dynamic range (LDR) images. Existing HDR
NVS methods are mainly based on NeRF. They suffer from long training time and
slow inference speed. In this paper, we propose a new framework, High Dynamic
Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views
and reconstruct LDR images with a user input exposure time. Specifically, we
design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses
spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to
render LDR color. The HDR and LDR colors are then fed into two Parallel
Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views.
To establish the data foundation for the research of 3D Gaussian
splatting-based methods in HDR NVS, we recalibrate the camera parameters and
compute the initial positions for Gaussian point clouds. Experiments
demonstrate that our HDR-GS surpasses the state-of-the-art NeRF-based method by
3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000x inference speed and
only requiring 6.3% training time. Code, models, and recalibrated data will be
publicly available at https://github.com/caiyuanhao1998/HDR-GS";Yuanhao Cai<author:sep>Zihao Xiao<author:sep>Yixun Liang<author:sep>Minghan Qin<author:sep>Yulun Zhang<author:sep>Xiaokang Yang<author:sep>Yaoyao Liu<author:sep>Alan Yuille;http://arxiv.org/pdf/2405.15125v2;cs.CV;The first 3D Gaussian Splatting-based method for HDR imaging;gaussian splatting<tag:sep>nerf
2405.15118v1;http://arxiv.org/abs/2405.15118v1;2024-05-24;GS-Hider: Hiding Messages into 3D Gaussian Splatting;"3D Gaussian Splatting (3DGS) has already become the emerging research focus
in the fields of 3D scene reconstruction and novel view synthesis. Given that
training a 3DGS requires a significant amount of time and computational cost,
it is crucial to protect the copyright, integrity, and privacy of such 3D
assets. Steganography, as a crucial technique for encrypted transmission and
copyright protection, has been extensively studied. However, it still lacks
profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS
possesses two distinct features: 1) explicit 3D representation; and 2)
real-time rendering speeds. These characteristics result in the 3DGS point
cloud files being public and transparent, with each Gaussian point having a
clear physical significance. Therefore, ensuring the security and fidelity of
the original 3D scene while embedding information into the 3DGS point cloud
files is an extremely challenging task. To solve the above-mentioned issue, we
first propose a steganography framework for 3DGS, dubbed GS-Hider, which can
embed 3D scenes and images into original GS point clouds in an invisible manner
and accurately extract the hidden messages. Specifically, we design a coupled
secured feature attribute to replace the original 3DGS's spherical harmonics
coefficients and then use a scene decoder and a message decoder to disentangle
the original RGB scene and the hidden message. Extensive experiments
demonstrated that the proposed GS-Hider can effectively conceal multimodal
messages without compromising rendering quality and possesses exceptional
security, robustness, capacity, and flexibility. Our project is available at:
https://xuanyuzhang21.github.io/project/gshider.";Xuanyu Zhang<author:sep>Jiarui Meng<author:sep>Runyi Li<author:sep>Zhipei Xu<author:sep>Yongbing Zhang<author:sep>Jian Zhang;http://arxiv.org/pdf/2405.15118v1;cs.CV;3DGS steganography;gaussian splatting<tag:sep>nerf
2405.15518v1;http://arxiv.org/abs/2405.15518v1;2024-05-24;Feature Splatting for Better Novel View Synthesis with Low Overlap;"3D Gaussian Splatting has emerged as a very promising scene representation,
achieving state-of-the-art quality in novel view synthesis significantly faster
than competing alternatives. However, its use of spherical harmonics to
represent scene colors limits the expressivity of 3D Gaussians and, as a
consequence, the capability of the representation to generalize as we move away
from the training views. In this paper, we propose to encode the color
information of 3D Gaussians into per-Gaussian feature vectors, which we denote
as Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are
first ""splatted"" into the image plane, then the corresponding feature vectors
are alpha-blended, and finally the blended vector is decoded by a small MLP to
render the RGB pixel values. To further inform the model, we concatenate a
camera embedding to the blended feature vector, to condition the decoding also
on the viewpoint information. Our experiments show that these novel model for
encoding the radiance considerably improves novel view synthesis for low
overlap views that are distant from the training views. Finally, we also show
the capacity and convenience of our feature vector representation,
demonstrating its capability not only to generate RGB values for novel views,
but also their per-pixel semantic labels. We will release the code upon
acceptance.
  Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting";T. Berriel Martins<author:sep>Javier Civera;http://arxiv.org/pdf/2405.15518v1;cs.CV;;gaussian splatting
2405.15227v1;http://arxiv.org/abs/2405.15227v1;2024-05-24;Neural Elevation Models for Terrain Mapping and Path Planning;"This work introduces Neural Elevations Models (NEMos), which adapt Neural
Radiance Fields to a 2.5D continuous and differentiable terrain model. In
contrast to traditional terrain representations such as digital elevation
models, NEMos can be readily generated from imagery, a low-cost data source,
and provide a lightweight representation of terrain through an implicit
continuous and differentiable height field. We propose a novel method for
jointly training a height field and radiance field within a NeRF framework,
leveraging quantile regression. Additionally, we introduce a path planning
algorithm that performs gradient-based optimization of a continuous cost
function for minimizing distance, slope changes, and control effort, enabled by
differentiability of the height field. We perform experiments on simulated and
real-world terrain imagery, demonstrating NEMos ability to generate
high-quality reconstructions and produce smoother paths compared to discrete
path planning methods. Future work will explore the incorporation of features
and semantics into the height field, creating a generalized terrain model.";Adam Dai<author:sep>Shubh Gupta<author:sep>Grace Gao;http://arxiv.org/pdf/2405.15227v1;cs.RO;;nerf
2405.14866v1;http://arxiv.org/abs/2405.14866v1;2024-05-23;Tele-Aloha: A Low-budget and High-authenticity Telepresence System Using  Sparse RGB Cameras;"In this paper, we present a low-budget and high-authenticity bidirectional
telepresence system, Tele-Aloha, targeting peer-to-peer communication
scenarios. Compared to previous systems, Tele-Aloha utilizes only four sparse
RGB cameras, one consumer-grade GPU, and one autostereoscopic screen to achieve
high-resolution (2048x2048), real-time (30 fps), low-latency (less than 150ms)
and robust distant communication. As the core of Tele-Aloha, we propose an
efficient novel view synthesis algorithm for upper-body. Firstly, we design a
cascaded disparity estimator for obtaining a robust geometry cue. Additionally
a neural rasterizer via Gaussian Splatting is introduced to project latent
features onto target view and to decode them into a reduced resolution.
Further, given the high-quality captured data, we leverage weighted blending
mechanism to refine the decoded image into the final resolution of 2K.
Exploiting world-leading autostereoscopic display and low-latency iris
tracking, users are able to experience a strong three-dimensional sense even
without any wearable head-mounted display device. Altogether, our telepresence
system demonstrates the sense of co-presence in real-life experiments,
inspiring the next generation of communication.";Hanzhang Tu<author:sep>Ruizhi Shao<author:sep>Xue Dong<author:sep>Shunyuan Zheng<author:sep>Hao Zhang<author:sep>Lili Chen<author:sep>Meili Wang<author:sep>Wenyu Li<author:sep>Siyan Ma<author:sep>Shengping Zhang<author:sep>Boyao Zhou<author:sep>Yebin Liu;http://arxiv.org/pdf/2405.14866v1;cs.CV;"Paper accepted by SIGGRAPH 2024. Project page:
  http://118.178.32.38/c/Tele-Aloha/";gaussian splatting
2405.14475v1;http://arxiv.org/abs/2405.14475v1;2024-05-23;MagicDrive3D: Controllable 3D Generation for Any-View Rendering in  Street Scenes;"While controllable generative models for images and videos have achieved
remarkable success, high-quality models for 3D scenes, particularly in
unbounded scenarios like autonomous driving, remain underdeveloped due to high
data acquisition costs. In this paper, we introduce MagicDrive3D, a novel
pipeline for controllable 3D street scene generation that supports
multi-condition control, including BEV maps, 3D objects, and text descriptions.
Unlike previous methods that reconstruct before training the generative models,
MagicDrive3D first trains a video generation model and then reconstructs from
the generated data. This innovative approach enables easily controllable
generation and static scene acquisition, resulting in high-quality scene
reconstruction. To address the minor errors in generated content, we propose
deformable Gaussian splatting with monocular depth initialization and
appearance modeling to manage exposure discrepancies across viewpoints.
Validated on the nuScenes dataset, MagicDrive3D generates diverse, high-quality
3D driving scenes that support any-view rendering and enhance downstream tasks
like BEV segmentation. Our results demonstrate the framework's superior
performance, showcasing its transformative potential for autonomous driving
simulation and beyond.";Ruiyuan Gao<author:sep>Kai Chen<author:sep>Zhihao Li<author:sep>Lanqing Hong<author:sep>Zhenguo Li<author:sep>Qiang Xu;http://arxiv.org/pdf/2405.14475v1;cs.CV;;gaussian splatting
2405.14871v1;http://arxiv.org/abs/2405.14871v1;2024-05-23;NeRF-Casting: Improved View-Dependent Appearance with Consistent  Reflections;"Neural Radiance Fields (NeRFs) typically struggle to reconstruct and render
highly specular objects, whose appearance varies quickly with changes in
viewpoint. Recent works have improved NeRF's ability to render detailed
specular appearance of distant environment illumination, but are unable to
synthesize consistent reflections of closer content. Moreover, these techniques
rely on large computationally-expensive neural networks to model outgoing
radiance, which severely limits optimization and rendering speed. We address
these issues with an approach based on ray tracing: instead of querying an
expensive neural network for the outgoing view-dependent radiance at points
along each camera ray, our model casts reflection rays from these points and
traces them through the NeRF representation to render feature vectors which are
decoded into color using a small inexpensive network. We demonstrate that our
model outperforms prior methods for view synthesis of scenes containing shiny
objects, and that it is the only existing NeRF method that can synthesize
photorealistic specular appearance and reflections in real-world scenes, while
requiring comparable optimization time to current state-of-the-art view
synthesis models.";Dor Verbin<author:sep>Pratul P. Srinivasan<author:sep>Peter Hedman<author:sep>Ben Mildenhall<author:sep>Benjamin Attal<author:sep>Richard Szeliski<author:sep>Jonathan T. Barron;http://arxiv.org/pdf/2405.14871v1;cs.CV;Project page: http://nerf-casting.github.io;nerf
2405.14455v1;http://arxiv.org/abs/2405.14455v1;2024-05-23;TIGER: Text-Instructed 3D Gaussian Retrieval and Coherent Editing;"Editing objects within a scene is a critical functionality required across a
broad spectrum of applications in computer vision and graphics. As 3D Gaussian
Splatting (3DGS) emerges as a frontier in scene representation, the effective
modification of 3D Gaussian scenes has become increasingly vital. This process
entails accurately retrieve the target objects and subsequently performing
modifications based on instructions. Though available in pieces, existing
techniques mainly embed sparse semantics into Gaussians for retrieval, and rely
on an iterative dataset update paradigm for editing, leading to over-smoothing
or inconsistency issues. To this end, this paper proposes a systematic
approach, namely TIGER, for coherent text-instructed 3D Gaussian retrieval and
editing. In contrast to the top-down language grounding approach for 3D
Gaussians, we adopt a bottom-up language aggregation strategy to generate a
denser language embedded 3D Gaussians that supports open-vocabulary retrieval.
To overcome the over-smoothing and inconsistency issues in editing, we propose
a Coherent Score Distillation (CSD) that aggregates a 2D image editing
diffusion model and a multi-view diffusion model for score distillation,
producing multi-view consistent editing with much finer details. In various
experiments, we demonstrate that our TIGER is able to accomplish more
consistent and realistic edits than prior work.";Teng Xu<author:sep>Jiamin Chen<author:sep>Peng Chen<author:sep>Youjia Zhang<author:sep>Junqing Yu<author:sep>Wei Yang;http://arxiv.org/pdf/2405.14455v1;cs.CV;;
2405.14580v1;http://arxiv.org/abs/2405.14580v1;2024-05-23;LDM: Large Tensorial SDF Model for Textured Mesh Generation;"Previous efforts have managed to generate production-ready 3D assets from
text or images. However, these methods primarily employ NeRF or 3D Gaussian
representations, which are not adept at producing smooth, high-quality
geometries required by modern rendering pipelines. In this paper, we propose
LDM, a novel feed-forward framework capable of generating high-fidelity,
illumination-decoupled textured mesh from a single image or text prompts. We
firstly utilize a multi-view diffusion model to generate sparse multi-view
inputs from single images or text prompts, and then a transformer-based model
is trained to predict a tensorial SDF field from these sparse multi-view image
inputs. Finally, we employ a gradient-based mesh optimization layer to refine
this model, enabling it to produce an SDF field from which high-quality
textured meshes can be extracted. Extensive experiments demonstrate that our
method can generate diverse, high-quality 3D mesh assets with corresponding
decomposed RGB textures within seconds.";Rengan Xie<author:sep>Wenting Zheng<author:sep>Kai Huang<author:sep>Yizheng Chen<author:sep>Qi Wang<author:sep>Qi Ye<author:sep>Wei Chen<author:sep>Yuchi Huo;http://arxiv.org/pdf/2405.14580v1;cs.GR;;nerf
2405.14452v1;http://arxiv.org/abs/2405.14452v1;2024-05-23;JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field  Representation and Compression;"Neural Radiance Field (NeRF) excels in photo-realistically static scenes,
inspiring numerous efforts to facilitate volumetric videos. However, rendering
dynamic and long-sequence radiance fields remains challenging due to the
significant data required to represent volumetric videos. In this paper, we
propose a novel end-to-end joint optimization scheme of dynamic NeRF
representation and compression, called JointRF, thus achieving significantly
improved quality and compression efficiency against the previous methods.
Specifically, JointRF employs a compact residual feature grid and a coefficient
feature grid to represent the dynamic NeRF. This representation handles large
motions without compromising quality while concurrently diminishing temporal
redundancy. We also introduce a sequential feature compression subnetwork to
further reduce spatial-temporal redundancy. Finally, the representation and
compression subnetworks are end-to-end trained combined within the JointRF.
Extensive experiments demonstrate that JointRF can achieve superior compression
performance across various datasets.";Zihan Zheng<author:sep>Houqiang Zhong<author:sep>Qiang Hu<author:sep>Xiaoyun Zhang<author:sep>Li Song<author:sep>Ya Zhang<author:sep>Yanfeng Wang;http://arxiv.org/pdf/2405.14452v1;cs.CV;8 pages, 5 figures;nerf
2405.14959v1;http://arxiv.org/abs/2405.14959v1;2024-05-23;EvGGS: A Collaborative Learning Framework for Event-based Generalizable  Gaussian Splatting;"Event cameras offer promising advantages such as high dynamic range and low
latency, making them well-suited for challenging lighting conditions and
fast-moving scenarios. However, reconstructing 3D scenes from raw event streams
is difficult because event data is sparse and does not carry absolute color
information. To release its potential in 3D reconstruction, we propose the
first event-based generalizable 3D reconstruction framework, called EvGGS,
which reconstructs scenes as 3D Gaussians from only event input in a
feedforward manner and can generalize to unseen cases without any retraining.
This framework includes a depth estimation module, an intensity reconstruction
module, and a Gaussian regression module. These submodules connect in a
cascading manner, and we collaboratively train them with a designed joint loss
to make them mutually promote. To facilitate related studies, we build a novel
event-based 3D dataset with various material objects and calibrated labels of
grayscale images, depth maps, camera poses, and silhouettes. Experiments show
models that have jointly trained significantly outperform those trained
individually. Our approach performs better than all baselines in reconstruction
quality, and depth/intensity predictions with satisfactory rendering speed.";Jiaxu Wang<author:sep>Junhao He<author:sep>Ziyi Zhang<author:sep>Mingyuan Sun<author:sep>Jingkai Sun<author:sep>Renjing Xu;http://arxiv.org/pdf/2405.14959v1;cs.CV;;gaussian splatting
2405.14276v2;http://arxiv.org/abs/2405.14276v2;2024-05-23;D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup;"Over the past years, we have observed an abundance of approaches for modeling
dynamic 3D scenes using Gaussian Splatting (GS). Such solutions use GS to
represent the scene's structure and the neural network to model dynamics. Such
approaches allow fast rendering and extracting each element of such a dynamic
scene. However, modifying such objects over time is challenging. SC-GS (Sparse
Controlled Gaussian Splatting) enhanced with Deformed Control Points partially
solves this issue. However, this approach necessitates selecting elements that
need to be kept fixed, as well as centroids that should be adjusted throughout
editing. Moreover, this task poses additional difficulties regarding the
re-productivity of such editing. To address this, we propose Dynamic
Multi-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspired
representation of dynamic GS. Additionally, we propose a strategy of linking
parameterized Gaussian splats, forming a Triangle Soup with the estimated mesh.
Consequently, we can separately construct new trajectories for the 3D objects
composing the scene. Thus, we can make the scene's dynamic editable over time
or while maintaining partial dynamics.";Joanna Waczyńska<author:sep>Piotr Borycki<author:sep>Joanna Kaleta<author:sep>Sławomir Tadeja<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2405.14276v2;cs.CV;;gaussian splatting
2405.14342v2;http://arxiv.org/abs/2405.14342v2;2024-05-23;RoGS: Large Scale Road Surface Reconstruction based on 2D Gaussian  Splatting;"Road surface reconstruction plays a crucial role in autonomous driving, which
can be used for road lane perception and autolabeling tasks. Recently,
mesh-based road surface reconstruction algorithms show promising reconstruction
results. However, these mesh-based methods suffer from slow speed and poor
rendering quality. In contrast, the 3D Gaussian Splatting (3DGS) shows superior
rendering speed and quality. Although 3DGS employs explicit Gaussian spheres to
represent the scene, it lacks the ability to directly represent the geometric
information of the scene. To address this limitation, we propose a novel
large-scale road surface reconstruction approach based on 2D Gaussian Splatting
(2DGS), named RoGS. The geometric shape of the road is explicitly represented
using 2D Gaussian surfels, where each surfel stores color, semantics, and
geometric information. Compared to Gaussian spheres, the Gaussian surfels
aligns more closely with the physical reality of the road. Distinct from
previous initialization methods that rely on point clouds for Gaussian spheres,
we introduce a trajectory-based initialization for Gaussian surfels. Thanks to
the explicit representation of the Gaussian surfels and a good initialization,
our method achieves a significant acceleration while improving reconstruction
quality. We achieve excellent results in reconstruction of roads surfaces in a
variety of challenging real-world scenes.";Zhiheng Feng<author:sep>Wenhua Wu<author:sep>Hesheng Wang;http://arxiv.org/pdf/2405.14342v2;cs.CV;;gaussian splatting
2405.14824v1;http://arxiv.org/abs/2405.14824v1;2024-05-23;Camera Relocalization in Shadow-free Neural Radiance Fields;"Camera relocalization is a crucial problem in computer vision and robotics.
Recent advancements in neural radiance fields (NeRFs) have shown promise in
synthesizing photo-realistic images. Several works have utilized NeRFs for
refining camera poses, but they do not account for lighting changes that can
affect scene appearance and shadow regions, causing a degraded pose
optimization process. In this paper, we propose a two-staged pipeline that
normalizes images with varying lighting and shadow conditions to improve camera
relocalization. We implement our scene representation upon a hash-encoded NeRF
which significantly boosts up the pose optimization process. To account for the
noisy image gradient computing problem in grid-based NeRFs, we further propose
a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient
averaging technique to smoothen the process. Experimental results on several
datasets with varying lighting conditions demonstrate that our method achieves
state-of-the-art results in camera relocalization under varying lighting
conditions. Code and data will be made publicly available.";Shiyao Xu<author:sep>Caiyun Liu<author:sep>Yuantao Chen<author:sep>Zhenxin Zhu<author:sep>Zike Yan<author:sep>Yongliang Shi<author:sep>Hao Zhao<author:sep>Guyue Zhou;http://arxiv.org/pdf/2405.14824v1;cs.CV;"Accepted by ICRA 2024. 8 pages, 5 figures, 3 tables. Codes and
  dataset: https://github.com/hnrna/ShadowfreeNeRF-CameraReloc";nerf
2405.14847v1;http://arxiv.org/abs/2405.14847v1;2024-05-23;Neural Directional Encoding for Efficient and Accurate View-Dependent  Appearance Modeling;"Novel-view synthesis of specular objects like shiny metals or glossy paints
remains a significant challenge. Not only the glossy appearance but also global
illumination effects, including reflections of other objects in the
environment, are critical components to faithfully reproduce a scene. In this
paper, we present Neural Directional Encoding (NDE), a view-dependent
appearance encoding of neural radiance fields (NeRF) for rendering specular
objects. NDE transfers the concept of feature-grid-based spatial encoding to
the angular domain, significantly improving the ability to model high-frequency
angular signals. In contrast to previous methods that use encoding functions
with only angular input, we additionally cone-trace spatial features to obtain
a spatially varying directional encoding, which addresses the challenging
interreflection effects. Extensive experiments on both synthetic and real
datasets show that a NeRF model with NDE (1) outperforms the state of the art
on view synthesis of specular objects, and (2) works with small networks to
allow fast (real-time) inference. The project webpage and source code are
available at: \url{https://lwwu2.github.io/nde/}.";Liwen Wu<author:sep>Sai Bi<author:sep>Zexiang Xu<author:sep>Fujun Luan<author:sep>Kai Zhang<author:sep>Iliyan Georgiev<author:sep>Kalyan Sunkavalli<author:sep>Ravi Ramamoorthi;http://arxiv.org/pdf/2405.14847v1;cs.CV;Accepted to CVPR 2024;nerf
2405.13694v1;http://arxiv.org/abs/2405.13694v1;2024-05-22;Gaussian Time Machine: A Real-Time Rendering Methodology for  Time-Variant Appearances;"Recent advancements in neural rendering techniques have significantly
enhanced the fidelity of 3D reconstruction. Notably, the emergence of 3D
Gaussian Splatting (3DGS) has marked a significant milestone by adopting a
discrete scene representation, facilitating efficient training and real-time
rendering. Several studies have successfully extended the real-time rendering
capability of 3DGS to dynamic scenes. However, a challenge arises when training
images are captured under vastly differing weather and lighting conditions.
This scenario poses a challenge for 3DGS and its variants in achieving accurate
reconstructions. Although NeRF-based methods (NeRF-W, CLNeRF) have shown
promise in handling such challenging conditions, their computational demands
hinder real-time rendering capabilities. In this paper, we present Gaussian
Time Machine (GTM) which models the time-dependent attributes of Gaussian
primitives with discrete time embedding vectors decoded by a lightweight
Multi-Layer-Perceptron(MLP). By adjusting the opacity of Gaussian primitives,
we can reconstruct visibility changes of objects. We further propose a
decomposed color model for improved geometric consistency. GTM achieved
state-of-the-art rendering fidelity on 3 datasets and is 100 times faster than
NeRF-based counterparts in rendering. Moreover, GTM successfully disentangles
the appearance changes and renders smooth appearance interpolation.";Licheng Shen<author:sep>Ho Ngai Chow<author:sep>Lingyun Wang<author:sep>Tong Zhang<author:sep>Mengqiu Wang<author:sep>Yuxing Han;http://arxiv.org/pdf/2405.13694v1;cs.CV;14 pages, 6 figures;gaussian splatting<tag:sep>nerf
2405.13943v1;http://arxiv.org/abs/2405.13943v1;2024-05-22;DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D  Reconstruction Via Gaussian Consensus;"The recent advances in 3D Gaussian Splatting (3DGS) show promising results on
the novel view synthesis (NVS) task. With its superior rendering performance
and high-fidelity rendering quality, 3DGS is excelling at its previous NeRF
counterparts. The most recent 3DGS method focuses either on improving the
instability of rendering efficiency or reducing the model size. On the other
hand, the training efficiency of 3DGS on large-scale scenes has not gained much
attention. In this work, we propose DoGaussian, a method that trains 3DGS
distributedly. Our method first decomposes a scene into K blocks and then
introduces the Alternating Direction Method of Multipliers (ADMM) into the
training procedure of 3DGS. During training, our DoGaussian maintains one
global 3DGS model on the master node and K local 3DGS models on the slave
nodes. The K local 3DGS models are dropped after training and we only query the
global 3DGS model during inference. The training time is reduced by scene
decomposition, and the training convergence and stability are guaranteed
through the consensus on the shared 3D Gaussians. Our method accelerates the
training of 3DGS by 6+ times when evaluated on large-scale scenes while
concurrently achieving state-of-the-art rendering quality. Our project page is
available at https://aibluefisher.github.io/DoGaussian.";Yu Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2405.13943v1;cs.CV;;gaussian splatting<tag:sep>nerf
2405.12477v1;http://arxiv.org/abs/2405.12477v1;2024-05-21;Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery;"Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human
reconstruction, it primarily relies on 2D pixel-level supervision, overlooking
the geometric complexity and topological relationships of different body parts.
To address this gap, we introduce the Hierarchical Graph Human Gaussian Control
(HUGS) framework for achieving high-fidelity 3D human reconstruction. Our
approach involves leveraging explicitly semantic priors of body parts to ensure
the consistency of geometric topology, thereby enabling the capture of the
complex geometrical and topological associations among body parts.
Additionally, we disentangle high-frequency features from global human features
to refine surface details in body parts. Extensive experiments demonstrate that
our method exhibits superior performance in human body reconstruction,
particularly in enhancing surface details and accurately reconstructing body
part junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/.";Hongsheng Wang<author:sep>Weiyue Zhang<author:sep>Sihao Liu<author:sep>Xinrui Zhou<author:sep>Shengyu Zhang<author:sep>Fei Wu<author:sep>Feng Lin;http://arxiv.org/pdf/2405.12477v1;cs.CV;;gaussian splatting
2405.12663v1;http://arxiv.org/abs/2405.12663v1;2024-05-21;LAGA: Layered 3D Avatar Generation and Customization via Gaussian  Splatting;"Creating and customizing a 3D clothed avatar from textual descriptions is a
critical and challenging task. Traditional methods often treat the human body
and clothing as inseparable, limiting users' ability to freely mix and match
garments. In response to this limitation, we present LAyered Gaussian Avatar
(LAGA), a carefully designed framework enabling the creation of high-fidelity
decomposable avatars with diverse garments. By decoupling garments from avatar,
our framework empowers users to conviniently edit avatars at the garment level.
Our approach begins by modeling the avatar using a set of Gaussian points
organized in a layered structure, where each layer corresponds to a specific
garment or the human body itself. To generate high-quality garments for each
layer, we introduce a coarse-to-fine strategy for diverse garment generation
and a novel dual-SDS loss function to maintain coherence between the generated
garments and avatar components, including the human body and other garments.
Moreover, we introduce three regularization losses to guide the movement of
Gaussians for garment transfer, allowing garments to be freely transferred to
various avatars. Extensive experimentation demonstrates that our approach
surpasses existing methods in the generation of 3D clothed humans.";Jia Gong<author:sep>Shenyu Ji<author:sep>Lin Geng Foo<author:sep>Kang Chen<author:sep>Hossein Rahmani<author:sep>Jun Liu;http://arxiv.org/pdf/2405.12663v1;cs.GR;;
2405.12728v1;http://arxiv.org/abs/2405.12728v1;2024-05-21;Leveraging Neural Radiance Fields for Pose Estimation of an Unknown  Space Object during Proximity Operations;"We address the estimation of the 6D pose of an unknown target spacecraft
relative to a monocular camera, a key step towards the autonomous rendezvous
and proximity operations required by future Active Debris Removal missions. We
present a novel method that enables an ""off-the-shelf"" spacecraft pose
estimator, which is supposed to known the target CAD model, to be applied on an
unknown target. Our method relies on an in-the wild NeRF, i.e., a Neural
Radiance Field that employs learnable appearance embeddings to represent
varying illumination conditions found in natural scenes. We train the NeRF
model using a sparse collection of images that depict the target, and in turn
generate a large dataset that is diverse both in terms of viewpoint and
illumination. This dataset is then used to train the pose estimation network.
We validate our method on the Hardware-In-the-Loop images of SPEED+ that
emulate lighting conditions close to those encountered on orbit. We demonstrate
that our method successfully enables the training of an off-the-shelf
spacecraft pose estimation network from a sparse set of images. Furthermore, we
show that a network trained using our method performs similarly to a model
trained on synthetic images generated using the CAD model of the target.";Antoine Legrand<author:sep>Renaud Detry<author:sep>Christophe De Vleeschouwer;http://arxiv.org/pdf/2405.12728v1;cs.CV;;nerf
2405.12806v1;http://arxiv.org/abs/2405.12806v1;2024-05-21;MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video;"Single-view clothed human reconstruction holds a central position in virtual
reality applications, especially in contexts involving intricate human motions.
It presents notable challenges in achieving realistic clothing deformation.
Current methodologies often overlook the influence of motion on surface
deformation, resulting in surfaces lacking the constraints imposed by global
motion. To overcome these limitations, we introduce an innovative framework,
Motion-Based 3D Clothed Humans Synthesis (MOSS), which employs kinematic
information to achieve motion-aware Gaussian split on the human surface. Our
framework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS)
and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher
distribution to propagate global motion across the body surface. The density
and rotation factors of this distribution explicitly control the Gaussians,
thereby enhancing the realism of the reconstructed surface. Additionally, to
address local occlusions in single-view, based on KGAS, UID identifies
significant surfaces, and geometric reconstruction is performed to compensate
for these deformations. Experimental results demonstrate that MOSS achieves
state-of-the-art visual quality in 3D clothed human synthesis from monocular
videos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94%
and 16.75% in LPIPS* respectively. Codes are available at
https://wanghongsheng01.github.io/MOSS/.";Hongsheng Wang<author:sep>Xiang Cai<author:sep>Xi Sun<author:sep>Jinhong Yue<author:sep>Shengyu Zhang<author:sep>Feng Lin<author:sep>Fei Wu;http://arxiv.org/pdf/2405.12806v1;cs.CV;;gaussian splatting<tag:sep>nerf
2405.12155v1;http://arxiv.org/abs/2405.12155v1;2024-05-20;Embracing Radiance Field Rendering in 6G: Over-the-Air Training and  Inference with 3D Contents;"The efficient representation, transmission, and reconstruction of
three-dimensional (3D) contents are becoming increasingly important for
sixth-generation (6G) networks that aim to merge virtual and physical worlds
for offering immersive communication experiences. Neural radiance field (NeRF)
and 3D Gaussian splatting (3D-GS) have recently emerged as two promising 3D
representation techniques based on radiance field rendering, which are able to
provide photorealistic rendering results for complex scenes. Therefore,
embracing NeRF and 3D-GS in 6G networks is envisioned to be a prominent
solution to support emerging 3D applications with enhanced quality of
experience. This paper provides a comprehensive overview on the integration of
NeRF and 3D-GS in 6G. First, we review the basics of the radiance field
rendering techniques, and highlight their applications and implementation
challenges over wireless networks. Next, we consider the over-the-air training
of NeRF and 3D-GS models over wireless networks by presenting various learning
techniques. We particularly focus on the federated learning design over a
hierarchical device-edge-cloud architecture. Then, we discuss three practical
rendering architectures of NeRF and 3D-GS models at wireless network edge. We
provide model compression approaches to facilitate the transmission of radiance
field models, and present rendering acceleration approaches and joint
computation and communication designs to enhance the rendering efficiency. In
particular, we propose a new semantic communication enabled 3D content
transmission design, in which the radiance field models are exploited as the
semantic knowledge base to reduce the communication overhead for distributed
inference. Furthermore, we present the utilization of radiance field rendering
in wireless applications like radio mapping and radio imaging.";Guanlin Wu<author:sep>Zhonghao Lyu<author:sep>Juyong Zhang<author:sep>Jie Xu;http://arxiv.org/pdf/2405.12155v1;cs.IT;15 pages,7 figures;gaussian splatting<tag:sep>nerf
2405.12420v1;http://arxiv.org/abs/2405.12420v1;2024-05-20;GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and  Texture Details;"Traditional 3D garment creation is labor-intensive, involving sketching,
modeling, UV mapping, and texturing, which are time-consuming and costly.
Recent advances in diffusion-based generative models have enabled new
possibilities for 3D garment generation from text prompts, images, and videos.
However, existing methods either suffer from inconsistencies among multi-view
images or require additional processes to separate cloth from the underlying
human model. In this paper, we propose GarmentDreamer, a novel method that
leverages 3D Gaussian Splatting (GS) as guidance to generate wearable,
simulation-ready 3D garment meshes from text prompts. In contrast to using
multi-view images directly predicted by generative models as guidance, our 3DGS
guidance ensures consistent optimization in both garment deformation and
texture synthesis. Our method introduces a novel garment augmentation module,
guided by normal and RGBA information, and employs implicit Neural Texture
Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate
diverse geometric and texture details. We validate the effectiveness of our
approach through comprehensive qualitative and quantitative experiments,
showcasing the superior performance of GarmentDreamer over state-of-the-art
alternatives. Our project page is available at:
https://xuan-li.github.io/GarmentDreamerDemo/.";Boqian Li<author:sep>Xuan Li<author:sep>Ying Jiang<author:sep>Tianyi Xie<author:sep>Feng Gao<author:sep>Huamin Wang<author:sep>Yin Yang<author:sep>Chenfanfu Jiang;http://arxiv.org/pdf/2405.12420v1;cs.CV;;gaussian splatting
2405.12369v2;http://arxiv.org/abs/2405.12369v2;2024-05-20;AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field;"3D Gaussian Splatting (3DGS) has recently advanced radiance field
reconstruction by offering superior capabilities for novel view synthesis and
real-time rendering speed. However, its strategy of blending optimization and
adaptive density control might lead to sub-optimal results; it can sometimes
yield noisy geometry and blurry artifacts due to prioritizing optimizing large
Gaussians at the cost of adequately densifying smaller ones. To address this,
we introduce AtomGS, consisting of Atomized Proliferation and Geometry-Guided
Optimization. The Atomized Proliferation constrains ellipsoid Gaussians of
various sizes into more uniform-sized Atom Gaussians. The strategy enhances the
representation of areas with fine features by placing greater emphasis on
densification in accordance with scene details. In addition, we proposed a
Geometry-Guided Optimization approach that incorporates an Edge-Aware Normal
Loss. This optimization method effectively smooths flat surfaces while
preserving intricate details. Our evaluation shows that AtomGS outperforms
existing state-of-the-art methods in rendering quality. Additionally, it
achieves competitive accuracy in geometry reconstruction and offers a
significant improvement in training speed over other SDF-based methods. More
interactive demos can be found in our website
(https://rongliu-leo.github.io/AtomGS/).";Rong Liu<author:sep>Rui Xu<author:sep>Yue Hu<author:sep>Meida Chen<author:sep>Andrew Feng;http://arxiv.org/pdf/2405.12369v2;cs.CV;;gaussian splatting
2405.12069v2;http://arxiv.org/abs/2405.12069v2;2024-05-20;Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with  Anchor Gaussian Guided Texture Warping;"By equipping the most recent 3D Gaussian Splatting representation with head
3D morphable models (3DMM), existing methods manage to create head avatars with
high fidelity. However, most existing methods only reconstruct a head without
the body, substantially limiting their application scenarios. We found that
naively applying Gaussians to model the clothed chest and shoulders tends to
result in blurry reconstruction and noisy floaters under novel poses. This is
because of the fundamental limitation of Gaussians and point clouds -- each
Gaussian or point can only have a single directional radiance without spatial
variance, therefore an unnecessarily large number of them is required to
represent complicated spatially varying texture, even for simple geometry. In
contrast, we propose to model the body part with a neural texture that consists
of coarse and pose-dependent fine colors. To properly render the body texture
for each view and pose without accurate geometry nor UV mapping, we optimize
another sparse set of Gaussians as anchors that constrain the neural warping
field that maps image plane coordinates to the texture space. We demonstrate
that Gaussian Head & Shoulders can fit the high-frequency details on the
clothed upper body with high fidelity and potentially improve the accuracy and
fidelity of the head region. We evaluate our method with casual phone-captured
and internet videos and show our method archives superior reconstruction
quality and robustness in both self and cross reenactment tasks. To fully
utilize the efficient rendering speed of Gaussian splatting, we additionally
propose an accelerated inference method of our trained model without
Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of
around 130 FPS for any subjects.";Tianhao Wu<author:sep>Jing Yang<author:sep>Zhilin Guo<author:sep>Jingyi Wan<author:sep>Fangcheng Zhong<author:sep>Cengiz Oztireli;http://arxiv.org/pdf/2405.12069v2;cs.CV;Project Page: https://gaussian-head-shoulders.netlify.app/;gaussian splatting
2405.12110v1;http://arxiv.org/abs/2405.12110v1;2024-05-20;CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization;"3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D
Gaussians to represent a scene. With sparse training views, 3DGS easily suffers
from overfitting, negatively impacting the reconstruction quality. This paper
introduces a new co-regularization perspective for improving sparse-view 3DGS.
When training two 3D Gaussian radiance fields with the same sparse views of a
scene, we observe that the two radiance fields exhibit \textit{point
disagreement} and \textit{rendering disagreement} that can unsupervisedly
predict reconstruction quality, stemming from the sampling implementation in
densification. We further quantify the point disagreement and rendering
disagreement by evaluating the registration between Gaussians' point
representations and calculating differences in their rendered pixels. The
empirical study demonstrates the negative correlation between the two
disagreements and accurate reconstruction, which allows us to identify
inaccurate reconstruction without accessing ground-truth information. Based on
the study, we propose CoR-GS, which identifies and suppresses inaccurate
reconstruction based on the two disagreements: (\romannumeral1) Co-pruning
considers Gaussians that exhibit high point disagreement in inaccurate
positions and prunes them. (\romannumeral2) Pseudo-view co-regularization
considers pixels that exhibit high rendering disagreement are inaccurately
rendered and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and
Blender demonstrate that CoR-GS effectively regularizes the scene geometry,
reconstructs the compact representations, and achieves state-of-the-art novel
view synthesis quality under sparse training views.";Jiawei Zhang<author:sep>Jiahe Li<author:sep>Xiaohan Yu<author:sep>Lei Huang<author:sep>Lin Gu<author:sep>Jin Zheng<author:sep>Xiao Bai;http://arxiv.org/pdf/2405.12110v1;cs.CV;Project page: https://jiaw-z.github.io/CoR-GS/;gaussian splatting<tag:sep>nerf
2405.11921v1;http://arxiv.org/abs/2405.11921v1;2024-05-20;MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror  Reflections;"3D Gaussian Splatting showcases notable advancements in photo-realistic and
real-time novel view synthesis. However, it faces challenges in modeling mirror
reflections, which exhibit substantial appearance variations from different
viewpoints. To tackle this problem, we present MirrorGaussian, the first method
for mirror scene reconstruction with real-time rendering based on 3D Gaussian
Splatting. The key insight is grounded on the mirror symmetry between the
real-world space and the virtual mirror space. We introduce an intuitive
dual-rendering strategy that enables differentiable rasterization of both the
real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the
former about the mirror plane. All 3D Gaussians are jointly optimized with the
mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality
and real-time rendering in scenes with mirrors, empowering scene editing like
adding new mirrors and objects. Comprehensive experiments on multiple datasets
demonstrate that our approach significantly outperforms existing methods,
achieving state-of-the-art results. Project page:
https://mirror-gaussian.github.io/.";Jiayue Liu<author:sep>Xiao Tang<author:sep>Freeman Cheng<author:sep>Roy Yang<author:sep>Zhihao Li<author:sep>Jianzhuang Liu<author:sep>Yi Huang<author:sep>Jiaqi Lin<author:sep>Shiyong Liu<author:sep>Xiaofei Wu<author:sep>Songcen Xu<author:sep>Chun Yuan;http://arxiv.org/pdf/2405.11921v1;cs.CV;;gaussian splatting
2405.12057v1;http://arxiv.org/abs/2405.12057v1;2024-05-20;NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo;"In this work we present a novel multi-view photometric stereo (PS) method.
Like many works in 3D reconstruction we are leveraging neural shape
representations and learnt renderers. However, our work differs from the
state-of-the-art multi-view PS methods such as PS-NeRF or SuperNormal we
explicity leverage per-pixel intensity renderings rather than relying mainly on
estimated normals.
  We model point light attenuation and explicitly raytrace cast shadows in
order to best approximate each points incoming radiance. This is used as input
to a fully neural material renderer that uses minimal prior assumptions and it
is jointly optimised with the surface. Finally, estimated normal and
segmentation maps can also incorporated in order to maximise the surface
accuracy.
  Our method is among the first to outperform the classical approach of
DiLiGenT-MV and achieves average 0.2mm Chamfer distance for objects imaged at
approx 1.5m distance away with approximate 400x400 resolution. Moreover, we
show robustness to poor normals in low light count scenario, achieving 0.27mm
Chamfer distance when pixel rendering is used instead of estimated normals.";Fotios Logothetis<author:sep>Ignas Budvytis<author:sep>Roberto Cipolla;http://arxiv.org/pdf/2405.12057v1;cs.CV;;nerf
2405.12218v1;http://arxiv.org/abs/2405.12218v1;2024-05-20;Fast Generalizable Gaussian Splatting Reconstruction from Multi-View  Stereo;"We present MVSGaussian, a new generalizable 3D Gaussian representation
approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct
unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware
Gaussian representations and decode them into Gaussian parameters. 2) To
further enhance performance, we propose a hybrid Gaussian rendering that
integrates an efficient volume rendering design for novel view synthesis. 3) To
support fast fine-tuning for specific scenes, we introduce a multi-view
geometric consistent aggregation strategy to effectively aggregate the point
clouds generated by the generalizable model, serving as the initialization for
per-scene optimization. Compared with previous generalizable NeRF-based
methods, which typically require minutes of fine-tuning and seconds of
rendering per image, MVSGaussian achieves real-time rendering with better
synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian
achieves better view synthesis with less training computational cost. Extensive
experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples
datasets validate that MVSGaussian attains state-of-the-art performance with
convincing generalizability, real-time rendering speed, and fast per-scene
optimization.";Tianqi Liu<author:sep>Guangcong Wang<author:sep>Shoukang Hu<author:sep>Liao Shen<author:sep>Xinyi Ye<author:sep>Yuhang Zang<author:sep>Zhiguo Cao<author:sep>Wei Li<author:sep>Ziwei Liu;http://arxiv.org/pdf/2405.12218v1;cs.CV;Project page: https://mvsgaussian.github.io/;gaussian splatting<tag:sep>nerf
2405.11541v1;http://arxiv.org/abs/2405.11541v1;2024-05-19;R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless  Environments;"Recently, ray tracing has gained renewed interest with the advent of
Reflective Intelligent Surfaces (RIS) technology, a key enabler of 6G wireless
communications due to its capability of intelligent manipulation of
electromagnetic waves. However, accurately modeling RIS-enabled wireless
environments poses significant challenges due to the complex variations caused
by various environmental factors and the mobility of RISs. In this paper, we
propose a novel modeling approach using Neural Radiance Fields (NeRF) to
characterize the dynamics of electromagnetic fields in such environments. Our
method utilizes NeRF-based ray tracing to intuitively capture and visualize the
complex dynamics of signal propagation, effectively modeling the complete
signal pathways from the transmitter to the RIS, and from the RIS to the
receiver. This two-stage process accurately characterizes multiple complex
transmission paths, enhancing our understanding of signal behavior in
real-world scenarios. Our approach predicts the signal field for any specified
RIS placement and receiver location, facilitating efficient RIS deployment.
Experimental evaluations using both simulated and real-world data validate the
significant benefits of our methodology.";Huiying Yang<author:sep>Zihan Jin<author:sep>Chenhao Wu<author:sep>Rujing Xiong<author:sep>Robert Caiming Qiu<author:sep>Zenan Ling;http://arxiv.org/pdf/2405.11541v1;cs.IT;;nerf
2405.11629v1;http://arxiv.org/abs/2405.11629v1;2024-05-19;Searching Realistic-Looking Adversarial Objects For Autonomous Driving  Systems;"Numerous studies on adversarial attacks targeting self-driving policies fail
to incorporate realistic-looking adversarial objects, limiting real-world
applicability. Building upon prior research that facilitated the transition of
adversarial objects from simulations to practical applications, this paper
discusses a modified gradient-based texture optimization method to discover
realistic-looking adversarial objects. While retaining the core architecture
and techniques of the prior research, the proposed addition involves an entity
termed the 'Judge'. This agent assesses the texture of a rendered object,
assigning a probability score reflecting its realism. This score is integrated
into the loss function to encourage the NeRF object renderer to concurrently
learn realistic and adversarial textures. The paper analyzes four strategies
for developing a robust 'Judge': 1) Leveraging cutting-edge vision-language
models. 2) Fine-tuning open-sourced vision-language models. 3) Pretraining
neurosymbolic systems. 4) Utilizing traditional image processing techniques.
Our findings indicate that strategies 1) and 4) yield less reliable outcomes,
pointing towards strategies 2) or 3) as more promising directions for future
research.";Shengxiang Sun<author:sep>Shenzhe Zhu;http://arxiv.org/pdf/2405.11629v1;cs.CV;;nerf
2405.11129v1;http://arxiv.org/abs/2405.11129v1;2024-05-18;MotionGS : Compact Gaussian Splatting SLAM by Motion Filter;"With their high-fidelity scene representation capability, the attention of
SLAM field is deeply attracted by the Neural Radiation Field (NeRF) and 3D
Gaussian Splatting (3DGS). Recently, there has been a Surge in NeRF-based SLAM,
while 3DGS-based SLAM is sparse. A novel 3DGS-based SLAM approach with a fusion
of deep visual feature, dual keyframe selection and 3DGS is presented in this
paper. Compared with the existing methods, the proposed selectively tracking is
achieved by feature extraction and motion filter on each frame. The joint
optimization of pose and 3D Gaussian runs through the entire mapping process.
Additionally, the coarse-to-fine pose estimation and compact Gaussian scene
representation are implemented by dual keyfeature selection and novel loss
functions. Experimental results demonstrate that the proposed algorithm not
only outperforms the existing methods in tracking and mapping, but also has
less memory usage.";Xinli Guo<author:sep>Peng Han<author:sep>Weidong Zhang<author:sep>Hongtian Chen;http://arxiv.org/pdf/2405.11129v1;cs.CV;;gaussian splatting<tag:sep>nerf
2405.11252v1;http://arxiv.org/abs/2405.11252v1;2024-05-18;Dreamer XL: Towards High-Resolution Text-to-3D Generation via Trajectory  Score Matching;"In this work, we propose a novel Trajectory Score Matching (TSM) method that
aims to solve the pseudo ground truth inconsistency problem caused by the
accumulated error in Interval Score Matching (ISM) when using the Denoising
Diffusion Implicit Models (DDIM) inversion process. Unlike ISM which adopts the
inversion process of DDIM to calculate on a single path, our TSM method
leverages the inversion process of DDIM to generate two paths from the same
starting point for calculation. Since both paths start from the same starting
point, TSM can reduce the accumulated error compared to ISM, thus alleviating
the problem of pseudo ground truth inconsistency. TSM enhances the stability
and consistency of the model's generated paths during the distillation process.
We demonstrate this experimentally and further show that ISM is a special case
of TSM. Furthermore, to optimize the current multi-stage optimization process
from high-resolution text to 3D generation, we adopt Stable Diffusion XL for
guidance. In response to the issues of abnormal replication and splitting
caused by unstable gradients during the 3D Gaussian splatting process when
using Stable Diffusion XL, we propose a pixel-by-pixel gradient clipping
method. Extensive experiments show that our model significantly surpasses the
state-of-the-art models in terms of visual quality and performance. Code:
\url{https://github.com/xingy038/Dreamer-XL}.";Xingyu Miao<author:sep>Haoran Duan<author:sep>Varun Ojha<author:sep>Jun Song<author:sep>Tejal Shah<author:sep>Yang Long<author:sep>Rajiv Ranjan;http://arxiv.org/pdf/2405.11252v1;cs.CV;;gaussian splatting
2405.10508v1;http://arxiv.org/abs/2405.10508v1;2024-05-17;ART3D: 3D Gaussian Splatting for Text-Guided Artistic Scenes Generation;"In this paper, we explore the existing challenges in 3D artistic scene
generation by introducing ART3D, a novel framework that combines diffusion
models and 3D Gaussian splatting techniques. Our method effectively bridges the
gap between artistic and realistic images through an innovative image semantic
transfer algorithm. By leveraging depth information and an initial artistic
image, we generate a point cloud map, addressing domain differences.
Additionally, we propose a depth consistency module to enhance 3D scene
consistency. Finally, the 3D scene serves as initial points for optimizing
Gaussian splats. Experimental results demonstrate ART3D's superior performance
in both content and structural consistency metrics when compared to existing
methods. ART3D significantly advances the field of AI in art creation by
providing an innovative solution for generating high-quality 3D artistic
scenes.";Pengzhi Li<author:sep>Chengshuai Tang<author:sep>Qinxuan Huang<author:sep>Zhiheng Li;http://arxiv.org/pdf/2405.10508v1;cs.CV;Accepted at CVPR 2024 Workshop on AI3DG;gaussian splatting
2405.11021v1;http://arxiv.org/abs/2405.11021v1;2024-05-17;Photorealistic 3D Urban Scene Reconstruction and Point Cloud Extraction  using Google Earth Imagery and Gaussian Splatting;"3D urban scene reconstruction and modelling is a crucial research area in
remote sensing with numerous applications in academia, commerce, industry, and
administration. Recent advancements in view synthesis models have facilitated
photorealistic 3D reconstruction solely from 2D images. Leveraging Google Earth
imagery, we construct a 3D Gaussian Splatting model of the Waterloo region
centered on the University of Waterloo and are able to achieve view-synthesis
results far exceeding previous 3D view-synthesis results based on neural
radiance fields which we demonstrate in our benchmark. Additionally, we
retrieved the 3D geometry of the scene using the 3D point cloud extracted from
the 3D Gaussian Splatting model which we benchmarked against our Multi-
View-Stereo dense reconstruction of the scene, thereby reconstructing both the
3D geometry and photorealistic lighting of the large-scale urban scene through
3D Gaussian Splatting";Kyle Gao<author:sep>Dening Lu<author:sep>Hongjie He<author:sep>Linlin Xu<author:sep>Jonathan Li;http://arxiv.org/pdf/2405.11021v1;cs.CV;;gaussian splatting
2405.10142v1;http://arxiv.org/abs/2405.10142v1;2024-05-16;GS-Planner: A Gaussian-Splatting-based Planning Framework for Active  High-Fidelity Reconstruction;"Active reconstruction technique enables robots to autonomously collect scene
data for full coverage, relieving users from tedious and time-consuming data
capturing process. However, designed based on unsuitable scene representations,
existing methods show unrealistic reconstruction results or the inability of
online quality evaluation. Due to the recent advancements in explicit radiance
field technology, online active high-fidelity reconstruction has become
achievable. In this paper, we propose GS-Planner, a planning framework for
active high-fidelity reconstruction using 3D Gaussian Splatting. With
improvement on 3DGS to recognize unobserved regions, we evaluate the
reconstruction quality and completeness of 3DGS map online to guide the robot.
Then we design a sampling-based active reconstruction strategy to explore the
unobserved areas and improve the reconstruction geometric and textural quality.
To establish a complete robot active reconstruction system, we choose quadrotor
as the robotic platform for its high agility. Then we devise a safety
constraint with 3DGS to generate executable trajectories for quadrotor
navigation in the 3DGS map. To validate the effectiveness of our method, we
conduct extensive experiments and ablation studies in highly realistic
simulation scenes.";Rui Jin<author:sep>Yuman Gao<author:sep>Haojian Lu<author:sep>Fei Gao;http://arxiv.org/pdf/2405.10142v1;cs.RO;;gaussian splatting
2405.10255v1;http://arxiv.org/abs/2405.10255v1;2024-05-16;When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks  via Multi-modal Large Language Models;"As large language models (LLMs) evolve, their integration with 3D spatial
data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for
understanding and interacting with physical spaces. This survey provides a
comprehensive overview of the methodologies enabling LLMs to process,
understand, and generate 3D data. Highlighting the unique advantages of LLMs,
such as in-context learning, step-by-step reasoning, open-vocabulary
capabilities, and extensive world knowledge, we underscore their potential to
significantly advance spatial comprehension and interaction within embodied
Artificial Intelligence (AI) systems. Our investigation spans various 3D data
representations, from point clouds to Neural Radiance Fields (NeRFs). It
examines their integration with LLMs for tasks such as 3D scene understanding,
captioning, question-answering, and dialogue, as well as LLM-based agents for
spatial reasoning, planning, and navigation. The paper also includes a brief
review of other methods that integrate 3D and language. The meta-analysis
presented in this paper reveals significant progress yet underscores the
necessity for novel approaches to harness the full potential of 3D-LLMs. Hence,
with this paper, we aim to chart a course for future research that explores and
expands the capabilities of 3D-LLMs in understanding and interacting with the
complex 3D world. To support this survey, we have established a project page
where papers related to our topic are organized and listed:
https://github.com/ActiveVisionLab/Awesome-LLM-3D.";Xianzheng Ma<author:sep>Yash Bhalgat<author:sep>Brandon Smart<author:sep>Shuai Chen<author:sep>Xinghui Li<author:sep>Jian Ding<author:sep>Jindong Gu<author:sep>Dave Zhenyu Chen<author:sep>Songyou Peng<author:sep>Jia-Wang Bian<author:sep>Philip H Torr<author:sep>Marc Pollefeys<author:sep>Matthias Nießner<author:sep>Ian D Reid<author:sep>Angel X. Chang<author:sep>Iro Laina<author:sep>Victor Adrian Prisacariu;http://arxiv.org/pdf/2405.10255v1;cs.CV;;nerf
2405.09717v1;http://arxiv.org/abs/2405.09717v1;2024-05-15;From NeRFs to Gaussian Splats, and Back;"For robotics applications where there is a limited number of (typically
ego-centric) views, parametric representations such as neural radiance fields
(NeRFs) generalize better than non-parametric ones such as Gaussian splatting
(GS) to views that are very different from those in the training data; GS
however can render much faster than NeRFs. We develop a procedure to convert
back and forth between the two. Our approach achieves the best of both NeRFs
(superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact
representation) and GS (real-time rendering and ability for easily modifying
the representation); the computational cost of these conversions is minor
compared to training the two from scratch.";Siming He<author:sep>Zach Osman<author:sep>Pratik Chaudhari;http://arxiv.org/pdf/2405.09717v1;cs.CV;;gaussian splatting<tag:sep>nerf
2405.08609v1;http://arxiv.org/abs/2405.08609v1;2024-05-14;Dynamic NeRF: A Review;"Neural Radiance Field(NeRF) is an novel implicit method to achieve the 3D
reconstruction and representation with a high resolution. After the first
research of NeRF is proposed, NeRF has gained a robust developing power and is
booming in the 3D modeling, representation and reconstruction areas. However
the first and most of the followed research projects based on NeRF is static,
which are weak in the practical applications. Therefore, more researcher are
interested and focused on the study of dynamic NeRF that is more feasible and
useful in practical applications or situations. Compared with the static NeRF,
implementing the Dynamic NeRF is more difficult and complex. But Dynamic is
more potential in the future even is the basic of Editable NeRF. In this
review, we made a detailed and abundant statement for the development and
important implementation principles of Dynamci NeRF. The analysis of main
principle and development of Dynamic NeRF is from 2021 to 2023, including the
most of the Dynamic NeRF projects. What is more, with colorful and novel
special designed figures and table, We also made a detailed comparison and
analysis of different features of various of Dynamic. Besides, we analyzed and
discussed the key methods to implement a Dynamic NeRF. The volume of the
reference papers is large. The statements and comparisons are multidimensional.
With a reading of this review, the whole development history and most of the
main design method or principles of Dynamic NeRF can be easy understood and
gained.";Jinwei Lin;http://arxiv.org/pdf/2405.08609v1;cs.CV;25 pages;nerf
2405.07472v1;http://arxiv.org/abs/2405.07472v1;2024-05-13;GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting  Editing with Image Prompting;"The increasing prominence of e-commerce has underscored the importance of
Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D
realm and rely heavily on extensive data for training. Research on 3D VTON
primarily centers on garment-body shape compatibility, a topic extensively
covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion
model has now been adapted for 3D editing via multi-viewpoint editing. In this
work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating
Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless
transition from 2D to 3D VTON, we propose, for the first time, the use of only
images as editing prompts for 3D editing. To further address issues, e.g., face
blurring, garment inaccuracy, and degraded viewpoint quality during editing, we
devise a three-stage refinement strategy to gradually mitigate potential
issues. Furthermore, we introduce a new editing strategy termed Edit Recall
Reconstruction (ERR) to tackle the limitations of previous editing strategies
in leading to complex geometric changes. Our comprehensive experiments
demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D
VTON while also establishing a novel starting point for image-prompting 3D
scene editing.";Haodong Chen<author:sep>Yongle Huang<author:sep>Haojian Huang<author:sep>Xiangsheng Ge<author:sep>Dian Shao;http://arxiv.org/pdf/2405.07472v1;cs.CV;On-going work;gaussian splatting
2405.07857v1;http://arxiv.org/abs/2405.07857v1;2024-05-13;Synergistic Integration of Coordinate Network and Tensorial Feature for  Improving Neural Radiance Fields from Sparse Inputs;"The multi-plane representation has been highlighted for its fast training and
inference across static and dynamic neural radiance fields. This approach
constructs relevant features via projection onto learnable grids and
interpolating adjacent vertices. However, it has limitations in capturing
low-frequency details and tends to overuse parameters for low-frequency
features due to its bias toward fine details, despite its multi-resolution
concept. This phenomenon leads to instability and inefficiency when training
poses are sparse. In this work, we propose a method that synergistically
integrates multi-plane representation with a coordinate-based network known for
strong bias toward low-frequency signals. The coordinate-based network is
responsible for capturing low-frequency details, while the multi-plane
representation focuses on capturing fine-grained details. We demonstrate that
using residual connections between them seamlessly preserves their own inherent
properties. Additionally, the proposed progressive training scheme accelerates
the disentanglement of these two features. We empirically show that the
proposed method achieves comparable results to explicit encoding with fewer
parameters, and particularly, it outperforms others for the static and dynamic
NeRFs under sparse inputs.";Mingyu Kim<author:sep>Jun-Seong Kim<author:sep>Se-Young Yun<author:sep>Jin-Hwa Kim;http://arxiv.org/pdf/2405.07857v1;cs.CV;"ICML2024 ; Project page is accessible at
  https://mingyukim87.github.io/SynergyNeRF ; Code is available at
  https://github.com/MingyuKim87/SynergyNeRF";nerf
2405.07178v1;http://arxiv.org/abs/2405.07178v1;2024-05-12;Hologram: Realtime Holographic Overlays via LiDAR Augmented  Reconstruction;"Guided by the hologram technology of the infamous Star Wars franchise, I
present an application that creates real-time holographic overlays using LiDAR
augmented 3D reconstruction. Prior attempts involve SLAM or NeRFs which either
require highly calibrated scenes, incur steep computation costs, or fail to
render dynamic scenes. I propose 3 high-fidelity reconstruction tools that can
run on a portable device, such as a iPhone 14 Pro, which can allow for metric
accurate facial reconstructions. My systems enable interactive and immersive
holographic experiences that can be used for a wide range of applications,
including augmented reality, telepresence, and entertainment.";Ekansh Agrawal;http://arxiv.org/pdf/2405.07178v1;cs.CV;;nerf
2405.07306v1;http://arxiv.org/abs/2405.07306v1;2024-05-12;Point Resampling and Ray Transformation Aid to Editable NeRF Models;"In NeRF-aided editing tasks, object movement presents difficulties in
supervision generation due to the introduction of variability in object
positions. Moreover, the removal operations of certain scene objects often lead
to empty regions, presenting challenges for NeRF models in inpainting them
effectively. We propose an implicit ray transformation strategy, allowing for
direct manipulation of the 3D object's pose by operating on the neural-point in
NeRF rays. To address the challenge of inpainting potential empty regions, we
present a plug-and-play inpainting module, dubbed differentiable neural-point
resampling (DNR), which interpolates those regions in 3D space at the original
ray locations within the implicit space, thereby facilitating object removal &
scene inpainting tasks. Importantly, employing DNR effectively narrows the gap
between ground truth and predicted implicit features, potentially increasing
the mutual information (MI) of the features across rays. Then, we leverage DNR
and ray transformation to construct a point-based editable NeRF pipeline
PR^2T-NeRF. Results primarily evaluated on 3D object removal & inpainting tasks
indicate that our pipeline achieves state-of-the-art performance. In addition,
our pipeline supports high-quality rendering visualization for diverse editing
operations without necessitating extra supervision.";Zhenyang Li<author:sep>Zilong Chen<author:sep>Feifan Qu<author:sep>Mingqing Wang<author:sep>Yizhou Zhao<author:sep>Kai Zhang<author:sep>Yifan Peng;http://arxiv.org/pdf/2405.07306v1;cs.CV;;nerf
2405.06945v1;http://arxiv.org/abs/2405.06945v1;2024-05-11;Direct Learning of Mesh and Appearance via 3D Gaussian Splatting;"Accurately reconstructing a 3D scene including explicit geometry information
is both attractive and challenging. Geometry reconstruction can benefit from
incorporating differentiable appearance models, such as Neural Radiance Fields
and 3D Gaussian Splatting (3DGS). In this work, we propose a learnable scene
model that incorporates 3DGS with an explicit geometry representation, namely a
mesh. Our model learns the mesh and appearance in an end-to-end manner, where
we bind 3D Gaussians to the mesh faces and perform differentiable rendering of
3DGS to obtain photometric supervision. The model creates an effective
information pathway to supervise the learning of the scene, including the mesh.
Experimental results demonstrate that the learned scene model not only achieves
state-of-the-art rendering quality but also supports manipulation using the
explicit mesh. In addition, our model has a unique advantage in adapting to
scene updates, thanks to the end-to-end learning of both mesh and appearance.";Ancheng Lin<author:sep>Jun Li;http://arxiv.org/pdf/2405.06945v1;cs.CV;;gaussian splatting
2405.07027v1;http://arxiv.org/abs/2405.07027v1;2024-05-11;TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural  Radiance Field Optimization;"The reliance on accurate camera poses is a significant barrier to the
widespread deployment of Neural Radiance Fields (NeRF) models for 3D
reconstruction and SLAM tasks. The existing method introduces monocular depth
priors to jointly optimize the camera poses and NeRF, which fails to fully
exploit the depth priors and neglects the impact of their inherent noise. In
this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that
enables training NeRF from unknown camera poses - by jointly optimizing
learnable parameters of the radiance field and camera poses. Our approach
explicitly utilizes monocular depth priors through three key advancements: 1)
we propose a novel depth-based ray sampling strategy based on the truncated
normal distribution, which improves the convergence speed and accuracy of pose
estimation; 2) to circumvent local minima and refine depth geometry, we
introduce a coarse-to-fine training strategy that progressively improves the
depth precision; 3) we propose a more robust inter-frame point constraint that
enhances robustness against depth noise during training. The experimental
results on three datasets demonstrate that TD-NeRF achieves superior
performance in the joint optimization of camera pose and NeRF, surpassing prior
works, and generates more accurate depth geometry. The implementation of our
method has been released at https://github.com/nubot-nudt/TD-NeRF.";Zhen Tan<author:sep>Zongtan Zhou<author:sep>Yangbing Ge<author:sep>Zi Wang<author:sep>Xieyuanli Chen<author:sep>Dewen Hu;http://arxiv.org/pdf/2405.07027v1;cs.CV;;nerf
2405.06408v1;http://arxiv.org/abs/2405.06408v1;2024-05-10;I3DGS: Improve 3D Gaussian Splatting from Multiple Dimensions;"3D Gaussian Splatting is a novel method for 3D view synthesis, which can gain
an implicit neural learning rendering result than the traditional neural
rendering technology but keep the more high-definition fast rendering speed.
But it is still difficult to achieve a fast enough efficiency on 3D Gaussian
Splatting for the practical applications. To Address this issue, we propose the
I3DS, a synthetic model performance improvement evaluation solution and
experiments test. From multiple and important levels or dimensions of the
original 3D Gaussian Splatting, we made more than two thousand various kinds of
experiments to test how the selected different items and components can make an
impact on the training efficiency of the 3D Gaussian Splatting model. In this
paper, we will share abundant and meaningful experiences and methods about how
to improve the training, performance and the impacts caused by different items
of the model. A special but normal Integer compression in base 95 and a
floating-point compression in base 94 with ASCII encoding and decoding
mechanism is presented. Many real and effective experiments and test results or
phenomena will be recorded. After a series of reasonable fine-tuning, I3DS can
gain excellent performance improvements than the previous one. The project code
is available as open source.";Jinwei Lin;http://arxiv.org/pdf/2405.06408v1;cs.CV;16 pages;gaussian splatting
2405.06547v1;http://arxiv.org/abs/2405.06547v1;2024-05-10;OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation;"One image to editable dynamic 3D model and video generation is novel
direction and change in the research area of single image to 3D representation
or 3D reconstruction of image. Gaussian Splatting has demonstrated its
advantages in implicit 3D reconstruction, compared with the original Neural
Radiance Fields. As the rapid development of technologies and principles,
people tried to used the Stable Diffusion models to generate targeted models
with text instructions. However, using the normal implicit machine learning
methods is hard to gain the precise motions and actions control, further more,
it is difficult to generate a long content and semantic continuous 3D video. To
address this issue, we propose the OneTo3D, a method and theory to used one
single image to generate the editable 3D model and generate the targeted
semantic continuous time-unlimited 3D video. We used a normal basic Gaussian
Splatting model to generate the 3D model from a single image, which requires
less volume of video memory and computer calculation ability. Subsequently, we
designed an automatic generation and self-adaptive binding mechanism for the
object armature. Combined with the re-editable motions and actions analyzing
and controlling algorithm we proposed, we can achieve a better performance than
the SOTA projects in the area of building the 3D model precise motions and
actions control, and generating a stable semantic continuous time-unlimited 3D
video with the input text instructions. Here we will analyze the detailed
implementation methods and theories analyses. Relative comparisons and
conclusions will be presented. The project code is open source.";Jinwei Lin;http://arxiv.org/pdf/2405.06547v1;cs.CV;24 pages, 13 figures, 2 tables;gaussian splatting
2405.06461v2;http://arxiv.org/abs/2405.06461v2;2024-05-10;SketchDream: Sketch-based Text-to-3D Generation and Editing;"Existing text-based 3D generation methods generate attractive results but
lack detailed geometry control. Sketches, known for their conciseness and
expressiveness, have contributed to intuitive 3D modeling but are confined to
producing texture-less mesh models within predefined categories. Integrating
sketch and text simultaneously for 3D generation promises enhanced control over
geometry and appearance but faces challenges from 2D-to-3D translation
ambiguity and multi-modal condition integration. Moreover, further editing of
3D models in arbitrary views will give users more freedom to customize their
models. However, it is difficult to achieve high generation quality, preserve
unedited regions, and manage proper interactions between shape components. To
solve the above issues, we propose a text-driven 3D content generation and
editing method, SketchDream, which supports NeRF generation from given
hand-drawn sketches and achieves free-view sketch-based local editing. To
tackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view
image generation diffusion model, which leverages depth guidance to establish
spatial correspondence. A 3D ControlNet with a 3D attention module is utilized
to control multi-view images and ensure their 3D consistency. To support local
editing, we further propose a coarse-to-fine editing approach: the coarse phase
analyzes component interactions and provides 3D masks to label edited regions,
while the fine stage generates realistic results with refined details by local
enhancement. Extensive experiments validate that our method generates
higher-quality results compared with a combination of 2D ControlNet and
image-to-3D generation techniques and achieves detailed control compared with
existing diffusion-based 3D editing approaches.";Feng-Lin Liu<author:sep>Hongbo Fu<author:sep>Yu-Kun Lai<author:sep>Lin Gao;http://arxiv.org/pdf/2405.06461v2;cs.GR;;nerf
2405.06762v1;http://arxiv.org/abs/2405.06762v1;2024-05-10;LIVE: LaTex Interactive Visual Editing;"LaTex coding is one of the main methods of writing an academic paper. When
writing a paper, abundant proper visual or graphic components will represent
more information volume than the textual data. However, most of the
implementation of LaTex graphic items are designed as static items that have
some weaknesses in representing more informative figures or tables with an
interactive reading experience. To address this problem, we propose LIVE, a
novel design methods idea to design interactive LaTex graphic items. To make a
lucid representation of the main idea of LIVE, we designed several novels
representing implementations that are interactive and enough explanation for
the basic level principles. Using LIVE can design more graphic items, which we
call the Gitems, and easily and automatically get the relationship of the
mutual application of a specific range of papers, which will add more vitality
and performance factors into writing of traditional papers especially the
review papers. For vividly representing the functions of LIVE, we use the
papers from NeRF as the example reference papers. The code of the
implementation project is open source.";Jinwei Lin;http://arxiv.org/pdf/2405.06762v1;cs.HC;8 pages, double column, ieee;nerf
2405.06241v1;http://arxiv.org/abs/2405.06241v1;2024-05-10;MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth  Smooth Regularization;"This letter introduces a novel framework for dense Visual Simultaneous
Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently Gaussian
Splatting-based SLAM has yielded promising results, but rely on RGB-D input and
is weak in tracking. To address these limitations, we uniquely integrates
advanced sparse visual odometry with a dense Gaussian Splatting scene
representation for the first time, thereby eliminating the dependency on depth
maps typical of Gaussian Splatting-based SLAM systems and enhancing tracking
robustness. Here, the sparse visual odometry tracks camera poses in RGB stream,
while Gaussian Splatting handles map reconstruction. These components are
interconnected through a Multi-View Stereo (MVS) depth estimation network. And
we propose a depth smooth loss to reduce the negative effect of estimated depth
maps. Furthermore, the consistency in scale between the sparse visual odometry
and the dense Gaussian map is preserved by Sparse-Dense Adjustment Ring (SDAR).
We have evaluated our system across various synthetic and real-world datasets.
The accuracy of our pose estimation surpasses existing methods and achieves
state-of-the-art performance. Additionally, it outperforms previous monocular
methods in terms of novel view synthesis fidelity, matching the results of
neural SLAM systems that utilize RGB-D input.";Pengcheng Zhu<author:sep>Yaoming Zhuang<author:sep>Baoquan Chen<author:sep>Li Li<author:sep>Chengdong Wu<author:sep>Zhanlin Liu;http://arxiv.org/pdf/2405.06241v1;cs.CV;"This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible";gaussian splatting
2405.06181v1;http://arxiv.org/abs/2405.06181v1;2024-05-10;Residual-NeRF: Learning Residual NeRFs for Transparent Object  Manipulation;"Transparent objects are ubiquitous in industry, pharmaceuticals, and
households. Grasping and manipulating these objects is a significant challenge
for robots. Existing methods have difficulty reconstructing complete depth maps
for challenging transparent objects, leaving holes in the depth reconstruction.
Recent work has shown neural radiance fields (NeRFs) work well for depth
perception in scenes with transparent objects, and these depth maps can be used
to grasp transparent objects with high accuracy. NeRF-based depth
reconstruction can still struggle with especially challenging transparent
objects and lighting conditions. In this work, we propose Residual-NeRF, a
method to improve depth perception and training speed for transparent objects.
Robots often operate in the same area, such as a kitchen. By first learning a
background NeRF of the scene without transparent objects to be manipulated, we
reduce the ambiguity faced by learning the changes with the new object. We
propose training two additional networks: a residual NeRF learns to infer
residual RGB values and densities, and a Mixnet learns how to combine
background and residual NeRFs. We contribute synthetic and real experiments
that suggest Residual-NeRF improves depth perception of transparent objects.
The results on synthetic data suggest Residual-NeRF outperforms the baselines
with a 46.1% lower RMSE and a 29.5% lower MAE. Real-world qualitative
experiments suggest Residual-NeRF leads to more robust depth maps with less
noise and fewer holes. Website: https://residual-nerf.github.io";Bardienus P. Duisterhof<author:sep>Yuemin Mao<author:sep>Si Heng Teng<author:sep>Jeffrey Ichnowski;http://arxiv.org/pdf/2405.06181v1;cs.CV;;nerf
2405.06214v1;http://arxiv.org/abs/2405.06214v1;2024-05-10;Aerial-NeRF: Adaptive Spatial Partitioning and Sampling for Large-Scale  Aerial Rendering;"Recent progress in large-scale scene rendering has yielded Neural Radiance
Fields (NeRF)-based models with an impressive ability to synthesize scenes
across small objects and indoor scenes. Nevertheless, extending this idea to
large-scale aerial rendering poses two critical problems. Firstly, a single
NeRF cannot render the entire scene with high-precision for complex large-scale
aerial datasets since the sampling range along each view ray is insufficient to
cover buildings adequately. Secondly, traditional NeRFs are infeasible to train
on one GPU to enable interactive fly-throughs for modeling massive images.
Instead, existing methods typically separate the whole scene into multiple
regions and train a NeRF on each region, which are unaccustomed to different
flight trajectories and difficult to achieve fast rendering. To that end, we
propose Aerial-NeRF with three innovative modifications for jointly adapting
NeRF in large-scale aerial rendering: (1) Designing an adaptive spatial
partitioning and selection method based on drones' poses to adapt different
flight trajectories; (2) Using similarity of poses instead of (expert) network
for rendering speedup to determine which region a new viewpoint belongs to; (3)
Developing an adaptive sampling approach for rendering performance improvement
to cover the entire buildings at different heights. Extensive experiments have
conducted to verify the effectiveness and efficiency of Aerial-NeRF, and new
state-of-the-art results have been achieved on two public large-scale aerial
datasets and presented SCUTic dataset. Note that our model allows us to perform
rendering over 4 times as fast as compared to multiple competitors. Our
dataset, code, and model are publicly available at https://drliuqi.github.io/.";Xiaohan Zhang<author:sep>Yukui Qiu<author:sep>Zhenyu Sun<author:sep>Qi Liu;http://arxiv.org/pdf/2405.06214v1;cs.CV;;nerf
2405.05768v1;http://arxiv.org/abs/2405.05768v1;2024-05-09;FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic  Gaussian Splatting;"Text-driven 3D indoor scene generation holds broad applications, ranging from
gaming and smart homes to AR/VR applications. Fast and high-fidelity scene
generation is paramount for ensuring user-friendly experiences. However,
existing methods are characterized by lengthy generation processes or
necessitate the intricate manual specification of motion parameters, which
introduces inconvenience for users. Furthermore, these methods often rely on
narrow-field viewpoint iterative generations, compromising global consistency
and overall scene quality. To address these issues, we propose FastScene, a
framework for fast and higher-quality 3D scene generation, while maintaining
the scene consistency. Specifically, given a text prompt, we generate a
panorama and estimate its depth, since the panorama encompasses information
about the entire scene and exhibits explicit geometric constraints. To obtain
high-quality novel views, we introduce the Coarse View Synthesis (CVS) and
Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene
consistency and view quality. Subsequently, we utilize Multi-View Projection
(MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for
scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses
other methods in both generation speed and quality with better scene
consistency. Notably, guided only by a text prompt, FastScene can generate a 3D
scene within a mere 15 minutes, which is at least one hour faster than
state-of-the-art methods, making it a paradigm for user-friendly scene
generation.";Yikun Ma<author:sep>Dandan Zhan<author:sep>Zhi Jin;http://arxiv.org/pdf/2405.05768v1;cs.CV;Accepted by IJCAI-2024;gaussian splatting
2405.05526v1;http://arxiv.org/abs/2405.05526v1;2024-05-09;Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview;"Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D
scene representation, offering high-fidelity renderings and reconstructions
from a set of sparse and unstructured sensor data. In the context of autonomous
robotics, where perception and understanding of the environment are pivotal,
NeRF holds immense promise for improving performance. In this paper, we present
a comprehensive survey and analysis of the state-of-the-art techniques for
utilizing NeRF to enhance the capabilities of autonomous robots. We especially
focus on the perception, localization and navigation, and decision-making
modules of autonomous robots and delve into tasks crucial for autonomous
operation, including 3D reconstruction, segmentation, pose estimation,
simultaneous localization and mapping (SLAM), navigation and planning, and
interaction. Our survey meticulously benchmarks existing NeRF-based methods,
providing insights into their strengths and limitations. Moreover, we explore
promising avenues for future research and development in this domain. Notably,
we discuss the integration of advanced techniques such as 3D Gaussian splatting
(3DGS), large language models (LLM), and generative AIs, envisioning enhanced
reconstruction efficiency, scene understanding, decision-making capabilities.
This survey serves as a roadmap for researchers seeking to leverage NeRFs to
empower autonomous robots, paving the way for innovative solutions that can
navigate and interact seamlessly in complex environments.";Yuhang Ming<author:sep>Xingrui Yang<author:sep>Weihan Wang<author:sep>Zheng Chen<author:sep>Jinglun Feng<author:sep>Yifan Xing<author:sep>Guofeng Zhang;http://arxiv.org/pdf/2405.05526v1;cs.RO;32 pages, 5 figures, 8 tables;gaussian splatting<tag:sep>nerf
2405.05663v1;http://arxiv.org/abs/2405.05663v1;2024-05-09;RPBG: Towards Robust Neural Point-based Graphics in the Wild;"Point-based representations have recently gained popularity in novel view
synthesis, for their unique advantages, e.g., intuitive geometric
representation, simple manipulation, and faster convergence. However, based on
our observation, these point-based neural re-rendering methods are only
expected to perform well under ideal conditions and suffer from noisy, patchy
points and unbounded scenes, which are challenging to handle but defacto common
in real applications. To this end, we revisit one such influential method,
known as Neural Point-based Graphics (NPBG), as our baseline, and propose
Robust Point-based Graphics (RPBG). We in-depth analyze the factors that
prevent NPBG from achieving satisfactory renderings on generic datasets, and
accordingly reform the pipeline to make it more robust to varying datasets
in-the-wild. Inspired by the practices in image restoration, we greatly enhance
the neural renderer to enable the attention-based correction of point
visibility and the inpainting of incomplete rasterization, with only acceptable
overheads. We also seek for a simple and lightweight alternative for
environment modeling and an iterative method to alleviate the problem of poor
geometry. By thorough evaluation on a wide range of datasets with different
shooting conditions and camera trajectories, RPBG stably outperforms the
baseline by a large margin, and exhibits its great robustness over
state-of-the-art NeRF-based variants. Code available at
https://github.com/QT-Zhu/RPBG.";Qingtian Zhu<author:sep>Zizhuang Wei<author:sep>Zhongtian Zheng<author:sep>Yifan Zhan<author:sep>Zhuyu Yao<author:sep>Jiawang Zhang<author:sep>Kejian Wu<author:sep>Yinqiang Zheng;http://arxiv.org/pdf/2405.05663v1;cs.CV;;nerf
2405.05749v2;http://arxiv.org/abs/2405.05749v2;2024-05-09;NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via  Generative Prior;"Audio-driven talking head generation is advancing from 2D to 3D content.
Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to
synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based
approach typically requires a large number of paired audio-visual data for each
identity, thereby limiting the scalability of the method. Although there have
been attempts to generate audio-driven 3D talking head animations with a single
image, the results are often unsatisfactory due to insufficient information on
obscured regions in the image. In this paper, we mainly focus on addressing the
overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where
facial animations are synthesized primarily in front-facing perspectives. We
propose a novel method, NeRFFaceSpeech, which enables to produce high-quality
3D-aware talking head. Using prior knowledge of generative models combined with
NeRF, our method can craft a 3D-consistent facial feature space corresponding
to a single image. Our spatial synchronization method employs audio-correlated
vertex dynamics of a parametric face model to transform static image features
into dynamic visuals through ray deformation, ensuring realistic 3D facial
motion. Moreover, we introduce LipaintNet that can replenish the lacking
information in the inner-mouth area, which can not be obtained from a given
single image. The network is trained in a self-supervised manner by utilizing
the generative capabilities without additional data. The comprehensive
experiments demonstrate the superiority of our method in generating
audio-driven talking heads from a single image with enhanced 3D consistency
compared to previous approaches. In addition, we introduce a quantitative way
of measuring the robustness of a model against pose changes for the first time,
which has been possible only qualitatively.";Gihoon Kim<author:sep>Kwanggyoon Seo<author:sep>Sihun Cha<author:sep>Junyong Noh;http://arxiv.org/pdf/2405.05749v2;cs.CV;11 pages, 5 figures;nerf
2405.05702v1;http://arxiv.org/abs/2405.05702v1;2024-05-09;NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap;"Gaussian Splatting has garnered widespread attention due to its exceptional
performance. Consequently, SLAM systems based on Gaussian Splatting have
emerged, leveraging its capabilities for rapid real-time rendering and
high-fidelity mapping. However, current Gaussian Splatting SLAM systems usually
struggle with large scene representation and lack effective loop closure
adjustments and scene generalization capabilities. To address these issues, we
introduce NGM-SLAM, the first GS-SLAM system that utilizes neural radiance
field submaps for progressive scene expression, effectively integrating the
strengths of neural radiance fields and 3D Gaussian Splatting. We have
developed neural implicit submaps as supervision and achieve high-quality scene
expression and online loop closure adjustments through Gaussian rendering of
fused submaps. Our results on multiple real-world scenes and large-scale scene
datasets demonstrate that our method can achieve accurate gap filling and
high-quality scene expression, supporting both monocular, stereo, and RGB-D
inputs, and achieving state-of-the-art scene reconstruction and tracking
performance.";Mingrui Li<author:sep>Jingwei Huang<author:sep>Lei Sun<author:sep>Aaron Xuxiang Tian<author:sep>Tianchen Deng<author:sep>Hongyu Wang;http://arxiv.org/pdf/2405.05702v1;cs.RO;9pages, 4 figures;gaussian splatting
2405.05800v1;http://arxiv.org/abs/2405.05800v1;2024-05-09;DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian  Representation;"User-friendly 3D object editing is a challenging task that has attracted
significant attention recently. The limitations of direct 3D object editing
without 2D prior knowledge have prompted increased attention towards utilizing
2D generative models for 3D editing. While existing methods like Instruct
NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly
due to semantic guided editing. In the realm of 3D representation, 3D Gaussian
Splatting emerges as a promising approach for its efficiency and natural
explicit property, facilitating precise editing tasks. Building upon these
insights, we propose DragGaussian, a 3D object drag-editing framework based on
3D Gaussian Splatting, leveraging diffusion models for interactive image
editing with open-vocabulary input. This framework enables users to perform
drag-based editing on pre-trained 3D Gaussian object models, producing modified
2D images through multi-view consistent editing. Our contributions include the
introduction of a new task, the development of DragGaussian for interactive
point-based 3D editing, and comprehensive validation of its effectiveness
through qualitative and quantitative experiments.";Sitian Shen<author:sep>Jing Xu<author:sep>Yuheng Yuan<author:sep>Xingyi Yang<author:sep>Qiuhong Shen<author:sep>Xinchao Wang;http://arxiv.org/pdf/2405.05800v1;cs.GR;;gaussian splatting<tag:sep>nerf
2405.05446v1;http://arxiv.org/abs/2405.05446v1;2024-05-08;GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of  Radiance Fields;"The 3D Gaussian splatting methods are getting popular. However, they work
directly on the signal, leading to a dense representation of the signal. Even
with some techniques such as pruning or distillation, the results are still
dense. In this paper, we propose to model the gradient of the original signal.
The gradients are much sparser than the original signal. Therefore, the
gradients use much less Gaussian splats, leading to the more efficient storage
and thus higher computational performance during both training and rendering.
Thanks to the sparsity, during the view synthesis, only a small mount of pixels
are needed, leading to much higher computational performance ($100\sim
1000\times$ faster). And the 2D image can be recovered from the gradients via
solving a Poisson equation with linear computation complexity. Several
experiments are performed to confirm the sparseness of the gradients and the
computation performance of the proposed method. The method can be applied
various applications, such as human body modeling and indoor environment
modeling.";Yuanhao Gong;http://arxiv.org/pdf/2405.05446v1;cs.CV;arXiv admin note: text overlap with arXiv:2404.09105;gaussian splatting
2405.05010v1;http://arxiv.org/abs/2405.05010v1;2024-05-08;${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields;"Neural fields (NeRF) have emerged as a promising approach for representing
continuous 3D scenes. Nevertheless, the lack of semantic encoding in NeRFs
poses a significant challenge for scene decomposition. To address this
challenge, we present a single model, Multi-Modal Decomposition NeRF
(${M^2D}$NeRF), that is capable of both text-based and visual patch-based
edits. Specifically, we use multi-modal feature distillation to integrate
teacher features from pretrained visual and language models into 3D semantic
feature volumes, thereby facilitating consistent 3D editing. To enforce
consistency between the visual and language features in our 3D feature volumes,
we introduce a multi-modal similarity constraint. We also introduce a
patch-based joint contrastive loss that helps to encourage object-regions to
coalesce in the 3D feature space, resulting in more precise boundaries.
Experiments on various real-world scenes show superior performance in 3D scene
decomposition tasks compared to prior NeRF-based methods.";Ning Wang<author:sep>Lefei Zhang<author:sep>Angel X Chang;http://arxiv.org/pdf/2405.05010v1;cs.CV;;nerf
2405.04416v2;http://arxiv.org/abs/2405.04416v2;2024-05-07;DistGrid: Scalable Scene Reconstruction with Distributed  Multi-resolution Hash Grid;"Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled
and indoor scene reconstruction. However, there exist some challenges when
reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network
capacity, while volume-based NeRFs are heavily memory-consuming when the scene
resolution increases. Recent approaches propose to geographically partition the
scene and learn each sub-region using an individual NeRF. Such partitioning
strategies help volume-based NeRF exceed the single GPU memory limit and scale
to larger scenes. However, this approach requires multiple background NeRF to
handle out-of-partition rays, which leads to redundancy of learning. Inspired
by the fact that the background of current partition is the foreground of
adjacent partition, we propose a scalable scene reconstruction method based on
joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is
divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding
Boxes, and a novel segmented volume rendering method is proposed to handle
cross-boundary rays, thereby eliminating the need for background NeRFs. The
experiments demonstrate that our method outperforms existing methods on all
evaluated large-scale scenes, and provides visually plausible scene
reconstruction. The scalability of our method on reconstruction quality is
further evaluated qualitatively and quantitatively.";Sidun Liu<author:sep>Peng Qiao<author:sep>Zongxin Ye<author:sep>Wenyu Li<author:sep>Yong Dou;http://arxiv.org/pdf/2405.04416v2;cs.CV;Originally submitted to Siggraph Asia 2023;nerf
2405.04345v1;http://arxiv.org/abs/2405.04345v1;2024-05-07;Novel View Synthesis with Neural Radiance Fields for Industrial Robot  Applications;"Neural Radiance Fields (NeRFs) have become a rapidly growing research field
with the potential to revolutionize typical photogrammetric workflows, such as
those used for 3D scene reconstruction. As input, NeRFs require multi-view
images with corresponding camera poses as well as the interior orientation. In
the typical NeRF workflow, the camera poses and the interior orientation are
estimated in advance with Structure from Motion (SfM). But the quality of the
resulting novel views, which depends on different parameters such as the number
and distribution of available images, as well as the accuracy of the related
camera poses and interior orientation, is difficult to predict. In addition,
SfM is a time-consuming pre-processing step, and its quality strongly depends
on the image content. Furthermore, the undefined scaling factor of SfM hinders
subsequent steps in which metric information is required. In this paper, we
evaluate the potential of NeRFs for industrial robot applications. We propose
an alternative to SfM pre-processing: we capture the input images with a
calibrated camera that is attached to the end effector of an industrial robot
and determine accurate camera poses with metric scale based on the robot
kinematics. We then investigate the quality of the novel views by comparing
them to ground truth, and by computing an internal quality measure based on
ensemble methods. For evaluation purposes, we acquire multiple datasets that
pose challenges for reconstruction typical of industrial applications, like
reflective objects, poor texture, and fine structures. We show that the
robot-based pose determination reaches similar accuracy as SfM in non-demanding
cases, while having clear advantages in more challenging scenarios. Finally, we
present first results of applying the ensemble method to estimate the quality
of the synthetic novel view in the absence of a ground truth.";Markus Hillemann<author:sep>Robert Langendörfer<author:sep>Max Heiken<author:sep>Max Mehltretter<author:sep>Andreas Schenk<author:sep>Martin Weinmann<author:sep>Stefan Hinz<author:sep>Christian Heipke<author:sep>Markus Ulrich;http://arxiv.org/pdf/2405.04345v1;cs.CV;"8 pages, 8 figures, accepted for publication in The International
  Archives of the Photogrammetry, Remote Sensing and Spatial Information
  Sciences (ISPRS Archives) 2024";nerf
2405.04378v2;http://arxiv.org/abs/2405.04378v2;2024-05-07;Splat-MOVER: Multi-Stage, Open-Vocabulary Robotic Manipulation via  Editable Gaussian Splatting;"We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic
manipulation, which leverages the editability of Gaussian Splatting (GSplat)
scene representations to enable multi-stage manipulation tasks. Splat-MOVER
consists of: (i) ASK-Splat, a GSplat representation that distills latent codes
for language semantics and grasp affordance into the 3D scene. ASK-Splat
enables geometric, semantic, and affordance understanding of 3D scenes, which
is critical for many robotics tasks; (ii) SEE-Splat, a real-time scene-editing
module using 3D semantic masking and infilling to visualize the motions of
objects that result from robot interactions in the real-world. SEE-Splat
creates a ""digital twin"" of the evolving environment throughout the
manipulation task; and (iii) Grasp-Splat, a grasp generation module that uses
ASK-Splat and SEE-Splat to propose candidate grasps for open-world objects.
ASK-Splat is trained in real-time from RGB images in a brief scanning phase
prior to operation, while SEE-Splat and Grasp-Splat run in real-time during
operation. We demonstrate the superior performance of Splat-MOVER in hardware
experiments on a Kinova robot compared to two recent baselines in four
single-stage, open-vocabulary manipulation tasks, as well as in four
multi-stage manipulation tasks using the edited scene to reflect scene changes
due to prior manipulation stages, which is not possible with the existing
baselines. Code for this project and a link to the project page will be made
available soon.";Ola Shorinwa<author:sep>Johnathan Tucker<author:sep>Aliyah Smith<author:sep>Aiden Swann<author:sep>Timothy Chen<author:sep>Roya Firoozi<author:sep>Monroe Kennedy III<author:sep>Mac Schwager;http://arxiv.org/pdf/2405.04378v2;cs.RO;;gaussian splatting
2405.03417v1;http://arxiv.org/abs/2405.03417v1;2024-05-06;Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review;"Image-based 3D reconstruction is a challenging task that involves inferring
the 3D shape of an object or scene from a set of input images. Learning-based
methods have gained attention for their ability to directly estimate 3D shapes.
This review paper focuses on state-of-the-art techniques for 3D reconstruction,
including the generation of novel, unseen views. An overview of recent
developments in the Gaussian Splatting method is provided, covering input
types, model structures, output representations, and training strategies.
Unresolved challenges and future directions are also discussed. Given the rapid
progress in this domain and the numerous opportunities for enhancing 3D
reconstruction methods, a comprehensive examination of algorithms appears
essential. Consequently, this study offers a thorough overview of the latest
advancements in Gaussian Splatting.";Anurag Dalal<author:sep>Daniel Hagen<author:sep>Kjell G. Robbersmyr<author:sep>Kristian Muri Knausgård;http://arxiv.org/pdf/2405.03417v1;cs.CV;24 pages;gaussian splatting
2405.03659v1;http://arxiv.org/abs/2405.03659v1;2024-05-06;A Construct-Optimize Approach to Sparse View Synthesis without Camera  Pose;"Novel view synthesis from a sparse set of input images is a challenging
problem of great practical interest, especially when camera poses are absent or
inaccurate. Direct optimization of camera poses and usage of estimated depths
in neural radiance field algorithms usually do not produce good results because
of the coupling between poses and depths, and inaccuracies in monocular depth
estimation. In this paper, we leverage the recent 3D Gaussian splatting method
to develop a novel construct-and-optimize method for sparse view synthesis
without camera poses. Specifically, we construct a solution progressively by
using monocular depth and projecting pixels back into the 3D world. During
construction, we optimize the solution by detecting 2D correspondences between
training views and the corresponding rendered images. We develop a unified
differentiable pipeline for camera registration and adjustment of both camera
poses and depths, followed by back-projection. We also introduce a novel notion
of an expected surface in Gaussian splatting, which is critical to our
optimization. These steps enable a coarse solution, which can then be low-pass
filtered and refined using standard optimization methods. We demonstrate
results on the Tanks and Temples and Static Hikes datasets with as few as three
widely-spaced views, showing significantly better quality than competing
methods, including those with approximate camera pose information. Moreover,
our results improve with more views and outperform previous InstantNGP and
Gaussian Splatting algorithms even when using half the dataset.";Kaiwen Jiang<author:sep>Yang Fu<author:sep>Mukund Varma T<author:sep>Yash Belhe<author:sep>Xiaolong Wang<author:sep>Hao Su<author:sep>Ravi Ramamoorthi;http://arxiv.org/pdf/2405.03659v1;cs.CV;;gaussian splatting
2405.02880v1;http://arxiv.org/abs/2405.02880v1;2024-05-05;Blending Distributed NeRFs with Tri-stage Robust Pose Optimization;"Due to the limited model capacity, leveraging distributed Neural Radiance
Fields (NeRFs) for modeling extensive urban environments has become a
necessity. However, current distributed NeRF registration approaches encounter
aliasing artifacts, arising from discrepancies in rendering resolutions and
suboptimal pose precision. These factors collectively deteriorate the fidelity
of pose estimation within NeRF frameworks, resulting in occlusion artifacts
during the NeRF blending stage. In this paper, we present a distributed NeRF
system with tri-stage pose optimization. In the first stage, precise poses of
images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine
strategy. In the second stage, we incorporate the inverting Mip-NeRF 360,
coupled with the truncated dynamic low-pass filter, to enable the achievement
of robust and precise poses, termed Frame2Model optimization. On top of this,
we obtain a coarse transformation between NeRFs in different coordinate
systems. In the third stage, we fine-tune the transformation between NeRFs by
Model2Model pose optimization. After obtaining precise transformation
parameters, we proceed to implement NeRF blending, showcasing superior
performance metrics in both real-world and simulation scenarios. Codes and data
will be publicly available at https://github.com/boilcy/Distributed-NeRF.";Baijun Ye<author:sep>Caiyun Liu<author:sep>Xiaoyu Ye<author:sep>Yuantao Chen<author:sep>Yuhai Wang<author:sep>Zike Yan<author:sep>Yongliang Shi<author:sep>Hao Zhao<author:sep>Guyue Zhou;http://arxiv.org/pdf/2405.02880v1;cs.CV;;nerf
2405.02859v1;http://arxiv.org/abs/2405.02859v1;2024-05-05;MVIP-NeRF: Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior;"Despite the emergence of successful NeRF inpainting methods built upon
explicit RGB and depth 2D inpainting supervisions, these methods are inherently
constrained by the capabilities of their underlying 2D inpainters. This is due
to two key reasons: (i) independently inpainting constituent images results in
view-inconsistent imagery, and (ii) 2D inpainters struggle to ensure
high-quality geometry completion and alignment with inpainted RGB images.
  To overcome these limitations, we propose a novel approach called MVIP-NeRF
that harnesses the potential of diffusion priors for NeRF inpainting,
addressing both appearance and geometry aspects. MVIP-NeRF performs joint
inpainting across multiple views to reach a consistent solution, which is
achieved via an iterative optimization process based on Score Distillation
Sampling (SDS). Apart from recovering the rendered RGB images, we also extract
normal maps as a geometric representation and define a normal SDS loss that
motivates accurate geometry inpainting and alignment with the appearance.
Additionally, we formulate a multi-view SDS score function to distill
generative priors simultaneously from different view images, ensuring
consistent visual completion when dealing with large view variations. Our
experimental results show better appearance and geometry recovery than previous
NeRF inpainting methods.";Honghua Chen<author:sep>Chen Change Loy<author:sep>Xingang Pan;http://arxiv.org/pdf/2405.02859v1;cs.CV;14 pages, 10 figures, conference;nerf
2405.02568v1;http://arxiv.org/abs/2405.02568v1;2024-05-04;ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface  Uncertainty;"Active learning in 3D scene reconstruction has been widely studied, as
selecting informative training views is critical for the reconstruction.
Recently, Neural Radiance Fields (NeRF) variants have shown performance
increases in active 3D reconstruction using image rendering or geometric
uncertainty. However, the simultaneous consideration of both uncertainties in
selecting informative views remains unexplored, while utilizing different types
of uncertainty can reduce the bias that arises in the early training stage with
sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate
views considering both uncertainties. ActiveNeuS provides a way to accumulate
image rendering uncertainty while avoiding the bias that the estimated
densities can introduce. ActiveNeuS computes the neural implicit surface
uncertainty, providing the color uncertainty along with the surface
information. It efficiently handles the bias by using the surface information
and a grid, enabling the fast selection of diverse viewpoints. Our method
outperforms previous works on popular datasets, Blender and DTU, showing that
the views selected by ActiveNeuS significantly improve performance.";Hyunseo Kim<author:sep>Hyeonseo Yang<author:sep>Taekyung Kim<author:sep>YoonSung Kim<author:sep>Jin-Hwa Kim<author:sep>Byoung-Tak Zhang;http://arxiv.org/pdf/2405.02568v1;cs.CV;;nerf
2405.02762v1;http://arxiv.org/abs/2405.02762v1;2024-05-04;TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for  Dynamic UAV-based Scenes;"In this paper, we present a new approach to bridge the domain gap between
synthetic and real-world data for un- manned aerial vehicle (UAV)-based
perception. Our formu- lation is designed for dynamic scenes, consisting of
moving objects or human actions, where the goal is to recognize the pose or
actions. We propose an extension of K-Planes Neural Radiance Field (NeRF),
wherein our algorithm stores a set of tiered feature vectors. The tiered
feature vectors are generated to effectively model conceptual information about
a scene as well as an image decoder that transforms output feature maps into
RGB images. Our technique leverages the information amongst both static and
dynamic objects within a scene and is able to capture salient scene attributes
of high altitude videos. We evaluate its performance on challenging datasets,
including Okutama Action and UG2, and observe considerable improvement in
accuracy over state of the art aerial perception algorithms.";Christopher Maxey<author:sep>Jaehoon Choi<author:sep>Yonghan Lee<author:sep>Hyungtae Lee<author:sep>Dinesh Manocha<author:sep>Heesung Kwon;http://arxiv.org/pdf/2405.02762v1;cs.CV;8 pages, submitted to IROS2024;nerf
2405.02066v1;http://arxiv.org/abs/2405.02066v1;2024-05-03;WateRF: Robust Watermarks in Radiance Fields for Protection of  Copyrights;"The advances in the Neural Radiance Fields (NeRF) research offer extensive
applications in diverse domains, but protecting their copyrights has not yet
been researched in depth. Recently, NeRF watermarking has been considered one
of the pivotal solutions for safely deploying NeRF-based 3D representations.
However, existing methods are designed to apply only to implicit or explicit
NeRF representations. In this work, we introduce an innovative watermarking
method that can be employed in both representations of NeRF. This is achieved
by fine-tuning NeRF to embed binary messages in the rendering process. In
detail, we propose utilizing the discrete wavelet transform in the NeRF space
for watermarking. Furthermore, we adopt a deferred back-propagation technique
and introduce a combination with the patch-wise loss to improve rendering
quality and bit accuracy with minimum trade-offs. We evaluate our method in
three different aspects: capacity, invisibility, and robustness of the embedded
watermarks in the 2D-rendered images. Our method achieves state-of-the-art
performance with faster training speed over the compared state-of-the-art
methods.";Youngdong Jang<author:sep>Dong In Lee<author:sep>MinHyuk Jang<author:sep>Jong Wook Kim<author:sep>Feng Yang<author:sep>Sangpil Kim;http://arxiv.org/pdf/2405.02066v1;cs.CV;;nerf
2405.02005v1;http://arxiv.org/abs/2405.02005v1;2024-05-03;HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft  HoloLens 2;"In the fields of photogrammetry, computer vision and computer graphics, the
task of neural 3D scene reconstruction has led to the exploration of various
techniques. Among these, 3D Gaussian Splatting stands out for its explicit
representation of scenes using 3D Gaussians, making it appealing for tasks like
3D point cloud extraction and surface reconstruction. Motivated by its
potential, we address the domain of 3D scene reconstruction, aiming to leverage
the capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting.
We present HoloGS, a novel workflow utilizing HoloLens sensor data, which
bypasses the need for pre-processing steps like Structure from Motion by
instantly accessing the required input data i.e. the images, camera poses and
the point cloud from depth sensing. We provide comprehensive investigations,
including the training process and the rendering quality, assessed through the
Peak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified
point cloud from Gaussian centers, measured by Chamfer Distance. We evaluate
our approach on two self-captured scenes: An outdoor scene of a cultural
heritage statue and an indoor scene of a fine-structured plant. Our results
show that the HoloLens data, including RGB images, corresponding camera poses,
and depth sensing based point clouds to initialize the Gaussians, are suitable
as input for 3D Gaussian Splatting.";Miriam Jäger<author:sep>Theodor Kapler<author:sep>Michael Feßenbecker<author:sep>Felix Birkelbach<author:sep>Markus Hillemann<author:sep>Boris Jutzi;http://arxiv.org/pdf/2405.02005v1;cs.CV;"8 pages, 9 figures, 2 tables. Will be published in the ISPRS The
  International Archives of Photogrammetry, Remote Sensing and Spatial
  Information Sciences";gaussian splatting
2405.02386v1;http://arxiv.org/abs/2405.02386v1;2024-05-03;Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic  Solids;"Despite significant advancements in Neural Radiance Fields (NeRFs), the
renderings may still suffer from aliasing and blurring artifacts, since it
remains a fundamental challenge to effectively and efficiently characterize
anisotropic areas induced by the cone-casting procedure. This paper introduces
a Ripmap-Encoded Platonic Solid representation to precisely and efficiently
featurize 3D anisotropic areas, achieving high-fidelity anti-aliasing
renderings. Central to our approach are two key components: Platonic Solid
Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D
space onto the unparalleled faces of a certain Platonic solid, such that the
anisotropic 3D areas can be projected onto planes with distinguishable
characterization. Meanwhile, each face of the Platonic solid is encoded by the
Ripmap encoding, which is constructed by anisotropically pre-filtering a
learnable feature grid, to enable featurzing the projected anisotropic areas
both precisely and efficiently by the anisotropic area-sampling. Extensive
experiments on both well-established synthetic datasets and a newly captured
real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art
rendering quality, particularly excelling in the fine details of repetitive
structures and textures, while maintaining relatively swift training times.";Junchen Liu<author:sep>Wenbo Hu<author:sep>Zhuo Yang<author:sep>Jianteng Chen<author:sep>Guoliang Wang<author:sep>Xiaoxue Chen<author:sep>Yantong Cai<author:sep>Huan-ang Gao<author:sep>Hao Zhao;http://arxiv.org/pdf/2405.02386v1;cs.CV;"SIGGRAPH 2024, Project page: https://junchenliu77.github.io/Rip-NeRF
  , Code: https://github.com/JunchenLiu77/Rip-NeRF";nerf
2405.02425v1;http://arxiv.org/abs/2405.02425v1;2024-05-03;Learning Robot Soccer from Egocentric Vision with Deep Reinforcement  Learning;"We apply multi-agent deep reinforcement learning (RL) to train end-to-end
robot soccer policies with fully onboard computation and sensing via egocentric
RGB vision. This setting reflects many challenges of real-world robotics,
including active perception, agile full-body control, and long-horizon planning
in a dynamic, partially-observable, multi-agent domain. We rely on large-scale,
simulation-based data generation to obtain complex behaviors from egocentric
vision which can be successfully transferred to physical robots using low-cost
sensors. To achieve adequate visual realism, our simulation combines rigid-body
physics with learned, realistic rendering via multiple Neural Radiance Fields
(NeRFs). We combine teacher-based multi-agent RL and cross-experiment data
reuse to enable the discovery of sophisticated soccer strategies. We analyze
active-perception behaviors including object tracking and ball seeking that
emerge when simply optimizing perception-agnostic soccer play. The agents
display equivalent levels of performance and agility as policies with access to
privileged, ground-truth state. To our knowledge, this paper constitutes a
first demonstration of end-to-end training for multi-agent robot soccer,
mapping raw pixel observations to joint-level actions, that can be deployed in
the real world. Videos of the game-play and analyses can be seen on our website
https://sites.google.com/view/vision-soccer .";Dhruva Tirumala<author:sep>Markus Wulfmeier<author:sep>Ben Moran<author:sep>Sandy Huang<author:sep>Jan Humplik<author:sep>Guy Lever<author:sep>Tuomas Haarnoja<author:sep>Leonard Hasenclever<author:sep>Arunkumar Byravan<author:sep>Nathan Batchelor<author:sep>Neil Sreendra<author:sep>Kushal Patel<author:sep>Marlon Gwira<author:sep>Francesco Nori<author:sep>Martin Riedmiller<author:sep>Nicolas Heess;http://arxiv.org/pdf/2405.02425v1;cs.RO;;nerf
2405.01333v1;http://arxiv.org/abs/2405.01333v1;2024-05-02;NeRF in Robotics: A Survey;"Meticulous 3D environment representations have been a longstanding goal in
computer vision and robotics fields. The recent emergence of neural implicit
representations has introduced radical innovation to this field as implicit
representations enable numerous capabilities. Among these, the Neural Radiance
Field (NeRF) has sparked a trend because of the huge representational
advantages, such as simplified mathematical models, compact environment
storage, and continuous scene representations. Apart from computer vision, NeRF
has also shown tremendous potential in the field of robotics. Thus, we create
this survey to provide a comprehensive understanding of NeRF in the field of
robotics. By exploring the advantages and limitations of NeRF, as well as its
current applications and future potential, we hope to shed light on this
promising area of research. Our survey is divided into two main sections:
\textit{The Application of NeRF in Robotics} and \textit{The Advance of NeRF in
Robotics}, from the perspective of how NeRF enters the field of robotics. In
the first section, we introduce and analyze some works that have been or could
be used in the field of robotics from the perception and interaction
perspectives. In the second section, we show some works related to improving
NeRF's own properties, which are essential for deploying NeRF in the field of
robotics. In the discussion section of the review, we summarize the existing
challenges and provide some valuable future research directions for reference.";Guangming Wang<author:sep>Lei Pan<author:sep>Songyou Peng<author:sep>Shaohui Liu<author:sep>Chenfeng Xu<author:sep>Yanzi Miao<author:sep>Wei Zhan<author:sep>Masayoshi Tomizuka<author:sep>Marc Pollefeys<author:sep>Hesheng Wang;http://arxiv.org/pdf/2405.01333v1;cs.RO;21 pages, 19 figures;nerf
2405.00676v1;http://arxiv.org/abs/2405.00676v1;2024-05-01;Spectrally Pruned Gaussian Fields with Neural Compensation;"Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered
attention for its fast rendering speed and high rendering quality. However,
this comes with high memory consumption, e.g., a well-trained Gaussian field
may utilize three million Gaussian primitives and over 700 MB of memory. We
credit this high memory footprint to the lack of consideration for the
relationship between primitives. In this paper, we propose a memory-efficient
Gaussian field named SUNDAE with spectral pruning and neural compensation. On
one hand, we construct a graph on the set of Gaussian primitives to model their
relationship and design a spectral down-sampling module to prune out primitives
while preserving desired signals. On the other hand, to compensate for the
quality loss of pruning Gaussians, we exploit a lightweight neural network head
to mix splatted features, which effectively compensates for quality losses
while capturing the relationship between primitives in its weights. We
demonstrate the performance of SUNDAE with extensive results. For example,
SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla
Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB
memory, on the Mip-NeRF360 dataset. Codes are publicly available at
https://runyiyang.github.io/projects/SUNDAE/.";Runyi Yang<author:sep>Zhenxin Zhu<author:sep>Zhou Jiang<author:sep>Baijun Ye<author:sep>Xiaoxue Chen<author:sep>Yifei Zhang<author:sep>Yuantao Chen<author:sep>Jian Zhao<author:sep>Hao Zhao;http://arxiv.org/pdf/2405.00676v1;cs.CV;"Code: https://github.com/RunyiYang/SUNDAE Project page:
  https://runyiyang.github.io/projects/SUNDAE/";gaussian splatting<tag:sep>nerf
2405.00630v1;http://arxiv.org/abs/2405.00630v1;2024-05-01;Depth Priors in Removal Neural Radiance Fields;"Neural Radiance Fields (NeRF) have shown impressive results in 3D
reconstruction and generating novel views. A key challenge within NeRF is the
editing of reconstructed scenes, such as object removal, which requires
maintaining consistency across multiple views and ensuring high-quality
synthesised perspectives. Previous studies have incorporated depth priors,
typically from LiDAR or sparse depth measurements provided by COLMAP, to
improve the performance of object removal in NeRF. However, these methods are
either costly or time-consuming. In this paper, we propose a novel approach
that integrates monocular depth estimates with NeRF-based object removal models
to significantly reduce time consumption and enhance the robustness and quality
of scene generation and object removal. We conducted a thorough evaluation of
COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy
in depth map generation. Our findings suggest that COLMAP can serve as an
effective alternative to a ground truth depth map where such information is
missing or costly to obtain. Additionally, we integrated various monocular
depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess
their capacity to improve object removal performance. Our experimental results
highlight the potential of monocular depth estimation to substantially improve
NeRF applications.";Zhihao Guo<author:sep>Peng Wang;http://arxiv.org/pdf/2405.00630v1;cs.CV;15 pages;nerf
2405.00900v2;http://arxiv.org/abs/2405.00900v2;2024-05-01;LidaRF: Delving into Lidar for Neural Radiance Field on Street Scenes;"Photorealistic simulation plays a crucial role in applications such as
autonomous driving, where advances in neural radiance fields (NeRFs) may allow
better scalability through the automatic creation of digital 3D assets.
However, reconstruction quality suffers on street scenes due to largely
collinear camera motions and sparser samplings at higher speeds. On the other
hand, the application often demands rendering from camera views that deviate
from the inputs to accurately simulate behaviors like lane changes. In this
paper, we propose several insights that allow a better utilization of Lidar
data to improve NeRF quality on street scenes. First, our framework learns a
geometric scene representation from Lidar, which is fused with the implicit
grid-based representation for radiance decoding, thereby supplying stronger
geometric information offered by explicit point cloud. Second, we put forth a
robust occlusion-aware depth supervision scheme, which allows utilizing
densified Lidar points by accumulation. Third, we generate augmented training
views from Lidar points for further improvement. Our insights translate to
largely improved novel view synthesis under real driving scenes.";Shanlin Sun<author:sep>Bingbing Zhuang<author:sep>Ziyu Jiang<author:sep>Buyu Liu<author:sep>Xiaohui Xie<author:sep>Manmohan Chandraker;http://arxiv.org/pdf/2405.00900v2;cs.CV;CVPR2024 Highlights;nerf
2405.00507v1;http://arxiv.org/abs/2405.00507v1;2024-05-01;NeRF-Guided Unsupervised Learning of RGB-D Registration;"This paper focuses on training a robust RGB-D registration model without
ground-truth pose supervision. Existing methods usually adopt a pairwise
training strategy based on differentiable rendering, which enforces the
photometric and the geometric consistency between the two registered frames as
supervision. However, this frame-to-frame framework suffers from poor
multi-view consistency due to factors such as lighting changes, geometry
occlusion and reflective materials. In this paper, we present NeRF-UR, a novel
frame-to-model optimization framework for unsupervised RGB-D registration.
Instead of frame-to-frame consistency, we leverage the neural radiance field
(NeRF) as a global model of the scene and use the consistency between the input
and the NeRF-rerendered frames for pose optimization. This design can
significantly improve the robustness in scenarios with poor multi-view
consistency and provides better learning signal for the registration model.
Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset,
Sim-RGBD, through a photo-realistic simulator to warm up the registration
model. By first training the registration model on Sim-RGBD and later
unsupervisedly fine-tuning on real data, our framework enables distilling the
capability of feature extraction and registration from simulation to reality.
Our method outperforms the state-of-the-art counterparts on two popular indoor
RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper
reproduction.";Zhinan Yu<author:sep>Zheng Qin<author:sep>Yijie Tang<author:sep>Yongjun Wang<author:sep>Renjiao Yi<author:sep>Chenyang Zhu<author:sep>Kai Xu;http://arxiv.org/pdf/2405.00507v1;cs.CV;;nerf
2404.19702v1;http://arxiv.org/abs/2404.19702v1;2024-04-30;GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting;"We propose GS-LRM, a scalable large reconstruction model that can predict
high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23
seconds on single A100 GPU. Our model features a very simple transformer-based
architecture; we patchify input posed images, pass the concatenated multi-view
image tokens through a sequence of transformer blocks, and decode final
per-pixel Gaussian parameters directly from these tokens for differentiable
rendering. In contrast to previous LRMs that can only reconstruct objects, by
predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large
variations in scale and complexity. We show that our model can work on both
object and scene captures by training it on Objaverse and RealEstate10K
respectively. In both scenarios, the models outperform state-of-the-art
baselines by a wide margin. We also demonstrate applications of our model in
downstream 3D generation tasks. Our project webpage is available at:
https://sai-bi.github.io/project/gs-lrm/ .";Kai Zhang<author:sep>Sai Bi<author:sep>Hao Tan<author:sep>Yuanbo Xiangli<author:sep>Nanxuan Zhao<author:sep>Kalyan Sunkavalli<author:sep>Zexiang Xu;http://arxiv.org/pdf/2404.19702v1;cs.CV;Project webpage: https://sai-bi.github.io/project/gs-lrm/;gaussian splatting
2404.19706v3;http://arxiv.org/abs/2404.19706v3;2024-04-30;RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting;"We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction
system with an RGBD camera for large-scale environments using Gaussian
splatting. The system features a compact Gaussian representation and a highly
efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be
either opaque or nearly transparent, with the opaque ones fitting the surface
and dominant colors, and transparent ones fitting residual colors. By rendering
depth in a different way from color rendering, we let a single opaque Gaussian
well fit a local surface region without the need of multiple overlapping
Gaussians, hence largely reducing the memory and computation cost. For
on-the-fly Gaussian optimization, we explicitly add Gaussians for three types
of pixels per frame: newly observed, with large color errors, and with large
depth errors. We also categorize all Gaussians into stable and unstable ones,
where the stable Gaussians are expected to well fit previously observed RGBD
images and otherwise unstable. We only optimize the unstable Gaussians and only
render the pixels occupied by unstable Gaussians. In this way, both the number
of Gaussians to be optimized and pixels to be rendered are largely reduced, and
the optimization can be done in real time. We show real-time reconstructions of
a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD
SLAM, our system achieves comparable high-quality reconstruction but with
around twice the speed and half the memory cost, and shows superior performance
in the realism of novel view synthesis and camera tracking accuracy.";Zhexi Peng<author:sep>Tianjia Shao<author:sep>Yong Liu<author:sep>Jingke Zhou<author:sep>Yin Yang<author:sep>Jingdong Wang<author:sep>Kun Zhou;http://arxiv.org/pdf/2404.19706v3;cs.CV;To be published in ACM SIGGRAPH 2024;gaussian splatting<tag:sep>nerf
2404.19204v1;http://arxiv.org/abs/2404.19204v1;2024-04-30;NeRF-Insert: 3D Local Editing with Multimodal Control Signals;"We propose NeRF-Insert, a NeRF editing framework that allows users to make
high-quality local edits with a flexible level of control. Unlike previous work
that relied on image-to-image models, we cast scene editing as an in-painting
problem, which encourages the global structure of the scene to be preserved.
Moreover, while most existing methods use only textual prompts to condition
edits, our framework accepts a combination of inputs of different modalities as
reference. More precisely, a user may provide a combination of textual and
visual inputs including images, CAD models, and binary image masks for
specifying a 3D region. We use generic image generation models to in-paint the
scene from multiple viewpoints, and lift the local edits to a 3D-consistent
NeRF edit. Compared to previous methods, our results show better visual quality
and also maintain stronger consistency with the original NeRF.";Benet Oriol Sabat<author:sep>Alessandro Achille<author:sep>Matthew Trager<author:sep>Stefano Soatto;http://arxiv.org/pdf/2404.19204v1;cs.CV;;nerf
2404.19398v2;http://arxiv.org/abs/2404.19398v2;2024-04-30;3D Gaussian Blendshapes for Head Avatar Animation;"We introduce 3D Gaussian blendshapes for modeling photorealistic head
avatars. Taking a monocular video as input, we learn a base head model of
neutral expression, along with a group of expression blendshapes, each of which
corresponds to a basis expression in classical parametric face models. Both the
neutral model and expression blendshapes are represented as 3D Gaussians, which
contain a few properties to depict the avatar appearance. The avatar model of
an arbitrary expression can be effectively generated by combining the neutral
model and expression blendshapes through linear blending of Gaussians with the
expression coefficients. High-fidelity head avatar animations can be
synthesized in real time using Gaussian splatting. Compared to state-of-the-art
methods, our Gaussian blendshape representation better captures high-frequency
details exhibited in input video, and achieves superior rendering performance.";Shengjie Ma<author:sep>Yanlin Weng<author:sep>Tianjia Shao<author:sep>Kun Zhou;http://arxiv.org/pdf/2404.19398v2;cs.GR;ACM SIGGRAPH Conference Proceedings 2024;gaussian splatting
2404.19015v1;http://arxiv.org/abs/2404.19015v1;2024-04-29;Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler  Solutions;"Neural Radiance Fields (NeRF) show impressive performance in photo-realistic
free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF
and ZipNeRF employ explicit models for faster optimization and rendering, as
compared to the NeRF that employs an implicit representation. However, both
implicit and explicit radiance fields require dense sampling of images in the
given scene. Their performance degrades significantly when only a sparse set of
views is available. Researchers find that supervising the depth estimated by a
radiance field helps train it effectively with fewer views. The depth
supervision is obtained either using classical approaches or neural networks
pre-trained on a large dataset. While the former may provide only sparse
supervision, the latter may suffer from generalization issues. As opposed to
the earlier approaches, we seek to learn the depth supervision by designing
augmented models and training them along with the main radiance field. Further,
we aim to design a framework of regularizations that can work across different
implicit and explicit radiance fields. We observe that certain features of
these radiance field models overfit to the observed images in the sparse-input
scenario. Our key finding is that reducing the capability of the radiance
fields with respect to positional encoding, the number of decomposed tensor
components or the size of the hash table, constrains the model to learn simpler
solutions, which estimate better depth in certain regions. By designing
augmented models based on such reduced capabilities, we obtain better depth
supervision for the main radiance field. We achieve state-of-the-art
view-synthesis performance with sparse input views on popular datasets
containing forward-facing and 360$^\circ$ scenes by employing the above
regularizations.";Nagabhushan Somraj<author:sep>Adithyan Karanayil<author:sep>Sai Harsha Mupparaju<author:sep>Rajiv Soundararajan;http://arxiv.org/pdf/2404.19015v1;cs.CV;"The source code for our model can be found on our project page:
  https://nagabhushansn95.github.io/publications/2024/Simple-RF.html. arXiv
  admin note: substantial text overlap with arXiv:2309.03955";nerf
2404.18454v1;http://arxiv.org/abs/2404.18454v1;2024-04-29;3D Gaussian Splatting with Deferred Reflection;"The advent of neural and Gaussian-based radiance field methods have achieved
great success in the field of novel view synthesis. However, specular
reflection remains non-trivial, as the high frequency radiance field is
notoriously difficult to fit stably and accurately. We present a deferred
shading method to effectively render specular reflection with Gaussian
splatting. The key challenge comes from the environment map reflection model,
which requires accurate surface normal while simultaneously bottlenecks normal
estimation with discontinuous gradients. We leverage the per-pixel reflection
gradients generated by deferred shading to bridge the optimization process of
neighboring Gaussians, allowing nearly correct normal estimations to gradually
propagate and eventually spread over all reflective objects. Our method
significantly outperforms state-of-the-art techniques and concurrent work in
synthesizing high-quality specular reflection effects, demonstrating a
consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic
and real-world scenes, while running at a frame rate almost identical to
vanilla Gaussian splatting.";Keyang Ye<author:sep>Qiming Hou<author:sep>Kun Zhou;http://arxiv.org/pdf/2404.18454v1;cs.CV;;gaussian splatting
2404.18929v1;http://arxiv.org/abs/2404.18929v1;2024-04-29;DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing;"We consider the problem of editing 3D objects and scenes based on open-ended
language instructions. The established paradigm to solve this problem is to use
a 2D image generator or editor to guide the 3D editing process. However, this
is often slow as it requires do update a computationally expensive 3D
representations such as a neural radiance field, and to do so by using
contradictory guidance from a 2D model which is inherently not multi-view
consistent. We thus introduce the Direct Gaussian Editor (DGE), a method that
addresses these issues in two ways. First, we modify a given high-quality image
editor like InstructPix2Pix to be multi-view consistent. We do so by utilizing
a training-free approach which integrates cues from the underlying 3D geometry
of the scene. Second, given a multi-view consistent edited sequence of images
of the object, we directly and efficiently optimize the 3D object
representation, which is based on 3D Gaussian Splatting. Because it does not
require to apply edits incrementally and iteratively, DGE is significantly more
efficient than existing approaches, and comes with other perks such as allowing
selective editing of parts of the scene.";Minghao Chen<author:sep>Iro Laina<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2404.18929v1;cs.CV;Project Page: https://silent-chen.github.io/DGE/;gaussian splatting
2404.18669v1;http://arxiv.org/abs/2404.18669v1;2024-04-29;Bootstrap 3D Reconstructed Scenes from 3D Gaussian Splatting;"Recent developments in neural rendering techniques have greatly enhanced the
rendering of photo-realistic 3D scenes across both academic and commercial
fields. The latest method, known as 3D Gaussian Splatting (3D-GS), has set new
benchmarks for rendering quality and speed. Nevertheless, the limitations of
3D-GS become pronounced in synthesizing new viewpoints, especially for views
that greatly deviate from those seen during training. Additionally, issues such
as dilation and aliasing arise when zooming in or out. These challenges can all
be traced back to a single underlying issue: insufficient sampling. In our
paper, we present a bootstrapping method that significantly addresses this
problem. This approach employs a diffusion model to enhance the rendering of
novel views using trained 3D-GS, thereby streamlining the training process. Our
results indicate that bootstrapping effectively reduces artifacts, as well as
clear enhancements on the evaluation metrics. Furthermore, we show that our
method is versatile and can be easily integrated, allowing various 3D
reconstruction projects to benefit from our approach.";Yifei Gao<author:sep>Jie Ou<author:sep>Lei Wang<author:sep>Jun Cheng;http://arxiv.org/pdf/2404.18669v1;cs.GR;;gaussian splatting
2404.19026v1;http://arxiv.org/abs/2404.19026v1;2024-04-29;MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and  Head Editing;"Creating high-fidelity head avatars from multi-view videos is a core issue
for many AR/VR applications. However, existing methods usually struggle to
obtain high-quality renderings for all different head components simultaneously
since they use one single representation to model components with drastically
different characteristics (e.g., skin vs. hair). In this paper, we propose a
Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components
with more suitable representations. Specifically, we select an enhanced FLAME
mesh as our facial representation and predict a UV displacement map to provide
per-vertex offsets for improved personalized geometric details. To achieve
photorealistic renderings, we obtain facial colors using deferred neural
rendering and disentangle neural textures into three meaningful parts. For hair
modeling, we first build a static canonical hair using 3D Gaussian Splatting. A
rigid transformation and an MLP-based deformation field are further applied to
handle complex dynamic expressions. Combined with our occlusion-aware blending,
MeGA generates higher-fidelity renderings for the whole head and naturally
supports more downstream tasks. Experiments on the NeRSemble dataset
demonstrate the effectiveness of our designs, outperforming previous
state-of-the-art methods and supporting various editing functionalities,
including hairstyle alteration and texture editing.";Cong Wang<author:sep>Di Kang<author:sep>He-Yi Sun<author:sep>Shen-Han Qian<author:sep>Zi-Xuan Wang<author:sep>Linchao Bao<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2404.19026v1;cs.CV;Project page: https://conallwang.github.io/MeGA_Pages/;gaussian splatting
2404.18394v1;http://arxiv.org/abs/2404.18394v1;2024-04-29;Reconstructing Satellites in 3D from Amateur Telescope Images;"This paper proposes a framework for the 3D reconstruction of satellites in
low-Earth orbit, utilizing videos captured by small amateur telescopes. The
video data obtained from these telescopes differ significantly from data for
standard 3D reconstruction tasks, characterized by intense motion blur,
atmospheric turbulence, pervasive background light pollution, extended focal
length and constrained observational perspectives. To address these challenges,
our approach begins with a comprehensive pre-processing workflow that
encompasses deep learning-based image restoration, feature point extraction and
camera pose initialization. We proceed with the application of an improved 3D
Gaussian splatting algorithm for reconstructing the 3D model. Our technique
supports simultaneous 3D Gaussian training and pose estimation, enabling the
robust generation of intricate 3D point clouds from sparse, noisy data. The
procedure is further bolstered by a post-editing phase designed to eliminate
noise points inconsistent with our prior knowledge of a satellite's geometric
constraints. We validate our approach using both synthetic datasets and actual
observations of China's Space Station, showcasing its significant advantages
over existing methods in reconstructing 3D space objects from ground-based
observations.";Zhiming Chang<author:sep>Boyang Liu<author:sep>Yifei Xia<author:sep>Youming Guo<author:sep>Boxin Shi<author:sep>He Sun;http://arxiv.org/pdf/2404.18394v1;cs.CV;;gaussian splatting
2404.19149v1;http://arxiv.org/abs/2404.19149v1;2024-04-29;SAGS: Structure-Aware 3D Gaussian Splatting;"Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the
way to real-time neural rendering overcoming the computational burden of
volumetric methods. Following the pioneering work of 3D-GS, several methods
have attempted to achieve compressible and high-fidelity performance
alternatives. However, by employing a geometry-agnostic optimization scheme,
these methods neglect the inherent 3D structure of the scene, thereby
restricting the expressivity and the quality of the representation, resulting
in various floating points and artifacts. In this work, we propose a
structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the
geometry of the scene, which reflects to state-of-the-art rendering performance
and reduced storage requirements on benchmark novel-view synthesis datasets.
SAGS is founded on a local-global graph representation that facilitates the
learning of complex scenes and enforces meaningful point displacements that
preserve the scene's geometry. Additionally, we introduce a lightweight version
of SAGS, using a simple yet effective mid-point interpolation scheme, which
showcases a compact representation of the scene with up to 24$\times$ size
reduction without the reliance on any compression strategies. Extensive
experiments across multiple benchmark datasets demonstrate the superiority of
SAGS compared to state-of-the-art 3D-GS methods under both rendering quality
and model size. Besides, we demonstrate that our structure-aware method can
effectively mitigate floating artifacts and irregular distortions of previous
methods while obtaining precise depth maps. Project page
https://eververas.github.io/SAGS/.";Evangelos Ververas<author:sep>Rolandos Alexandros Potamias<author:sep>Jifei Song<author:sep>Jiankang Deng<author:sep>Stefanos Zafeiriou;http://arxiv.org/pdf/2404.19149v1;cs.CV;15 pages, 8 figures, 3 tables;gaussian splatting<tag:sep>nerf
2404.19038v1;http://arxiv.org/abs/2404.19038v1;2024-04-29;Embedded Representation Learning Network for Animating Styled Video  Portrait;"The talking head generation recently attracted considerable attention due to
its widespread application prospects, especially for digital avatars and 3D
animation design. Inspired by this practical demand, several works explored
Neural Radiance Fields (NeRF) to synthesize the talking heads. However, these
methods based on NeRF face two challenges: (1) Difficulty in generating
style-controllable talking heads. (2) Displacement artifacts around the neck in
rendered images. To overcome these two challenges, we propose a novel
generative paradigm \textit{Embedded Representation Learning Network} (ERLNet)
with two learning stages. First, the \textit{ audio-driven FLAME} (ADF) module
is constructed to produce facial expression and head pose sequences
synchronized with content audio and style video. Second, given the sequence
deduced by the ADF, one novel \textit{dual-branch fusion NeRF} (DBF-NeRF)
explores these contents to render the final images. Extensive empirical studies
demonstrate that the collaboration of these two stages effectively facilitates
our method to render a more realistic talking head than the existing
algorithms.";Tianyong Wang<author:sep>Xiangyu Liang<author:sep>Wangguandong Zheng<author:sep>Dan Niu<author:sep>Haifeng Xia<author:sep>Siyu Xia;http://arxiv.org/pdf/2404.19038v1;cs.CV;;nerf
2404.19040v1;http://arxiv.org/abs/2404.19040v1;2024-04-29;GSTalker: Real-time Audio-Driven Talking Face Generation via Deformable  Gaussian Splatting;"We present GStalker, a 3D audio-driven talking face generation model with
Gaussian Splatting for both fast training (40 minutes) and real-time rendering
(125 FPS) with a 3$\sim$5 minute video for training material, in comparison
with previous 2D and 3D NeRF-based modeling frameworks which require hours of
training and seconds of rendering per frame. Specifically, GSTalker learns an
audio-driven Gaussian deformation field to translate and transform 3D Gaussians
to synchronize with audio information, in which multi-resolution hashing
grid-based tri-plane and temporal smooth module are incorporated to learn
accurate deformation for fine-grained facial details. In addition, a
pose-conditioned deformation field is designed to model the stabilized torso.
To enable efficient optimization of the condition Gaussian deformation field,
we initialize 3D Gaussians by learning a coarse static Gaussian representation.
Extensive experiments in person-specific videos with audio tracks validate that
GSTalker can generate high-fidelity and audio-lips synchronized results with
fast training and real-time rendering speed.";Bo Chen<author:sep>Shoukang Hu<author:sep>Qi Chen<author:sep>Chenpeng Du<author:sep>Ran Yi<author:sep>Yanmin Qian<author:sep>Xie Chen;http://arxiv.org/pdf/2404.19040v1;cs.CV;;gaussian splatting<tag:sep>nerf
2404.18284v1;http://arxiv.org/abs/2404.18284v1;2024-04-28;S3-SLAM: Sparse Tri-plane Encoding for Neural Implicit SLAM;"With the emergence of Neural Radiance Fields (NeRF), neural implicit
representations have gained widespread applications across various domains,
including simultaneous localization and mapping. However, current neural
implicit SLAM faces a challenging trade-off problem between performance and the
number of parameters. To address this problem, we propose sparse tri-plane
encoding, which efficiently achieves scene reconstruction at resolutions up to
512 using only 2~4% of the commonly used tri-plane parameters (reduced from
100MB to 2~4MB). On this basis, we design S3-SLAM to achieve rapid and
high-quality tracking and mapping through sparsifying plane parameters and
integrating orthogonal features of tri-plane. Furthermore, we develop
hierarchical bundle adjustment to achieve globally consistent geometric
structures and reconstruct high-resolution appearance. Experimental results
demonstrate that our approach achieves competitive tracking and scene
reconstruction with minimal parameters on three datasets. Source code will soon
be available.";Zhiyao Zhang<author:sep>Yunzhou Zhang<author:sep>Yanmin Wu<author:sep>Bin Zhao<author:sep>Xingshuo Wang<author:sep>Rui Tian;http://arxiv.org/pdf/2404.18284v1;cs.CV;;nerf
2404.17890v1;http://arxiv.org/abs/2404.17890v1;2024-04-27;DPER: Diffusion Prior Driven Neural Representation for Limited Angle and  Sparse View CT Reconstruction;"Limited-angle and sparse-view computed tomography (LACT and SVCT) are crucial
for expanding the scope of X-ray CT applications. However, they face challenges
due to incomplete data acquisition, resulting in diverse artifacts in the
reconstructed CT images. Emerging implicit neural representation (INR)
techniques, such as NeRF, NeAT, and NeRP, have shown promise in
under-determined CT imaging reconstruction tasks. However, the unsupervised
nature of INR architecture imposes limited constraints on the solution space,
particularly for the highly ill-posed reconstruction task posed by LACT and
ultra-SVCT. In this study, we introduce the Diffusion Prior Driven Neural
Representation (DPER), an advanced unsupervised framework designed to address
the exceptionally ill-posed CT reconstruction inverse problems. DPER adopts the
Half Quadratic Splitting (HQS) algorithm to decompose the inverse problem into
data fidelity and distribution prior sub-problems. The two sub-problems are
respectively addressed by INR reconstruction scheme and pre-trained score-based
diffusion model. This combination initially preserves the implicit image local
consistency prior from INR. Additionally, it effectively augments the
feasibility of the solution space for the inverse problem through the
generative diffusion model, resulting in increased stability and precision in
the solutions. We conduct comprehensive experiments to evaluate the performance
of DPER on LACT and ultra-SVCT reconstruction with two public datasets (AAPM
and LIDC). The results show that our method outperforms the state-of-the-art
reconstruction methods on in-domain datasets, while achieving significant
performance improvements on out-of-domain datasets.";Chenhe Du<author:sep>Xiyue Lin<author:sep>Qing Wu<author:sep>Xuanyu Tian<author:sep>Ying Su<author:sep>Zhe Luo<author:sep>Hongjiang Wei<author:sep>S. Kevin Zhou<author:sep>Jingyi Yu<author:sep>Yuyao Zhang;http://arxiv.org/pdf/2404.17890v1;eess.IV;15 pages, 10 figures;nerf
2404.17528v1;http://arxiv.org/abs/2404.17528v1;2024-04-26;Geometry-aware Reconstruction and Fusion-refined Rendering for  Generalizable Neural Radiance Fields;"Generalizable NeRF aims to synthesize novel views for unseen scenes. Common
practices involve constructing variance-based cost volumes for geometry
reconstruction and encoding 3D descriptors for decoding novel views. However,
existing methods show limited generalization ability in challenging conditions
due to inaccurate geometry, sub-optimal descriptors, and decoding strategies.
We address these issues point by point. First, we find the variance-based cost
volume exhibits failure patterns as the features of pixels corresponding to the
same point can be inconsistent across different views due to occlusions or
reflections. We introduce an Adaptive Cost Aggregation (ACA) approach to
amplify the contribution of consistent pixel pairs and suppress inconsistent
ones. Unlike previous methods that solely fuse 2D features into descriptors,
our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D
context into descriptors through spatial and inter-view interaction. When
decoding the descriptors, we observe the two existing decoding strategies excel
in different areas, which are complementary. A Consistency-Aware Fusion (CAF)
strategy is proposed to leverage the advantages of both. We incorporate the
above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware
Reconstruction and Fusion-refined Rendering (GeFu). GeFu attains
state-of-the-art performance across multiple datasets. Code is available at
https://github.com/TQTQliu/GeFu .";Tianqi Liu<author:sep>Xinyi Ye<author:sep>Min Shi<author:sep>Zihao Huang<author:sep>Zhiyu Pan<author:sep>Zhan Peng<author:sep>Zhiguo Cao;http://arxiv.org/pdf/2404.17528v1;cs.CV;Accepted by CVPR 2024. Project page: https://gefucvpr24.github.io;nerf
2404.17215v1;http://arxiv.org/abs/2404.17215v1;2024-04-26;SLAM for Indoor Mapping of Wide Area Construction Environments;"Simultaneous localization and mapping (SLAM), i.e., the reconstruction of the
environment represented by a (3D) map and the concurrent pose estimation, has
made astonishing progress. Meanwhile, large scale applications aiming at the
data collection in complex environments like factory halls or construction
sites are becoming feasible. However, in contrast to small scale scenarios with
building interiors separated to single rooms, shop floors or construction areas
require measures at larger distances in potentially texture less areas under
difficult illumination. Pose estimation is further aggravated since no GNSS
measures are available as it is usual for such indoor applications. In our
work, we realize data collection in a large factory hall by a robot system
equipped with four stereo cameras as well as a 3D laser scanner. We apply our
state-of-the-art LiDAR and visual SLAM approaches and discuss the respective
pros and cons of the different sensor types for trajectory estimation and dense
map generation in such an environment. Additionally, dense and accurate depth
maps are generated by 3D Gaussian splatting, which we plan to use in the
context of our project aiming on the automatic construction and site
monitoring.";Vincent Ress<author:sep>Wei Zhang<author:sep>David Skuddis<author:sep>Norbert Haala<author:sep>Uwe Soergel;http://arxiv.org/pdf/2404.17215v1;cs.RO;;gaussian splatting
2404.16510v1;http://arxiv.org/abs/2404.16510v1;2024-04-25;Interactive3D: Create What You Want by Interactive 3D Generation;"3D object generation has undergone significant advancements, yielding
high-quality results. However, fall short of achieving precise user control,
often yielding results that do not align with user expectations, thus limiting
their applicability. User-envisioning 3D object generation faces significant
challenges in realizing its concepts using current generative models due to
limited interaction capabilities. Existing methods mainly offer two approaches:
(i) interpreting textual instructions with constrained controllability, or (ii)
reconstructing 3D objects from 2D images. Both of them limit customization to
the confines of the 2D reference and potentially introduce undesirable
artifacts during the 3D lifting process, restricting the scope for direct and
versatile 3D modifications. In this work, we introduce Interactive3D, an
innovative framework for interactive 3D generation that grants users precise
control over the generative process through extensive 3D interaction
capabilities. Interactive3D is constructed in two cascading stages, utilizing
distinct 3D representations. The first stage employs Gaussian Splatting for
direct user interaction, allowing modifications and guidance of the generative
direction at any intermediate step through (i) Adding and Removing components,
(ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv)
Semantic Editing. Subsequently, the Gaussian splats are transformed into
InstantNGP. We introduce a novel (v) Interactive Hash Refinement module to
further add details and extract the geometry in the second stage. Our
experiments demonstrate that Interactive3D markedly improves the
controllability and quality of 3D generation. Our project webpage is available
at \url{https://interactive-3d.github.io/}.";Shaocong Dong<author:sep>Lihe Ding<author:sep>Zhanpeng Huang<author:sep>Zibin Wang<author:sep>Tianfan Xue<author:sep>Dan Xu;http://arxiv.org/pdf/2404.16510v1;cs.GR;project page: https://interactive-3d.github.io/;gaussian splatting
2404.16429v1;http://arxiv.org/abs/2404.16429v1;2024-04-25;Depth Supervised Neural Surface Reconstruction from Airborne Imagery;"While originally developed for novel view synthesis, Neural Radiance Fields
(NeRFs) have recently emerged as an alternative to multi-view stereo (MVS).
Triggered by a manifold of research activities, promising results have been
gained especially for texture-less, transparent, and reflecting surfaces, while
such scenarios remain challenging for traditional MVS-based approaches.
However, most of these investigations focus on close-range scenarios, with
studies for airborne scenarios still missing. For this task, NeRFs face
potential difficulties at areas of low image redundancy and weak data evidence,
as often found in street canyons, facades or building shadows. Furthermore,
training such networks is computationally expensive. Thus, the aim of our work
is twofold: First, we investigate the applicability of NeRFs for aerial image
blocks representing different characteristics like nadir-only, oblique and
high-resolution imagery. Second, during these investigations we demonstrate the
benefit of integrating depth priors from tie-point measures, which are provided
during presupposed Bundle Block Adjustment. Our work is based on the
state-of-the-art framework VolSDF, which models 3D scenes by signed distance
functions (SDFs), since this is more applicable for surface reconstruction
compared to the standard volumetric representation in vanilla NeRFs. For
evaluation, the NeRF-based reconstructions are compared to results of a
publicly available benchmark dataset for airborne images.";Vincent Hackstein<author:sep>Paul Fauth-Mayer<author:sep>Matthias Rothermel<author:sep>Norbert Haala;http://arxiv.org/pdf/2404.16429v1;cs.CV;;nerf
2404.16323v1;http://arxiv.org/abs/2404.16323v1;2024-04-25;DIG3D: Marrying Gaussian Splatting with Deformable Transformer for  Single Image 3D Reconstruction;"In this paper, we study the problem of 3D reconstruction from a single-view
RGB image and propose a novel approach called DIG3D for 3D object
reconstruction and novel view synthesis. Our method utilizes an encoder-decoder
framework which generates 3D Gaussians in decoder with the guidance of
depth-aware image features from encoder. In particular, we introduce the use of
deformable transformer, allowing efficient and effective decoding through 3D
reference point and multi-layer refinement adaptations. By harnessing the
benefits of 3D Gaussians, our approach offers an efficient and accurate
solution for 3D reconstruction from single-view images. We evaluate our method
on the ShapeNet SRN dataset, getting PSNR of 24.21 and 24.98 in car and chair
dataset, respectively. The result outperforming the recent method by around
2.25%, demonstrating the effectiveness of our method in achieving superior
results.";Jiamin Wu<author:sep>Kenkun Liu<author:sep>Han Gao<author:sep>Xiaoke Jiang<author:sep>Lei Zhang;http://arxiv.org/pdf/2404.16323v1;cs.CV;;gaussian splatting
2404.16012v2;http://arxiv.org/abs/2404.16012v2;2024-04-24;GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with  Audio-Driven 3D Gaussian Splatting;"We propose GaussianTalker, a novel framework for real-time generation of
pose-controllable talking heads. It leverages the fast rendering capabilities
of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly
controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS
representation of the head and deforms it in sync with the audio. A key insight
is to encode the 3D Gaussian attributes into a shared implicit feature
representation, where it is merged with audio features to manipulate each
Gaussian attribute. This design exploits the spatial-aware features and
enforces interactions between neighboring points. The feature embeddings are
then fed to a spatial-audio attention module, which predicts frame-wise offsets
for the attributes of each Gaussian. It is more stable than previous
concatenation or multiplication approaches for manipulating the numerous
Gaussians and their intricate parameters. Experimental results showcase
GaussianTalker's superiority in facial fidelity, lip synchronization accuracy,
and rendering speed compared to previous methods. Specifically, GaussianTalker
achieves a remarkable rendering speed up to 120 FPS, surpassing previous
benchmarks. Our code is made available at
https://github.com/KU-CVLAB/GaussianTalker/ .";Kyusun Cho<author:sep>Joungbin Lee<author:sep>Heeji Yoon<author:sep>Yeobin Hong<author:sep>Jaehoon Ko<author:sep>Sangjun Ahn<author:sep>Seungryong Kim;http://arxiv.org/pdf/2404.16012v2;cs.CV;Project Page: https://ku-cvlab.github.io/GaussianTalker;gaussian splatting
2404.15891v2;http://arxiv.org/abs/2404.15891v2;2024-04-24;OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian  Segmentation;"Recent advancements in 3D reconstruction technologies have paved the way for
high-quality and real-time rendering of complex 3D scenes. Despite these
achievements, a notable challenge persists: it is difficult to precisely
reconstruct specific objects from large scenes. Current scene reconstruction
techniques frequently result in the loss of object detail textures and are
unable to reconstruct object portions that are occluded or unseen in views. To
address this challenge, we delve into the meticulous 3D reconstruction of
specific objects within large scenes and propose a framework termed OMEGAS:
Object Mesh Extraction from Large Scenes Guided by GAussian Segmentation.
OMEGAS employs a multi-step approach, grounded in several excellent
off-the-shelf methodologies. Specifically, initially, we utilize the Segment
Anything Model (SAM) to guide the segmentation of 3D Gaussian Splatting (3DGS),
thereby creating a basic 3DGS model of the target object. Then, we leverage
large-scale diffusion priors to further refine the details of the 3DGS model,
especially aimed at addressing invisible or occluded object portions from the
original scene views. Subsequently, by re-rendering the 3DGS model onto the
scene views, we achieve accurate object segmentation and effectively remove the
background. Finally, these target-only images are used to improve the 3DGS
model further and extract the definitive 3D object mesh by the SuGaR model. In
various scenarios, our experiments demonstrate that OMEGAS significantly
surpasses existing scene reconstruction methods. Our project page is at:
https://github.com/CrystalWlz/OMEGAS";Lizhi Wang<author:sep>Feng Zhou<author:sep>Jianqin Yin;http://arxiv.org/pdf/2404.15891v2;cs.CV;arXiv admin note: text overlap with arXiv:2311.17061 by other authors;gaussian splatting
2404.15707v1;http://arxiv.org/abs/2404.15707v1;2024-04-24;ESR-NeRF: Emissive Source Reconstruction Using LDR Multi-view Images;"Existing NeRF-based inverse rendering methods suppose that scenes are
exclusively illuminated by distant light sources, neglecting the potential
influence of emissive sources within a scene. In this work, we confront this
limitation using LDR multi-view images captured with emissive sources turned on
and off. Two key issues must be addressed: 1) ambiguity arising from the
limited dynamic range along with unknown lighting details, and 2) the expensive
computational cost in volume rendering to backtrace the paths leading to final
object colors. We present a novel approach, ESR-NeRF, leveraging neural
networks as learnable functions to represent ray-traced fields. By training
networks to satisfy light transport segments, we regulate outgoing radiances,
progressively identifying emissive sources while being aware of reflection
areas. The results on scenes encompassing emissive sources with various
properties demonstrate the superiority of ESR-NeRF in qualitative and
quantitative ways. Our approach also extends its applicability to the scenes
devoid of emissive sources, achieving lower CD metrics on the DTU dataset.";Jinseo Jeong<author:sep>Junseo Koo<author:sep>Qimeng Zhang<author:sep>Gunhee Kim;http://arxiv.org/pdf/2404.15707v1;cs.CV;CVPR 2024;nerf
2404.16221v1;http://arxiv.org/abs/2404.16221v1;2024-04-24;NeRF-XL: Scaling NeRFs with Multiple GPUs;"We present NeRF-XL, a principled method for distributing Neural Radiance
Fields (NeRFs) across multiple GPUs, thus enabling the training and rendering
of NeRFs with an arbitrarily large capacity. We begin by revisiting existing
multi-GPU approaches, which decompose large scenes into multiple independently
trained NeRFs, and identify several fundamental issues with these methods that
hinder improvements in reconstruction quality as additional computational
resources (GPUs) are used in training. NeRF-XL remedies these issues and
enables the training and rendering of NeRFs with an arbitrary number of
parameters by simply using more hardware. At the core of our method lies a
novel distributed training and rendering formulation, which is mathematically
equivalent to the classic single-GPU case and minimizes communication between
GPUs. By unlocking NeRFs with arbitrarily large parameter counts, our approach
is the first to reveal multi-GPU scaling laws for NeRFs, showing improvements
in reconstruction quality with larger parameter counts and speed improvements
with more GPUs. We demonstrate the effectiveness of NeRF-XL on a wide variety
of datasets, including the largest open-source dataset to date, MatrixCity,
containing 258K images covering a 25km^2 city area.";Ruilong Li<author:sep>Sanja Fidler<author:sep>Angjoo Kanazawa<author:sep>Francis Williams;http://arxiv.org/pdf/2404.16221v1;cs.CV;Webpage: https://research.nvidia.com/labs/toronto-ai/nerfxl/;nerf
2404.15259v1;http://arxiv.org/abs/2404.15259v1;2024-04-23;FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient  Descent;"This paper introduces FlowMap, an end-to-end differentiable method that
solves for precise camera poses, camera intrinsics, and per-frame dense depth
of a video sequence. Our method performs per-video gradient-descent
minimization of a simple least-squares objective that compares the optical flow
induced by depth, intrinsics, and poses against correspondences obtained via
off-the-shelf optical flow and point tracking. Alongside the use of point
tracks to encourage long-term geometric consistency, we introduce
differentiable re-parameterizations of depth, intrinsics, and pose that are
amenable to first-order optimization. We empirically show that camera
parameters and dense depth recovered by our method enable photo-realistic novel
view synthesis on 360-degree trajectories using Gaussian Splatting. Our method
not only far outperforms prior gradient-descent based bundle adjustment
methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM
method, on the downstream task of 360-degree novel view synthesis (even though
our method is purely gradient-descent based, fully differentiable, and presents
a complete departure from conventional SfM).";Cameron Smith<author:sep>David Charatan<author:sep>Ayush Tewari<author:sep>Vincent Sitzmann;http://arxiv.org/pdf/2404.15259v1;cs.CV;Project website: https://cameronosmith.github.io/flowmap/;gaussian splatting
2404.15538v1;http://arxiv.org/abs/2404.15538v1;2024-04-23;DreamCraft: Text-Guided Generation of Functional 3D Environments in  Minecraft;"Procedural Content Generation (PCG) algorithms enable the automatic
generation of complex and diverse artifacts. However, they don't provide
high-level control over the generated content and typically require domain
expertise. In contrast, text-to-3D methods allow users to specify desired
characteristics in natural language, offering a high amount of flexibility and
expressivity. But unlike PCG, such approaches cannot guarantee functionality,
which is crucial for certain applications like game design. In this paper, we
present a method for generating functional 3D artifacts from free-form text
prompts in the open-world game Minecraft. Our method, DreamCraft, trains
quantized Neural Radiance Fields (NeRFs) to represent artifacts that, when
viewed in-game, match given text descriptions. We find that DreamCraft produces
more aligned in-game artifacts than a baseline that post-processes the output
of an unconstrained NeRF. Thanks to the quantized representation of the
environment, functional constraints can be integrated using specialized loss
terms. We show how this can be leveraged to generate 3D structures that match a
target distribution or obey certain adjacency rules over the block types.
DreamCraft inherits a high degree of expressivity and controllability from the
NeRF, while still being able to incorporate functional constraints through
domain-specific objectives.";Sam Earle<author:sep>Filippos Kokkinos<author:sep>Yuhe Nie<author:sep>Julian Togelius<author:sep>Roberta Raileanu;http://arxiv.org/pdf/2404.15538v1;cs.GR;16 pages, 9 figures, accepted to Foundation of Digital Games 2024;nerf
2404.15264v1;http://arxiv.org/abs/2404.15264v1;2024-04-23;TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via  Gaussian Splatting;"Radiance fields have demonstrated impressive performance in synthesizing
lifelike 3D talking heads. However, due to the difficulty in fitting steep
appearance changes, the prevailing paradigm that presents facial motions by
directly modifying point appearance may lead to distortions in dynamic regions.
To tackle this challenge, we introduce TalkingGaussian, a deformation-based
radiance fields framework for high-fidelity talking head synthesis. Leveraging
the point-based Gaussian Splatting, facial motions can be represented in our
method by applying smooth and continuous deformations to persistent Gaussian
primitives, without requiring to learn the difficult appearance change like
previous methods. Due to this simplification, precise facial motions can be
synthesized while keeping a highly intact facial feature. Under such a
deformation paradigm, we further identify a face-mouth motion inconsistency
that would affect the learning of detailed speaking motions. To address this
conflict, we decompose the model into two branches separately for the face and
inside mouth areas, therefore simplifying the learning tasks to help
reconstruct more accurate motion and structure of the mouth region. Extensive
experiments demonstrate that our method renders high-quality lip-synchronized
talking head videos, with better facial fidelity and higher efficiency compared
with previous methods.";Jiahe Li<author:sep>Jiawei Zhang<author:sep>Xiao Bai<author:sep>Jin Zheng<author:sep>Xin Ning<author:sep>Jun Zhou<author:sep>Lin Gu;http://arxiv.org/pdf/2404.15264v1;cs.CV;Project page: https://fictionarry.github.io/TalkingGaussian/;gaussian splatting
2404.13921v1;http://arxiv.org/abs/2404.13921v1;2024-04-22;NeRF-DetS: Enhancing Multi-View 3D Object Detection with  Sampling-adaptive Network of Continuous NeRF-based Representation;"As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and
3D perception, demonstrating that perceptual tasks can benefit from novel view
synthesis methods like NeRF, significantly improving the performance of indoor
multi-view 3D object detection. Using the geometry MLP of NeRF to direct the
attention of detection head to crucial parts and incorporating self-supervised
loss from novel view rendering contribute to the achieved improvement. To
better leverage the notable advantages of the continuous representation through
neural rendering in space, we introduce a novel 3D perception network
structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level
Sampling-Adaptive Network, making the sampling process adaptively from coarse
to fine. Also, we propose a superior multi-view information fusion method,
known as Multi-head Weighted Fusion. This fusion approach efficiently addresses
the challenge of losing multi-view information when using arithmetic mean,
while keeping low computational costs. NeRF-DetS outperforms competitive
NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement
in mAP@.25 and mAP@.50, respectively.";Chi Huang<author:sep>Xinyang Li<author:sep>Shengchuan Zhang<author:sep>Liujuan Cao<author:sep>Rongrong Ji;http://arxiv.org/pdf/2404.13921v1;cs.CV;;nerf
2404.13816v2;http://arxiv.org/abs/2404.13816v2;2024-04-22;Neural Radiance Field in Autonomous Driving: A Survey;"Neural Radiance Field (NeRF) has garnered significant attention from both
academia and industry due to its intrinsic advantages, particularly its
implicit representation and novel view synthesis capabilities. With the rapid
advancements in deep learning, a multitude of methods have emerged to explore
the potential applications of NeRF in the domain of Autonomous Driving (AD).
However, a conspicuous void is apparent within the current literature. To
bridge this gap, this paper conducts a comprehensive survey of NeRF's
applications in the context of AD. Our survey is structured to categorize
NeRF's applications in Autonomous Driving (AD), specifically encompassing
perception, 3D reconstruction, simultaneous localization and mapping (SLAM),
and simulation. We delve into in-depth analysis and summarize the findings for
each application category, and conclude by providing insights and discussions
on future directions in this field. We hope this paper serves as a
comprehensive reference for researchers in this domain. To the best of our
knowledge, this is the first survey specifically focused on the applications of
NeRF in the Autonomous Driving domain.";Lei He<author:sep>Leheng Li<author:sep>Wenchao Sun<author:sep>Zeyu Han<author:sep>Yichen Liu<author:sep>Sifa Zheng<author:sep>Jianqiang Wang<author:sep>Keqiang Li;http://arxiv.org/pdf/2404.13816v2;cs.CV;;nerf
2404.14037v2;http://arxiv.org/abs/2404.14037v2;2024-04-22;GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian  Splatting;"Recent works on audio-driven talking head synthesis using Neural Radiance
Fields (NeRF) have achieved impressive results. However, due to inadequate pose
and expression control caused by NeRF implicit representation, these methods
still have some limitations, such as unsynchronized or unnatural lip movements,
and visual jitter and artifacts. In this paper, we propose GaussianTalker, a
novel method for audio-driven talking head synthesis based on 3D Gaussian
Splatting. With the explicit representation property of 3D Gaussians, intuitive
control of the facial motion is achieved by binding Gaussians to 3D facial
models. GaussianTalker consists of two modules, Speaker-specific Motion
Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator
achieves accurate lip movements specific to the target speaker through
universalized audio feature extraction and customized lip motion generation.
Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance
facial detail representation via a latent pose, delivering stable and realistic
rendered videos. Extensive experimental results suggest that GaussianTalker
outperforms existing state-of-the-art methods in talking head synthesis,
delivering precise lip synchronization and exceptional visual quality. Our
method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU,
significantly exceeding the threshold for real-time rendering performance, and
can potentially be deployed on other hardware platforms.";Hongyun Yu<author:sep>Zhan Qu<author:sep>Qihang Yu<author:sep>Jianchuan Chen<author:sep>Zhonghua Jiang<author:sep>Zhiwen Chen<author:sep>Shengyu Zhang<author:sep>Jimin Xu<author:sep>Fei Wu<author:sep>Chengfei Lv<author:sep>Gang Yu;http://arxiv.org/pdf/2404.14037v2;cs.CV;https://yuhongyun777.github.io/GaussianTalker/;nerf
2404.13896v2;http://arxiv.org/abs/2404.13896v2;2024-04-22;CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with  Complex Trajectory;"Neural radiance field (NeRF) has achieved impressive results in high-quality
3D scene reconstruction. However, NeRF heavily relies on precise camera poses.
While recent works like BARF have introduced camera pose optimization within
NeRF, their applicability is limited to simple trajectory scenes. Existing
methods struggle while tackling complex trajectories involving large rotations.
To address this limitation, we propose CT-NeRF, an incremental reconstruction
optimization pipeline using only RGB images without pose and depth input. In
this pipeline, we first propose a local-global bundle adjustment under a pose
graph connecting neighboring frames to enforce the consistency between poses to
escape the local minima caused by only pose consistency with the scene
structure. Further, we instantiate the consistency between poses as a
reprojected geometric image distance constraint resulting from pixel-level
correspondences between input image pairs. Through the incremental
reconstruction, CT-NeRF enables the recovery of both camera poses and scene
structure and is capable of handling scenes with complex trajectories. We
evaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and
Free-Dataset, which feature complex trajectories. Results show CT-NeRF
outperforms existing methods in novel view synthesis and pose estimation
accuracy.";Yunlong Ran<author:sep>Yanxu Li<author:sep>Qi Ye<author:sep>Yuchi Huo<author:sep>Zechun Bai<author:sep>Jiahao Sun<author:sep>Jiming Chen;http://arxiv.org/pdf/2404.13896v2;cs.CV;;nerf
2404.14249v1;http://arxiv.org/abs/2404.14249v1;2024-04-22;CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and  View-consistent 3D Semantic Understanding;"The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time
synthesis of novel views in 3D scenes. Currently, it primarily focuses on
geometry and appearance modeling, while lacking the semantic understanding of
scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from
Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to
efficiently comprehend 3D environments without annotated semantic data. In
specific, rather than straightforwardly learning and rendering high-dimensional
semantic features of 3D Gaussians, which significantly diminishes the
efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC
exploits the inherent unified semantics within objects to learn compact yet
effective semantic representations of 3D Gaussians, enabling highly efficient
rendering (>100 FPS). Additionally, to address the semantic ambiguity, caused
by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we
introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the
multi-view consistency originated from the 3D model. 3DCS imposes cross-view
semantic consistency constraints by leveraging refined, self-predicted
pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing
precise and view-consistent segmentation results. Extensive experiments
demonstrate that our method remarkably outperforms existing state-of-the-art
approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on
Replica and ScanNet datasets, respectively, while maintaining real-time
rendering speed. Furthermore, our approach exhibits superior performance even
with sparse input data, verifying the robustness of our method.";Guibiao Liao<author:sep>Jiankun Li<author:sep>Zhenyu Bao<author:sep>Xiaoqing Ye<author:sep>Jingdong Wang<author:sep>Qing Li<author:sep>Kanglin Liu;http://arxiv.org/pdf/2404.14249v1;cs.CV;https://github.com/gbliao/CLIP-GS;gaussian splatting
2404.14410v1;http://arxiv.org/abs/2404.14410v1;2024-04-22;Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D  Glimpses;"In this paper, we present a method to reconstruct the world and multiple
dynamic humans in 3D from a monocular video input. As a key idea, we represent
both the world and multiple humans via the recently emerging 3D Gaussian
Splatting (3D-GS) representation, enabling to conveniently and efficiently
compose and render them together. In particular, we address the scenarios with
severely limited and sparse observations in 3D human reconstruction, a common
challenge encountered in the real world. To tackle this challenge, we introduce
a novel approach to optimize the 3D-GS representation in a canonical space by
fusing the sparse cues in the common space, where we leverage a pre-trained 2D
diffusion model to synthesize unseen views while keeping the consistency with
the observed 2D appearances. We demonstrate our method can reconstruct
high-quality animatable 3D humans in various challenging examples, in the
presence of occlusion, image crops, few-shot, and extremely sparse
observations. After reconstruction, our method is capable of not only rendering
the scene in any novel views at arbitrary time instances, but also editing the
3D scene by removing individual humans or applying different motions for each
human. Through various experiments, we demonstrate the quality and efficiency
of our methods over alternative existing approaches.";Inhee Lee<author:sep>Byungjun Kim<author:sep>Hanbyul Joo;http://arxiv.org/pdf/2404.14410v1;cs.CV;The project page is available at https://snuvclab.github.io/gtu/;
2404.13541v1;http://arxiv.org/abs/2404.13541v1;2024-04-21;Generalizable Novel-View Synthesis using a Stereo Camera;"In this paper, we propose the first generalizable view synthesis approach
that specifically targets multi-view stereo-camera images. Since recent stereo
matching has demonstrated accurate geometry prediction, we introduce stereo
matching into novel-view synthesis for high-quality geometry reconstruction. To
this end, this paper proposes a novel framework, dubbed StereoNeRF, which
integrates stereo matching into a NeRF-based generalizable view synthesis
approach. StereoNeRF is equipped with three key components to effectively
exploit stereo matching in novel-view synthesis: a stereo feature extractor, a
depth-guided plane-sweeping, and a stereo depth loss. Moreover, we propose the
StereoNVS dataset, the first multi-view dataset of stereo-camera images,
encompassing a wide variety of both real and synthetic scenes. Our experimental
results demonstrate that StereoNeRF surpasses previous approaches in
generalizable view synthesis.";Haechan Lee<author:sep>Wonjoon Jin<author:sep>Seung-Hwan Baek<author:sep>Sunghyun Cho;http://arxiv.org/pdf/2404.13541v1;cs.CV;"Accepted to CVPR 2024. Project page URL:
  https://jinwonjoon.github.io/stereonerf/";nerf
2404.13711v2;http://arxiv.org/abs/2404.13711v2;2024-04-21;ArtNeRF: A Stylized Neural Field for 3D-Aware Cartoonized Face Synthesis;"Recent advances in generative visual models and neural radiance fields have
greatly boosted 3D-aware image synthesis and stylization tasks. However,
previous NeRF-based work is limited to single scene stylization, training a
model to generate 3D-aware cartoon faces with arbitrary styles remains
unsolved. We propose ArtNeRF, a novel face stylization framework derived from
3D-aware GAN to tackle this problem. In this framework, we utilize an
expressive generator to synthesize stylized faces and a triple-branch
discriminator module to improve the visual quality and style consistency of the
generated faces. Specifically, a style encoder based on contrastive learning is
leveraged to extract robust low-dimensional embeddings of style images,
empowering the generator with the knowledge of various styles. To smooth the
training process of cross-domain transfer learning, we propose an adaptive
style blending module which helps inject style information and allows users to
freely tune the level of stylization. We further introduce a neural rendering
module to achieve efficient real-time rendering of images with higher
resolutions. Extensive experiments demonstrate that ArtNeRF is versatile in
generating high-quality 3D-aware cartoon faces with arbitrary styles.";Zichen Tang<author:sep>Hongyu Yang;http://arxiv.org/pdf/2404.13711v2;cs.CV;;nerf
2404.13679v1;http://arxiv.org/abs/2404.13679v1;2024-04-21;GScream: Learning 3D Geometry and Feature Consistent Gaussian Splatting  for Object Removal;"This paper tackles the intricate challenge of object removal to update the
radiance field using the 3D Gaussian Splatting. The main challenges of this
task lie in the preservation of geometric consistency and the maintenance of
texture coherence in the presence of the substantial discrete nature of
Gaussian primitives. We introduce a robust framework specifically designed to
overcome these obstacles. The key insight of our approach is the enhancement of
information exchange among visible and invisible areas, facilitating content
restoration in terms of both geometry and texture. Our methodology begins with
optimizing the positioning of Gaussian primitives to improve geometric
consistency across both removed and visible areas, guided by an online
registration process informed by monocular depth estimation. Following this, we
employ a novel feature propagation mechanism to bolster texture coherence,
leveraging a cross-attention design that bridges sampling Gaussians from both
uncertain and certain areas. This innovative approach significantly refines the
texture coherence within the final radiance field. Extensive experiments
validate that our method not only elevates the quality of novel view synthesis
for scenes undergoing object removal but also showcases notable efficiency
gains in training and rendering speeds.";Yuxin Wang<author:sep>Qianyi Wu<author:sep>Guofeng Zhang<author:sep>Dan Xu;http://arxiv.org/pdf/2404.13679v1;cs.CV;Project Page: https://w-ted.github.io/publications/gscream;gaussian splatting
2404.13346v1;http://arxiv.org/abs/2404.13346v1;2024-04-20;EC-SLAM: Real-time Dense Neural RGB-D SLAM System with Effectively  Constrained Global Bundle Adjustment;"We introduce EC-SLAM, a real-time dense RGB-D simultaneous localization and
mapping (SLAM) system utilizing Neural Radiance Fields (NeRF). Although recent
NeRF-based SLAM systems have demonstrated encouraging outcomes, they have yet
to completely leverage NeRF's capability to constrain pose optimization. By
employing an effectively constrained global bundle adjustment (BA) strategy,
our system makes use of NeRF's implicit loop closure correction capability.
This improves the tracking accuracy by reinforcing the constraints on the
keyframes that are most pertinent to the optimized current frame. In addition,
by implementing a feature-based and uniform sampling strategy that minimizes
the number of ineffective constraint points for pose optimization, we mitigate
the effects of random sampling in NeRF. EC-SLAM utilizes sparse parametric
encodings and the truncated signed distance field (TSDF) to represent the map
in order to facilitate efficient fusion, resulting in reduced model parameters
and accelerated convergence velocity. A comprehensive evaluation conducted on
the Replica, ScanNet, and TUM datasets showcases cutting-edge performance,
including enhanced reconstruction accuracy resulting from precise pose
estimation, 21 Hz run time, and tracking precision improvements of up to 50\%.
The source code is available at https://github.com/Lightingooo/EC-SLAM.";Guanghao Li<author:sep>Qi Chen<author:sep>YuXiang Yan<author:sep>Jian Pu;http://arxiv.org/pdf/2404.13346v1;cs.RO;;nerf
2404.13437v1;http://arxiv.org/abs/2404.13437v1;2024-04-20;High-fidelity Endoscopic Image Synthesis by Utilizing Depth-guided  Neural Surfaces;"In surgical oncology, screening colonoscopy plays a pivotal role in providing
diagnostic assistance, such as biopsy, and facilitating surgical navigation,
particularly in polyp detection. Computer-assisted endoscopic surgery has
recently gained attention and amalgamated various 3D computer vision
techniques, including camera localization, depth estimation, surface
reconstruction, etc. Neural Radiance Fields (NeRFs) and Neural Implicit
Surfaces (NeuS) have emerged as promising methodologies for deriving accurate
3D surface models from sets of registered images, addressing the limitations of
existing colon reconstruction approaches stemming from constrained camera
movement.
  However, the inadequate tissue texture representation and confused scale
problem in monocular colonoscopic image reconstruction still impede the
progress of the final rendering results. In this paper, we introduce a novel
method for colon section reconstruction by leveraging NeuS applied to
endoscopic images, supplemented by a single frame of depth map. Notably, we
pioneered the exploration of utilizing only one frame depth map in
photorealistic reconstruction and neural rendering applications while this
single depth map can be easily obtainable from other monocular depth estimation
networks with an object scale. Through rigorous experimentation and validation
on phantom imagery, our approach demonstrates exceptional accuracy in
completely rendering colon sections, even capturing unseen portions of the
surface. This breakthrough opens avenues for achieving stable and consistently
scaled reconstructions, promising enhanced quality in cancer screening
procedures and treatment interventions.";Baoru Huang<author:sep>Yida Wang<author:sep>Anh Nguyen<author:sep>Daniel Elson<author:sep>Francisco Vasconcelos<author:sep>Danail Stoyanov;http://arxiv.org/pdf/2404.13437v1;cs.CV;;nerf
2404.12777v1;http://arxiv.org/abs/2404.12777v1;2024-04-19;EfficientGS: Streamlining Gaussian Splatting for Large-Scale  High-Resolution Scene Representation;"In the domain of 3D scene representation, 3D Gaussian Splatting (3DGS) has
emerged as a pivotal technology. However, its application to large-scale,
high-resolution scenes (exceeding 4k$\times$4k pixels) is hindered by the
excessive computational requirements for managing a large number of Gaussians.
Addressing this, we introduce 'EfficientGS', an advanced approach that
optimizes 3DGS for high-resolution, large-scale scenes. We analyze the
densification process in 3DGS and identify areas of Gaussian
over-proliferation. We propose a selective strategy, limiting Gaussian increase
to key primitives, thereby enhancing the representational efficiency.
Additionally, we develop a pruning mechanism to remove redundant Gaussians,
those that are merely auxiliary to adjacent ones. For further enhancement, we
integrate a sparse order increment for Spherical Harmonics (SH), designed to
alleviate storage constraints and reduce training overhead. Our empirical
evaluations, conducted on a range of datasets including extensive 4K+ aerial
images, demonstrate that 'EfficientGS' not only expedites training and
rendering times but also achieves this with a model size approximately tenfold
smaller than conventional 3DGS while maintaining high rendering fidelity.";Wenkai Liu<author:sep>Tao Guan<author:sep>Bin Zhu<author:sep>Lili Ju<author:sep>Zikai Song<author:sep>Dan Li<author:sep>Yuesong Wang<author:sep>Wei Yang;http://arxiv.org/pdf/2404.12777v1;cs.CV;;gaussian splatting
2404.12888v1;http://arxiv.org/abs/2404.12888v1;2024-04-19;Learn2Talk: 3D Talking Face Learns from 2D Talking Face;"Speech-driven facial animation methods usually contain two main classes, 3D
and 2D talking face, both of which attract considerable research attention in
recent years. However, to the best of our knowledge, the research on 3D talking
face does not go deeper as 2D talking face, in the aspect of
lip-synchronization (lip-sync) and speech perception. To mind the gap between
the two sub-fields, we propose a learning framework named Learn2Talk, which can
construct a better 3D talking face network by exploiting two expertise points
from the field of 2D talking face. Firstly, inspired by the audio-video sync
network, a 3D sync-lip expert model is devised for the pursuit of lip-sync
between audio and 3D facial motion. Secondly, a teacher model selected from 2D
talking face methods is used to guide the training of the audio-to-3D motions
regression network to yield more 3D vertex accuracy. Extensive experiments show
the advantages of the proposed framework in terms of lip-sync, vertex accuracy
and speech perception, compared with state-of-the-arts. Finally, we show two
applications of the proposed framework: audio-visual speech recognition and
speech-driven 3D Gaussian Splatting based avatar animation.";Yixiang Zhuang<author:sep>Baoping Cheng<author:sep>Yao Cheng<author:sep>Yuntao Jin<author:sep>Renshuai Liu<author:sep>Chengyang Li<author:sep>Xuan Cheng<author:sep>Jing Liao<author:sep>Juncong Lin;http://arxiv.org/pdf/2404.12888v1;cs.CV;;gaussian splatting
2404.12970v1;http://arxiv.org/abs/2404.12970v1;2024-04-19;FlyNeRF: NeRF-Based Aerial Mapping for High-Quality 3D Scene  Reconstruction;"Current methods for 3D reconstruction and environmental mapping frequently
face challenges in achieving high precision, highlighting the need for
practical and effective solutions. In response to this issue, our study
introduces FlyNeRF, a system integrating Neural Radiance Fields (NeRF) with
drone-based data acquisition for high-quality 3D reconstruction. Utilizing
unmanned aerial vehicle (UAV) for capturing images and corresponding spatial
coordinates, the obtained data is subsequently used for the initial NeRF-based
3D reconstruction of the environment. Further evaluation of the reconstruction
render quality is accomplished by the image evaluation neural network developed
within the scope of our system. According to the results of the image
evaluation module, an autonomous algorithm determines the position for
additional image capture, thereby improving the reconstruction quality. The
neural network introduced for render quality assessment demonstrates an
accuracy of 97%. Furthermore, our adaptive methodology enhances the overall
reconstruction quality, resulting in an average improvement of 2.5 dB in Peak
Signal-to-Noise Ratio (PSNR) for the 10% quantile. The FlyNeRF demonstrates
promising results, offering advancements in such fields as environmental
monitoring, surveillance, and digital twins, where high-fidelity 3D
reconstructions are crucial.";Maria Dronova<author:sep>Vladislav Cheremnykh<author:sep>Alexey Kotcov<author:sep>Aleksey Fedoseev<author:sep>Dzmitry Tsetserukou;http://arxiv.org/pdf/2404.12970v1;cs.RO;;nerf
2404.11852v1;http://arxiv.org/abs/2404.11852v1;2024-04-18;Cicero: Addressing Algorithmic and Architectural Bottlenecks in Neural  Rendering by Radiance Warping and Memory Optimizations;"Neural Radiance Field (NeRF) is widely seen as an alternative to traditional
physically-based rendering. However, NeRF has not yet seen its adoption in
resource-limited mobile systems such as Virtual and Augmented Reality (VR/AR),
because it is simply extremely slow. On a mobile Volta GPU, even the
state-of-the-art NeRF models generally execute only at 0.8 FPS. We show that
the main performance bottlenecks are both algorithmic and architectural. We
introduce, CICERO, to tame both forms of inefficiencies. We first introduce two
algorithms, one fundamentally reduces the amount of work any NeRF model has to
execute, and the other eliminates irregular DRAM accesses. We then describe an
on-chip data layout strategy that eliminates SRAM bank conflicts. A pure
software implementation of CICERO offers an 8.0x speed-up and 7.9x energy
saving over a mobile Volta GPU. When compared to a baseline with a dedicated
DNN accelerator, our speed-up and energy reduction increase to 28.2x and 37.8x,
respectively - all with minimal quality loss (less than 1.0 dB peak
signal-to-noise ratio reduction).";Yu Feng<author:sep>Zihan Liu<author:sep>Jingwen Leng<author:sep>Minyi Guo<author:sep>Yuhao Zhu;http://arxiv.org/pdf/2404.11852v1;cs.AR;;nerf
2404.12379v2;http://arxiv.org/abs/2404.12379v2;2024-04-18;Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular  Videos;"Modern 3D engines and graphics pipelines require mesh as a memory-efficient
representation, which allows efficient rendering, geometry processing, texture
editing, and many other downstream operations. However, it is still highly
difficult to obtain high-quality mesh in terms of structure and detail from
monocular visual observations. The problem becomes even more challenging for
dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh
(DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh
given a single monocular video. Our work leverages the recent advancement in 3D
Gaussian Splatting to construct the mesh sequence with temporal consistency
from a video. Building on top of this representation, DG-Mesh recovers
high-quality meshes from the Gaussian points and can track the mesh vertices
over time, which enables applications such as texture editing on dynamic
objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly
distributed Gaussians, resulting better mesh reconstruction through mesh-guided
densification and pruning on the deformed Gaussians. By applying
cycle-consistent deformation between the canonical and the deformed space, we
can project the anchored Gaussian back to the canonical space and optimize
Gaussians across all time frames. During the evaluation on different datasets,
DG-Mesh provides significantly better mesh reconstruction and rendering than
baselines. Project page: https://www.liuisabella.com/DG-Mesh/";Isabella Liu<author:sep>Hao Su<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2404.12379v2;cs.CV;Project page: https://www.liuisabella.com/DG-Mesh/;gaussian splatting
2404.12385v1;http://arxiv.org/abs/2404.12385v1;2024-04-18;MeshLRM: Large Reconstruction Model for High-Quality Mesh;"We propose MeshLRM, a novel LRM-based approach that can reconstruct a
high-quality mesh from merely four input images in less than one second.
Different from previous large reconstruction models (LRMs) that focus on
NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction
and rendering within the LRM framework. This allows for end-to-end mesh
reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.
Moreover, we improve the LRM architecture by simplifying several complex
designs in previous LRMs. MeshLRM's NeRF initialization is sequentially trained
with low- and high-resolution images; this new LRM training strategy enables
significantly faster convergence and thereby leads to better quality with less
compute. Our approach achieves state-of-the-art mesh reconstruction from
sparse-view inputs and also allows for many downstream applications, including
text-to-3D and single-image-to-3D generation. Project page:
https://sarahweiii.github.io/meshlrm/";Xinyue Wei<author:sep>Kai Zhang<author:sep>Sai Bi<author:sep>Hao Tan<author:sep>Fujun Luan<author:sep>Valentin Deschaintre<author:sep>Kalyan Sunkavalli<author:sep>Hao Su<author:sep>Zexiang Xu;http://arxiv.org/pdf/2404.12385v1;cs.CV;;nerf
2404.11897v1;http://arxiv.org/abs/2404.11897v1;2024-04-18;AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height  Large-scale Outdoor Scene Rendering;"Existing neural radiance fields (NeRF)-based novel view synthesis methods for
large-scale outdoor scenes are mainly built on a single altitude. Moreover,
they often require a priori camera shooting height and scene scope, leading to
inefficient and impractical applications when camera altitude changes. In this
work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce
the training cost of building good reconstructions by synthesizing
free-viewpoint images based on varying altitudes of scenes. Specifically, to
tackle the detail variation problem from low altitude (drone-level) to high
altitude (satellite-level), a source image selection method and an
attention-based feature fusion approach are developed to extract and fuse the
most relevant features of target view from multi-height images for
high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF
achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only
requires a half hour of training time to reach the competitive PSNR as compared
to the latest BungeeNeRF.";Jingfeng Guo<author:sep>Xiaohan Zhang<author:sep>Baozhu Zhao<author:sep>Qi Liu;http://arxiv.org/pdf/2404.11897v1;cs.CV;;nerf
2404.12547v2;http://arxiv.org/abs/2404.12547v2;2024-04-18;Does Gaussian Splatting need SFM Initialization?;"3D Gaussian Splatting has recently been embraced as a versatile and effective
method for scene reconstruction and novel view synthesis, owing to its
high-quality results and compatibility with hardware rasterization. Despite its
advantages, Gaussian Splatting's reliance on high-quality point cloud
initialization by Structure-from-Motion (SFM) algorithms is a significant
limitation to be overcome. To this end, we investigate various initialization
strategies for Gaussian Splatting and delve into how volumetric reconstructions
from Neural Radiance Fields (NeRF) can be utilized to bypass the dependency on
SFM data. Our findings demonstrate that random initialization can perform much
better if carefully designed and that by employing a combination of improved
initialization strategies and structure distillation from low-cost NeRF models,
it is possible to achieve equivalent results, or at times even superior, to
those obtained from SFM initialization.";Yalda Foroutan<author:sep>Daniel Rebain<author:sep>Kwang Moo Yi<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2404.12547v2;cs.CV;14 pages, 6 figures;gaussian splatting<tag:sep>nerf
2404.11401v1;http://arxiv.org/abs/2404.11401v1;2024-04-17;RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled  Neural Rendering;"We propose RainyScape, an unsupervised framework for reconstructing clean
scenes from a collection of multi-view rainy images. RainyScape consists of two
main modules: a neural rendering module and a rain-prediction module that
incorporates a predictor network and a learnable latent embedding that captures
the rain characteristics of the scene. Specifically, based on the spectral bias
property of neural networks, we first optimize the neural rendering pipeline to
obtain a low-frequency scene representation. Subsequently, we jointly optimize
the two modules, driven by the proposed adaptive direction-sensitive
gradient-based reconstruction loss, which encourages the network to distinguish
between scene details and rain streaks, facilitating the propagation of
gradients to the relevant components. Extensive experiments on both the classic
neural radiance field and the recently proposed 3D Gaussian splatting
demonstrate the superiority of our method in effectively eliminating rain
streaks and rendering clean images, achieving state-of-the-art performance. The
constructed high-quality dataset and source code will be publicly available.";Xianqiang Lyu<author:sep>Hui Liu<author:sep>Junhui Hou;http://arxiv.org/pdf/2404.11401v1;cs.CV;;gaussian splatting
2404.11419v1;http://arxiv.org/abs/2404.11419v1;2024-04-17;SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping;"We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose
a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM
(NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing
NeRF-SLAM systems consistently exhibit inferior tracking performance compared
to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via
image alignment and photometric bundle-adjustment. Such optimization processes
are difficult to optimize due to the narrow basin of attraction of the
optimization loss in image space (local minima) and the lack of initial
correspondences. We mitigate these limitations by implementing a Gaussian
pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking
optimization strategy. Furthermore, NeRF systems encounter challenges in
converging to the right geometry with limited input views. While prior
approaches use a Signed-Distance Function (SDF)-based NeRF and directly
supervise SDF values by approximating ground truth SDF through depth
measurements, this often results in suboptimal geometry. In contrast, our
method employs a volume density representation and introduces a novel KL
regularizer on the ray termination distribution, constraining scene geometry to
consist of empty space and opaque surfaces. Our solution implements both local
and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate
(KL regularizer) SLAM solution. We conduct experiments on multiple datasets
(ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in
reconstruction accuracy.";Vincent Cartillier<author:sep>Grant Schindler<author:sep>Irfan Essa;http://arxiv.org/pdf/2404.11419v1;cs.CV;;nerf
2404.11358v2;http://arxiv.org/abs/2404.11358v2;2024-04-17;DeblurGS: Gaussian Splatting for Camera Motion Blur;"Although significant progress has been made in reconstructing sharp 3D scenes
from motion-blurred images, a transition to real-world applications remains
challenging. The primary obstacle stems from the severe blur which leads to
inaccuracies in the acquisition of initial camera poses through
Structure-from-Motion, a critical aspect often overlooked by previous
approaches. To address this challenge, we propose DeblurGS, a method to
optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the
noisy camera pose initialization. We restore a fine-grained sharp scene by
leveraging the remarkable reconstruction capability of 3D Gaussian Splatting.
Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry
observation and synthesizes corresponding blurry renderings for the
optimization process. Furthermore, we propose Gaussian Densification Annealing
strategy to prevent the generation of inaccurate Gaussians at erroneous
locations during the early training stages when camera motion is still
imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves
state-of-the-art performance in deblurring and novel view synthesis for
real-world and synthetic benchmark datasets, as well as field-captured blurry
smartphone videos.";Jeongtaek Oh<author:sep>Jaeyoung Chung<author:sep>Dongwoo Lee<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2404.11358v2;cs.CV;;gaussian splatting
2404.11285v1;http://arxiv.org/abs/2404.11285v1;2024-04-17;Novel View Synthesis for Cinematic Anatomy on Mobile and Immersive  Displays;"Interactive photorealistic visualization of 3D anatomy (i.e., Cinematic
Anatomy) is used in medical education to explain the structure of the human
body. It is currently restricted to frontal teaching scenarios, where the
demonstrator needs a powerful GPU and high-speed access to a large storage
device where the dataset is hosted. We demonstrate the use of novel view
synthesis via compressed 3D Gaussian splatting to overcome this restriction and
to enable students to perform cinematic anatomy on lightweight mobile devices
and in virtual reality environments. We present an automatic approach for
finding a set of images that captures all potentially seen structures in the
data. By mixing closeup views with images from a distance, the splat
representation can recover structures up to the voxel resolution. The use of
Mip-Splatting enables smooth transitions when the focal length is increased.
Even for GB datasets, the final renderable representation can usually be
compressed to less than 70 MB, enabling interactive rendering on low-end
devices using rasterization.";Simon Niedermayr<author:sep>Christoph Neuhauser<author:sep>Kaloian Petkov<author:sep>Klaus Engel<author:sep>Rüdiger Westermann;http://arxiv.org/pdf/2404.11285v1;cs.GR;;gaussian splatting
2404.10484v1;http://arxiv.org/abs/2404.10484v1;2024-04-16;AbsGS: Recovering Fine Details for 3D Gaussian Splatting;"3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with
differentiable rasterization to achieve high-quality novel view synthesis
results while providing advanced real-time rendering performance. However, due
to the flaw of its adaptive density control strategy in 3D-GS, it frequently
suffers from over-reconstruction issue in intricate scenes containing
high-frequency details, leading to blurry rendered images. The underlying
reason for the flaw has still been under-explored. In this work, we present a
comprehensive analysis of the cause of aforementioned artifacts, namely
gradient collision, which prevents large Gaussians in over-reconstructed
regions from splitting. To address this issue, we propose the novel
homodirectional view-space positional gradient as the criterion for
densification. Our strategy efficiently identifies large Gaussians in
over-reconstructed regions, and recovers fine details by splitting. We evaluate
our proposed method on various challenging datasets. The experimental results
indicate that our approach achieves the best rendering quality with reduced or
similar memory consumption. Our method is easy to implement and can be
incorporated into a wide variety of most recent Gaussian Splatting-based
methods. We will open source our codes upon formal publication. Our project
page is available at: https://ty424.github.io/AbsGS.github.io/";Zongxin Ye<author:sep>Wenyu Li<author:sep>Sidun Liu<author:sep>Peng Qiao<author:sep>Yong Dou;http://arxiv.org/pdf/2404.10484v1;cs.CV;;gaussian splatting
2404.10625v1;http://arxiv.org/abs/2404.10625v1;2024-04-16;Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks;"NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or
GIRAFFE have shown very high rendering quality under large representational
variety. However, rendering with Neural Radiance Fields poses challenges for 3D
applications: First, the significant computational demands of NeRF rendering
preclude its use on low-power devices, such as mobiles and VR/AR headsets.
Second, implicit representations based on neural networks are difficult to
incorporate into explicit 3D scenes, such as VR environments or video games. 3D
Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit
3D representation that can be rendered efficiently at high frame rates. In this
work, we present a novel approach that combines the high rendering quality of
NeRF-based 3D-aware GANs with the flexibility and computational advantages of
3DGS. By training a decoder that maps implicit NeRF representations to explicit
3D Gaussian Splatting attributes, we can integrate the representational
diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting
for the first time. Additionally, our approach allows for a high resolution GAN
inversion and real-time GAN editing with 3D Gaussian Splatting scenes.";Florian Barthel<author:sep>Arian Beckmann<author:sep>Wieland Morgenstern<author:sep>Anna Hilsmann<author:sep>Peter Eisert;http://arxiv.org/pdf/2404.10625v1;cs.CV;CVPRW;gaussian splatting<tag:sep>nerf
2404.10441v1;http://arxiv.org/abs/2404.10441v1;2024-04-16;1st Place Solution for ICCV 2023 OmniObject3D Challenge: Sparse-View  Reconstruction;"In this report, we present the 1st place solution for ICCV 2023 OmniObject3D
Challenge: Sparse-View Reconstruction. The challenge aims to evaluate
approaches for novel view synthesis and surface reconstruction using only a few
posed images of each object. We utilize Pixel-NeRF as the basic model, and
apply depth supervision as well as coarse-to-fine positional encoding. The
experiments demonstrate the effectiveness of our approach in improving
sparse-view reconstruction quality. We ranked first in the final test with a
PSNR of 25.44614.";Hang Du<author:sep>Yaping Xue<author:sep>Weidong Dai<author:sep>Xuejun Yan<author:sep>Jingjing Wang;http://arxiv.org/pdf/2404.10441v1;cs.CV;;nerf
2404.10272v1;http://arxiv.org/abs/2404.10272v1;2024-04-16;Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using  VDB Grid and Hierarchical Ray Traversal;"Transmittance estimators such as Occupancy Grid (OG) can accelerate the
training and rendering of Neural Radiance Field (NeRF) by predicting important
samples that contributes much to the generated image. However, OG manages
occupied regions in the form of the dense binary grid, in which there are many
blocks with the same values that cause redundant examination of voxels'
emptiness in ray-tracing. In our work, we introduce two techniques to improve
the efficiency of ray-tracing in trained OG without fine-tuning. First, we
replace the dense grids with VDB grids to reduce the spatial redundancy.
Second, we use hierarchical digital differential analyzer (HDDA) to efficiently
trace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF
360 datasets show that our proposed method successfully accelerates rendering
NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in
average, compared to a fast implementation of OG, NerfAcc, without losing the
quality of rendered images.";Yoshio Kato<author:sep>Shuhei Tarashima;http://arxiv.org/pdf/2404.10272v1;cs.CV;"Short paper for CVPR Neural Rendering Intelligence Workshop 2024.
  Code: https://github.com/Yosshi999/faster-occgrid";nerf
2404.10772v1;http://arxiv.org/abs/2404.10772v1;2024-04-16;Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in  Unbounded Scenes;"Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view
synthesis results, while allowing the rendering of high-resolution images in
real-time. However, leveraging 3D Gaussians for surface reconstruction poses
significant challenges due to the explicit and disconnected nature of 3D
Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel
approach for efficient, high-quality, and compact surface reconstruction in
unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of
3D Gaussians, enabling direct geometry extraction from 3D Gaussians by
identifying its levelset, without resorting to Poisson reconstruction or TSDF
fusion as in previous work. We approximate the surface normal of Gaussians as
the normal of the ray-Gaussian intersection plane, enabling the application of
regularization that significantly enhances geometry. Furthermore, we develop an
efficient geometry extraction method utilizing marching tetrahedra, where the
tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's
complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based
methods in surface reconstruction and novel view synthesis. Further, it
compares favorably to, or even outperforms, neural implicit methods in both
quality and speed.";Zehao Yu<author:sep>Torsten Sattler<author:sep>Andreas Geiger;http://arxiv.org/pdf/2404.10772v1;cs.CV;"Project page:
  https://niujinshuchong.github.io/gaussian-opacity-fields";gaussian splatting
2404.10318v1;http://arxiv.org/abs/2404.10318v1;2024-04-16;SRGS: Super-Resolution 3D Gaussian Splatting;"Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel
explicit 3D representation. This approach relies on the representation power of
Gaussian primitives to provide a high-quality rendering. However, primitives
optimized at low resolution inevitably exhibit sparsity and texture deficiency,
posing a challenge for achieving high-resolution novel view synthesis (HRNVS).
To address this problem, we propose Super-Resolution 3D Gaussian Splatting
(SRGS) to perform the optimization in a high-resolution (HR) space. The
sub-pixel constraint is introduced for the increased viewpoints in HR space,
exploiting the sub-pixel cross-view information of the multiple low-resolution
(LR) views. The gradient accumulated from more viewpoints will facilitate the
densification of primitives. Furthermore, a pre-trained 2D super-resolution
model is integrated with the sub-pixel constraint, enabling these dense
primitives to learn faithful texture features. In general, our method focuses
on densification and texture learning to effectively enhance the representation
ability of primitives. Experimentally, our method achieves high rendering
quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on
challenging datasets such as Mip-NeRF 360 and Tanks & Temples. Related codes
will be released upon acceptance.";Xiang Feng<author:sep>Yongbo He<author:sep>Yubo Wang<author:sep>Yan Yang<author:sep>Zhenzhong Kuang<author:sep>Yu Jun<author:sep>Jianping Fan<author:sep>Jiajun ding;http://arxiv.org/pdf/2404.10318v1;cs.CV;submit ACM MM 2024;gaussian splatting<tag:sep>nerf
2404.10603v1;http://arxiv.org/abs/2404.10603v1;2024-04-16;Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences;"Leveraging multi-view diffusion models as priors for 3D optimization have
alleviated the problem of 3D consistency, e.g., the Janus face problem or the
content drift problem, in zero-shot text-to-3D models. However, the 3D
geometric fidelity of the output remains an unresolved issue; albeit the
rendered 2D views are realistic, the underlying geometry may contain errors
such as unreasonable concavities. In this work, we propose CorrespondentDream,
an effective method to leverage annotation-free, cross-view correspondences
yielded from the diffusion U-Net to provide additional 3D prior to the NeRF
optimization process. We find that these correspondences are strongly
consistent with human perception, and by adopting it in our loss design, we are
able to produce NeRF models with geometries that are more coherent with common
sense, e.g., more smoothed object surface, yielding higher 3D fidelity. We
demonstrate the efficacy of our approach through various comparative
qualitative results and a solid user study.";Seungwook Kim<author:sep>Kejie Li<author:sep>Xueqing Deng<author:sep>Yichun Shi<author:sep>Minsu Cho<author:sep>Peng Wang;http://arxiv.org/pdf/2404.10603v1;cs.CV;25 pages, 22 figures, accepted to CVPR 2024;nerf
2404.09458v1;http://arxiv.org/abs/2404.09458v1;2024-04-15;CompGS: Efficient 3D Scene Representation via Compressed Gaussian  Splatting;"Gaussian splatting, renowned for its exceptional rendering quality and
efficiency, has emerged as a prominent technique in 3D scene representation.
However, the substantial data volume of Gaussian splatting impedes its
practical utility in real-world applications. Herein, we propose an efficient
3D scene representation, named Compressed Gaussian Splatting (CompGS), which
harnesses compact Gaussian primitives for faithful 3D scene modeling with a
remarkably reduced data size. To ensure the compactness of Gaussian primitives,
we devise a hybrid primitive structure that captures predictive relationships
between each other. Then, we exploit a small set of anchor primitives for
prediction, allowing the majority of primitives to be encapsulated into highly
compact residual forms. Moreover, we develop a rate-constrained optimization
scheme to eliminate redundancies within such hybrid primitives, steering our
CompGS towards an optimal trade-off between bitrate consumption and
representation efficacy. Experimental results show that the proposed CompGS
significantly outperforms existing methods, achieving superior compactness in
3D scene representation without compromising model accuracy and rendering
quality. Our code will be released on GitHub for further research.";Xiangrui Liu<author:sep>Xinju Wu<author:sep>Pingping Zhang<author:sep>Shiqi Wang<author:sep>Zhu Li<author:sep>Sam Kwong;http://arxiv.org/pdf/2404.09458v1;cs.CV;Submitted to a conference;gaussian splatting
2404.09833v1;http://arxiv.org/abs/2404.09833v1;2024-04-15;Video2Game: Real-time, Interactive, Realistic and Browser-Compatible  Environment from a Single Video;"Creating high-quality and interactive virtual environments, such as games and
simulators, often involves complex and costly manual modeling processes. In
this paper, we present Video2Game, a novel approach that automatically converts
videos of real-world scenes into realistic and interactive game environments.
At the heart of our system are three core components:(i) a neural radiance
fields (NeRF) module that effectively captures the geometry and visual
appearance of the scene; (ii) a mesh module that distills the knowledge from
NeRF for faster rendering; and (iii) a physics module that models the
interactions and physical dynamics among the objects. By following the
carefully designed pipeline, one can construct an interactable and actionable
digital replica of the real world. We benchmark our system on both indoor and
large-scale outdoor scenes. We show that we can not only produce
highly-realistic renderings in real-time, but also build interactive games on
top.";Hongchi Xia<author:sep>Zhi-Hao Lin<author:sep>Wei-Chiu Ma<author:sep>Shenlong Wang;http://arxiv.org/pdf/2404.09833v1;cs.CV;CVPR 2024. Project page (with code): https://video2game.github.io/;nerf
2404.09995v1;http://arxiv.org/abs/2404.09995v1;2024-04-15;Taming Latent Diffusion Model for Neural Radiance Field Inpainting;"Neural Radiance Field (NeRF) is a representation for 3D reconstruction from
multi-view images. Despite some recent work showing preliminary success in
editing a reconstructed NeRF with diffusion prior, they remain struggling to
synthesize reasonable geometry in completely uncovered regions. One major
reason is the high diversity of synthetic contents from the diffusion model,
which hinders the radiance field from converging to a crisp and deterministic
geometry. Moreover, applying latent diffusion models on real data often yields
a textural shift incoherent to the image condition due to auto-encoding errors.
These two problems are further reinforced with the use of pixel-distance
losses. To address these issues, we propose tempering the diffusion model's
stochasticity with per-scene customization and mitigating the textural shift
with masked adversarial training. During the analyses, we also found the
commonly used pixel and perceptual losses are harmful in the NeRF inpainting
task. Through rigorous experiments, our framework yields state-of-the-art NeRF
inpainting results on various real-world scenes. Project page:
https://hubert0527.github.io/MALD-NeRF";Chieh Hubert Lin<author:sep>Changil Kim<author:sep>Jia-Bin Huang<author:sep>Qinbo Li<author:sep>Chih-Yao Ma<author:sep>Johannes Kopf<author:sep>Ming-Hsuan Yang<author:sep>Hung-Yu Tseng;http://arxiv.org/pdf/2404.09995v1;cs.CV;Project page: https://hubert0527.github.io/MALD-NeRF;nerf
2404.09748v1;http://arxiv.org/abs/2404.09748v1;2024-04-15;LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted  Gaussian Primitives;"Large garages are ubiquitous yet intricate scenes in our daily lives, posing
challenges characterized by monotonous colors, repetitive patterns, reflective
surfaces, and transparent vehicle glass. Conventional Structure from Motion
(SfM) methods for camera pose estimation and 3D reconstruction fail in these
environments due to poor correspondence construction. To address these
challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting
approach for large-scale garage modeling and rendering. We develop a handheld
scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate
accurate LiDAR and image data scanning. With this Polar device, we present a
GarageWorld dataset consisting of five expansive garage scenes with diverse
geometric structures and will release the dataset to the community for further
research. We demonstrate that the collected LiDAR point cloud by the Polar
device enhances a suite of 3D Gaussian splatting algorithms for garage scene
modeling and rendering. We also propose a novel depth regularizer for 3D
Gaussian splatting algorithm training, effectively eliminating floating
artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian
renderer for real-time viewing on web-based devices. Additionally, we explore a
hybrid representation that combines the advantages of traditional mesh in
depicting simple geometry and colors (e.g., walls and the ground) with modern
3D Gaussian representations capturing complex details and high-frequency
textures. This strategy achieves an optimal balance between memory performance
and rendering quality. Experimental results on our dataset, along with
ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering
quality and resource efficiency.";Jiadi Cui<author:sep>Junming Cao<author:sep>Yuhui Zhong<author:sep>Liao Wang<author:sep>Fuqiang Zhao<author:sep>Penghao Wang<author:sep>Yifan Chen<author:sep>Zhipeng He<author:sep>Lan Xu<author:sep>Yujiao Shi<author:sep>Yingliang Zhang<author:sep>Jingyi Yu;http://arxiv.org/pdf/2404.09748v1;cs.CV;Project Page: https://jdtsui.github.io/letsgo/;gaussian splatting
2404.09591v1;http://arxiv.org/abs/2404.09591v1;2024-04-15;3D Gaussian Splatting as Markov Chain Monte Carlo;"While 3D Gaussian Splatting has recently become popular for neural rendering,
current methods rely on carefully engineered cloning and splitting strategies
for placing Gaussians, which does not always generalize and may lead to
poor-quality renderings. In addition, for real-world scenes, they rely on a
good initial point cloud to perform well. In this work, we rethink 3D Gaussians
as random samples drawn from an underlying probability distribution describing
the physical representation of the scene -- in other words, Markov Chain Monte
Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates are
strikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update. As
with MCMC, samples are nothing but past visit locations, adding new Gaussians
under our framework can simply be realized without heuristics as placing
Gaussians at existing Gaussian locations. To encourage using fewer Gaussians
for efficiency, we introduce an L1-regularizer on the Gaussians. On various
standard evaluation scenes, we show that our method provides improved rendering
quality, easy control over the number of Gaussians, and robustness to
initialization.";Shakiba Kheradmand<author:sep>Daniel Rebain<author:sep>Gopal Sharma<author:sep>Weiwei Sun<author:sep>Jeff Tseng<author:sep>Hossam Isack<author:sep>Abhishek Kar<author:sep>Andrea Tagliasacchi<author:sep>Kwang Moo Yi;http://arxiv.org/pdf/2404.09591v1;cs.CV;;gaussian splatting
2404.09412v1;http://arxiv.org/abs/2404.09412v1;2024-04-15;DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred  Shading;"Reconstructing and editing 3D objects and scenes both play crucial roles in
computer graphics and computer vision. Neural radiance fields (NeRFs) can
achieve realistic reconstruction and editing results but suffer from
inefficiency in rendering. Gaussian splatting significantly accelerates
rendering by rasterizing Gaussian ellipsoids. However, Gaussian splatting
utilizes a single Spherical Harmonic (SH) function to model both texture and
lighting, limiting independent editing capabilities of these components.
Recently, attempts have been made to decouple texture and lighting with the
Gaussian splatting representation but may fail to produce plausible geometry
and decomposition results on reflective scenes. Additionally, the forward
shading technique they employ introduces noticeable blending artifacts during
relighting, as the geometry attributes of Gaussians are optimized under the
original illumination and may not be suitable for novel lighting conditions. To
address these issues, we introduce DeferredGS, a method for decoupling and
editing the Gaussian splatting representation using deferred shading. To
achieve successful decoupling, we model the illumination with a learnable
environment map and define additional attributes such as texture parameters and
normal direction on Gaussians, where the normal is distilled from a jointly
trained signed distance function. More importantly, we apply deferred shading,
resulting in more realistic relighting effects compared to previous methods.
Both qualitative and quantitative experiments demonstrate the superior
performance of DeferredGS in novel view synthesis and editing tasks.";Tong Wu<author:sep>Jia-Mu Sun<author:sep>Yu-Kun Lai<author:sep>Yuewen Ma<author:sep>Leif Kobbelt<author:sep>Lin Gao;http://arxiv.org/pdf/2404.09412v1;cs.CV;;gaussian splatting<tag:sep>nerf
2404.09271v1;http://arxiv.org/abs/2404.09271v1;2024-04-14;VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field;"Visual relocalization is a key technique to autonomous driving, robotics, and
virtual/augmented reality. After decades of explorations, absolute pose
regression (APR), scene coordinate regression (SCR), and hierarchical methods
(HMs) have become the most popular frameworks. However, in spite of high
efficiency, APRs and SCRs have limited accuracy especially in large-scale
outdoor scenes; HMs are accurate but need to store a large number of 2D
descriptors for matching, resulting in poor efficiency. In this paper, we
propose an efficient and accurate framework, called VRS-NeRF, for visual
relocalization with sparse neural radiance field. Precisely, we introduce an
explicit geometric map (EGM) for 3D map representation and an implicit learning
map (ILM) for sparse patches rendering. In this localization process, EGP
provides priors of spare 2D points and ILM utilizes these sparse points to
render patches with sparse NeRFs for matching. This allows us to discard a
large number of 2D descriptors so as to reduce the map size. Moreover,
rendering patches only for useful points rather than all pixels in the whole
image reduces the rendering time significantly. This framework inherits the
accuracy of HMs and discards their low efficiency. Experiments on 7Scenes,
CambridgeLandmarks, and Aachen datasets show that our method gives much better
accuracy than APRs and SCRs, and close performance to HMs but is much more
efficient.";Fei Xue<author:sep>Ignas Budvytis<author:sep>Daniel Olmeda Reino<author:sep>Roberto Cipolla;http://arxiv.org/pdf/2404.09271v1;cs.CV;source code https://github.com/feixue94/vrs-nerf;nerf
2404.09227v1;http://arxiv.org/abs/2404.09227v1;2024-04-14;DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation  Modeling;"Recent progress in text-to-3D creation has been propelled by integrating the
potent prior of Diffusion Models from text-to-image generation into the 3D
domain. Nevertheless, generating 3D scenes characterized by multiple instances
and intricate arrangements remains challenging. In this study, we present
DreamScape, a method for creating highly consistent 3D scenes solely from
textual descriptions, leveraging the strong 3D representation capabilities of
Gaussian Splatting and the complex arrangement abilities of large language
models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene
representation, consisting of semantic primitives (objects) and their spatial
transformations and relationships derived directly from text prompts using
LLMs. This compositional representation allows for local-to-global optimization
of the entire scene. A progressive scale control is tailored during local
object generation, ensuring that objects of different sizes and densities adapt
to the scene, which addresses training instability issue arising from simple
blending in the subsequent global optimization stage. To mitigate potential
biases of LLM priors, we model collision relationships between objects at the
global level, enhancing physical correctness and overall realism. Additionally,
to generate pervasive objects like rain and snow distributed extensively across
the scene, we introduce a sparse initialization and densification strategy.
Experiments demonstrate that DreamScape offers high usability and
controllability, enabling the generation of high-fidelity 3D scenes from only
text prompts and achieving state-of-the-art performance compared to other
methods.";Xuening Yuan<author:sep>Hongyu Yang<author:sep>Yueming Zhao<author:sep>Di Huang;http://arxiv.org/pdf/2404.09227v1;cs.CV;;gaussian splatting
2404.09105v1;http://arxiv.org/abs/2404.09105v1;2024-04-14;EGGS: Edge Guided Gaussian Splatting for Radiance Fields;"The Gaussian splatting methods are getting popular. However, their loss
function only contains the $\ell_1$ norm and the structural similarity between
the rendered and input images, without considering the edges in these images.
It is well-known that the edges in an image provide important information.
Therefore, in this paper, we propose an Edge Guided Gaussian Splatting (EGGS)
method that leverages the edges in the input images. More specifically, we give
the edge region a higher weight than the flat region. With such edge guidance,
the resulting Gaussian particles focus more on the edges instead of the flat
regions. Moreover, such edge guidance does not crease the computation cost
during the training and rendering stage. The experiments confirm that such
simple edge-weighted loss function indeed improves about $1\sim2$ dB on several
difference data sets. With simply plugging in the edge guidance, the proposed
method can improve all Gaussian splatting methods in different scenarios, such
as human head modeling, building 3D reconstruction, etc.";Yuanhao Gong;http://arxiv.org/pdf/2404.09105v1;cs.CV;;gaussian splatting
2404.08966v2;http://arxiv.org/abs/2404.08966v2;2024-04-13;LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via  Eulerian Motion Field;"Cinemagraph is a unique form of visual media that combines elements of still
photography and subtle motion to create a captivating experience. However, the
majority of videos generated by recent works lack depth information and are
confined to the constraints of 2D image space. In this paper, inspired by
significant progress in the field of novel view synthesis (NVS) achieved by 3D
Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from
2D image space to 3D space using 3D Gaussian modeling. To achieve this, we
first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from
multi-view images of static scenes,incorporating shape regularization terms to
prevent blurring or artifacts caused by object deformation. We then adopt an
autoencoder tailored for 3D Gaussian to project it into feature space. To
maintain the local continuity of the scene, we devise SuperGaussian for
clustering based on the acquired features. By calculating the similarity
between clusters and employing a two-stage estimation method, we derive an
Eulerian motion field to describe velocities across the entire scene. The 3D
Gaussian points then move within the estimated Eulerian motion field. Through
bidirectional animation techniques, we ultimately generate a 3D Cinemagraph
that exhibits natural and seamlessly loopable dynamics. Experiment results
validate the effectiveness of our approach, demonstrating high-quality and
visually appealing scene generation. The project is available at
https://pokerlishao.github.io/LoopGaussian/.";Jiyang Li<author:sep>Lechao Cheng<author:sep>Zhangye Wang<author:sep>Tingting Mu<author:sep>Jingxuan He;http://arxiv.org/pdf/2404.08966v2;cs.CV;10 pages;gaussian splatting
2404.08312v1;http://arxiv.org/abs/2404.08312v1;2024-04-12;GPN: Generative Point-based NeRF;"Scanning real-life scenes with modern registration devices typically gives
incomplete point cloud representations, primarily due to the limitations of
partial scanning, 3D occlusions, and dynamic light conditions. Recent works on
processing incomplete point clouds have always focused on point cloud
completion. However, these approaches do not ensure consistency between the
completed point cloud and the captured images regarding color and geometry. We
propose using Generative Point-based NeRF (GPN) to reconstruct and repair a
partial cloud by fully utilizing the scanning images and the corresponding
reconstructed cloud. The repaired point cloud can achieve multi-view
consistency with the captured images at high spatial resolution. For the
finetunes of a single scene, we optimize the global latent condition by
incorporating an Auto-Decoder architecture while retaining multi-view
consistency. As a result, the generated point clouds are smooth, plausible, and
geometrically consistent with the partial scanning images. Extensive
experiments on ShapeNet demonstrate that our works achieve competitive
performances to the other state-of-the-art point cloud-based neural scene
rendering and editing performances.";Haipeng Wang;http://arxiv.org/pdf/2404.08312v1;cs.CV;;nerf
2404.08252v1;http://arxiv.org/abs/2404.08252v1;2024-04-12;MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based  Monocular Guidance;"The latest regularized Neural Radiance Field (NeRF) approaches produce poor
geometry and view extrapolation for multiview stereo (MVS) benchmarks such as
ETH3D. In this paper, we aim to create 3D models that provide accurate geometry
and view synthesis, partially closing the large geometric performance gap
between NeRF and traditional MVS methods. We propose a patch-based approach
that effectively leverages monocular surface normal and relative depth
predictions. The patch-based ray sampling also enables the appearance
regularization of normalized cross-correlation (NCC) and structural similarity
(SSIM) between randomly sampled virtual and training views. We further show
that ""density restrictions"" based on sparse structure-from-motion points can
help greatly improve geometric accuracy with a slight drop in novel view
synthesis metrics. Our experiments show 4x the performance of RegNeRF and 8x
that of FreeNeRF on average F1@2cm for ETH3D MVS benchmark, suggesting a
fruitful research direction to improve the geometric accuracy of NeRF-based
models, and sheds light on a potential future approach to enable NeRF-based
optimization to eventually outperform traditional MVS.";Yuqun Wu<author:sep>Jae Yong Lee<author:sep>Chuhang Zou<author:sep>Shenlong Wang<author:sep>Derek Hoiem;http://arxiv.org/pdf/2404.08252v1;cs.CV;26 pages, 15 figures;nerf
2404.08449v2;http://arxiv.org/abs/2404.08449v2;2024-04-12;OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering;"Rendering dynamic 3D human from monocular videos is crucial for various
applications such as virtual reality and digital entertainment. Most methods
assume the people is in an unobstructed scene, while various objects may cause
the occlusion of body parts in real-life scenarios. Previous method utilizing
NeRF for surface rendering to recover the occluded areas, but it requiring more
than one day to train and several seconds to render, failing to meet the
requirements of real-time interactive applications. To address these issues, we
propose OccGaussian based on 3D Gaussian Splatting, which can be trained within
6 minutes and produces high-quality human renderings up to 160 FPS with
occluded input. OccGaussian initializes 3D Gaussian distributions in the
canonical space, and we perform occlusion feature query at occluded regions,
the aggregated pixel-align feature is extracted to compensate for the missing
information. Then we use Gaussian Feature MLP to further process the feature
along with the occlusion-aware loss functions to better perceive the occluded
area. Extensive experiments both in simulated and real-world occlusions,
demonstrate that our method achieves comparable or even superior performance
compared to the state-of-the-art method. And we improving training and
inference speeds by 250x and 800x, respectively. Our code will be available for
research purposes.";Jingrui Ye<author:sep>Zongkai Zhang<author:sep>Yujiao Jiang<author:sep>Qingmin Liao<author:sep>Wenming Yang<author:sep>Zongqing Lu;http://arxiv.org/pdf/2404.08449v2;cs.CV;;gaussian splatting<tag:sep>nerf
2404.07933v1;http://arxiv.org/abs/2404.07933v1;2024-04-11;Boosting Self-Supervision for Single-View Scene Completion via Knowledge  Distillation;"Inferring scene geometry from images via Structure from Motion is a
long-standing and fundamental problem in computer vision. While classical
approaches and, more recently, depth map predictions only focus on the visible
parts of a scene, the task of scene completion aims to reason about geometry
even in occluded regions. With the popularity of neural radiance fields
(NeRFs), implicit representations also became popular for scene completion by
predicting so-called density fields. Unlike explicit approaches. e.g.
voxel-based methods, density fields also allow for accurate depth prediction
and novel-view synthesis via image-based rendering. In this work, we propose to
fuse the scene reconstruction from multiple images and distill this knowledge
into a more accurate single-view scene reconstruction. To this end, we propose
Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed
images, trained fully self-supervised only from image data. Using knowledge
distillation, we use MVBTS to train a single-view scene completion network via
direct supervision called KDBTS. It achieves state-of-the-art performance on
occupancy prediction, especially in occluded regions.";Keonhee Han<author:sep>Dominik Muhle<author:sep>Felix Wimbauer<author:sep>Daniel Cremers;http://arxiv.org/pdf/2404.07933v1;cs.CV;;nerf
2404.07474v1;http://arxiv.org/abs/2404.07474v1;2024-04-11;G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images;"Novel view synthesis aims to generate new view images of a given view image
collection. Recent attempts address this problem relying on 3D geometry priors
(e.g., shapes, sizes, and positions) learned from multi-view images. However,
such methods encounter the following limitations: 1) they require a set of
multi-view images as training data for a specific scene (e.g., face, car or
chair), which is often unavailable in many real-world scenarios; 2) they fail
to extract the geometry priors from single-view images due to the lack of
multi-view supervision. In this paper, we propose a Geometry-enhanced NeRF
(G-NeRF), which seeks to enhance the geometry priors by a geometry-guided
multi-view synthesis approach, followed by a depth-aware training. In the
synthesis process, inspired that existing 3D GAN models can unconditionally
synthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D
GAN models, such as EG3D, as a free source to provide geometry priors through
synthesizing multi-view data. Simultaneously, to further improve the geometry
quality of the synthetic data, we introduce a truncation method to effectively
sample latent codes within 3D GAN models. To tackle the absence of multi-view
supervision for single-view images, we design the depth-aware training
approach, incorporating a depth-aware discriminator to guide geometry priors
through depth maps. Experiments demonstrate the effectiveness of our method in
terms of both qualitative and quantitative results.";Zixiong Huang<author:sep>Qi Chen<author:sep>Libo Sun<author:sep>Yifan Yang<author:sep>Naizhou Wang<author:sep>Mingkui Tan<author:sep>Qi Wu;http://arxiv.org/pdf/2404.07474v1;cs.CV;CVPR 2024 Accepted Paper;nerf
2404.07991v1;http://arxiv.org/abs/2404.07991v1;2024-04-11;GoMAvatar: Efficient Animatable Human Modeling from Monocular Video  Using Gaussians-on-Mesh;"We introduce GoMAvatar, a novel approach for real-time, memory-efficient,
high-quality animatable human modeling. GoMAvatar takes as input a single
monocular video to create a digital avatar capable of re-articulation in new
poses and real-time rendering from novel viewpoints, while seamlessly
integrating with rasterization-based graphics pipelines. Central to our method
is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering
quality and speed of Gaussian splatting with geometry modeling and
compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and
various YouTube videos. GoMAvatar matches or surpasses current monocular human
modeling algorithms in rendering quality and significantly outperforms them in
computational efficiency (43 FPS) while being memory-efficient (3.63 MB per
subject).";Jing Wen<author:sep>Xiaoming Zhao<author:sep>Zhongzheng Ren<author:sep>Alexander G. Schwing<author:sep>Shenlong Wang;http://arxiv.org/pdf/2404.07991v1;cs.CV;"CVPR 2024; project page: https://wenj.github.io/GoMAvatar/";gaussian splatting
2404.07993v1;http://arxiv.org/abs/2404.07993v1;2024-04-11;Connecting NeRFs, Images, and Text;"Neural Radiance Fields (NeRFs) have emerged as a standard framework for
representing 3D scenes and objects, introducing a novel data type for
information exchange and storage. Concurrently, significant progress has been
made in multimodal representation learning for text and image data. This paper
explores a novel research direction that aims to connect the NeRF modality with
other modalities, similar to established methodologies for images and text. To
this end, we propose a simple framework that exploits pre-trained models for
NeRF representations alongside multimodal models for text and image processing.
Our framework learns a bidirectional mapping between NeRF embeddings and those
obtained from corresponding images and text. This mapping unlocks several novel
and useful applications, including NeRF zero-shot classification and NeRF
retrieval from images or text.";Francesco Ballerini<author:sep>Pierluigi Zama Ramirez<author:sep>Roberto Mirabella<author:sep>Samuele Salti<author:sep>Luigi Di Stefano;http://arxiv.org/pdf/2404.07993v1;cs.CV;Accepted at CVPRW-INRV 2024;nerf
2404.07762v2;http://arxiv.org/abs/2404.07762v2;2024-04-11;NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous  Driving;"We present a versatile NeRF-based simulator for testing autonomous driving
(AD) software systems, designed with a focus on sensor-realistic closed-loop
evaluation and the creation of safety-critical scenarios. The simulator learns
from sequences of real-world driving sensor data and enables reconfigurations
and renderings of new, unseen scenarios. In this work, we use our simulator to
test the responses of AD models to safety-critical scenarios inspired by the
European New Car Assessment Programme (Euro NCAP). Our evaluation reveals that,
while state-of-the-art end-to-end planners excel in nominal driving scenarios
in an open-loop setting, they exhibit critical flaws when navigating our
safety-critical scenarios in a closed-loop setting. This highlights the need
for advancements in the safety and real-world usability of end-to-end planners.
By publicly releasing our simulator and scenarios as an easy-to-run evaluation
suite, we invite the research community to explore, refine, and validate their
AD models in controlled, yet highly configurable and challenging
sensor-realistic environments. Code and instructions can be found at
https://github.com/wljungbergh/NeuroNCAP";William Ljungbergh<author:sep>Adam Tonderski<author:sep>Joakim Johnander<author:sep>Holger Caesar<author:sep>Kalle Åström<author:sep>Michael Felsberg<author:sep>Christoffer Petersson;http://arxiv.org/pdf/2404.07762v2;cs.CV;;nerf
2404.06832v1;http://arxiv.org/abs/2404.06832v1;2024-04-10;SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection;"Detecting anomalies in images has become a well-explored problem in both
academia and industry. State-of-the-art algorithms are able to detect defects
in increasingly difficult settings and data modalities. However, most current
methods are not suited to address 3D objects captured from differing poses.
While solutions using Neural Radiance Fields (NeRFs) have been proposed, they
suffer from excessive computation requirements, which hinder real-world
usability. For this reason, we propose the novel 3D Gaussian splatting-based
framework SplatPose which, given multi-view images of a 3D object, accurately
estimates the pose of unseen views in a differentiable manner, and detects
anomalies in them. We achieve state-of-the-art results in both training and
inference speed, and detection performance, even when using less training data
than competing methods. We thoroughly evaluate our framework using the recently
proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly
detection (MAD) data set.";Mathis Kruse<author:sep>Marco Rudolph<author:sep>Dominik Woiwode<author:sep>Bodo Rosenhahn;http://arxiv.org/pdf/2404.06832v1;cs.CV;Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024;gaussian splatting<tag:sep>nerf
2404.06753v1;http://arxiv.org/abs/2404.06753v1;2024-04-10;MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D  Reconstruction of Indoor Scenes from Monocular RGB Views;"Current monocular 3D scene reconstruction (3DR) works are either
fully-supervised, or not generalizable, or implicit in 3D representation. We
propose a novel framework - MonoSelfRecon that for the first time achieves
explicit 3D mesh reconstruction for generalizable indoor scenes with monocular
RGB views by purely self-supervision on voxel-SDF (signed distance function).
MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and
a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF
in self-supervision. We propose novel self-supervised losses, which not only
support pure self-supervision, but can be used together with supervised signals
to further boost supervised training. Our experiments show that ""MonoSelfRecon""
trained in pure self-supervision outperforms current best self-supervised
indoor depth estimation models and is comparable to 3DR models trained in fully
supervision with depth annotations. MonoSelfRecon is not restricted by specific
model design, which can be used to any models with voxel-SDF for purely
self-supervised manner.";Runfa Li<author:sep>Upal Mahbub<author:sep>Vasudev Bhaskaran<author:sep>Truong Nguyen;http://arxiv.org/pdf/2404.06753v1;cs.CV;;nerf
2404.06710v3;http://arxiv.org/abs/2404.06710v3;2024-04-10;SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike  Camera;"One of the most critical factors in achieving sharp Novel View Synthesis
(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) is the quality of the training images. However,
Conventional RGB cameras are susceptible to motion blur. In contrast,
neuromorphic cameras like event and spike cameras inherently capture more
comprehensive temporal information, which can provide a sharp representation of
the scene as additional training data. Recent methods have explored the
integration of event cameras to improve the quality of NVS. The event-RGB
approaches have some limitations, such as high training costs and the inability
to work effectively in the background. Instead, our study introduces a new
method that uses the spike camera to overcome these limitations. By considering
texture reconstruction from spike streams as ground truth, we design the
Texture from Spike (TfS) loss. Since the spike camera relies on temporal
integration instead of temporal differentiation used by event cameras, our
proposed TfS loss maintains manageable training costs. It handles foreground
objects with backgrounds simultaneously. We also provide a real-world dataset
captured with our spike-RGB camera system to facilitate future research
endeavors. We conduct extensive experiments using synthetic and real-world
datasets to demonstrate that our design can enhance novel view synthesis across
NeRF and 3DGS. The code and dataset will be made available for public access.";Gaole Dai<author:sep>Zhenyu Wang<author:sep>Qinwen Xu<author:sep>Ming Lu<author:sep>Wen Chen<author:sep>Boxin Shi<author:sep>Shanghang Zhang<author:sep>Tiejun Huang;http://arxiv.org/pdf/2404.06710v3;cs.CV;;gaussian splatting<tag:sep>nerf
2404.07199v1;http://arxiv.org/abs/2404.07199v1;2024-04-10;RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth  Diffusion;"We introduce RealmDreamer, a technique for generation of general
forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D
Gaussian Splatting representation to match complex text prompts. We initialize
these splats by utilizing the state-of-the-art text-to-image generators,
lifting their samples into 3D, and computing the occlusion volume. We then
optimize this representation across multiple views as a 3D inpainting task with
image-conditional diffusion models. To learn correct geometric structure, we
incorporate a depth diffusion model by conditioning on the samples from the
inpainting model, giving rich geometric structure. Finally, we finetune the
model using sharpened samples from image generators. Notably, our technique
does not require video or multi-view data and can synthesize a variety of
high-quality 3D scenes in different styles, consisting of multiple objects. Its
generality additionally allows 3D synthesis from a single image.";Jaidev Shriram<author:sep>Alex Trevithick<author:sep>Lingjie Liu<author:sep>Ravi Ramamoorthi;http://arxiv.org/pdf/2404.07199v1;cs.CV;Project Page: https://realmdreamer.github.io/;gaussian splatting
2404.06903v1;http://arxiv.org/abs/2404.06903v1;2024-04-10;DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic  Gaussian Splatting;"The increasing demand for virtual reality applications has highlighted the
significance of crafting immersive 3D assets. We present a text-to-3D
360$^{\circ}$ scene generation pipeline that facilitates the creation of
comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of
minutes. Our approach utilizes the generative power of a 2D diffusion model and
prompt self-refinement to create a high-quality and globally coherent panoramic
image. This image acts as a preliminary ""flat"" (2D) scene representation.
Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to
enable real-time exploration. To produce consistent 3D geometry, our pipeline
constructs a spatially coherent structure by aligning the 2D monocular depth
into a globally optimized point cloud. This point cloud serves as the initial
state for the centroids of 3D Gaussians. In order to address invisible issues
inherent in single-view inputs, we impose semantic and geometric constraints on
both synthesized and input camera views as regularizations. These guide the
optimization of Gaussians, aiding in the reconstruction of unseen regions. In
summary, our method offers a globally consistent 3D scene within a
360$^{\circ}$ perspective, providing an enhanced immersive experience over
existing techniques. Project website at: http://dreamscene360.github.io/";Shijie Zhou<author:sep>Zhiwen Fan<author:sep>Dejia Xu<author:sep>Haoran Chang<author:sep>Pradyumna Chari<author:sep>Tejas Bharadwaj<author:sep>Suya You<author:sep>Zhangyang Wang<author:sep>Achuta Kadambi;http://arxiv.org/pdf/2404.06903v1;cs.CV;;gaussian splatting
2404.06727v1;http://arxiv.org/abs/2404.06727v1;2024-04-10;Bayesian NeRF: Quantifying Uncertainty with Volume Density in Neural  Radiance Fields;"We present the Bayesian Neural Radiance Field (NeRF), which explicitly
quantifies uncertainty in geometric volume structures without the need for
additional networks, making it adept for challenging observations and
uncontrolled images. NeRF diverges from traditional geometric methods by
offering an enriched scene representation, rendering color and density in 3D
space from various viewpoints. However, NeRF encounters limitations in relaxing
uncertainties by using geometric structure information, leading to inaccuracies
in interpretation under insufficient real-world observations. Recent research
efforts aimed at addressing this issue have primarily relied on empirical
methods or auxiliary networks. To fundamentally address this issue, we propose
a series of formulational extensions to NeRF. By introducing generalized
approximations and defining density-related uncertainty, our method seamlessly
extends to manage uncertainty not only for RGB but also for depth, without the
need for additional networks or empirical assumptions. In experiments we show
that our method significantly enhances performance on RGB and depth images in
the comprehensive dataset, demonstrating the reliability of the Bayesian NeRF
approach to quantifying uncertainty based on the geometric structure.";Sibeak Lee<author:sep>Kyeongsu Kang<author:sep>Hyeonwoo Yu;http://arxiv.org/pdf/2404.06727v1;cs.CV;;nerf
2404.06926v1;http://arxiv.org/abs/2404.06926v1;2024-04-10;Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D  Gaussian Splatting;"We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian
Splatting as the mapping backend. Leveraging robust pose estimates from our
LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic
mapping system is proposed in this paper. We initialize 3D Gaussians from
colorized LiDAR points and optimize them using differentiable rendering powered
by 3D Gaussian Splatting. Meticulously designed strategies are employed to
incrementally expand the Gaussian map and adaptively control its density,
ensuring high-quality mapping with real-time capability. Experiments conducted
in diverse scenarios demonstrate the superior performance of our method
compared to existing radiance-field-based SLAM systems.";Xiaolei Lang<author:sep>Laijian Li<author:sep>Hang Zhang<author:sep>Feng Xiong<author:sep>Mu Xu<author:sep>Yong Liu<author:sep>Xingxing Zuo<author:sep>Jiajun Lv;http://arxiv.org/pdf/2404.06926v1;cs.RO;Submitted to IROS 2024;gaussian splatting
2404.06814v1;http://arxiv.org/abs/2404.06814v1;2024-04-10;Zero-shot Point Cloud Completion Via 2D Priors;"3D point cloud completion is designed to recover complete shapes from
partially observed point clouds. Conventional completion methods typically
depend on extensive point cloud data for training %, with their effectiveness
often constrained to object categories similar to those seen during training.
In contrast, we propose a zero-shot framework aimed at completing partially
observed point clouds across any unseen categories. Leveraging point rendering
via Gaussian Splatting, we develop techniques of Point Cloud Colorization and
Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion
models to infer missing regions. Experimental results on both synthetic and
real-world scanned point clouds demonstrate that our approach outperforms
existing methods in completing a variety of objects without any requirement for
specific training data.";Tianxin Huang<author:sep>Zhiwen Yan<author:sep>Yuyang Zhao<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2404.06814v1;cs.CV;;gaussian splatting
2404.06246v1;http://arxiv.org/abs/2404.06246v1;2024-04-09;GHNeRF: Learning Generalizable Human Features with Efficient Neural  Radiance Fields;"Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising
results in 3D scene representations, including 3D human representations.
However, these representations often lack crucial information on the underlying
human pose and structure, which is crucial for AR/VR applications and games. In
this paper, we introduce a novel approach, termed GHNeRF, designed to address
these limitations by learning 2D/3D joint locations of human subjects with NeRF
representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract
essential human features from 2D images, which are then incorporated into the
NeRF framework in order to encode human biomechanic features. This allows our
network to simultaneously learn biomechanic features, such as joint locations,
along with human geometry and texture. To assess the effectiveness of our
method, we conduct a comprehensive comparison with state-of-the-art human NeRF
techniques and joint estimation algorithms. Our results show that GHNeRF can
achieve state-of-the-art results in near real-time.";Arnab Dey<author:sep>Di Yang<author:sep>Rohith Agaram<author:sep>Antitza Dantcheva<author:sep>Andrew I. Comport<author:sep>Srinath Sridhar<author:sep>Jean Martinet;http://arxiv.org/pdf/2404.06246v1;cs.CV;;nerf
2404.06429v1;http://arxiv.org/abs/2404.06429v1;2024-04-09;Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion;"Benefiting from the rapid development of 2D diffusion models, 3D content
creation has made significant progress recently. One promising solution
involves the fine-tuning of pre-trained 2D diffusion models to harness their
capacity for producing multi-view images, which are then lifted into accurate
3D models via methods like fast-NeRFs or large reconstruction models. However,
as inconsistency still exists and limited generated resolution, the generation
results of such methods still lack intricate textures and complex geometries.
To solve this problem, we propose Magic-Boost, a multi-view conditioned
diffusion model that significantly refines coarse generative results through a
brief period of SDS optimization ($\sim15$min). Compared to the previous text
or single image based diffusion models, Magic-Boost exhibits a robust
capability to generate images with high consistency from pseudo synthesized
multi-view images. It provides precise SDS guidance that well aligns with the
identity of the input images, enriching the local detail in both geometry and
texture of the initial generative results. Extensive experiments show
Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D
assets with rich geometric and textural details. (Project Page:
https://magic-research.github.io/magic-boost/)";Fan Yang<author:sep>Jianfeng Zhang<author:sep>Yichun Shi<author:sep>Bowen Chen<author:sep>Chenxu Zhang<author:sep>Huichao Zhang<author:sep>Xiaofeng Yang<author:sep>Jiashi Feng<author:sep>Guosheng Lin;http://arxiv.org/pdf/2404.06429v1;cs.CV;;nerf
2404.06152v1;http://arxiv.org/abs/2404.06152v1;2024-04-09;HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields;"In recent advancements in novel view synthesis, generalizable Neural Radiance
Fields (NeRF) based methods applied to human subjects have shown remarkable
results in generating novel views from few images. However, this generalization
ability cannot capture the underlying structural features of the skeleton
shared across all instances. Building upon this, we introduce HFNeRF: a novel
generalizable human feature NeRF aimed at generating human biomechanic features
using a pre-trained image encoder. While previous human NeRF methods have shown
promising results in the generation of photorealistic virtual avatars, such
methods lack underlying human structure or biomechanic features such as
skeleton or joint information that are crucial for downstream applications
including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D
pre-trained foundation models toward learning human features in 3D using neural
rendering, and then volume rendering towards generating 2D feature maps. We
evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as
features. The proposed method is fully differentiable, allowing to successfully
learn color, geometry, and human skeleton in a simultaneous manner. This paper
presents preliminary results of HFNeRF, illustrating its potential in
generating realistic virtual avatars with biomechanic features using NeRF.";Arnab Dey<author:sep>Di Yang<author:sep>Antitza Dantcheva<author:sep>Jean Martinet;http://arxiv.org/pdf/2404.06152v1;cs.CV;;nerf
2404.06109v1;http://arxiv.org/abs/2404.06109v1;2024-04-09;Revising Densification in Gaussian Splatting;"In this paper, we address the limitations of Adaptive Density Control (ADC)
in 3D Gaussian Splatting (3DGS), a scene representation method achieving
high-quality, photorealistic results for novel view synthesis. ADC has been
introduced for automatic 3D point primitive management, controlling
densification and pruning, however, with certain limitations in the
densification logic. Our main contribution is a more principled, pixel-error
driven formulation for density control in 3DGS, leveraging an auxiliary,
per-pixel error function as the criterion for densification. We further
introduce a mechanism to control the total number of primitives generated per
scene and correct a bias in the current opacity handling strategy of ADC during
cloning operations. Our approach leads to consistent quality improvements
across a variety of benchmark scenes, without sacrificing the method's
efficiency.";Samuel Rota Bulò<author:sep>Lorenzo Porzi<author:sep>Peter Kontschieder;http://arxiv.org/pdf/2404.06109v1;cs.CV;;gaussian splatting
2404.06091v1;http://arxiv.org/abs/2404.06091v1;2024-04-09;Hash3D: Training-free Acceleration for 3D Generation;"The evolution of 3D generative modeling has been notably propelled by the
adoption of 2D diffusion models. Despite this progress, the cumbersome
optimization process per se presents a critical hurdle to efficiency. In this
paper, we introduce Hash3D, a universal acceleration for 3D generation without
model training. Central to Hash3D is the insight that feature-map redundancy is
prevalent in images rendered from camera positions and diffusion time-steps in
close proximity. By effectively hashing and reusing these feature maps across
neighboring timesteps and camera angles, Hash3D substantially prevents
redundant calculations, thus accelerating the diffusion model's inference in 3D
generation tasks. We achieve this through an adaptive grid-based hashing.
Surprisingly, this feature-sharing mechanism not only speed up the generation
but also enhances the smoothness and view consistency of the synthesized 3D
objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models,
demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency
by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian
splatting largely speeds up 3D model creation, reducing text-to-3D processing
to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The
project page is at https://adamdad.github.io/hash3D/.";Xingyi Yang<author:sep>Xinchao Wang;http://arxiv.org/pdf/2404.06091v1;cs.CV;https://adamdad.github.io/hash3D/;
2404.06128v1;http://arxiv.org/abs/2404.06128v1;2024-04-09;Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for  Realistic Endoscopic Reconstruction;"Within colorectal cancer diagnostics, conventional colonoscopy techniques
face critical limitations, including a limited field of view and a lack of
depth information, which can impede the detection of precancerous lesions.
Current methods struggle to provide comprehensive and accurate 3D
reconstructions of the colonic surface which can help minimize the missing
regions and reinspection for pre-cancerous polyps. Addressing this, we
introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting
(3D GS) combined with a Recurrent Neural Network-based Simultaneous
Localization and Mapping (RNNSLAM) system. By introducing geometric and depth
regularization into the 3D GS framework, our approach ensures more accurate
alignment of Gaussians with the colon surface, resulting in smoother 3D
reconstructions with novel viewing of detailed textures and structures.
Evaluations across three diverse datasets show that Gaussian Pancakes enhances
novel view synthesis quality, surpassing current leading methods with a 18%
boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster
rendering and more than 10X shorter training times, making it a practical tool
for real-time applications. Hence, this holds promise for achieving clinical
translation for better detection and diagnosis of colorectal cancer.";Sierra Bonilla<author:sep>Shuai Zhang<author:sep>Dimitrios Psychogyios<author:sep>Danail Stoyanov<author:sep>Francisco Vasconcelos<author:sep>Sophia Bano;http://arxiv.org/pdf/2404.06128v1;cs.CV;12 pages, 5 figures;gaussian splatting
2404.06270v2;http://arxiv.org/abs/2404.06270v2;2024-04-09;3D Geometry-aware Deformable Gaussian Splatting for Dynamic View  Synthesis;"In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting
method for dynamic view synthesis. Existing neural radiance fields (NeRF) based
solutions learn the deformation in an implicit manner, which cannot incorporate
3D scene geometry. Therefore, the learned deformation is not necessarily
geometrically coherent, which results in unsatisfactory dynamic view synthesis
and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new
representation of the 3D scene, building upon which the 3D geometry could be
exploited in learning the complex 3D deformation. Specifically, the scenes are
represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized
to move and rotate over time to model the deformation. To enforce the 3D scene
geometry constraint during deformation, we explicitly extract 3D geometry
features and integrate them in learning the 3D deformation. In this way, our
solution achieves 3D geometry-aware deformation modeling, which enables
improved dynamic view synthesis and 3D dynamic reconstruction. Extensive
experimental results on both synthetic and real datasets prove the superiority
of our solution, which achieves new state-of-the-art performance.
  The project is available at https://npucvr.github.io/GaGS/";Zhicheng Lu<author:sep>Xiang Guo<author:sep>Le Hui<author:sep>Tianrui Chen<author:sep>Min Yang<author:sep>Xiao Tang<author:sep>Feng Zhu<author:sep>Yuchao Dai;http://arxiv.org/pdf/2404.06270v2;cs.CV;Accepted by CVPR 2024. Project page: https://npucvr.github.io/GaGS/;gaussian splatting<tag:sep>nerf
2404.05220v1;http://arxiv.org/abs/2404.05220v1;2024-04-08;StylizedGS: Controllable Stylization for 3D Gaussian Splatting;"With the rapid development of XR, 3D generation and editing are becoming more
and more important, among which, stylization is an important tool of 3D
appearance editing. It can achieve consistent 3D artistic stylization given a
single reference style image and thus is a user-friendly editing way. However,
recent NeRF-based 3D stylization methods face efficiency issues that affect the
actual user experience and the implicit nature limits its ability to transfer
the geometric pattern styles. Additionally, the ability for artists to exert
flexible control over stylized scenes is considered highly desirable, fostering
an environment conducive to creative exploration. In this paper, we introduce
StylizedGS, a 3D neural style transfer framework with adaptable control over
perceptual factors based on 3D Gaussian Splatting (3DGS) representation. The
3DGS brings the benefits of high efficiency. We propose a GS filter to
eliminate floaters in the reconstruction which affects the stylization effects
before stylization. Then the nearest neighbor-based style loss is introduced to
achieve stylization by fine-tuning the geometry and color parameters of 3DGS,
while a depth preservation loss with other regularizations is proposed to
prevent the tampering of geometry content. Moreover, facilitated by specially
designed losses, StylizedGS enables users to control color, stylized scale and
regions during the stylization to possess customized capabilities. Our method
can attain high-quality stylization results characterized by faithful
brushstrokes and geometric consistency with flexible controls. Extensive
experiments across various scenes and styles demonstrate the effectiveness and
efficiency of our method concerning both stylization quality and inference FPS.";Dingxi Zhang<author:sep>Zhuoxun Chen<author:sep>Yu-Jie Yuan<author:sep>Fang-Lue Zhang<author:sep>Zhenliang He<author:sep>Shiguang Shan<author:sep>Lin Gao;http://arxiv.org/pdf/2404.05220v1;cs.CV;;gaussian splatting<tag:sep>nerf
2404.05236v1;http://arxiv.org/abs/2404.05236v1;2024-04-08;Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation;"Recently, a surge of 3D style transfer methods has been proposed that
leverage the scene reconstruction power of a pre-trained neural radiance field
(NeRF). To successfully stylize a scene this way, one must first reconstruct a
photo-realistic radiance field from collected images of the scene. However,
when only sparse input views are available, pre-trained few-shot NeRFs often
suffer from high-frequency artifacts, which are generated as a by-product of
high-frequency details for improving reconstruction quality. Is it possible to
generate more faithful stylized scenes from sparse inputs by directly
optimizing encoding-based scene representation with target style? In this
paper, we consider the stylization of sparse-view scenes in terms of
disentangling content semantics and style textures. We propose a coarse-to-fine
sparse-view scene stylization framework, where a novel hierarchical
encoding-based neural representation is designed to generate high-quality
stylized scenes directly from implicit scene representations. We also propose a
new optimization strategy with content strength annealing to achieve realistic
stylization and better content preservation. Extensive experiments demonstrate
that our method can achieve high-quality stylization of sparse-view scenes and
outperforms fine-tuning-based baselines in terms of stylization quality and
efficiency.";Y. Wang<author:sep>A. Gao<author:sep>Y. Gong<author:sep>Y. Zeng;http://arxiv.org/pdf/2404.05236v1;cs.CV;;nerf
2404.05163v1;http://arxiv.org/abs/2404.05163v1;2024-04-08;Semantic Flow: Learning Semantic Field of Dynamic Scenes from Monocular  Videos;"In this work, we pioneer Semantic Flow, a neural semantic representation of
dynamic scenes from monocular videos. In contrast to previous NeRF methods that
reconstruct dynamic scenes from the colors and volume densities of individual
points, Semantic Flow learns semantics from continuous flows that contain rich
3D motion information. As there is 2D-to-3D ambiguity problem in the viewing
direction when extracting 3D flow features from 2D video frames, we consider
the volume densities as opacity priors that describe the contributions of flow
features to the semantics on the frames. More specifically, we first learn a
flow network to predict flows in the dynamic scene, and propose a flow feature
aggregation module to extract flow features from video frames. Then, we propose
a flow attention module to extract motion information from flow features, which
is followed by a semantic network to output semantic logits of flows. We
integrate the logits with volume densities in the viewing direction to
supervise the flow features with semantic labels on video frames. Experimental
results show that our model is able to learn from multiple dynamic scenes and
supports a series of new tasks such as instance-level scene editing, semantic
completions, dynamic scene tracking and semantic adaption on novel scenes.
Codes are available at https://github.com/tianfr/Semantic-Flow/.";Fengrui Tian<author:sep>Yueqi Duan<author:sep>Angtian Wang<author:sep>Jianfei Guo<author:sep>Shaoyi Du;http://arxiv.org/pdf/2404.05163v1;cs.CV;"Accepted by ICLR 2024, Codes are available at
  https://github.com/tianfr/Semantic-Flow/";nerf
2404.04875v1;http://arxiv.org/abs/2404.04875v1;2024-04-07;NeRF2Points: Large-Scale Point Cloud Generation From Street Views'  Radiance Field Optimization;"Neural Radiance Fields (NeRF) have emerged as a paradigm-shifting methodology
for the photorealistic rendering of objects and environments, enabling the
synthesis of novel viewpoints with remarkable fidelity. This is accomplished
through the strategic utilization of object-centric camera poses characterized
by significant inter-frame overlap. This paper explores a compelling,
alternative utility of NeRF: the derivation of point clouds from aggregated
urban landscape imagery. The transmutation of street-view data into point
clouds is fraught with complexities, attributable to a nexus of interdependent
variables. First, high-quality point cloud generation hinges on precise camera
poses, yet many datasets suffer from inaccuracies in pose metadata. Also, the
standard approach of NeRF is ill-suited for the distinct characteristics of
street-view data from autonomous vehicles in vast, open settings. Autonomous
vehicle cameras often record with limited overlap, leading to blurring,
artifacts, and compromised pavement representation in NeRF-based point clouds.
In this paper, we present NeRF2Points, a tailored NeRF variant for urban point
cloud synthesis, notable for its high-quality output from RGB inputs alone. Our
paper is supported by a bespoke, high-resolution 20-kilometer urban street
dataset, designed for point cloud generation and evaluation. NeRF2Points
adeptly navigates the inherent challenges of NeRF-based point cloud synthesis
through the implementation of the following strategic innovations: (1)
Integration of Weighted Iterative Geometric Optimization (WIGO) and Structure
from Motion (SfM) for enhanced camera pose accuracy, elevating street-view data
precision. (2) Layered Perception and Integrated Modeling (LPiM) is designed
for distinct radiance field modeling in urban environments, resulting in
coherent point cloud representations.";Peng Tu<author:sep>Xun Zhou<author:sep>Mingming Wang<author:sep>Xiaojun Yang<author:sep>Bo Peng<author:sep>Ping Chen<author:sep>Xiu Su<author:sep>Yawen Huang<author:sep>Yefeng Zheng<author:sep>Chang Xu;http://arxiv.org/pdf/2404.04875v1;cs.CV;18 pages;nerf
2404.04880v1;http://arxiv.org/abs/2404.04880v1;2024-04-07;GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric  Reconstruction Using Gaussian Splatting and NeRF;"We introduce a novel large-scale scene reconstruction benchmark that utilizes
newly developed 3D representation approaches: Gaussian Splatting and Neural
Radiance Fields, on our expansive GauU-Scene V2 dataset. GauU-Scene V2
encompasses over 6.5 square kilometers and features a comprehensive RGB dataset
coupled with LiDAR ground truth. This dataset offers a unique blend of urban
and academic environments for advanced spatial analysis, covering more than 6.5
km2. We also provide detailed supplementary information on data collection
protocols. Furthermore, we present an easy-to-follow pipeline to align the
COLMAP sparse point cloud with the detailed LiDAR dataset. Our evaluation of
U-Scene, which includes a detailed analysis across various novel viewpoints
using image-based metrics such as SSIM, LPIPS, and PSNR, shows contradictory
results when applying geometric-based metrics, such as Chamfer distance. This
leads to doubts about the reliability of current image-based measurement
matrices and geometric extraction methods on Gaussian Splatting. We also make
the dataset available on the following anonymous project page";Butian Xiong<author:sep>Nanjun Zheng<author:sep>Zhen Li;http://arxiv.org/pdf/2404.04880v1;cs.CV;8 pages(No reference) 6 figures 4 tabs;gaussian splatting<tag:sep>nerf
2404.04908v1;http://arxiv.org/abs/2404.04908v1;2024-04-07;Dual-Camera Smooth Zoom on Mobile Phones;"When zooming between dual cameras on a mobile, noticeable jumps in geometric
content and image color occur in the preview, inevitably affecting the user's
zoom experience. In this work, we introduce a new task, ie, dual-camera smooth
zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI)
technique is a potential solution but struggles with ground-truth collection.
To address the issue, we suggest a data factory solution where continuous
virtual cameras are assembled to generate DCSZ data by rendering reconstructed
3D models of the scene. In particular, we propose a novel dual-camera smooth
zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is
introduced to construct a specific 3D model for each virtual camera. With the
proposed data factory, we construct a synthetic dataset for DCSZ, and we
utilize it to fine-tune FI models. In addition, we collect real-world dual-zoom
images without ground-truth for evaluation. Extensive experiments are conducted
with multiple FI methods. The results show that the fine-tuned FI models
achieve a significant performance improvement over the original ones on DCSZ
task. The datasets, codes, and pre-trained models will be publicly available.";Renlong Wu<author:sep>Zhilu Zhang<author:sep>Yu Yang<author:sep>Wangmeng Zuo;http://arxiv.org/pdf/2404.04908v1;cs.CV;24;gaussian splatting
2404.04913v1;http://arxiv.org/abs/2404.04913v1;2024-04-07;CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality  Novel-view Synthesis;"Neural Radiance Fields (NeRF) have achieved huge success in effectively
capturing and representing 3D objects and scenes. However, several factors have
impeded its further proliferation as next-generation 3D media. To establish a
ubiquitous presence in everyday media formats, such as images and videos, it is
imperative to devise a solution that effectively fulfills three key objectives:
fast encoding and decoding time, compact model sizes, and high-quality
renderings. Despite significant advancements, a comprehensive algorithm that
adequately addresses all objectives has yet to be fully realized. In this work,
we present CodecNeRF, a neural codec for NeRF representations, consisting of a
novel encoder and decoder architecture that can generate a NeRF representation
in a single forward pass. Furthermore, inspired by the recent
parameter-efficient finetuning approaches, we develop a novel finetuning method
to efficiently adapt the generated NeRF representations to a new test instance,
leading to high-quality image renderings and compact code sizes. The proposed
CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF,
achieved unprecedented compression performance of more than 150x and 20x
reduction in encoding time while maintaining (or improving) the image quality
on widely used 3D object datasets, such as ShapeNet and Objaverse.";Gyeongjin Kang<author:sep>Younggeun Lee<author:sep>Eunbyung Park;http://arxiv.org/pdf/2404.04913v1;cs.CV;"34 pages, 22 figures, Project page:
  https://gynjn.github.io/Codec-NeRF/";nerf
2404.04687v1;http://arxiv.org/abs/2404.04687v1;2024-04-06;Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion;"Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent
technique in computer vision and graphics for reconstructing 3D scenes. GS
represents a scene as a set of 3D Gaussians with varying opacities and employs
a computationally efficient splatting operation along with analytical
derivatives to compute the 3D Gaussian parameters given scene images captured
from various viewpoints. Unfortunately, capturing surround view ($360^{\circ}$
viewpoint) images is impossible or impractical in many real-world imaging
scenarios, including underwater imaging, rooms inside a building, and
autonomous navigation. In these restricted baseline imaging scenarios, the GS
algorithm suffers from a well-known 'missing cone' problem, which results in
poor reconstruction along the depth axis. In this manuscript, we demonstrate
that using transient data (from sonars) allows us to address the missing cone
problem by sampling high-frequency data along the depth axis. We extend the
Gaussian splatting algorithms for two commonly used sonars and propose fusion
algorithms that simultaneously utilize RGB camera data and sonar data. Through
simulations, emulations, and hardware experiments across various imaging
scenarios, we show that the proposed fusion algorithms lead to significantly
better novel view synthesis (5 dB improvement in PSNR) and 3D geometry
reconstruction (60% lower Chamfer distance).";Ziyuan Qu<author:sep>Omkar Vengurlekar<author:sep>Mohamad Qadri<author:sep>Kevin Zhang<author:sep>Michael Kaess<author:sep>Christopher Metzler<author:sep>Suren Jayasuriya<author:sep>Adithya Pediredla;http://arxiv.org/pdf/2404.04687v1;cs.CV;;gaussian splatting
2404.04526v1;http://arxiv.org/abs/2404.04526v1;2024-04-06;DATENeRF: Depth-Aware Text-based Editing of NeRFs;"Recent advancements in diffusion models have shown remarkable proficiency in
editing 2D images based on text prompts. However, extending these techniques to
edit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual
2D frames can result in inconsistencies across multiple views. Our crucial
insight is that a NeRF scene's geometry can serve as a bridge to integrate
these 2D edits. Utilizing this geometry, we employ a depth-conditioned
ControlNet to enhance the coherence of each 2D image modification. Moreover, we
introduce an inpainting approach that leverages the depth information of NeRF
scenes to distribute 2D edits across different images, ensuring robustness
against errors and resampling challenges. Our results reveal that this
methodology achieves more consistent, lifelike, and detailed edits than
existing leading methods for text-driven NeRF scene editing.";Sara Rojas<author:sep>Julien Philip<author:sep>Kai Zhang<author:sep>Sai Bi<author:sep>Fujun Luan<author:sep>Bernard Ghanem<author:sep>Kalyan Sunkavall;http://arxiv.org/pdf/2404.04526v1;cs.CV;"14 pages, Conference paper, 3D Scene Editing, Neural Rendering,
  Diffusion Models";nerf
2404.04211v1;http://arxiv.org/abs/2404.04211v1;2024-04-05;Robust Gaussian Splatting;"In this paper, we address common error sources for 3D Gaussian Splatting
(3DGS) including blur, imperfect camera poses, and color inconsistencies, with
the goal of improving its robustness for practical applications like
reconstructions from handheld phone captures. Our main contribution involves
modeling motion blur as a Gaussian distribution over camera poses, allowing us
to address both camera pose refinement and motion blur correction in a unified
way. Additionally, we propose mechanisms for defocus blur compensation and for
addressing color in-consistencies caused by ambient light, shadows, or due to
camera-related factors like varying white balancing settings. Our proposed
solutions integrate in a seamless way with the 3DGS formulation while
maintaining its benefits in terms of training efficiency and rendering speed.
We experimentally validate our contributions on relevant benchmark datasets
including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and
thus consistent improvements over relevant baselines.";François Darmon<author:sep>Lorenzo Porzi<author:sep>Samuel Rota-Bulò<author:sep>Peter Kontschieder;http://arxiv.org/pdf/2404.04211v1;cs.CV;;gaussian splatting<tag:sep>nerf
2404.03736v1;http://arxiv.org/abs/2404.03736v1;2024-04-04;SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer;"Recent advances in 2D/3D generative models enable the generation of dynamic
3D objects from a single-view video. Existing approaches utilize score
distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D
Gaussians. However, these methods struggle to strike a balance among reference
view alignment, spatio-temporal consistency, and motion fidelity under
single-view conditions due to the implicit nature of NeRF or the intricate
dense Gaussian motion prediction. To address these issues, this paper proposes
an efficient, sparse-controlled video-to-4D framework named SC4D, that
decouples motion and appearance to achieve superior video-to-4D generation.
Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian
Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity
of the learned motion and shape. Comprehensive experimental results demonstrate
that our method surpasses existing methods in both quality and efficiency. In
addition, facilitated by the disentangled modeling of motion and appearance of
SC4D, we devise a novel application that seamlessly transfers the learned
motion onto a diverse array of 4D entities according to textual descriptions.";Zijie Wu<author:sep>Chaohui Yu<author:sep>Yanqin Jiang<author:sep>Chenjie Cao<author:sep>Fan Wang<author:sep>Xiang Bai;http://arxiv.org/pdf/2404.03736v1;cs.CV;Project Page: https://sc4d.github.io/;nerf
2404.03126v1;http://arxiv.org/abs/2404.03126v1;2024-04-04;GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis;"We present GaSpCT, a novel view synthesis and 3D scene representation method
used to generate novel projection views for Computer Tomography (CT) scans. We
adapt the Gaussian Splatting framework to enable novel view synthesis in CT
based on limited sets of 2D image projections and without the need for
Structure from Motion (SfM) methodologies. Therefore, we reduce the total
scanning duration and the amount of radiation dose the patient receives during
the scan. We adapted the loss function to our use-case by encouraging a
stronger background and foreground distinction using two sparsity promoting
regularizers: a beta loss and a total variation (TV) loss. Finally, we
initialize the Gaussian locations across the 3D space using a uniform prior
distribution of where the brain's positioning would be expected to be within
the field of view. We evaluate the performance of our model using brain CT
scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and
demonstrate that the rendered novel views closely match the original projection
views of the simulated scan, and have better performance than other implicit 3D
scene representations methodologies. Furthermore, we empirically observe
reduced training time compared to neural network based image synthesis for
sparse-view CT image reconstruction. Finally, the memory requirements of the
Gaussian Splatting representations are reduced by 17% compared to the
equivalent voxel grid image representations.";Emmanouil Nikolakakis<author:sep>Utkarsh Gupta<author:sep>Jonathan Vengosh<author:sep>Justin Bui<author:sep>Razvan Marinescu;http://arxiv.org/pdf/2404.03126v1;eess.IV;Under Review Process for MICCAI 2024;gaussian splatting
2404.03349v1;http://arxiv.org/abs/2404.03349v1;2024-04-04;VF-NeRF: Viewshed Fields for Rigid NeRF Registration;"3D scene registration is a fundamental problem in computer vision that seeks
the best 6-DoF alignment between two scenes. This problem was extensively
investigated in the case of point clouds and meshes, but there has been
relatively limited work regarding Neural Radiance Fields (NeRF). In this paper,
we consider the problem of rigid registration between two NeRFs when the
position of the original cameras is not given. Our key novelty is the
introduction of Viewshed Fields (VF), an implicit function that determines, for
each 3D point, how likely it is to be viewed by the original cameras. We
demonstrate how VF can help in the various stages of NeRF registration, with an
extensive evaluation showing that VF-NeRF achieves SOTA results on various
datasets with different capturing approaches such as LLFF and Objaverese.";Leo Segre<author:sep>Shai Avidan;http://arxiv.org/pdf/2404.03349v1;cs.CV;;nerf
2404.03654v2;http://arxiv.org/abs/2404.03654v2;2024-04-04;RaFE: Generative Radiance Fields Restoration;"NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel
view synthesis and 3D reconstruction, but its performance is sensitive to input
image quality, which struggles to achieve high-fidelity rendering when provided
with low-quality sparse input viewpoints. Previous methods for NeRF restoration
are tailored for specific degradation type, ignoring the generality of
restoration. To overcome this limitation, we propose a generic radiance fields
restoration pipeline, named RaFE, which applies to various types of
degradations, such as low resolution, blurriness, noise, compression artifacts,
or their combinations. Our approach leverages the success of off-the-shelf 2D
restoration methods to recover the multi-view images individually. Instead of
reconstructing a blurred NeRF by averaging inconsistencies, we introduce a
novel approach using Generative Adversarial Networks (GANs) for NeRF generation
to better accommodate the geometric and appearance inconsistencies present in
the multi-view images. Specifically, we adopt a two-level tri-plane
architecture, where the coarse level remains fixed to represent the low-quality
NeRF, and a fine-level residual tri-plane to be added to the coarse level is
modeled as a distribution with GAN to capture potential variations in
restoration. We validate RaFE on both synthetic and real cases for various
restoration tasks, demonstrating superior performance in both quantitative and
qualitative evaluations, surpassing other 3D restoration methods specific to
single task. Please see our project website
https://zkaiwu.github.io/RaFE-Project/.";Zhongkai Wu<author:sep>Ziyu Wan<author:sep>Jing Zhang<author:sep>Jing Liao<author:sep>Dong Xu;http://arxiv.org/pdf/2404.03654v2;cs.CV;Project Page: https://zkaiwu.github.io/RaFE;nerf
2404.03613v1;http://arxiv.org/abs/2404.03613v1;2024-04-04;Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian  Splatting;"As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view
synthesis, it is a natural extension to deform a canonical 3DGS to multiple
frames. However, previous works fail to accurately reconstruct dynamic scenes,
especially 1) static parts moving along nearby dynamic parts, and 2) some
dynamic areas are blurry. We attribute the failure to the wrong design of the
deformation field, which is built as a coordinate-based function. This approach
is problematic because 3DGS is a mixture of multiple fields centered at the
Gaussians, not just a single coordinate-based framework. To resolve this
problem, we define the deformation as a function of per-Gaussian embeddings and
temporal embeddings. Moreover, we decompose deformations as coarse and fine
deformations to model slow and fast movements, respectively. Also, we introduce
an efficient training strategy for faster convergence and higher quality.
Project page: https://jeongminb.github.io/e-d3dgs/";Jeongmin Bae<author:sep>Seoha Kim<author:sep>Youngsik Yun<author:sep>Hahyun Lee<author:sep>Gun Bang<author:sep>Youngjung Uh;http://arxiv.org/pdf/2404.03613v1;cs.CV;Preprint;gaussian splatting
2404.03202v2;http://arxiv.org/abs/2404.03202v2;2024-04-04;OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field  Reconstruction using Omnidirectional Images;"Photorealistic reconstruction relying on 3D Gaussian Splatting has shown
promising potential in robotics. However, the current 3D Gaussian Splatting
system only supports radiance field reconstruction using undistorted
perspective images. In this paper, we present OmniGS, a novel omnidirectional
Gaussian splatting system, to take advantage of omnidirectional images for fast
radiance field reconstruction. Specifically, we conduct a theoretical analysis
of spherical camera model derivatives in 3D Gaussian Splatting. According to
the derivatives, we then implement a new GPU-accelerated omnidirectional
rasterizer that directly splats 3D Gaussians onto the equirectangular screen
space for omnidirectional image rendering. As a result, we realize
differentiable optimization of the radiance field without the requirement of
cube-map rectification or tangent-plane approximation. Extensive experiments
conducted in egocentric and roaming scenarios demonstrate that our method
achieves state-of-the-art reconstruction quality and high rendering speed using
omnidirectional images. To benefit the research community, the code will be
made publicly available once the paper is published.";Longwei Li<author:sep>Huajian Huang<author:sep>Sai-Kit Yeung<author:sep>Hui Cheng;http://arxiv.org/pdf/2404.03202v2;cs.CV;7 pages, 4 figures;gaussian splatting
2404.03650v1;http://arxiv.org/abs/2404.03650v1;2024-04-04;OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features  and Rendered Novel Views;"Large visual-language models (VLMs), like CLIP, enable open-set image
segmentation to segment arbitrary concepts from an image in a zero-shot manner.
This goes beyond the traditional closed-set assumption, i.e., where models can
only segment classes from a pre-defined training set. More recently, first
works on open-set segmentation in 3D scenes have appeared in the literature.
These methods are heavily influenced by closed-set 3D convolutional approaches
that process point clouds or polygon meshes. However, these 3D scene
representations do not align well with the image-based nature of the
visual-language models. Indeed, point cloud and 3D meshes typically have a
lower resolution than images and the reconstructed 3D scene geometry might not
project well to the underlying 2D image sequences used to compute pixel-aligned
CLIP features. To address these challenges, we propose OpenNeRF which naturally
operates on posed images and directly encodes the VLM features within the NeRF.
This is similar in spirit to LERF, however our work shows that using pixel-wise
VLM features (instead of global CLIP features) results in an overall less
complex architecture without the need for additional DINO regularization. Our
OpenNeRF further leverages NeRF's ability to render novel views and extract
open-set VLM features from areas that are not well observed in the initial
posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF
outperforms recent open-vocabulary methods such as LERF and OpenScene by at
least +4.9 mIoU.";Francis Engelmann<author:sep>Fabian Manhardt<author:sep>Michael Niemeyer<author:sep>Keisuke Tateno<author:sep>Marc Pollefeys<author:sep>Federico Tombari;http://arxiv.org/pdf/2404.03650v1;cs.CV;ICLR 2024, Project page: https://opennerf.github.io;nerf
2404.02514v1;http://arxiv.org/abs/2404.02514v1;2024-04-03;Freditor: High-Fidelity and Transferable NeRF Editing by Frequency  Decomposition;"This paper enables high-fidelity, transferable NeRF editing by frequency
decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D
scenes while suffering from blurry results, and fail to capture detailed
structures caused by the inconsistency between 2D editings. Our critical
insight is that low-frequency components of images are more
multiview-consistent after editing compared with their high-frequency parts.
Moreover, the appearance style is mainly exhibited on the low-frequency
components, and the content details especially reside in high-frequency parts.
This motivates us to perform editing on low-frequency components, which results
in high-fidelity edited scenes. In addition, the editing is performed in the
low-frequency feature space, enabling stable intensity control and novel scene
transfer. Comprehensive experiments conducted on photorealistic datasets
demonstrate the superior performance of high-fidelity and transferable NeRF
editing. The project page is at \url{https://aigc3d.github.io/freditor}.";Yisheng He<author:sep>Weihao Yuan<author:sep>Siyu Zhu<author:sep>Zilong Dong<author:sep>Liefeng Bo<author:sep>Qixing Huang;http://arxiv.org/pdf/2404.02514v1;cs.CV;;nerf
2404.02788v1;http://arxiv.org/abs/2404.02788v1;2024-04-03;GenN2N: Generative NeRF2NeRF Translation;"We present GenN2N, a unified NeRF-to-NeRF translation framework for various
NeRF translation tasks such as text-driven NeRF editing, colorization,
super-resolution, inpainting, etc. Unlike previous methods designed for
individual translation tasks with task-specific schemes, GenN2N achieves all
these NeRF editing tasks by employing a plug-and-play image-to-image translator
to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF
space. Since the 3D consistency of 2D edits may not be assured, we propose to
model the distribution of the underlying 3D edits through a generative model
that can cover all possible edited NeRFs. To model the distribution of 3D
edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes
images while decoding NeRFs. The latent space is trained to align with a
Gaussian distribution and the NeRFs are supervised through an adversarial loss
on its renderings. To ensure the latent code does not depend on 2D viewpoints
but truly reflects the 3D edits, we also regularize the latent code through a
contrastive learning scheme. Extensive experiments on various editing tasks
show GenN2N, as a universal framework, performs as well or better than
task-specific specialists while possessing flexible generative power. More
results on our project page: https://xiangyueliu.github.io/GenN2N/";Xiangyue Liu<author:sep>Han Xue<author:sep>Kunming Luo<author:sep>Ping Tan<author:sep>Li Yi;http://arxiv.org/pdf/2404.02788v1;cs.CV;"Accepted to CVPR 2024. Project page:
  https://xiangyueliu.github.io/GenN2N/";nerf
2404.02617v1;http://arxiv.org/abs/2404.02617v1;2024-04-03;Neural Radiance Fields with Torch Units;"Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction
methods widely used in industrial applications. Although prevalent methods
achieve considerable improvements in small-scale scenes, accomplishing
reconstruction in complex and large-scale scenes is still challenging. First,
the background in complex scenes shows a large variance among different views.
Second, the current inference pattern, $i.e.$, a pixel only relies on an
individual camera ray, fails to capture contextual information. To solve these
problems, we propose to enlarge the ray perception field and build up the
sample points interactions. In this paper, we design a novel inference pattern
that encourages a single camera ray possessing more contextual information, and
models the relationship among sample points on each camera ray. To hold
contextual information,a camera ray in our proposed method can render a patch
of pixels simultaneously. Moreover, we replace the MLP in neural radiance field
models with distance-aware convolutions to enhance the feature propagation
among sample points from the same camera ray. To summarize, as a torchlight, a
ray in our proposed method achieves rendering a patch of image. Thus, we call
the proposed method, Torch-NeRF. Extensive experiments on KITTI-360 and LLFF
show that the Torch-NeRF exhibits excellent performance.";Bingnan Ni<author:sep>Huanyu Wang<author:sep>Dongfeng Bai<author:sep>Minghe Weng<author:sep>Dexin Qi<author:sep>Weichao Qiu<author:sep>Bingbing Liu;http://arxiv.org/pdf/2404.02617v1;cs.CV;;nerf
2404.02410v1;http://arxiv.org/abs/2404.02410v1;2024-04-03;TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding  Autonomous Driving Scenes;"Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize
3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR
data capabilities but also overlooks the potential advantages of fusing LiDAR
with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera
Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both
LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and
novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D
mesh) and implicit (hierarchical octree feature) 3D representation derived from
LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D
Gaussian's properties are not only initialized in alignment with the 3D mesh
which provides more completed 3D shape and color information, but are also
endowed with broader contextual information through retrieved octree implicit
features. During the Gaussian Splatting optimization process, the 3D mesh
offers dense depth information as supervision, which enhances the training
process by learning of a robust geometry. Comprehensive evaluations conducted
on the Waymo Open Dataset and nuScenes Dataset validate our method's
state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our
method demonstrates fast training and achieves real-time RGB and depth
rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in
resolution of 1600x900 (nuScenes) in urban scenarios.";Cheng Zhao<author:sep>Su Sun<author:sep>Ruoyu Wang<author:sep>Yuliang Guo<author:sep>Jun-Jun Wan<author:sep>Zhou Huang<author:sep>Xinyu Huang<author:sep>Yingjie Victor Chen<author:sep>Liu Ren;http://arxiv.org/pdf/2404.02410v1;cs.CV;;gaussian splatting
2404.02742v1;http://arxiv.org/abs/2404.02742v1;2024-04-03;LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis;"Although neural radiance fields (NeRFs) have achieved triumphs in image novel
view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS
methods employ a simple shift from image NVS methods while ignoring the dynamic
nature and the large-scale reconstruction problem of LiDAR point clouds. In
light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for
novel space-time LiDAR view synthesis. In consideration of the sparsity and
large-scale characteristics, we design a 4D hybrid representation combined with
multi-planar and grid features to achieve effective reconstruction in a
coarse-to-fine manner. Furthermore, we introduce geometric constraints derived
from point clouds to improve temporal consistency. For the realistic synthesis
of LiDAR point clouds, we incorporate the global optimization of ray-drop
probability to preserve cross-region patterns. Extensive experiments on
KITTI-360 and NuScenes datasets demonstrate the superiority of our method in
accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes
are available at https://github.com/ispc-lab/LiDAR4D.";Zehan Zheng<author:sep>Fan Lu<author:sep>Weiyi Xue<author:sep>Guang Chen<author:sep>Changjun Jiang;http://arxiv.org/pdf/2404.02742v1;cs.CV;"Accepted by CVPR 2024. Project Page:
  https://dyfcalid.github.io/LiDAR4D";nerf
2404.01812v1;http://arxiv.org/abs/2404.01812v1;2024-04-02;Uncertainty-aware Active Learning of NeRF-based Object Models for Robot  Manipulators using Visual and Re-orientation Actions;"Manipulating unseen objects is challenging without a 3D representation, as
objects generally have occluded surfaces. This requires physical interaction
with objects to build their internal representations. This paper presents an
approach that enables a robot to rapidly learn the complete 3D model of a given
object for manipulation in unfamiliar orientations. We use an ensemble of
partially constructed NeRF models to quantify model uncertainty to determine
the next action (a visual or re-orientation action) by optimizing
informativeness and feasibility. Further, our approach determines when and how
to grasp and re-orient an object given its partial NeRF model and re-estimates
the object pose to rectify misalignments introduced during the interaction.
Experiments with a simulated Franka Emika Robot Manipulator operating in a
tabletop environment with benchmark objects demonstrate an improvement of (i)
14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth
reconstruction of the object surface (F-score) and (iii) 71% in the task
success rate of manipulating objects a-priori unseen orientations/stable
configurations in the scene; over current methods. The project page can be
found here: https://actnerf.github.io.";Saptarshi Dasgupta<author:sep>Akshat Gupta<author:sep>Shreshth Tuli<author:sep>Rohan Paul;http://arxiv.org/pdf/2404.01812v1;cs.RO;"This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible";nerf
2404.02155v1;http://arxiv.org/abs/2404.02155v1;2024-04-02;Alpha Invariance: On Inverse Scaling Between Distance and Volume Density  in Neural Radiance Fields;"Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of
volumetric densities in neural radiance fields, i.e., the densities double when
scene size is halved, and vice versa. We call this property alpha invariance.
For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing
both distance and volume densities in log space, and 2) a
discretization-agnostic initialization strategy to guarantee high ray
transmittance. We revisit a few popular radiance field models and find that
these systems use various heuristics to deal with issues arising from scene
scaling. We test their behaviors and show our recipe to be more robust.";Joshua Ahn<author:sep>Haochen Wang<author:sep>Raymond A. Yeh<author:sep>Greg Shakhnarovich;http://arxiv.org/pdf/2404.02155v1;cs.CV;CVPR 2024. project page https://pals.ttic.edu/p/alpha-invariance;nerf
2404.01810v1;http://arxiv.org/abs/2404.01810v1;2024-04-02;Surface Reconstruction from Gaussian Splatting via Novel Stereo Views;"The Gaussian splatting for radiance field rendering method has recently
emerged as an efficient approach for accurate scene representation. It
optimizes the location, size, color, and shape of a cloud of 3D Gaussian
elements to visually match, after projection, or splatting, a set of given
images taken from various viewing directions. And yet, despite the proximity of
Gaussian elements to the shape boundaries, direct surface reconstruction of
objects in the scene is a challenge.
  We propose a novel approach for surface reconstruction from Gaussian
splatting models. Rather than relying on the Gaussian elements' locations as a
prior for surface reconstruction, we leverage the superior novel-view synthesis
capabilities of 3DGS. To that end, we use the Gaussian splatting model to
render pairs of stereo-calibrated novel views from which we extract depth
profiles using a stereo matching method. We then combine the extracted RGB-D
images into a geometrically consistent surface. The resulting reconstruction is
more accurate and shows finer details when compared to other methods for
surface reconstruction from Gaussian splatting models, while requiring
significantly less compute time compared to other surface reconstruction
methods.
  We performed extensive testing of the proposed method on in-the-wild scenes,
taken by a smartphone, showcasing its superior reconstruction abilities.
Additionally, we tested the proposed method on the Tanks and Temples benchmark,
and it has surpassed the current leading method for surface reconstruction from
Gaussian splatting models. Project page: https://gs2mesh.github.io/.";Yaniv Wolf<author:sep>Amit Bracha<author:sep>Ron Kimmel;http://arxiv.org/pdf/2404.01810v1;cs.CV;Project Page: https://gs2mesh.github.io/;gaussian splatting
2404.02185v1;http://arxiv.org/abs/2404.02185v1;2024-04-02;NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for  Memory-Efficient Scene Representation;"The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene
modeling and novel-view synthesis. As a kind of visual media for 3D scene
representation, compression with high rate-distortion performance is an eternal
target. Motivated by advances in neural compression and neural field
representation, we propose NeRFCodec, an end-to-end NeRF compression framework
that integrates non-linear transform, quantization, and entropy coding for
memory-efficient scene representation. Since training a non-linear transform
directly on a large scale of NeRF feature planes is impractical, we discover
that pre-trained neural 2D image codec can be utilized for compressing the
features when adding content-specific parameters. Specifically, we reuse neural
2D image codec but modify its encoder and decoder heads, while keeping the
other parts of the pre-trained decoder frozen. This allows us to train the full
pipeline via supervision of rendering loss and entropy loss, yielding the
rate-distortion balance by updating the content-specific parameters. At test
time, the bitstreams containing latent code, feature decoder head, and other
side information are transmitted for communication. Experimental results
demonstrate our method outperforms existing NeRF compression methods, enabling
high-quality novel view synthesis with a memory budget of 0.5 MB.";Sicheng Li<author:sep>Hao Li<author:sep>Yiyi Liao<author:sep>Lu Yu;http://arxiv.org/pdf/2404.02185v1;cs.CV;Accepted at CVPR2024. The source code will be released;nerf
2404.01296v1;http://arxiv.org/abs/2404.01296v1;2024-04-01;MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space;"We introduce a novel framework for 3D human avatar generation and
personalization, leveraging text prompts to enhance user engagement and
customization. Central to our approach are key innovations aimed at overcoming
the challenges in photo-realistic avatar synthesis. Firstly, we utilize a
conditional Neural Radiance Fields (NeRF) model, trained on a large-scale
unannotated multi-view dataset, to create a versatile initial solution space
that accelerates and diversifies avatar generation. Secondly, we develop a
geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models,
to ensure superior view invariance and enable direct optimization of avatar
geometry. These foundational ideas are complemented by our optimization
pipeline built on Variational Score Distillation (VSD), which mitigates texture
loss and over-saturation issues. As supported by our extensive experiments,
these strategies collectively enable the creation of custom avatars with
unparalleled visual quality and better adherence to input text prompts. You can
find more results and videos in our website:
https://syntec-research.github.io/MagicMirror";Armand Comas-Massagué<author:sep>Di Qiu<author:sep>Menglei Chai<author:sep>Marcel Bühler<author:sep>Amit Raj<author:sep>Ruiqi Gao<author:sep>Qiangeng Xu<author:sep>Mark Matthews<author:sep>Paulo Gotardo<author:sep>Octavia Camps<author:sep>Sergio Orts-Escolano<author:sep>Thabo Beeler;http://arxiv.org/pdf/2404.01296v1;cs.CV;;nerf
2404.01300v1;http://arxiv.org/abs/2404.01300v1;2024-04-01;NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields;"Neural fields excel in computer vision and robotics due to their ability to
understand the 3D visual world such as inferring semantics, geometry, and
dynamics. Given the capabilities of neural fields in densely representing a 3D
scene from 2D images, we ask the question: Can we scale their self-supervised
pretraining, specifically using masked autoencoders, to generate effective 3D
representations from posed RGB images. Owing to the astounding success of
extending transformers to novel data modalities, we employ standard 3D Vision
Transformers to suit the unique formulation of NeRFs. We leverage NeRF's
volumetric grid as a dense input to the transformer, contrasting it with other
3D representations such as pointclouds where the information density can be
uneven, and the representation is irregular. Due to the difficulty of applying
masked autoencoders to an implicit representation, such as NeRF, we opt for
extracting an explicit representation that canonicalizes scenes across domains
by employing the camera trajectory for sampling. Our goal is made possible by
masking random patches from NeRF's radiance and density grid and employing a
standard 3D Swin Transformer to reconstruct the masked patches. In doing so,
the model can learn the semantic and spatial structure of complete scenes. We
pretrain this representation at scale on our proposed curated posed-RGB data,
totaling over 1.6 million images. Once pretrained, the encoder is used for
effective 3D transfer learning. Our novel self-supervised pretraining for
NeRFs, NeRF-MAE, scales remarkably well and improves performance on various
challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,
NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF
scene understanding baselines on Front3D and ScanNet datasets with an absolute
performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.";Muhammad Zubair Irshad<author:sep>Sergey Zakahrov<author:sep>Vitor Guizilini<author:sep>Adrien Gaidon<author:sep>Zsolt Kira<author:sep>Rares Ambrus;http://arxiv.org/pdf/2404.01300v1;cs.CV;29 pages, 13 figures. Project Page: https://nerf-mae.github.io/;nerf
2404.00875v2;http://arxiv.org/abs/2404.00875v2;2024-04-01;DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable  Primitive Assembly;"We present a differentiable rendering framework to learn structured 3D
abstractions in the form of primitive assemblies from sparse RGB images
capturing a 3D object. By leveraging differentiable volume rendering, our
method does not require 3D supervision. Architecturally, our network follows
the general pipeline of an image-conditioned neural radiance field (NeRF)
exemplified by pixelNeRF for color prediction. As our core contribution, we
introduce differential primitive assembly (DPA) into NeRF to output a 3D
occupancy field in place of density prediction, where the predicted occupancies
serve as opacity values for volume rendering. Our network, coined DPA-Net,
produces a union of convexes, each as an intersection of convex quadric
primitives, to approximate the target 3D object, subject to an abstraction loss
and a masking loss, both defined in the image space upon volume rendering. With
test-time adaptation and additional sampling and loss designs aimed at
improving the accuracy and compactness of the obtained assemblies, our method
demonstrates superior performance over state-of-the-art alternatives for 3D
primitive abstraction from sparse views.";Fenggen Yu<author:sep>Yiming Qian<author:sep>Xu Zhang<author:sep>Francisca Gil-Ureta<author:sep>Brian Jackson<author:sep>Eric Bennett<author:sep>Hao Zhang;http://arxiv.org/pdf/2404.00875v2;cs.CV;14 pages;nerf
2404.00992v1;http://arxiv.org/abs/2404.00992v1;2024-04-01;SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency  Guidance;"Neural Radiance Field (NeRF) technology has made significant strides in
creating novel viewpoints. However, its effectiveness is hampered when working
with sparsely available views, often leading to performance dips due to
overfitting. FreeNeRF attempts to overcome this limitation by integrating
implicit geometry regularization, which incrementally improves both geometry
and textures. Nonetheless, an initial low positional encoding bandwidth results
in the exclusion of high-frequency elements. The quest for a holistic approach
that simultaneously addresses overfitting and the preservation of
high-frequency details remains ongoing. This study introduces a novel feature
matching based sparse geometry regularization module. This module excels in
pinpointing high-frequency keypoints, thereby safeguarding the integrity of
fine details. Through progressive refinement of geometry and textures across
NeRF iterations, we unveil an effective few-shot neural rendering architecture,
designated as SGCNeRF, for enhanced novel view synthesis. Our experiments
demonstrate that SGCNeRF not only achieves superior geometry-consistent
outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in
PSNR on the LLFF and DTU datasets, respectively.";Yuru Xiao<author:sep>Xianming Liu<author:sep>Deming Zhai<author:sep>Kui Jiang<author:sep>Junjun Jiang<author:sep>Xiangyang Ji;http://arxiv.org/pdf/2404.00992v1;cs.CV;;nerf
2404.00987v1;http://arxiv.org/abs/2404.00987v1;2024-04-01;FlexiDreamer: Single Image-to-3D Generation with FlexiCubes;"3D content generation from text prompts or single images has made remarkable
progress in quality and speed recently. One of its dominant paradigms involves
generating consistent multi-view images followed by a sparse-view
reconstruction. However, due to the challenge of directly deforming the mesh
representation to approach the target topology, most methodologies learn an
implicit representation (such as NeRF) during the sparse-view reconstruction
and acquire the target mesh by a post-processing extraction. Although the
implicit representation can effectively model rich 3D information, its training
typically entails a long convergence time. In addition, the post-extraction
operation from the implicit field also leads to undesirable visual artifacts.
In this paper, we propose FlexiDreamer, a novel single image-to-3d generation
framework that reconstructs the target mesh in an end-to-end manner. By
leveraging a flexible gradient-based extraction known as FlexiCubes, our method
circumvents the defects brought by the post-processing and facilitates a direct
acquisition of the target mesh. Furthermore, we incorporate a multi-resolution
hash grid encoding scheme that progressively activates the encoding levels into
the implicit field in FlexiCubes to help capture geometric details for per-step
optimization. Notably, FlexiDreamer recovers a dense 3D structure from a
single-view image in approximately 1 minute on a single NVIDIA A100 GPU,
outperforming previous methodologies by a large margin.";Ruowen Zhao<author:sep>Zhengyi Wang<author:sep>Yikai Wang<author:sep>Zihan Zhou<author:sep>Jun Zhu;http://arxiv.org/pdf/2404.00987v1;cs.CV;project page:https://flexidreamer.github.io;nerf
2404.01400v1;http://arxiv.org/abs/2404.01400v1;2024-04-01;NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented  Camera Pose Regressor and Uncertainty Quantification;"In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful
tool for 3D reconstruction and novel view synthesis. However, the computational
cost of NeRF rendering and degradation in quality due to the presence of
artifacts pose significant challenges for its application in real-time and
robust robotic tasks, especially on embedded systems. This paper introduces a
novel framework that integrates NeRF-derived localization information with
Visual-Inertial Odometry(VIO) to provide a robust solution for robotic
navigation in a real-time. By training an absolute pose regression network with
augmented image data rendered from a NeRF and quantifying its uncertainty, our
approach effectively counters positional drift and enhances system reliability.
We also establish a mathematically sound foundation for combining visual
inertial navigation with camera localization neural networks, considering
uncertainty under a Bayesian framework. Experimental validation in the
photorealistic simulation environment demonstrates significant improvements in
accuracy compared to a conventional VIO approach.";Juyeop Han<author:sep>Lukas Lao Beyer<author:sep>Guilherme V. Cavalheiro<author:sep>Sertac Karaman;http://arxiv.org/pdf/2404.01400v1;cs.RO;8 pages, 5 figures, 2 tables;nerf
2404.01241v2;http://arxiv.org/abs/2404.01241v2;2024-04-01;StructLDM: Structured Latent Diffusion for 3D Human Generation;"Recent 3D human generative models have achieved remarkable progress by
learning 3D-aware GANs from 2D images. However, existing 3D human generative
methods model humans in a compact 1D latent space, ignoring the articulated
structure and semantics of human body topology. In this paper, we explore more
expressive and higher-dimensional latent space for 3D human modeling and
propose StructLDM, a diffusion-based unconditional 3D human generative model,
which is learned from 2D images. StructLDM solves the challenges imposed due to
the high-dimensional growth of latent space with three key designs: 1) A
semantic structured latent space defined on the dense surface manifold of a
statistical human body template. 2) A structured 3D-aware auto-decoder that
factorizes the global latent space into several semantic body parts
parameterized by a set of conditional structured local NeRFs anchored to the
body template, which embeds the properties learned from the 2D training data
and can be decoded to render view-consistent humans under different poses and
clothing styles. 3) A structured latent diffusion model for generative human
appearance sampling. Extensive experiments validate StructLDM's
state-of-the-art generation performance and illustrate the expressiveness of
the structured latent space over the well-adopted 1D latent space. Notably,
StructLDM enables different levels of controllable 3D human generation and
editing, including pose/view/shape control, and high-level tasks including
compositional generations, part-aware clothing editing, 3D virtual try-on, etc.
Our project page is at: https://taohuumd.github.io/projects/StructLDM/.";Tao Hu<author:sep>Fangzhou Hong<author:sep>Ziwei Liu;http://arxiv.org/pdf/2404.01241v2;cs.CV;Project page: https://taohuumd.github.io/projects/StructLDM/;nerf
2404.01133v1;http://arxiv.org/abs/2404.01133v1;2024-04-01;CityGaussian: Real-time High-quality Large-Scale Scene Rendering with  Gaussians;"The advancement of real-time 3D scene reconstruction and novel view synthesis
has been significantly propelled by 3D Gaussian Splatting (3DGS). However,
effectively training large-scale 3DGS and rendering it in real-time across
various scales remains challenging. This paper introduces CityGaussian
(CityGS), which employs a novel divide-and-conquer training approach and
Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and
rendering. Specifically, the global scene prior and adaptive training data
selection enables efficient training and seamless fusion. Based on fused
Gaussian primitives, we generate different detail levels through compression,
and realize fast rendering across various scales through the proposed
block-wise detail levels selection and aggregation strategy. Extensive
experimental results on large-scale scenes demonstrate that our approach
attains state-of-theart rendering quality, enabling consistent real-time
rendering of largescale scenes across vastly different scales. Our project page
is available at https://dekuliutesla.github.io/citygs/.";Yang Liu<author:sep>He Guan<author:sep>Chuanchen Luo<author:sep>Lue Fan<author:sep>Junran Peng<author:sep>Zhaoxiang Zhang;http://arxiv.org/pdf/2404.01133v1;cs.CV;Project Page: https://dekuliutesla.github.io/citygs/;gaussian splatting
2404.00874v1;http://arxiv.org/abs/2404.00874v1;2024-04-01;DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF;"We present DiSR-NeRF, a diffusion-guided framework for view-consistent
super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement
for high-resolution (HR) reference images by leveraging existing powerful 2D
super-resolution models. Nonetheless, independent SR 2D images are often
inconsistent across different views. We thus propose Iterative 3D
Synchronization (I3DS) to mitigate the inconsistency problem via the inherent
multi-view consistency property of NeRF. Specifically, our I3DS alternates
between upscaling low-resolution (LR) rendered images with diffusion models,
and updating the underlying 3D representation with standard NeRF training. We
further introduce Renoised Score Distillation (RSD), a novel score-distillation
objective for 2D image resolution. Our RSD combines features from ancestral
sampling and Score Distillation Sampling (SDS) to generate sharp images that
are also LR-consistent. Qualitative and quantitative results on both synthetic
and real-world datasets demonstrate that our DiSR-NeRF can achieve better
results on NeRF super-resolution compared with existing works. Code and video
results available at the project website.";Jie Long Lee<author:sep>Chen Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2404.00874v1;cs.CV;;nerf
2404.00891v1;http://arxiv.org/abs/2404.00891v1;2024-04-01;Marrying NeRF with Feature Matching for One-step Pose Estimation;"Given the image collection of an object, we aim at building a real-time
image-based pose estimation method, which requires neither its CAD model nor
hours of object-specific training. Recent NeRF-based methods provide a
promising solution by directly optimizing the pose from pixel loss between
rendered and target images. However, during inference, they require long
converging time, and suffer from local minima, making them impractical for
real-time robot applications. We aim at solving this problem by marrying image
matching with NeRF. With 2D matches and depth rendered by NeRF, we directly
solve the pose in one step by building 2D-3D correspondences between target and
initial view, thus allowing for real-time prediction. Moreover, to improve the
accuracy of 2D-3D correspondences, we propose a 3D consistent point mining
strategy, which effectively discards unfaithful points reconstruted by NeRF.
Moreover, current NeRF-based methods naively optimizing pixel loss fail at
occluded images. Thus, we further propose a 2D matches based sampling strategy
to preclude the occluded area. Experimental results on representative datasets
prove that our method outperforms state-of-the-art methods, and improves
inference efficiency by 90x, achieving real-time prediction at 6 FPS.";Ronghan Chen<author:sep>Yang Cong<author:sep>Yu Ren;http://arxiv.org/pdf/2404.00891v1;cs.CV;ICRA, 2024. Video https://www.youtube.com/watch?v=70fgUobOFWo;nerf
2404.01053v1;http://arxiv.org/abs/2404.01053v1;2024-04-01;HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior;"We present HAHA - a novel approach for animatable human avatar generation
from monocular input videos. The proposed method relies on learning the
trade-off between the use of Gaussian splatting and a textured mesh for
efficient and high fidelity rendering. We demonstrate its efficiency to animate
and render full-body human avatars controlled via the SMPL-X parametric model.
Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh
where it is necessary, like hair and out-of-mesh clothing. This results in a
minimal number of Gaussians being used to represent the full avatar, and
reduced rendering artifacts. This allows us to handle the animation of small
body parts such as fingers that are traditionally disregarded. We demonstrate
the effectiveness of our approach on two open datasets: SnapshotPeople and
X-Humans. Our method demonstrates on par reconstruction quality to the
state-of-the-art on SnapshotPeople, while using less than a third of Gaussians.
HAHA outperforms previous state-of-the-art on novel poses from X-Humans both
quantitatively and qualitatively.";David Svitov<author:sep>Pietro Morerio<author:sep>Lourdes Agapito<author:sep>Alessio Del Bue;http://arxiv.org/pdf/2404.01053v1;cs.CV;;gaussian splatting
2404.00923v1;http://arxiv.org/abs/2404.00923v1;2024-04-01;MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements;"Simultaneous localization and mapping is essential for position tracking and
scene understanding. 3D Gaussian-based map representations enable
photorealistic reconstruction and real-time rendering of scenes using multiple
posed cameras. We show for the first time that using 3D Gaussians for map
representation with unposed camera images and inertial measurements can enable
accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural
radiance field-based representations by enabling faster rendering, scale
awareness, and improved trajectory tracking. Our framework enables
keyframe-based mapping and tracking utilizing loss functions that incorporate
relative pose transformations from pre-integrated inertial measurements, depth
estimates, and measures of photometric rendering quality. We also release a
multi-modal dataset, UT-MM, collected from a mobile robot equipped with a
camera and an inertial measurement unit. Experimental evaluation on several
scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking
and 5% improvement in photometric rendering quality compared to the current
3DGS SLAM state-of-the-art, while allowing real-time rendering of a
high-resolution dense 3D map. Project Webpage:
https://vita-group.github.io/MM3DGS-SLAM";Lisong C. Sun<author:sep>Neel P. Bhatt<author:sep>Jonathan C. Liu<author:sep>Zhiwen Fan<author:sep>Zhangyang Wang<author:sep>Todd E. Humphreys<author:sep>Ufuk Topcu;http://arxiv.org/pdf/2404.00923v1;cs.CV;Project Webpage: https://vita-group.github.io/MM3DGS-SLAM;gaussian splatting
2404.01168v1;http://arxiv.org/abs/2404.01168v1;2024-04-01;Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting;"3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the
realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much
like its predecessor Neural Radiance Fields (NeRF), struggles to accurately
model physical reflections, particularly in mirrors that are ubiquitous in
real-world scenes. This oversight mistakenly perceives reflections as separate
entities that physically exist, resulting in inaccurate reconstructions and
inconsistent reflective properties across varied viewpoints. To address this
pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework
devised to master the intricacies of mirror geometries and reflections, paving
the way for the generation of realistically depicted mirror reflections. By
ingeniously incorporating mirror attributes into the 3DGS and leveraging the
principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to
observe from behind the mirror, enriching the realism of scene renderings.
Extensive assessments, spanning both synthetic and real-world scenes, showcase
our method's ability to render novel views with enhanced fidelity in real-time,
surpassing the state-of-the-art Mirror-NeRF specifically within the challenging
mirror regions. Our code will be made publicly available for reproducible
research.";Jiarui Meng<author:sep>Haijie Li<author:sep>Yanmin Wu<author:sep>Qiankun Gao<author:sep>Shuzhou Yang<author:sep>Jian Zhang<author:sep>Siwei Ma;http://arxiv.org/pdf/2404.01168v1;cs.CV;22 pages, 7 figures;gaussian splatting<tag:sep>nerf
2404.00674v1;http://arxiv.org/abs/2404.00674v1;2024-03-31;Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated  Objects;"We present Knowledge NeRF to synthesize novel views for dynamic
scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering
them from arbitrary perspectives is a challenging problem with applications in
various domains. Previous dynamic NeRF methods learn the deformation of
articulated objects from monocular videos. However, qualities of their
reconstructed scenes are limited.To clearly reconstruct dynamic scenes, we
propose a new framework by considering two frames at a time.We pretrain a NeRF
model for an articulated object.When articulated objects moves, Knowledge NeRF
learns to generate novel views at the new state by incorporating past knowledge
in the pretrained NeRF model with minimal observations in the present state. We
propose a projection module to adapt NeRF for dynamic scenes, learning the
correspondence between pretrained knowledge base and current states.
Experimental results demonstrate the effectiveness of our method in
reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge
NeRF is a new pipeline and promising solution for novel view synthesis in
dynamic articulated objects. The data and implementation are publicly available
at https://github.com/RussRobin/Knowledge_NeRF.";Wenxiao Cai<author:sep>Xinyue Leiınst<author:sep>Xinyu He<author:sep>Junming Leo Chen<author:sep>Yangang Wang;http://arxiv.org/pdf/2404.00674v1;cs.CV;;nerf
2404.00769v1;http://arxiv.org/abs/2404.00769v1;2024-03-31;An Active Perception Game for Robust Autonomous Exploration;"We formulate active perception for an autonomous agent that explores an
unknown environment as a two-player zero-sum game: the agent aims to maximize
information gained from the environment while the environment aims to minimize
the information gained by the agent. In each episode, the environment reveals a
set of actions with their potentially erroneous information gain. In order to
select the best action, the robot needs to recover the true information gain
from the erroneous one. The robot does so by minimizing the discrepancy between
its estimate of information gain and the true information gain it observes
after taking the action. We propose an online convex optimization algorithm
that achieves sub-linear expected regret $O(T^{3/4})$ for estimating the
information gain. We also provide a bound on the regret of active perception
performed by any (near-)optimal prediction and trajectory selection algorithms.
We evaluate this approach using semantic neural radiance fields (NeRFs) in
simulated realistic 3D environments to show that the robot can discover up to
12% more objects using the improved estimate of the information gain. On the
M3ED dataset, the proposed algorithm reduced the error of information gain
prediction in occupancy map by over 67%. In real-world experiments using
occupancy maps on a Jackal ground robot, we show that this approach can
calculate complicated trajectories that efficiently explore all occluded
regions.";Siming He<author:sep>Yuezhan Tao<author:sep>Igor Spasojevic<author:sep>Vijay Kumar<author:sep>Pratik Chaudhari;http://arxiv.org/pdf/2404.00769v1;cs.RO;;nerf
2404.00714v1;http://arxiv.org/abs/2404.00714v1;2024-03-31;Neural Radiance Field-based Visual Rendering: A Comprehensive Review;"In recent years, Neural Radiance Fields (NeRF) has made remarkable progress
in the field of computer vision and graphics, providing strong technical
support for solving key tasks including 3D scene understanding, new perspective
synthesis, human body reconstruction, robotics, and so on, the attention of
academics to this research result is growing. As a revolutionary neural
implicit field representation, NeRF has caused a continuous research boom in
the academic community. Therefore, the purpose of this review is to provide an
in-depth analysis of the research literature on NeRF within the past two years,
to provide a comprehensive academic perspective for budding researchers. In
this paper, the core architecture of NeRF is first elaborated in detail,
followed by a discussion of various improvement strategies for NeRF, and case
studies of NeRF in diverse application scenarios, demonstrating its practical
utility in different domains. In terms of datasets and evaluation metrics, This
paper details the key resources needed for NeRF model training. Finally, this
paper provides a prospective discussion on the future development trends and
potential challenges of NeRF, aiming to provide research inspiration for
researchers in the field and to promote the further development of related
technologies.";Mingyuan Yao<author:sep>Yukang Huo<author:sep>Yang Ran<author:sep>Qingbin Tian<author:sep>Ruifeng Wang<author:sep>Haihua Wang;http://arxiv.org/pdf/2404.00714v1;cs.CV;35 pages, 22 figures, 14 tables, 18 formulas;nerf
2404.00345v1;http://arxiv.org/abs/2404.00345v1;2024-03-30;MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview  and Text;"The generation of 3D scenes from user-specified conditions offers a promising
avenue for alleviating the production burden in 3D applications. Previous
studies required significant effort to realize the desired scene, owing to
limited control conditions. We propose a method for controlling and generating
3D scenes under multimodal conditions using partial images, layout information
represented in the top view, and text prompts. Combining these conditions to
generate a 3D scene involves the following significant difficulties: (1) the
creation of large datasets, (2) reflection on the interaction of multimodal
conditions, and (3) domain dependence of the layout conditions. We decompose
the process of 3D scene generation into 2D image generation from the given
conditions and 3D scene generation from 2D images. 2D image generation is
achieved by fine-tuning a pretrained text-to-image model with a small
artificial dataset of partial images and layouts, and 3D scene generation is
achieved by layout-conditioned depth estimation and neural radiance fields
(NeRF), thereby avoiding the creation of large datasets. The use of a common
representation of spatial information using 360-degree images allows for the
consideration of multimodal condition interactions and reduces the domain
dependence of the layout control. The experimental results qualitatively and
quantitatively demonstrated that the proposed method can generate 3D scenes in
diverse domains, from indoor to outdoor, according to multimodal conditions.";Takayuki Hara<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2404.00345v1;cs.CV;Project Page: https://hara012.github.io/MaGRITTe-project;nerf
2404.00409v1;http://arxiv.org/abs/2404.00409v1;2024-03-30;3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting;"In this paper, we present an implicit surface reconstruction method with 3D
Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D
reconstruction with intricate details while inheriting the high efficiency and
rendering quality of 3DGS. The key insight is incorporating an implicit signed
distance field (SDF) within 3D Gaussians to enable them to be aligned and
jointly optimized. First, we introduce a differentiable SDF-to-opacity
transformation function that converts SDF values into corresponding Gaussians'
opacities. This function connects the SDF and 3D Gaussians, allowing for
unified optimization and enforcing surface constraints on the 3D Gaussians.
During learning, optimizing the 3D Gaussians provides supervisory signals for
SDF learning, enabling the reconstruction of intricate details. However, this
only provides sparse supervisory signals to the SDF at locations occupied by
Gaussians, which is insufficient for learning a continuous SDF. Then, to
address this limitation, we incorporate volumetric rendering and align the
rendered geometric attributes (depth, normal) with those derived from 3D
Gaussians. This consistency regularization introduces supervisory signals to
locations not covered by discrete 3D Gaussians, effectively eliminating
redundant surfaces outside the Gaussian sampling range. Our extensive
experimental results demonstrate that our 3DGSR method enables high-quality 3D
surface reconstruction while preserving the efficiency and rendering quality of
3DGS. Besides, our method competes favorably with leading surface
reconstruction techniques while offering a more efficient learning process and
much better rendering qualities. The code will be available at
https://github.com/CVMI-Lab/3DGSR.";Xiaoyang Lyu<author:sep>Yang-Tian Sun<author:sep>Yi-Hua Huang<author:sep>Xiuzhe Wu<author:sep>Ziyi Yang<author:sep>Yilun Chen<author:sep>Jiangmiao Pang<author:sep>Xiaojuan Qi;http://arxiv.org/pdf/2404.00409v1;cs.CV;;gaussian splatting
2403.20032v1;http://arxiv.org/abs/2403.20032v1;2024-03-29;HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban  Scenes;"The rapid growth of 3D Gaussian Splatting (3DGS) has revolutionized neural
rendering, enabling real-time production of high-quality renderings. However,
the previous 3DGS-based methods have limitations in urban scenes due to
reliance on initial Structure-from-Motion(SfM) points and difficulties in
rendering distant, sky and low-texture areas. To overcome these challenges, we
propose a hybrid optimization method named HO-Gaussian, which combines a
grid-based volume with the 3DGS pipeline. HO-Gaussian eliminates the dependency
on SfM point initialization, allowing for rendering of urban scenes, and
incorporates the Point Densitification to enhance rendering quality in
problematic regions during training. Furthermore, we introduce Gaussian
Direction Encoding as an alternative for spherical harmonics in the rendering
pipeline, which enables view-dependent color representation. To account for
multi-camera systems, we introduce neural warping to enhance object consistency
across different cameras. Experimental results on widely used autonomous
driving datasets demonstrate that HO-Gaussian achieves photo-realistic
rendering in real-time on multi-camera urban datasets.";Zhuopeng Li<author:sep>Yilin Zhang<author:sep>Chenming Wu<author:sep>Jianke Zhu<author:sep>Liangjun Zhang;http://arxiv.org/pdf/2403.20032v1;cs.CV;;gaussian splatting
2403.20079v1;http://arxiv.org/abs/2403.20079v1;2024-03-29;SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior;"Novel View Synthesis (NVS) for street scenes play a critical role in the
autonomous driving simulation. The current mainstream technique to achieve it
is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS). Although thrilling progress has been made, when handling
street scenes, current methods struggle to maintain rendering quality at the
viewpoint that deviates significantly from the training viewpoints. This issue
stems from the sparse training views captured by a fixed camera on a moving
vehicle. To tackle this problem, we propose a novel approach that enhances the
capacity of 3DGS by leveraging prior from a Diffusion Model along with
complementary multi-modal data. Specifically, we first fine-tune a Diffusion
Model by adding images from adjacent frames as condition, meanwhile exploiting
depth data from LiDAR point clouds to supply additional spatial information.
Then we apply the Diffusion Model to regularize the 3DGS at unseen views during
training. Experimental results validate the effectiveness of our method
compared with current state-of-the-art models, and demonstrate its advance in
rendering images from broader views.";Zhongrui Yu<author:sep>Haoran Wang<author:sep>Jinze Yang<author:sep>Hanzhang Wang<author:sep>Zeke Xie<author:sep>Yunfeng Cai<author:sep>Jiale Cao<author:sep>Zhong Ji<author:sep>Mingming Sun;http://arxiv.org/pdf/2403.20079v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.20034v1;http://arxiv.org/abs/2403.20034v1;2024-03-29;NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking  With Depth Completion and Denoising;"In recent years, there have been significant advancements in 3D
reconstruction and dense RGB-D SLAM systems. One notable development is the
application of Neural Radiance Fields (NeRF) in these systems, which utilizes
implicit neural representation to encode 3D scenes. This extension of NeRF to
SLAM has shown promising results. However, the depth images obtained from
consumer-grade RGB-D sensors are often sparse and noisy, which poses
significant challenges for 3D reconstruction and affects the accuracy of the
representation of the scene geometry. Moreover, the original hierarchical
feature grid with occupancy value is inaccurate for scene geometry
representation. Furthermore, the existing methods select random pixels for
camera tracking, which leads to inaccurate localization and is not robust in
real-world indoor environments. To this end, we present NeSLAM, an advanced
framework that achieves accurate and dense depth estimation, robust camera
tracking, and realistic synthesis of novel views. First, a depth completion and
denoising network is designed to provide dense geometry prior and guide the
neural implicit representation optimization. Second, the occupancy scene
representation is replaced with Signed Distance Field (SDF) hierarchical scene
representation for high-quality reconstruction and view synthesis. Furthermore,
we also propose a NeRF-based self-supervised feature tracking algorithm for
robust real-time tracking. Experiments on various indoor datasets demonstrate
the effectiveness and accuracy of the system in reconstruction, tracking
quality, and novel view synthesis.";Tianchen Deng<author:sep>Yanbo Wang<author:sep>Hongle Xie<author:sep>Hesheng Wang<author:sep>Jingchuan Wang<author:sep>Danwei Wang<author:sep>Weidong Chen;http://arxiv.org/pdf/2403.20034v1;cs.CV;;nerf
2403.20153v1;http://arxiv.org/abs/2403.20153v1;2024-03-29;Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D  Generative Prior;"Recent methods for audio-driven talking head synthesis often optimize neural
radiance fields (NeRF) on a monocular talking portrait video, leveraging its
capability to render high-fidelity and 3D-consistent novel-view frames.
However, they often struggle to reconstruct complete face geometry due to the
absence of comprehensive 3D information in the input monocular videos. In this
paper, we introduce a novel audio-driven talking head synthesis framework,
called Talk3D, that can faithfully reconstruct its plausible facial geometries
by effectively adopting the pre-trained 3D-aware generative prior. Given the
personalized 3D generative model, we present a novel audio-guided attention
U-Net architecture that predicts the dynamic face variations in the NeRF space
driven by audio. Furthermore, our model is further modulated by audio-unrelated
conditioning tokens which effectively disentangle variations unrelated to audio
features. Compared to existing methods, our method excels in generating
realistic facial geometries even under extreme head poses. We also conduct
extensive experiments showing our approach surpasses state-of-the-art
benchmarks in terms of both quantitative and qualitative evaluations.";Jaehoon Ko<author:sep>Kyusun Cho<author:sep>Joungbin Lee<author:sep>Heeji Yoon<author:sep>Sangmin Lee<author:sep>Sangjun Ahn<author:sep>Seungryong Kim;http://arxiv.org/pdf/2403.20153v1;cs.CV;Project page: https://ku-cvlab.github.io/Talk3D/;nerf
2403.20159v1;http://arxiv.org/abs/2403.20159v1;2024-03-29;HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation  in Urban Scenes;"Online dense mapping of urban scenes forms a fundamental cornerstone for
scene understanding and navigation of autonomous vehicles. Recent advancements
in mapping methods are mainly based on NeRF, whose rendering speed is too slow
to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering
speed hundreds of times faster than NeRF, holds greater potential in online
dense mapping. However, integrating 3DGS into a street-view dense mapping
framework still faces two challenges, including incomplete reconstruction due
to the absence of geometric information beyond the LiDAR coverage area and
extensive computation for reconstruction in large urban scenes. To this end, we
propose HGS-Mapping, an online dense mapping framework in unbounded large-scale
scenes. To attain complete construction, our framework introduces Hybrid
Gaussian Representation, which models different parts of the entire scene using
Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian
initialization mechanism and an adaptive update method to achieve high-fidelity
and rapid reconstruction. To the best of our knowledge, we are the first to
integrate Gaussian representation into online dense mapping of urban scenes.
Our approach achieves SOTA reconstruction accuracy while only employing 66%
number of Gaussians, leading to 20% faster reconstruction speed.";Ke Wu<author:sep>Kaizhao Zhang<author:sep>Zhiwei Zhang<author:sep>Shanshuai Yuan<author:sep>Muer Tie<author:sep>Julong Wei<author:sep>Zijun Xu<author:sep>Jieru Zhao<author:sep>Zhongxue Gan<author:sep>Wenchao Ding;http://arxiv.org/pdf/2403.20159v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.19920v1;http://arxiv.org/abs/2403.19920v1;2024-03-29;MI-NeRF: Learning a Single Face NeRF from Multiple Identities;"In this work, we introduce a method that learns a single dynamic neural
radiance field (NeRF) from monocular talking face videos of multiple
identities. NeRFs have shown remarkable results in modeling the 4D dynamics and
appearance of human faces. However, they require per-identity optimization.
Although recent approaches have proposed techniques to reduce the training and
rendering time, increasing the number of identities can be expensive. We
introduce MI-NeRF (multi-identity NeRF), a single unified network that models
complex non-rigid facial motion for multiple identities, using only monocular
videos of arbitrary length. The core premise in our method is to learn the
non-linear interactions between identity and non-identity specific information
with a multiplicative module. By training on multiple videos simultaneously,
MI-NeRF not only reduces the total training time compared to standard
single-identity NeRFs, but also demonstrates robustness in synthesizing novel
expressions for any input identity. We present results for both facial
expression transfer and talking face video synthesis. Our method can be further
personalized for a target identity given only a short video.";Aggelina Chatziagapi<author:sep>Grigorios G. Chrysos<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2403.19920v1;cs.CV;Project page: https://aggelinacha.github.io/MI-NeRF/;nerf
2403.19985v1;http://arxiv.org/abs/2403.19985v1;2024-03-29;Stable Surface Regularization for Fast Few-Shot NeRF;"This paper proposes an algorithm for synthesizing novel views under few-shot
setup. The main concept is to develop a stable surface regularization technique
called Annealing Signed Distance Function (ASDF), which anneals the surface in
a coarse-to-fine manner to accelerate convergence speed. We observe that the
Eikonal loss - which is a widely known geometric regularization - requires
dense training signal to shape different level-sets of SDF, leading to
low-fidelity results under few-shot training. In contrast, the proposed surface
regularization successfully reconstructs scenes and produce high-fidelity
geometry with stable training. Our method is further accelerated by utilizing
grid representation and monocular geometric priors. Finally, the proposed
approach is up to 45 times faster than existing few-shot novel view synthesis
methods, and it produces comparable results in the ScanNet dataset and
NeRF-Real dataset.";Byeongin Joung<author:sep>Byeong-Uk Lee<author:sep>Jaesung Choe<author:sep>Ukcheol Shin<author:sep>Minjun Kang<author:sep>Taeyeop Lee<author:sep>In So Kweon<author:sep>Kuk-Jin Yoon;http://arxiv.org/pdf/2403.19985v1;cs.CV;3DV 2024;nerf
2403.20309v1;http://arxiv.org/abs/2403.20309v1;2024-03-29;InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40  Seconds;"While novel view synthesis (NVS) has made substantial progress in 3D computer
vision, it typically requires an initial estimation of camera intrinsics and
extrinsics from dense viewpoints. This pre-processing is usually conducted via
a Structure-from-Motion (SfM) pipeline, a procedure that can be slow and
unreliable, particularly in sparse-view scenarios with insufficient matched
features for accurate reconstruction. In this work, we integrate the strengths
of point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with
end-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved
issues in NVS under unconstrained settings, which encompasses pose-free and
sparse view challenges. Our framework, InstantSplat, unifies dense stereo
priors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview &
pose-free images in less than 1 minute. Specifically, InstantSplat comprises a
Coarse Geometric Initialization (CGI) module that swiftly establishes a
preliminary scene structure and camera parameters across all training views,
utilizing globally-aligned 3D point maps derived from a pre-trained dense
stereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO)
module, which jointly optimizes the 3D Gaussian attributes and the initialized
poses with pose regularization. Experiments conducted on the large-scale
outdoor Tanks & Temples datasets demonstrate that InstantSplat significantly
improves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error
(ATE) by 80%. These establish InstantSplat as a viable solution for scenarios
involving posefree and sparse-view conditions. Project page:
instantsplat.github.io.";Zhiwen Fan<author:sep>Wenyan Cong<author:sep>Kairun Wen<author:sep>Kevin Wang<author:sep>Jian Zhang<author:sep>Xinghao Ding<author:sep>Danfei Xu<author:sep>Boris Ivanovic<author:sep>Marco Pavone<author:sep>Georgios Pavlakos<author:sep>Zhangyang Wang<author:sep>Yue Wang;http://arxiv.org/pdf/2403.20309v1;cs.CV;;gaussian splatting
2403.20275v1;http://arxiv.org/abs/2403.20275v1;2024-03-29;Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for  Reconstructing Challenging Surfaces;"Touch and vision go hand in hand, mutually enhancing our ability to
understand the world. From a research perspective, the problem of mixing touch
and vision is underexplored and presents interesting challenges. To this end,
we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data
(local depth maps) with multi-view vision data to achieve surface
reconstruction and novel view synthesis. Our method optimises 3D Gaussian
primitives to accurately model the object's geometry at points of contact. By
creating a framework that decreases the transmittance at touch locations, we
achieve a refined surface reconstruction, ensuring a uniformly smooth depth
map. Touch is particularly useful when considering non-Lambertian objects (e.g.
shiny or reflective surfaces) since contemporary methods tend to fail to
reconstruct with fidelity specular highlights. By combining vision and tactile
sensing, we achieve more accurate geometry reconstructions with fewer images
than prior methods. We conduct evaluation on objects with glossy and reflective
surfaces and demonstrate the effectiveness of our approach, offering
significant improvements in reconstruction quality.";Mauro Comi<author:sep>Alessio Tonioni<author:sep>Max Yang<author:sep>Jonathan Tremblay<author:sep>Valts Blukis<author:sep>Yijiong Lin<author:sep>Nathan F. Lepora<author:sep>Laurence Aitchison;http://arxiv.org/pdf/2403.20275v1;cs.CV;17 pages;gaussian splatting
2403.20013v1;http://arxiv.org/abs/2403.20013v1;2024-03-29;DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal;"When capturing images through the glass during rainy or snowy weather
conditions, the resulting images often contain waterdrops adhered on the glass
surface, and these waterdrops significantly degrade the image quality and
performance of many computer vision algorithms. To tackle these limitations, we
propose a method to reconstruct the clear 3D scene implicitly from multi-view
images degraded by waterdrops. Our method exploits an attention network to
predict the location of waterdrops and then train a Neural Radiance Fields to
recover the 3D scene implicitly. By leveraging the strong scene representation
capabilities of NeRF, our method can render high-quality novel-view images with
waterdrops removed. Extensive experimental results on both synthetic and real
datasets show that our method is able to generate clear 3D scenes and
outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal
methods.";Yunhao Li<author:sep>Jing Wu<author:sep>Lingzhe Zhao<author:sep>Peidong Liu;http://arxiv.org/pdf/2403.20013v1;cs.CV;;nerf
2403.20018v1;http://arxiv.org/abs/2403.20018v1;2024-03-29;SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image;"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene representation from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, which not
only reduces storage requirements but also offers potential privacy protection.
Inspired by this, to take one step further, our approach builds upon the
powerful 3D scene representation capabilities of neural radiance fields (NeRF).
Specifically, we formulate the physical imaging process of SCI as part of the
training of NeRF, allowing us to exploit its impressive performance in
capturing complex scene structures. To assess the effectiveness of our method,
we conduct extensive evaluations using both synthetic data and real data
captured by our SCI system. Extensive experimental results demonstrate that our
proposed approach surpasses the state-of-the-art methods in terms of image
reconstruction and novel view image synthesis. Moreover, our method also
exhibits the ability to restore high frame-rate multi-view consistent images by
leveraging SCI and the rendering capabilities of NeRF. The code is available at
https://github.com/WU-CVGL/SCINeRF.";Yunhao Li<author:sep>Xiaodong Wang<author:sep>Ping Wang<author:sep>Xin Yuan<author:sep>Peidong Liu;http://arxiv.org/pdf/2403.20018v1;eess.IV;;nerf
2403.19586v1;http://arxiv.org/abs/2403.19586v1;2024-03-28;TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D  DSA Rendering;"Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical
imaging technique that provides a series of 2D images captured at different
stages and angles during the process of contrast agent filling blood vessels.
It plays a significant role in the diagnosis of cerebrovascular diseases.
Improving the rendering quality and speed under sparse sampling is important
for observing the status and location of lesions. The current methods exhibit
inadequate rendering quality in sparse views and suffer from slow rendering
speed. To overcome these limitations, we propose TOGS, a Gaussian splatting
method with opacity offset over time, which can effectively improve the
rendering quality and speed of 4D DSA. We introduce an opacity offset table for
each Gaussian to model the temporal variations in the radiance of the contrast
agent. By interpolating the opacity offset table, the opacity variation of the
Gaussian at different time points can be determined. This enables us to render
the 2D DSA image at that specific moment. Additionally, we introduced a Smooth
loss term in the loss function to mitigate overfitting issues that may arise in
the model when dealing with sparse view scenarios. During the training phase,
we randomly prune Gaussians, thereby reducing the storage overhead of the
model. The experimental results demonstrate that compared to previous methods,
this model achieves state-of-the-art reconstruction quality under the same
number of training views. Additionally, it enables real-time rendering while
maintaining low storage overhead. The code will be publicly available.";Shuai Zhang<author:sep>Huangxuan Zhao<author:sep>Zhenghong Zhou<author:sep>Guanjun Wu<author:sep>Chuansheng Zheng<author:sep>Xinggang Wang<author:sep>Wenyu Liu;http://arxiv.org/pdf/2403.19586v1;cs.CV;;gaussian splatting
2403.19495v1;http://arxiv.org/abs/2403.19495v1;2024-03-28;CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians;"The field of 3D reconstruction from images has rapidly evolved in the past
few years, first with the introduction of Neural Radiance Field (NeRF) and more
recently with 3D Gaussian Splatting (3DGS). The latter provides a significant
edge over NeRF in terms of the training and inference speed, as well as the
reconstruction quality. Although 3DGS works well for dense input images, the
unstructured point-cloud like representation quickly overfits to the more
challenging setup of extremely sparse input images (e.g., 3 images), creating a
representation that appears as a jumble of needles from novel views. To address
this issue, we propose regularized optimization and depth-based initialization.
Our key idea is to introduce a structured Gaussian representation that can be
controlled in 2D image space. We then constraint the Gaussians, in particular
their position, and prevent them from moving independently during optimization.
Specifically, we introduce single and multiview constraints through an implicit
convolutional decoder and a total variation loss, respectively. With the
coherency introduced to the Gaussians, we further constrain the optimization
through a flow-based loss function. To support our regularized optimization, we
propose an approach to initialize the Gaussians using monocular depth estimates
at each input view. We demonstrate significant improvements compared to the
state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.";Avinash Paliwal<author:sep>Wei Ye<author:sep>Jinhui Xiong<author:sep>Dmytro Kotovenko<author:sep>Rakesh Ranjan<author:sep>Vikas Chandra<author:sep>Nima Khademi Kalantari;http://arxiv.org/pdf/2403.19495v1;cs.CV;Project page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS;gaussian splatting<tag:sep>nerf
2403.19632v1;http://arxiv.org/abs/2403.19632v1;2024-03-28;GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond;"We present GauStudio, a novel modular framework for modeling 3D Gaussian
Splatting (3DGS) to provide standardized, plug-and-play components for users to
easily customize and implement a 3DGS pipeline. Supported by our framework, we
propose a hybrid Gaussian representation with foreground and skyball background
models. Experiments demonstrate this representation reduces artifacts in
unbounded outdoor scenes and improves novel view synthesis. Finally, we propose
Gaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuse
approach for high-fidelity mesh reconstruction from 3DGS inputs without
fine-tuning. Overall, our GauStudio framework, hybrid representation, and GauS
approach enhance 3DGS modeling and rendering capabilities, enabling
higher-quality novel view synthesis and surface reconstruction.";Chongjie Ye<author:sep>Yinyu Nie<author:sep>Jiahao Chang<author:sep>Yuantao Chen<author:sep>Yihao Zhi<author:sep>Xiaoguang Han;http://arxiv.org/pdf/2403.19632v1;cs.CV;Code: https://github.com/GAP-LAB-CUHK-SZ/gaustudio;gaussian splatting
2403.19615v1;http://arxiv.org/abs/2403.19615v1;2024-03-28;SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing;"In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian
Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs
modifying the training procedure of Gaussian splatting, our method functions at
test-time and is training-free. Specifically, SA-GS can be applied to any
pretrained Gaussian splatting field as a plugin to significantly improve the
field's anti-alising performance. The core technique is to apply 2D
scale-adaptive filters to each Gaussian during test time. As pointed out by
Mip-Splatting, observing Gaussians at different frequencies leads to mismatches
between the Gaussian scales during training and testing. Mip-Splatting resolves
this issue using 3D smoothing and 2D Mip filters, which are unfortunately not
aware of testing frequency. In this work, we show that a 2D scale-adaptive
filter that is informed of testing frequency can effectively match the Gaussian
scale, thus making the Gaussian primitive distribution remain consistent across
different testing frequencies. When scale inconsistency is eliminated, sampling
rates smaller than the scene frequency result in conventional jaggedness, and
we propose to integrate the projected 2D Gaussian within each pixel during
testing. This integration is actually a limiting case of super-sampling, which
significantly improves anti-aliasing performance over vanilla Gaussian
Splatting. Through extensive experiments using various settings and both
bounded and unbounded scenes, we show SA-GS performs comparably with or better
than Mip-Splatting. Note that super-sampling and integration are only effective
when our scale-adaptive filtering is activated. Our codes, data and models are
available at https://github.com/zsy1987/SA-GS.";Xiaowei Song<author:sep>Jv Zheng<author:sep>Shiran Yuan<author:sep>Huan-ang Gao<author:sep>Jingwei Zhao<author:sep>Xiang He<author:sep>Weihao Gu<author:sep>Hao Zhao;http://arxiv.org/pdf/2403.19615v1;cs.CV;"Project page: https://kevinsong729.github.io/project-pages/SA-GS/
  Code: https://github.com/zsy1987/SA-GS";gaussian splatting
2403.19655v1;http://arxiv.org/abs/2403.19655v1;2024-03-28;GaussianCube: Structuring Gaussian Splatting using Optimal Transport for  3D Generative Modeling;"3D Gaussian Splatting (GS) have achieved considerable improvement over Neural
Radiance Fields in terms of 3D fitting fidelity and rendering speed. However,
this unstructured representation with scattered Gaussians poses a significant
challenge for generative modeling. To address the problem, we introduce
GaussianCube, a structured GS representation that is both powerful and
efficient for generative modeling. We achieve this by first proposing a
modified densification-constrained GS fitting algorithm which can yield
high-quality fitting results using a fixed number of free Gaussians, and then
re-arranging the Gaussians into a predefined voxel grid via Optimal Transport.
The structured grid representation allows us to use standard 3D U-Net as our
backbone in diffusion generative modeling without elaborate designs. Extensive
experiments conducted on ShapeNet and OmniObject3D show that our model achieves
state-of-the-art generation results both qualitatively and quantitatively,
underscoring the potential of GaussianCube as a powerful and versatile 3D
representation.";Bowen Zhang<author:sep>Yiji Cheng<author:sep>Jiaolong Yang<author:sep>Chunyu Wang<author:sep>Feng Zhao<author:sep>Yansong Tang<author:sep>Dong Chen<author:sep>Baining Guo;http://arxiv.org/pdf/2403.19655v1;cs.CV;Project Page: https://gaussiancube.github.io/;gaussian splatting
2403.19780v1;http://arxiv.org/abs/2403.19780v1;2024-03-28;Mitigating Motion Blur in Neural Radiance Fields with Events and Frames;"Neural Radiance Fields (NeRFs) have shown great potential in novel view
synthesis. However, they struggle to render sharp images when the data used for
training is affected by motion blur. On the other hand, event cameras excel in
dynamic scenes as they measure brightness changes with microsecond resolution
and are thus only marginally affected by blur. Recent methods attempt to
enhance NeRF reconstructions under camera motion by fusing frames and events.
However, they face challenges in recovering accurate color content or constrain
the NeRF to a set of predefined camera poses, harming reconstruction quality in
challenging conditions. This paper proposes a novel formulation addressing
these issues by leveraging both model- and learning-based modules. We
explicitly model the blur formation process, exploiting the event double
integral as an additional model-based prior. Additionally, we model the
event-pixel response using an end-to-end learnable response function, allowing
our method to adapt to non-idealities in the real event-camera sensor. We show,
on synthetic and real data, that the proposed approach outperforms existing
deblur NeRFs that use only frames as well as those that combine frames and
events by +6.13dB and +2.48dB, respectively.";Marco Cannici<author:sep>Davide Scaramuzza;http://arxiv.org/pdf/2403.19780v1;cs.CV;"IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2024";nerf
2403.19319v1;http://arxiv.org/abs/2403.19319v1;2024-03-28;Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field  Representation and Generation;"We present Mesh2NeRF, an approach to derive ground-truth radiance fields from
textured meshes for 3D generation tasks. Many 3D generative approaches
represent 3D scenes as radiance fields for training. Their ground-truth
radiance fields are usually fitted from multi-view renderings from a
large-scale synthetic 3D dataset, which often results in artifacts due to
occlusions or under-fitting issues. In Mesh2NeRF, we propose an analytic
solution to directly obtain ground-truth radiance fields from 3D meshes,
characterizing the density field with an occupancy function featuring a defined
surface thickness, and determining view-dependent color through a reflection
function considering both the mesh and environment lighting. Mesh2NeRF extracts
accurate radiance fields which provides direct supervision for training
generative NeRFs and single scene representation. We validate the effectiveness
of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in
PSNR for view synthesis in single scene representation on the ABO dataset, a
0.69 PSNR enhancement in the single-view conditional generation of ShapeNet
Cars, and notably improved mesh extraction from NeRF in the unconditional
generation of Objaverse Mugs.";Yujin Chen<author:sep>Yinyu Nie<author:sep>Benjamin Ummenhofer<author:sep>Reiner Birkl<author:sep>Michael Paulitsch<author:sep>Matthias Müller<author:sep>Matthias Nießner;http://arxiv.org/pdf/2403.19319v1;cs.CV;"Project page: https://terencecyj.github.io/projects/Mesh2NeRF/ Video:
  https://youtu.be/oufv1N3f7iY";nerf
2403.19607v1;http://arxiv.org/abs/2403.19607v1;2024-03-28;SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent  Objects;"Acquiring accurate depth information of transparent objects using
off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and
Robotics. Depth estimation/completion methods are typically employed and
trained on datasets with quality depth labels acquired from either simulation,
additional sensors or specialized data collection setups and known 3d models.
However, acquiring reliable depth information for datasets at scale is not
straightforward, limiting training scalability and generalization. Neural
Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide
success in novel view synthesis and shape recovery. However, heuristics and
controlled environments (lights, backgrounds, etc) are often required to
accurately capture specular surfaces. In this paper, we propose using Visual
Foundation Models (VFMs) for segmentation in a zero-shot, label-free way to
guide the NeRF reconstruction process for these objects via the simultaneous
reconstruction of semantic fields and extensions to increase robustness. Our
proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant
performance on depth completion datasets for transparent objects and robotic
grasping.";Avinash Ummadisingu<author:sep>Jongkeum Choi<author:sep>Koki Yamane<author:sep>Shimpei Masuda<author:sep>Naoki Fukaya<author:sep>Kuniyuki Takahashi;http://arxiv.org/pdf/2403.19607v1;cs.RO;"8 pages. An accompanying video is available at
  https://www.youtube.com/watch?v=S4NCoUq4bmE";nerf
2403.19243v1;http://arxiv.org/abs/2403.19243v1;2024-03-28;Sine Activated Low-Rank Matrices for Parameter Efficient Learning;"Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model accuracy. Our
method proves to be an adaptable enhancement for existing low-rank models, as
evidenced by its successful application in Vision Transformers (ViT), Large
Language Models (LLMs), Neural Radiance Fields (NeRF), and 3D shape modeling.
This demonstrates the wide-ranging potential and efficiency of our proposed
technique.";Yiping Ji<author:sep>Hemanth Saratchandran<author:sep>Cameron Gordon<author:sep>Zeyu Zhang<author:sep>Simon Lucey;http://arxiv.org/pdf/2403.19243v1;cs.LG;The first two authors contributed equally;nerf
2403.18784v2;http://arxiv.org/abs/2403.18784v2;2024-03-27;SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable  Surface;"We present SplatFace, a novel Gaussian splatting framework designed for 3D
human face reconstruction without reliance on accurate pre-determined geometry.
Our method is designed to simultaneously deliver both high-quality novel view
rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D
Morphable Model (3DMM) to provide a surface geometric structure, making it
possible to reconstruct faces with a limited set of input images. We introduce
a joint optimization strategy that refines both the Gaussians and the morphable
surface through a synergistic non-rigid alignment process. A novel distance
metric, splat-to-surface, is proposed to improve alignment by considering both
the Gaussian position and covariance. The surface information is also utilized
to incorporate a world-space densification process, resulting in superior
reconstruction quality. Our experimental analysis demonstrates that the
proposed method is competitive with both other Gaussian splatting techniques in
novel view synthesis and other 3D reconstruction methods in producing 3D face
meshes with high geometric precision.";Jiahao Luo<author:sep>Jing Liu<author:sep>James Davis;http://arxiv.org/pdf/2403.18784v2;cs.CV;;gaussian splatting
2403.18795v2;http://arxiv.org/abs/2403.18795v2;2024-03-27;Gamba: Marry Gaussian Splatting with Mamba for single view 3D  reconstruction;"We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.";Qiuhong Shen<author:sep>Xuanyu Yi<author:sep>Zike Wu<author:sep>Pan Zhou<author:sep>Hanwang Zhang<author:sep>Shuicheng Yan<author:sep>Xinchao Wang;http://arxiv.org/pdf/2403.18795v2;cs.CV;;gaussian splatting<tag:sep>nerf
2403.18711v1;http://arxiv.org/abs/2403.18711v1;2024-03-27;SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable  Transient-Free 3D reconstruction from Satellite Imagery;"Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.";Camille Billouard<author:sep>Dawa Derksen<author:sep>Emmanuelle Sarrazin<author:sep>Bruno Vallet;http://arxiv.org/pdf/2403.18711v1;cs.CV;"5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP";nerf
2403.18476v1;http://arxiv.org/abs/2403.18476v1;2024-03-27;Modeling uncertainty for Gaussian Splatting;"We present Stochastic Gaussian Splatting (SGS): the first framework for
uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the
novel-view synthesis field by achieving impressive reconstruction quality at a
fraction of the computational cost of Neural Radiance Fields (NeRF). However,
contrary to the latter, it still lacks the ability to provide information about
the confidence associated with their outputs. To address this limitation, in
this paper, we introduce a Variational Inference-based approach that seamlessly
integrates uncertainty prediction into the common rendering pipeline of GS.
Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new
term in the loss function, enabling optimization of uncertainty estimation
alongside image reconstruction. Experimental results on the LLFF dataset
demonstrate that our method outperforms existing approaches in terms of both
image rendering quality and uncertainty estimation accuracy. Overall, our
framework equips practitioners with valuable insights into the reliability of
synthesized views, facilitating safer decision-making in real-world
applications.";Luca Savant<author:sep>Diego Valsesia<author:sep>Enrico Magli;http://arxiv.org/pdf/2403.18476v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.17898v1;http://arxiv.org/abs/2403.17898v1;2024-03-26;Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D  Gaussians;"The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering
fidelity and efficiency compared to NeRF-based neural scene representations.
While demonstrating the potential for real-time rendering, 3D-GS encounters
rendering bottlenecks in large scenes with complex details due to an excessive
number of Gaussian primitives located within the viewing frustum. This
limitation is particularly noticeable in zoom-out views and can lead to
inconsistent rendering speeds in scenes with varying details. Moreover, it
often struggles to capture the corresponding level of details at different
scales with its heuristic density control operation. Inspired by the
Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an
LOD-structured 3D Gaussian approach supporting level-of-detail decomposition
for scene representation that contributes to the final rendering results. Our
model dynamically selects the appropriate level from the set of
multi-resolution anchor points, ensuring consistent rendering performance with
adaptive LOD adjustments while maintaining high-fidelity rendering results.";Kerui Ren<author:sep>Lihan Jiang<author:sep>Tao Lu<author:sep>Mulin Yu<author:sep>Linning Xu<author:sep>Zhangkai Ni<author:sep>Bo Dai;http://arxiv.org/pdf/2403.17898v1;cs.CV;Project page: https://city-super.github.io/octree-gs/;gaussian splatting<tag:sep>nerf
2403.17537v1;http://arxiv.org/abs/2403.17537v1;2024-03-26;NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using  Heuristics-Guided Segmentation;"Neural Radiance Field (NeRF) has been widely recognized for its excellence in
novel view synthesis and 3D scene reconstruction. However, their effectiveness
is inherently tied to the assumption of static scenes, rendering them
susceptible to undesirable artifacts when confronted with transient distractors
such as moving objects or shadows. In this work, we propose a novel paradigm,
namely ""Heuristics-Guided Segmentation"" (HuGS), which significantly enhances
the separation of static scenes from transient distractors by harmoniously
combining the strengths of hand-crafted heuristics and state-of-the-art
segmentation models, thus significantly transcending the limitations of
previous solutions. Furthermore, we delve into the meticulous design of
heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based
heuristics and color residual heuristics, catering to a diverse range of
texture profiles. Extensive experiments demonstrate the superiority and
robustness of our method in mitigating transient distractors for NeRFs trained
in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.";Jiahao Chen<author:sep>Yipeng Qin<author:sep>Lingjie Liu<author:sep>Jiangbo Lu<author:sep>Guanbin Li;http://arxiv.org/pdf/2403.17537v1;cs.CV;To appear in CVPR2024;nerf
2403.17888v1;http://arxiv.org/abs/2403.17888v1;2024-03-26;2D Gaussian Splatting for Geometrically Accurate Radiance Fields;"3D Gaussian Splatting (3DGS) has recently revolutionized radiance field
reconstruction, achieving high quality novel view synthesis and fast rendering
speed without baking. However, 3DGS fails to accurately represent surfaces due
to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian
Splatting (2DGS), a novel approach to model and reconstruct geometrically
accurate radiance fields from multi-view images. Our key idea is to collapse
the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D
Gaussians, 2D Gaussians provide view-consistent geometry while modeling
surfaces intrinsically. To accurately recover thin surfaces and achieve stable
optimization, we introduce a perspective-accurate 2D splatting process
utilizing ray-splat intersection and rasterization. Additionally, we
incorporate depth distortion and normal consistency terms to further enhance
the quality of the reconstructions. We demonstrate that our differentiable
renderer allows for noise-free and detailed geometry reconstruction while
maintaining competitive appearance quality, fast training speed, and real-time
rendering. Our code will be made publicly available.";Binbin Huang<author:sep>Zehao Yu<author:sep>Anpei Chen<author:sep>Andreas Geiger<author:sep>Shenghua Gao;http://arxiv.org/pdf/2403.17888v1;cs.CV;12 pages, 12 figures;gaussian splatting
2403.17822v1;http://arxiv.org/abs/2403.17822v1;2024-03-26;DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing;"3D Gaussian splatting, a novel differentiable rendering technique, has
achieved state-of-the-art novel view synthesis results with high rendering
speeds and relatively low training times. However, its performance on scenes
commonly seen in indoor datasets is poor due to the lack of geometric
constraints during optimization. We extend 3D Gaussian splatting with depth and
normal cues to tackle challenging indoor datasets and showcase techniques for
efficient mesh extraction, an important downstream application. Specifically,
we regularize the optimization procedure with depth information, enforce local
smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians
supervised by normal cues to achieve better alignment with the true scene
geometry. We improve depth estimation and novel view synthesis results over
baselines and show how this simple yet effective regularization technique can
be used to directly extract meshes from the Gaussian representation yielding
more physically accurate reconstructions on indoor scenes. Our code will be
released in https://github.com/maturk/dn-splatter.";Matias Turkulainen<author:sep>Xuqian Ren<author:sep>Iaroslav Melekhov<author:sep>Otto Seiskari<author:sep>Esa Rahtu<author:sep>Juho Kannala;http://arxiv.org/pdf/2403.17822v1;cs.CV;;gaussian splatting
2403.16885v1;http://arxiv.org/abs/2403.16885v1;2024-03-25;CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance  Fields from Sparse Inputs;"Neural Radiance Fields (NeRF) have shown impressive capabilities for
photorealistic novel view synthesis when trained on dense inputs. However, when
trained on sparse inputs, NeRF typically encounters issues of incorrect density
or color predictions, mainly due to insufficient coverage of the scene causing
partial and sparse supervision, thus leading to significant performance
degradation. While existing works mainly consider ray-level consistency to
construct 2D learning regularization based on rendered color, depth, or
semantics on image planes, in this paper we propose a novel approach that
models 3D spatial field consistency to improve NeRF's performance with sparse
inputs. Specifically, we first adopt a voxel-based ray sampling strategy to
ensure that the sampled rays intersect with a certain voxel in 3D space. We
then randomly sample additional points within the voxel and apply a Transformer
to infer the properties of other points on each ray, which are then
incorporated into the volume rendering. By backpropagating through the
rendering loss, we enhance the consistency among neighboring points.
Additionally, we propose to use a contrastive loss on the encoder output of the
Transformer to further improve consistency within each voxel. Experiments
demonstrate that our method yields significant improvement over different
radiance fields in the sparse inputs setting, and achieves comparable
performance with current works.";Yingji Zhong<author:sep>Lanqing Hong<author:sep>Zhenguo Li<author:sep>Dan Xu;http://arxiv.org/pdf/2403.16885v1;cs.CV;"The paper is accepted by CVPR 2024. Project page is available at
  https://zhongyingji.github.io/CVT-xRF";nerf
2403.17001v1;http://arxiv.org/abs/2403.17001v1;2024-03-25;VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation;"Recent innovations on text-to-3D generation have featured Score Distillation
Sampling (SDS), which enables the zero-shot learning of implicit 3D models
(NeRF) by directly distilling prior knowledge from 2D diffusion models.
However, current SDS-based models still struggle with intricate text prompts
and commonly result in distorted 3D models with unrealistic textures or
cross-view inconsistency issues. In this work, we introduce a novel Visual
Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the
visual appearance knowledge in 2D visual prompt to boost text-to-3D generation.
Instead of solely supervising SDS with text prompt, VP3D first capitalizes on
2D diffusion model to generate a high-quality image from input text, which
subsequently acts as visual prompt to strengthen SDS optimization with explicit
visual appearance. Meanwhile, we couple the SDS optimization with additional
differentiable reward function that encourages rendering images of 3D models to
better visually align with 2D visual prompt and semantically match with text
prompt. Through extensive experiments, we show that the 2D Visual Prompt in our
VP3D significantly eases the learning of visual appearance of 3D models and
thus leads to higher visual fidelity with more detailed textures. It is also
appealing in view that when replacing the self-generating visual prompt with a
given reference image, VP3D is able to trigger a new task of stylized
text-to-3D generation. Our project page is available at
https://vp3d-cvpr24.github.io.";Yang Chen<author:sep>Yingwei Pan<author:sep>Haibo Yang<author:sep>Ting Yao<author:sep>Tao Mei;http://arxiv.org/pdf/2403.17001v1;cs.CV;"CVPR 2024; Project page: https://vp3d-cvpr24.github.io";nerf
2403.16410v1;http://arxiv.org/abs/2403.16410v1;2024-03-25;Spike-NeRF: Neural Radiance Field Based On Spike Camera;"As a neuromorphic sensor with high temporal resolution, spike cameras offer
notable advantages over traditional cameras in high-speed vision applications
such as high-speed optical estimation, depth estimation, and object tracking.
Inspired by the success of the spike camera, we proposed Spike-NeRF, the first
Neural Radiance Field derived from spike data, to achieve 3D reconstruction and
novel viewpoint synthesis of high-speed scenes. Instead of the multi-view
images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike
streams captured by a moving spike camera in a very short time. To reconstruct
a correct and stable 3D scene from high-frequency but unstable spike data, we
devised spike masks along with a distinctive loss function. We evaluate our
method qualitatively and numerically on several challenging synthetic scenes
generated by blender with the spike camera simulator. Our results demonstrate
that Spike-NeRF produces more visually appealing results than the existing
methods and the baseline we proposed in high-speed scenes. Our code and data
will be released soon.";Yijia Guo<author:sep>Yuanxi Bai<author:sep>Liwen Hu<author:sep>Mianzhi Liu<author:sep>Ziyi Guo<author:sep>Lei Ma<author:sep>Tiejun Huang;http://arxiv.org/pdf/2403.16410v1;cs.CV;This paper is accepted by ICME2024;nerf
2403.17237v1;http://arxiv.org/abs/2403.17237v1;2024-03-25;DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric  Diffusion;"We present DreamPolisher, a novel Gaussian Splatting based method with
geometric guidance, tailored to learn cross-view consistency and intricate
detail from textual descriptions. While recent progress on text-to-3D
generation methods have been promising, prevailing methods often fail to ensure
view-consistency and textural richness. This problem becomes particularly
noticeable for methods that work with text input alone. To address this, we
propose a two-stage Gaussian Splatting based approach that enforces geometric
consistency among views. Initially, a coarse 3D generation undergoes refinement
via geometric optimization. Subsequently, we use a ControlNet driven refiner
coupled with the geometric consistency term to improve both texture fidelity
and overall consistency of the generated 3D asset. Empirical evaluations across
diverse textual prompts spanning various object categories demonstrate the
efficacy of DreamPolisher in generating consistent and realistic 3D objects,
aligning closely with the semantics of the textual instructions.";Yuanze Lin<author:sep>Ronald Clark<author:sep>Philip Torr;http://arxiv.org/pdf/2403.17237v1;cs.CV;Project webpage: https://yuanze-lin.me/DreamPolisher_page/;gaussian splatting
2403.16964v1;http://arxiv.org/abs/2403.16964v1;2024-03-25;GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction;"Presenting a 3D scene from multiview images remains a core and long-standing
challenge in computer vision and computer graphics. Two main requirements lie
in rendering and reconstruction. Notably, SOTA rendering quality is usually
achieved with neural volumetric rendering techniques, which rely on aggregated
point/primitive-wise color and neglect the underlying scene geometry. Learning
of neural implicit surfaces is sparked from the success of neural rendering.
Current works either constrain the distribution of density fields or the shape
of primitives, resulting in degraded rendering quality and flaws on the learned
scene surfaces. The efficacy of such methods is limited by the inherent
constraints of the chosen neural representation, which struggles to capture
fine surface details, especially for larger, more intricate scenes. To address
these issues, we introduce GSDF, a novel dual-branch architecture that combines
the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS)
representation with neural Signed Distance Fields (SDF). The core idea is to
leverage and enhance the strengths of each branch while alleviating their
limitation through mutual guidance and joint supervision. We show on diverse
scenes that our design unlocks the potential for more accurate and detailed
surface reconstructions, and at the meantime benefits 3DGS rendering with
structures that are more aligned with the underlying geometry.";Mulin Yu<author:sep>Tao Lu<author:sep>Linning Xu<author:sep>Lihan Jiang<author:sep>Yuanbo Xiangli<author:sep>Bo Dai;http://arxiv.org/pdf/2403.16964v1;cs.CV;Project page: https://city-super.github.io/GSDF;gaussian splatting
2403.16292v1;http://arxiv.org/abs/2403.16292v1;2024-03-24;latentSplat: Autoencoding Variational Gaussians for Fast Generalizable  3D Reconstruction;"We present latentSplat, a method to predict semantic Gaussians in a 3D latent
space that can be splatted and decoded by a light-weight generative 2D
architecture. Existing methods for generalizable 3D reconstruction either do
not enable fast inference of high resolution novel views due to slow volume
rendering, or are limited to interpolation of close input views, even in
simpler settings with a single central object, where 360-degree generalization
is possible. In this work, we combine a regression-based approach with a
generative model, moving towards both of these capabilities within the same
method, trained purely on readily available real video data. The core of our
method are variational 3D Gaussians, a representation that efficiently encodes
varying uncertainty within a latent space consisting of 3D feature Gaussians.
From these Gaussians, specific instances can be sampled and rendered via
efficient Gaussian splatting and a fast, generative decoder network. We show
that latentSplat outperforms previous works in reconstruction quality and
generalization, while being fast and scalable to high-resolution data.";Christopher Wewer<author:sep>Kevin Raj<author:sep>Eddy Ilg<author:sep>Bernt Schiele<author:sep>Jan Eric Lenssen;http://arxiv.org/pdf/2403.16292v1;cs.CV;Project website: https://geometric-rl.mpi-inf.mpg.de/latentsplat/;gaussian splatting
2403.16095v1;http://arxiv.org/abs/2403.16095v1;2024-03-24;CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D  Gaussian Field;"Recently neural radiance fields (NeRF) have been widely exploited as 3D
representations for dense simultaneous localization and mapping (SLAM). Despite
their notable successes in surface modeling and novel view synthesis, existing
NeRF-based methods are hindered by their computationally intensive and
time-consuming volume rendering pipeline. This paper presents an efficient
dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D
Gaussian field with high consistency and geometric stability. Through an
in-depth analysis of Gaussian Splatting, we propose several techniques to
construct a consistent and stable 3D Gaussian field suitable for tracking and
mapping. Additionally, a novel depth uncertainty model is proposed to ensure
the selection of valuable Gaussian primitives during optimization, thereby
improving tracking efficiency and accuracy. Experiments on various datasets
demonstrate that CG-SLAM achieves superior tracking and mapping performance
with a notable tracking speed of up to 15 Hz. We will make our source code
publicly available. Project page: https://zju3dv.github.io/cg-slam.";Jiarui Hu<author:sep>Xianhao Chen<author:sep>Boyin Feng<author:sep>Guanglin Li<author:sep>Liangjing Yang<author:sep>Hujun Bao<author:sep>Guofeng Zhang<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2403.16095v1;cs.CV;Project Page: https://zju3dv.github.io/cg-slam;gaussian splatting<tag:sep>nerf
2403.16092v1;http://arxiv.org/abs/2403.16092v1;2024-03-24;Are NeRFs ready for autonomous driving? Towards closing the  real-to-simulation gap;"Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing
autonomous driving (AD) research, offering scalable closed-loop simulation and
data augmentation capabilities. However, to trust the results achieved in
simulation, one needs to ensure that AD systems perceive real and rendered data
in the same way. Although the performance of rendering methods is increasing,
many scenarios will remain inherently challenging to reconstruct faithfully. To
this end, we propose a novel perspective for addressing the real-to-simulated
data gap. Rather than solely focusing on improving rendering fidelity, we
explore simple yet effective methods to enhance perception model robustness to
NeRF artifacts without compromising performance on real data. Moreover, we
conduct the first large-scale investigation into the real-to-simulated data gap
in an AD setting using a state-of-the-art neural rendering technique.
Specifically, we evaluate object detectors and an online mapping model on real
and simulated data, and study the effects of different pre-training strategies.
Our results show notable improvements in model robustness to simulated data,
even improving real-world performance in some cases. Last, we delve into the
correlation between the real-to-simulated gap and image reconstruction metrics,
identifying FID and LPIPS as strong indicators.";Carl Lindström<author:sep>Georg Hess<author:sep>Adam Lilja<author:sep>Maryam Fatemi<author:sep>Lars Hammarstrand<author:sep>Christoffer Petersson<author:sep>Lennart Svensson;http://arxiv.org/pdf/2403.16092v1;cs.CV;;nerf
2403.16080v2;http://arxiv.org/abs/2403.16080v2;2024-03-24;PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic  Human Modeling;"High-quality human reconstruction and photo-realistic rendering of a dynamic
scene is a long-standing problem in computer vision and graphics. Despite
considerable efforts invested in developing various capture systems and
reconstruction algorithms, recent advancements still struggle with loose or
oversized clothing and overly complex poses. In part, this is due to the
challenges of acquiring high-quality human datasets. To facilitate the
development of these fields, in this paper, we present PKU-DyMVHumans, a
versatile human-centric dataset for high-fidelity reconstruction and rendering
of dynamic human scenarios from dense multi-view videos. It comprises 8.2
million frames captured by more than 56 synchronized cameras across diverse
scenarios. These sequences comprise 32 human subjects across 45 different
scenarios, each with a high-detailed appearance and realistic human motion.
Inspired by recent advancements in neural radiance field (NeRF)-based scene
representations, we carefully set up an off-the-shelf framework that is easy to
provide those state-of-the-art NeRF-based implementations and benchmark on
PKU-DyMVHumans dataset. It is paving the way for various applications like
fine-grained foreground/background decomposition, high-quality human
reconstruction and photo-realistic novel view synthesis of a dynamic scene.
Extensive studies are performed on the benchmark, demonstrating new
observations and challenges that emerge from using such high-fidelity dynamic
data. The dataset is available at: https://pku-dymvhumans.github.io.";Xiaoyun Zheng<author:sep>Liwei Liao<author:sep>Xufeng Li<author:sep>Jianbo Jiao<author:sep>Rongjie Wang<author:sep>Feng Gao<author:sep>Shiqi Wang<author:sep>Ronggang Wang;http://arxiv.org/pdf/2403.16080v2;cs.CV;;nerf
2403.15981v2;http://arxiv.org/abs/2403.15981v2;2024-03-24;Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance  Fields;"Accurate collection of plant phenotyping is critical to optimising
sustainable farming practices in precision agriculture. Traditional phenotyping
in controlled laboratory environments, while valuable, falls short in
understanding plant growth under real-world conditions. Emerging sensor and
digital technologies offer a promising approach for direct phenotyping of
plants in farm environments. This study investigates a learning-based
phenotyping method using the Neural Radiance Field to achieve accurate in-situ
phenotyping of pepper plants in greenhouse environments. To quantitatively
evaluate the performance of this method, traditional point cloud registration
on 3D scanning data is implemented for comparison. Experimental result shows
that NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the
3D scanning methods. The mean distance error between the scanner-based method
and the NeRF-based method is 0.865mm. This study shows that the learning-based
NeRF method achieves similar accuracy to 3D scanning-based methods but with
improved scalability and robustness.";Junhong Zhao<author:sep>Wei Ying<author:sep>Yaoqiang Pan<author:sep>Zhenfeng Yi<author:sep>Chao Chen<author:sep>Kewei Hu<author:sep>Hanwen Kang;http://arxiv.org/pdf/2403.15981v2;cs.CV;;nerf
2403.16141v1;http://arxiv.org/abs/2403.16141v1;2024-03-24;Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes;"Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic
scenes often involve explicit modeling of scene dynamics. However, this
approach faces challenges in modeling scene dynamics in urban environments,
where moving objects of various categories and scales are present. In such
settings, it becomes crucial to effectively eliminate moving objects to
accurately reconstruct static backgrounds. Our research introduces an
innovative method, termed here as Entity-NeRF, which combines the strengths of
knowledge-based and statistical strategies. This approach utilizes entity-wise
statistics, leveraging entity segmentation and stationary entity classification
through thing/stuff segmentation. To assess our methodology, we created an
urban scene dataset masked with moving objects. Our comprehensive experiments
demonstrate that Entity-NeRF notably outperforms existing techniques in
removing moving objects and reconstructing static urban backgrounds, both
quantitatively and qualitatively.";Takashi Otonari<author:sep>Satoshi Ikehata<author:sep>Kiyoharu Aizawa;http://arxiv.org/pdf/2403.16141v1;cs.CV;"Accepted by IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2024), Project website:
  https://otonari726.github.io/entitynerf/";nerf
2403.16043v1;http://arxiv.org/abs/2403.16043v1;2024-03-24;Semantic Is Enough: Only Semantic Information For NeRF Reconstruction;"Recent research that combines implicit 3D representation with semantic
information, like Semantic-NeRF, has proven that NeRF model could perform
excellently in rendering 3D structures with semantic labels. This research aims
to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing
solely on semantic output and removing the RGB output component. We reformulate
the model and its training procedure to leverage only the cross-entropy loss
between the model semantic output and the ground truth semantic images,
removing the colour data traditionally used in the original Semantic-NeRF
approach. We then conduct a series of identical experiments using the original
and the modified Semantic-NeRF model. Our primary objective is to obverse the
impact of this modification on the model performance by Semantic-NeRF, focusing
on tasks such as scene understanding, object detection, and segmentation. The
results offer valuable insights into the new way of rendering the scenes and
provide an avenue for further research and development in semantic-focused 3D
scene understanding.";Ruibo Wang<author:sep>Song Zhang<author:sep>Ping Huang<author:sep>Donghai Zhang<author:sep>Wei Yan;http://arxiv.org/pdf/2403.16043v1;cs.CV;;nerf
2403.16224v1;http://arxiv.org/abs/2403.16224v1;2024-03-24;Inverse Rendering of Glossy Objects via the Neural Plenoptic Function  and Radiance Fields;"Inverse rendering aims at recovering both geometry and materials of objects.
It provides a more compatible reconstruction for conventional rendering
engines, compared with the neural radiance fields (NeRFs). On the other hand,
existing NeRF-based inverse rendering methods cannot handle glossy objects with
local light interactions well, as they typically oversimplify the illumination
as a 2D environmental map, which assumes infinite lights only. Observing the
superiority of NeRFs in recovering radiance fields, we propose a novel 5D
Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more
accurate lighting-object interactions can be formulated via the rendering
equation. We also design a material-aware cone sampling strategy to efficiently
integrate lights inside the BRDF lobes with the help of pre-filtered radiance
fields. Our method has two stages: the geometry of the target object and the
pre-filtered environmental radiance fields are reconstructed in the first
stage, and materials of the target object are estimated in the second stage
with the proposed NeP and material-aware cone sampling strategy. Extensive
experiments on the proposed real-world and synthetic datasets demonstrate that
our method can reconstruct high-fidelity geometry/materials of challenging
glossy objects with complex lighting interactions from nearby objects. Project
webpage: https://whyy.site/paper/nep";Haoyuan Wang<author:sep>Wenbo Hu<author:sep>Lei Zhu<author:sep>Rynson W. H. Lau;http://arxiv.org/pdf/2403.16224v1;cs.CV;CVPR 2024 paper. Project webpage https://whyy.site/paper/nep;nerf
2403.15704v1;http://arxiv.org/abs/2403.15704v1;2024-03-23;Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image  Collections;"Novel view synthesis from unconstrained in-the-wild images remains a
meaningful but challenging task. The photometric variation and transient
occluders in those unconstrained images make it difficult to reconstruct the
original scene accurately. Previous approaches tackle the problem by
introducing a global appearance feature in Neural Radiance Fields (NeRF).
However, in the real world, the unique appearance of each tiny point in a scene
is determined by its independent intrinsic material attributes and the varying
environmental impacts it receives. Inspired by this fact, we propose Gaussian
in the wild (GS-W), a method that uses 3D Gaussian points to reconstruct the
scene and introduces separated intrinsic and dynamic appearance feature for
each point, capturing the unchanged scene appearance along with dynamic
variation like illumination and weather. Additionally, an adaptive sampling
strategy is presented to allow each Gaussian point to focus on the local and
detailed information more effectively. We also reduce the impact of transient
occluders using a 2D visibility map. More experiments have demonstrated better
reconstruction quality and details of GS-W compared to previous methods, with a
$1000\times$ increase in rendering speed.";Dongbin Zhang<author:sep>Chuming Wang<author:sep>Weitao Wang<author:sep>Peihao Li<author:sep>Minghan Qin<author:sep>Haoqian Wang;http://arxiv.org/pdf/2403.15704v1;cs.CV;14 pages, 5 figures;gaussian splatting<tag:sep>nerf
2403.15705v1;http://arxiv.org/abs/2403.15705v1;2024-03-23;UPNeRF: A Unified Framework for Monocular 3D Object Reconstruction and  Pose Estimation;"Monocular 3D reconstruction for categorical objects heavily relies on
accurately perceiving each object's pose. While gradient-based optimization
within a NeRF framework updates initially given poses, this paper highlights
that such a scheme fails when the initial pose even moderately deviates from
the true pose. Consequently, existing methods often depend on a third-party 3D
object to provide an initial object pose, leading to increased complexity and
generalization issues. To address these challenges, we present UPNeRF, a
Unified framework integrating Pose estimation and NeRF-based reconstruction,
bringing us closer to real-time monocular 3D object reconstruction. UPNeRF
decouples the object's dimension estimation and pose refinement to resolve the
scale-depth ambiguity, and introduces an effective projected-box representation
that generalizes well cross different domains. While using a dedicated pose
estimator that smoothly integrates into an object-centric NeRF, UPNeRF is free
from external 3D detectors. UPNeRF achieves state-of-the-art results in both
reconstruction and pose estimation tasks on the nuScenes dataset. Furthermore,
UPNeRF exhibits exceptional Cross-dataset generalization on the KITTI and Waymo
datasets, surpassing prior methods with up to 50% reduction in rotation and
translation error.";Yuliang Guo<author:sep>Abhinav Kumar<author:sep>Cheng Zhao<author:sep>Ruoyu Wang<author:sep>Xinyu Huang<author:sep>Liu Ren;http://arxiv.org/pdf/2403.15705v1;cs.CV;;nerf
2403.15791v1;http://arxiv.org/abs/2403.15791v1;2024-03-23;DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving  Environment for Real-World Performance Validation;"In this study, we introduce the DriveEnv-NeRF framework, which leverages
Neural Radiance Fields (NeRF) to enable the validation and faithful forecasting
of the efficacy of autonomous driving agents in a targeted real-world scene.
Standard simulator-based rendering often fails to accurately reflect real-world
performance due to the sim-to-real gap, which represents the disparity between
virtual simulations and real-world conditions. To mitigate this gap, we propose
a workflow for building a high-fidelity simulation environment of the targeted
real-world scene using NeRF. This approach is capable of rendering realistic
images from novel viewpoints and constructing 3D meshes for emulating
collisions. The validation of these capabilities through the comparison of
success rates in both simulated and real environments demonstrates the benefits
of using DriveEnv-NeRF as a real-world performance indicator. Furthermore, the
DriveEnv-NeRF framework can serve as a training environment for autonomous
driving agents under various lighting conditions. This approach enhances the
robustness of the agents and reduces performance degradation when deployed to
the target real scene, compared to agents fully trained using the standard
simulator rendering pipeline.";Mu-Yi Shen<author:sep>Chia-Chi Hsu<author:sep>Hao-Yu Hou<author:sep>Yu-Chen Huang<author:sep>Wei-Fang Sun<author:sep>Chia-Che Chang<author:sep>Yu-Lun Liu<author:sep>Chun-Yi Lee;http://arxiv.org/pdf/2403.15791v1;cs.RO;Project page: https://github.com/muyishen2040/DriveEnvNeRF;nerf
2403.14939v1;http://arxiv.org/abs/2403.14939v1;2024-03-22;STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians;"Recent progress in pre-trained diffusion models and 3D generation have
spurred interest in 4D content creation. However, achieving high-fidelity 4D
generation with spatial-temporal consistency remains a challenge. In this work,
we propose STAG4D, a novel framework that combines pre-trained diffusion models
with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing
inspiration from 3D generation techniques, we utilize a multi-view diffusion
model to initialize multi-view images anchoring on the input video frames,
where the video can be either real-world captured or generated by a video
diffusion model. To ensure the temporal consistency of the multi-view sequence
initialization, we introduce a simple yet effective fusion strategy to leverage
the first frame as a temporal anchor in the self-attention computation. With
the almost consistent multi-view sequences, we then apply the score
distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian
spatting is specially crafted for the generation task, where an adaptive
densification strategy is proposed to mitigate the unstable Gaussian gradient
for robust optimization. Notably, the proposed pipeline does not require any
pre-training or fine-tuning of diffusion networks, offering a more accessible
and practical solution for the 4D generation task. Extensive experiments
demonstrate that our method outperforms prior 4D generation works in rendering
quality, spatial-temporal consistency, and generation robustness, setting a new
state-of-the-art for 4D generation from diverse inputs, including text, image,
and video.";Yifei Zeng<author:sep>Yanqin Jiang<author:sep>Siyu Zhu<author:sep>Yuanxun Lu<author:sep>Youtian Lin<author:sep>Hao Zhu<author:sep>Weiming Hu<author:sep>Xun Cao<author:sep>Yao Yao;http://arxiv.org/pdf/2403.14939v1;cs.CV;;gaussian splatting
2403.15530v1;http://arxiv.org/abs/2403.15530v1;2024-03-22;Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian  Splatting;"3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis
results while advancing real-time rendering performance. However, it relies
heavily on the quality of the initial point cloud, resulting in blurring and
needle-like artifacts in areas with insufficient initializing points. This is
mainly attributed to the point cloud growth condition in 3DGS that only
considers the average gradient magnitude of points from observable views,
thereby failing to grow for large Gaussians that are observable for many
viewpoints while many of them are only covered in the boundaries. To this end,
we propose a novel method, named Pixel-GS, to take into account the number of
pixels covered by the Gaussian in each view during the computation of the
growth condition. We regard the covered pixel numbers as the weights to
dynamically average the gradients from different views, such that the growth of
large Gaussians can be prompted. As a result, points within the areas with
insufficient initializing points can be grown more effectively, leading to a
more accurate and detailed reconstruction. In addition, we propose a simple yet
effective strategy to scale the gradient field according to the distance to the
camera, to suppress the growth of floaters near the camera. Extensive
experiments both qualitatively and quantitatively demonstrate that our method
achieves state-of-the-art rendering quality while maintaining real-time
rendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.";Zheng Zhang<author:sep>Wenbo Hu<author:sep>Yixing Lao<author:sep>Tong He<author:sep>Hengshuang Zhao;http://arxiv.org/pdf/2403.15530v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.15624v1;http://arxiv.org/abs/2403.15624v1;2024-03-22;Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian  Splatting;"Open-vocabulary 3D scene understanding presents a significant challenge in
computer vision, withwide-ranging applications in embodied agents and augmented
reality systems. Previous approaches haveadopted Neural Radiance Fields (NeRFs)
to analyze 3D scenes. In this paper, we introduce SemanticGaussians, a novel
open-vocabulary scene understanding approach based on 3D Gaussian Splatting.
Our keyidea is distilling pre-trained 2D semantics into 3D Gaussians. We design
a versatile projection approachthat maps various 2Dsemantic features from
pre-trained image encoders into a novel semantic component of 3D Gaussians,
withoutthe additional training required by NeRFs. We further build a 3D
semantic network that directly predictsthe semantic component from raw 3D
Gaussians for fast inference. We explore several applications ofSemantic
Gaussians: semantic segmentation on ScanNet-20, where our approach attains a
4.2% mIoU and 4.0%mAcc improvement over prior open-vocabulary scene
understanding counterparts; object part segmentation,sceneediting, and
spatial-temporal segmentation with better qualitative results over 2D and 3D
baselines,highlighting its versatility and effectiveness on supporting diverse
downstream tasks.";Jun Guo<author:sep>Xiaojian Ma<author:sep>Yue Fan<author:sep>Huaping Liu<author:sep>Qing Li;http://arxiv.org/pdf/2403.15624v1;cs.CV;Project page: see https://semantic-gaussians.github.io;gaussian splatting<tag:sep>nerf
2403.15272v1;http://arxiv.org/abs/2403.15272v1;2024-03-22;WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization;"Despite the advancements in deep learning for camera relocalization tasks,
obtaining ground truth pose labels required for the training process remains a
costly endeavor. While current weakly supervised methods excel in lightweight
label generation, their performance notably declines in scenarios with sparse
views. In response to this challenge, we introduce WSCLoc, a system capable of
being customized to various deep learning-based relocalization models to
enhance their performance under weakly-supervised and sparse view conditions.
This is realized with two stages. In the initial stage, WSCLoc employs a
multilayer perceptron-based structure called WFT-NeRF to co-optimize image
reconstruction quality and initial pose information. To ensure a stable
learning process, we incorporate temporal information as input. Furthermore,
instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to
explicitly enforce a scale constraint. In the second stage, we co-optimize the
pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by
Time-Encoding based Random View Synthesis and supervised by inter-frame
geometric constraints that consider pose, depth, and RGB information. We
validate our approaches on two publicly available datasets, one outdoor and one
indoor. Our experimental results demonstrate that our weakly-supervised
relocalization solutions achieve superior pose estimation accuracy in
sparse-view scenarios, comparable to state-of-the-art camera relocalization
methods. We will make our code publicly available.";Jialu Wang<author:sep>Kaichen Zhou<author:sep>Andrew Markham<author:sep>Niki Trigoni;http://arxiv.org/pdf/2403.15272v1;cs.CV;;nerf
2403.15124v1;http://arxiv.org/abs/2403.15124v1;2024-03-22;EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic  Surgeries using Gaussian Splatting;"Precise camera tracking, high-fidelity 3D tissue reconstruction, and
real-time online visualization are critical for intrabody medical imaging
devices such as endoscopes and capsule robots. However, existing SLAM
(Simultaneous Localization and Mapping) methods often struggle to achieve both
complete high-quality surgical field reconstruction and efficient computation,
restricting their intraoperative applications among endoscopic surgeries. In
this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic
surgeries, which integrates streamlined Gaussian representation and
differentiable rasterization to facilitate over 100 fps rendering speed during
online camera tracking and tissue reconstructing. Extensive experiments show
that EndoGSLAM achieves a better trade-off between intraoperative availability
and reconstruction quality than traditional or neural SLAM approaches, showing
tremendous potential for endoscopic surgeries. The project page is at
https://EndoGSLAM.loping151.com";Kailing Wang<author:sep>Chen Yang<author:sep>Yuehao Wang<author:sep>Sikuang Li<author:sep>Yan Wang<author:sep>Qi Dou<author:sep>Xiaokang Yang<author:sep>Wei Shen;http://arxiv.org/pdf/2403.15124v1;cs.CV;;gaussian splatting
2403.14166v1;http://arxiv.org/abs/2403.14166v1;2024-03-21;Mini-Splatting: Representing Scenes with a Constrained Number of  Gaussians;"In this study, we explore the challenge of efficiently representing scenes
with a constrained number of Gaussians. Our analysis shifts from traditional
graphics and 2D computer vision to the perspective of point clouds,
highlighting the inefficient spatial distribution of Gaussian representation as
a key limitation in model performance. To address this, we introduce strategies
for densification including blur split and depth reinitialization, and
simplification through Gaussian binarization and sampling. These techniques
reorganize the spatial positions of the Gaussians, resulting in significant
improvements across various datasets and benchmarks in terms of rendering
quality, resource consumption, and storage compression. Our proposed
Mini-Splatting method integrates seamlessly with the original rasterization
pipeline, providing a strong baseline for future research in
Gaussian-Splatting-based works.";Guangchi Fang<author:sep>Bing Wang;http://arxiv.org/pdf/2403.14166v1;cs.CV;;
2403.14530v1;http://arxiv.org/abs/2403.14530v1;2024-03-21;HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression;"3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel
view synthesis, boasting rapid rendering speed with high fidelity. However, the
substantial Gaussians and their associated attributes necessitate effective
compression techniques. Nevertheless, the sparse and unorganized nature of the
point cloud of Gaussians (or anchors in our paper) presents challenges for
compression. To address this, we make use of the relations between the
unorganized anchors and the structured hash grid, leveraging their mutual
information for context modeling, and propose a Hash-grid Assisted Context
(HAC) framework for highly compact 3DGS representation. Our approach introduces
a binary hash grid to establish continuous spatial consistencies, allowing us
to unveil the inherent spatial relations of anchors through a carefully
designed context model. To facilitate entropy coding, we utilize Gaussian
distributions to accurately estimate the probability of each quantized
attribute, where an adaptive quantization module is proposed to enable
high-precision quantization of these attributes for improved fidelity
restoration. Additionally, we incorporate an adaptive masking strategy to
eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer
to explore context-based compression for 3DGS representation, resulting in a
remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while
simultaneously improving fidelity, and achieving over $11\times$ size reduction
over SOTA 3DGS compression approach Scaffold-GS. Our code is available here:
https://github.com/YihangChen-ee/HAC";Yihang Chen<author:sep>Qianyi Wu<author:sep>Jianfei Cai<author:sep>Mehrtash Harandi<author:sep>Weiyao Lin;http://arxiv.org/pdf/2403.14530v1;cs.CV;"Project Page: https://yihangchen-ee.github.io/project_hac/ Code:
  https://github.com/YihangChen-ee/HAC";gaussian splatting
2403.14627v1;http://arxiv.org/abs/2403.14627v1;2024-03-21;MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images;"We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model
learned from sparse multi-view images. To accurately localize the Gaussian
centers, we propose to build a cost volume representation via plane sweeping in
the 3D space, where the cross-view feature similarities stored in the cost
volume can provide valuable geometry cues to the estimation of depth. We learn
the Gaussian primitives' opacities, covariances, and spherical harmonics
coefficients jointly with the Gaussian centers while only relying on
photometric supervision. We demonstrate the importance of the cost volume
representation in learning feed-forward Gaussian Splatting models via extensive
experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks,
our model achieves state-of-the-art performance with the fastest feed-forward
inference speed (22 fps). Compared to the latest state-of-the-art method
pixelSplat, our model uses $10\times $ fewer parameters and infers more than
$2\times$ faster while providing higher appearance and geometry quality as well
as better cross-dataset generalization.";Yuedong Chen<author:sep>Haofei Xu<author:sep>Chuanxia Zheng<author:sep>Bohan Zhuang<author:sep>Marc Pollefeys<author:sep>Andreas Geiger<author:sep>Tat-Jen Cham<author:sep>Jianfei Cai;http://arxiv.org/pdf/2403.14627v1;cs.CV;"Project page: https://donydchen.github.io/mvsplat Code:
  https://github.com/donydchen/mvsplat";gaussian splatting
2403.14839v1;http://arxiv.org/abs/2403.14839v1;2024-03-21;Hyperspectral Neural Radiance Fields;"Hyperspectral Imagery (HSI) has been used in many applications to
non-destructively determine the material and/or chemical compositions of
samples. There is growing interest in creating 3D hyperspectral
reconstructions, which could provide both spatial and spectral information
while also mitigating common HSI challenges such as non-Lambertian surfaces and
translucent objects. However, traditional 3D reconstruction with HSI is
difficult due to technological limitations of hyperspectral cameras. In recent
years, Neural Radiance Fields (NeRFs) have seen widespread success in creating
high quality volumetric 3D representations of scenes captured by a variety of
camera models. Leveraging recent advances in NeRFs, we propose computing a
hyperspectral 3D reconstruction in which every point in space and view
direction is characterized by wavelength-dependent radiance and transmittance
spectra. To evaluate our approach, a dataset containing nearly 2000
hyperspectral images across 8 scenes and 2 cameras was collected. We perform
comparisons against traditional RGB NeRF baselines and apply ablation testing
with alternative spectra representations. Finally, we demonstrate the potential
of hyperspectral NeRFs for hyperspectral super-resolution and imaging sensor
simulation. We show that our hyperspectral NeRF approach enables creating fast,
accurate volumetric 3D hyperspectral scenes and enables several new
applications and areas for future study.";Gerry Chen<author:sep>Sunil Kumar Narayanan<author:sep>Thomas Gautier Ottou<author:sep>Benjamin Missaoui<author:sep>Harsh Muriki<author:sep>Cédric Pradalier<author:sep>Yongsheng Chen;http://arxiv.org/pdf/2403.14839v1;cs.CV;"Main paper: 15 pages + 2 pages references. Supplemental/Appendix: 6
  pages";nerf
2403.14619v1;http://arxiv.org/abs/2403.14619v1;2024-03-21;ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D  Decomposition;"3D decomposition/segmentation still remains a challenge as large-scale 3D
annotated data is not readily available. Contemporary approaches typically
leverage 2D machine-generated segments, integrating them for 3D consistency.
While the majority of these methods are based on NeRFs, they face a potential
weakness that the instance/semantic embedding features derive from independent
MLPs, thus preventing the segmentation network from learning the geometric
details of the objects directly through radiance and density. In this paper, we
propose ClusteringSDF, a novel approach to achieve both segmentation and
reconstruction in 3D via the neural implicit surface representation,
specifically Signal Distance Function (SDF), where the segmentation rendering
is directly integrated with the volume rendering of neural implicit surfaces.
Although based on ObjectSDF++, ClusteringSDF no longer requires the
ground-truth segments for supervision while maintaining the capability of
reconstructing individual object surfaces, but purely with the noisy and
inconsistent labels from pre-trained models.As the core of ClusteringSDF, we
introduce a high-efficient clustering mechanism for lifting the 2D labels to 3D
and the experimental results on the challenging scenes from ScanNet and Replica
datasets show that ClusteringSDF can achieve competitive performance compared
against the state-of-the-art with significantly reduced training time.";Tianhao Wu<author:sep>Chuanxia Zheng<author:sep>Tat-Jen Cham<author:sep>Qianyi Wu;http://arxiv.org/pdf/2403.14619v1;cs.CV;Project Page: https://sm0kywu.github.io/ClusteringSDF/;nerf
2403.14376v1;http://arxiv.org/abs/2403.14376v1;2024-03-21;InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space  Complexity;"The conventional mesh-based Level of Detail (LoD) technique, exemplified by
applications such as Google Earth and many game engines, exhibits the
capability to holistically represent a large scene even the Earth, and achieves
rendering with a space complexity of O(log n). This constrained data
requirement not only enhances rendering efficiency but also facilitates dynamic
data fetching, thereby enabling a seamless 3D navigation experience for users.
In this work, we extend this proven LoD technique to Neural Radiance Fields
(NeRF) by introducing an octree structure to represent the scenes in different
scales. This innovative approach provides a mathematically simple and elegant
representation with a rendering space complexity of O(log n), aligned with the
efficiency of mesh-based LoD techniques. We also present a novel training
strategy that maintains a complexity of O(n). This strategy allows for parallel
training with minimal overhead, ensuring the scalability and efficiency of our
proposed method. Our contribution is not only in extending the capabilities of
existing techniques but also in establishing a foundation for scalable and
efficient large-scale scene representation using NeRF and octree structures.";Jiabin Liang<author:sep>Lanqing Zhang<author:sep>Zhuoran Zhao<author:sep>Xiangyu Xu;http://arxiv.org/pdf/2403.14376v1;cs.CV;;nerf
2403.14244v1;http://arxiv.org/abs/2403.14244v1;2024-03-21;Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering;"The 3D Gaussian splatting method has drawn a lot of attention, thanks to its
high performance in training and high quality of the rendered image. However,
it uses anisotropic Gaussian kernels to represent the scene. Although such
anisotropic kernels have advantages in representing the geometry, they lead to
difficulties in terms of computation, such as splitting or merging two kernels.
In this paper, we propose to use isotropic Gaussian kernels to avoid such
difficulties in the computation, leading to a higher performance method. The
experiments confirm that the proposed method is about {\bf 100X} faster without
losing the geometry representation accuracy. The proposed method can be applied
in a large range applications where the radiance field is needed, such as 3D
reconstruction, view synthesis, and dynamic object modeling.";Yuanhao Gong<author:sep>Lantao Yu<author:sep>Guanghui Yue;http://arxiv.org/pdf/2403.14244v1;cs.CV;;gaussian splatting
2403.14053v1;http://arxiv.org/abs/2403.14053v1;2024-03-21;Leveraging Thermal Modality to Enhance Reconstruction in Low-Light  Conditions;"Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view
synthesis by learning the implicit volumetric representation of a scene from
multi-view images, which faithfully convey the colorimetric information.
However, sensor noises will contaminate low-value pixel signals, and the lossy
camera image signal processor will further remove near-zero intensities in
extremely dark situations, deteriorating the synthesis performance. Existing
approaches reconstruct low-light scenes from raw images but struggle to recover
texture and boundary details in dark regions. Additionally, they are unsuitable
for high-speed models relying on explicit representations. To address these
issues, we present Thermal-NeRF, which takes thermal and visible raw images as
inputs, considering the thermal camera is robust to the illumination variation
and raw images preserve any possible clues in the dark, to accomplish visible
and thermal view synthesis simultaneously. Also, the first multi-view thermal
and visible dataset (MVTV) is established to support the research on multimodal
NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and
noise smoothing and provides better synthesis performance than previous work.
Finally, we demonstrate that both modalities are beneficial to each other in 3D
reconstruction.";Jiacong Xu<author:sep>Mingqian Liao<author:sep>K Ram Prabhakar<author:sep>Vishal M. Patel;http://arxiv.org/pdf/2403.14053v1;cs.CV;25 pages, 13 figures;nerf
2403.14370v2;http://arxiv.org/abs/2403.14370v2;2024-03-21;SyncTweedies: A General Generative Framework Based on Synchronized  Diffusions;"We introduce a general framework for generating diverse visual content,
including ambiguous images, panorama images, mesh textures, and Gaussian splat
textures, by synchronizing multiple diffusion processes. We present exhaustive
investigation into all possible scenarios for synchronizing multiple diffusion
processes through a canonical space and analyze their characteristics across
applications. In doing so, we reveal a previously unexplored case: averaging
the outputs of Tweedie's formula while conducting denoising in multiple
instance spaces. This case also provides the best quality with the widest
applicability to downstream tasks. We name this case SyncTweedies. In our
experiments generating visual content aforementioned, we demonstrate the
superior quality of generation by SyncTweedies compared to other
synchronization methods, optimization-based and iterative-update-based methods.";Jaihoon Kim<author:sep>Juil Koo<author:sep>Kyeongmin Yeo<author:sep>Minhyuk Sung;http://arxiv.org/pdf/2403.14370v2;cs.CV;Project page: https://synctweedies.github.io/;
2403.14412v1;http://arxiv.org/abs/2403.14412v1;2024-03-21;CombiNeRF: A Combination of Regularization Techniques for Few-Shot  Neural Radiance Field View Synthesis;"Neural Radiance Fields (NeRFs) have shown impressive results for novel view
synthesis when a sufficiently large amount of views are available. When dealing
with few-shot settings, i.e. with a small set of input views, the training
could overfit those views, leading to artifacts and geometric and chromatic
inconsistencies in the resulting rendering. Regularization is a valid solution
that helps NeRF generalization. On the other hand, each of the most recent NeRF
regularization techniques aim to mitigate a specific rendering problem.
Starting from this observation, in this paper we propose CombiNeRF, a framework
that synergically combines several regularization techniques, some of them
novel, in order to unify the benefits of each. In particular, we regularize
single and neighboring rays distributions and we add a smoothness term to
regularize near geometries. After these geometric approaches, we propose to
exploit Lipschitz regularization to both NeRF density and color networks and to
use encoding masks for input features regularization. We show that CombiNeRF
outperforms the state-of-the-art methods with few-shot settings in several
publicly available datasets. We also present an ablation study on the LLFF and
NeRF-Synthetic datasets that support the choices made. We release with this
paper the open-source implementation of our framework.";Matteo Bonotto<author:sep>Luigi Sarrocco<author:sep>Daniele Evangelista<author:sep>Marco Imperoli<author:sep>Alberto Pretto;http://arxiv.org/pdf/2403.14412v1;cs.CV;"This paper has been accepted for publication at the 2024
  International Conference on 3D Vision (3DV)";nerf
2403.14554v1;http://arxiv.org/abs/2403.14554v1;2024-03-21;Gaussian Frosting: Editable Complex Radiance Fields with Real-Time  Rendering;"We propose Gaussian Frosting, a novel mesh-based representation for
high-quality rendering and editing of complex 3D effects in real-time. Our
approach builds on the recent 3D Gaussian Splatting framework, which optimizes
a set of 3D Gaussians to approximate a radiance field from images. We propose
first extracting a base mesh from Gaussians during optimization, then building
and refining an adaptive layer of Gaussians with a variable thickness around
the mesh to better capture the fine details and volumetric effects near the
surface, such as hair or grass. We call this layer Gaussian Frosting, as it
resembles a coating of frosting on a cake. The fuzzier the material, the
thicker the frosting. We also introduce a parameterization of the Gaussians to
enforce them to stay inside the frosting layer and automatically adjust their
parameters when deforming, rescaling, editing or animating the mesh. Our
representation allows for efficient rendering using Gaussian splatting, as well
as editing and animation by modifying the base mesh. We demonstrate the
effectiveness of our method on various synthetic and real scenes, and show that
it outperforms existing surface-based approaches. We will release our code and
a web-based viewer as additional contributions. Our project page is the
following: https://anttwo.github.io/frosting/";Antoine Guédon<author:sep>Vincent Lepetit;http://arxiv.org/pdf/2403.14554v1;cs.CV;Project Webpage: https://anttwo.github.io/frosting/;gaussian splatting
2403.13348v1;http://arxiv.org/abs/2403.13348v1;2024-03-20;MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with  Wireless Coordination;"This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework
that leverages wireless signal-based coordination between robots and Neural
Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D
reconstruction, including inter-robot pose estimation, localization uncertainty
quantification, and active best-next-view selection. We introduce a method for
using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate
relative poses between robots, as well as quantifying and incorporating the
uncertainty embedded in the wireless localization of these pose estimates into
the NeRF training loss to mitigate the impact of inaccurate camera poses.
Furthermore, we propose an active view selection approach that accounts for
robot pose uncertainty when determining the next-best views to improve the 3D
reconstruction, enabling faster convergence through intelligent view selection.
Extensive experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our framework in theory and in practice. Leveraging wireless
coordination and localization uncertainty-aware training, MULAN-WC can achieve
high-quality 3d reconstruction which is close to applying the ground truth
camera poses. Furthermore, the quantification of the information gain from a
novel view enables consistent rendering quality improvement with incrementally
captured images by commending the robot the novel view position. Our hardware
experiments showcase the practicality of deploying MULAN-WC to real robotic
systems.";Weiying Wang<author:sep>Victor Cai<author:sep>Stephanie Gil;http://arxiv.org/pdf/2403.13348v1;cs.RO;;nerf
2403.13806v1;http://arxiv.org/abs/2403.13806v1;2024-03-20;RadSplat: Radiance Field-Informed Gaussian Splatting for Robust  Real-Time Rendering with 900+ FPS;"Recent advances in view synthesis and real-time rendering have achieved
photorealistic quality at impressive rendering speeds. While Radiance
Field-based methods achieve state-of-the-art quality in challenging scenarios
such as in-the-wild captures and large-scale scenes, they often suffer from
excessively high compute requirements linked to volumetric rendering. Gaussian
Splatting-based methods, on the other hand, rely on rasterization and naturally
achieve real-time rendering but suffer from brittle optimization heuristics
that underperform on more challenging scenes. In this work, we present
RadSplat, a lightweight method for robust real-time rendering of complex
scenes. Our main contributions are threefold. First, we use radiance fields as
a prior and supervision signal for optimizing point-based scene
representations, leading to improved quality and more robust optimization.
Next, we develop a novel pruning technique reducing the overall point count
while maintaining high quality, leading to smaller and more compact scene
representations with faster inference speeds. Finally, we propose a novel
test-time filtering approach that further accelerates rendering and allows to
scale to larger, house-sized scenes. We find that our method enables
state-of-the-art synthesis of complex captures at 900+ FPS.";Michael Niemeyer<author:sep>Fabian Manhardt<author:sep>Marie-Julie Rakotosaona<author:sep>Michael Oechsle<author:sep>Daniel Duckworth<author:sep>Rama Gosula<author:sep>Keisuke Tateno<author:sep>John Bates<author:sep>Dominik Kaeser<author:sep>Federico Tombari;http://arxiv.org/pdf/2403.13806v1;cs.CV;Project page at https://m-niemeyer.github.io/radsplat/;gaussian splatting
2403.13327v1;http://arxiv.org/abs/2403.13327v1;2024-03-20;Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation  for Natural Camera Motion;"High-quality scene reconstruction and novel view synthesis based on Gaussian
Splatting (3DGS) typically require steady, high-quality photographs, often
impractical to capture with handheld cameras. We present a method that adapts
to camera motion and allows high-quality scene reconstruction with handheld
video data suffering from motion blur and rolling shutter distortion. Our
approach is based on detailed modelling of the physical image formation process
and utilizes velocities estimated using visual-inertial odometry (VIO). Camera
poses are considered non-static during the exposure time of a single image
frame and camera poses are further optimized in the reconstruction process. We
formulate a differentiable rendering pipeline that leverages screen space
approximation to efficiently incorporate rolling-shutter and motion blur
effects into the 3DGS framework. Our results with both synthetic and real data
demonstrate superior performance in mitigating camera motion over existing
methods, thereby advancing 3DGS in naturalistic settings.";Otto Seiskari<author:sep>Jerry Ylilammi<author:sep>Valtteri Kaatrasalo<author:sep>Pekka Rantalankila<author:sep>Matias Turkulainen<author:sep>Juho Kannala<author:sep>Esa Rahtu<author:sep>Arno Solin;http://arxiv.org/pdf/2403.13327v1;cs.CV;Source code available at https://github.com/SpectacularAI/3dgs-deblur;gaussian splatting
2403.12365v1;http://arxiv.org/abs/2403.12365v1;2024-03-19;GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation;"Creating 4D fields of Gaussian Splatting from images or videos is a
challenging task due to its under-constrained nature. While the optimization
can draw photometric reference from the input videos or be regulated by
generative models, directly supervising Gaussian motions remains underexplored.
In this paper, we introduce a novel concept, Gaussian flow, which connects the
dynamics of 3D Gaussians and pixel velocities between consecutive frames. The
Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into
the image space. This differentiable process enables direct dynamic supervision
from optical flow. Our method significantly benefits 4D dynamic content
generation and 4D novel view synthesis with Gaussian Splatting, especially for
contents with rich motions that are hard to be handled by existing methods. The
common color drifting issue that happens in 4D generation is also resolved with
improved Guassian dynamics. Superior visual quality on extensive experiments
demonstrates our method's effectiveness. Quantitative and qualitative
evaluations show that our method achieves state-of-the-art results on both
tasks of 4D generation and 4D novel view synthesis. Project page:
https://zerg-overmind.github.io/GaussianFlow.github.io/";Quankai Gao<author:sep>Qiangeng Xu<author:sep>Zhe Cao<author:sep>Ben Mildenhall<author:sep>Wenchao Ma<author:sep>Le Chen<author:sep>Danhang Tang<author:sep>Ulrich Neumann;http://arxiv.org/pdf/2403.12365v1;cs.CV;;gaussian splatting
2403.12957v1;http://arxiv.org/abs/2403.12957v1;2024-03-19;GVGEN: Text-to-3D Generation with Volumetric Representation;"In recent years, 3D Gaussian splatting has emerged as a powerful technique
for 3D reconstruction and generation, known for its fast and high-quality
rendering capabilities. To address these shortcomings, this paper introduces a
novel diffusion-based framework, GVGEN, designed to efficiently generate 3D
Gaussian representations from text input. We propose two innovative
techniques:(1) Structured Volumetric Representation. We first arrange
disorganized 3D Gaussian points as a structured form GaussianVolume. This
transformation allows the capture of intricate texture details within a volume
composed of a fixed number of Gaussians. To better optimize the representation
of these details, we propose a unique pruning and densifying method named the
Candidate Pool Strategy, enhancing detail fidelity through selective
optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the
generation of GaussianVolume and empower the model to generate instances with
detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially
constructs a basic geometric structure, followed by the prediction of complete
Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in
qualitative and quantitative assessments compared to existing 3D generation
methods. Simultaneously, it maintains a fast generation speed ($\sim$7
seconds), effectively striking a balance between quality and efficiency.";Xianglong He<author:sep>Junyi Chen<author:sep>Sida Peng<author:sep>Di Huang<author:sep>Yangguang Li<author:sep>Xiaoshui Huang<author:sep>Chun Yuan<author:sep>Wanli Ouyang<author:sep>Tong He;http://arxiv.org/pdf/2403.12957v1;cs.CV;project page: https://gvgen.github.io/;gaussian splatting
2403.12682v1;http://arxiv.org/abs/2403.12682v1;2024-03-19;IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single  image and a NeRF model;"We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera
pose of a given image, building on the Neural Radiance Fields (NeRF)
formulation. IFFNeRF is specifically designed to operate in real-time and
eliminates the need for an initial pose guess that is proximate to the sought
solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface
points from within the NeRF model. From these sampled points, we cast rays and
deduce the color for each ray through pixel-level view synthesis. The camera
pose can then be estimated as the solution to a Least Squares problem by
selecting correspondences between the query image and the resulting bundle. We
facilitate this process through a learned attention mechanism, bridging the
query image embedding with the embedding of parameterized rays, thereby
matching rays pertinent to the image. Through synthetic and real evaluation
settings, we show that our method can improve the angular and translation error
accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing
at 34fps on consumer hardware and not requiring the initial pose guess.";Matteo Bortolon<author:sep>Theodore Tsesmelis<author:sep>Stuart James<author:sep>Fabio Poiesi<author:sep>Alessio Del Bue;http://arxiv.org/pdf/2403.12682v1;cs.CV;"Accepted ICRA 2024, Project page:
  https://mbortolon97.github.io/iffnerf/";nerf
2403.12839v1;http://arxiv.org/abs/2403.12839v1;2024-03-19;Global-guided Focal Neural Radiance Field for Large-scale Scene  Rendering;"Neural radiance fields~(NeRF) have recently been applied to render
large-scale scenes. However, their limited model capacity typically results in
blurred rendering results. Existing large-scale NeRFs primarily address this
limitation by partitioning the scene into blocks, which are subsequently
handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and
processed independently, lead to inconsistencies in geometry and appearance
across the scene. Consequently, the rendering quality fails to exhibit
significant improvement despite the expansion of model capacity. In this work,
we present global-guided focal neural radiance field (GF-NeRF) that achieves
high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a
two-stage (Global and Focal) architecture and a global-guided training
strategy. The global stage obtains a continuous representation of the entire
scene while the focal stage decomposes the scene into multiple blocks and
further processes them with distinct sub-encoders. Leveraging this two-stage
architecture, sub-encoders only need fine-tuning based on the global encoder,
thus reducing training complexity in the focal stage while maintaining
scene-wide consistency. Spatial information and error information from the
global stage also benefit the sub-encoders to focus on crucial areas and
effectively capture more details of large-scale scenes. Notably, our approach
does not rely on any prior knowledge about the target scene, attributing
GF-NeRF adaptable to various large-scale scene types, including street-view and
aerial-view scenes. We demonstrate that our method achieves high-fidelity,
natural rendering results on various types of large-scale datasets. Our project
page: https://shaomq2187.github.io/GF-NeRF/";Mingqi Shao<author:sep>Feng Xiong<author:sep>Hang Zhang<author:sep>Shuang Yang<author:sep>Mu Xu<author:sep>Wei Bian<author:sep>Xueqian Wang;http://arxiv.org/pdf/2403.12839v1;cs.CV;;nerf
2403.13206v1;http://arxiv.org/abs/2403.13206v1;2024-03-19;Depth-guided NeRF Training via Earth Mover's Distance;"Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of
predicted viewpoints. However, the photometric loss often does not provide
enough information to disambiguate between different possible geometries
yielding the same image. Previous work has thus incorporated depth supervision
during NeRF training, leveraging dense predictions from pre-trained depth
networks as pseudo-ground truth. While these depth priors are assumed to be
perfect once filtered for noise, in practice, their accuracy is more
challenging to capture. This work proposes a novel approach to uncertainty in
depth priors for NeRF supervision. Instead of using custom-trained depth or
uncertainty priors, we use off-the-shelf pretrained diffusion models to predict
depth and capture uncertainty during the denoising process. Because we know
that depth priors are prone to errors, we propose to supervise the ray
termination distance distribution with Earth Mover's Distance instead of
enforcing the rendered depth to replicate the depth prior exactly through
L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth
metrics by a large margin while maintaining performance on photometric
measures.";Anita Rau<author:sep>Josiah Aklilu<author:sep>F. Christopher Holsinger<author:sep>Serena Yeung-Levy;http://arxiv.org/pdf/2403.13206v1;cs.CV;Preprint. Under review;nerf
2403.12535v1;http://arxiv.org/abs/2403.12535v1;2024-03-19;High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided  Densification and Regularized Optimization;"We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that
provides metrically accurate pose tracking and visually realistic
reconstruction. To this end, we first propose a Gaussian densification strategy
based on the rendering loss to map unobserved areas and refine reobserved
areas. Second, we introduce extra regularization parameters to alleviate the
forgetting problem in the continuous mapping problem, where parameters tend to
overfit the latest frame and result in decreasing rendering quality for
previous frames. Both mapping and tracking are performed with Gaussian
parameters by minimizing re-rendering loss in a differentiable way. Compared to
recent neural and concurrently developed gaussian splatting RGBD SLAM
baselines, our method achieves state-of-the-art results on the synthetic
dataset Replica and competitive results on the real-world dataset TUM.";Shuo Sun<author:sep>Malcolm Mielle<author:sep>Achim J. Lilienthal<author:sep>Martin Magnusson;http://arxiv.org/pdf/2403.12535v1;cs.RO;submitted to IROS24;gaussian splatting
2403.12800v1;http://arxiv.org/abs/2403.12800v1;2024-03-19;Learning Neural Volumetric Pose Features for Camera Localization;"We introduce a novel neural volumetric pose feature, termed PoseMap, designed
to enhance camera localization by encapsulating the information between images
and the associated camera poses. Our framework leverages an Absolute Pose
Regression (APR) architecture, together with an augmented NeRF module. This
integration not only facilitates the generation of novel views to enrich the
training dataset but also enables the learning of effective pose features.
Additionally, we extend our architecture for self-supervised online alignment,
allowing our method to be used and fine-tuned for unlabelled images within a
unified framework. Experiments demonstrate that our method achieves 14.28% and
20.51% performance gain on average in indoor and outdoor benchmark scenes,
outperforming existing APR methods with state-of-the-art accuracy.";Jingyu Lin<author:sep>Jiaqi Gu<author:sep>Bojian Wu<author:sep>Lubin Fan<author:sep>Renjie Chen<author:sep>Ligang Liu<author:sep>Jieping Ye;http://arxiv.org/pdf/2403.12800v1;cs.CV;14 pages, 9 figures;nerf
2403.13199v1;http://arxiv.org/abs/2403.13199v1;2024-03-19;DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced  Images;"Neural radiance fields (NeRFs) show potential for transforming images
captured worldwide into immersive 3D visual experiences. However, most of this
captured visual data remains siloed in our camera rolls as these images contain
personal details. Even if made public, the problem of learning 3D
representations of billions of scenes captured daily in a centralized manner is
computationally intractable. Our approach, DecentNeRF, is the first attempt at
decentralized, crowd-sourced NeRFs that require $\sim 10^4\times$ less server
computing for a scene than a centralized approach. Instead of sending the raw
data, our approach requires users to send a 3D representation, distributing the
high computation cost of training centralized NeRFs between the users. It
learns photorealistic scene representations by decomposing users' 3D views into
personal and global NeRFs and a novel optimally weighted aggregation of only
the latter. We validate the advantage of our approach to learn NeRFs with
photorealism and minimal server computation cost on structured synthetic and
real-world photo tourism datasets. We further analyze how secure aggregation of
global NeRFs in DecentNeRF minimizes the undesired reconstruction of personal
content by the server.";Zaid Tasneem<author:sep>Akshat Dave<author:sep>Abhishek Singh<author:sep>Kushagra Tiwary<author:sep>Praneeth Vepakomma<author:sep>Ashok Veeraraghavan<author:sep>Ramesh Raskar;http://arxiv.org/pdf/2403.13199v1;cs.CV;;nerf
2403.12722v1;http://arxiv.org/abs/2403.12722v1;2024-03-19;HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting;"Holistic understanding of urban scenes based on RGB images is a challenging
yet important problem. It encompasses understanding both the geometry and
appearance to enable novel view synthesis, parsing semantic labels, and
tracking moving objects. Despite considerable progress, existing approaches
often focus on specific aspects of this task and require additional inputs such
as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we
introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic
urban scene understanding. Our main idea involves the joint optimization of
geometry, appearance, semantics, and motion using a combination of static and
dynamic 3D Gaussians, where moving object poses are regularized via physical
constraints. Our approach offers the ability to render new viewpoints in
real-time, yielding 2D and 3D semantic information with high accuracy, and
reconstruct dynamic scenes, even in scenarios where 3D bounding box detection
are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2
demonstrate the effectiveness of our approach.";Hongyu Zhou<author:sep>Jiahao Shao<author:sep>Lu Xu<author:sep>Dongfeng Bai<author:sep>Weichao Qiu<author:sep>Bingbing Liu<author:sep>Yue Wang<author:sep>Andreas Geiger<author:sep>Yiyi Liao;http://arxiv.org/pdf/2403.12722v1;cs.CV;Our project page is at https://xdimlab.github.io/hugs_website;gaussian splatting
2403.12550v2;http://arxiv.org/abs/2403.12550v2;2024-03-19;RGBD GS-ICP SLAM;"Simultaneous Localization and Mapping (SLAM) with dense representation plays
a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)
applications. Recent advancements in dense representation SLAM have highlighted
the potential of leveraging neural scene representation and 3D Gaussian
representation for high-fidelity spatial representation. In this paper, we
propose a novel dense representation SLAM approach with a fusion of Generalized
Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast
to existing methods, we utilize a single Gaussian map for both tracking and
mapping, resulting in mutual benefits. Through the exchange of covariances
between tracking and mapping processes with scale alignment techniques, we
minimize redundant computations and achieve an efficient system. Additionally,
we enhance tracking accuracy and mapping quality through our keyframe selection
methods. Experimental results demonstrate the effectiveness of our approach,
showing an incredibly fast speed up to 107 FPS (for the entire system) and
superior quality of the reconstructed map.";Seongbo Ha<author:sep>Jiung Yeon<author:sep>Hyeonwoo Yu;http://arxiv.org/pdf/2403.12550v2;cs.CV;;gaussian splatting
2403.11865v1;http://arxiv.org/abs/2403.11865v1;2024-03-18;Exploring Multi-modal Neural Scene Representations With Applications on  Thermal Imaging;"Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard
for the task of novel view synthesis when trained on a set of RGB images. In
this paper, we conduct a comprehensive evaluation of neural scene
representations, such as NeRFs, in the context of multi-modal learning.
Specifically, we present four different strategies of how to incorporate a
second modality, other than RGB, into NeRFs: (1) training from scratch
independently on both modalities; (2) pre-training on RGB and fine-tuning on
the second modality; (3) adding a second branch; and (4) adding a separate
component to predict (color) values of the additional modality. We chose
thermal imaging as second modality since it strongly differs from RGB in terms
of radiosity, making it challenging to integrate into neural scene
representations. For the evaluation of the proposed strategies, we captured a
new publicly available multi-view dataset, ThermalMix, consisting of six common
objects and about 360 RGB and thermal images in total. We employ cross-modality
calibration prior to data capturing, leading to high-quality alignments between
RGB and thermal images. Our findings reveal that adding a second branch to NeRF
performs best for novel view synthesis on thermal images while also yielding
compelling results on RGB. Finally, we also show that our analysis generalizes
to other modalities, including near-infrared images and depth maps. Project
page: https://mert-o.github.io/ThermalNeRF/.";Mert Özer<author:sep>Maximilian Weiherer<author:sep>Martin Hundhausen<author:sep>Bernhard Egger;http://arxiv.org/pdf/2403.11865v1;cs.CV;24 pages, 14 figures;nerf
2403.12154v1;http://arxiv.org/abs/2403.12154v1;2024-03-18;ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View  Synthesis;"Thermal scene reconstruction exhibit great potential for applications across
a broad spectrum of fields, including building energy consumption analysis and
non-destructive testing. However, existing methods typically require dense
scene measurements and often rely on RGB images for 3D geometry reconstruction,
with thermal information being projected post-reconstruction. This two-step
strategy, adopted due to the lack of texture in thermal images, can lead to
disparities between the geometry and temperatures of the reconstructed objects
and those of the actual scene. To address this challenge, we propose
ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields,
capable of rendering new RGB and thermal views of a scene jointly. To overcome
the lack of texture in thermal images, we use paired RGB and thermal images to
learn scene density, while distinct networks estimate color and temperature
information. Furthermore, we introduce ThermoScenes, a new dataset to palliate
the lack of available RGB+thermal datasets for scene reconstruction.
Experimental results validate that ThermoNeRF achieves accurate thermal image
synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement
of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a
state-of-the-art NeRF method.";Mariam Hassan<author:sep>Florent Forest<author:sep>Olga Fink<author:sep>Malcolm Mielle;http://arxiv.org/pdf/2403.12154v1;cs.CV;;nerf
2403.11776v1;http://arxiv.org/abs/2403.11776v1;2024-03-18;DVN-SLAM: Dynamic Visual Neural SLAM Based on Local-Global Encoding;"Recent research on Simultaneous Localization and Mapping (SLAM) based on
implicit representation has shown promising results in indoor environments.
However, there are still some challenges: the limited scene representation
capability of implicit encodings, the uncertainty in the rendering process from
implicit representations, and the disruption of consistency by dynamic objects.
To address these challenges, we propose a real-time dynamic visual SLAM system
based on local-global fusion neural implicit representation, named DVN-SLAM. To
improve the scene representation capability, we introduce a local-global fusion
neural implicit representation that enables the construction of an implicit map
while considering both global structure and local details. To tackle
uncertainties arising from the rendering process, we design an information
concentration loss for optimization, aiming to concentrate scene information on
object surfaces. The proposed DVN-SLAM achieves competitive performance in
localization and mapping across multiple datasets. More importantly, DVN-SLAM
demonstrates robustness in dynamic scenes, a trait that sets it apart from
other NeRF-based methods.";Wenhua Wu<author:sep>Guangming Wang<author:sep>Ting Deng<author:sep>Sebastian Aegidius<author:sep>Stuart Shanks<author:sep>Valerio Modugno<author:sep>Dimitrios Kanoulas<author:sep>Hesheng Wang;http://arxiv.org/pdf/2403.11776v1;cs.CV;;nerf
2403.11868v2;http://arxiv.org/abs/2403.11868v2;2024-03-18;View-Consistent 3D Editing with Gaussian Splatting;"The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,
offering efficient, high-fidelity rendering and enabling precise local
manipulations. Currently, diffusion-based 2D editing models are harnessed to
modify multi-view rendered images, which then guide the editing of 3DGS models.
However, this approach faces a critical issue of multi-view inconsistency,
where the guidance images exhibit significant discrepancies across views,
leading to mode collapse and visual artifacts of 3DGS. To this end, we
introduce View-consistent Editing (VcEdit), a novel framework that seamlessly
incorporates 3DGS into image editing processes, ensuring multi-view consistency
in edited guidance images and effectively mitigating mode collapse issues.
VcEdit employs two innovative consistency modules: the Cross-attention
Consistency Module and the Editing Consistency Module, both designed to reduce
inconsistencies in edited images. By incorporating these consistency modules
into an iterative pattern, VcEdit proficiently resolves the issue of multi-view
inconsistency, facilitating high-quality 3DGS editing across a diverse range of
scenes.";Yuxuan Wang<author:sep>Xuanyu Yi<author:sep>Zike Wu<author:sep>Na Zhao<author:sep>Long Chen<author:sep>Hanwang Zhang;http://arxiv.org/pdf/2403.11868v2;cs.GR;;gaussian splatting
2403.11812v1;http://arxiv.org/abs/2403.11812v1;2024-03-18;Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from  Aerial Imagery;"We present a neural radiance field method for urban-scale semantic and
building-level instance segmentation from aerial images by lifting noisy 2D
labels to 3D. This is a challenging problem due to two primary reasons.
Firstly, objects in urban aerial images exhibit substantial variations in size,
including buildings, cars, and roads, which pose a significant challenge for
accurate 2D segmentation. Secondly, the 2D labels generated by existing
segmentation methods suffer from the multi-view inconsistency problem,
especially in the case of aerial images, where each image captures only a small
portion of the entire scene. To overcome these limitations, we first introduce
a scale-adaptive semantic label fusion strategy that enhances the segmentation
of objects of varying sizes by combining labels predicted from different
altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then
introduce a novel cross-view instance label grouping strategy based on the 3D
scene representation to mitigate the multi-view inconsistency problem in the 2D
instance labels. Furthermore, we exploit multi-view reconstructed depth priors
to improve the geometric quality of the reconstructed radiance field, resulting
in enhanced segmentation results. Experiments on multiple real-world
urban-scale datasets demonstrate that our approach outperforms existing
methods, highlighting its effectiveness.";Yuqi Zhang<author:sep>Guanying Chen<author:sep>Jiaxing Chen<author:sep>Shuguang Cui;http://arxiv.org/pdf/2403.11812v1;cs.CV;CVPR 2024: https://zyqz97.github.io/Aerial_Lifting/;nerf
2403.11899v1;http://arxiv.org/abs/2403.11899v1;2024-03-18;GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with  Noisy Polarization Priors;"Learning surfaces from neural radiance field (NeRF) became a rising topic in
Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods
demonstrated their ability to reconstruct accurate 3D shapes of Lambertian
scenes. However, their results on reflective scenes are unsatisfactory due to
the entanglement of specular radiance and complicated geometry. To address the
challenges, we propose a Gaussian-based representation of normals in SDF
fields. Supervised by polarization priors, this representation guides the
learning of geometry behind the specular reflection and captures more details
than existing methods. Moreover, we propose a reweighting strategy in the
optimization process to alleviate the noise issue of polarization priors. To
validate the effectiveness of our design, we capture polarimetric information,
and ground truth meshes in additional reflective scenes with various geometry.
We also evaluated our framework on the PANDORA dataset. Comparisons prove our
method outperforms existing neural 3D reconstruction methods in reflective
scenes by a large margin.";LI Yang<author:sep>WU Ruizheng<author:sep>LI Jiyong<author:sep>CHEN Ying-cong;http://arxiv.org/pdf/2403.11899v1;cs.CV;"Accepted to ICLR 2024 Poster. For the Appendix, please see
  http://yukiumi13.github.io/gnerp_page";nerf
2403.11573v2;http://arxiv.org/abs/2403.11573v2;2024-03-18;Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for  Resolving Class-imbalance Problem;"Typical LiDAR-based 3D object detection models are trained in a supervised
manner with real-world data collection, which is often imbalanced over classes
(or long-tailed). To deal with it, augmenting minority-class examples by
sampling ground truth (GT) LiDAR points from a database and pasting them into a
scene of interest is often used, but challenges still remain: inflexibility in
locating GT samples and limited sample diversity. In this work, we propose to
leverage pseudo-LiDAR point clouds generated (at a low cost) from videos
capturing a surround view of miniatures or real-world objects of minor classes.
Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of
three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D
view synthesis model, (ii) object-level domain alignment with LiDAR intensity
estimation and (iii) a hybrid context-aware placement method from ground and
map information. We demonstrate the superiority and generality of our method
through performance improvements in extensive experiments conducted on three
popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the
datasets with large domain gaps captured by different LiDAR configurations. Our
code and data will be publicly available upon publication.";Mincheol Chang<author:sep>Siyeong Lee<author:sep>Jinkyu Kim<author:sep>Namil Kim;http://arxiv.org/pdf/2403.11573v2;cs.CV;28 pages, 12 figures, 11 tables;nerf
2403.11427v1;http://arxiv.org/abs/2403.11427v1;2024-03-18;BAGS: Building Animatable Gaussian Splatting from a Monocular Video with  Diffusion Priors;"Animatable 3D reconstruction has significant applications across various
fields, primarily relying on artists' handcraft creation. Recently, some
studies have successfully constructed animatable 3D models from monocular
videos. However, these approaches require sufficient view coverage of the
object within the input video and typically necessitate significant time and
computational costs for training and rendering. This limitation restricts the
practical applications. In this work, we propose a method to build animatable
3D Gaussian Splatting from monocular video with diffusion priors. The 3D
Gaussian representations significantly accelerate the training and rendering
process, and the diffusion priors allow the method to learn 3D models with
limited viewpoints. We also present the rigid regularization to enhance the
utilization of the priors. We perform an extensive evaluation across various
real-world videos, demonstrating its superior performance compared to the
current state-of-the-art methods.";Tingyang Zhang<author:sep>Qingzhe Gao<author:sep>Weiyu Li<author:sep>Libin Liu<author:sep>Baoquan Chen;http://arxiv.org/pdf/2403.11427v1;cs.CV;https://talegqz.github.io/BAGS/;gaussian splatting
2403.11577v1;http://arxiv.org/abs/2403.11577v1;2024-03-18;3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal  Calibration;"Reliable multimodal sensor fusion algorithms require accurate spatiotemporal
calibration. Recently, targetless calibration techniques based on implicit
neural representations have proven to provide precise and robust results.
Nevertheless, such methods are inherently slow to train given the high
computational overhead caused by the large number of sampled points required
for volume rendering. With the recent introduction of 3D Gaussian Splatting as
a faster alternative to implicit representation methods, we propose to leverage
this new rendering approach to achieve faster multi-sensor calibration. We
introduce 3DGS-Calib, a new calibration method that relies on the speed and
rendering accuracy of 3D Gaussian Splatting to achieve multimodal
spatiotemporal calibration that is accurate, robust, and with a substantial
speed-up compared to methods relying on implicit neural representations. We
demonstrate the superiority of our proposal with experimental results on
sequences from KITTI-360, a widely used driving dataset.";Quentin Herau<author:sep>Moussab Bennehar<author:sep>Arthur Moreau<author:sep>Nathan Piasco<author:sep>Luis Roldao<author:sep>Dzmitry Tsishkou<author:sep>Cyrille Migniot<author:sep>Pascal Vasseur<author:sep>Cédric Demonceaux;http://arxiv.org/pdf/2403.11577v1;cs.CV;Under review;gaussian splatting
2403.11396v1;http://arxiv.org/abs/2403.11396v1;2024-03-18;Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot  Navigation and 3D Scene Understanding with FisherRF;"This work proposes a novel approach to bolster both the robot's risk
assessment and safety measures while deepening its understanding of 3D scenes,
which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian
Splatting. To further enhance these capabilities, we incorporate additional
sampled views from the environment with the RF model. One of our key
contributions is the introduction of Risk-aware Environment Masking (RaEM),
which prioritizes crucial information by selecting the next-best-view that
maximizes the expected information gain. This targeted approach aims to
minimize uncertainties surrounding the robot's path and enhance the safety of
its navigation. Our method offers a dual benefit: improved robot safety and
increased efficiency in risk-aware 3D scene reconstruction and understanding.
Extensive experiments in real-world scenarios demonstrate the effectiveness of
our proposed approach, highlighting its potential to establish a robust and
safety-focused framework for active robot exploration and 3D scene
understanding.";Guangyi Liu<author:sep>Wen Jiang<author:sep>Boshu Lei<author:sep>Vivek Pandey<author:sep>Kostas Daniilidis<author:sep>Nader Motee;http://arxiv.org/pdf/2403.11396v1;cs.RO;;
2403.11678v1;http://arxiv.org/abs/2403.11678v1;2024-03-18;Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous  Scenes;"We present a method enabling the scaling of NeRFs to learn a large number of
semantically-similar scenes. We combine two techniques to improve the required
training time and memory cost per scene. First, we learn a 3D-aware latent
space in which we train Tri-Plane scene representations, hence reducing the
resolution at which scenes are learned. Moreover, we present a way to share
common information across scenes, hence allowing for a reduction of model
complexity to learn a particular scene. Our method reduces effective per-scene
memory costs by 44% and per-scene time costs by 86% when training 1000 scenes.
Our project page can be found at https://3da-ae.github.io .";Antoine Schnepf<author:sep>Karim Kassab<author:sep>Jean-Yves Franceschi<author:sep>Laurent Caraffa<author:sep>Flavian Vasile<author:sep>Jeremie Mary<author:sep>Andrew Comport<author:sep>Valérie Gouet-Brunet;http://arxiv.org/pdf/2403.11678v1;cs.CV;;nerf
2403.12198v1;http://arxiv.org/abs/2403.12198v1;2024-03-18;FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo  Endoscopic Videos;"Reconstruction of endoscopic scenes is an important asset for various medical
applications, from post-surgery analysis to educational training. Neural
rendering has recently shown promising results in endoscopic reconstruction
with deforming tissue. However, the setup has been restricted to a static
endoscope, limited deformation, or required an external tracking device to
retrieve camera pose information of the endoscopic camera. With FLex we adress
the challenging setup of a moving endoscope within a highly dynamic environment
of deforming tissue. We propose an implicit scene separation into multiple
overlapping 4D neural radiance fields (NeRFs) and a progressive optimization
scheme jointly optimizing for reconstruction and camera poses from scratch.
This improves the ease-of-use and allows to scale reconstruction capabilities
in time to process surgical videos of 5,000 frames and more; an improvement of
more than ten times compared to the state of the art while being agnostic to
external tracking information. Extensive evaluations on the StereoMIS dataset
show that FLex significantly improves the quality of novel view synthesis while
maintaining competitive pose accuracy.";Florian Philipp Stilz<author:sep>Mert Asim Karaoglu<author:sep>Felix Tristram<author:sep>Nassir Navab<author:sep>Benjamin Busam<author:sep>Alexander Ladikos;http://arxiv.org/pdf/2403.12198v1;cs.CV;;nerf
2403.11679v1;http://arxiv.org/abs/2403.11679v1;2024-03-18;NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using  3D Gaussian Splatting;"We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D
Gaussian representation, that enables robust 3D semantic mapping, accurate
camera tracking, and high-quality rendering in real-time. In the system, we
propose a Spatially Consistent Feature Fusion model to reduce the effect of
erroneous estimates from pre-trained segmentation head on semantic
reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we
employ a lightweight encoder-decoder to compress the high-dimensional semantic
features into a compact 3D Gaussian representation, mitigating the burden of
excessive memory consumption. Furthermore, we leverage the advantage of 3D
Gaussian splatting, which enables efficient and differentiable novel view
rendering, and propose a Virtual Camera View Pruning method to eliminate
outlier GS points, thereby effectively enhancing the quality of scene
representations. Our NEDS-SLAM method demonstrates competitive performance over
existing dense semantic SLAM methods in terms of mapping and tracking accuracy
on Replica and ScanNet datasets, while also showing excellent capabilities in
3D dense semantic mapping.";Yiming Ji<author:sep>Yang Liu<author:sep>Guanghu Xie<author:sep>Boyu Ma<author:sep>Zongwu Xie;http://arxiv.org/pdf/2403.11679v1;cs.CV;;gaussian splatting
2403.11831v2;http://arxiv.org/abs/2403.11831v2;2024-03-18;BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting;"While neural rendering has demonstrated impressive capabilities in 3D scene
reconstruction and novel view synthesis, it heavily relies on high-quality
sharp images and accurate camera poses. Numerous approaches have been proposed
to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly
encountered in real-world scenarios such as low-light or long-exposure
conditions. However, the implicit representation of NeRF struggles to
accurately recover intricate details from severely motion-blurred images and
cannot achieve real-time rendering. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time
rendering by explicitly optimizing point clouds as Gaussian spheres.
  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle
Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian
representation and handles severe motion-blurred images with inaccurate camera
poses to achieve high-quality scene reconstruction. Our method models the
physical image formation process of motion-blurred images and jointly learns
the parameters of Gaussians while recovering camera motion trajectories during
exposure time.
  In our experiments, we demonstrate that BAD-Gaussians not only achieves
superior rendering quality compared to previous state-of-the-art deblur neural
rendering methods on both synthetic and real datasets but also enables
real-time rendering capabilities.
  Our project page and source code is available at
https://lingzhezhao.github.io/BAD-Gaussians/";Lingzhe Zhao<author:sep>Peng Wang<author:sep>Peidong Liu;http://arxiv.org/pdf/2403.11831v2;cs.CV;"Project Page and Source Code:
  https://lingzhezhao.github.io/BAD-Gaussians/";gaussian splatting<tag:sep>nerf
2403.11909v1;http://arxiv.org/abs/2403.11909v1;2024-03-18;RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF;"Recent advances in neural rendering have enabled highly photorealistic 3D
scene reconstruction and novel view synthesis. Despite this progress, current
state-of-the-art methods struggle to reconstruct high frequency detail, due to
factors such as a low-frequency bias of radiance fields and inaccurate camera
calibration. One approach to mitigate this issue is to enhance images
post-rendering. 2D enhancers can be pre-trained to recover some detail but are
agnostic to scene geometry and do not easily generalize to new distributions of
image degradation. Conversely, existing 3D enhancers are able to transfer
detail from nearby training images in a generalizable manner, but suffer from
inaccurate camera calibration and can propagate errors from the geometry into
rendered images. We propose a neural rendering enhancer, RoGUENeRF, which
exploits the best of both paradigms. Our method is pre-trained to learn a
general enhancer while also leveraging information from nearby training images
via robust 3D alignment and geometry-aware fusion. Our approach restores
high-frequency textures while maintaining geometric consistency and is also
robust to inaccurate camera calibration. We show that RoGUENeRF substantially
enhances the rendering quality of a wide range of neural rendering baselines,
e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the
real world 360v2 dataset.";Sibi Catley-Chandar<author:sep>Richard Shaw<author:sep>Gregory Slabaugh<author:sep>Eduardo Perez-Pellitero;http://arxiv.org/pdf/2403.11909v1;cs.CV;;nerf
2403.11453v1;http://arxiv.org/abs/2403.11453v1;2024-03-18;Bridging 3D Gaussian and Mesh for Freeview Video Rendering;"This is only a preview version of GauMesh. Recently, primitive-based
rendering has been proven to achieve convincing results in solving the problem
of modeling and rendering the 3D dynamic scene from 2D images. Despite this, in
the context of novel view synthesis, each type of primitive has its inherent
defects in terms of representation ability. It is difficult to exploit the mesh
to depict the fuzzy geometry. Meanwhile, the point-based splatting (e.g. the 3D
Gaussian Splatting) method usually produces artifacts or blurry pixels in the
area with smooth geometry and sharp textures. As a result, it is difficult,
even not impossible, to represent the complex and dynamic scene with a single
type of primitive. To this end, we propose a novel approach, GauMesh, to bridge
the 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a
sequence of tracked mesh as initialization, our goal is to simultaneously
optimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians,
and the deformation field. At a specific time, we perform $\alpha$-blending on
the RGB and opacity values based on the merged and re-ordered z-buffers from
mesh and 3D Gaussian rasterizations. This produces the final rendering, which
is supervised by the ground-truth image. Experiments demonstrate that our
approach adapts the appropriate type of primitives to represent the different
parts of the dynamic scene and outperforms all the baseline methods in both
quantitative and qualitative comparisons without losing render speed.";Yuting Xiao<author:sep>Xuan Wang<author:sep>Jiafei Li<author:sep>Hongrui Cai<author:sep>Yanbo Fan<author:sep>Nan Xue<author:sep>Minghui Yang<author:sep>Yujun Shen<author:sep>Shenghua Gao;http://arxiv.org/pdf/2403.11453v1;cs.GR;7 pages;gaussian splatting
2403.11447v1;http://arxiv.org/abs/2403.11447v1;2024-03-18;Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene  Reconstruction;"3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene
reconstruction. However, existing methods focus mainly on extending static 3DGS
into a time-variant representation, while overlooking the rich motion
information carried by 2D observations, thus suffering from performance
degradation and model redundancy. To address the above problem, we propose a
novel motion-aware enhancement framework for dynamic scene reconstruction,
which mines useful motion cues from optical flow to improve different paradigms
of dynamic 3DGS. Specifically, we first establish a correspondence between 3D
Gaussian movements and pixel-level flow. Then a novel flow augmentation method
is introduced with additional insights into uncertainty and loss collaboration.
Moreover, for the prevalent deformation-based paradigm that presents a harder
optimization problem, a transient-aware deformation auxiliary module is
proposed. We conduct extensive experiments on both multi-view and monocular
scenes to verify the merits of our work. Compared with the baselines, our
method shows significant superiority in both rendering quality and efficiency.";Zhiyang Guo<author:sep>Wengang Zhou<author:sep>Li Li<author:sep>Min Wang<author:sep>Houqiang Li;http://arxiv.org/pdf/2403.11447v1;cs.CV;;gaussian splatting
2403.11625v2;http://arxiv.org/abs/2403.11625v2;2024-03-18;GaussNav: Gaussian Splatting for Visual Navigation;"In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to
locate a specific object depicted in a goal image within an unexplored
environment. The primary difficulty of IIN stems from the necessity of
recognizing the target object across varying viewpoints and rejecting potential
distractors.
  Existing map-based navigation methods largely adopt the representation form
of Bird's Eye View (BEV) maps, which, however, lack the representation of
detailed textures in a scene.
  To address the above issues, we propose a new Gaussian Splatting Navigation
(abbreviated as GaussNav) framework for IIN task, which constructs a novel map
representation based on 3D Gaussian Splatting (3DGS).
  The proposed framework enables the agent to not only memorize the geometry
and semantic information of the scene, but also retain the textural features of
objects.
  Our GaussNav framework demonstrates a significant leap in performance,
evidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to
0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset.
  Our code will be made publicly available.";Xiaohan Lei<author:sep>Min Wang<author:sep>Wengang Zhou<author:sep>Houqiang Li;http://arxiv.org/pdf/2403.11625v2;cs.CV;conference;gaussian splatting
2403.11460v1;http://arxiv.org/abs/2403.11460v1;2024-03-18;Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning;"In this work, we present Fed3DGS, a scalable 3D reconstruction framework
based on 3D Gaussian splatting (3DGS) with federated learning. Existing
city-scale reconstruction methods typically adopt a centralized approach, which
gathers all data in a central server and reconstructs scenes. The approach
hampers scalability because it places a heavy load on the server and demands
extensive data storage when reconstructing scenes on a scale beyond city-scale.
In pursuit of a more scalable 3D reconstruction, we propose a federated
learning framework with 3DGS, which is a decentralized framework and can
potentially use distributed computational resources across millions of clients.
We tailor a distillation-based model update scheme for 3DGS and introduce
appearance modeling for handling non-IID data in the scenario of 3D
reconstruction with federated learning. We simulate our method on several
large-scale benchmarks, and our method demonstrates rendered image quality
comparable to centralized approaches. In addition, we also simulate our method
with data collected in different seasons, demonstrating that our framework can
reflect changes in the scenes and our appearance modeling captures changes due
to seasonal variations.";Teppei Suzuki;http://arxiv.org/pdf/2403.11460v1;cs.CV;Code: https://github.com/DensoITLab/Fed3DGS;gaussian splatting
2403.11589v1;http://arxiv.org/abs/2403.11589v1;2024-03-18;UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures  for Human Avatar Modeling;"Reconstructing photo-realistic drivable human avatars from multi-view image
sequences has been a popular and challenging topic in the field of computer
vision and graphics. While existing NeRF-based methods can achieve high-quality
novel view rendering of human models, both training and inference processes are
time-consuming. Recent approaches have utilized 3D Gaussians to represent the
human body, enabling faster training and rendering. However, they undermine the
importance of the mesh guidance and directly predict Gaussians in 3D space with
coarse mesh guidance. This hinders the learning procedure of the Gaussians and
tends to produce blurry textures. Therefore, we propose UV Gaussians, which
models the 3D human body by jointly learning mesh deformations and 2D UV-space
Gaussian textures. We utilize the embedding of UV map to learn Gaussian
textures in 2D space, leveraging the capabilities of powerful 2D networks to
extract features. Additionally, through an independent Mesh network, we
optimize pose-dependent geometric deformations, thereby guiding Gaussian
rendering and significantly enhancing rendering quality. We collect and process
a new dataset of human motion, which includes multi-view images, scanned
models, parametric model registration, and corresponding texture maps.
Experimental results demonstrate that our method achieves state-of-the-art
synthesis of novel view and novel pose. The code and data will be made
available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the
paper is accepted.";Yujiao Jiang<author:sep>Qingmin Liao<author:sep>Xiaoyu Li<author:sep>Li Ma<author:sep>Qi Zhang<author:sep>Chaopeng Zhang<author:sep>Zongqing Lu<author:sep>Ying Shan;http://arxiv.org/pdf/2403.11589v1;cs.CV;;nerf
2403.11364v1;http://arxiv.org/abs/2403.11364v1;2024-03-17;Creating Seamless 3D Maps Using Radiance Fields;"It is desirable to create 3D object models and 3D maps from 2D input images
for applications such as navigation, virtual tourism, and urban planning. The
traditional methods of creating 3D maps, (such as photogrammetry), require a
large number of images and odometry. Additionally, traditional methods have
difficulty with reflective surfaces and specular reflections; windows and
chrome in the scene can be problematic. Google Road View is a familiar
application, which uses traditional methods to fuse a collection of 2D input
images into the illusion of a 3D map. However, Google Road View does not create
an actual 3D object model, only a collection of views. The objective of this
work is to create an actual 3D object model using updated techniques. Neural
Radiance Fields (NeRF[1]) has emerged as a potential solution, offering the
capability to produce more precise and intricate 3D maps. Gaussian Splatting[4]
is another contemporary technique. This investigation compares Neural Radiance
Fields to Gaussian Splatting, and describes some of their inner workings. Our
primary contribution is a method for improving the results of the 3D
reconstructed models. Our results indicate that Gaussian Splatting was superior
to the NeRF technique.";Sai Tarun Sathyan<author:sep>Thomas B. Kinsman;http://arxiv.org/pdf/2403.11364v1;cs.CV;10 pages with figures;gaussian splatting<tag:sep>nerf
2403.11367v1;http://arxiv.org/abs/2403.11367v1;2024-03-17;3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual  ReLocalization;"This paper presents a novel system designed for 3D mapping and visual
relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and
camera data to create accurate and visually plausible representations of the
environment. By leveraging LiDAR data to initiate the training of the 3D
Gaussian Splatting map, our system constructs maps that are both detailed and
geometrically accurate. To mitigate excessive GPU memory usage and facilitate
rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.
This preparation makes our method well-suited for visual localization tasks,
enabling efficient identification of correspondences between the query image
and the rendered image from the Gaussian Splatting map via normalized
cross-correlation (NCC). Additionally, we refine the camera pose of the query
image using feature-based matching and the Perspective-n-Point (PnP) technique.
The effectiveness, adaptability, and precision of our system are demonstrated
through extensive evaluation on the KITTI360 dataset.";Peng Jiang<author:sep>Gaurav Pandey<author:sep>Srikanth Saripalli;http://arxiv.org/pdf/2403.11367v1;cs.CV;8 pages, 7 figures;gaussian splatting
2403.09973v1;http://arxiv.org/abs/2403.09973v1;2024-03-15;Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive  Experience;"We have built a custom mobile multi-camera large-space dense light field
capture system, which provides a series of high-quality and sufficiently dense
light field images for various scenarios. Our aim is to contribute to the
development of popular 3D scene reconstruction algorithms such as IBRnet, NeRF,
and 3D Gaussian splitting. More importantly, the collected dataset, which is
much denser than existing datasets, may also inspire space-oriented light field
reconstruction, which is potentially different from object-centric 3D
reconstruction, for immersive VR/AR experiences. We utilized a total of 40
GoPro 10 cameras, capturing images of 5k resolution. The number of photos
captured for each scene is no less than 1000, and the average density (view
number within a unit sphere) is 134.68. It is also worth noting that our system
is capable of efficiently capturing large outdoor scenes. Addressing the
current lack of large-space and dense light field datasets, we made efforts to
include elements such as sky, reflections, lights and shadows that are of
interest to researchers in the field of 3D reconstruction during the data
capture process. Finally, we validated the effectiveness of our provided
dataset on three popular algorithms and also integrated the reconstructed 3DGS
results into the Unity engine, demonstrating the potential of utilizing our
datasets to enhance the realism of virtual reality (VR) and create feasible
interactive spaces. The dataset is available at our project website.";Xiaohang Yu<author:sep>Zhengxian Yang<author:sep>Shi Pan<author:sep>Yuqi Han<author:sep>Haoxiang Wang<author:sep>Jun Zhang<author:sep>Shi Yan<author:sep>Borong Lin<author:sep>Lei Yang<author:sep>Tao Yu<author:sep>Lu Fang;http://arxiv.org/pdf/2403.09973v1;cs.CV;;nerf
2403.10242v1;http://arxiv.org/abs/2403.10242v1;2024-03-15;FDGaussian: Fast Gaussian Splatting from Single Image via  Geometric-aware Diffusion Model;"Reconstructing detailed 3D objects from single-view images remains a
challenging task due to the limited information available. In this paper, we
introduce FDGaussian, a novel two-stage framework for single-image 3D
reconstruction. Recent methods typically utilize pre-trained 2D diffusion
models to generate plausible novel views from the input image, yet they
encounter issues with either multi-view inconsistency or lack of geometric
fidelity. To overcome these challenges, we propose an orthogonal plane
decomposition mechanism to extract 3D geometric features from the 2D input,
enabling the generation of consistent multi-view images. Moreover, we further
accelerate the state-of-the-art Gaussian Splatting incorporating epipolar
attention to fuse images from different viewpoints. We demonstrate that
FDGaussian generates images with high consistency across different views and
reconstructs high-quality 3D objects, both qualitatively and quantitatively.
More examples can be found at our website https://qjfeng.net/FDGaussian/.";Qijun Feng<author:sep>Zhen Xing<author:sep>Zuxuan Wu<author:sep>Yu-Gang Jiang;http://arxiv.org/pdf/2403.10242v1;cs.CV;;gaussian splatting
2403.10103v1;http://arxiv.org/abs/2403.10103v1;2024-03-15;DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video;"Recent advancements in dynamic neural radiance field methods have yielded
remarkable outcomes. However, these approaches rely on the assumption of sharp
input images. When faced with motion blur, existing dynamic NeRF methods often
struggle to generate high-quality novel views. In this paper, we propose
DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views
from a monocular video affected by motion blur. To account for motion blur in
input images, we simultaneously capture the camera trajectory and object
Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we
employ a global cross-time rendering approach to ensure consistent temporal
coherence across the entire scene. We curate a dataset comprising diverse
dynamic scenes that are specifically tailored for our task. Experimental
results on our dataset demonstrate that our method outperforms existing
approaches in generating sharp novel views from motion-blurred inputs while
maintaining spatial-temporal consistency of the scene.";Huiqiang Sun<author:sep>Xingyi Li<author:sep>Liao Shen<author:sep>Xinyi Ye<author:sep>Ke Xian<author:sep>Zhiguo Cao;http://arxiv.org/pdf/2403.10103v1;cs.CV;;nerf
2403.10147v1;http://arxiv.org/abs/2403.10147v1;2024-03-15;GGRt: Towards Generalizable 3D Gaussians without Pose Priors in  Real-Time;"This paper presents GGRt, a novel approach to generalizable novel view
synthesis that alleviates the need for real camera poses, complexity in
processing high-resolution images, and lengthy optimization processes, thus
facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in
real-world scenarios. Specifically, we design a novel joint learning framework
that consists of an Iterative Pose Optimization Network (IPO-Net) and a
Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,
the proposed framework can inherently estimate robust relative pose information
from the image observations and thus primarily alleviate the requirement of
real camera poses. Moreover, we implement a deferred back-propagation mechanism
that enables high-resolution training and inference, overcoming the resolution
constraints of previous methods. To enhance the speed and efficiency, we
further introduce a progressive Gaussian cache module that dynamically adjusts
during training and inference. As the first pose-free generalizable 3D-GS
framework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at
$\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our
method outperforms existing NeRF-based pose-free techniques in terms of
inference speed and effectiveness. It can also approach the real pose-based
3D-GS methods. Our contributions provide a significant leap forward for the
integration of computer vision and computer graphics into practical
applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open
datasets and enabling real-time rendering for immersive experiences.";Hao Li<author:sep>Yuanyuan Gao<author:sep>Dingwen Zhang<author:sep>Chenming Wu<author:sep>Yalun Dai<author:sep>Chen Zhao<author:sep>Haocheng Feng<author:sep>Errui Ding<author:sep>Jingdong Wang<author:sep>Junwei Han;http://arxiv.org/pdf/2403.10147v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.10516v1;http://arxiv.org/abs/2403.10516v1;2024-03-15;FeatUp: A Model-Agnostic Framework for Features at Any Resolution;"Deep features are a cornerstone of computer vision research, capturing image
semantics and enabling the community to solve downstream tasks even in the
zero- or few-shot regime. However, these features often lack the spatial
resolution to directly perform dense prediction tasks like segmentation and
depth prediction because models aggressively pool information over large areas.
In this work, we introduce FeatUp, a task- and model-agnostic framework to
restore lost spatial information in deep features. We introduce two variants of
FeatUp: one that guides features with high-resolution signal in a single
forward pass, and one that fits an implicit model to a single image to
reconstruct features at any resolution. Both approaches use a multi-view
consistency loss with deep analogies to NeRFs. Our features retain their
original semantics and can be swapped into existing applications to yield
resolution and performance gains even without re-training. We show that FeatUp
significantly outperforms other feature upsampling and image super-resolution
approaches in class activation map generation, transfer learning for
segmentation and depth prediction, and end-to-end training for semantic
segmentation.";Stephanie Fu<author:sep>Mark Hamilton<author:sep>Laura Brandt<author:sep>Axel Feldman<author:sep>Zhoutong Zhang<author:sep>William T. Freeman;http://arxiv.org/pdf/2403.10516v1;cs.CV;"Accepted to the International Conference on Learning Representations
  (ICLR) 2024";nerf
2403.10297v1;http://arxiv.org/abs/2403.10297v1;2024-03-15;Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints  Scene Coordinate Regression;"Classical structural-based visual localization methods offer high accuracy
but face trade-offs in terms of storage, speed, and privacy. A recent
innovation, keypoint scene coordinate regression (KSCR) named D2S addresses
these issues by leveraging graph attention networks to enhance keypoint
relationships and predict their 3D coordinates using a simple multilayer
perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using
established 2D-3D correspondences. While KSCR achieves competitive results,
rivaling state-of-the-art image-retrieval methods like HLoc across multiple
benchmarks, its performance is hindered when data samples are limited due to
the deep learning model's reliance on extensive data. This paper proposes a
solution to this challenge by introducing a pipeline for keypoint descriptor
synthesis using Neural Radiance Field (NeRF). By generating novel poses and
feeding them into a trained NeRF model to create new views, our approach
enhances the KSCR's generalization capabilities in data-scarce environments.
The proposed system could significantly improve localization accuracy by up to
50\% and cost only a fraction of time for data synthesis. Furthermore, its
modular design allows for the integration of multiple NeRFs, offering a
versatile and efficient solution for visual localization. The implementation is
publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.";Huy-Hoang Bui<author:sep>Bach-Thuan Bui<author:sep>Dinh-Tuan Tran<author:sep>Joo-Ho Lee;http://arxiv.org/pdf/2403.10297v1;cs.CV;;nerf
2403.09981v1;http://arxiv.org/abs/2403.09981v1;2024-03-15;Controllable Text-to-3D Generation via Surface-Aligned Gaussian  Splatting;"While text-to-3D and image-to-3D generation tasks have received considerable
attention, one important but under-explored field between them is controllable
text-to-3D generation, which we mainly focus on in this work. To address this
task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network
architecture designed to enhance existing pre-trained multi-view diffusion
models by integrating additional input conditions, such as edge, depth, normal,
and scribble maps. Our innovation lies in the introduction of a conditioning
module that controls the base diffusion model using both local and global
embeddings, which are computed from the input condition images and camera
poses. Once trained, MVControl is able to offer 3D diffusion guidance for
optimization-based 3D generation. And, 2) we propose an efficient multi-stage
3D generation pipeline that leverages the benefits of recent large
reconstruction models and score distillation algorithm. Building upon our
MVControl architecture, we employ a unique hybrid diffusion guidance method to
direct the optimization process. In pursuit of efficiency, we adopt 3D
Gaussians as our representation instead of the commonly used implicit
representations. We also pioneer the use of SuGaR, a hybrid representation that
binds Gaussians to mesh triangle faces. This approach alleviates the issue of
poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained
geometry on the mesh. Extensive experiments demonstrate that our method
achieves robust generalization and enables the controllable generation of
high-quality 3D content.";Zhiqi Li<author:sep>Yiming Chen<author:sep>Lingzhe Zhao<author:sep>Peidong Liu;http://arxiv.org/pdf/2403.09981v1;cs.CV;Project page: https://lizhiqi49.github.io/MVControl/;
2403.10340v1;http://arxiv.org/abs/2403.10340v1;2024-03-15;Thermal-NeRF: Neural Radiance Fields from an Infrared Camera;"In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant
potential in encoding highly-detailed 3D geometry and environmental appearance,
positioning themselves as a promising alternative to traditional explicit
representation for 3D scene reconstruction. However, the predominant reliance
on RGB imaging presupposes ideal lighting conditions: a premise frequently
unmet in robotic applications plagued by poor lighting or visual obstructions.
This limitation overlooks the capabilities of infrared (IR) cameras, which
excel in low-light detection and present a robust alternative under such
adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first
method that estimates a volumetric scene representation in the form of a NeRF
solely from IR imaging. By leveraging a thermal mapping and structural thermal
constraint derived from the thermal characteristics of IR imaging, our method
showcasing unparalleled proficiency in recovering NeRFs in visually degraded
scenes where RGB-based methods fall short. We conduct extensive experiments to
demonstrate that Thermal-NeRF can achieve superior quality compared to existing
methods. Furthermore, we contribute a dataset for IR-based NeRF applications,
paving the way for future research in IR NeRF reconstruction.";Tianxiang Ye<author:sep>Qi Wu<author:sep>Junyuan Deng<author:sep>Guoqing Liu<author:sep>Liu Liu<author:sep>Songpengcheng Xia<author:sep>Liang Pang<author:sep>Wenxian Yu<author:sep>Ling Pei;http://arxiv.org/pdf/2403.10340v1;cs.CV;;nerf
2403.10050v1;http://arxiv.org/abs/2403.10050v1;2024-03-15;Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian  Splatting Editing;"3D Gaussian splatting, emerging as a groundbreaking approach, has drawn
increasing attention for its capabilities of high-fidelity reconstruction and
real-time rendering. However, it couples the appearance and geometry of the
scene within the Gaussian attributes, which hinders the flexibility of editing
operations, such as texture swapping. To address this issue, we propose a novel
approach, namely Texture-GS, to disentangle the appearance from the geometry by
representing it as a 2D texture mapped onto the 3D surface, thereby
facilitating appearance editing. Technically, the disentanglement is achieved
by our proposed texture mapping module, which consists of a UV mapping MLP to
learn the UV coordinates for the 3D Gaussian centers, a local Taylor expansion
of the MLP to efficiently approximate the UV coordinates for the ray-Gaussian
intersections, and a learnable texture to capture the fine-grained appearance.
Extensive experiments on the DTU dataset demonstrate that our method not only
facilitates high-fidelity appearance editing but also achieves real-time
rendering on consumer-level devices, e.g. a single RTX 2080 Ti GPU.";Tian-Xing Xu<author:sep>Wenbo Hu<author:sep>Yu-Kun Lai<author:sep>Ying Shan<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2403.10050v1;cs.CV;;gaussian splatting
2403.10119v1;http://arxiv.org/abs/2403.10119v1;2024-03-15;URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural  Radiance Fields;"We propose a novel rolling shutter bundle adjustment method for neural
radiance fields (NeRF), which utilizes the unordered rolling shutter (RS)
images to obtain the implicit 3D representation. Existing NeRF methods suffer
from low-quality images and inaccurate initial camera poses due to the RS
effect in the image, whereas, the previous method that incorporates the RS into
NeRF requires strict sequential data input, limiting its widespread
applicability. In constant, our method recovers the physical formation of RS
images by estimating camera poses and velocities, thereby removing the input
constraints on sequential data. Moreover, we adopt a coarse-to-fine training
strategy, in which the RS epipolar constraints of the pairwise frames in the
scene graph are used to detect the camera poses that fall into local minima.
The poses detected as outliers are corrected by the interpolation method with
neighboring poses. The experimental results validate the effectiveness of our
method over state-of-the-art works and demonstrate that the reconstruction of
3D representations is not constrained by the requirement of video sequence
input.";Bo Xu<author:sep>Ziao Liu<author:sep>Mengqi Guo<author:sep>Jiancheng Li<author:sep>Gim Hee Li;http://arxiv.org/pdf/2403.10119v1;cs.CV;;nerf
2403.10427v1;http://arxiv.org/abs/2403.10427v1;2024-03-15;SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians;"Implicit neural representation methods have shown impressive advancements in
learning 3D scenes from unstructured in-the-wild photo collections but are
still limited by the large computational cost of volumetric rendering. More
recently, 3D Gaussian Splatting emerged as a much faster alternative with
superior rendering quality and training efficiency, especially for small-scale
and object-centric scenarios. Nevertheless, this technique suffers from poor
performance on unstructured in-the-wild data. To tackle this, we extend over 3D
Gaussian Splatting to handle unstructured image collections. We achieve this by
modeling appearance to seize photometric variations in the rendered images.
Additionally, we introduce a new mechanism to train transient Gaussians to
handle the presence of scene occluders in an unsupervised manner. Experiments
on diverse photo collection scenes and multi-pass acquisition of outdoor
landmarks show the effectiveness of our method over prior works achieving
state-of-the-art results with improved efficiency.";Hiba Dahmani<author:sep>Moussab Bennehar<author:sep>Nathan Piasco<author:sep>Luis Roldao<author:sep>Dzmitry Tsishkou;http://arxiv.org/pdf/2403.10427v1;cs.CV;;gaussian splatting
2403.09413v1;http://arxiv.org/abs/2403.09413v1;2024-03-14;Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting;"3D Gaussian splatting (3DGS) has recently demonstrated impressive
capabilities in real-time novel view synthesis and 3D reconstruction. However,
3DGS heavily depends on the accurate initialization derived from
Structure-from-Motion (SfM) methods. When trained with randomly initialized
point clouds, 3DGS fails to maintain its ability to produce high-quality
images, undergoing large performance drops of 4-5 dB in PSNR. Through extensive
analysis of SfM initialization in the frequency domain and analysis of a 1D
regression task with multiple 1D Gaussians, we propose a novel optimization
strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D
Gaussian Splatting), that successfully trains 3D Gaussians from random point
clouds. We show the effectiveness of our strategy through quantitative and
qualitative comparisons on multiple datasets, largely improving the performance
in all settings. Our project page and code can be found at
https://ku-cvlab.github.io/RAIN-GS.";Jaewoo Jung<author:sep>Jisang Han<author:sep>Honggyu An<author:sep>Jiwon Kang<author:sep>Seonghoon Park<author:sep>Seungryong Kim;http://arxiv.org/pdf/2403.09413v1;cs.CV;Project Page: https://ku-cvlab.github.io/RAIN-GS;gaussian splatting
2403.09875v1;http://arxiv.org/abs/2403.09875v1;2024-03-14;Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting;"In this work, we propose a novel method to supervise 3D Gaussian Splatting
(3DGS) scenes using optical tactile sensors. Optical tactile sensors have
become widespread in their use in robotics for manipulation and object
representation; however, raw optical tactile sensor data is unsuitable to
directly supervise a 3DGS scene. Our representation leverages a Gaussian
Process Implicit Surface to implicitly represent the object, combining many
touches into a unified representation with uncertainty. We merge this model
with a monocular depth estimation network, which is aligned in a two stage
process, coarsely aligning with a depth camera and then finely adjusting to
match our touch data. For every training image, our method produces a
corresponding fused depth and uncertainty map. Utilizing this additional
information, we propose a new loss function, variance weighted depth supervised
loss, for training the 3DGS scene model. We leverage the DenseTact optical
tactile sensor and RealSense RGB-D camera to show that combining touch and
vision in this manner leads to quantitatively and qualitatively better results
than vision or touch alone in a few-view scene syntheses on opaque as well as
on reflective and transparent objects. Please see our project page at
http://armlabstanford.github.io/touch-gs";Aiden Swann<author:sep>Matthew Strong<author:sep>Won Kyung Do<author:sep>Gadiel Sznaier Camps<author:sep>Mac Schwager<author:sep>Monroe Kennedy III;http://arxiv.org/pdf/2403.09875v1;cs.RO;;gaussian splatting
2403.09419v1;http://arxiv.org/abs/2403.09419v1;2024-03-14;RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban  Scenes;"The task of separating dynamic objects from static environments using NeRFs
has been widely studied in recent years. However, capturing large-scale scenes
still poses a challenge due to their complex geometric structures and
unconstrained dynamics. Without the help of 3D motion cues, previous methods
often require simplified setups with slow camera motion and only a few/single
dynamic actors, leading to suboptimal solutions in most urban setups. To
overcome such limitations, we present RoDUS, a pipeline for decomposing static
and dynamic elements in urban scenes, with thoughtfully separated NeRF models
for moving and non-moving components. Our approach utilizes a robust
kernel-based initialization coupled with 4D semantic information to selectively
guide the learning process. This strategy enables accurate capturing of the
dynamics in the scene, resulting in reduced artifacts caused by NeRF on
background reconstruction, all by using self-supervision. Notably, experimental
evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of
our method in decomposing challenging urban scenes into precise static and
dynamic components.";Thang-Anh-Quan Nguyen<author:sep>Luis Roldão<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Dzmitry Tsishkou;http://arxiv.org/pdf/2403.09419v1;cs.CV;;nerf
2403.09143v1;http://arxiv.org/abs/2403.09143v1;2024-03-14;A New Split Algorithm for 3D Gaussian Splatting;"3D Gaussian splatting models, as a novel explicit 3D representation, have
been applied in many domains recently, such as explicit geometric editing and
geometry generation. Progress has been rapid. However, due to their mixed
scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred
or needle-like effect near the surface. At the same time, 3D Gaussian splatting
models tend to flatten large untextured regions, yielding a very sparse point
cloud. These problems are caused by the non-uniform nature of 3D Gaussian
splatting models, so in this paper, we propose a new 3D Gaussian splitting
algorithm, which can produce a more uniform and surface-bounded 3D Gaussian
splatting model. Our algorithm splits an $N$-dimensional Gaussian into two
N-dimensional Gaussians. It ensures consistency of mathematical characteristics
and similarity of appearance, allowing resulting 3D Gaussian splatting models
to be more uniform and a better fit to the underlying surface, and thus more
suitable for explicit editing, point cloud extraction and other tasks.
Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form
solution, making it readily applicable to any 3D Gaussian model.";Qiyuan Feng<author:sep>Gengchen Cao<author:sep>Haoxiang Chen<author:sep>Tai-Jiang Mu<author:sep>Ralph R. Martin<author:sep>Shi-Min Hu;http://arxiv.org/pdf/2403.09143v1;cs.GR;11 pages, 10 figures;gaussian splatting
2403.09477v1;http://arxiv.org/abs/2403.09477v1;2024-03-14;VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance  Fields;"Autonomous mobile robots are an increasingly integral part of modern factory
and warehouse operations. Obstacle detection, avoidance and path planning are
critical safety-relevant tasks, which are often solved using expensive LiDAR
sensors and depth cameras. We propose to use cost-effective low-resolution
ranging sensors, such as ultrasonic and infrared time-of-flight sensors by
developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance
Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution
Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from
ultrasonic and infrared sensors and utilizes them to update the occupancy grid
used for ray marching. Experimental evaluation in 2D demonstrates that
VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds
regarding coverage. Notably, in small environments, its accuracy aligns with
that of LiDAR measurements, while in larger ones, it is bounded by the utilized
ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic
and infrared sensors is highly effective when dealing with sparse data and low
view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the
mapping capabilities and increases the training speed by 46% compared to
Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for
cost-effective local mapping in mobile robotics, with potential applications in
safety and navigation tasks. The code can be found at
https://github.com/ethz-asl/virus nerf.";Nicolaj Schmid<author:sep>Cornelius von Einem<author:sep>Cesar Cadena<author:sep>Roland Siegwart<author:sep>Lorenz Hruby<author:sep>Florian Tschopp;http://arxiv.org/pdf/2403.09477v1;cs.RO;;nerf
2403.09577v1;http://arxiv.org/abs/2403.09577v1;2024-03-14;The NeRFect Match: Exploring NeRF Features for Visual Localization;"In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene
representation for visual localization. Recently, NeRF has been employed to
enhance pose regression and scene coordinate regression models by augmenting
the training database, providing auxiliary supervision through rendered images,
or serving as an iterative refinement module. We extend its recognized
advantages -- its ability to provide a compact scene representation with
realistic appearances and accurate geometry -- by exploring the potential of
NeRF's internal features in establishing precise 2D-3D matches for
localization. To this end, we conduct a comprehensive examination of NeRF's
implicit knowledge, acquired through view synthesis, for matching under various
conditions. This includes exploring different matching network architectures,
extracting encoder features at multiple layers, and varying training
configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D
matching function that capitalizes on the internal knowledge of NeRF learned
via view synthesis. Our evaluation of NeRFMatch on standard localization
benchmarks, within a structure-based pipeline, sets a new state-of-the-art for
localization performance on Cambridge Landmarks.";Qunjie Zhou<author:sep>Maxim Maximov<author:sep>Or Litany<author:sep>Laura Leal-Taixé;http://arxiv.org/pdf/2403.09577v1;cs.CV;;nerf
2403.09439v1;http://arxiv.org/abs/2403.09439v1;2024-03-14;3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation;"Text-driven 3D scene generation techniques have made rapid progress in recent
years. Their success is mainly attributed to using existing generative models
to iteratively perform image warping and inpainting to generate 3D scenes.
However, these methods heavily rely on the outputs of existing models, leading
to error accumulation in geometry and appearance that prevent the models from
being used in various scenarios (e.g., outdoor and unreal scenarios). To
address this limitation, we generatively refine the newly generated local views
by querying and aggregating global 3D information, and then progressively
generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF
as a unified representation of the 3D scene to constrain global 3D consistency,
and propose a generative refinement network to synthesize new contents with
higher quality by exploiting the natural image prior from 2D diffusion model as
well as the global 3D information of the current scene. Our extensive
experiments demonstrate that, in comparison to previous methods, our approach
supports wide variety of scene generation and arbitrary camera trajectories
with improved visual quality and 3D consistency.";Frank Zhang<author:sep>Yibo Zhang<author:sep>Quan Zheng<author:sep>Rui Ma<author:sep>Wei Hua<author:sep>Hujun Bao<author:sep>Weiwei Xu<author:sep>Changqing Zou;http://arxiv.org/pdf/2403.09439v1;cs.CV;11 pages, 7 figures;nerf
2403.09079v1;http://arxiv.org/abs/2403.09079v1;2024-03-14;PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF  Priors;"Autonomous vehicles rely extensively on perception systems to navigate and
interpret their surroundings. Despite significant advancements in these systems
recently, challenges persist under conditions like occlusion, extreme lighting,
or in unfamiliar urban areas. Unlike these systems, humans do not solely depend
on immediate observations to perceive the environment. In navigating new
cities, humans gradually develop a preliminary mental map to supplement
real-time perception during subsequent visits. Inspired by this human approach,
we introduce a novel framework, Pre-Sight, that leverages past traversals to
construct static prior memories, enhancing online perception in later
navigations. Our method involves optimizing a city-scale neural radiance field
with data from previous journeys to generate neural priors. These priors, rich
in semantic and geometric details, are derived without manual annotations and
can seamlessly augment various state-of-the-art perception models, improving
their efficacy with minimal additional computational cost. Experimental results
on the nuScenes dataset demonstrate the framework's high compatibility with
diverse online perception models. Specifically, it shows remarkable
improvements in HD-map construction and occupancy prediction tasks,
highlighting its potential as a new perception framework for autonomous driving
systems. Our code will be released at
https://github.com/yuantianyuan01/PreSight.";Tianyuan Yuan<author:sep>Yucheng Mao<author:sep>Jiawei Yang<author:sep>Yicheng Liu<author:sep>Yue Wang<author:sep>Hang Zhao;http://arxiv.org/pdf/2403.09079v1;cs.CV;;nerf
2403.09637v1;http://arxiv.org/abs/2403.09637v1;2024-03-14;GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary  Robotic Grasping;"Constructing a 3D scene capable of accommodating open-ended language queries,
is a pivotal pursuit, particularly within the domain of robotics. Such
technology facilitates robots in executing object manipulations based on human
language directives. To tackle this challenge, some research efforts have been
dedicated to the development of language-embedded implicit fields. However,
implicit fields (e.g. NeRF) encounter limitations due to the necessity of
processing a large number of input views for reconstruction, coupled with their
inherent inefficiencies in inference. Thus, we present the GaussianGrasper,
which utilizes 3D Gaussian Splatting to explicitly represent the scene as a
collection of Gaussian primitives. Our approach takes a limited set of RGB-D
views and employs a tile-based splatting technique to create a feature field.
In particular, we propose an Efficient Feature Distillation (EFD) module that
employs contrastive learning to efficiently and accurately distill language
embeddings derived from foundational models. With the reconstructed geometry of
the Gaussian field, our method enables the pre-trained grasping model to
generate collision-free grasp pose candidates. Furthermore, we propose a
normal-guided grasp module to select the best grasp pose. Through comprehensive
real-world experiments, we demonstrate that GaussianGrasper enables robots to
accurately query and grasp objects with language instructions, providing a new
solution for language-guided manipulation tasks. Data and codes can be
available at https://github.com/MrSecant/GaussianGrasper.";Yuhang Zheng<author:sep>Xiangyu Chen<author:sep>Yupeng Zheng<author:sep>Songen Gu<author:sep>Runyi Yang<author:sep>Bu Jin<author:sep>Pengfei Li<author:sep>Chengliang Zhong<author:sep>Zengmao Wang<author:sep>Lina Liu<author:sep>Chao Yang<author:sep>Dawei Wang<author:sep>Zhen Chen<author:sep>Xiaoxiao Long<author:sep>Meiqing Wang;http://arxiv.org/pdf/2403.09637v1;cs.RO;;gaussian splatting<tag:sep>nerf
2403.08551v2;http://arxiv.org/abs/2403.08551v2;2024-03-13;GaussianImage: 1000 FPS Image Representation and Compression by 2D  Gaussian Splatting;"Implicit neural representations (INRs) recently achieved great success in
image representation and compression, offering high visual quality and fast
rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are
available. However, this requirement often hinders their use on low-end devices
with limited memory. In response, we propose a groundbreaking paradigm of image
representation and compression by 2D Gaussian Splatting, named GaussianImage.
We first introduce 2D Gaussian to represent the image, where each Gaussian has
8 parameters including position, covariance and color. Subsequently, we unveil
a novel rendering algorithm based on accumulated summation. Remarkably, our
method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster
fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation
performance, but also delivers a faster rendering speed of 1500-2000 FPS
regardless of parameter size. Furthermore, we integrate existing vector
quantization technique to build an image codec. Experimental results
demonstrate that our codec attains rate-distortion performance comparable to
compression-based INRs such as COIN and COIN++, while facilitating decoding
speeds of approximately 1000 FPS. Additionally, preliminary proof of concept
shows that our codec surpasses COIN and COIN++ in performance when using
partial bits-back coding.";Xinjie Zhang<author:sep>Xingtong Ge<author:sep>Tongda Xu<author:sep>Dailan He<author:sep>Yan Wang<author:sep>Hongwei Qin<author:sep>Guo Lu<author:sep>Jing Geng<author:sep>Jun Zhang;http://arxiv.org/pdf/2403.08551v2;eess.IV;;gaussian splatting
2403.08310v1;http://arxiv.org/abs/2403.08310v1;2024-03-13;StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance  Fields;"4D style transfer aims at transferring arbitrary visual style to the
synthesized novel views of a dynamic 4D scene with varying viewpoints and
times. Existing efforts on 3D style transfer can effectively combine the visual
features of style images and neural radiance fields (NeRF) but fail to handle
the 4D dynamic scenes limited by the static scene assumption. Consequently, we
aim to handle the novel challenging problem of 4D style transfer for the first
time, which further requires the consistency of stylized results on dynamic
objects. In this paper, we introduce StyleDyRF, a method that represents the 4D
feature space by deforming a canonical feature volume and learns a linear style
transformation matrix on the feature volume in a data-driven fashion. To obtain
the canonical feature volume, the rays at each time step are deformed with the
geometric prior of a pre-trained dynamic NeRF to render the feature map under
the supervision of pre-trained visual encoders. With the content and style cues
in the canonical feature volume and the style image, we can learn the style
transformation matrix from their covariance matrices with lightweight neural
networks. The learned style transformation matrix can reflect a direct matching
of feature covariance from the content volume to the given style pattern, in
analogy with the optimization of the Gram matrix in traditional 2D neural style
transfer. The experimental results show that our method not only renders 4D
photorealistic style transfer results in a zero-shot manner but also
outperforms existing methods in terms of visual quality and consistency.";Hongbin Xu<author:sep>Weitao Chen<author:sep>Feng Xiao<author:sep>Baigui Sun<author:sep>Wenxiong Kang;http://arxiv.org/pdf/2403.08310v1;cs.CV;"In submission. The code and model are released at:
  https://github.com/ToughStoneX/StyleDyRF";nerf
2403.08498v1;http://arxiv.org/abs/2403.08498v1;2024-03-13;Gaussian Splatting in Style;"Scene stylization extends the work of neural style transfer to three spatial
dimensions. A vital challenge in this problem is to maintain the uniformity of
the stylized appearance across a multi-view setting. A vast majority of the
previous works achieve this by optimizing the scene with a specific style
image. In contrast, we propose a novel architecture trained on a collection of
style images, that at test time produces high quality stylized novel views. Our
work builds up on the framework of 3D Gaussian splatting. For a given scene, we
take the pretrained Gaussians and process them using a multi resolution hash
grid and a tiny MLP to obtain the conditional stylised views. The explicit
nature of 3D Gaussians give us inherent advantages over NeRF-based methods
including geometric consistency, along with having a fast training and
rendering regime. This enables our method to be useful for vast practical use
cases such as in augmented or virtual reality applications. Through our
experiments, we show our methods achieve state-of-the-art performance with
superior visual quality on various indoor and outdoor real-world data.";Abhishek Saroha<author:sep>Mariia Gladkova<author:sep>Cecilia Curreli<author:sep>Tarun Yenamandra<author:sep>Daniel Cremers;http://arxiv.org/pdf/2403.08498v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.08733v2;http://arxiv.org/abs/2403.08733v2;2024-03-13;GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting  Editing;"We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed
by the 3D Gaussian Splatting (3DGS).
  Our method first renders a collection of images by using the 3DGS and edits
them by using a pre-trained 2D diffusion model (ControlNet) based on the input
prompt, which is then used to optimise the 3D model.
  Our key contribution is multi-view consistent editing, which enables editing
all images together instead of iteratively editing one image while updating the
3D model as in previous works.
  It leads to faster editing as well as higher visual quality.
  This is achieved by the two terms:
  (a) depth-conditioned editing that enforces geometric consistency across
multi-view images by leveraging naturally consistent depth maps.
  (b) attention-based latent code alignment that unifies the appearance of
edited images by conditioning their editing to several reference views through
self and cross-view attention between images' latent representations.
  Experiments demonstrate that our method achieves faster editing and better
visual results than previous state-of-the-art methods.";Jing Wu<author:sep>Jia-Wang Bian<author:sep>Xinghui Li<author:sep>Guangrun Wang<author:sep>Ian Reid<author:sep>Philip Torr<author:sep>Victor Adrian Prisacariu;http://arxiv.org/pdf/2403.08733v2;cs.CV;17 pages;gaussian splatting
2403.08156v1;http://arxiv.org/abs/2403.08156v1;2024-03-13;NeRF-Supervised Feature Point Detection and Description;"Feature point detection and description is the backbone for various computer
vision applications, such as Structure-from-Motion, visual SLAM, and visual
place recognition. While learning-based methods have surpassed traditional
handcrafted techniques, their training often relies on simplistic
homography-based simulations of multi-view perspectives, limiting model
generalisability. This paper introduces a novel approach leveraging neural
radiance fields (NeRFs) for realistic multi-view training data generation. We
create a diverse multi-view dataset using NeRFs, consisting of indoor and
outdoor scenes. Our proposed methodology adapts state-of-the-art feature
detectors and descriptors to train on NeRF-synthesised views supervised by
perspective projective geometry. Our experiments demonstrate that the proposed
methods achieve competitive or superior performance on standard benchmarks for
relative pose estimation, point cloud registration, and homography estimation
while requiring significantly less training data compared to existing
approaches.";Ali Youssef<author:sep>Francisco Vasconcelos;http://arxiv.org/pdf/2403.08156v1;cs.CV;;nerf
2403.08321v1;http://arxiv.org/abs/2403.08321v1;2024-03-13;ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic  Manipulation;"Performing language-conditioned robotic manipulation tasks in unstructured
environments is highly demanded for general intelligent robots. Conventional
robotic manipulation methods usually learn semantic representation of the
observation for action prediction, which ignores the scene-level spatiotemporal
dynamics for human goal completion. In this paper, we propose a dynamic
Gaussian Splatting method named ManiGaussian for multi-task robotic
manipulation, which mines scene dynamics via future scene reconstruction.
Specifically, we first formulate the dynamic Gaussian Splatting framework that
infers the semantics propagation in the Gaussian embedding space, where the
semantic representation is leveraged to predict the optimal robot action. Then,
we build a Gaussian world model to parameterize the distribution in our dynamic
Gaussian Splatting framework, which provides informative supervision in the
interactive environment via future scene reconstruction. We evaluate our
ManiGaussian on 10 RLBench tasks with 166 variations, and the results
demonstrate our framework can outperform the state-of-the-art methods by 13.1\%
in average success rate.";Guanxing Lu<author:sep>Shiyi Zhang<author:sep>Ziwei Wang<author:sep>Changliu Liu<author:sep>Jiwen Lu<author:sep>Yansong Tang;http://arxiv.org/pdf/2403.08321v1;cs.RO;;gaussian splatting
2403.07547v1;http://arxiv.org/abs/2403.07547v1;2024-03-12;SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields;"Neural radiance fields (NeRF) has attracted considerable attention for their
exceptional ability in synthesizing novel views with high fidelity. However,
the presence of motion blur, resulting from slight camera movements during
extended shutter exposures, poses a significant challenge, potentially
compromising the quality of the reconstructed 3D scenes. While recent studies
have addressed this issue, they do not consider the continuous dynamics of
camera movements during image acquisition, leading to inaccurate scene
reconstruction. Additionally, these methods are plagued by slow training and
rendering speed. To effectively handle these issues, we propose sequential
motion understanding radiance fields (SMURF), a novel approach that employs
neural ordinary differential equation (Neural-ODE) to model continuous camera
motion and leverages the explicit volumetric representation method for faster
training and robustness to motion-blurred input images. The core idea of the
SMURF is continuous motion blurring kernel (CMBK), a unique module designed to
model a continuous camera movements for processing blurry inputs. Our model,
rigorously evaluated against benchmark datasets, demonstrates state-of-the-art
performance both quantitatively and qualitatively.";Jungho Lee<author:sep>Dogyoon Lee<author:sep>Minhyeok Lee<author:sep>Donghyung Kim<author:sep>Sangyoun Lee;http://arxiv.org/pdf/2403.07547v1;cs.CV;"25 pages, 10 figures, Code is available at
  https://github.com/Jho-Yonsei/SMURF";nerf
2403.08125v1;http://arxiv.org/abs/2403.08125v1;2024-03-12;Q-SLAM: Quadric Representations for Monocular SLAM;"Monocular SLAM has long grappled with the challenge of accurately modeling 3D
geometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular
SLAM have shown promise, yet these methods typically focus on novel view
synthesis rather than precise 3D geometry modeling. This focus results in a
significant disconnect between NeRF applications, i.e., novel-view synthesis
and the requirements of SLAM. We identify that the gap results from the
volumetric representations used in NeRF, which are often dense and noisy. In
this study, we propose a novel approach that reimagines volumetric
representations through the lens of quadric forms. We posit that most scene
components can be effectively represented as quadric planes. Leveraging this
assumption, we reshape the volumetric representations with million of cubes by
several quadric planes, which leads to more accurate and efficient modeling of
3D scenes in SLAM contexts. Our method involves two key steps: First, we use
the quadric assumption to enhance coarse depth estimations obtained from
tracking modules, e.g., Droid-SLAM. This step alone significantly improves
depth estimation accuracy. Second, in the subsequent mapping phase, we diverge
from previous NeRF-based SLAM methods that distribute sampling points across
the entire volume space. Instead, we concentrate sampling points around quadric
planes and aggregate them using a novel quadric-decomposed Transformer.
Additionally, we introduce an end-to-end joint optimization strategy that
synchronizes pose estimation with 3D reconstruction.";Chensheng Peng<author:sep>Chenfeng Xu<author:sep>Yue Wang<author:sep>Mingyu Ding<author:sep>Heng Yang<author:sep>Masayoshi Tomizuka<author:sep>Kurt Keutzer<author:sep>Marco Pavone<author:sep>Wei Zhan;http://arxiv.org/pdf/2403.08125v1;cs.CV;;nerf
2403.07807v1;http://arxiv.org/abs/2403.07807v1;2024-03-12;StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting;"We introduce StyleGaussian, a novel 3D style transfer technique that allows
instant transfer of any image's style to a 3D scene at 10 frames per second
(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style
transfer without compromising its real-time rendering ability and multi-view
consistency. It achieves instant style transfer with three steps: embedding,
transfer, and decoding. Initially, 2D VGG scene features are embedded into
reconstructed 3D Gaussians. Next, the embedded features are transformed
according to a reference style image. Finally, the transformed features are
decoded into the stylized RGB. StyleGaussian has two novel designs. The first
is an efficient feature rendering strategy that first renders low-dimensional
features and then maps them into high-dimensional features while embedding VGG
features. It cuts the memory consumption significantly and enables 3DGS to
render the high-dimensional memory-intensive features. The second is a
K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized
features, it eliminates the 2D CNN operations that compromise strict multi-view
consistency. Extensive experiments show that StyleGaussian achieves instant 3D
stylization with superior stylization quality while preserving real-time
rendering and strict multi-view consistency. Project page:
https://kunhao-liu.github.io/StyleGaussian/";Kunhao Liu<author:sep>Fangneng Zhan<author:sep>Muyu Xu<author:sep>Christian Theobalt<author:sep>Ling Shao<author:sep>Shijian Lu;http://arxiv.org/pdf/2403.07807v1;cs.CV;;gaussian splatting
2403.07494v1;http://arxiv.org/abs/2403.07494v1;2024-03-12;SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM;"We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D
Gaussian representation, that enables accurate 3D semantic mapping, robust
camera tracking, and high-quality rendering in real-time. In this system, we
incorporate semantic feature embedding into 3D Gaussian representation, which
effectively encodes semantic information within the spatial layout of the
environment for precise semantic scene representation. Furthermore, we propose
feature-level loss for updating 3D Gaussian representation, enabling
higher-level guidance for 3D Gaussian optimization. In addition, to reduce
cumulative drift and improve reconstruction accuracy, we introduce
semantic-informed bundle adjustment leveraging semantic associations for joint
optimization of 3D Gaussian representation and camera poses, leading to more
robust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates
superior performance over existing dense semantic SLAM methods in terms of
mapping and tracking accuracy on Replica and ScanNet datasets, while also
showing excellent capabilities in novel-view semantic synthesis and 3D semantic
mapping.";Siting Zhu<author:sep>Renjie Qin<author:sep>Guangming Wang<author:sep>Jiuming Liu<author:sep>Hesheng Wang;http://arxiv.org/pdf/2403.07494v1;cs.RO;;gaussian splatting
2403.06877v1;http://arxiv.org/abs/2403.06877v1;2024-03-11;SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields  for Robotic Inspection;"We present a neural-field-based large-scale reconstruction system that fuses
lidar and vision data to generate high-quality reconstructions that are
geometrically accurate and capture photo-realistic textures. This system adapts
the state-of-the-art neural radiance field (NeRF) representation to also
incorporate lidar data which adds strong geometric constraints on the depth and
surface normals. We exploit the trajectory from a real-time lidar SLAM system
to bootstrap a Structure-from-Motion (SfM) procedure to both significantly
reduce the computation time and to provide metric scale which is crucial for
lidar depth loss. We use submapping to scale the system to large-scale
environments captured over long trajectories. We demonstrate the reconstruction
system with data from a multi-camera, lidar sensor suite onboard a legged
robot, hand-held while scanning building scenes for 600 metres, and onboard an
aerial robot surveying a multi-storey mock disaster site-building. Website:
https://ori-drs.github.io/projects/silvr/";Yifu Tao<author:sep>Yash Bhalgat<author:sep>Lanke Frank Tarimo Fu<author:sep>Matias Mattamala<author:sep>Nived Chebrolu<author:sep>Maurice Fallon;http://arxiv.org/pdf/2403.06877v1;cs.RO;"Accepted at ICRA 2024; Website:
  https://ori-drs.github.io/projects/silvr/";nerf
2403.06908v1;http://arxiv.org/abs/2403.06908v1;2024-03-11;FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization;"3D Gaussian splatting has achieved very impressive performance in real-time
novel view synthesis. However, it often suffers from over-reconstruction during
Gaussian densification where high-variance image regions are covered by a few
large Gaussians only, leading to blur and artifacts in the rendered images. We
design a progressive frequency regularization (FreGS) technique to tackle the
over-reconstruction issue within the frequency space. Specifically, FreGS
performs coarse-to-fine Gaussian densification by exploiting low-to-high
frequency components that can be easily extracted with low-pass and high-pass
filters in the Fourier space. By minimizing the discrepancy between the
frequency spectrum of the rendered image and the corresponding ground truth, it
achieves high-quality Gaussian densification and alleviates the
over-reconstruction of Gaussian splatting effectively. Experiments over
multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and
Deep Blending) show that FreGS achieves superior novel view synthesis and
outperforms the state-of-the-art consistently.";Jiahui Zhang<author:sep>Fangneng Zhan<author:sep>Muyu Xu<author:sep>Shijian Lu<author:sep>Eric Xing;http://arxiv.org/pdf/2403.06908v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.06912v2;http://arxiv.org/abs/2403.06912v2;2024-03-11;DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with  Global-Local Depth Normalization;"Radiance fields have demonstrated impressive performance in synthesizing
novel views from sparse input views, yet prevailing methods suffer from high
training costs and slow inference speed. This paper introduces DNGaussian, a
depth-regularized framework based on 3D Gaussian radiance fields, offering
real-time and high-quality few-shot novel view synthesis at low costs. Our
motivation stems from the highly efficient representation and surprising
quality of the recent 3D Gaussian Splatting, despite it will encounter a
geometry degradation when input views decrease. In the Gaussian radiance
fields, we find this degradation in scene geometry primarily lined to the
positioning of Gaussian primitives and can be mitigated by depth constraint.
Consequently, we propose a Hard and Soft Depth Regularization to restore
accurate scene geometry under coarse monocular depth supervision while
maintaining a fine-grained color appearance. To further refine detailed
geometry reshaping, we introduce Global-Local Depth Normalization, enhancing
the focus on small local depth changes. Extensive experiments on LLFF, DTU, and
Blender datasets demonstrate that DNGaussian outperforms state-of-the-art
methods, achieving comparable or better results with significantly reduced
memory cost, a $25 \times$ reduction in training time, and over $3000 \times$
faster rendering speed.";Jiahe Li<author:sep>Jiawei Zhang<author:sep>Xiao Bai<author:sep>Jin Zheng<author:sep>Xin Ning<author:sep>Jun Zhou<author:sep>Lin Gu;http://arxiv.org/pdf/2403.06912v2;cs.CV;"Accepted at CVPR 2024. Project page:
  https://fictionarry.github.io/DNGaussian/";gaussian splatting
2403.06505v1;http://arxiv.org/abs/2403.06505v1;2024-03-11;Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis;"The neural radiance field (NeRF) has emerged as a prominent methodology for
synthesizing realistic images of novel views. While neural radiance
representations based on voxels or mesh individually offer distinct advantages,
excelling in either rendering quality or speed, each has limitations in the
other aspect. In response, we propose a pioneering hybrid representation named
Vosh, seamlessly combining both voxel and mesh components in hybrid rendering
for view synthesis. Vosh is meticulously crafted by optimizing the voxel grid
of NeRF, strategically with selected voxels replaced by mesh. Therefore, it
excels in fast rendering scenes with simple geometry and textures through its
mesh component, while simultaneously enabling high-quality rendering in
intricate regions by leveraging voxel component. The flexibility of Vosh is
showcased through the ability to adjust hybrid ratios, providing users the
ability to control the balance between rendering quality and speed based on
flexible usage. Experimental results demonstrates that our method achieves
commendable trade-off between rendering quality and speed, and notably has
real-time performance on mobile devices.";Chenhao Zhang<author:sep>Yongyang Zhou<author:sep>Lei Zhang;http://arxiv.org/pdf/2403.06505v1;cs.CV;;nerf
2403.06394v2;http://arxiv.org/abs/2403.06394v2;2024-03-11;FSViewFusion: Few-Shots View Generation of Novel Objects;"Novel view synthesis has observed tremendous developments since the arrival
of NeRFs. However, Nerf models overfit on a single scene, lacking
generalization to out of distribution objects. Recently, diffusion models have
exhibited remarkable performance on introducing generalization in view
synthesis. Inspired by these advancements, we explore the capabilities of a
pretrained stable diffusion model for view synthesis without explicit 3D
priors. Specifically, we base our method on a personalized text to image model,
Dreambooth, given its strong ability to adapt to specific novel objects with a
few shots. Our research reveals two interesting findings. First, we observe
that Dreambooth can learn the high level concept of a view, compared to
arguably more complex strategies which involve finetuning diffusions on large
amounts of multi-view data. Second, we establish that the concept of a view can
be disentangled and transferred to a novel object irrespective of the original
object's identify from which the views are learnt. Motivated by this, we
introduce a learning strategy, FSViewFusion, which inherits a specific view
through only one image sample of a single scene, and transfers the knowledge to
a novel object, learnt from few shots, using low rank adapters. Through
extensive experiments we demonstrate that our method, albeit simple, is
efficient in generating reliable view samples for in the wild images. Code and
models will be released.";Rukhshanda Hussain<author:sep>Hui Xian Grace Lim<author:sep>Borchun Chen<author:sep>Mubarak Shah<author:sep>Ser Nam Lim;http://arxiv.org/pdf/2403.06394v2;cs.CV;;nerf
2403.06092v1;http://arxiv.org/abs/2403.06092v1;2024-03-10;Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View  Synthesis?;"Neural Radiance Field (NeRF) has achieved superior performance for novel view
synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a
volume rendering procedure, however, when fewer known views are given (i.e.,
few-shot view synthesis), the model is prone to overfit the given views. To
handle this issue, previous efforts have been made towards leveraging learned
priors or introducing additional regularizations. In contrast, in this paper,
we for the first time provide an orthogonal method from the perspective of
network structure. Given the observation that trivially reducing the number of
model parameters alleviates the overfitting issue, but at the cost of missing
details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs
(i.e., location and viewing direction) of the vanilla MLP into each layer to
prevent the overfitting issue without harming detailed synthesis. To further
reduce the artifacts, we propose to model colors and volume density separately
and present two regularization terms. Extensive experiments on multiple
datasets demonstrate that: 1) although the proposed mi-MLP is easy to
implement, it is surprisingly effective as it boosts the PSNR of the baseline
from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art
results on a wide range of benchmarks. We will release the code upon
publication.";Hanxin Zhu<author:sep>Tianyu He<author:sep>Xin Li<author:sep>Bingchen Li<author:sep>Zhibo Chen;http://arxiv.org/pdf/2403.06092v1;cs.CV;Accepted by CVPR 2024;nerf
2403.05907v1;http://arxiv.org/abs/2403.05907v1;2024-03-09;Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous  Driving;"Recent studies have highlighted the promising application of NeRF in
autonomous driving contexts. However, the complexity of outdoor environments,
combined with the restricted viewpoints in driving scenarios, complicates the
task of precisely reconstructing scene geometry. Such challenges often lead to
diminished quality in reconstructions and extended durations for both training
and rendering. To tackle these challenges, we present Lightning NeRF. It uses
an efficient hybrid scene representation that effectively utilizes the geometry
prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly
improves the novel view synthesis performance of NeRF and reduces computational
overheads. Through evaluations on real-world datasets, such as KITTI-360,
Argoverse2, and our private dataset, we demonstrate that our approach not only
exceeds the current state-of-the-art in novel view synthesis quality but also
achieves a five-fold increase in training speed and a ten-fold improvement in
rendering speed. Codes are available at
https://github.com/VISION-SJTU/Lightning-NeRF .";Junyi Cao<author:sep>Zhichao Li<author:sep>Naiyan Wang<author:sep>Chao Ma;http://arxiv.org/pdf/2403.05907v1;cs.CV;Accepted to ICRA 2024;nerf
2403.05783v1;http://arxiv.org/abs/2403.05783v1;2024-03-09;Large Generative Model Assisted 3D Semantic Communication;"Semantic Communication (SC) is a novel paradigm for data transmission in 6G.
However, there are several challenges posed when performing SC in 3D scenarios:
1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain
channel estimation. To address these issues, we propose a Generative AI Model
assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor
(3DSE), which employs generative AI models, including Segment Anything Model
(SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D
scenario based on user requirements. The extracted 3D semantics are represented
as multi-perspective images of the goal-oriented 3D object. Then, we present an
Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective
images, in which we use a semantic encoder with two output heads to perform
semantic encoding and mask redundant semantics in the latent semantic space,
respectively. Next, we design a conditional Generative adversarial network and
Diffusion model aided-Channel Estimation (GDCE) to estimate and refine the
Channel State Information (CSI) of physical channels. Finally, simulation
results demonstrate the advantages of the proposed GAM-3DSC system in
effectively transmitting the goal-oriented 3D scenario.";Feibo Jiang<author:sep>Yubo Peng<author:sep>Li Dong<author:sep>Kezhi Wang<author:sep>Kun Yang<author:sep>Cunhua Pan<author:sep>Xiaohu You;http://arxiv.org/pdf/2403.05783v1;cs.IT;13 pages,13 figures,1 table;nerf
2403.05087v1;http://arxiv.org/abs/2403.05087v1;2024-03-08;SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded  Gaussian Splatting;"We present SplattingAvatar, a hybrid 3D representation of photorealistic
human avatars with Gaussian Splatting embedded on a triangle mesh, which
renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We
disentangle the motion and appearance of a virtual human with explicit mesh
geometry and implicit appearance modeling with Gaussian Splatting. The
Gaussians are defined by barycentric coordinates and displacement on a triangle
mesh as Phong surfaces. We extend lifted optimization to simultaneously
optimize the parameters of the Gaussians while walking on the triangle mesh.
SplattingAvatar is a hybrid representation of virtual humans where the mesh
represents low-frequency motion and surface deformation, while the Gaussians
take over the high-frequency geometry and detailed appearance. Unlike existing
deformation methods that rely on an MLP-based linear blend skinning (LBS) field
for motion, we control the rotation and translation of the Gaussians directly
by mesh, which empowers its compatibility with various animation techniques,
e.g., skeletal animation, blend shapes, and mesh editing. Trainable from
monocular videos for both full-body and head avatars, SplattingAvatar shows
state-of-the-art rendering quality across multiple datasets.";Zhijing Shao<author:sep>Zhaolong Wang<author:sep>Zhuang Li<author:sep>Duotun Wang<author:sep>Xiangru Lin<author:sep>Yu Zhang<author:sep>Mingming Fan<author:sep>Zeyu Wang;http://arxiv.org/pdf/2403.05087v1;cs.GR;"[CVPR 2024] Code and data are available at
  https://github.com/initialneil/SplattingAvatar";gaussian splatting
2403.05154v1;http://arxiv.org/abs/2403.05154v1;2024-03-08;GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian  Splatting;"We present GSEdit, a pipeline for text-guided 3D object editing based on
Gaussian Splatting models. Our method enables the editing of the style and
appearance of 3D objects without altering their main details, all in a matter
of minutes on consumer hardware. We tackle the problem by leveraging Gaussian
splatting to represent 3D scenes, and we optimize the model while progressively
varying the image supervision by means of a pretrained image-based diffusion
model. The input object may be given as a 3D triangular mesh, or directly
provided as Gaussians from a generative model such as DreamGaussian. GSEdit
ensures consistency across different viewpoints, maintaining the integrity of
the original object's information. Compared to previously proposed methods
relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making
3D editing tasks much faster. Our editing process is refined via the
application of the SDS loss, ensuring that our edits are both precise and
accurate. Our comprehensive evaluation demonstrates that GSEdit effectively
alters object shape and appearance following the given textual instructions
while preserving their coherence and detail.";Francesco Palandra<author:sep>Andrea Sanchietti<author:sep>Daniele Baieri<author:sep>Emanuele Rodolà;http://arxiv.org/pdf/2403.05154v1;cs.CV;15 pages, 7 figures;gaussian splatting<tag:sep>nerf
2403.04116v1;http://arxiv.org/abs/2403.04116v1;2024-03-07;Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis;"X-ray is widely applied for transmission imaging due to its stronger
penetration than natural light. When rendering novel view X-ray projections,
existing methods mainly based on NeRF suffer from long training time and slow
inference speed. In this paper, we propose a 3D Gaussian splatting-based
framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we
redesign a radiative Gaussian point cloud model inspired by the isotropic
nature of X-ray imaging. Our model excludes the influence of view direction
when learning to predict the radiation intensity of 3D points. Based on this
model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA
implementation. Secondly, we customize an Angle-pose Cuboid Uniform
Initialization (ACUI) strategy that directly uses the parameters of the X-ray
scanner to compute the camera information and then uniformly samples point
positions within a cuboid enclosing the scanned object. Experiments show that
our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying
less than 15% training time and over 73x inference speed. The application on
sparse-view CT reconstruction also reveals the practical values of our method.
Code and models will be publicly available at
https://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training
process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .";Yuanhao Cai<author:sep>Yixun Liang<author:sep>Jiahao Wang<author:sep>Angtian Wang<author:sep>Yulun Zhang<author:sep>Xiaokang Yang<author:sep>Zongwei Zhou<author:sep>Alan Yuille;http://arxiv.org/pdf/2403.04116v1;eess.IV;"The first 3D Gaussian Splatting-based method for X-ray 3D
  reconstruction";gaussian splatting<tag:sep>nerf
2403.04508v2;http://arxiv.org/abs/2403.04508v2;2024-03-07;Finding Waldo: Towards Efficient Exploration of NeRF Scene Spaces;"Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D
reconstruction and novel view synthesis in recent years due to their remarkable
performance. Despite the huge interest in NeRF methods, a practical use case of
NeRFs has largely been ignored; the exploration of the scene space modelled by
a NeRF. In this paper, for the first time in the literature, we propose and
formally define the scene exploration framework as the efficient discovery of
NeRF model inputs (i.e. coordinates and viewing angles), using which one can
render novel views that adhere to user-selected criteria. To remedy the lack of
approaches addressing scene exploration, we first propose two baseline methods
called Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).
We then cast scene exploration as an optimization problem, and propose the
criteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient
exploration. We test all three approaches with various criteria (e.g. saliency
maximization, image quality maximization, photo-composition quality
improvement) and show that our EGPS performs more favourably than other
baselines. We finally highlight key points and limitations, and outline
directions for future research in scene exploration.";Evangelos Skartados<author:sep>Mehmet Kerim Yucel<author:sep>Bruno Manganelli<author:sep>Anastasios Drosou<author:sep>Albert Saà-Garriga;http://arxiv.org/pdf/2403.04508v2;cs.CV;Accepted at ACM MMSys'24;nerf
2403.04114v1;http://arxiv.org/abs/2403.04114v1;2024-03-07;Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs;"Deep learning methods for perception are the cornerstone of many robotic
systems. Despite their potential for impressive performance, obtaining
real-world training data is expensive, and can be impractically difficult for
some tasks. Sim-to-real transfer with domain randomization offers a potential
workaround, but often requires extensive manual tuning and results in models
that are brittle to distribution shift between sim and real. In this work, we
introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF
model that is the centerpiece of a real-to-sim pipeline for synthesizing
training data targeted to scenes and objects from the real world. COV-NeRF
extracts objects from real images and composes them into new scenes, generating
photorealistic renderings and many types of 2D and 3D supervision, including
depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the
rendering quality of modern NeRF methods, and can be used to rapidly close the
sim-to-real gap across a variety of perceptual modalities.";Nikhil Mishra<author:sep>Maximilian Sieb<author:sep>Pieter Abbeel<author:sep>Xi Chen;http://arxiv.org/pdf/2403.04114v1;cs.RO;ICRA 2024;nerf
2403.04926v1;http://arxiv.org/abs/2403.04926v1;2024-03-07;BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel  Modeling;"Recent efforts in using 3D Gaussians for scene reconstruction and novel view
synthesis can achieve impressive results on curated benchmarks; however, images
captured in real life are often blurry. In this work, we analyze the robustness
of Gaussian-Splatting-based methods against various image blur, such as motion
blur, defocus blur, downscaling blur, \etc. Under these degradations,
Gaussian-Splatting-based methods tend to overfit and produce worse results than
Neural-Radiance-Field-based methods. To address this issue, we propose Blur
Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling
capacities such that a 3D-consistent and high quality scene can be
reconstructed despite image-wise blur. Specifically, we model blur by
estimating per-pixel convolution kernels from a Blur Proposal Network (BPN).
BPN is designed to consider spatial, color, and depth variations of the scene
to maximize modeling capacity. Additionally, BPN also proposes a
quality-assessing mask, which indicates regions where blur occur. Finally, we
introduce a coarse-to-fine kernel optimization scheme; this optimization scheme
is fast and avoids sub-optimal solutions due to a sparse point cloud
initialization, which often occurs when we apply Structure-from-Motion on
blurry images. We demonstrate that BAGS achieves photorealistic renderings
under various challenging blur conditions and imaging geometry, while
significantly improving upon existing approaches.";Cheng Peng<author:sep>Yutao Tang<author:sep>Yifan Zhou<author:sep>Nengyu Wang<author:sep>Xijun Liu<author:sep>Deming Li<author:sep>Rama Chellappa;http://arxiv.org/pdf/2403.04926v1;cs.CV;;gaussian splatting
2403.04115v2;http://arxiv.org/abs/2403.04115v2;2024-03-07;DNAct: Diffusion Guided Multi-Task 3D Policy Learning;"This paper presents DNAct, a language-conditioned multi-task policy framework
that integrates neural rendering pre-training and diffusion training to enforce
multi-modality learning in action sequence spaces. To learn a generalizable
multi-task policy with few demonstrations, the pre-training phase of DNAct
leverages neural rendering to distill 2D semantic features from foundation
models such as Stable Diffusion to a 3D space, which provides a comprehensive
semantic understanding regarding the scene. Consequently, it allows various
applications to challenging robotic tasks requiring rich 3D semantics and
accurate geometry. Furthermore, we introduce a novel approach utilizing
diffusion training to learn a vision and language feature that encapsulates the
inherent multi-modality in the multi-task demonstrations. By reconstructing the
action sequences from different tasks via the diffusion process, the model is
capable of distinguishing different modalities and thus improving the
robustness and the generalizability of the learned representation. DNAct
significantly surpasses SOTA NeRF-based multi-task manipulation approaches with
over 30% improvement in success rate. Project website: dnact.github.io.";Ge Yan<author:sep>Yueh-Hua Wu<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2403.04115v2;cs.RO;;nerf
2403.03608v1;http://arxiv.org/abs/2403.03608v1;2024-03-06;GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D  Scene Understanding;"Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance
Fields (NeRF) have emerged as a popular research topic in 3D vision. In this
work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),
which uniquely takes image semantics into the synthesis process so that both
novel view images and the associated semantic maps can be produced for unseen
scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and
Depth-Guided Visual rendering. The former is able to observe multi-view image
inputs to extract semantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs both image and
semantic rendering with improved performances. Our experiments not only confirm
that GSNeRF performs favorably against prior works on both novel-view image and
semantic segmentation synthesis but the effectiveness of our sampling strategy
for visual rendering is further verified.";Zi-Ting Chou<author:sep>Sheng-Yu Huang<author:sep>I-Jieh Liu<author:sep>Yu-Chiang Frank Wang;http://arxiv.org/pdf/2403.03608v1;cs.CV;Accepted by CVPR2024;nerf
2403.03241v1;http://arxiv.org/abs/2403.03241v1;2024-03-05;A Deep Learning Framework for Wireless Radiation Field Reconstruction  and Channel Prediction;"We present NeWRF, a deep learning framework for predicting wireless channels.
Wireless channel prediction is a long-standing problem in the wireless
community and is a key technology for improving the coverage of wireless
network deployments. Today, a wireless deployment is evaluated by a site survey
which is a cumbersome process requiring an experienced engineer to perform
extensive channel measurements. To reduce the cost of site surveys, we develop
NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF).
NeWRF trains a neural network model with a sparse set of channel measurements,
and predicts the wireless channel accurately at any location in the site. We
introduce a series of techniques that integrate wireless propagation properties
into the NeRF framework to account for the fundamental differences between the
behavior of light and wireless signals. We conduct extensive evaluations of our
framework and show that our approach can accurately predict channels at
unvisited locations with significantly lower measurement density than prior
state-of-the-art";Haofan Lu<author:sep>Christopher Vattheuer<author:sep>Baharan Mirzasoleiman<author:sep>Omid Abari;http://arxiv.org/pdf/2403.03241v1;cs.NI;;nerf
2403.02751v1;http://arxiv.org/abs/2403.02751v1;2024-03-05;Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps;"We present Splat-Nav, a navigation pipeline that consists of a real-time safe
planning module and a robust state estimation module designed to operate in the
Gaussian Splatting (GSplat) environment representation, a popular emerging 3D
scene representation from computer vision. We formulate rigorous collision
constraints that can be computed quickly to build a guaranteed-safe polytope
corridor through the map. We then optimize a B-spline trajectory through this
corridor. We also develop a real-time, robust state estimation module by
interpreting the GSplat representation as a point cloud. The module enables the
robot to localize its global pose with zero prior knowledge from RGB-D images
using point cloud alignment, and then track its own pose as it moves through
the scene from RGB images using image-to-point cloud localization. We also
incorporate semantics into the GSplat in order to obtain better images for
localization. All of these modules operate mainly on CPU, freeing up GPU
resources for tasks like real-time scene reconstruction. We demonstrate the
safety and robustness of our pipeline in both simulation and hardware, where we
show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude
faster than Neural Radiance Field (NeRF)-based navigation methods, thereby
enabling real-time navigation.";Timothy Chen<author:sep>Ola Shorinwa<author:sep>Weijia Zeng<author:sep>Joseph Bruno<author:sep>Philip Dames<author:sep>Mac Schwager;http://arxiv.org/pdf/2403.02751v1;cs.RO;;gaussian splatting<tag:sep>nerf
2403.02265v1;http://arxiv.org/abs/2403.02265v1;2024-03-04;DaReNeRF: Direction-aware Representation for Dynamic Scenes;"Addressing the intricate challenge of modeling and re-rendering dynamic
scenes, most recent approaches have sought to simplify these complexities using
plane-based explicit representations, overcoming the slow training time issues
associated with methods like Neural Radiance Fields (NeRF) and implicit
representations. However, the straightforward decomposition of 4D dynamic
scenes into multiple 2D plane-based representations proves insufficient for
re-rendering high-fidelity scenes with complex motions. In response, we present
a novel direction-aware representation (DaRe) approach that captures scene
dynamics from six different directions. This learned representation undergoes
an inverse dual-tree complex wavelet transformation (DTCWT) to recover
plane-based information. DaReNeRF computes features for each space-time point
by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny
MLP for color regression and leveraging volume rendering in training yield
state-of-the-art performance in novel view synthesis for complex dynamic
scenes. Notably, to address redundancy introduced by the six real and six
imaginary direction-aware wavelet coefficients, we introduce a trainable
masking approach, mitigating storage issues without significant performance
decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared
to prior art while delivering superior performance.";Ange Lou<author:sep>Benjamin Planche<author:sep>Zhongpai Gao<author:sep>Yamin Li<author:sep>Tianyu Luan<author:sep>Hao Ding<author:sep>Terrence Chen<author:sep>Jack Noble<author:sep>Ziyan Wu;http://arxiv.org/pdf/2403.02265v1;cs.CV;Accepted at CVPR 2024. Paper + supplementary material;nerf
2403.02063v1;http://arxiv.org/abs/2403.02063v1;2024-03-04;Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input  Views;"Novel-view synthesis with sparse input views is important for real-world
applications like AR/VR and autonomous driving. Recent methods have integrated
depth information into NeRFs for sparse input synthesis, leveraging depth prior
for geometric and spatial understanding. However, most existing works tend to
overlook inaccuracies within depth maps and have low time efficiency. To
address these issues, we propose a depth-guided robust and fast point cloud
fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel
grid of features. A point cloud is constructed for each input view,
characterized within the voxel grid using matrices and vectors. We accumulate
the point cloud of each input view to construct the fused point cloud of the
entire scene. Each voxel determines its density and appearance by referring to
the point cloud of the entire scene. Through point cloud fusion and voxel grid
fine-tuning, inaccuracies in depth values are refined or substituted by those
from other views. Moreover, our method can achieve faster reconstruction and
greater compactness through effective vector-matrix decomposition. Experimental
results underline the superior performance and time efficiency of our approach
compared to state-of-the-art baselines.";Shuai Guo<author:sep>Qiuwen Wang<author:sep>Yijie Gao<author:sep>Rong Xie<author:sep>Li Song;http://arxiv.org/pdf/2403.02063v1;cs.CV;;nerf
2403.01137v1;http://arxiv.org/abs/2403.01137v1;2024-03-02;Neural radiance fields-based holography [Invited];"This study presents a novel approach for generating holograms based on the
neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data
is difficult in hologram computation. NeRF is a state-of-the-art technique for
3D light-field reconstruction from 2D images based on volume rendering. The
NeRF can rapidly predict new-view images that do not include a training
dataset. In this study, we constructed a rendering pipeline directly from a 3D
light field generated from 2D images by NeRF for hologram generation using deep
neural networks within a reasonable time. The pipeline comprises three main
components: the NeRF, a depth predictor, and a hologram generator, all
constructed using deep neural networks. The pipeline does not include any
physical calculations. The predicted holograms of a 3D scene viewed from any
direction were computed using the proposed pipeline. The simulation and
experimental results are presented.";Minsung Kang<author:sep>Fan Wang<author:sep>Kai Kumano<author:sep>Tomoyoshi Ito<author:sep>Tomoyoshi Shimobaba;http://arxiv.org/pdf/2403.01137v1;cs.CV;;nerf
2403.01325v1;http://arxiv.org/abs/2403.01325v1;2024-03-02;NeRF-VPT: Learning Novel View Representations with Neural Radiance  Fields via View Prompt Tuning;"Neural Radiance Fields (NeRF) have garnered remarkable success in novel view
synthesis. Nonetheless, the task of generating high-quality images for novel
views persists as a critical challenge. While the existing efforts have
exhibited commendable progress, capturing intricate details, enhancing
textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics
warrant further focused attention and advancement. In this work, we propose
NeRF-VPT, an innovative method for novel view synthesis to address these
challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning
paradigm, wherein RGB information gained from preceding rendering outcomes
serves as instructive visual prompts for subsequent rendering stages, with the
aspiration that the prior knowledge embedded in the prompts can facilitate the
gradual enhancement of rendered image quality. NeRF-VPT only requires sampling
RGB data from previous stage renderings as priors at each training stage,
without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is
plug-and-play and can be readily integrated into existing methods. By
conducting comparative analyses of our NeRF-VPT against several NeRF-based
approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360,
Real Forward-Facing, Replica dataset, and a user-captured dataset, we
substantiate that our NeRF-VPT significantly elevates baseline performance and
proficiently generates more high-quality novel view images than all the
compared state-of-the-art methods. Furthermore, the cascading learning of
NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in
a significant enhancement of accuracy for sparse-view novel view synthesis. The
source code and dataset are available at
\url{https://github.com/Freedomcls/NeRF-VPT}.";Linsheng Chen<author:sep>Guangrun Wang<author:sep>Liuchun Yuan<author:sep>Keze Wang<author:sep>Ken Deng<author:sep>Philip H. S. Torr;http://arxiv.org/pdf/2403.01325v1;cs.CV;AAAI 2024;nerf
2403.01058v1;http://arxiv.org/abs/2403.01058v1;2024-03-02;Neural Field Classifiers via Target Encoding and Classification Loss;"Neural field methods have seen great progress in various long-standing tasks
in computer vision and computer graphics, including novel view synthesis and
geometry reconstruction. As existing neural field methods try to predict some
coordinate-based continuous target values, such as RGB for Neural Radiance
Field (NeRF), all of these methods are regression models and are optimized by
some regression loss. However, are regression models really better than
classification models for neural field methods? In this work, we try to visit
this very fundamental but overlooked question for neural fields from a machine
learning perspective. We successfully propose a novel Neural Field Classifier
(NFC) framework which formulates existing neural field methods as
classification tasks rather than regression tasks. The proposed NFC can easily
transform arbitrary Neural Field Regressor (NFR) into its classification
variant via employing a novel Target Encoding module and optimizing a
classification loss. By encoding a continuous regression target into a
high-dimensional discrete encoding, we naturally formulate a multi-label
classification task. Extensive experiments demonstrate the impressive
effectiveness of NFC at the nearly free extra computational costs. Moreover,
NFC also shows robustness to sparse inputs, corrupted images, and dynamic
scenes.";Xindi Yang<author:sep>Zeke Xie<author:sep>Xiong Zhou<author:sep>Boyu Liu<author:sep>Buhua Liu<author:sep>Yi Liu<author:sep>Haoran Wang<author:sep>Yunfeng Cai<author:sep>Mingming Sun;http://arxiv.org/pdf/2403.01058v1;cs.CV;"ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables";nerf
2403.00228v1;http://arxiv.org/abs/2403.00228v1;2024-03-01;DISORF: A Distributed Online NeRF Training and Rendering Framework for  Mobile Robots;"We present a framework, DISORF, to enable online 3D reconstruction and
visualization of scenes captured by resource-constrained mobile robots and edge
devices. To address the limited compute capabilities of edge devices and
potentially limited network availability, we design a framework that
efficiently distributes computation between the edge device and remote server.
We leverage on-device SLAM systems to generate posed keyframes and transmit
them to remote servers that can perform high quality 3D reconstruction and
visualization at runtime by leveraging NeRF models. We identify a key challenge
with online NeRF training where naive image sampling strategies can lead to
significant degradation in rendering quality. We propose a novel shifted
exponential frame sampling method that addresses this challenge for online NeRF
training. We demonstrate the effectiveness of our framework in enabling
high-quality real-time reconstruction and visualization of unknown scenes as
they are captured and streamed from cameras in mobile robots and edge devices.";Chunlin Li<author:sep>Ruofan Liang<author:sep>Hanrui Fan<author:sep>Zhengen Zhang<author:sep>Sankeerth Durvasula<author:sep>Nandita Vijaykumar;http://arxiv.org/pdf/2403.00228v1;cs.RO;;nerf
2402.19441v1;http://arxiv.org/abs/2402.19441v1;2024-02-29;3D Gaussian Model for Animation and Texturing;"3D Gaussian Splatting has made a marked impact on neural rendering by
achieving impressive fidelity and performance. Despite this achievement,
however, it is not readily applicable to developing interactive applications.
Real-time applications like XR apps and games require functions such as
animation, UV-mapping, and model editing simultaneously manipulated through the
usage of a 3D model. We propose a modeling that is analogous to typical 3D
models, which we call 3D Gaussian Model (3DGM); it provides a manipulatable
proxy for novel animation and texture transfer. By binding the 3D Gaussians in
texture space and re-projecting them back to world space through implicit shell
mapping, we show how our 3D modeling can serve as a valid rendering methodology
for interactive applications. It is further noted that recently, 3D mesh
reconstruction works have been able to produce high-quality mesh for rendering.
Our work, on the other hand, only requires an approximated geometry for
rendering an object in high fidelity. Applicationwise, we will show that our
proxy-based 3DGM is capable of driving novel animation without animated
training data and texture transferring via UV mapping of the 3D Gaussians. We
believe the result indicates the potential of our work for enabling interactive
applications for 3D Gaussian Splatting.";Xiangzhi Eric Wang<author:sep>Zackary P. T. Sin;http://arxiv.org/pdf/2402.19441v1;cs.GR;;gaussian splatting
2402.18196v1;http://arxiv.org/abs/2402.18196v1;2024-02-28;NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human  Pose Estimation in Top-View Fisheye Images;"Human pose estimation (HPE) in the top-view using fisheye cameras presents a
promising and innovative application domain. However, the availability of
datasets capturing this viewpoint is extremely limited, especially those with
high-quality 2D and 3D keypoint annotations. Addressing this gap, we leverage
the capabilities of Neural Radiance Fields (NeRF) technique to establish a
comprehensive pipeline for generating human pose datasets from existing 2D and
3D datasets, specifically tailored for the top-view fisheye perspective.
Through this pipeline, we create a novel dataset NToP570K (NeRF-powered
Top-view human Pose dataset for fisheye cameras with over 570 thousand images),
and conduct an extensive evaluation of its efficacy in enhancing neural
networks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-B
model achieves an improvement in AP of 33.3 % on our validation set for 2D HPE
after finetuning on our training set. A similarly finetuned HybrIK-Transformer
model gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.";Jingrui Yu<author:sep>Dipankar Nandi<author:sep>Roman Seidel<author:sep>Gangolf Hirtz;http://arxiv.org/pdf/2402.18196v1;cs.CV;;nerf
2402.17115v1;http://arxiv.org/abs/2402.17115v1;2024-02-27;CharNeRF: 3D Character Generation from Concept Art;"3D modeling holds significant importance in the realms of AR/VR and gaming,
allowing for both artistic creativity and practical applications. However, the
process is often time-consuming and demands a high level of skill. In this
paper, we present a novel approach to create volumetric representations of 3D
characters from consistent turnaround concept art, which serves as the standard
input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been
a game-changer in image-based 3D reconstruction, to the best of our knowledge,
there is no known research that optimizes the pipeline for concept art. To
harness the potential of concept art, with its defined body poses and specific
view angles, we propose encoding it as priors for our model. We train the
network to make use of these priors for various 3D points through a learnable
view-direction-attended multi-head self-attention layer. Additionally, we
demonstrate that a combination of ray sampling and surface sampling enhances
the inference capabilities of our network. Our model is able to generate
high-quality 360-degree views of characters. Subsequently, we provide a simple
guideline to better leverage our model to extract the 3D mesh. It is important
to note that our model's inferencing capabilities are influenced by the
training data's characteristics, primarily focusing on characters with a single
head, two arms, and two legs. Nevertheless, our methodology remains versatile
and adaptable to concept art from diverse subject matters, without imposing any
specific assumptions on the data.";Eddy Chu<author:sep>Yiyang Chen<author:sep>Chedy Raissi<author:sep>Anand Bhojan;http://arxiv.org/pdf/2402.17115v1;cs.CV;;nerf
2402.17292v1;http://arxiv.org/abs/2402.17292v1;2024-02-27;DivAvatar: Diverse 3D Avatar Generation with a Single Prompt;"Text-to-Avatar generation has recently made significant strides due to
advancements in diffusion models. However, most existing work remains
constrained by limited diversity, producing avatars with subtle differences in
appearance for a given text prompt. We design DivAvatar, a novel framework that
generates diverse avatars, empowering 3D creatives with a multitude of distinct
and richly varied 3D avatars from a single text prompt. Different from most
existing work that exploits scene-specific 3D representations such as NeRF,
DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse
avatar generation from simply noise sampling in inference time. DivAvatar has
two key designs that help achieve generation diversity and visual quality. The
first is a noise sampling technique during training phase which is critical in
generating diverse appearances. The second is a semantic-aware zoom mechanism
and a novel depth loss, the former producing appearances of high textual
fidelity by separate fine-tuning of specific body parts and the latter
improving geometry quality greatly by smoothing the generated mesh in the
features space. Extensive experiments show that DivAvatar is highly versatile
in generating avatars of diverse appearances.";Weijing Tao<author:sep>Biwen Lei<author:sep>Kunhao Liu<author:sep>Shijian Lu<author:sep>Miaomiao Cui<author:sep>Xuansong Xie<author:sep>Chunyan Miao;http://arxiv.org/pdf/2402.17292v1;cs.CV;;nerf
2402.17768v1;http://arxiv.org/abs/2402.17768v1;2024-02-27;Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning;"A common failure mode for policies trained with imitation is compounding
execution errors at test time. When the learned policy encounters states that
were not present in the expert demonstrations, the policy fails, leading to
degenerate behavior. The Dataset Aggregation, or DAgger approach to this
problem simply collects more data to cover these failure states. However, in
practice, this is often prohibitively expensive. In this work, we propose
Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without
the cost for eye-in-hand imitation learning problems. Instead of collecting new
samples to cover out-of-distribution states, DMD uses recent advances in
diffusion models to create these samples with diffusion models. This leads to
robust performance from few demonstrations. In experiments conducted for
non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a
success rate of 80% with as few as 8 expert demonstrations, where naive
behavior cloning reaches only 20%. DMD also outperform competing NeRF-based
augmentation schemes by 50%.";Xiaoyu Zhang<author:sep>Matthew Chang<author:sep>Pranav Kumar<author:sep>Saurabh Gupta;http://arxiv.org/pdf/2402.17768v1;cs.RO;"for project website with video, see
  https://sites.google.com/view/diffusion-meets-dagger";nerf
2402.17427v1;http://arxiv.org/abs/2402.17427v1;2024-02-27;VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction;"Existing NeRF-based methods for large scene reconstruction often have
limitations in visual quality and rendering speed. While the recent 3D Gaussian
Splatting works well on small-scale and object-centric scenes, scaling it up to
large scenes poses challenges due to limited video memory, long optimization
time, and noticeable appearance variations. To address these challenges, we
present VastGaussian, the first method for high-quality reconstruction and
real-time rendering on large scenes based on 3D Gaussian Splatting. We propose
a progressive partitioning strategy to divide a large scene into multiple
cells, where the training cameras and point cloud are properly distributed with
an airspace-aware visibility criterion. These cells are merged into a complete
scene after parallel optimization. We also introduce decoupled appearance
modeling into the optimization process to reduce appearance variations in the
rendered images. Our approach outperforms existing NeRF-based methods and
achieves state-of-the-art results on multiple large scene datasets, enabling
fast optimization and high-fidelity real-time rendering.";Jiaqi Lin<author:sep>Zhihao Li<author:sep>Xiao Tang<author:sep>Jianzhuang Liu<author:sep>Shiyong Liu<author:sep>Jiayue Liu<author:sep>Yangdi Lu<author:sep>Xiaofei Wu<author:sep>Songcen Xu<author:sep>Youliang Yan<author:sep>Wenming Yang;http://arxiv.org/pdf/2402.17427v1;cs.CV;"Accepted to CVPR 2024. Project website:
  https://vastgaussian.github.io";gaussian splatting<tag:sep>nerf
2402.17364v1;http://arxiv.org/abs/2402.17364v1;2024-02-27;Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis;"Recent works in implicit representations, such as Neural Radiance Fields
(NeRF), have advanced the generation of realistic and animatable head avatars
from video sequences. These implicit methods are still confronted by visual
artifacts and jitters, since the lack of explicit geometric constraints poses a
fundamental challenge in accurately modeling complex facial deformations. In
this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid
representation that encodes explicit dynamic meshes by neural networks to
ensure geometric consistency across various motions and viewpoints. DynTet is
parameterized by the coordinate-based networks which learn signed distance,
deformation, and material texture, anchoring the training data into a
predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently
decodes textured meshes with a consistent topology, enabling fast rendering
through a differentiable rasterizer and supervision via a pixel loss. To
enhance training efficiency, we incorporate classical 3D Morphable Models to
facilitate geometry learning and define a canonical space for simplifying
texture learning. These advantages are readily achievable owing to the
effective geometric representation employed in DynTet. Compared with prior
works, DynTet demonstrates significant improvements in fidelity, lip
synchronization, and real-time performance according to various metrics. Beyond
producing stable and visually appealing synthesis videos, our method also
outputs the dynamic meshes which is promising to enable many emerging
applications.";Zicheng Zhang<author:sep>Ruobing Zheng<author:sep>Ziwen Liu<author:sep>Congying Han<author:sep>Tianqi Li<author:sep>Meng Wang<author:sep>Tiande Guo<author:sep>Jingdong Chen<author:sep>Bonan Li<author:sep>Ming Yang;http://arxiv.org/pdf/2402.17364v1;cs.CV;CVPR 2024;nerf
2402.16407v1;http://arxiv.org/abs/2402.16407v1;2024-02-26;CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency;"Neural Radiance Field (NeRF) has shown impressive results in novel view
synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR),
thanks to its ability to represent scenes continuously. However, when just a
few input view images are available, NeRF tends to overfit the given views and
thus make the estimated depths of pixels share almost the same value. Unlike
previous methods that conduct regularization by introducing complex priors or
additional supervisions, we propose a simple yet effective method that
explicitly builds depth-aware consistency across input views to tackle this
challenge. Our key insight is that by forcing the same spatial points to be
sampled repeatedly in different input views, we are able to strengthen the
interactions between views and therefore alleviate the overfitting problem. To
achieve this, we build the neural networks on layered representations
(\textit{i.e.}, multiplane images), and the sampling point can thus be
resampled on multiple discrete planes. Furthermore, to regularize the unseen
target views, we constrain the rendered colors and depths from different input
views to be the same. Although simple, extensive experiments demonstrate that
our proposed method can achieve better synthesis quality over state-of-the-art
methods.";Hanxin Zhu<author:sep>Tianyu He<author:sep>Zhibo Chen;http://arxiv.org/pdf/2402.16407v1;cs.CV;"Accepted by IEEE Conference on Virtual Reality and 3D User Interfaces
  (IEEE VR 2024)";nerf
2402.16308v1;http://arxiv.org/abs/2402.16308v1;2024-02-26;DreamUp3D: Object-Centric Generative Models for Single-View 3D Scene  Understanding and Real-to-Sim Transfer;"3D scene understanding for robotic applications exhibits a unique set of
requirements including real-time inference, object-centric latent
representation learning, accurate 6D pose estimation and 3D reconstruction of
objects. Current methods for scene understanding typically rely on a
combination of trained models paired with either an explicit or learnt
volumetric representation, all of which have their own drawbacks and
limitations. We introduce DreamUp3D, a novel Object-Centric Generative Model
(OCGM) designed explicitly to perform inference on a 3D scene informed only by
a single RGB-D image. DreamUp3D is a self-supervised model, trained end-to-end,
and is capable of segmenting objects, providing 3D object reconstructions,
generating object-centric latent representations and accurate per-object 6D
pose estimates. We compare DreamUp3D to baselines including NeRFs, pre-trained
CLIP-features, ObSurf, and ObPose, in a range of tasks including 3D scene
reconstruction, object matching and object pose estimation. Our experiments
show that our model outperforms all baselines by a significant margin in
real-world scenarios displaying its applicability for 3D scene understanding
tasks while meeting the strict demands exhibited in robotics applications.";Yizhe Wu<author:sep>Haitz Sáez de Ocáriz Borde<author:sep>Jack Collins<author:sep>Oiwi Parker Jones<author:sep>Ingmar Posner;http://arxiv.org/pdf/2402.16308v1;cs.RO;;nerf
2402.17797v2;http://arxiv.org/abs/2402.17797v2;2024-02-26;Neural Radiance Fields in Medical Imaging: Challenges and Next Steps;"Neural Radiance Fields (NeRF), as a pioneering technique in computer vision,
offer great potential to revolutionize medical imaging by synthesizing
three-dimensional representations from the projected two-dimensional image
data. However, they face unique challenges when applied to medical
applications. This paper presents a comprehensive examination of applications
of NeRFs in medical imaging, highlighting four imminent challenges, including
fundamental imaging principles, inner structure requirement, object boundary
definition, and color density significance. We discuss current methods on
different organs and discuss related limitations. We also review several
datasets and evaluation metrics and propose several promising directions for
future research.";Xin Wang<author:sep>Shu Hu<author:sep>Heng Fan<author:sep>Hongtu Zhu<author:sep>Xin Li;http://arxiv.org/pdf/2402.17797v2;eess.IV;;nerf
2402.16936v1;http://arxiv.org/abs/2402.16936v1;2024-02-26;Disentangled 3D Scene Generation with Layout Learning;"We introduce a method to generate 3D scenes that are disentangled into their
component objects. This disentanglement is unsupervised, relying only on the
knowledge of a large pretrained text-to-image model. Our key insight is that
objects can be discovered by finding parts of a 3D scene that, when rearranged
spatially, still produce valid configurations of the same scene. Concretely,
our method jointly optimizes multiple NeRFs from scratch - each representing
its own object - along with a set of layouts that composite these objects into
scenes. We then encourage these composited scenes to be in-distribution
according to the image generator. We show that despite its simplicity, our
approach successfully generates 3D scenes decomposed into individual objects,
enabling new capabilities in text-to-3D content creation. For results and an
interactive demo, see our project page at https://dave.ml/layoutlearning/";Dave Epstein<author:sep>Ben Poole<author:sep>Ben Mildenhall<author:sep>Alexei A. Efros<author:sep>Aleksander Holynski;http://arxiv.org/pdf/2402.16936v1;cs.CV;;nerf
2402.16366v1;http://arxiv.org/abs/2402.16366v1;2024-02-26;SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field;"Representing the Neural Radiance Field (NeRF) with the explicit voxel grid
(EVG) is a promising direction for improving NeRFs. However, the EVG
representation is not efficient for storage and transmission because of the
terrific memory cost. Current methods for compressing EVG mainly inherit the
methods designed for neural network compression, such as pruning and
quantization, which do not take full advantage of the spatial correlation of
voxels. Inspired by prosperous digital image compression techniques, this paper
proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG
compression. The proposed framework can remove spatial redundancy efficiently
for better compression performance.Moreover, we model the bitrate and design a
novel form of the loss function, where we can jointly optimize compression
ratio and distortion to achieve higher coding efficiency. Extensive experiments
demonstrate that our method can achieve 32% bit saving compared to the
state-of-the-art method VQRF on multiple representative test datasets, with
comparable training time.";Zetian Song<author:sep>Wenhong Duan<author:sep>Yuhuai Zhang<author:sep>Shiqi Wang<author:sep>Siwei Ma<author:sep>Wen Gao;http://arxiv.org/pdf/2402.16366v1;cs.CV;;nerf
2402.15870v1;http://arxiv.org/abs/2402.15870v1;2024-02-24;Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian  Splatting;"The recent advancements in 3D Gaussian splatting (3D-GS) have not only
facilitated real-time rendering through modern GPU rasterization pipelines but
have also attained state-of-the-art rendering quality. Nevertheless, despite
its exceptional rendering quality and performance on standard datasets, 3D-GS
frequently encounters difficulties in accurately modeling specular and
anisotropic components. This issue stems from the limited ability of spherical
harmonics (SH) to represent high-frequency information. To overcome this
challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic
spherical Gaussian (ASG) appearance field instead of SH for modeling the
view-dependent appearance of each 3D Gaussian. Additionally, we have developed
a coarse-to-fine training strategy to improve learning efficiency and eliminate
floaters caused by overfitting in real-world scenes. Our experimental results
demonstrate that our method surpasses existing approaches in terms of rendering
quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to
model scenes with specular and anisotropic components without increasing the
number of 3D Gaussians. This improvement extends the applicability of 3D GS to
handle intricate scenarios with specular and anisotropic surfaces.";Ziyi Yang<author:sep>Xinyu Gao<author:sep>Yangtian Sun<author:sep>Yihua Huang<author:sep>Xiaoyang Lyu<author:sep>Wen Zhou<author:sep>Shaohui Jiao<author:sep>Xiaojuan Qi<author:sep>Xiaogang Jin;http://arxiv.org/pdf/2402.15870v1;cs.CV;;gaussian splatting
2402.14196v1;http://arxiv.org/abs/2402.14196v1;2024-02-22;Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields;"Despite the remarkable achievements of neural radiance fields (NeRF) in
representing 3D scenes and generating novel view images, the aliasing issue,
rendering ""jaggies"" or ""blurry"" images at varying camera distances, remains
unresolved in most existing approaches. The recently proposed mip-NeRF has
addressed this challenge by rendering conical frustums instead of rays.
However, it relies on MLP architecture to represent the radiance fields,
missing out on the fast training speed offered by the latest grid-based
methods. In this work, we present mip-Grid, a novel approach that integrates
anti-aliasing techniques into grid-based representations for radiance fields,
mitigating the aliasing artifacts while enjoying fast training time. The
proposed method generates multi-scale grids by applying simple convolution
operations over a shared grid representation and uses the scale-aware
coordinate to retrieve features at different scales from the generated
multi-scale grids. To test the effectiveness, we integrated the proposed method
into the two recent representative grid-based methods, TensoRF and K-Planes.
Experimental results demonstrate that mip-Grid greatly improves the rendering
performance of both methods and even outperforms mip-NeRF on multi-scale
datasets while achieving significantly faster training time. For code and demo
videos, please see https://stnamjef.github.io/mipgrid.github.io/.";Seungtae Nam<author:sep>Daniel Rho<author:sep>Jong Hwan Ko<author:sep>Eunbyung Park;http://arxiv.org/pdf/2402.14196v1;cs.CV;Accepted to NeurIPS 2023;nerf
2402.14415v1;http://arxiv.org/abs/2402.14415v1;2024-02-22;TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via  Direct Taylor-based Grid Optimization;"Coordinate-based neural implicit representation or implicit fields have been
widely studied for 3D geometry representation or novel view synthesis.
Recently, a series of efforts have been devoted to accelerating the speed and
improving the quality of the coordinate-based implicit field learning. Instead
of learning heavy MLPs to predict the neural implicit values for the query
coordinates, neural voxels or grids combined with shallow MLPs have been
proposed to achieve high-quality implicit field learning with reduced
optimization time. On the other hand, lightweight field representations such as
linear grid have been proposed to further improve the learning speed. In this
paper, we aim for both fast and high-quality implicit field learning, and
propose TaylorGrid, a novel implicit field representation which can be
efficiently computed via direct Taylor expansion optimization on 2D or 3D
grids. As a general representation, TaylorGrid can be adapted to different
implicit fields learning tasks such as SDF learning or NeRF. From extensive
quantitative and qualitative comparisons, TaylorGrid achieves a balance between
the linear grid and neural voxels, showing its superiority in fast and
high-quality implicit field learning.";Renyi Mao<author:sep>Qingshan Xu<author:sep>Peng Zheng<author:sep>Ye Wang<author:sep>Tieru Wu<author:sep>Rui Ma;http://arxiv.org/pdf/2402.14415v1;cs.CV;;nerf
2402.14650v1;http://arxiv.org/abs/2402.14650v1;2024-02-22;GaussianPro: 3D Gaussian Splatting with Progressive Propagation;"The advent of 3D Gaussian Splatting (3DGS) has recently brought about a
revolution in the field of neural rendering, facilitating high-quality
renderings at real-time speed. However, 3DGS heavily depends on the initialized
point cloud produced by Structure-from-Motion (SfM) techniques. When tackling
with large-scale scenes that unavoidably contain texture-less surfaces, the SfM
techniques always fail to produce enough points in these surfaces and cannot
provide good initialization for 3DGS. As a result, 3DGS suffers from difficult
optimization and low-quality renderings. In this paper, inspired by classical
multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that
applies a progressive propagation strategy to guide the densification of the 3D
Gaussians. Compared to the simple split and clone strategies used in 3DGS, our
method leverages the priors of the existing reconstructed geometries of the
scene and patch matching techniques to produce new Gaussians with accurate
positions and orientations. Experiments on both large-scale and small-scale
scenes validate the effectiveness of our method, where our method significantly
surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in
terms of PSNR.";Kai Cheng<author:sep>Xiaoxiao Long<author:sep>Kaizhi Yang<author:sep>Yao Yao<author:sep>Wei Yin<author:sep>Yuexin Ma<author:sep>Wenping Wang<author:sep>Xuejin Chen;http://arxiv.org/pdf/2402.14650v1;cs.CV;"See the project page for code, data:
  https://kcheng1021.github.io/gaussianpro.github.io";gaussian splatting
2402.14464v1;http://arxiv.org/abs/2402.14464v1;2024-02-22;NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth  Supervision for Indoor Multi-View 3D Detection;"NeRF-Det has achieved impressive performance in indoor multi-view 3D
detection by innovatively utilizing NeRF to enhance representation learning.
Despite its notable performance, we uncover three decisive shortcomings in its
current design, including semantic ambiguity, inappropriate sampling, and
insufficient utilization of depth supervision. To combat the aforementioned
problems, we present three corresponding solutions: 1) Semantic Enhancement. We
project the freely available 3D segmentation annotations onto the 2D plane and
leverage the corresponding 2D semantic maps as the supervision signal,
significantly enhancing the semantic awareness of multi-view detectors. 2)
Perspective-aware Sampling. Instead of employing the uniform sampling strategy,
we put forward the perspective-aware sampling policy that samples densely near
the camera while sparsely in the distance, more effectively collecting the
valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to
directly regressing the depth values that are difficult to optimize, we divide
the depth range of each scene into a fixed number of ordinal bins and
reformulate the depth prediction as the combination of the classification of
depth bins as well as the regression of the residual depth values, thereby
benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has
exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets.
Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9%
in mAP@0.25 and +3.5% in mAP@0.50. The code will be publicly at
https://github.com/mrsempress/NeRF-Detplusplus.";Chenxi Huang<author:sep>Yuenan Hou<author:sep>Weicai Ye<author:sep>Di Huang<author:sep>Xiaoshui Huang<author:sep>Binbin Lin<author:sep>Deng Cai<author:sep>Wanli Ouyang;http://arxiv.org/pdf/2402.14464v1;cs.CV;7 pages, 2 figures;nerf
2402.14792v1;http://arxiv.org/abs/2402.14792v1;2024-02-22;Consolidating Attention Features for Multi-view Image Editing;"Large-scale text-to-image models enable a wide range of image editing
techniques, using text prompts or even spatial controls. However, applying
these editing methods to multi-view images depicting a single scene leads to
3D-inconsistent results. In this work, we focus on spatial control-based
geometric manipulations and introduce a method to consolidate the editing
process across various views. We build on two insights: (1) maintaining
consistent features throughout the generative process helps attain consistency
in multi-view editing, and (2) the queries in self-attention layers
significantly influence the image structure. Hence, we propose to improve the
geometric consistency of the edited images by enforcing the consistency of the
queries. To do so, we introduce QNeRF, a neural radiance field trained on the
internal query features of the edited images. Once trained, QNeRF can render
3D-consistent queries, which are then softly injected back into the
self-attention layers during generation, greatly improving multi-view
consistency. We refine the process through a progressive, iterative method that
better consolidates queries across the diffusion timesteps. We compare our
method to a range of existing techniques and demonstrate that it can achieve
better multi-view consistency and higher fidelity to the input scene. These
advantages allow us to train NeRFs with fewer visual artifacts, that are better
aligned with the target geometry.";Or Patashnik<author:sep>Rinon Gal<author:sep>Daniel Cohen-Or<author:sep>Jun-Yan Zhu<author:sep>Fernando De la Torre;http://arxiv.org/pdf/2402.14792v1;cs.CV;"Project Page at
  https://qnerf-consolidation.github.io/qnerf-consolidation/";nerf
2402.14586v2;http://arxiv.org/abs/2402.14586v2;2024-02-22;FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View  Synthesis;"We present a novel framework, called FrameNeRF, designed to apply
off-the-shelf fast high-fidelity NeRF models with fast training speed and high
rendering quality for few-shot novel view synthesis tasks. The training
stability of fast high-fidelity models is typically constrained to dense views,
making them unsuitable for few-shot novel view synthesis tasks. To address this
limitation, we utilize a regularization model as a data generator to produce
dense views from sparse inputs, facilitating subsequent training of fast
high-fidelity models. Since these dense views are pseudo ground truth generated
by the regularization model, original sparse images are then used to fine-tune
the fast high-fidelity model. This process helps the model learn realistic
details and correct artifacts introduced in earlier stages. By leveraging an
off-the-shelf regularization model and a fast high-fidelity model, our approach
achieves state-of-the-art performance across various benchmark datasets.";Yan Xing<author:sep>Pan Wang<author:sep>Ligang Liu<author:sep>Daolun Li<author:sep>Li Zhang;http://arxiv.org/pdf/2402.14586v2;cs.CV;;nerf
2402.13510v1;http://arxiv.org/abs/2402.13510v1;2024-02-21;SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural  Radiance Fields;"The widespread adoption of implicit neural representations, especially Neural
Radiance Fields (NeRF), highlights a growing need for editing capabilities in
implicit 3D models, essential for tasks like scene post-processing and 3D
content creation. Despite previous efforts in NeRF editing, challenges remain
due to limitations in editing flexibility and quality. The key issue is
developing a neural representation that supports local edits for real-time
updates. Current NeRF editing methods, offering pixel-level adjustments or
detailed geometry and color modifications, are mostly limited to static scenes.
This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level
editing in dynamic settings, specifically targeting the D-NeRF network. It
allows for consistent edits across sequences by mapping editing actions to a
specific timeframe, freezing the deformation network responsible for dynamic
scene representation, and using a teacher-student approach to integrate
changes.";Zhentao Huang<author:sep>Yukun Shi<author:sep>Neil Bruce<author:sep>Minglun Gong;http://arxiv.org/pdf/2402.13510v1;cs.CV;;nerf
2402.13827v1;http://arxiv.org/abs/2402.13827v1;2024-02-21;Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering  of 3D Gaussian Splatting;"3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms
the neural radiance field (NeRF) in terms of both speed and image quality.
3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects
these Gaussians onto the 2D image plane for rendering. However, during the
rendering process, a substantial number of unnecessary 3D Gaussians exist for
the current view direction, resulting in significant computation costs
associated with their identification. In this paper, we propose a computational
reduction technique that quickly identifies unnecessary 3D Gaussians in
real-time for rendering the current view without compromising image quality.
This is accomplished through the offline clustering of 3D Gaussians that are
close in distance, followed by the projection of these clusters onto a 2D image
plane during runtime. Additionally, we analyze the bottleneck associated with
the proposed technique when executed on GPUs and propose an efficient hardware
architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360
dataset, the proposed technique excludes 63% of 3D Gaussians on average before
the 2D image projection, which reduces the overall rendering computation by
almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The
proposed accelerator also achieves a speedup of 10.7x compared to a GPU.";Joongho Jo<author:sep>Hyeongwon Kim<author:sep>Jongsun Park;http://arxiv.org/pdf/2402.13827v1;cs.CV;;gaussian splatting<tag:sep>nerf
2402.12792v1;http://arxiv.org/abs/2402.12792v1;2024-02-20;OccFlowNet: Towards Self-supervised Occupancy Estimation via  Differentiable Rendering and Occupancy Flow;"Semantic occupancy has recently gained significant traction as a prominent 3D
scene representation. However, most existing methods rely on large and costly
datasets with fine-grained 3D voxel labels for training, which limits their
practicality and scalability, increasing the need for self-monitored learning
in this domain. In this work, we present a novel approach to occupancy
estimation inspired by neural radiance field (NeRF) using only 2D labels, which
are considerably easier to acquire. In particular, we employ differentiable
volumetric rendering to predict depth and semantic maps and train a 3D network
based on 2D supervision only. To enhance geometric accuracy and increase the
supervisory signal, we introduce temporal rendering of adjacent time steps.
Additionally, we introduce occupancy flow as a mechanism to handle dynamic
objects in the scene and ensure their temporal consistency. Through extensive
experimentation we demonstrate that 2D supervision only is sufficient to
achieve state-of-the-art performance compared to methods using 3D labels, while
outperforming concurrent 2D approaches. When combining 2D supervision with 3D
labels, temporal rendering and occupancy flow we outperform all previous
occupancy estimation models significantly. We conclude that the proposed
rendering supervision and occupancy flow advances occupancy estimation and
further bridges the gap towards self-supervised learning in this domain.";Simon Boeder<author:sep>Fabian Gigengack<author:sep>Benjamin Risse;http://arxiv.org/pdf/2402.12792v1;cs.CV;;nerf
2402.13255v1;http://arxiv.org/abs/2402.13255v1;2024-02-20;How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey;"Over the past two decades, research in the field of Simultaneous Localization
and Mapping (SLAM) has undergone a significant evolution, highlighting its
critical role in enabling autonomous exploration of unknown environments. This
evolution ranges from hand-crafted methods, through the era of deep learning,
to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D
Gaussian Splatting (3DGS) representations. Recognizing the growing body of
research and the absence of a comprehensive survey on the topic, this paper
aims to provide the first comprehensive overview of SLAM progress through the
lens of the latest advancements in radiance fields. It sheds light on the
background, evolutionary path, inherent strengths and limitations, and serves
as a fundamental reference to highlight the dynamic progress and specific
challenges.";Fabio Tosi<author:sep>Youmin Zhang<author:sep>Ziren Gong<author:sep>Erik Sandström<author:sep>Stefano Mattoccia<author:sep>Martin R. Oswald<author:sep>Matteo Poggi;http://arxiv.org/pdf/2402.13255v1;cs.CV;;gaussian splatting<tag:sep>nerf
2402.13252v1;http://arxiv.org/abs/2402.13252v1;2024-02-20;Improving Robustness for Joint Optimization of Camera Poses and  Decomposed Low-Rank Tensorial Radiance Fields;"In this paper, we propose an algorithm that allows joint refinement of camera
pose and scene geometry represented by decomposed low-rank tensor, using only
2D images as supervision. First, we conduct a pilot study based on a 1D signal
and relate our findings to 3D scenarios, where the naive joint pose
optimization on voxel-based NeRFs can easily lead to sub-optimal solutions.
Moreover, based on the analysis of the frequency spectrum, we propose to apply
convolutional Gaussian filters on 2D and 3D radiance fields for a
coarse-to-fine training schedule that enables joint camera pose optimization.
Leveraging the decomposition property in decomposed low-rank tensor, our method
achieves an equivalent effect to brute-force 3D convolution with only incurring
little computational overhead. To further improve the robustness and stability
of joint optimization, we also propose techniques of smoothed 2D supervision,
randomly scaled kernel parameters, and edge-guided loss mask. Extensive
quantitative and qualitative evaluations demonstrate that our proposed
framework achieves superior performance in novel view synthesis as well as
rapid convergence for optimization.";Bo-Yu Cheng<author:sep>Wei-Chen Chiu<author:sep>Yu-Lun Liu;http://arxiv.org/pdf/2402.13252v1;cs.CV;"AAAI 2024. Project page:
  https://alex04072000.github.io/Joint-TensoRF/";nerf
2402.13226v2;http://arxiv.org/abs/2402.13226v2;2024-02-20;NeRF Solves Undersampled MRI Reconstruction;"This article presents a novel undersampled magnetic resonance imaging (MRI)
technique that leverages the concept of Neural Radiance Field (NeRF). With
radial undersampling, the corresponding imaging problem can be reformulated
into an image modeling task from sparse-view rendered data; therefore, a high
dimensional MR image is obtainable from undersampled k-space data by taking
advantage of implicit neural representation. A multi-layer perceptron, which is
designed to output an image intensity from a spatial coordinate, learns the MR
physics-driven rendering relation between given measurement data and desired
image. Effective undersampling strategies for high-quality neural
representation are investigated. The proposed method serves two benefits: (i)
The learning is based fully on single undersampled k-space data, not a bunch of
measured data and target image sets. It can be used potentially for diagnostic
MR imaging, such as fetal MRI, where data acquisition is relatively rare or
limited against diversity of clinical images while undersampled reconstruction
is highly demanded. (ii) A reconstructed MR image is a scan-specific
representation highly adaptive to the given k-space measurement. Numerous
experiments validate the feasibility and capability of the proposed approach.";Tae Jun Jang<author:sep>Chang Min Hyun;http://arxiv.org/pdf/2402.13226v2;eess.IV;;nerf
2402.12377v1;http://arxiv.org/abs/2402.12377v1;2024-02-19;Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based  View Synthesis;"While surface-based view synthesis algorithms are appealing due to their low
computational requirements, they often struggle to reproduce thin structures.
In contrast, more expensive methods that model the scene's geometry as a
volumetric density field (e.g. NeRF) excel at reconstructing fine geometric
detail. However, density fields often represent geometry in a ""fuzzy"" manner,
which hinders exact localization of the surface. In this work, we modify
density fields to encourage them to converge towards surfaces, without
compromising their ability to reconstruct thin structures. First, we employ a
discrete opacity grid representation instead of a continuous density field,
which allows opacity values to discontinuously transition from zero to one at
the surface. Second, we anti-alias by casting multiple rays per pixel, which
allows occlusion boundaries and subpixel structures to be modelled without
using semi-transparent voxels. Third, we minimize the binary entropy of the
opacity values, which facilitates the extraction of surface geometry by
encouraging opacity values to binarize towards the end of training. Lastly, we
develop a fusion-based meshing strategy followed by mesh simplification and
appearance model fitting. The compact meshes produced by our model can be
rendered in real-time on mobile devices and achieve significantly higher view
synthesis quality compared to existing mesh-based approaches.";Christian Reiser<author:sep>Stephan Garbin<author:sep>Pratul P. Srinivasan<author:sep>Dor Verbin<author:sep>Richard Szeliski<author:sep>Ben Mildenhall<author:sep>Jonathan T. Barron<author:sep>Peter Hedman<author:sep>Andreas Geiger;http://arxiv.org/pdf/2402.12377v1;cs.CV;Project page at https://binary-opacity-grid.github.io;nerf
2402.12184v1;http://arxiv.org/abs/2402.12184v1;2024-02-19;Colorizing Monochromatic Radiance Fields;"Though Neural Radiance Fields (NeRF) can produce colorful 3D representations
of the world by using a set of 2D images, such ability becomes non-existent
when only monochromatic images are provided. Since color is necessary in
representing the world, reproducing color from monochromatic radiance fields
becomes crucial. To achieve this goal, instead of manipulating the
monochromatic radiance fields directly, we consider it as a
representation-prediction task in the Lab color space. By first constructing
the luminance and density representation using monochromatic images, our
prediction stage can recreate color representation on the basis of an image
colorization module. We then reproduce a colorful implicit model through the
representation of luminance, density, and color. Extensive experiments have
been conducted to validate the effectiveness of our approaches. Our project
page: https://liquidammonia.github.io/color-nerf.";Yean Cheng<author:sep>Renjie Wan<author:sep>Shuchen Weng<author:sep>Chengxuan Zhu<author:sep>Yakun Chang<author:sep>Boxin Shi;http://arxiv.org/pdf/2402.12184v1;cs.CV;;nerf
2402.11141v1;http://arxiv.org/abs/2402.11141v1;2024-02-17;Semantically-aware Neural Radiance Fields for Visual Scene  Understanding: A Comprehensive Review;"This review thoroughly examines the role of semantically-aware Neural
Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of
over 250 scholarly papers. It explores how NeRFs adeptly infer 3D
representations for both stationary and dynamic objects in a scene. This
capability is pivotal for generating high-quality new viewpoints, completing
missing scene details (inpainting), conducting comprehensive scene segmentation
(panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and
extracting object-centric 3D models. A significant aspect of this study is the
application of semantic labels as viewpoint-invariant functions, which
effectively map spatial coordinates to a spectrum of semantic labels, thus
facilitating the recognition of distinct objects within the scene. Overall,
this survey highlights the progression and diverse applications of
semantically-aware neural radiance fields in the context of visual scene
interpretation.";Thang-Anh-Quan Nguyen<author:sep>Amine Bourki<author:sep>Mátyás Macudzinski<author:sep>Anthony Brunel<author:sep>Mohammed Bennamoun;http://arxiv.org/pdf/2402.11141v1;cs.CV;;nerf
2402.10128v1;http://arxiv.org/abs/2402.10128v1;2024-02-15;GES: Generalized Exponential Splatting for Efficient Radiance Field  Rendering;"Advancements in 3D Gaussian Splatting have significantly accelerated 3D
reconstruction and generation. However, it may require a large number of
Gaussians, which creates a substantial memory footprint. This paper introduces
GES (Generalized Exponential Splatting), a novel representation that employs
Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer
particles to represent a scene and thus significantly outperforming Gaussian
Splatting methods in efficiency with a plug-and-play replacement ability for
Gaussian-based utilities. GES is validated theoretically and empirically in
both principled 1D setup and realistic 3D scenes.
  It is shown to represent signals with sharp edges more accurately, which are
typically challenging for Gaussians due to their inherent low-pass
characteristics. Our empirical analysis demonstrates that GEF outperforms
Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and
parabolic signals), thereby reducing the need for extensive splitting
operations that increase the memory footprint of Gaussian Splatting. With the
aid of a frequency-modulated loss, GES achieves competitive performance in
novel-view synthesis benchmarks while requiring less than half the memory
storage of Gaussian Splatting and increasing the rendering speed by up to 39%.
The code is available on the project website https://abdullahamdi.com/ges .";Abdullah Hamdi<author:sep>Luke Melas-Kyriazi<author:sep>Guocheng Qian<author:sep>Jinjie Mai<author:sep>Ruoshi Liu<author:sep>Carl Vondrick<author:sep>Bernard Ghanem<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2402.10128v1;cs.CV;preprint;gaussian splatting
2402.10259v2;http://arxiv.org/abs/2402.10259v2;2024-02-15;GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object  with Gaussian Splatting;"Reconstructing and rendering 3D objects from highly sparse views is of
critical importance for promoting applications of 3D vision techniques and
improving user experience. However, images from sparse views only contain very
limited 3D information, leading to two significant challenges: 1) Difficulty in
building multi-view consistency as images for matching are too few; 2)
Partially omitted or highly compressed object information as view coverage is
insufficient. To tackle these challenges, we propose GaussianObject, a
framework to represent and render the 3D object with Gaussian splatting, that
achieves high rendering quality with only 4 input images. We first introduce
techniques of visual hull and floater elimination which explicitly inject
structure priors into the initial optimization process for helping build
multi-view consistency, yielding a coarse 3D Gaussian representation. Then we
construct a Gaussian repair model based on diffusion models to supplement the
omitted object information, where Gaussians are further refined. We design a
self-generating strategy to obtain image pairs for training the repair model.
Our GaussianObject is evaluated on several challenging datasets, including
MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction
results from only 4 views and significantly outperforming previous
state-of-the-art methods.";Chen Yang<author:sep>Sikuang Li<author:sep>Jiemin Fang<author:sep>Ruofan Liang<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wei Shen<author:sep>Qi Tian;http://arxiv.org/pdf/2402.10259v2;cs.CV;Project page: https://gaussianobject.github.io/;gaussian splatting<tag:sep>nerf
2402.10344v1;http://arxiv.org/abs/2402.10344v1;2024-02-15;Evaluating NeRFs for 3D Plant Geometry Reconstruction in Field  Conditions;"We evaluate different Neural Radiance Fields (NeRFs) techniques for
reconstructing (3D) plants in varied environments, from indoor settings to
outdoor fields. Traditional techniques often struggle to capture the complex
details of plants, which is crucial for botanical and agricultural
understanding. We evaluate three scenarios with increasing complexity and
compare the results with the point cloud obtained using LiDAR as ground truth
data. In the most realistic field scenario, the NeRF models achieve a 74.65% F1
score with 30 minutes of training on the GPU, highlighting the efficiency and
accuracy of NeRFs in challenging environments. These findings not only
demonstrate the potential of NeRF in detailed and realistic 3D plant modeling
but also suggest practical approaches for enhancing the speed and efficiency of
the 3D reconstruction process.";Muhammad Arbab Arshad<author:sep>Talukder Jubery<author:sep>James Afful<author:sep>Anushrut Jignasu<author:sep>Aditya Balu<author:sep>Baskar Ganapathysubramanian<author:sep>Soumik Sarkar<author:sep>Adarsh Krishnamurthy;http://arxiv.org/pdf/2402.10344v1;cs.CV;;nerf
2402.09325v1;http://arxiv.org/abs/2402.09325v1;2024-02-14;PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames  in Autonomous Driving Environments;"Large-scale 3D scene reconstruction and novel view synthesis are vital for
autonomous vehicles, especially utilizing temporally sparse LiDAR frames.
However, conventional explicit representations remain a significant bottleneck
towards representing the reconstructed and synthetic scenes at unlimited
resolution. Although the recently developed neural radiance fields (NeRF) have
shown compelling results in implicit representations, the problem of
large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR
frames remains unexplored. To bridge this gap, we propose a 3D scene
reconstruction and novel view synthesis framework called parent-child neural
radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF,
the framework implements hierarchical spatial partitioning and multi-level
scene representation, including scene, segment, and point levels. The
multi-level scene representation enhances the efficient utilization of sparse
LiDAR point cloud data and enables the rapid acquisition of an approximate
volumetric scene representation. With extensive experiments, PC-NeRF is proven
to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in
large-scale scenes. Moreover, PC-NeRF can effectively handle situations with
sparse LiDAR frames and demonstrate high deployment efficiency with limited
training epochs. Our approach implementation and the pre-trained models are
available at https://github.com/biter0088/pc-nerf.";Xiuzhong Hu<author:sep>Guangming Xiong<author:sep>Zheng Zang<author:sep>Peng Jia<author:sep>Yuxuan Han<author:sep>Junyi Ma;http://arxiv.org/pdf/2402.09325v1;cs.CV;arXiv admin note: substantial text overlap with arXiv:2310.00874;nerf
2402.08622v1;http://arxiv.org/abs/2402.08622v1;2024-02-13;NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs;"A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry
and appearance of a scene. We here ask the question whether we can transfer the
appearance from a source NeRF onto a target 3D geometry in a semantically
meaningful way, such that the resulting new NeRF retains the target geometry
but has an appearance that is an analogy to the source NeRF. To this end, we
generalize classic image analogies from 2D images to NeRFs. We leverage
correspondence transfer along semantic affinity that is driven by semantic
features from large, pre-trained 2D image models to achieve multi-view
consistent appearance transfer. Our method allows exploring the mix-and-match
product space of 3D geometry and appearance. We show that our method
outperforms traditional stylization-based methods and that a large majority of
users prefer our method over several typical baselines.";Michael Fischer<author:sep>Zhengqin Li<author:sep>Thu Nguyen-Phuoc<author:sep>Aljaz Bozic<author:sep>Zhao Dong<author:sep>Carl Marshall<author:sep>Tobias Ritschel;http://arxiv.org/pdf/2402.08622v1;cs.CV;Project page: https://mfischer-ucl.github.io/nerf_analogies/;nerf
2402.08682v1;http://arxiv.org/abs/2402.08682v1;2024-02-13;IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality  3D Generation;"Most text-to-3D generators build upon off-the-shelf text-to-image models
trained on billions of images. They use variants of Score Distillation Sampling
(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation
is to fine-tune the 2D generator to be multi-view aware, which can help
distillation or can be combined with reconstruction networks to output 3D
objects directly. In this paper, we further explore the design space of
text-to-3D models. We significantly improve multi-view generation by
considering video instead of image generators. Combined with a 3D
reconstruction algorithm which, by using Gaussian splatting, can optimize a
robust image-based loss, we directly produce high-quality 3D outputs from the
generated views. Our new method, IM-3D, reduces the number of evaluations of
the 2D generator network 10-100x, resulting in a much more efficient pipeline,
better quality, fewer geometric inconsistencies, and higher yield of usable 3D
assets.";Luke Melas-Kyriazi<author:sep>Iro Laina<author:sep>Christian Rupprecht<author:sep>Natalia Neverova<author:sep>Andrea Vedaldi<author:sep>Oran Gafni<author:sep>Filippos Kokkinos;http://arxiv.org/pdf/2402.08682v1;cs.CV;;gaussian splatting
2402.08784v1;http://arxiv.org/abs/2402.08784v1;2024-02-13;Preconditioners for the Stochastic Training of Implicit Neural  Representations;"Implicit neural representations have emerged as a powerful technique for
encoding complex continuous multidimensional signals as neural networks,
enabling a wide range of applications in computer vision, robotics, and
geometry. While Adam is commonly used for training due to its stochastic
proficiency, it entails lengthy training durations. To address this, we explore
alternative optimization techniques for accelerated training without
sacrificing accuracy. Traditional second-order optimizers like L-BFGS are
suboptimal in stochastic settings, making them unsuitable for large-scale data
sets. Instead, we propose stochastic training using curvature-aware diagonal
preconditioners, showcasing their effectiveness across various signal
modalities such as images, shape reconstruction, and Neural Radiance Fields
(NeRF).";Shin-Fang Chng<author:sep>Hemanth Saratchandran<author:sep>Simon Lucey;http://arxiv.org/pdf/2402.08784v1;cs.CV;The first two authors contributed equally;nerf
2402.08138v1;http://arxiv.org/abs/2402.08138v1;2024-02-13;H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object  Surface Fields;"Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance
Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D
indoor scene reconstruction. We introduce a novel two-phase learning approach,
H2O-SDF, that discriminates between object and non-object regions within indoor
environments. This method achieves a nuanced balance, carefully preserving the
geometric integrity of room layouts while also capturing intricate surface
details of specific objects. A cornerstone of our two-phase learning framework
is the introduction of the Object Surface Field (OSF), a novel concept designed
to mitigate the persistent vanishing gradient problem that has previously
hindered the capture of high-frequency details in other methods. Our proposed
approach is validated through several experiments that include ablation
studies.";Minyoung Park<author:sep>Mirae Do<author:sep>YeonJae Shin<author:sep>Jaeseok Yoo<author:sep>Jongkwang Hong<author:sep>Joongrock Kim<author:sep>Chul Lee;http://arxiv.org/pdf/2402.08138v1;cs.CV;;nerf
2402.07648v1;http://arxiv.org/abs/2402.07648v1;2024-02-12;DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable  Object Manipulation;"Manipulating deformable objects is a ubiquitous task in household
environments, demanding adequate representation and accurate dynamics
prediction due to the objects' infinite degrees of freedom. This work proposes
DeformNet, which utilizes latent space modeling with a learned 3D
representation model to tackle these challenges effectively. The proposed
representation model combines a PointNet encoder and a conditional neural
radiance field (NeRF), facilitating a thorough acquisition of object
deformations and variations in lighting conditions. To model the complex
dynamics, we employ a recurrent state-space model (RSSM) that accurately
predicts the transformation of the latent representation over time. Extensive
simulation experiments with diverse objectives demonstrate the generalization
capabilities of DeformNet for various deformable object manipulation tasks,
even in the presence of previously unseen goals. Finally, we deploy DeformNet
on an actual UR5 robotic arm to demonstrate its capability in real-world
scenarios.";Chenchang Li<author:sep>Zihao Ai<author:sep>Tong Wu<author:sep>Xiaosa Li<author:sep>Wenbo Ding<author:sep>Huazhe Xu;http://arxiv.org/pdf/2402.07648v1;cs.RO;"7 pages, Submitted to 2024 IEEE International Conference on Robotics
  and Automation (ICRA), Japan, Yokohama";nerf
2402.07207v1;http://arxiv.org/abs/2402.07207v1;2024-02-11;GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided  Generative Gaussian Splatting;"We present GALA3D, generative 3D GAussians with LAyout-guided control, for
effective compositional text-to-3D generation. We first utilize large language
models (LLMs) to generate the initial layout and introduce a layout-guided 3D
Gaussian representation for 3D content generation with adaptive geometric
constraints. We then propose an object-scene compositional optimization
mechanism with conditioned diffusion to collaboratively generate realistic 3D
scenes with consistent geometry, texture, scale, and accurate interactions
among multiple objects while simultaneously adjusting the coarse layout priors
extracted from the LLMs to align with the generated scene. Experiments show
that GALA3D is a user-friendly, end-to-end framework for state-of-the-art
scene-level 3D content generation and controllable editing while ensuring the
high fidelity of object-level entities within the scene. Source codes and
models will be available at https://gala3d.github.io/.";Xiaoyu Zhou<author:sep>Xingjian Ran<author:sep>Yajiao Xiong<author:sep>Jinlin He<author:sep>Zhiwei Lin<author:sep>Yongtao Wang<author:sep>Deqing Sun<author:sep>Ming-Hsuan Yang;http://arxiv.org/pdf/2402.07207v1;cs.CV;;gaussian splatting
2402.07310v1;http://arxiv.org/abs/2402.07310v1;2024-02-11;BioNeRF: Biologically Plausible Neural Radiance Fields for View  Synthesis;"This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.";Leandro A. Passos<author:sep>Douglas Rodrigues<author:sep>Danilo Jodas<author:sep>Kelton A. P. Costa<author:sep>João Paulo Papa;http://arxiv.org/pdf/2402.07310v1;cs.CV;;nerf
2402.07181v1;http://arxiv.org/abs/2402.07181v1;2024-02-11;3D Gaussian as a New Vision Era: A Survey;"3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the
field of Computer Graphics, offering explicit scene representation and novel
view synthesis without the reliance on neural networks, such as Neural Radiance
Fields (NeRF). This technique has found diverse applications in areas such as
robotics, urban mapping, autonomous navigation, and virtual reality/augmented
reality, just name a few. Given the growing popularity and expanding research
in 3D Gaussian Splatting, this paper presents a comprehensive survey of
relevant papers from the past year. We organize the survey into taxonomies
based on characteristics and applications, providing an introduction to the
theoretical underpinnings of 3D Gaussian Splatting. Our goal through this
survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a
valuable reference for seminal works in the field, and inspire future research
directions, as discussed in our concluding section.";Ben Fei<author:sep>Jingyi Xu<author:sep>Rui Zhang<author:sep>Qingyuan Zhou<author:sep>Weidong Yang<author:sep>Ying He;http://arxiv.org/pdf/2402.07181v1;cs.CV;;gaussian splatting<tag:sep>nerf
2402.06149v1;http://arxiv.org/abs/2402.06149v1;2024-02-09;HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting;"Creating digital avatars from textual prompts has long been a desirable yet
challenging task. Despite the promising outcomes obtained through 2D diffusion
priors in recent works, current methods face challenges in achieving
high-quality and animated avatars effectively. In this paper, we present
$\textbf{HeadStudio}$, a novel framework that utilizes 3D Gaussian splatting to
generate realistic and animated avatars from text prompts. Our method drives 3D
Gaussians semantically to create a flexible and achievable appearance through
the intermediate FLAME representation. Specifically, we incorporate the FLAME
into both 3D representation and score distillation: 1) FLAME-based 3D Gaussian
splatting, driving 3D Gaussian points by rigging each point to a FLAME mesh. 2)
FLAME-based score distillation sampling, utilizing FLAME-based fine-grained
control signal to guide score distillation from the text prompt. Extensive
experiments demonstrate the efficacy of HeadStudio in generating animatable
avatars from textual prompts, exhibiting visually appealing appearances. The
avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel
views at a resolution of 1024. They can be smoothly controlled by real-world
speech and video. We hope that HeadStudio can advance digital avatar creation
and that the present method can widely be applied across various domains.";Zhenglin Zhou<author:sep>Fan Ma<author:sep>Hehe Fan<author:sep>Yi Yang;http://arxiv.org/pdf/2402.06149v1;cs.CV;9 pages, 8 figures;gaussian splatting
2402.06390v1;http://arxiv.org/abs/2402.06390v1;2024-02-09;ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake  Generation using NeRF and Gaussian Splatting;"Numerous emerging deep-learning techniques have had a substantial impact on
computer graphics. Among the most promising breakthroughs are the recent rise
of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the
object's shape and color in neural network weights using a handful of images
with known camera positions to generate novel views. In contrast, GS provides
accelerated training and inference without a decrease in rendering quality by
encoding the object's characteristics in a collection of Gaussian
distributions. These two techniques have found many use cases in spatial
computing and other domains. On the other hand, the emergence of deepfake
methods has sparked considerable controversy. Such techniques can have a form
of artificial intelligence-generated videos that closely mimic authentic
footage. Using generative models, they can modify facial features, enabling the
creation of altered identities or facial expressions that exhibit a remarkably
realistic appearance to a real person. Despite these controversies, deepfake
can offer a next-generation solution for avatar creation and gaming when of
desirable quality. To that end, we show how to combine all these emerging
technologies to obtain a more plausible outcome. Our ImplicitDeepfake1 uses the
classical deepfake algorithm to modify all training images separately and then
train NeRF and GS on modified faces. Such relatively simple strategies can
produce plausible 3D deepfake-based avatars.";Georgii Stanishevskii<author:sep>Jakub Steczkiewicz<author:sep>Tomasz Szczepanik<author:sep>Sławomir Tadeja<author:sep>Jacek Tabor<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2402.06390v1;cs.CV;;gaussian splatting<tag:sep>nerf
2402.06198v2;http://arxiv.org/abs/2402.06198v2;2024-02-09;GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D  Pretraining from Real-World Data;"3D Shape represented as point cloud has achieve advancements in multimodal
pre-training to align image and language descriptions, which is curial to
object identification, classification, and retrieval. However, the discrete
representations of point cloud lost the object's surface shape information and
creates a gap between rendering results and 2D correspondences. To address this
problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D
Gaussian Splatting) into multimodal pre-training to enhance 3D representation.
GS-CLIP leverages a pre-trained vision-language model for a learned common
visual and textual space on massive real world image-text pairs and then learns
a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel
Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature.
As a general framework for language-image-3D pre-training, GS-CLIP is agnostic
to 3D backbone networks. Experiments on challenging shows that GS-CLIP
significantly improves the state-of-the-art, outperforming the previously best
results.";Haoyuan Li<author:sep>Yanpeng Zhou<author:sep>Yihan Zeng<author:sep>Hang Xu<author:sep>Xiaodan Liang;http://arxiv.org/pdf/2402.06198v2;cs.CV;"The content of the technical report needs to be updated and retracted
  to avoid other impacts";gaussian splatting
2402.04648v1;http://arxiv.org/abs/2402.04648v1;2024-02-07;OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language  Foundation Models for 3D Semantic Understanding;"The development of Neural Radiance Fields (NeRFs) has provided a potent
representation for encapsulating the geometric and appearance characteristics
of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D
semantic perception tasks has been a recent focus. However, current methods
that extract semantics directly from Contrastive Language-Image Pretraining
(CLIP) for semantic field learning encounter difficulties due to noisy and
view-inconsistent semantics provided by CLIP. To tackle these limitations, we
propose OV-NeRF, which exploits the potential of pre-trained vision and
language foundation models to enhance semantic field learning through proposed
single-view and cross-view strategies. First, from the single-view perspective,
we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask
proposals derived from SAM to rectify the noisy semantics of each training
view, facilitating accurate semantic field learning. Second, from the
cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy
to address the challenge raised by view-inconsistent semantics. Rather than
invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the
3D consistent semantics generated from the well-trained semantic field itself
for semantic field training, aiming to reduce ambiguity and enhance overall
semantic consistency across different views. Extensive experiments validate our
OV-NeRF outperforms current state-of-the-art methods, achieving a significant
improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet,
respectively. Furthermore, our approach exhibits consistent superior results
across various CLIP configurations, further verifying its robustness.";Guibiao Liao<author:sep>Kaichen Zhou<author:sep>Zhenyu Bao<author:sep>Kanglin Liu<author:sep>Qing Li;http://arxiv.org/pdf/2402.04648v1;cs.CV;;nerf
2402.04554v2;http://arxiv.org/abs/2402.04554v2;2024-02-07;BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial  Imagery;"In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields
(NeRF) designed specifically for reconstructing large-scale scenes using aerial
imagery. Unlike previous research focused on small-scale and object-centric
NeRF reconstruction, our approach addresses multiple challenges, including (1)
Addressing the issue of slow training and rendering associated with large
models. (2) Meeting the computational demands necessitated by modeling a
substantial number of images, requiring extensive resources such as
high-performance GPUs. (3) Overcoming significant artifacts and low visual
fidelity commonly observed in large-scale reconstruction tasks due to limited
model capacity. Specifically, we present a novel bird-view pose-based spatial
decomposition algorithm that decomposes a large aerial image set into multiple
small sets with appropriately sized overlaps, allowing us to train individual
NeRFs of sub-scene. This decomposition approach not only decouples rendering
time from the scene size but also enables rendering to scale seamlessly to
arbitrarily large environments. Moreover, it allows for per-block updates of
the environment, enhancing the flexibility and adaptability of the
reconstruction process. Additionally, we propose a projection-guided novel view
re-rendering strategy, which aids in effectively utilizing the independently
trained sub-scenes to generate superior rendering results. We evaluate our
approach on existing datasets as well as against our own drone footage,
improving reconstruction speed by 10x over classical photogrammetry software
and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with
similar rendering quality.";Huiqing Zhang<author:sep>Yifei Xue<author:sep>Ming Liao<author:sep>Yizhen Lao;http://arxiv.org/pdf/2402.04554v2;cs.CV;;nerf
2402.04829v1;http://arxiv.org/abs/2402.04829v1;2024-02-07;NeRF as Non-Distant Environment Emitter in Physics-based Inverse  Rendering;"Physics-based inverse rendering aims to jointly optimize shape, materials,
and lighting from captured 2D images. Here lighting is an important part of
achieving faithful light transport simulation. While the environment map is
commonly used as the lighting model in inverse rendering, we show that its
distant lighting assumption leads to spatial invariant lighting, which can be
an inaccurate approximation in real-world inverse rendering. We propose to use
NeRF as a spatially varying environment lighting model and build an inverse
rendering pipeline using NeRF as the non-distant environment emitter. By
comparing our method with the environment map on real and synthetic datasets,
we show that our NeRF-based emitter models the scene lighting more accurately
and leads to more accurate inverse rendering. Project page and video:
https://nerfemitterpbir.github.io/.";Jingwang Ling<author:sep>Ruihan Yu<author:sep>Feng Xu<author:sep>Chun Du<author:sep>Shuang Zhao;http://arxiv.org/pdf/2402.04829v1;cs.CV;Project page and video: https://nerfemitterpbir.github.io/;nerf
2402.04796v1;http://arxiv.org/abs/2402.04796v1;2024-02-07;Mesh-based Gaussian Splatting for Real-time Large-scale Deformation;"Neural implicit representations, including Neural Distance Fields and Neural
Radiance Fields, have demonstrated significant capabilities for reconstructing
surfaces with complicated geometry and topology, and generating novel views of
a scene. Nevertheless, it is challenging for users to directly deform or
manipulate these implicit representations with large deformations in the
real-time fashion. Gaussian Splatting(GS) has recently become a promising
method with explicit geometry for representing static scenes and facilitating
high-quality and real-time synthesis of novel views. However,it cannot be
easily deformed due to the use of discrete Gaussians and lack of explicit
topology. To address this, we develop a novel GS-based method that enables
interactive deformation. Our key idea is to design an innovative mesh-based GS
representation, which is integrated into Gaussian learning and manipulation. 3D
Gaussians are defined over an explicit mesh, and they are bound with each
other: the rendering of 3D Gaussians guides the mesh face split for adaptive
refinement, and the mesh face split directs the splitting of 3D Gaussians.
Moreover, the explicit mesh constraints help regularize the Gaussian
distribution, suppressing poor-quality Gaussians(e.g. misaligned
Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and
avoiding artifacts during deformation. Based on this representation, we further
introduce a large-scale Gaussian deformation technique to enable deformable GS,
which alters the parameters of 3D Gaussians according to the manipulation of
the associated mesh. Our method benefits from existing mesh deformation
datasets for more realistic data-driven Gaussian deformation. Extensive
experiments show that our approach achieves high-quality reconstruction and
effective deformation, while maintaining the promising rendering results at a
high frame rate(65 FPS on average).";Lin Gao<author:sep>Jie Yang<author:sep>Bo-Tao Zhang<author:sep>Jia-Mu Sun<author:sep>Yu-Jie Yuan<author:sep>Hongbo Fu<author:sep>Yu-Kun Lai;http://arxiv.org/pdf/2402.04796v1;cs.GR;11 pages, 7 figures;gaussian splatting
2402.04081v1;http://arxiv.org/abs/2402.04081v1;2024-02-06;Improved Generalization of Weight Space Networks via Augmentations;"Learning in deep weight spaces (DWS), where neural networks process the
weights of other neural networks, is an emerging research direction, with
applications to 2D and 3D neural fields (INRs, NeRFs), as well as making
inferences about other types of neural networks. Unfortunately, weight space
models tend to suffer from substantial overfitting. We empirically analyze the
reasons for this overfitting and find that a key reason is the lack of
diversity in DWS datasets. While a given object can be represented by many
different weight configurations, typical INR training sets fail to capture
variability across INRs that represent the same object. To address this, we
explore strategies for data augmentation in weight spaces and propose a MixUp
method adapted for weight spaces. We demonstrate the effectiveness of these
methods in two setups. In classification, they improve performance similarly to
having up to 10 times more data. In self-supervised contrastive learning, they
yield substantial 5-10% gains in downstream classification.";Aviv Shamsian<author:sep>Aviv Navon<author:sep>David W. Zhang<author:sep>Yan Zhang<author:sep>Ethan Fetaya<author:sep>Gal Chechik<author:sep>Haggai Maron;http://arxiv.org/pdf/2402.04081v1;cs.LG;Under Review;nerf
2402.03723v1;http://arxiv.org/abs/2402.03723v1;2024-02-06;Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos;"Creating controllable 3D human portraits from casual smartphone videos is
highly desirable due to their immense value in AR/VR applications. The recent
development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering
quality and training efficiency. However, it still remains a challenge to
accurately model and disentangle head movements and facial expressions from a
single-view capture to achieve high-quality renderings. In this paper, we
introduce Rig3DGS to address this challenge. We represent the entire scene,
including the dynamic subject, using a set of 3D Gaussians in a canonical
space. Using a set of control signals, such as head pose and expressions, we
transform them to the 3D space with learned deformations to generate the
desired rendering. Our key innovation is a carefully designed deformation
method which is guided by a learnable prior derived from a 3D morphable model.
This approach is highly efficient in training and effective in controlling
facial expressions, head positions, and view synthesis across various captures.
We demonstrate the effectiveness of our learned deformation through extensive
quantitative and qualitative experiments. The project page can be found at
http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html";Alfredo Rivero<author:sep>ShahRukh Athar<author:sep>Zhixin Shu<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2402.03723v1;cs.CV;;gaussian splatting
2402.03246v1;http://arxiv.org/abs/2402.03246v1;2024-02-05;SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM;"Semantic understanding plays a crucial role in Dense Simultaneous
Localization and Mapping (SLAM), facilitating comprehensive scene
interpretation. Recent advancements that integrate Gaussian Splatting into SLAM
systems have demonstrated its effectiveness in generating high-quality
renderings through the use of explicit 3D Gaussian representations. Building on
this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system
grounded in 3D Gaussians, which provides precise 3D semantic segmentation
alongside high-fidelity reconstructions. Specifically, we propose to employ
multi-channel optimization during the mapping process, integrating appearance,
geometric, and semantic constraints with key-frame optimization to enhance
reconstruction quality. Extensive experiments demonstrate that SGS-SLAM
delivers state-of-the-art performance in camera pose estimation, map
reconstruction, and semantic segmentation, outperforming existing methods
meanwhile preserving real-time rendering ability.";Mingrui Li<author:sep>Shuhong Liu<author:sep>Heng Zhou;http://arxiv.org/pdf/2402.03246v1;cs.CV;;gaussian splatting
2402.03307v2;http://arxiv.org/abs/2402.03307v2;2024-02-05;4D Gaussian Splatting: Towards Efficient Novel View Synthesis for  Dynamic Scenes;"We consider the problem of novel view synthesis (NVS) for dynamic scenes.
Recent neural approaches have accomplished exceptional NVS results for static
3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior
efforts often encode dynamics by learning a canonical space plus implicit or
explicit deformation fields, which struggle in challenging scenarios like
sudden movements or capturing high-fidelity renderings. In this paper, we
introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic
scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D
Gaussian Splatting in static scenes. We model dynamics at each timestamp by
temporally slicing the 4D Gaussians, which naturally compose dynamic 3D
Gaussians and can be seamlessly projected into images. As an explicit
spatial-temporal representation, 4DGS demonstrates powerful capabilities for
modeling complicated dynamics and fine details, especially for scenes with
abrupt motions. We further implement our temporal slicing and splatting
techniques in a highly optimized CUDA acceleration framework, achieving
real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and
583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions
showcase the superior efficiency and effectiveness of 4DGS, which consistently
outperforms existing methods both quantitatively and qualitatively.";Yuanxing Duan<author:sep>Fangyin Wei<author:sep>Qiyu Dai<author:sep>Yuhang He<author:sep>Wenzheng Chen<author:sep>Baoquan Chen;http://arxiv.org/pdf/2402.03307v2;cs.CV;;gaussian splatting
2402.02906v1;http://arxiv.org/abs/2402.02906v1;2024-02-05;ViewFusion: Learning Composable Diffusion Models for Novel View  Synthesis;"Deep learning is providing a wealth of new approaches to the old problem of
novel view synthesis, from Neural Radiance Field (NeRF) based approaches to
end-to-end style architectures. Each approach offers specific strengths but
also comes with specific limitations in their applicability. This work
introduces ViewFusion, a state-of-the-art end-to-end generative approach to
novel view synthesis with unparalleled flexibility. ViewFusion consists in
simultaneously applying a diffusion denoising step to any number of input views
of a scene, then combining the noise gradients obtained for each view with an
(inferred) pixel-weighting mask, ensuring that for each region of the target
scene only the most informative input views are taken into account. Our
approach resolves several limitations of previous approaches by (1) being
trainable and generalizing across multiple scenes and object classes, (2)
adaptively taking in a variable number of pose-free views at both train and
test time, (3) generating plausible views even in severely undetermined
conditions (thanks to its generative nature) -- all while generating views of
quality on par or even better than state-of-the-art methods. Limitations
include not generating a 3D embedding of the scene, resulting in a relatively
slow inference speed, and our method only being tested on the relatively small
dataset NMR. Code is available.";Bernard Spiegl<author:sep>Andrea Perin<author:sep>Stéphane Deny<author:sep>Alexander Ilin;http://arxiv.org/pdf/2402.02906v1;cs.CV;;nerf
2402.01524v1;http://arxiv.org/abs/2402.01524v1;2024-02-02;HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation;"Neural radiance fields (NeRFs) are a widely accepted standard for
synthesizing new 3D object views from a small number of base images. However,
NeRFs have limited generalization properties, which means that we need to use
significant computational resources to train individual architectures for each
item we want to represent. To address this issue, we propose a few-shot
learning approach based on the hypernetwork paradigm that does not require
gradient optimization during inference. The hypernetwork gathers information
from the training data and generates an update for universal weights. As a
result, we have developed an efficient method for generating a high-quality 3D
object representation from a small number of images in a single step. This has
been confirmed by direct comparison with the state-of-the-art solutions and a
comprehensive ablation study.";Paweł Batorski<author:sep>Dawid Malarz<author:sep>Marcin Przewięźlikowski<author:sep>Marcin Mazur<author:sep>Sławomir Tadeja<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2402.01524v1;cs.CV;;nerf
2402.01217v2;http://arxiv.org/abs/2402.01217v2;2024-02-02;Taming Uncertainty in Sparse-view Generalizable NeRF via Indirect  Diffusion Guidance;"Neural Radiance Fields (NeRF) have demonstrated effectiveness in synthesizing
novel views. However, their reliance on dense inputs and scene-specific
optimization has limited their broader applicability. Generalizable NeRFs
(Gen-NeRF), while intended to address this, often produce blurring artifacts in
unobserved regions with sparse inputs, which are full of uncertainty. In this
paper, we aim to diminish the uncertainty in Gen-NeRF for plausible renderings.
We assume that NeRF's inability to effectively mitigate this uncertainty stems
from its inherent lack of generative capacity. Therefore, we innovatively
propose an Indirect Diffusion-guided NeRF framework, termed ID-NeRF, to address
this uncertainty from a generative perspective by leveraging a distilled
diffusion prior as guidance. Specifically, to avoid model confusion caused by
directly regularizing with inconsistent samplings as in previous methods, our
approach introduces a strategy to indirectly inject the inherently missing
imagination into the learned implicit function through a diffusion-guided
latent space. Empirical evaluation across various benchmarks demonstrates the
superior performance of our approach in handling uncertainty with sparse
inputs.";Yaokun Li<author:sep>Chao Gou<author:sep>Guang Tan;http://arxiv.org/pdf/2402.01217v2;cs.CV;;nerf
2402.01950v1;http://arxiv.org/abs/2402.01950v1;2024-02-02;ConRF: Zero-shot Stylization of 3D Scenes with Conditioned Radiation  Fields;"Most of the existing works on arbitrary 3D NeRF style transfer required
retraining on each single style condition. This work aims to achieve zero-shot
controlled stylization in 3D scenes utilizing text or visual input as
conditioning factors. We introduce ConRF, a novel method of zero-shot
stylization. Specifically, due to the ambiguity of CLIP features, we employ a
conversion process that maps the CLIP feature space to the style space of a
pre-trained VGG network and then refine the CLIP multi-modal knowledge into a
style transfer neural radiation field. Additionally, we use a 3D volumetric
representation to perform local style transfer. By combining these operations,
ConRF offers the capability to utilize either text or images as references,
resulting in the generation of sequences with novel views enhanced by global or
local stylization. Our experiment demonstrates that ConRF outperforms other
existing methods for 3D scene and single-text stylization in terms of visual
quality.";Xingyu Miao<author:sep>Yang Bai<author:sep>Haoran Duan<author:sep>Fan Wan<author:sep>Yawen Huang<author:sep>Yang Long<author:sep>Yefeng Zheng;http://arxiv.org/pdf/2402.01950v1;cs.CV;;nerf
2402.01380v1;http://arxiv.org/abs/2402.01380v1;2024-02-02;Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate  Distortion Optimization;"Volumetric videos, benefiting from immersive 3D realism and interactivity,
hold vast potential for various applications, while the tremendous data volume
poses significant challenges for compression. Recently, NeRF has demonstrated
remarkable potential in volumetric video compression thanks to its simple
representation and powerful 3D modeling capabilities, where a notable work is
ReRF. However, ReRF separates the modeling from compression process, resulting
in suboptimal compression efficiency. In contrast, in this paper, we propose a
volumetric video compression method based on dynamic NeRF in a more compact
manner. Specifically, we decompose the NeRF representation into the coefficient
fields and the basis fields, incrementally updating the basis fields in the
temporal domain to achieve dynamic modeling. Additionally, we perform
end-to-end joint optimization on the modeling and compression process to
further improve the compression efficiency. Extensive experiments demonstrate
that our method achieves higher compression efficiency compared to ReRF on
various datasets.";Zhiyu Zhang<author:sep>Guo Lu<author:sep>Huanxiong Liang<author:sep>Anni Tang<author:sep>Qiang Hu<author:sep>Li Song;http://arxiv.org/pdf/2402.01380v1;cs.CV;;nerf
2402.01485v1;http://arxiv.org/abs/2402.01485v1;2024-02-02;Di-NeRF: Distributed NeRF for Collaborative Learning with Unknown  Relative Poses;"Collaborative mapping of unknown environments can be done faster and more
robustly than a single robot. However, a collaborative approach requires a
distributed paradigm to be scalable and deal with communication issues. This
work presents a fully distributed algorithm enabling a group of robots to
collectively optimize the parameters of a Neural Radiance Field (NeRF). The
algorithm involves the communication of each robot's trained NeRF parameters
over a mesh network, where each robot trains its NeRF and has access to its own
visual data only. Additionally, the relative poses of all robots are jointly
optimized alongside the model parameters, enabling mapping with unknown
relative camera poses. We show that multi-robot systems can benefit from
differentiable and robust 3D reconstruction optimized from multiple NeRFs.
Experiments on real-world and synthetic data demonstrate the efficiency of the
proposed algorithm. See the website of the project for videos of the
experiments and supplementary
material(https://sites.google.com/view/di-nerf/home).";Mahboubeh Asadi<author:sep>Kourosh Zareinia<author:sep>Sajad Saeedi;http://arxiv.org/pdf/2402.01485v1;cs.RO;9 pages, 11 figures, Submitted to IEEE-RA-L;nerf
2402.01915v1;http://arxiv.org/abs/2402.01915v1;2024-02-02;Robust Inverse Graphics via Probabilistic Inference;"How do we infer a 3D scene from a single image in the presence of corruptions
like rain, snow or fog? Straightforward domain randomization relies on knowing
the family of corruptions ahead of time. Here, we propose a Bayesian
approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene
prior and an uninformative uniform corruption prior, making it applicable to a
wide range of corruptions. Given a single image, RIG performs posterior
inference jointly over the scene and the corruption. We demonstrate this idea
by training a neural radiance field (NeRF) scene prior and using a secondary
NeRF to represent the corruptions over which we place an uninformative prior.
RIG, trained only on clean data, outperforms depth estimators and alternative
NeRF approaches that perform point estimation instead of full inference. The
results hold for a number of scene prior architectures based on normalizing
flows and diffusion models. For the latter, we develop reconstruction-guidance
with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is
applicable in the presence of auxiliary latent variables such as the
corruption. RIG demonstrates how scene priors can be used beyond generation
tasks.";Tuan Anh Le<author:sep>Pavel Sountsov<author:sep>Matthew D. Hoffman<author:sep>Ben Lee<author:sep>Brian Patton<author:sep>Rif A. Saurous;http://arxiv.org/pdf/2402.01915v1;cs.CV;;nerf
2402.01459v3;http://arxiv.org/abs/2402.01459v3;2024-02-02;GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting;"Recently, a range of neural network-based methods for image rendering have
been introduced. One such widely-researched neural radiance field (NeRF) relies
on a neural network to represent 3D scenes, allowing for realistic view
synthesis from a small number of 2D images. However, most NeRF models are
constrained by long training and inference times. In comparison, Gaussian
Splatting (GS) is a novel, state-of-the-art technique for rendering points in a
3D scene by approximating their contribution to image pixels through Gaussian
distributions, warranting fast training and swift, real-time rendering. A
drawback of GS is the absence of a well-defined approach for its conditioning
due to the necessity to condition several hundred thousand Gaussian components.
To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which
allows modification of Gaussian components in a similar way as meshes. We
parameterize each Gaussian component by the vertices of the mesh face.
Furthermore, our model needs mesh initialization on input or estimated mesh
during training. We also define Gaussian splats solely based on their location
on the mesh, allowing for automatic adjustments in position, scale, and
rotation during animation. As a result, we obtain a real-time rendering of
editable GS.";Joanna Waczyńska<author:sep>Piotr Borycki<author:sep>Sławomir Tadeja<author:sep>Jacek Tabor<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2402.01459v3;cs.CV;;gaussian splatting<tag:sep>nerf
2402.00525v1;http://arxiv.org/abs/2402.00525v1;2024-02-01;StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time  Rendering;"Gaussian Splatting has emerged as a prominent model for constructing 3D
representations from images across diverse domains. However, the efficiency of
the 3D Gaussian Splatting rendering pipeline relies on several simplifications.
Notably, reducing Gaussian to 2D splats with a single view-space depth
introduces popping and blending artifacts during view rotation. Addressing this
issue requires accurate per-pixel depth computation, yet a full per-pixel sort
proves excessively costly compared to a global sort operation. In this paper,
we present a novel hierarchical rasterization approach that systematically
resorts and culls splats with minimal processing overhead. Our software
rasterizer effectively eliminates popping artifacts and view inconsistencies,
as demonstrated through both quantitative and qualitative measurements.
Simultaneously, our method mitigates the potential for cheating view-dependent
effects with popping, ensuring a more authentic representation. Despite the
elimination of cheating, our approach achieves comparable quantitative results
for test images, while increasing the consistency for novel view synthesis in
motion. Due to its design, our hierarchical approach is only 4% slower on
average than the original Gaussian Splatting. Notably, enforcing consistency
enables a reduction in the number of Gaussians by approximately half with
nearly identical quality and view-consistency. Consequently, rendering
performance is nearly doubled, making our approach 1.6x faster than the
original Gaussian Splatting, with a 50% reduction in memory requirements.";Lukas Radl<author:sep>Michael Steiner<author:sep>Mathias Parger<author:sep>Alexander Weinrauch<author:sep>Bernhard Kerbl<author:sep>Markus Steinberger;http://arxiv.org/pdf/2402.00525v1;cs.GR;Video: https://youtu.be/RJQlSORNkr0;gaussian splatting
2402.00752v2;http://arxiv.org/abs/2402.00752v2;2024-02-01;Optimal Projection for 3D Gaussian Splatting;"3D Gaussian Splatting has garnered extensive attention and application in
real-time neural rendering. Concurrently, concerns have been raised about the
limitations of this technology in aspects such as point cloud storage,
performance , and robustness in sparse viewpoints , leading to various
improvements. However, there has been a notable lack of attention to the
projection errors introduced by the local affine approximation inherent in the
splatting itself, and the consequential impact of these errors on the quality
of photo-realistic rendering. This paper addresses the projection error
function of 3D Gaussian Splatting, commencing with the residual error from the
first-order Taylor expansion of the projection function $\phi$. The analysis
establishes a correlation between the error and the Gaussian mean position.
Subsequently, leveraging function optimization theory, this paper analyzes the
function's minima to provide an optimal projection strategy for Gaussian
Splatting referred to Optimal Gaussian Splatting. Experimental validation
further confirms that this projection methodology reduces artifacts, resulting
in a more convincingly realistic rendering.";Letian Huang<author:sep>Jiayang Bai<author:sep>Jie Guo<author:sep>Yanwen Guo;http://arxiv.org/pdf/2402.00752v2;cs.CV;;gaussian splatting
2402.00864v1;http://arxiv.org/abs/2402.00864v1;2024-02-01;ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields;"We introduce ViCA-NeRF, the first view-consistency-aware method for 3D
editing with text instructions. In addition to the implicit neural radiance
field (NeRF) modeling, our key insight is to exploit two sources of
regularization that explicitly propagate the editing information across
different views, thus ensuring multi-view consistency. For geometric
regularization, we leverage the depth information derived from NeRF to
establish image correspondences between different views. For learned
regularization, we align the latent codes in the 2D diffusion model between
edited and unedited images, enabling us to edit key views and propagate the
update throughout the entire scene. Incorporating these two strategies, our
ViCA-NeRF operates in two stages. In the initial stage, we blend edits from
different views to create a preliminary 3D edit. This is followed by a second
stage of NeRF training, dedicated to further refining the scene's appearance.
Experimental results demonstrate that ViCA-NeRF provides more flexible,
efficient (3 times faster) editing with higher levels of consistency and
details, compared with the state of the art. Our code is publicly available.";Jiahua Dong<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2402.00864v1;cs.CV;"Neurips2023; project page: https://github.com/Dongjiahua/VICA-NeRF";nerf
2402.00827v1;http://arxiv.org/abs/2402.00827v1;2024-02-01;Emo-Avatar: Efficient Monocular Video Style Avatar through Texture  Rendering;"Artistic video portrait generation is a significant and sought-after task in
the fields of computer graphics and vision. While various methods have been
developed that integrate NeRFs or StyleGANs with instructional editing models
for creating and editing drivable portraits, these approaches face several
challenges. They often rely heavily on large datasets, require extensive
customization processes, and frequently result in reduced image quality. To
address the above problems, we propose the Efficient Monotonic Video Style
Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's
capacity for producing dynamic, drivable portrait videos. We proposed a
two-stage deferred neural rendering pipeline. In the first stage, we utilize
few-shot PTI initialization to initialize the StyleGAN generator through
several extreme poses sampled from the video to capture the consistent
representation of aligned faces from the target portrait. In the second stage,
we propose a Laplacian pyramid for high-frequency texture sampling from UV maps
deformed by dynamic flow of expression for motion-aware texture prior
integration to provide torso features to enhance StyleGAN's ability to generate
complete and upper body for portrait video rendering. Emo-Avatar reduces style
customization time from hours to merely 5 minutes compared with existing
methods. In addition, Emo-Avatar requires only a single reference image for
editing and employs region-aware contrastive learning with semantic invariant
CLIP guidance, ensuring consistent high-resolution output and identity
preservation. Through both quantitative and qualitative assessments, Emo-Avatar
demonstrates superior performance over existing methods in terms of training
efficiency, rendering quality and editability in self- and cross-reenactment.";Pinxin Liu<author:sep>Luchuan Song<author:sep>Daoan Zhang<author:sep>Hang Hua<author:sep>Yunlong Tang<author:sep>Huaijin Tu<author:sep>Jiebo Luo<author:sep>Chenliang Xu;http://arxiv.org/pdf/2402.00827v1;cs.CV;;nerf
2402.00763v1;http://arxiv.org/abs/2402.00763v1;2024-02-01;360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming;"3D Gaussian Splatting (3D-GS) has recently attracted great attention with
real-time and photo-realistic renderings. This technique typically takes
perspective images as input and optimizes a set of 3D elliptical Gaussians by
splatting them onto the image planes, resulting in 2D Gaussians. However,
applying 3D-GS to panoramic inputs presents challenges in effectively modeling
the projection onto the spherical surface of ${360^\circ}$ images using 2D
Gaussians. In practical applications, input panoramas are often sparse, leading
to unreliable initialization of 3D Gaussians and subsequent degradation of
3D-GS quality. In addition, due to the under-constrained geometry of
texture-less planes (e.g., walls and floors), 3D-GS struggles to model these
flat regions with elliptical Gaussians, resulting in significant floaters in
novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$
Gaussian splatting for a limited set of panoramic inputs. Instead of splatting
3D Gaussians directly onto the spherical surface, 360-GS projects them onto the
tangent plane of the unit sphere and then maps them to the spherical
projections. This adaptation enables the representation of the projection using
Gaussians. We guide the optimization of 360-GS by exploiting layout priors
within panoramas, which are simple to obtain and contain strong structural
information about the indoor scene. Our experimental results demonstrate that
360-GS allows panoramic rendering and outperforms state-of-the-art methods with
fewer artifacts in novel view synthesis, thus providing immersive roaming in
indoor scenarios.";Jiayang Bai<author:sep>Letian Huang<author:sep>Jie Guo<author:sep>Wen Gong<author:sep>Yuanqi Li<author:sep>Yanwen Guo;http://arxiv.org/pdf/2402.00763v1;cs.CV;11 pages, 10 figures;gaussian splatting
2401.17857v2;http://arxiv.org/abs/2401.17857v2;2024-01-31;Segment Anything in 3D Gaussians;"3D Gaussian Splatting has emerged as an alternative 3D representation of
Neural Radiance Fields (NeRFs), benefiting from its high-quality rendering
results and real-time rendering speed. Considering the 3D Gaussian
representation remains unparsed, it is necessary first to execute object
segmentation within this domain. Subsequently, scene editing and collision
detection can be performed, proving vital to a multitude of applications, such
as virtual reality (VR), augmented reality (AR), game/movie production, etc. In
this paper, we propose a novel approach to achieve object segmentation in 3D
Gaussian via an interactive procedure without any training process and learned
parameters. We refer to the proposed method as SA-GS, for Segment Anything in
3D Gaussians. Given a set of clicked points in a single input view, SA-GS can
generalize SAM to achieve 3D consistent segmentation via the proposed
multi-view mask generation and view-wise label assignment methods. We also
propose a cross-view label-voting approach to assign labels from different
views. In addition, in order to address the boundary roughness issue of
segmented objects resulting from the non-negligible spatial sizes of 3D
Gaussian located at the boundary, SA-GS incorporates the simple but effective
Gaussian Decomposition scheme. Extensive experiments demonstrate that SA-GS
achieves high-quality 3D segmentation results, which can also be easily applied
for scene editing and collision detection tasks. Codes will be released soon.";Xu Hu<author:sep>Yuxi Wang<author:sep>Lue Fan<author:sep>Junsong Fan<author:sep>Junran Peng<author:sep>Zhen Lei<author:sep>Qing Li<author:sep>Zhaoxiang Zhang;http://arxiv.org/pdf/2401.17857v2;cs.CV;;gaussian splatting<tag:sep>nerf
2401.18075v1;http://arxiv.org/abs/2401.18075v1;2024-01-31;CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting;"We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene
Forecasting, a method for predicting future 3D scenes given past observations,
such as 2D ego-centric images. Our method maps an image to a distribution over
plausible 3D latent scene configurations using a probabilistic encoder, and
predicts the evolution of the hypothesized scenes through time. Our latent
scene representation conditions a global Neural Radiance Field (NeRF) to
represent a 3D scene model, which enables explainable predictions and
straightforward downstream applications. This approach extends beyond previous
neural rendering work by considering complex scenarios of uncertainty in
environmental states and dynamics. We employ a two-stage training of
Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we
auto-regressively predict latent scene representations as a partially
observable Markov decision process, utilizing a mixture density network. We
demonstrate the utility of our method in realistic scenarios using the CARLA
driving simulator, where CARFF can be used to enable efficient trajectory and
contingency planning in complex multi-agent autonomous driving scenarios
involving visual occlusions.";Jiezhi Yang<author:sep>Khushi Desai<author:sep>Charles Packer<author:sep>Harshil Bhatia<author:sep>Nicholas Rhinehart<author:sep>Rowan McAllister<author:sep>Joseph Gonzalez;http://arxiv.org/pdf/2401.18075v1;cs.CV;;nerf
2401.17121v1;http://arxiv.org/abs/2401.17121v1;2024-01-30;Physical Priors Augmented Event-Based 3D Reconstruction;"3D neural implicit representations play a significant component in many
robotic applications. However, reconstructing neural radiance fields (NeRF)
from realistic event data remains a challenge due to the sparsities and the
lack of information when only event streams are available. In this paper, we
utilize motion, geometry, and density priors behind event data to impose strong
physical constraints to augment NeRF training. The proposed novel pipeline can
directly benefit from those priors to reconstruct 3D scenes without additional
inputs. Moreover, we present a novel density-guided patch-based sampling
strategy for robust and efficient learning, which not only accelerates training
procedures but also conduces to expressions of local geometries. More
importantly, we establish the first large dataset for event-based 3D
reconstruction, which contains 101 objects with various materials and
geometries, along with the groundtruth of images and depth maps for all camera
viewpoints, which significantly facilitates other research in the related
fields. The code and dataset will be publicly available at
https://github.com/Mercerai/PAEv3d.";Jiaxu Wang<author:sep>Junhao He<author:sep>Ziyi Zhang<author:sep>Renjing Xu;http://arxiv.org/pdf/2401.17121v1;cs.RO;6 pages, 6 figures, ICRA 2024;nerf
2401.16663v1;http://arxiv.org/abs/2401.16663v1;2024-01-30;VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System  in Virtual Reality;"As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain
momentum, there's a growing focus on the development of engagements with 3D
virtual content. Unfortunately, traditional techniques for content creation,
editing, and interaction within these virtual spaces are fraught with
difficulties. They tend to be not only engineering-intensive but also require
extensive expertise, which adds to the frustration and inefficiency in virtual
object manipulation. Our proposed VR-GS system represents a leap forward in
human-centered 3D content interaction, offering a seamless and intuitive user
experience. By developing a physical dynamics-aware interactive Gaussian
Splatting in a Virtual Reality setting, and constructing a highly efficient
two-level embedding strategy alongside deformable body simulations, VR-GS
ensures real-time execution with highly realistic dynamic responses. The
components of our Virtual Reality system are designed for high efficiency and
effectiveness, starting from detailed scene reconstruction and object
segmentation, advancing through multi-view image in-painting, and extending to
interactive physics-based editing. The system also incorporates real-time
deformation embedding and dynamic shadow casting, ensuring a comprehensive and
engaging virtual experience.Our project page is available at:
https://yingjiang96.github.io/VR-GS/.";Ying Jiang<author:sep>Chang Yu<author:sep>Tianyi Xie<author:sep>Xuan Li<author:sep>Yutao Feng<author:sep>Huamin Wang<author:sep>Minchen Li<author:sep>Henry Lau<author:sep>Feng Gao<author:sep>Yin Yang<author:sep>Chenfanfu Jiang;http://arxiv.org/pdf/2401.16663v1;cs.HC;;gaussian splatting
2401.16416v2;http://arxiv.org/abs/2401.16416v2;2024-01-29;Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian  Splatting;"In the realm of robot-assisted minimally invasive surgery, dynamic scene
reconstruction can significantly enhance downstream tasks and improve surgical
outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to
prominence for their exceptional ability to reconstruct scenes. Nonetheless,
these methods are hampered by slow inference, prolonged training, and
substantial computational demands. Additionally, some rely on stereo depth
estimation, which is often infeasible due to the high costs and logistical
challenges associated with stereo cameras. Moreover, the monocular
reconstruction quality for deformable scenes is currently inadequate. To
overcome these obstacles, we present Endo-4DGS, an innovative, real-time
endoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting
(GS) and requires no ground truth depth data. This method extends 3D GS by
incorporating a temporal component and leverages a lightweight MLP to capture
temporal Gaussian deformations. This effectively facilitates the reconstruction
of dynamic surgical scenes with variable conditions. We also integrate
Depth-Anything to generate pseudo-depth maps from monocular views, enhancing
the depth-guided reconstruction process. Our approach has been validated on two
surgical datasets, where it can effectively render in real-time, compute
efficiently, and reconstruct with remarkable accuracy. These results underline
the vast potential of Endo-4DGS to improve surgical assistance.";Yiming Huang<author:sep>Beilei Cui<author:sep>Long Bai<author:sep>Ziqi Guo<author:sep>Mengya Xu<author:sep>Hongliang Ren;http://arxiv.org/pdf/2401.16416v2;cs.CV;;gaussian splatting<tag:sep>nerf
2401.16144v1;http://arxiv.org/abs/2401.16144v1;2024-01-29;Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance  Fields;"Neural radiance fields (NeRFs) have exhibited potential in synthesizing
high-fidelity views of 3D scenes but the standard training paradigm of NeRF
presupposes an equal importance for each image in the training set. This
assumption poses a significant challenge for rendering specific views
presenting intricate geometries, thereby resulting in suboptimal performance.
In this paper, we take a closer look at the implications of the current
training paradigm and redesign this for more superior rendering quality by
NeRFs. Dividing input views into multiple groups based on their visual
similarities and training individual models on each of these groups enables
each model to specialize on specific regions without sacrificing speed or
efficiency. Subsequently, the knowledge of these specialized models is
aggregated into a single entity via a teacher-student distillation paradigm,
enabling spatial efficiency for online render-ing. Empirically, we evaluate our
novel training framework on two publicly available datasets, namely NeRF
synthetic and Tanks&Temples. Our evaluation demonstrates that our DaC training
pipeline enhances the rendering quality of a state-of-the-art baseline model
while exhibiting convergence to a superior minimum.";Rongkai Ma<author:sep>Leo Lebrat<author:sep>Rodrigo Santa Cruz<author:sep>Gil Avraham<author:sep>Yan Zuo<author:sep>Clinton Fookes<author:sep>Olivier Salvado;http://arxiv.org/pdf/2401.16144v1;cs.CV;;nerf
2401.15318v1;http://arxiv.org/abs/2401.15318v1;2024-01-27;Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting;"We demonstrate the feasibility of integrating physics-based animations of
solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in
virtual scenes reconstructed using 3DGS. Leveraging the coherence of the
Gaussian splatting and position-based dynamics (PBD) in the underlying
representation, we manage rendering, view synthesis, and the dynamics of solids
and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each
Gaussian kernel with an added normal, aligning the kernel's orientation with
the surface normal to refine the PBD simulation. This approach effectively
eliminates spiky noises that arise from rotational deformation in solids. It
also allows us to integrate physically based rendering to augment the dynamic
surface reflections on fluids. Consequently, our framework is capable of
realistically reproducing surface highlights on dynamic fluids and facilitating
interactions between scene objects and fluids from new views. For more
information, please visit our project page at
\url{https://amysteriouscat.github.io/GaussianSplashing/}.";Yutao Feng<author:sep>Xiang Feng<author:sep>Yintong Shang<author:sep>Ying Jiang<author:sep>Chang Yu<author:sep>Zeshun Zong<author:sep>Tianjia Shao<author:sep>Hongzhi Wu<author:sep>Kun Zhou<author:sep>Chenfanfu Jiang<author:sep>Yin Yang;http://arxiv.org/pdf/2401.15318v1;cs.GR;;gaussian splatting
2401.14828v1;http://arxiv.org/abs/2401.14828v1;2024-01-26;TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And  Image-Prompts;"Text-driven 3D scene editing has gained significant attention owing to its
convenience and user-friendliness. However, existing methods still lack
accurate control of the specified appearance and location of the editing result
due to the inherent limitations of the text description. To this end, we
propose a 3D scene editing framework, TIPEditor, that accepts both text and
image prompts and a 3D bounding box to specify the editing region. With the
image prompt, users can conveniently specify the detailed appearance/style of
the target content in complement to the text description, enabling accurate
control of the appearance. Specifically, TIP-Editor employs a stepwise 2D
personalization strategy to better learn the representation of the existing
scene and the reference image, in which a localization loss is proposed to
encourage correct object placement as specified by the bounding box.
Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as
the 3D representation to facilitate local editing while keeping the background
unchanged. Extensive experiments have demonstrated that TIP-Editor conducts
accurate editing following the text and image prompts in the specified bounding
box region, consistently outperforming the baselines in editing quality, and
the alignment to the prompts, qualitatively and quantitatively.";Jingyu Zhuang<author:sep>Di Kang<author:sep>Yan-Pei Cao<author:sep>Guanbin Li<author:sep>Liang Lin<author:sep>Ying Shan;http://arxiv.org/pdf/2401.14828v1;cs.CV;Under review;gaussian splatting
2401.14726v1;http://arxiv.org/abs/2401.14726v1;2024-01-26;3D Reconstruction and New View Synthesis of Indoor Environments based on  a Dual Neural Radiance Field;"Simultaneously achieving 3D reconstruction and new view synthesis for indoor
environments has widespread applications but is technically very challenging.
State-of-the-art methods based on implicit neural functions can achieve
excellent 3D reconstruction results, but their performances on new view
synthesis can be unsatisfactory. The exciting development of neural radiance
field (NeRF) has revolutionized new view synthesis, however, NeRF-based models
can fail to reconstruct clean geometric surfaces. We have developed a dual
neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry
reconstruction and view rendering. Du-NeRF contains two geometric fields, one
derived from the SDF field to facilitate geometric reconstruction and the other
derived from the density field to boost new view synthesis. One of the
innovative features of Du-NeRF is that it decouples a view-independent
component from the density field and uses it as a label to supervise the
learning process of the SDF field. This reduces shape-radiance ambiguity and
enables geometry and color to benefit from each other during the learning
process. Extensive experiments demonstrate that Du-NeRF can significantly
improve the performance of novel view synthesis and 3D reconstruction for
indoor environments and it is particularly effective in constructing areas
containing fine geometries that do not obey multi-view color consistency.";Zhenyu Bao<author:sep>Guibiao Liao<author:sep>Zhongyuan Zhao<author:sep>Kanglin Liu<author:sep>Qing Li<author:sep>Guoping Qiu;http://arxiv.org/pdf/2401.14726v1;cs.CV;20 pages, 8 figures;nerf
2401.14032v1;http://arxiv.org/abs/2401.14032v1;2024-01-25;GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D  Reconstruction Dataset Using Gaussian Splatting;"We introduce a novel large-scale scene reconstruction benchmark using the
newly developed 3D representation approach, Gaussian Splatting, on our
expansive U-Scene dataset. U-Scene encompasses over one and a half square
kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground
truth. For data acquisition, we employed the Matrix 300 drone equipped with the
high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This
dataset, offers a unique blend of urban and academic environments for advanced
spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with
Gaussian Splatting includes a detailed analysis across various novel
viewpoints. We also juxtapose these results with those derived from our
accurate point cloud dataset, highlighting significant differences that
underscore the importance of combine multi-modal information";Butian Xiong<author:sep>Zhuo Li<author:sep>Zhen Li;http://arxiv.org/pdf/2401.14032v1;cs.CV;IJCAI2024 submit, 8 pages;gaussian splatting
2401.14257v2;http://arxiv.org/abs/2401.14257v2;2024-01-25;Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation;"Recently, text-to-3D approaches have achieved high-fidelity 3D content
generation using text description. However, the generated objects are
stochastic and lack fine-grained control. Sketches provide a cheap approach to
introduce such fine-grained control. Nevertheless, it is challenging to achieve
flexible control from these sketches due to their abstraction and ambiguity. In
this paper, we present a multi-view sketch-guided text-to-3D generation
framework (namely, Sketch2NeRF) to add sketch control to 3D generation.
Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable
Diffusion and ControlNet) to supervise the optimization of a 3D scene
represented by a neural radiance field (NeRF). We propose a novel synchronized
generation and reconstruction method to effectively optimize the NeRF. In the
experiments, we collected two kinds of multi-view sketch datasets to evaluate
the proposed method. We demonstrate that our method can synthesize 3D
consistent contents with fine-grained sketch control while being high-fidelity
to text prompts. Extensive results show that our method achieves
state-of-the-art performance in terms of sketch similarity and text alignment.";Minglin Chen<author:sep>Weihao Yuan<author:sep>Yukun Wang<author:sep>Zhe Sheng<author:sep>Yisheng He<author:sep>Zilong Dong<author:sep>Liefeng Bo<author:sep>Yulan Guo;http://arxiv.org/pdf/2401.14257v2;cs.CV;11 pages, 9 figures;nerf
2401.14354v1;http://arxiv.org/abs/2401.14354v1;2024-01-25;Learning Robust Generalizable Radiance Field with Visibility and Feature  Augmented Point Representation;"This paper introduces a novel paradigm for the generalizable neural radiance
field (NeRF). Previous generic NeRF methods combine multiview stereo techniques
with image-based neural rendering for generalization, yielding impressive
results, while suffering from three issues. First, occlusions often result in
inconsistent feature matching. Then, they deliver distortions and artifacts in
geometric discontinuities and locally sharp shapes due to their individual
process of sampled points and rough feature aggregation. Third, their
image-based representations experience severe degradations when source views
are not near enough to the target view. To address challenges, we propose the
first paradigm that constructs the generalizable neural field based on
point-based rather than image-based rendering, which we call the Generalizable
neural Point Field (GPF). Our approach explicitly models visibilities by
geometric priors and augments them with neural features. We propose a novel
nonuniform log sampling strategy to improve both rendering speed and
reconstruction quality. Moreover, we present a learnable kernel spatially
augmented with features for feature aggregations, mitigating distortions at
places with drastically varying geometries. Besides, our representation can be
easily manipulated. Experiments show that our model can deliver better
geometries, view consistencies, and rendering quality than all counterparts and
benchmarks on three datasets in both generalization and finetuning settings,
preliminarily proving the potential of the new paradigm for generalizable NeRF.";Jiaxu Wang<author:sep>Ziyi Zhang<author:sep>Renjing Xu;http://arxiv.org/pdf/2401.14354v1;cs.CV;International Conference on Learning Representations 2024;nerf
2401.13352v1;http://arxiv.org/abs/2401.13352v1;2024-01-24;EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable  Endoscopic Tissues Reconstruction;"The accurate 3D reconstruction of deformable soft body tissues from
endoscopic videos is a pivotal challenge in medical applications such as VR
surgery and medical image analysis. Existing methods often struggle with
accuracy and the ambiguity of hallucinated tissue parts, limiting their
practical utility. In this work, we introduce EndoGaussians, a novel approach
that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This
method marks the first use of Gaussian Splatting in this context, overcoming
the limitations of previous NeRF-based techniques. Our method sets new
state-of-the-art standards, as demonstrated by quantitative assessments on
various endoscope datasets. These advancements make our method a promising tool
for medical professionals, offering more reliable and efficient 3D
reconstructions for practical applications in the medical field.";Yangsen Chen<author:sep>Hao Wang;http://arxiv.org/pdf/2401.13352v1;cs.CV;;gaussian splatting<tag:sep>nerf
2401.12456v1;http://arxiv.org/abs/2401.12456v1;2024-01-23;Exploration and Improvement of Nerf-based 3D Scene Editing Techniques;"NeRF's high-quality scene synthesis capability was quickly accepted by
scholars in the years after it was proposed, and significant progress has been
made in 3D scene representation and synthesis. However, the high computational
cost limits intuitive and efficient editing of scenes, making NeRF's
development in the scene editing field facing many challenges. This paper
reviews the preliminary explorations of scholars on NeRF in the scene or object
editing field in recent years, mainly changing the shape and texture of scenes
or objects in new synthesized scenes; through the combination of residual
models such as GaN and Transformer with NeRF, the generalization ability of
NeRF scene editing has been further expanded, including realizing real-time new
perspective editing feedback, multimodal editing of text synthesized 3D scenes,
4D synthesis performance, and in-depth exploration in light and shadow editing,
initially achieving optimization of indirect touch editing and detail
representation in complex scenes. Currently, most NeRF editing methods focus on
the touch points and materials of indirect points, but when dealing with more
complex or larger 3D scenes, it is difficult to balance accuracy, breadth,
efficiency, and quality. Overcoming these challenges may become the direction
of future NeRF 3D scene editing technology.";Shun Fang<author:sep>Ming Cui<author:sep>Xing Feng<author:sep>Yanan Zhang;http://arxiv.org/pdf/2401.12456v1;cs.CV;;nerf
2401.12451v1;http://arxiv.org/abs/2401.12451v1;2024-01-23;Methods and strategies for improving the novel view synthesis quality of  neural radiation field;"Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a
scene from 2D images and synthesize realistic novel view images. This
technology has received widespread attention from the industry and has good
application prospects. In response to the problem that the rendering quality of
NeRF images needs to be improved, many researchers have proposed various
methods to improve the rendering quality in the past three years. The latest
relevant papers are classified and reviewed, the technical principles behind
quality improvement are analyzed, and the future evolution direction of quality
improvement methods is discussed. This study can help researchers quickly
understand the current state and evolutionary context of technology in this
field, which is helpful in inspiring the development of more efficient
algorithms and promoting the application of NeRF technology in related fields.";Shun Fang<author:sep>Ming Cui<author:sep>Xing Feng<author:sep>Yanna Lv;http://arxiv.org/pdf/2401.12451v1;cs.CV;;nerf
2401.12900v4;http://arxiv.org/abs/2401.12900v4;2024-01-23;PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar  Animation with 3D Gaussian Splatting;"Despite much progress, achieving real-time high-fidelity head avatar
animation is still difficult and existing methods have to trade-off between
speed and quality. 3DMM based methods often fail to model non-facial structures
such as eyeglasses and hairstyles, while neural implicit models suffer from
deformation inflexibility and rendering inefficiency. Although 3D Gaussian has
been demonstrated to possess promising capability for geometry representation
and radiance field reconstruction, applying 3D Gaussian in head avatar creation
remains a major challenge since it is difficult for 3D Gaussian to model the
head shape variations caused by changing poses and expressions. In this paper,
we introduce PSAvatar, a novel framework for animatable head avatar creation
that utilizes discrete geometric primitive to create a parametric morphable
shape model and employs 3D Gaussian for fine detail representation and high
fidelity rendering. The parametric morphable shape model is a Point-based
Morphable Shape Model (PMSM) which uses points instead of meshes for 3D
representation to achieve enhanced representation flexibility. The PMSM first
converts the FLAME mesh to points by sampling on the surfaces as well as off
the meshes to enable the reconstruction of not only surface-like structures but
also complex geometries such as eyeglasses and hairstyles. By aligning these
points with the head shape in an analysis-by-synthesis manner, the PMSM makes
it possible to utilize 3D Gaussian for fine detail representation and
appearance modeling, thus enabling the creation of high-fidelity avatars. We
show that PSAvatar can reconstruct high-fidelity head avatars of a variety of
subjects and the avatars can be animated in real-time ($\ge$ 25 fps at a
resolution of 512 $\times$ 512 ).";Zhongyuan Zhao<author:sep>Zhenyu Bao<author:sep>Qing Li<author:sep>Guoping Qiu<author:sep>Kanglin Liu;http://arxiv.org/pdf/2401.12900v4;cs.GR;13 pages, 10 figures;gaussian splatting
2401.12568v1;http://arxiv.org/abs/2401.12568v1;2024-01-23;NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for  Talking Face Synthesis;"Talking face synthesis driven by audio is one of the current research
hotspots in the fields of multidimensional signal processing and multimedia.
Neural Radiance Field (NeRF) has recently been brought to this research field
in order to enhance the realism and 3D effect of the generated faces. However,
most existing NeRF-based methods either burden NeRF with complex learning tasks
while lacking methods for supervised multimodal feature fusion, or cannot
precisely map audio to the facial region related to speech movements. These
reasons ultimately result in existing methods generating inaccurate lip shapes.
This paper moves a portion of NeRF learning tasks ahead and proposes a talking
face synthesis method via NeRF with attention-based disentanglement (NeRF-AD).
In particular, an Attention-based Disentanglement module is introduced to
disentangle the face into Audio-face and Identity-face using speech-related
facial action unit (AU) information. To precisely regulate how audio affects
the talking face, we only fuse the Audio-face with audio feature. In addition,
AU information is also utilized to supervise the fusion of these two
modalities. Extensive qualitative and quantitative experiments demonstrate that
our NeRF-AD outperforms state-of-the-art methods in generating realistic
talking face videos, including image quality and lip synchronization. To view
video results, please refer to https://xiaoxingliu02.github.io/NeRF-AD.";Chongke Bi<author:sep>Xiaoxing Liu<author:sep>Zhilei Liu;http://arxiv.org/pdf/2401.12568v1;cs.CV;Accepted by ICASSP 2024;nerf
2401.12561v1;http://arxiv.org/abs/2401.12561v1;2024-01-23;EndoGaussian: Gaussian Splatting for Deformable Surgical Scene  Reconstruction;"Reconstructing deformable tissues from endoscopic stereo videos is essential
in many downstream surgical applications. However, existing methods suffer from
slow inference speed, which greatly limits their practical use. In this paper,
we introduce EndoGaussian, a real-time surgical scene reconstruction framework
that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical
scenes as canonical Gaussians and a time-dependent deformation field, which
predicts Gaussian deformations at novel timestamps. Due to the efficient
Gaussian representation and parallel rendering pipeline, our framework
significantly accelerates the rendering speed compared to previous methods. In
addition, we design the deformation field as the combination of a lightweight
encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian
tracking with a minor rendering burden. Furthermore, we design a holistic
Gaussian initialization method to fully leverage the surface distribution
prior, achieved by searching informative points from across the input image
sequence. Experiments on public endoscope datasets demonstrate that our method
can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain)
while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and
the fastest training speed (within 2 min/scene), showing significant promise
for intraoperative surgery applications. Code is available at:
\url{https://yifliu3.github.io/EndoGaussian/}.";Yifan Liu<author:sep>Chenxin Li<author:sep>Chen Yang<author:sep>Yixuan Yuan;http://arxiv.org/pdf/2401.12561v1;cs.CV;;gaussian splatting
2401.11985v1;http://arxiv.org/abs/2401.11985v1;2024-01-22;Scaling Face Interaction Graph Networks to Real World Scenes;"Accurately simulating real world object dynamics is essential for various
applications such as robotics, engineering, graphics, and design. To better
capture complex real dynamics such as contact and friction, learned simulators
based on graph networks have recently shown great promise. However, applying
these learned simulators to real scenes comes with two major challenges: first,
scaling learned simulators to handle the complexity of real world scenes which
can involve hundreds of objects each with complicated 3D shapes, and second,
handling inputs from perception rather than 3D state information. Here we
introduce a method which substantially reduces the memory required to run
graph-based learned simulators. Based on this memory-efficient simulation
model, we then present a perceptual interface in the form of editable NeRFs
which can convert real-world scenes into a structured representation that can
be processed by graph network simulator. We show that our method uses
substantially less memory than previous graph-based simulators while retaining
their accuracy, and that the simulators learned in synthetic environments can
be applied to real world scenes captured from multiple camera angles. This
paves the way for expanding the application of learned simulators to settings
where only perceptual information is available at inference time.";Tatiana Lopez-Guevara<author:sep>Yulia Rubanova<author:sep>William F. Whitney<author:sep>Tobias Pfaff<author:sep>Kimberly Stachenfeld<author:sep>Kelsey R. Allen;http://arxiv.org/pdf/2401.11985v1;cs.LG;16 pages, 12 figures;nerf
2401.12175v1;http://arxiv.org/abs/2401.12175v1;2024-01-22;Single-View 3D Human Digitalization with Large Reconstruction Models;"In this paper, we introduce Human-LRM, a single-stage feed-forward Large
Reconstruction Model designed to predict human Neural Radiance Fields (NeRF)
from a single image. Our approach demonstrates remarkable adaptability in
training using extensive datasets containing 3D scans and multi-view capture.
Furthermore, to enhance the model's applicability for in-the-wild scenarios
especially with occlusions, we propose a novel strategy that distills
multi-view reconstruction into single-view via a conditional triplane diffusion
model. This generative extension addresses the inherent variations in human
body shapes when observed from a single view, and makes it possible to
reconstruct the full body human from an occluded image. Through extensive
experiments, we show that Human-LRM surpasses previous methods by a significant
margin on several benchmarks.";Zhenzhen Weng<author:sep>Jingyuan Liu<author:sep>Hao Tan<author:sep>Zhan Xu<author:sep>Yang Zhou<author:sep>Serena Yeung-Levy<author:sep>Jimei Yang;http://arxiv.org/pdf/2401.12175v1;cs.CV;;nerf
2401.11711v1;http://arxiv.org/abs/2401.11711v1;2024-01-22;HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided  Neural Radiance Fields for Sparse View Inputs;"Neural Radiance Fields (NeRF) have garnered considerable attention as a
paradigm for novel view synthesis by learning scene representations from
discrete observations. Nevertheless, NeRF exhibit pronounced performance
degradation when confronted with sparse view inputs, consequently curtailing
its further applicability. In this work, we introduce Hierarchical Geometric,
Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can
address the aforementioned limitation and enhance consistency of geometry,
semantic content, and appearance across different views. We propose
Hierarchical Geometric Guidance (HGG) to incorporate the attachment of
Structure from Motion (SfM), namely sparse depth prior, into the scene
representations. Different from direct depth supervision, HGG samples volume
points from local-to-global geometric regions, mitigating the misalignment
caused by inherent bias in the depth prior. Furthermore, we draw inspiration
from notable variations in semantic consistency observed across images of
different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn
the coarse-to-fine semantic content, which corresponds to the coarse-to-fine
scene representations. Experimental results demonstrate that HG3-NeRF can
outperform other state-of-the-art methods on different standard benchmarks and
achieve high-fidelity synthesis results for sparse view inputs.";Zelin Gao<author:sep>Weichen Dai<author:sep>Yu Zhang;http://arxiv.org/pdf/2401.11711v1;cs.CV;13 pages, 6 figures;nerf
2401.11535v1;http://arxiv.org/abs/2401.11535v1;2024-01-21;Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting;"Surgical 3D reconstruction is a critical area of research in robotic surgery,
with recent works adopting variants of dynamic radiance fields to achieve
success in 3D reconstruction of deformable tissues from single-viewpoint
videos. However, these methods often suffer from time-consuming optimization or
inferior quality, limiting their adoption in downstream tasks. Inspired by 3D
Gaussian Splatting, a recent trending 3D representation, we present EndoGS,
applying Gaussian Splatting for deformable endoscopic tissue reconstruction.
Specifically, our approach incorporates deformation fields to handle dynamic
scenes, depth-guided supervision to optimize 3D targets with a single
viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a
result, EndoGS reconstructs and renders high-quality deformable endoscopic
tissues from a single-viewpoint video, estimated depth maps, and labeled tool
masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS
achieves superior rendering quality. Code is available at
https://github.com/HKU-MedAI/EndoGS.";Lingting Zhu<author:sep>Zhao Wang<author:sep>Zhenchao Jin<author:sep>Guying Lin<author:sep>Lequan Yu;http://arxiv.org/pdf/2401.11535v1;cs.CV;Work in progress. 10 pages, 4 figures;gaussian splatting
2401.09720v2;http://arxiv.org/abs/2401.09720v2;2024-01-18;GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting;"In this work, we propose a novel clothed human reconstruction method called
GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural
radiance based models, 3D Gaussian Splatting has recently demonstrated great
performance in terms of training time and rendering quality. However, applying
the static 3D Gaussian Splatting model to the dynamic human reconstruction
problem is non-trivial due to complicated non-rigid deformations and rich cloth
details. To address these challenges, our method considers explicit pose-guided
deformation to associate dynamic Gaussians across the canonical space and the
observation space, introducing a physically-based prior with regularized
transformations helps mitigate ambiguity between the two spaces. During the
training process, we further propose a pose refinement strategy to update the
pose regression for compensating the inaccurate initial estimation and a
split-with-scale mechanism to enhance the density of regressed point clouds.
The experiments validate that our method can achieve state-of-the-art
photorealistic novel-view rendering results with high-quality details for
dynamic clothed human bodies, along with explicit geometry reconstruction.";Mengtian Li<author:sep>Shengxiang Yao<author:sep>Zhifeng Xie<author:sep>Keyu Chen;http://arxiv.org/pdf/2401.09720v2;cs.CV;;gaussian splatting
2401.09495v4;http://arxiv.org/abs/2401.09495v4;2024-01-17;IPR-NeRF: Ownership Verification meets Neural Radiance Field;"Neural Radiance Field (NeRF) models have gained significant attention in the
computer vision community in the recent past with state-of-the-art visual
quality and produced impressive demonstrations. Since then, technopreneurs have
sought to leverage NeRF models into a profitable business. Therefore, NeRF
models make it worth the risk of plagiarizers illegally copying,
re-distributing, or misusing those models. This paper proposes a comprehensive
intellectual property (IP) protection framework for the NeRF model in both
black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a
diffusion-based solution is introduced to embed and extract the watermark via a
two-stage optimization process. In the white-box setting, a designated digital
signature is embedded into the weights of the NeRF model by adopting the sign
loss objective. Our extensive experiments demonstrate that not only does our
approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models,
but it is also robust against both ambiguity and removal attacks compared to
prior arts.";Win Kent Ong<author:sep>Kam Woh Ng<author:sep>Chee Seng Chan<author:sep>Yi Zhe Song<author:sep>Tao Xiang;http://arxiv.org/pdf/2401.09495v4;cs.CV;"Error on result tabulation of state of the art method which might
  cause misleading to readers";nerf
2401.08937v1;http://arxiv.org/abs/2401.08937v1;2024-01-17;ICON: Incremental CONfidence for Joint Pose and Radiance Field  Optimization;"Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View
Synthesis (NVS) given a set of 2D images. However, NeRF training requires
accurate camera pose for each input view, typically obtained by
Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax
this constraint, but they still often rely on decent initial poses which they
can refine. Here we aim at removing the requirement for pose initialization. We
present Incremental CONfidence (ICON), an optimization procedure for training
NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate
initial guess for poses. Further, ICON introduces ``confidence"": an adaptive
measure of model quality used to dynamically reweight gradients. ICON relies on
high-confidence poses to learn NeRF, and high-confidence 3D structure (as
encoded by NeRF) to learn poses. We show that ICON, without prior pose
initialization, achieves superior performance in both CO3D and HO3D versus
methods which use SfM pose.";Weiyao Wang<author:sep>Pierre Gleize<author:sep>Hao Tang<author:sep>Xingyu Chen<author:sep>Kevin J Liang<author:sep>Matt Feiszli;http://arxiv.org/pdf/2401.08937v1;cs.CV;;nerf
2401.08045v1;http://arxiv.org/abs/2401.08045v1;2024-01-16;Forging Vision Foundation Models for Autonomous Driving: Challenges,  Methodologies, and Opportunities;"The rise of large foundation models, trained on extensive datasets, is
revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4
showcase their adaptability by extracting intricate patterns and performing
effectively across diverse tasks, thereby serving as potent building blocks for
a wide range of AI applications. Autonomous driving, a vibrant front in AI
applications, remains challenged by the lack of dedicated vision foundation
models (VFMs). The scarcity of comprehensive training data, the need for
multi-sensor integration, and the diverse task-specific architectures pose
significant obstacles to the development of VFMs in this field. This paper
delves into the critical challenge of forging VFMs tailored specifically for
autonomous driving, while also outlining future directions. Through a
systematic analysis of over 250 papers, we dissect essential techniques for VFM
development, including data preparation, pre-training strategies, and
downstream task adaptation. Moreover, we explore key advancements such as NeRF,
diffusion models, 3D Gaussian Splatting, and world models, presenting a
comprehensive roadmap for future research. To empower researchers, we have
built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an
open-access repository constantly updated with the latest advancements in
forging VFMs for autonomous driving.";Xu Yan<author:sep>Haiming Zhang<author:sep>Yingjie Cai<author:sep>Jingming Guo<author:sep>Weichao Qiu<author:sep>Bin Gao<author:sep>Kaiqiang Zhou<author:sep>Yue Zhao<author:sep>Huan Jin<author:sep>Jiantao Gao<author:sep>Zhen Li<author:sep>Lihui Jiang<author:sep>Wei Zhang<author:sep>Hongbo Zhang<author:sep>Dengxin Dai<author:sep>Bingbing Liu;http://arxiv.org/pdf/2401.08045v1;cs.CV;Github Repo: https://github.com/zhanghm1995/Forge_VFM4AD;gaussian splatting<tag:sep>nerf
2401.08742v1;http://arxiv.org/abs/2401.08742v1;2024-01-16;Fast Dynamic 3D Object Generation from a Single-view Video;"Generating dynamic three-dimensional (3D) object from a single-view video is
challenging due to the lack of 4D labeled data. Existing methods extend
text-to-3D pipelines by transferring off-the-shelf image generation models such
as score distillation sampling, but they are slow and expensive to scale (e.g.,
150 minutes per object) due to the need for back-propagating the
information-limited supervision signals through a large pretrained model. To
address this limitation, we propose an efficient video-to-4D object generation
framework called Efficient4D. It generates high-quality spacetime-consistent
images under different camera views, and then uses them as labeled data to
directly train a novel 4D Gaussian splatting model with explicit point cloud
geometry, enabling real-time rendering under continuous camera trajectories.
Extensive experiments on synthetic and real videos show that Efficient4D offers
a remarkable 10-fold increase in speed when compared to prior art alternatives
while preserving the same level of innovative view synthesis quality. For
example, Efficient4D takes only 14 minutes to model a dynamic object.";Zijie Pan<author:sep>Zeyu Yang<author:sep>Xiatian Zhu<author:sep>Li Zhang;http://arxiv.org/pdf/2401.08742v1;cs.CV;Technical report;gaussian splatting
2401.08140v2;http://arxiv.org/abs/2401.08140v2;2024-01-16;ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process;"Neural radiance fields (NeRFs) have gained popularity across various
applications. However, they face challenges in the sparse view setting, lacking
sufficient constraints from volume rendering. Reconstructing and understanding
a 3D scene from sparse and unconstrained cameras is a long-standing problem in
classical computer vision with diverse applications. While recent works have
explored NeRFs in sparse, unconstrained view scenarios, their focus has been
primarily on enhancing reconstruction and novel view synthesis. Our approach
takes a broader perspective by posing the question: ""from where has each point
been seen?"" -- which gates how well we can understand and reconstruct it. In
other words, we aim to determine the origin or provenance of each 3D point and
its associated information under sparse, unconstrained views. We introduce
ProvNeRF, a model that enriches a traditional NeRF representation by
incorporating per-point provenance, modeling likely source locations for each
point. We achieve this by extending implicit maximum likelihood estimation
(IMLE) for stochastic processes. Notably, our method is compatible with any
pre-trained NeRF model and the associated training camera poses. We demonstrate
that modeling per-point provenance offers several advantages, including
uncertainty estimation, criteria-based view selection, and improved novel view
synthesis, compared to state-of-the-art methods. Please visit our project page
at https://provnerf.github.io";Kiyohiro Nakayama<author:sep>Mikaela Angelina Uy<author:sep>Yang You<author:sep>Ke Li<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2401.08140v2;cs.CV;;nerf
2401.07935v1;http://arxiv.org/abs/2401.07935v1;2024-01-15;6-DoF Grasp Pose Evaluation and Optimization via Transfer Learning from  NeRFs;"We address the problem of robotic grasping of known and unknown objects using
implicit behavior cloning. We train a grasp evaluation model from a small
number of demonstrations that outputs higher values for grasp candidates that
are more likely to succeed in grasping. This evaluation model serves as an
objective function, that we maximize to identify successful grasps. Key to our
approach is the utilization of learned implicit representations of visual and
geometric features derived from a pre-trained NeRF. Though trained exclusively
in a simulated environment with simplified objects and 4-DoF top-down grasps,
our evaluation model and optimization procedure demonstrate generalization to
6-DoF grasps and novel objects both in simulation and in real-world settings,
without the need for additional data. Supplementary material is available at:
https://gergely-soti.github.io/grasp";Gergely Sóti<author:sep>Xi Huang<author:sep>Christian Wurll<author:sep>Björn Hein;http://arxiv.org/pdf/2401.07935v1;cs.RO;;nerf
2401.05750v1;http://arxiv.org/abs/2401.05750v1;2024-01-11;GO-NeRF: Generating Virtual Objects in Neural Radiance Fields;"Despite advances in 3D generation, the direct creation of 3D objects within
an existing 3D scene represented as NeRF remains underexplored. This process
requires not only high-quality 3D object generation but also seamless
composition of the generated 3D content into the existing NeRF. To this end, we
propose a new method, GO-NeRF, capable of utilizing scene context for
high-quality and harmonious 3D object generation within an existing NeRF. Our
method employs a compositional rendering formulation that allows the generated
3D objects to be seamlessly composited into the scene utilizing learned
3D-aware opacity maps without introducing unintended scene modification.
Moreover, we also develop tailored optimization objectives and training
strategies to enhance the model's ability to exploit scene context and mitigate
artifacts, such as floaters, originating from 3D object generation within a
scene. Extensive experiments on both feed-forward and $360^o$ scenes show the
superior performance of our proposed GO-NeRF in generating objects harmoniously
composited with surrounding scenes and synthesizing high-quality novel view
images. Project page at {\url{https://daipengwa.github.io/GO-NeRF/}.";Peng Dai<author:sep>Feitong Tan<author:sep>Xin Yu<author:sep>Yinda Zhang<author:sep>Xiaojuan Qi;http://arxiv.org/pdf/2401.05750v1;cs.CV;12 pages;nerf
2401.06003v1;http://arxiv.org/abs/2401.06003v1;2024-01-11;TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering;"Point-based radiance field rendering has demonstrated impressive results for
novel view synthesis, offering a compelling blend of rendering quality and
computational efficiency. However, also latest approaches in this domain are
not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.
2023] struggles when tasked with rendering highly detailed scenes, due to
blurring and cloudy artifacts. On the other hand, ADOP [R\""uckert et al. 2022]
can accommodate crisper images, but the neural reconstruction network decreases
performance, it grapples with temporal instability and it is unable to
effectively address large gaps in the point cloud.
  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that
combines ideas from both Gaussian Splatting and ADOP. The fundamental concept
behind our novel technique involves rasterizing points into a screen-space
image pyramid, with the selection of the pyramid layer determined by the
projected point size. This approach allows rendering arbitrarily large points
using a single trilinear write. A lightweight neural network is then used to
reconstruct a hole-free image including detail beyond splat resolution.
Importantly, our render pipeline is entirely differentiable, allowing for
automatic optimization of both point sizes and positions.
  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art
methods in terms of rendering quality while maintaining a real-time frame rate
of 60 frames per second on readily available hardware. This performance extends
to challenging scenarios, such as scenes featuring intricate geometry,
expansive landscapes, and auto-exposed footage.";Linus Franke<author:sep>Darius Rückert<author:sep>Laura Fink<author:sep>Marc Stamminger;http://arxiv.org/pdf/2401.06003v1;cs.CV;;gaussian splatting
2401.06116v1;http://arxiv.org/abs/2401.06116v1;2024-01-11;Gaussian Shadow Casting for Neural Characters;"Neural character models can now reconstruct detailed geometry and texture
from video, but they lack explicit shadows and shading, leading to artifacts
when generating novel views and poses or during relighting. It is particularly
difficult to include shadows as they are a global effect and the required
casting of secondary rays is costly. We propose a new shadow model using a
Gaussian density proxy that replaces sampling with a simple analytic formula.
It supports dynamic motion and is tailored for shadow computation, thereby
avoiding the affine projection approximation and sorting required by the
closely related Gaussian splatting. Combined with a deferred neural rendering
model, our Gaussian shadows enable Lambertian shading and shadow casting with
minimal overhead. We demonstrate improved reconstructions, with better
separation of albedo, shading, and shadows in challenging outdoor scenes with
direct sun light and hard shadows. Our method is able to optimize the light
direction without any input from the user. As a result, novel poses have fewer
shadow artifacts and relighting in novel scenes is more realistic compared to
the state-of-the-art methods, providing new ways to pose neural characters in
novel environments, increasing their applicability.";Luis Bolanos<author:sep>Shih-Yang Su<author:sep>Helge Rhodin;http://arxiv.org/pdf/2401.06116v1;cs.CV;14 pages, 13 figures;gaussian splatting
2401.05925v2;http://arxiv.org/abs/2401.05925v2;2024-01-11;CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with  Dual Feature Fusion;"We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a
method for compact 3D-consistent scene segmentation at fast rendering speed
with only RGB images input. Previous NeRF-based segmentation methods have
relied on time-consuming neural scene optimization. While recent 3D Gaussian
Splatting has notably improved speed, existing Gaussian-based segmentation
methods struggle to produce compact masks, especially in zero-shot
segmentation. This issue probably stems from their straightforward assignment
of learnable parameters to each Gaussian, resulting in a lack of robustness
against cross-view inconsistent 2D machine-generated labels. Our method aims to
address this problem by employing Dual Feature Fusion Network as Gaussians'
segmentation field. Specifically, we first optimize 3D Gaussians under RGB
supervision. After Gaussian Locating, DINO features extracted from images are
applied through explicit unprojection, which are further incorporated with
spatial features from the efficient point cloud processing network. Feature
aggregation is utilized to fuse them in a global-to-local strategy for compact
segmentation features. Experimental results show that our model outperforms
baselines on both semantic and panoptic zero-shot segmentation task, meanwhile
consumes less than 10\% inference time compared to NeRF-based methods. Code and
more results will be available at https://David-Dou.github.io/CoSSegGaussians.";Bin Dou<author:sep>Tianyu Zhang<author:sep>Yongjia Ma<author:sep>Zhaohui Wang<author:sep>Zejian Yuan;http://arxiv.org/pdf/2401.05925v2;cs.CV;Correct writing details;nerf
2401.06191v1;http://arxiv.org/abs/2401.06191v1;2024-01-11;TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation;"In recent years, the neural radiance field (NeRF) model has gained popularity
due to its ability to recover complex 3D scenes. Following its success, many
approaches proposed different NeRF representations in order to further improve
both runtime and performance. One such example is Triplane, in which NeRF is
represented using three 2D feature planes. This enables easily using existing
2D neural networks in this framework, e.g., to generate the three planes.
Despite its advantage, the triplane representation lagged behind in its 3D
recovery quality compared to NeRF solutions. In this work, we propose
TriNeRFLet, a 2D wavelet-based multiscale triplane representation for NeRF,
which closes the 3D recovery performance gap and is competitive with current
state-of-the-art methods. Building upon the triplane framework, we also propose
a novel super-resolution (SR) technique that combines a diffusion model with
TriNeRFLet for improving NeRF resolution.";Rajaei Khatib<author:sep>Raja Giryes;http://arxiv.org/pdf/2401.06191v1;cs.CV;webpage link: https://rajaeekh.github.io/trinerflet-web;nerf
2401.06052v1;http://arxiv.org/abs/2401.06052v1;2024-01-11;Fast High Dynamic Range Radiance Fields for Dynamic Scenes;"Neural Radiances Fields (NeRF) and their extensions have shown great success
in representing 3D scenes and synthesizing novel-view images. However, most
NeRF methods take in low-dynamic-range (LDR) images, which may lose details,
especially with nonuniform illumination. Some previous NeRF methods attempt to
introduce high-dynamic-range (HDR) techniques but mainly target static scenes.
To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF
framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images
captured with various exposures. A learnable exposure mapping function is
constructed to obtain adaptive exposure values for each image. Based on the
monotonically increasing prior, a camera response function is designed for
stable learning. With the proposed model, high-quality novel-view images at any
time point can be rendered with any desired exposure. We further construct a
dataset containing multiple dynamic scenes captured with diverse exposures for
evaluation. All the datasets and code are available at
\url{https://guanjunwu.github.io/HDR-HexPlane/}.";Guanjun Wu<author:sep>Taoran Yi<author:sep>Jiemin Fang<author:sep>Wenyu Liu<author:sep>Xinggang Wang;http://arxiv.org/pdf/2401.06052v1;cs.CV;3DV 2024. Project page: https://guanjunwu.github.io/HDR-HexPlane;nerf
2401.05335v1;http://arxiv.org/abs/2401.05335v1;2024-01-10;InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes;"We introduce InseRF, a novel method for generative object insertion in the
NeRF reconstructions of 3D scenes. Based on a user-provided textual description
and a 2D bounding box in a reference viewpoint, InseRF generates new objects in
3D scenes. Recently, methods for 3D scene editing have been profoundly
transformed, owing to the use of strong priors of text-to-image diffusion
models in 3D generative modeling. Existing methods are mostly effective in
editing 3D scenes via style and appearance changes or removing existing
objects. Generating new objects, however, remains a challenge for such methods,
which we address in this study. Specifically, we propose grounding the 3D
object insertion to a 2D object insertion in a reference view of the scene. The
2D edit is then lifted to 3D using a single-view object reconstruction method.
The reconstructed object is then inserted into the scene, guided by the priors
of monocular depth estimation methods. We evaluate our method on various 3D
scenes and provide an in-depth analysis of the proposed components. Our
experiments with generative insertion of objects in several 3D scenes indicate
the effectiveness of our method compared to the existing methods. InseRF is
capable of controllable and 3D-consistent object insertion without requiring
explicit 3D information as input. Please visit our project page at
https://mohamad-shahbazi.github.io/inserf.";Mohamad Shahbazi<author:sep>Liesbeth Claessens<author:sep>Michael Niemeyer<author:sep>Edo Collins<author:sep>Alessio Tonioni<author:sep>Luc Van Gool<author:sep>Federico Tombari;http://arxiv.org/pdf/2401.05335v1;cs.CV;;nerf
2401.04861v1;http://arxiv.org/abs/2401.04861v1;2024-01-10;CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from  Monocular Video;"The goal of our work is to generate high-quality novel views from monocular
videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have
shown impressive performance by leveraging time-varying dynamic radiation
fields. However, these methods have limitations when it comes to accurately
modeling the motion of complex objects, which can lead to inaccurate and blurry
renderings of details. To address this limitation, we propose a novel approach
that builds upon a recent generalization NeRF, which aggregates nearby views
onto new viewpoints. However, such methods are typically only effective for
static scenes. To overcome this challenge, we introduce a module that operates
in both the time and frequency domains to aggregate the features of object
motion. This allows us to learn the relationship between frames and generate
higher-quality images. Our experiments demonstrate significant improvements
over state-of-the-art methods on dynamic scene datasets. Specifically, our
approach outperforms existing methods in terms of both the accuracy and visual
quality of the synthesized views.";Xingyu Miao<author:sep>Yang Bai<author:sep>Haoran Duan<author:sep>Yawen Huang<author:sep>Fan Wan<author:sep>Yang Long<author:sep>Yefeng Zheng;http://arxiv.org/pdf/2401.04861v1;cs.CV;;nerf
2401.05516v1;http://arxiv.org/abs/2401.05516v1;2024-01-10;FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D  Neural Radiance Fields;"We present FPRF, a feed-forward photorealistic style transfer method for
large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with
arbitrary, multiple style reference images without additional optimization
while preserving multi-view appearance consistency. Prior arts required tedious
per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF
efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D
neural radiance field, which inherits AdaIN's feed-forward stylization
machinery, supporting arbitrary style reference images. Furthermore, FPRF
supports multi-reference stylization with the semantic correspondence matching
and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also
preserves multi-view consistency by applying semantic matching and style
transfer processes directly onto queried features in 3D space. In experiments,
we demonstrate that FPRF achieves favorable photorealistic quality 3D scene
stylization for large-scale scenes with diverse reference images. Project page:
https://kim-geonu.github.io/FPRF/";GeonU Kim<author:sep>Kim Youwang<author:sep>Tae-Hyun Oh;http://arxiv.org/pdf/2401.05516v1;cs.CV;Project page: https://kim-geonu.github.io/FPRF/;
2401.05583v1;http://arxiv.org/abs/2401.05583v1;2024-01-10;Diffusion Priors for Dynamic View Synthesis from Monocular Videos;"Dynamic novel view synthesis aims to capture the temporal evolution of visual
content within videos. Existing methods struggle to distinguishing between
motion and structure, particularly in scenarios where camera poses are either
unknown or constrained compared to object motion. Furthermore, with information
solely from reference images, it is extremely challenging to hallucinate unseen
regions that are occluded or partially observed in the given videos. To address
these issues, we first finetune a pretrained RGB-D diffusion model on the video
frames using a customization technique. Subsequently, we distill the knowledge
from the finetuned model to a 4D representations encompassing both dynamic and
static Neural Radiance Fields (NeRF) components. The proposed pipeline achieves
geometric consistency while preserving the scene identity. We perform thorough
experiments to evaluate the efficacy of the proposed method qualitatively and
quantitatively. Our results demonstrate the robustness and utility of our
approach in challenging cases, further advancing dynamic novel view synthesis.";Chaoyang Wang<author:sep>Peiye Zhuang<author:sep>Aliaksandr Siarohin<author:sep>Junli Cao<author:sep>Guocheng Qian<author:sep>Hsin-Ying Lee<author:sep>Sergey Tulyakov;http://arxiv.org/pdf/2401.05583v1;cs.CV;;nerf
2401.04099v1;http://arxiv.org/abs/2401.04099v1;2024-01-08;AGG: Amortized Generative 3D Gaussians for Single Image to 3D;"Given the growing need for automatic 3D content creation pipelines, various
3D representations have been studied to generate 3D objects from a single
image. Due to its superior rendering efficiency, 3D Gaussian splatting-based
models have recently excelled in both 3D reconstruction and generation. 3D
Gaussian splatting approaches for image to 3D generation are often
optimization-based, requiring many computationally expensive score-distillation
steps. To overcome these challenges, we introduce an Amortized Generative 3D
Gaussian framework (AGG) that instantly produces 3D Gaussians from a single
image, eliminating the need for per-instance optimization. Utilizing an
intermediate hybrid representation, AGG decomposes the generation of 3D
Gaussian locations and other appearance attributes for joint optimization.
Moreover, we propose a cascaded pipeline that first generates a coarse
representation of the 3D data and later upsamples it with a 3D Gaussian
super-resolution module. Our method is evaluated against existing
optimization-based 3D Gaussian frameworks and sampling-based pipelines
utilizing other 3D representations, where AGG showcases competitive generation
abilities both qualitatively and quantitatively while being several orders of
magnitude faster. Project page: https://ir1d.github.io/AGG/";Dejia Xu<author:sep>Ye Yuan<author:sep>Morteza Mardani<author:sep>Sifei Liu<author:sep>Jiaming Song<author:sep>Zhangyang Wang<author:sep>Arash Vahdat;http://arxiv.org/pdf/2401.04099v1;cs.CV;Project page: https://ir1d.github.io/AGG/;gaussian splatting
2401.03771v1;http://arxiv.org/abs/2401.03771v1;2024-01-08;NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation;"The capabilities of monocular depth estimation (MDE) models are limited by
the availability of sufficient and diverse datasets. In the case of MDE models
for autonomous driving, this issue is exacerbated by the linearity of the
captured data trajectories. We propose a NeRF-based data augmentation pipeline
to introduce synthetic data with more diverse viewing directions into training
datasets and demonstrate the benefits of our approach to model performance and
robustness. Our data augmentation pipeline, which we call ""NeRFmentation"",
trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on
relevant metrics, and uses them to generate synthetic RGB-D images captured
from new viewing directions. In this work, we apply our technique in
conjunction with three state-of-the-art MDE architectures on the popular
autonomous driving dataset KITTI, augmenting its training set of the Eigen
split. We evaluate the resulting performance gain on the original test set, a
separate popular driving set, and our own synthetic test set.";Casimir Feldmann<author:sep>Niall Siegenheim<author:sep>Nikolas Hars<author:sep>Lovro Rabuzin<author:sep>Mert Ertugrul<author:sep>Luca Wolfart<author:sep>Marc Pollefeys<author:sep>Zuria Bauer<author:sep>Martin R. Oswald;http://arxiv.org/pdf/2401.03771v1;cs.CV;;nerf
2401.03890v1;http://arxiv.org/abs/2401.03890v1;2024-01-08;A Survey on 3D Gaussian Splatting;"3D Gaussian splatting (3D GS) has recently emerged as a transformative
technique in the explicit radiance field and computer graphics landscape. This
innovative approach, characterized by the utilization of millions of 3D
Gaussians, represents a significant departure from the neural radiance field
(NeRF) methodologies, which predominantly use implicit, coordinate-based models
to map spatial coordinates to pixel values. 3D GS, with its explicit scene
representations and differentiable rendering algorithms, not only promises
real-time rendering capabilities but also introduces unprecedented levels of
control and editability. This positions 3D GS as a potential game-changer for
the next generation of 3D reconstruction and representation. In the present
paper, we provide the first systematic overview of the recent developments and
critical contributions in the domain of 3D GS. We begin with a detailed
exploration of the underlying principles and the driving forces behind the
advent of 3D GS, setting the stage for understanding its significance. A focal
point of our discussion is the practical applicability of 3D GS. By
facilitating real-time performance, 3D GS opens up a plethora of applications,
ranging from virtual reality to interactive media and beyond. This is
complemented by a comparative analysis of leading 3D GS models, evaluated
across various benchmark tasks to highlight their performance and practical
utility. The survey concludes by identifying current challenges and suggesting
potential avenues for future research in this domain. Through this survey, we
aim to provide a valuable resource for both newcomers and seasoned researchers,
fostering further exploration and advancement in applicable and explicit
radiance field representation.";Guikun Chen<author:sep>Wenguan Wang;http://arxiv.org/pdf/2401.03890v1;cs.CV;Ongoing project;gaussian splatting<tag:sep>nerf
2401.03257v1;http://arxiv.org/abs/2401.03257v1;2024-01-06;RustNeRF: Robust Neural Radiance Field with Low-Quality Images;"Recent work on Neural Radiance Fields (NeRF) exploits multi-view 3D
consistency, achieving impressive results in 3D scene modeling and
high-fidelity novel-view synthesis. However, there are limitations. First,
existing methods assume enough high-quality images are available for training
the NeRF model, ignoring real-world image degradation. Second, previous methods
struggle with ambiguity in the training set due to unmodeled inconsistencies
among different views. In this work, we present RustNeRF for real-world
high-quality NeRF. To improve NeRF's robustness under real-world inputs, we
train a 3D-aware preprocessing network that incorporates real-world degradation
modeling. We propose a novel implicit multi-view guidance to address
information loss during image degradation and restoration. Extensive
experiments demonstrate RustNeRF's advantages over existing approaches under
real-world degradation. The code will be released.";Mengfei Li<author:sep>Ming Lu<author:sep>Xiaofang Li<author:sep>Shanghang Zhang;http://arxiv.org/pdf/2401.03257v1;cs.CV;;nerf
2401.03203v1;http://arxiv.org/abs/2401.03203v1;2024-01-06;Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity  Monocular Dense Mapping;"In this paper, we introduce Hi-Map, a novel monocular dense mapping approach
based on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to
achieve efficient and high-fidelity mapping using only posed RGB inputs. Our
method eliminates the need for external depth priors derived from e.g., a depth
estimation model. Our key idea is to represent the scene as a hierarchical
feature grid that encodes the radiance and then factorizes it into feature
planes and vectors. As such, the scene representation becomes simpler and more
generalizable for fast and smooth convergence on new observations. This allows
for efficient computation while alleviating noise patterns by reducing the
complexity of the scene representation. Buttressed by the hierarchical
factorized representation, we leverage the Sign Distance Field (SDF) as a proxy
of rendering for inferring the volume density, demonstrating high mapping
fidelity. Moreover, we introduce a dual-path encoding strategy to strengthen
the photometric cues and further boost the mapping quality, especially for the
distant and textureless regions. Extensive experiments demonstrate our method's
superiority in geometric and textural accuracy over the state-of-the-art
NeRF-based monocular mapping methods.";Tongyan Hua<author:sep>Haotian Bai<author:sep>Zidong Cao<author:sep>Ming Liu<author:sep>Dacheng Tao<author:sep>Lin Wang;http://arxiv.org/pdf/2401.03203v1;cs.CV;;nerf
2401.02620v1;http://arxiv.org/abs/2401.02620v1;2024-01-05;Progress and Prospects in 3D Generative AI: A Technical Overview  including 3D human;"While AI-generated text and 2D images continue to expand its territory, 3D
generation has gradually emerged as a trend that cannot be ignored. Since the
year 2023 an abundant amount of research papers has emerged in the domain of 3D
generation. This growth encompasses not just the creation of 3D objects, but
also the rapid development of 3D character and motion generation. Several key
factors contribute to this progress. The enhanced fidelity in stable diffusion,
coupled with control methods that ensure multi-view consistency, and realistic
human models like SMPL-X, contribute synergistically to the production of 3D
models with remarkable consistency and near-realistic appearances. The
advancements in neural network-based 3D storing and rendering models, such as
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have
accelerated the efficiency and realism of neural rendered models. Furthermore,
the multimodality capabilities of large language models have enabled language
inputs to transcend into human motion outputs. This paper aims to provide a
comprehensive overview and summary of the relevant papers published mostly
during the latter half year of 2023. It will begin by discussing the AI
generated object models in 3D, followed by the generated 3D human models, and
finally, the generated 3D human motions, culminating in a conclusive summary
and a vision for the future.";Song Bai<author:sep>Jie Li;http://arxiv.org/pdf/2401.02620v1;cs.AI;;gaussian splatting<tag:sep>nerf
2401.02588v1;http://arxiv.org/abs/2401.02588v1;2024-01-05;Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting;"The accelerating deployment of spacecraft in orbit have generated interest in
on-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possible unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. This requires robust characterization of the
target's geometry. In this article, we present an approach for mapping
geometries of satellites on orbit based on 3D Gaussian Splatting that can run
on computing resources available on current spaceflight hardware. We
demonstrate model training and 3D rendering performance on a
hardware-in-the-loop satellite mock-up under several realistic lighting and
motion conditions. Our model is shown to be capable of training on-board and
rendering higher quality novel views of an unknown satellite nearly 2 orders of
magnitude faster than previous NeRF-based algorithms. Such on-board
capabilities are critical to enable downstream machine intelligence tasks
necessary for autonomous guidance, navigation, and control tasks.";Van Minh Nguyen<author:sep>Emma Sandidge<author:sep>Trupti Mahendrakar<author:sep>Ryan T. White;http://arxiv.org/pdf/2401.02588v1;cs.CV;11 pages, 5 figures;gaussian splatting<tag:sep>nerf
2401.02616v1;http://arxiv.org/abs/2401.02616v1;2024-01-05;FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face  Video Editing on Dynamic NeRF;"The success of the GAN-NeRF structure has enabled face editing on NeRF to
maintain 3D view consistency. However, achieving simultaneously multi-view
consistency and temporal coherence while editing video sequences remains a
formidable challenge. This paper proposes a novel face video editing
architecture built upon the dynamic face GAN-NeRF structure, which effectively
utilizes video sequences to restore the latent code and 3D face geometry. By
editing the latent code, multi-view consistent editing on the face can be
ensured, as validated by multiview stereo reconstruction on the resulting
edited images in our dynamic NeRF. As the estimation of face geometries occurs
on a frame-by-frame basis, this may introduce a jittering issue. We propose a
stabilizer that maintains temporal coherence by preserving smooth changes of
face expressions in consecutive frames. Quantitative and qualitative analyses
reveal that our method, as the pioneering 4D face video editor, achieves
state-of-the-art performance in comparison to existing 2D or 3D-based
approaches independently addressing identity and motion. Codes will be
released.";Hao Zhang<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2401.02616v1;cs.CV;Our code will be available at: https://github.com/ZHANG1023/FED-NeRF;nerf
2401.02281v1;http://arxiv.org/abs/2401.02281v1;2024-01-04;PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for  6DOF Object Pose Dataset Generation;"We introduce Physically Enhanced Gaussian Splatting Simulation System
(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset
generator based on 3D Gaussian Splatting. Environment and object
representations can be easily obtained using commodity cameras to reconstruct
with Gaussian Splatting. PEGASUS allows the composition of new scenes by
merging the respective underlying Gaussian Splatting point cloud of an
environment with one or multiple objects. Leveraging a physics engine enables
the simulation of natural object placement within a scene through interaction
between meshes extracted for the objects and the environment. Consequently, an
extensive amount of new scenes - static or dynamic - can be created by
combining different environments and objects. By rendering scenes from various
perspectives, diverse data points such as RGB images, depth maps, semantic
masks, and 6DoF object poses can be extracted. Our study demonstrates that
training on data generated by PEGASUS enables pose estimation networks to
successfully transfer from synthetic data to real-world data. Moreover, we
introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This
dataset includes spherical scans that captures images from both object
hemisphere and the Gaussian Splatting reconstruction, making them compatible
with PEGASUS.";Lukas Meyer<author:sep>Floris Erich<author:sep>Yusuke Yoshiyasu<author:sep>Marc Stamminger<author:sep>Noriaki Ando<author:sep>Yukiyasu Domae;http://arxiv.org/pdf/2401.02281v1;cs.CV;Project Page: https://meyerls.github.io/pegasus_web;gaussian splatting
2401.01970v1;http://arxiv.org/abs/2401.01970v1;2024-01-03;FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D  Scene Understanding;"Precisely perceiving the geometric and semantic properties of real-world 3D
objects is crucial for the continued evolution of augmented reality and robotic
applications. To this end, we present \algfull{} (\algname{}), which
incorporates vision-language embeddings of foundation models into 3D Gaussian
Splatting (GS). The key contribution of this work is an efficient method to
reconstruct and represent 3D vision-language models. This is achieved by
distilling feature maps generated from image-based foundation models into those
rendered from our 3D model. To ensure high-quality rendering and fast training,
we introduce a novel scene representation by integrating strengths from both GS
and multi-resolution hash encodings (MHE). Our effective training procedure
also introduces a pixel alignment loss that makes the rendered feature distance
of same semantic entities close, following the pixel-level semantic boundaries.
Our results demonstrate remarkable multi-view semantic consistency,
facilitating diverse downstream tasks, beating state-of-the-art methods by
$\mathbf{10.2}$ percent on open-vocabulary language-based object detection,
despite that we are $\mathbf{851\times}$ faster for inference. This research
explores the intersection of vision, language, and 3D scene representation,
paving the way for enhanced scene understanding in uncontrolled real-world
environments. We plan to release the code upon paper acceptance.";Xingxing Zuo<author:sep>Pouya Samangouei<author:sep>Yunwen Zhou<author:sep>Yan Di<author:sep>Mingyang Li;http://arxiv.org/pdf/2401.01970v1;cs.CV;19 pages, Project page coming soon;gaussian splatting
2401.01647v1;http://arxiv.org/abs/2401.01647v1;2024-01-03;SIGNeRF: Scene Integrated Generation for Neural Radiance Fields;"Advances in image diffusion models have recently led to notable improvements
in the generation of high-quality images. In combination with Neural Radiance
Fields (NeRFs), they enabled new opportunities in 3D generation. However, most
generative 3D approaches are object-centric and applying them to editing
existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel
approach for fast and controllable NeRF scene editing and scene-integrated
object generation. A new generative update strategy ensures 3D consistency
across the edited images, without requiring iterative optimization. We find
that depth-conditioned diffusion models inherently possess the capability to
generate 3D consistent views by requesting a grid of images instead of single
views. Based on these insights, we introduce a multi-view reference sheet of
modified images. Our method updates an image collection consistently based on
the reference sheet and refines the original NeRF with the newly generated
image set in one go. By exploiting the depth conditioning mechanism of the
image diffusion model, we gain fine control over the spatial location of the
edit and enforce shape guidance by a selected region or an external mesh.";Jan-Niklas Dihlmann<author:sep>Andreas Engelhardt<author:sep>Hendrik Lensch;http://arxiv.org/pdf/2401.01647v1;cs.CV;Project Page: https://signerf.jdihlmann.com;nerf
2401.01339v1;http://arxiv.org/abs/2401.01339v1;2024-01-02;Street Gaussians for Modeling Dynamic Urban Scenes;"This paper aims to tackle the problem of modeling dynamic urban street scenes
from monocular videos. Recent methods extend NeRF by incorporating tracked
vehicle poses to animate vehicles, enabling photo-realistic view synthesis of
dynamic urban street scenes. However, significant limitations are their slow
training and rendering speed, coupled with the critical need for high precision
in tracked vehicle poses. We introduce Street Gaussians, a new explicit scene
representation that tackles all these limitations. Specifically, the dynamic
urban street is represented as a set of point clouds equipped with semantic
logits and 3D Gaussians, each associated with either a foreground vehicle or
the background. To model the dynamics of foreground object vehicles, each
object point cloud is optimized with optimizable tracked poses, along with a
dynamic spherical harmonics model for the dynamic appearance. The explicit
representation allows easy composition of object vehicles and background, which
in turn allows for scene editing operations and rendering at 133 FPS
(1066$\times$1600 resolution) within half an hour of training. The proposed
method is evaluated on multiple challenging benchmarks, including KITTI and
Waymo Open datasets. Experiments show that the proposed method consistently
outperforms state-of-the-art methods across all datasets. Furthermore, the
proposed representation delivers performance on par with that achieved using
precise ground-truth poses, despite relying only on poses from an off-the-shelf
tracker. The code is available at https://zju3dv.github.io/street_gaussians/.";Yunzhi Yan<author:sep>Haotong Lin<author:sep>Chenxu Zhou<author:sep>Weijie Wang<author:sep>Haiyang Sun<author:sep>Kun Zhan<author:sep>Xianpeng Lang<author:sep>Xiaowei Zhou<author:sep>Sida Peng;http://arxiv.org/pdf/2401.01339v1;cs.CV;Project page: https://zju3dv.github.io/street_gaussians/;nerf
2401.01216v1;http://arxiv.org/abs/2401.01216v1;2024-01-02;Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable  Noise;"Neural radiance fields (NeRF) have been proposed as an innovative 3D
representation method. While attracting lots of attention, NeRF faces critical
issues such as information confidentiality and security. Steganography is a
technique used to embed information in another object as a means of protecting
information security. Currently, there are few related studies on NeRF
steganography, facing challenges in low steganography quality, model weight
damage, and a limited amount of steganographic information. This paper proposes
a novel NeRF steganography method based on trainable noise: Noise-NeRF.
Furthermore, we propose the Adaptive Pixel Selection strategy and Pixel
Perturbation strategy to improve the steganography quality and efficiency. The
extensive experiments on open-source datasets show that Noise-NeRF provides
state-of-the-art performances in both steganography quality and rendering
quality, as well as effectiveness in super-resolution image steganography.";Qinglong Huang<author:sep>Yong Liao<author:sep>Yanbin Hao<author:sep>Pengyuan Zhou;http://arxiv.org/pdf/2401.01216v1;cs.CV;;nerf
2401.00979v1;http://arxiv.org/abs/2401.00979v1;2024-01-02;3D Visibility-aware Generalizable Neural Radiance Fields for Interacting  Hands;"Neural radiance fields (NeRFs) are promising 3D representations for scenes,
objects, and humans. However, most existing methods require multi-view inputs
and per-scene training, which limits their real-life applications. Moreover,
current methods focus on single-subject cases, leaving scenes of interacting
hands that involve severe inter-hand occlusions and challenging view variations
remain unsolved. To tackle these issues, this paper proposes a generalizable
visibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically,
given an image of interacting hands as input, our VA-NeRF first obtains a
mesh-based representation of hands and extracts their corresponding geometric
and textural features. Subsequently, a feature fusion module that exploits the
visibility of query points and mesh vertices is introduced to adaptively merge
features of both hands, enabling the recovery of features in unseen areas.
Additionally, our VA-NeRF is optimized together with a novel discriminator
within an adversarial learning paradigm. In contrast to conventional
discriminators that predict a single real/fake label for the synthesized image,
the proposed discriminator generates a pixel-wise visibility map, providing
fine-grained supervision for unseen areas and encouraging the VA-NeRF to
improve the visual quality of synthesized images. Experiments on the
Interhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms
conventional NeRFs significantly. Project Page:
\url{https://github.com/XuanHuang0/VANeRF}.";Xuan Huang<author:sep>Hanhui Li<author:sep>Zejun Yang<author:sep>Zhisheng Wang<author:sep>Xiaodan Liang;http://arxiv.org/pdf/2401.00979v1;cs.CV;Accepted by AAAI-24;nerf
2401.00825v1;http://arxiv.org/abs/2401.00825v1;2024-01-01;Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using  Sharpness Prior;"Neural Radiance Fields (NeRF) have shown remarkable performance in neural
rendering-based novel view synthesis. However, NeRF suffers from severe visual
quality degradation when the input images have been captured under imperfect
conditions, such as poor illumination, defocus blurring, and lens aberrations.
Especially, defocus blur is quite common in the images when they are normally
captured using cameras. Although few recent studies have proposed to render
sharp images of considerably high-quality, yet they still face many key
challenges. In particular, those methods have employed a Multi-Layer Perceptron
(MLP) based NeRF, which requires tremendous computational time. To overcome
these shortcomings, this paper proposes a novel technique Sharp-NeRF -- a
grid-based NeRF that renders clean and sharp images from the input blurry
images within half an hour of training. To do so, we used several grid-based
kernels to accurately model the sharpness/blurriness of the scene. The
sharpness level of the pixels is computed to learn the spatially varying blur
kernels. We have conducted experiments on the benchmarks consisting of blurry
images and have evaluated full-reference and non-reference metrics. The
qualitative and quantitative results have revealed that our approach renders
the sharp novel views with vivid colors and fine details, and it has
considerably faster training time than the previous works. Our project page is
available at https://benhenryl.github.io/SharpNeRF/";Byeonghyeon Lee<author:sep>Howoong Lee<author:sep>Usman Ali<author:sep>Eunbyung Park;http://arxiv.org/pdf/2401.00825v1;cs.CV;Accepted to WACV 2024;nerf
2401.00834v1;http://arxiv.org/abs/2401.00834v1;2024-01-01;Deblurring 3D Gaussian Splatting;"Recent studies in Radiance Fields have paved the robust way for novel view
synthesis with their photorealistic rendering quality. Nevertheless, they
usually employ neural networks and volumetric rendering, which are costly to
train and impede their broad use in various real-time applications due to the
lengthy rendering time. Lately 3D Gaussians splatting-based approach has been
proposed to model the 3D scene, and it achieves remarkable visual quality while
rendering the images in real-time. However, it suffers from severe degradation
in the rendering quality if the training images are blurry. Blurriness commonly
occurs due to the lens defocusing, object motion, and camera shake, and it
inevitably intervenes in clean image acquisition. Several previous studies have
attempted to render clean and sharp images from blurry input images using
neural fields. The majority of those works, however, are designed only for
volumetric rendering-based neural radiance fields and are not straightforwardly
applicable to rasterization-based 3D Gaussian splatting methods. Thus, we
propose a novel real-time deblurring framework, deblurring 3D Gaussian
Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the
covariance of each 3D Gaussian to model the scene blurriness. While deblurring
3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct
fine and sharp details from blurry images. A variety of experiments have been
conducted on the benchmark, and the results have revealed the effectiveness of
our approach for deblurring. Qualitative results are available at
https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/";Byeonghyeon Lee<author:sep>Howoong Lee<author:sep>Xiangyu Sun<author:sep>Usman Ali<author:sep>Eunbyung Park;http://arxiv.org/pdf/2401.00834v1;cs.CV;19 pages, 8 figures;gaussian splatting
2401.00616v2;http://arxiv.org/abs/2401.00616v2;2024-01-01;GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for  One-shot Generalizable Neural Radiance Fields;"In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task
which targets synthesizing photo-realistic novel views given only one reference
image per scene. Previous One-shot Generalizable Neural Radiance Fields
(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,
yet suffer the blurry issue due to the encoder-only architecture that highly
relies on the limited reference image. On the other hand, recent
diffusion-based image-to-3d methods show vivid plausible results via distilling
pre-trained 2D diffusion models into a 3D representation, yet require tedious
per-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a
Generative Detail compensation framework via GAN and Diffusion that is both
inference-time finetuning-free and with vivid plausible details. In detail,
following a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a
One-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer
(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model
into the existing OG-NeRF pipeline for primarily relieving the blurry issue
with in-distribution priors captured from the training dataset, achieving a
good balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at
the fine stage, Diff3DE further leverages the pre-trained image diffusion
models to complement rich out-distribution details while maintaining decent 3D
consistency. Extensive experiments on both the synthetic and real-world
datasets show that GD$^2$-NeRF noticeably improves the details while without
per-scene finetuning.";Xiao Pan<author:sep>Zongxin Yang<author:sep>Shuai Bai<author:sep>Yi Yang;http://arxiv.org/pdf/2401.00616v2;cs.CV;"Reading with Macbook Preview is recommended for best quality;
  Submitted to Journal";nerf
2401.00871v1;http://arxiv.org/abs/2401.00871v1;2023-12-30;PlanarNeRF: Online Learning of Planar Primitives with Neural Radiance  Fields;"Identifying spatially complete planar primitives from visual data is a
crucial task in computer vision. Prior methods are largely restricted to either
2D segment recovery or simplifying 3D structures, even with extensive plane
annotations. We present PlanarNeRF, a novel framework capable of detecting
dense 3D planes through online learning. Drawing upon the neural field
representation, PlanarNeRF brings three major contributions. First, it enhances
3D plane detection with concurrent appearance and geometry knowledge. Second, a
lightweight plane fitting module is proposed to estimate plane parameters.
Third, a novel global memory bank structure with an update mechanism is
introduced, ensuring consistent cross-frame correspondence. The flexible
architecture of PlanarNeRF allows it to function in both 2D-supervised and
self-supervised solutions, in each of which it can effectively learn from
sparse training signals, significantly improving training efficiency. Through
extensive experiments, we demonstrate the effectiveness of PlanarNeRF in
various scenarios and remarkable improvement over existing works.";Zheng Chen<author:sep>Qingan Yan<author:sep>Huangying Zhan<author:sep>Changjiang Cai<author:sep>Xiangyu Xu<author:sep>Yuzhong Huang<author:sep>Weihan Wang<author:sep>Ziyue Feng<author:sep>Lantao Liu<author:sep>Yi Xu;http://arxiv.org/pdf/2401.00871v1;cs.CV;;nerf
2401.00208v1;http://arxiv.org/abs/2401.00208v1;2023-12-30;Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with  Generative Diffusion Models;"Current Neural Radiance Fields (NeRF) can generate photorealistic novel
views. For editing 3D scenes represented by NeRF, with the advent of generative
models, this paper proposes Inpaint4DNeRF to capitalize on state-of-the-art
stable diffusion models (e.g., ControlNet) for direct generation of the
underlying completed background content, regardless of static or dynamic. The
key advantages of this generative approach for NeRF inpainting are twofold.
First, after rough mask propagation, to complete or fill in previously occluded
content, we can individually generate a small subset of completed images with
plausible content, called seed images, from which simple 3D geometry proxies
can be derived. Second and the remaining problem is thus 3D multiview
consistency among all completed images, now guided by the seed images and their
3D proxies. Without other bells and whistles, our generative Inpaint4DNeRF
baseline framework is general which can be readily extended to 4D dynamic
NeRFs, where temporal consistency can be naturally handled in a similar way as
our multiview consistency.";Han Jiang<author:sep>Haosen Sun<author:sep>Ruoxuan Li<author:sep>Chi-Keung Tang<author:sep>Yu-Wing Tai;http://arxiv.org/pdf/2401.00208v1;cs.CV;;nerf
2312.17561v1;http://arxiv.org/abs/2312.17561v1;2023-12-29;Informative Rays Selection for Few-Shot Neural Radiance Fields;"Neural Radiance Fields (NeRF) have recently emerged as a powerful method for
image-based 3D reconstruction, but the lengthy per-scene optimization limits
their practical usage, especially in resource-constrained settings. Existing
approaches solve this issue by reducing the number of input views and
regularizing the learned volumetric representation with either complex losses
or additional inputs from other modalities. In this paper, we present KeyNeRF,
a simple yet effective method for training NeRF in few-shot scenarios by
focusing on key informative rays. Such rays are first selected at camera level
by a view selection algorithm that promotes baseline diversity while
guaranteeing scene coverage, then at pixel level by sampling from a probability
distribution based on local image entropy. Our approach performs favorably
against state-of-the-art methods, while requiring minimal changes to existing
NeRF codebases.";Marco Orsingher<author:sep>Anthony Dell'Eva<author:sep>Paolo Zani<author:sep>Paolo Medici<author:sep>Massimo Bertozzi;http://arxiv.org/pdf/2312.17561v1;cs.CV;To appear at VISAPP 2024;nerf
2312.17142v2;http://arxiv.org/abs/2312.17142v2;2023-12-28;DreamGaussian4D: Generative 4D Gaussian Splatting;"Remarkable progress has been made in 4D content generation recently. However,
existing methods suffer from long optimization time, lack of motion
controllability, and a low level of detail. In this paper, we introduce
DreamGaussian4D, an efficient 4D generation framework that builds on 4D
Gaussian Splatting representation. Our key insight is that the explicit
modeling of spatial transformations in Gaussian Splatting makes it more
suitable for the 4D generation setting compared with implicit representations.
DreamGaussian4D reduces the optimization time from several hours to just a few
minutes, allows flexible control of the generated 3D motion, and produces
animated meshes that can be efficiently rendered in 3D engines.";Jiawei Ren<author:sep>Liang Pan<author:sep>Jiaxiang Tang<author:sep>Chi Zhang<author:sep>Ang Cao<author:sep>Gang Zeng<author:sep>Ziwei Liu;http://arxiv.org/pdf/2312.17142v2;cs.CV;"Technical report. Project page is at
  https://jiawei-ren.github.io/projects/dreamgaussian4d Code is at
  https://github.com/jiawei-ren/dreamgaussian4d";gaussian splatting
2312.16457v1;http://arxiv.org/abs/2312.16457v1;2023-12-27;City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web;"NeRF has significantly advanced 3D scene reconstruction, capturing intricate
details across various environments. Existing methods have successfully
leveraged radiance field baking to facilitate real-time rendering of small
scenes. However, when applied to large-scale scenes, these techniques encounter
significant challenges, struggling to provide a seamless real-time experience
due to limited resources in computation, memory, and bandwidth. In this paper,
we propose City-on-Web, which represents the whole scene by partitioning it
into manageable blocks, each with its own Level-of-Detail, ensuring high
fidelity, efficient memory management and fast rendering. Meanwhile, we
carefully design the training and inference process such that the final
rendering result on web is consistent with training. Thanks to our novel
representation and carefully designed training/inference process, we are the
first to achieve real-time rendering of large-scale scenes in
resource-constrained environments. Extensive experimental results demonstrate
that our method facilitates real-time rendering of large-scale scenes on a web
platform, achieving 32FPS at 1080P resolution with an RTX 3060 GPU, while
simultaneously achieving a quality that closely rivals that of state-of-the-art
methods. Project page: https://ustc3dv.github.io/City-on-Web/";Kaiwen Song<author:sep>Juyong Zhang;http://arxiv.org/pdf/2312.16457v1;cs.CV;Project page: https://ustc3dv.github.io/City-on-Web/;nerf
2312.15942v1;http://arxiv.org/abs/2312.15942v1;2023-12-26;Pano-NeRF: Synthesizing High Dynamic Range Novel Views with Geometry  from Sparse Low Dynamic Range Panoramic Images;"Panoramic imaging research on geometry recovery and High Dynamic Range (HDR)
reconstruction becomes a trend with the development of Extended Reality (XR).
Neural Radiance Fields (NeRF) provide a promising scene representation for both
tasks without requiring extensive prior data. However, in the case of inputting
sparse Low Dynamic Range (LDR) panoramic images, NeRF often degrades with
under-constrained geometry and is unable to reconstruct HDR radiance from LDR
inputs. We observe that the radiance from each pixel in panoramic images can be
modeled as both a signal to convey scene lighting information and a light
source to illuminate other pixels. Hence, we propose the irradiance fields from
sparse LDR panoramic images, which increases the observation counts for
faithful geometry recovery and leverages the irradiance-radiance attenuation
for HDR reconstruction. Extensive experiments demonstrate that the irradiance
fields outperform state-of-the-art methods on both geometry recovery and HDR
reconstruction and validate their effectiveness. Furthermore, we show a
promising byproduct of spatially-varying lighting estimation. The code is
available at https://github.com/Lu-Zhan/Pano-NeRF.";Zhan Lu<author:sep>Qian Zheng<author:sep>Boxin Shi<author:sep>Xudong Jiang;http://arxiv.org/pdf/2312.15942v1;cs.CV;;nerf
2312.16047v1;http://arxiv.org/abs/2312.16047v1;2023-12-26;2D-Guided 3D Gaussian Segmentation;"Recently, 3D Gaussian, as an explicit 3D representation method, has
demonstrated strong competitiveness over NeRF (Neural Radiance Fields) in terms
of expressing complex scenes and training duration. These advantages signal a
wide range of applications for 3D Gaussians in 3D understanding and editing.
Meanwhile, the segmentation of 3D Gaussians is still in its infancy. The
existing segmentation methods are not only cumbersome but also incapable of
segmenting multiple objects simultaneously in a short amount of time. In
response, this paper introduces a 3D Gaussian segmentation method implemented
with 2D segmentation as supervision. This approach uses input 2D segmentation
maps to guide the learning of the added 3D Gaussian semantic information, while
nearest neighbor clustering and statistical filtering refine the segmentation
results. Experiments show that our concise method can achieve comparable
performances on mIOU and mAcc for multi-object segmentation as previous
single-object segmentation methods.";Kun Lan<author:sep>Haoran Li<author:sep>Haolin Shi<author:sep>Wenjun Wu<author:sep>Yong Liao<author:sep>Lin Wang<author:sep>Pengyuan Zhou;http://arxiv.org/pdf/2312.16047v1;cs.CV;;nerf
2312.16256v2;http://arxiv.org/abs/2312.16256v2;2023-12-26;DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision;"We have witnessed significant progress in deep learning-based 3D vision,
ranging from neural radiance field (NeRF) based 3D representation learning to
applications in novel view synthesis (NVS). However, existing scene-level
datasets for deep learning-based 3D vision, limited to either synthetic
environments or a narrow selection of real-world scenes, are quite
insufficient. This insufficiency not only hinders a comprehensive benchmark of
existing methods but also caps what could be explored in deep learning-based 3D
analysis. To address this critical gap, we present DL3DV-10K, a large-scale
scene dataset, featuring 51.2 million frames from 10,510 videos captured from
65 types of point-of-interest (POI) locations, covering both bounded and
unbounded scenes, with different levels of reflection, transparency, and
lighting. We conducted a comprehensive benchmark of recent NVS methods on
DL3DV-10K, which revealed valuable insights for future research in NVS. In
addition, we have obtained encouraging results in a pilot study to learn
generalizable NeRF from DL3DV-10K, which manifests the necessity of a
large-scale scene-level dataset to forge a path toward a foundation model for
learning 3D representation. Our DL3DV-10K dataset, benchmark results, and
models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.";Lu Ling<author:sep>Yichen Sheng<author:sep>Zhi Tu<author:sep>Wentian Zhao<author:sep>Cheng Xin<author:sep>Kun Wan<author:sep>Lantao Yu<author:sep>Qianyu Guo<author:sep>Zixun Yu<author:sep>Yawen Lu<author:sep>Xuanmao Li<author:sep>Xingpeng Sun<author:sep>Rohan Ashok<author:sep>Aniruddha Mukherjee<author:sep>Hao Kang<author:sep>Xiangrui Kong<author:sep>Gang Hua<author:sep>Tianyi Zhang<author:sep>Bedrich Benes<author:sep>Aniket Bera;http://arxiv.org/pdf/2312.16256v2;cs.CV;;nerf
2312.16084v1;http://arxiv.org/abs/2312.16084v1;2023-12-26;LangSplat: 3D Language Gaussian Splatting;"Human lives in a 3D world and commonly uses natural language to interact with
a 3D scene. Modeling a 3D language field to support open-ended language queries
in 3D has gained increasing attention recently. This paper introduces
LangSplat, which constructs a 3D language field that enables precise and
efficient open-vocabulary querying within 3D spaces. Unlike existing methods
that ground CLIP language embeddings in a NeRF model, LangSplat advances the
field by utilizing a collection of 3D Gaussians, each encoding language
features distilled from CLIP, to represent the language field. By employing a
tile-based splatting technique for rendering language features, we circumvent
the costly rendering process inherent in NeRF. Instead of directly learning
CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and
then learns language features on the scene-specific latent space, thereby
alleviating substantial memory demands imposed by explicit modeling. Existing
methods struggle with imprecise and vague 3D language fields, which fail to
discern clear boundaries between objects. We delve into this issue and propose
to learn hierarchical semantics using SAM, thereby eliminating the need for
extensively querying the language field across various scales and the
regularization of DINO features. Extensive experiments on open-vocabulary 3D
object localization and semantic segmentation demonstrate that LangSplat
significantly outperforms the previous state-of-the-art method LERF by a large
margin. Notably, LangSplat is extremely efficient, achieving a {\speed}
$\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We
strongly recommend readers to check out our video results at
https://langsplat.github.io";Minghan Qin<author:sep>Wanhua Li<author:sep>Jiawei Zhou<author:sep>Haoqian Wang<author:sep>Hanspeter Pfister;http://arxiv.org/pdf/2312.16084v1;cs.CV;Project Page: https://langsplat.github.io;gaussian splatting<tag:sep>nerf
2312.15711v1;http://arxiv.org/abs/2312.15711v1;2023-12-25;Neural BSSRDF: Object Appearance Representation Including Heterogeneous  Subsurface Scattering;"Monte Carlo rendering of translucent objects with heterogeneous scattering
properties is often expensive both in terms of memory and computation. If we do
path tracing and use a high dynamic range lighting environment, the rendering
becomes computationally heavy. We propose a compact and efficient neural method
for representing and rendering the appearance of heterogeneous translucent
objects. The neural representation function resembles a bidirectional
scattering-surface reflectance distribution function (BSSRDF). However,
conventional BSSRDF models assume a planar half-space medium and only surface
variation of the material, which is often not a good representation of the
appearance of real-world objects. Our method represents the BSSRDF of a full
object taking its geometry and heterogeneities into account. This is similar to
a neural radiance field, but our representation works for an arbitrary distant
lighting environment. In a sense, we present a version of neural precomputed
radiance transfer that captures all-frequency relighting of heterogeneous
translucent objects. We use a multi-layer perceptron (MLP) with skip
connections to represent the appearance of an object as a function of spatial
position, direction of observation, and direction of incidence. The latter is
considered a directional light incident across the entire non-self-shadowed
part of the object. We demonstrate the ability of our method to store highly
complex materials while having high accuracy when comparing to reference images
of the represented object in unseen lighting environments. As compared with
path tracing of a heterogeneous light scattering volume behind a refractive
interface, our method more easily enables importance sampling of the directions
of incidence and can be integrated into existing rendering frameworks while
achieving interactive frame rates.";Thomson TG<author:sep>Jeppe Revall Frisvad<author:sep>Ravi Ramamoorthi<author:sep>Henrik Wann Jensen;http://arxiv.org/pdf/2312.15711v1;cs.GR;;
2312.16215v1;http://arxiv.org/abs/2312.16215v1;2023-12-24;SUNDIAL: 3D Satellite Understanding through Direct, Ambient, and Complex  Lighting Decomposition;"3D modeling from satellite imagery is essential in areas of environmental
science, urban planning, agriculture, and disaster response. However,
traditional 3D modeling techniques face unique challenges in the remote sensing
context, including limited multi-view baselines over extensive regions, varying
direct, ambient, and complex illumination conditions, and time-varying scene
changes across captures. In this work, we introduce SUNDIAL, a comprehensive
approach to 3D reconstruction of satellite imagery using neural radiance
fields. We jointly learn satellite scene geometry, illumination components, and
sun direction in this single-model approach, and propose a secondary shadow ray
casting technique to 1) improve scene geometry using oblique sun angles to
render shadows, 2) enable physically-based disentanglement of scene albedo and
illumination, and 3) determine the components of illumination from direct,
ambient (sky), and complex sources. To achieve this, we incorporate lighting
cues and geometric priors from remote sensing literature in a neural rendering
approach, modeling physical properties of satellite scenes such as shadows,
scattered sky illumination, and complex illumination and shading of vegetation
and water. We evaluate the performance of SUNDIAL against existing NeRF-based
techniques for satellite scene modeling and demonstrate improved scene and
lighting disentanglement, novel view and lighting rendering, and geometry and
sun direction estimation on challenging scenes with small baselines, sparse
inputs, and variable illumination.";Nikhil Behari<author:sep>Akshat Dave<author:sep>Kushagra Tiwary<author:sep>William Yang<author:sep>Ramesh Raskar;http://arxiv.org/pdf/2312.16215v1;cs.CV;8 pages, 6 figures;nerf
2312.15253v1;http://arxiv.org/abs/2312.15253v1;2023-12-23;Efficient Deformable Tissue Reconstruction via Orthogonal Neural Plane;"Intraoperative imaging techniques for reconstructing deformable tissues in
vivo are pivotal for advanced surgical systems. Existing methods either
compromise on rendering quality or are excessively computationally intensive,
often demanding dozens of hours to perform, which significantly hinders their
practical application. In this paper, we introduce Fast Orthogonal Plane
(Forplane), a novel, efficient framework based on neural radiance fields (NeRF)
for the reconstruction of deformable tissues. We conceptualize surgical
procedures as 4D volumes, and break them down into static and dynamic fields
comprised of orthogonal neural planes. This factorization iscretizes the
four-dimensional space, leading to a decreased memory usage and faster
optimization. A spatiotemporal importance sampling scheme is introduced to
improve performance in regions with tool occlusion as well as large motions and
accelerate training. An efficient ray marching method is applied to skip
sampling among empty regions, significantly improving inference speed. Forplane
accommodates both binocular and monocular endoscopy videos, demonstrating its
extensive applicability and flexibility. Our experiments, carried out on two in
vivo datasets, the EndoNeRF and Hamlyn datasets, demonstrate the effectiveness
of our framework. In all cases, Forplane substantially accelerates both the
optimization process (by over 100 times) and the inference process (by over 15
times) while maintaining or even improving the quality across a variety of
non-rigid deformations. This significant performance improvement promises to be
a valuable asset for future intraoperative surgical applications. The code of
our project is now available at https://github.com/Loping151/ForPlane.";Chen Yang<author:sep>Kailing Wang<author:sep>Yuehao Wang<author:sep>Qi Dou<author:sep>Xiaokang Yang<author:sep>Wei Shen;http://arxiv.org/pdf/2312.15253v1;cs.CV;;nerf
2312.16197v1;http://arxiv.org/abs/2312.16197v1;2023-12-23;INFAMOUS-NeRF: ImproviNg FAce MOdeling Using Semantically-Aligned  Hypernetworks with Neural Radiance Fields;"We propose INFAMOUS-NeRF, an implicit morphable face model that introduces
hypernetworks to NeRF to improve the representation power in the presence of
many training subjects. At the same time, INFAMOUS-NeRF resolves the classic
hypernetwork tradeoff of representation power and editability by learning
semantically-aligned latent spaces despite the subject-specific models, all
without requiring a large pretrained model. INFAMOUS-NeRF further introduces a
novel constraint to improve NeRF rendering along the face boundary. Our
constraint can leverage photometric surface rendering and multi-view
supervision to guide surface color prediction and improve rendering near the
surface. Finally, we introduce a novel, loss-guided adaptive sampling method
for more effective NeRF training by reducing the sampling redundancy. We show
quantitatively and qualitatively that our method achieves higher representation
power than prior face modeling methods in both controlled and in-the-wild
settings. Code and models will be released upon publication.";Andrew Hou<author:sep>Feng Liu<author:sep>Zhiyuan Ren<author:sep>Michel Sarkis<author:sep>Ning Bi<author:sep>Yiying Tong<author:sep>Xiaoming Liu;http://arxiv.org/pdf/2312.16197v1;cs.CV;;nerf
2312.15258v1;http://arxiv.org/abs/2312.15258v1;2023-12-23;Human101: Training 100+FPS Human Gaussians in 100s from 1 View;"Reconstructing the human body from single-view videos plays a pivotal role in
the virtual reality domain. One prevalent application scenario necessitates the
rapid reconstruction of high-fidelity 3D digital humans while simultaneously
ensuring real-time rendering and interaction. Existing methods often struggle
to fulfill both requirements. In this paper, we introduce Human101, a novel
framework adept at producing high-fidelity dynamic 3D human reconstructions
from 1-view videos by training 3D Gaussians in 100 seconds and rendering in
100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which
provides an explicit and efficient representation of 3D humans. Standing apart
from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric
Forward Gaussian Animation method to deform the parameters of 3D Gaussians,
thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an
impressive 60+ FPS and rendering 512-resolution images at 100+ FPS).
Experimental results indicate that our approach substantially eclipses current
methods, clocking up to a 10 times surge in frames per second and delivering
comparable or superior rendering quality. Code and demos will be released at
https://github.com/longxiang-ai/Human101.";Mingwei Li<author:sep>Jiachen Tao<author:sep>Zongxin Yang<author:sep>Yi Yang;http://arxiv.org/pdf/2312.15258v1;cs.CV;Website: https://github.com/longxiang-ai/Human101;gaussian splatting<tag:sep>nerf
2312.15242v1;http://arxiv.org/abs/2312.15242v1;2023-12-23;CaLDiff: Camera Localization in NeRF via Pose Diffusion;"With the widespread use of NeRF-based implicit 3D representation, the need
for camera localization in the same representation becomes manifestly apparent.
Doing so not only simplifies the localization process -- by avoiding an
outside-the-NeRF-based localization -- but also has the potential to offer the
benefit of enhanced localization. This paper studies the problem of localizing
cameras in NeRF using a diffusion model for camera pose adjustment. More
specifically, given a pre-trained NeRF model, we train a diffusion model that
iteratively updates randomly initialized camera poses, conditioned upon the
image to be localized. At test time, a new camera is localized in two steps:
first, coarse localization using the proposed pose diffusion process, followed
by local refinement steps of a pose inversion process in NeRF. In fact, the
proposed camera localization by pose diffusion (CaLDiff) method also integrates
the pose inversion steps within the diffusion process. Such integration offers
significantly better localization, thanks to our downstream refinement-aware
diffusion process. Our exhaustive experiments on challenging real-world data
validate our method by providing significantly better results than the compared
methods and the established baselines. Our source code will be made publicly
available.";Rashik Shrestha<author:sep>Bishad Koju<author:sep>Abhigyan Bhusal<author:sep>Danda Pani Paudel<author:sep>François Rameau;http://arxiv.org/pdf/2312.15242v1;cs.CV;;nerf
2312.15059v1;http://arxiv.org/abs/2312.15059v1;2023-12-22;Deformable 3D Gaussian Splatting for Animatable Human Avatars;"Recent advances in neural radiance fields enable novel view synthesis of
photo-realistic images in dynamic settings, which can be applied to scenarios
with human animation. Commonly used implicit backbones to establish accurate
models, however, require many input views and additional annotations such as
human masks, UV maps and depth maps. In this work, we propose ParDy-Human
(Parameterized Dynamic Human Avatar), a fully explicit approach to construct a
digital avatar from as little as a single monocular sequence. ParDy-Human
introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D
Gaussians are deformed by a human pose model to animate the avatar. Our method
is composed of two parts: A first module that deforms canonical 3D Gaussians
according to SMPL vertices and a consecutive module that further takes their
designed joint encodings and predicts per Gaussian deformations to deal with
dynamics beyond SMPL vertex deformations. Images are then synthesized by a
rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic
human avatars which requires significantly fewer training views and images. Our
avatars learning is free of additional annotations such as masks and can be
trained with variable backgrounds while inferring full-resolution images
efficiently even on consumer hardware. We provide experimental evidence to show
that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and
THUman4.0 datasets both quantitatively and visually.";HyunJun Jung<author:sep>Nikolas Brasch<author:sep>Jifei Song<author:sep>Eduardo Perez-Pellitero<author:sep>Yiren Zhou<author:sep>Zhihao Li<author:sep>Nassir Navab<author:sep>Benjamin Busam;http://arxiv.org/pdf/2312.15059v1;cs.CV;;gaussian splatting
2312.14915v1;http://arxiv.org/abs/2312.14915v1;2023-12-22;PoseGen: Learning to Generate 3D Human Pose Dataset with NeRF;"This paper proposes an end-to-end framework for generating 3D human pose
datasets using Neural Radiance Fields (NeRF). Public datasets generally have
limited diversity in terms of human poses and camera viewpoints, largely due to
the resource-intensive nature of collecting 3D human pose data. As a result,
pose estimators trained on public datasets significantly underperform when
applied to unseen out-of-distribution samples. Previous works proposed
augmenting public datasets by generating 2D-3D pose pairs or rendering a large
amount of random data. Such approaches either overlook image rendering or
result in suboptimal datasets for pre-trained models. Here we propose PoseGen,
which learns to generate a dataset (human 3D poses and images) with a feedback
loss from a given pre-trained pose estimator. In contrast to prior art, our
generated data is optimized to improve the robustness of the pre-trained model.
The objective of PoseGen is to learn a distribution of data that maximizes the
prediction error of a given pre-trained model. As the learned data distribution
contains OOD samples of the pre-trained model, sampling data from such a
distribution for further fine-tuning a pre-trained model improves the
generalizability of the model. This is the first work that proposes NeRFs for
3D human data generation. NeRFs are data-driven and do not require 3D scans of
humans. Therefore, using NeRF for data generation is a new direction for
convenient user-specific data generation. Our extensive experiments show that
the proposed PoseGen improves two baseline models (SPIN and HybrIK) on four
datasets with an average 6% relative improvement.";Mohsen Gholami<author:sep>Rabab Ward<author:sep>Z. Jane Wang;http://arxiv.org/pdf/2312.14915v1;cs.CV;;nerf
2312.14664v1;http://arxiv.org/abs/2312.14664v1;2023-12-22;Density Uncertainty Quantification with NeRF-Ensembles: Impact of Data  and Scene Constraints;"In the fields of computer graphics, computer vision and photogrammetry,
Neural Radiance Fields (NeRFs) are a major topic driving current research and
development. However, the quality of NeRF-generated 3D scene reconstructions
and subsequent surface reconstructions, heavily relies on the network output,
particularly the density. Regarding this critical aspect, we propose to utilize
NeRF-Ensembles that provide a density uncertainty estimate alongside the mean
density. We demonstrate that data constraints such as low-quality images and
poses lead to a degradation of the training process, increased density
uncertainty and decreased predicted density. Even with high-quality input data,
the density uncertainty varies based on scene constraints such as acquisition
constellations, occlusions and material properties. NeRF-Ensembles not only
provide a tool for quantifying the uncertainty but exhibit two promising
advantages: Enhanced robustness and artifact removal. Through the utilization
of NeRF-Ensembles instead of single NeRFs, small outliers are removed, yielding
a smoother output with improved completeness of structures. Furthermore,
applying percentile-based thresholds on density uncertainty outliers proves to
be effective for the removal of large (foggy) artifacts in post-processing. We
conduct our methodology on 3 different datasets: (i) synthetic benchmark
dataset, (ii) real benchmark dataset, (iii) real data under realistic recording
conditions and sensors.";Miriam Jäger<author:sep>Steven Landgraf<author:sep>Boris Jutzi;http://arxiv.org/pdf/2312.14664v1;cs.CV;21 pages, 12 figures, 5 tables;nerf
2312.13729v2;http://arxiv.org/abs/2312.13729v2;2023-12-21;Gaussian Splatting with NeRF-based Color and Opacity;"Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of
neural networks to capture the intricacies of 3D objects. By encoding the shape
and color information within neural network weights, NeRFs excel at producing
strikingly sharp novel views of 3D objects. Recently, numerous generalizations
of NeRFs utilizing generative models have emerged, expanding its versatility.
In contrast, Gaussian Splatting (GS) offers a similar renders quality with
faster training and inference as it does not need neural networks to work. We
encode information about the 3D objects in the set of Gaussian distributions
that can be rendered in 3D similarly to classical meshes. Unfortunately, GS are
difficult to condition since they usually require circa hundred thousand
Gaussian components. To mitigate the caveats of both models, we propose a
hybrid model that uses GS representation of the 3D object's shape and
NeRF-based encoding of color and opacity. Our model uses Gaussian distributions
with trainable positions (i.e. means of Gaussian), shape (i.e. covariance of
Gaussian), color and opacity, and neural network, which takes parameters of
Gaussian and viewing direction to produce changes in color and opacity.
Consequently, our model better describes shadows, light reflections, and
transparency of 3D objects.";Dawid Malarz<author:sep>Weronika Smolak<author:sep>Jacek Tabor<author:sep>Sławomir Tadeja<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2312.13729v2;cs.CV;;gaussian splatting<tag:sep>nerf
2312.13832v1;http://arxiv.org/abs/2312.13832v1;2023-12-21;SyncDreamer for 3D Reconstruction of Endangered Animal Species with NeRF  and NeuS;"The main aim of this study is to demonstrate how innovative view synthesis
and 3D reconstruction techniques can be used to create models of endangered
species using monocular RGB images. To achieve this, we employed SyncDreamer to
produce unique perspectives and NeuS and NeRF to reconstruct 3D
representations. We chose four different animals, including the oriental stork,
frog, dragonfly, and tiger, as our subjects for this study. Our results show
that the combination of SyncDreamer, NeRF, and NeuS techniques can successfully
create 3D models of endangered animals. However, we also observed that NeuS
produced blurry images, while NeRF generated sharper but noisier images. This
study highlights the potential of modeling endangered animals and offers a new
direction for future research in this field. By showcasing the effectiveness of
these advanced techniques, we hope to encourage further exploration and
development of techniques for preserving and studying endangered species.";Ahmet Haydar Ornek<author:sep>Deniz Sen<author:sep>Esmanur Civil;http://arxiv.org/pdf/2312.13832v1;cs.CV;8 figures;nerf
2312.13528v1;http://arxiv.org/abs/2312.13528v1;2023-12-21;DyBluRF: Dynamic Deblurring Neural Radiance Fields for Blurry Monocular  Video;"Video view synthesis, allowing for the creation of visually appealing frames
from arbitrary viewpoints and times, offers immersive viewing experiences.
Neural radiance fields, particularly NeRF, initially developed for static
scenes, have spurred the creation of various methods for video view synthesis.
However, the challenge for video view synthesis arises from motion blur, a
consequence of object or camera movement during exposure, which hinders the
precise synthesis of sharp spatio-temporal views. In response, we propose a
novel dynamic deblurring NeRF framework for blurry monocular video, called
DyBluRF, consisting of an Interleave Ray Refinement (IRR) stage and a Motion
Decomposition-based Deblurring (MDD) stage. Our DyBluRF is the first that
addresses and handles the novel view synthesis for blurry monocular video. The
IRR stage jointly reconstructs dynamic 3D scenes and refines the inaccurate
camera pose information to combat imprecise pose information extracted from the
given blurry frames. The MDD stage is a novel incremental latent sharp-rays
prediction (ILSP) approach for the blurry monocular video frames by decomposing
the latent sharp rays into global camera motion and local object motion
components. Extensive experimental results demonstrate that our DyBluRF
outperforms qualitatively and quantitatively the very recent state-of-the-art
methods. Our project page including source codes and pretrained model are
publicly available at https://kaist-viclab.github.io/dyblurf-site/.";Minh-Quan Viet Bui<author:sep>Jongmin Park<author:sep>Jihyong Oh<author:sep>Munchurl Kim;http://arxiv.org/pdf/2312.13528v1;cs.CV;"The first three authors contributed equally to this work. Please
  visit our project page at https://kaist-viclab.github.io/dyblurf-site/";nerf
2312.14239v1;http://arxiv.org/abs/2312.14239v1;2023-12-21;PlatoNeRF: 3D Reconstruction in Plato's Cave via Single-View Two-Bounce  Lidar;"3D reconstruction from a single-view is challenging because of the ambiguity
from monocular cues and lack of information about occluded regions. Neural
radiance fields (NeRF), while popular for view synthesis and 3D reconstruction,
are typically reliant on multi-view images. Existing methods for single-view 3D
reconstruction with NeRF rely on either data priors to hallucinate views of
occluded regions, which may not be physically accurate, or shadows observed by
RGB cameras, which are difficult to detect in ambient light and low albedo
backgrounds. We propose using time-of-flight data captured by a single-photon
avalanche diode to overcome these limitations. Our method models two-bounce
optical paths with NeRF, using lidar transient data for supervision. By
leveraging the advantages of both NeRF and two-bounce light measured by lidar,
we demonstrate that we can reconstruct visible and occluded geometry without
data priors or reliance on controlled ambient lighting or scene albedo. In
addition, we demonstrate improved generalization under practical constraints on
sensor spatial- and temporal-resolution. We believe our method is a promising
direction as single-photon lidars become ubiquitous on consumer devices, such
as phones, tablets, and headsets.";Tzofi Klinghoffer<author:sep>Xiaoyu Xiang<author:sep>Siddharth Somasundaram<author:sep>Yuchen Fan<author:sep>Christian Richardt<author:sep>Ramesh Raskar<author:sep>Rakesh Ranjan;http://arxiv.org/pdf/2312.14239v1;cs.CV;Project Page: https://platonerf.github.io/;nerf
2312.14124v1;http://arxiv.org/abs/2312.14124v1;2023-12-21;Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance  Generation;"Controllable generation of 3D assets is important for many practical
applications like content creation in movies, games and engineering, as well as
in AR/VR. Recently, diffusion models have shown remarkable results in
generation quality of 3D objects. However, none of the existing models enable
disentangled generation to control the shape and appearance separately. For the
first time, we present a suitable representation for 3D diffusion models to
enable such disentanglement by introducing a hybrid point cloud and neural
radiance field approach. We model a diffusion process over point positions
jointly with a high-dimensional feature space for a local density and radiance
decoder. While the point positions represent the coarse shape of the object,
the point features allow modeling the geometry and appearance details. This
disentanglement enables us to sample both independently and therefore to
control both separately. Our approach sets a new state of the art in generation
compared to previous disentanglement-capable methods by reduced FID scores of
30-90% and is on-par with other non disentanglement-capable state-of-the art
methods.";Philipp Schröppel<author:sep>Christopher Wewer<author:sep>Jan Eric Lenssen<author:sep>Eddy Ilg<author:sep>Thomas Brox;http://arxiv.org/pdf/2312.14124v1;cs.CV;;
2312.13494v1;http://arxiv.org/abs/2312.13494v1;2023-12-21;Visual Tomography: Physically Faithful Volumetric Models of Partially  Translucent Objects;"When created faithfully from real-world data, Digital 3D representations of
objects can be useful for human or computer-assisted analysis. Such models can
also serve for generating training data for machine learning approaches in
settings where data is difficult to obtain or where too few training data
exists, e.g. by providing novel views or images in varying conditions. While
the vast amount of visual 3D reconstruction approaches focus on non-physical
models, textured object surfaces or shapes, in this contribution we propose a
volumetric reconstruction approach that obtains a physical model including the
interior of partially translucent objects such as plankton or insects. Our
technique photographs the object under different poses in front of a bright
white light source and computes absorption and scattering per voxel. It can be
interpreted as visual tomography that we solve by inverse raytracing. We
additionally suggest a method to convert non-physical NeRF media into a
physically-based volumetric grid for initialization and illustrate the
usefulness of the approach using two real-world plankton validation sets, the
lab-scanned models being finally also relighted and virtually submerged in a
scenario with augmented medium and illumination conditions. Please visit the
project homepage at www.marine.informatik.uni-kiel.de/go/vito";David Nakath<author:sep>Xiangyu Weng<author:sep>Mengkun She<author:sep>Kevin Köser;http://arxiv.org/pdf/2312.13494v1;cs.CV;Accepted for publication at 3DV '24;nerf
2312.13980v1;http://arxiv.org/abs/2312.13980v1;2023-12-21;Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion  Models with RL Finetuning;"Recent advancements in the text-to-3D task leverage finetuned text-to-image
diffusion models to generate multi-view images, followed by NeRF
reconstruction. Yet, existing supervised finetuned (SFT) diffusion models still
suffer from multi-view inconsistency and the resulting NeRF artifacts. Although
training longer with SFT improves consistency, it also causes distribution
shift, which reduces diversity and realistic details. We argue that the SFT of
multi-view diffusion models resembles the instruction finetuning stage of the
LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods.
Essentially, RLFT methods optimize models beyond their SFT data distribution by
using their own outputs, effectively mitigating distribution shift. To this
end, we introduce Carve3D, a RLFT method coupled with the Multi-view
Reconstruction Consistency (MRC) metric, to improve the consistency of
multi-view diffusion models. To compute MRC on a set of multi-view images, we
compare them with their corresponding renderings of the reconstructed NeRF at
the same viewpoints. We validate the robustness of MRC with extensive
experiments conducted under controlled inconsistency levels. We enhance the
base RLFT algorithm to stabilize the training process, reduce distribution
shift, and identify scaling laws. Through qualitative and quantitative
experiments, along with a user study, we demonstrate Carve3D's improved
multi-view consistency, the resulting superior NeRF reconstruction quality, and
minimal distribution shift compared to longer SFT. Project webpage:
https://desaixie.github.io/carve-3d.";Desai Xie<author:sep>Jiahao Li<author:sep>Hao Tan<author:sep>Xin Sun<author:sep>Zhixin Shu<author:sep>Yi Zhou<author:sep>Sai Bi<author:sep>Sören Pirk<author:sep>Arie E. Kaufman;http://arxiv.org/pdf/2312.13980v1;cs.CV;Project webpage: https://desaixie.github.io/carve-3d;nerf
2312.13763v2;http://arxiv.org/abs/2312.13763v2;2023-12-21;Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed  Diffusion Models;"Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.";Huan Ling<author:sep>Seung Wook Kim<author:sep>Antonio Torralba<author:sep>Sanja Fidler<author:sep>Karsten Kreis;http://arxiv.org/pdf/2312.13763v2;cs.CV;"Project page:
  https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/";gaussian splatting
2312.14154v1;http://arxiv.org/abs/2312.14154v1;2023-12-21;Virtual Pets: Animatable Animal Generation in 3D Scenes;"Toward unlocking the potential of generative models in immersive 4D
experiences, we introduce Virtual Pet, a novel pipeline to model realistic and
diverse motions for target animal species within a 3D environment. To
circumvent the limited availability of 3D motion data aligned with
environmental geometry, we leverage monocular internet videos and extract
deformable NeRF representations for the foreground and static NeRF
representations for the background. For this, we develop a reconstruction
strategy, encompassing species-level shared template learning and per-video
fine-tuning. Utilizing the reconstructed data, we then train a conditional 3D
motion model to learn the trajectory and articulation of foreground animals in
the context of 3D backgrounds. We showcase the efficacy of our pipeline with
comprehensive qualitative and quantitative evaluations using cat videos. We
also demonstrate versatility across unseen cats and indoor environments,
producing temporally coherent 4D outputs for enriched virtual experiences.";Yen-Chi Cheng<author:sep>Chieh Hubert Lin<author:sep>Chaoyang Wang<author:sep>Yash Kant<author:sep>Sergey Tulyakov<author:sep>Alexander Schwing<author:sep>Liangyan Gui<author:sep>Hsin-Ying Lee;http://arxiv.org/pdf/2312.14154v1;cs.CV;Preprint. Project page: https://yccyenchicheng.github.io/VirtualPets/;nerf
2312.13277v1;http://arxiv.org/abs/2312.13277v1;2023-12-20;Deep Learning on 3D Neural Fields;"In recent years, Neural Fields (NFs) have emerged as an effective tool for
encoding diverse continuous signals such as images, videos, audio, and 3D
shapes. When applied to 3D data, NFs offer a solution to the fragmentation and
limitations associated with prevalent discrete representations. However, given
that NFs are essentially neural networks, it remains unclear whether and how
they can be seamlessly integrated into deep learning pipelines for solving
downstream tasks. This paper addresses this research problem and introduces
nf2vec, a framework capable of generating a compact latent representation for
an input NF in a single inference pass. We demonstrate that nf2vec effectively
embeds 3D objects represented by the input NFs and showcase how the resulting
embeddings can be employed in deep learning pipelines to successfully address
various tasks, all while processing exclusively NFs. We test this framework on
several NFs used to represent 3D surfaces, such as unsigned/signed distance and
occupancy fields. Moreover, we demonstrate the effectiveness of our approach
with more complex NFs that encompass both geometry and appearance of 3D objects
such as neural radiance fields.";Pierluigi Zama Ramirez<author:sep>Luca De Luigi<author:sep>Daniele Sirocchi<author:sep>Adriano Cardace<author:sep>Riccardo Spezialetti<author:sep>Francesco Ballerini<author:sep>Samuele Salti<author:sep>Luigi Di Stefano;http://arxiv.org/pdf/2312.13277v1;cs.CV;"Extended version of the paper ""Deep Learning on Implicit Neural
  Representations of Shapes"" that was presented at ICLR 2023. arXiv admin note:
  text overlap with arXiv:2302.05438";
2312.13285v1;http://arxiv.org/abs/2312.13285v1;2023-12-20;UniSDF: Unifying Neural Representations for High-Fidelity 3D  Reconstruction of Complex Scenes with Reflections;"Neural 3D scene representations have shown great potential for 3D
reconstruction from 2D images. However, reconstructing real-world captures of
complex scenes still remains a challenge. Existing generic 3D reconstruction
methods often struggle to represent fine geometric details and do not
adequately model reflective surfaces of large-scale scenes. Techniques that
explicitly focus on reflective surfaces can model complex and detailed
reflections by exploiting better reflection parameterizations. However, we
observe that these methods are often not robust in real unbounded scenarios
where non-reflective as well as reflective components are present. In this
work, we propose UniSDF, a general purpose 3D reconstruction method that can
reconstruct large complex scenes with reflections. We investigate both
view-based as well as reflection-based color prediction parameterization
techniques and find that explicitly blending these representations in 3D space
enables reconstruction of surfaces that are more geometrically accurate,
especially for reflective surfaces. We further combine this representation with
a multi-resolution grid backbone that is trained in a coarse-to-fine manner,
enabling faster reconstructions than prior methods. Extensive experiments on
object-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF
360 and Ref-NeRF real demonstrate that our method is able to robustly
reconstruct complex large-scale scenes with fine details and reflective
surfaces. Please see our project page at
https://fangjinhuawang.github.io/UniSDF.";Fangjinhua Wang<author:sep>Marie-Julie Rakotosaona<author:sep>Michael Niemeyer<author:sep>Richard Szeliski<author:sep>Marc Pollefeys<author:sep>Federico Tombari;http://arxiv.org/pdf/2312.13285v1;cs.CV;Project page: https://fangjinhuawang.github.io/UniSDF;nerf
2312.13102v1;http://arxiv.org/abs/2312.13102v1;2023-12-20;SpecNeRF: Gaussian Directional Encoding for Specular Reflections;"Neural radiance fields have achieved remarkable performance in modeling the
appearance of 3D scenes. However, existing approaches still struggle with the
view-dependent appearance of glossy surfaces, especially under complex lighting
of indoor environments. Unlike existing methods, which typically assume distant
lighting like an environment map, we propose a learnable Gaussian directional
encoding to better model the view-dependent effects under near-field lighting
conditions. Importantly, our new directional encoding captures the
spatially-varying nature of near-field lighting and emulates the behavior of
prefiltered environment maps. As a result, it enables the efficient evaluation
of preconvolved specular color at any 3D location with varying roughness
coefficients. We further introduce a data-driven geometry prior that helps
alleviate the shape radiance ambiguity in reflection modeling. We show that our
Gaussian directional encoding and geometry prior significantly improve the
modeling of challenging specular reflections in neural radiance fields, which
helps decompose appearance into more physically meaningful components.";Li Ma<author:sep>Vasu Agrawal<author:sep>Haithem Turki<author:sep>Changil Kim<author:sep>Chen Gao<author:sep>Pedro Sander<author:sep>Michael Zollhöfer<author:sep>Christian Richardt;http://arxiv.org/pdf/2312.13102v1;cs.CV;Project page: https://limacv.github.io/SpecNeRF_web/;nerf
2312.13332v2;http://arxiv.org/abs/2312.13332v2;2023-12-20;Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM;"The opacity of rigid 3D scenes with opaque surfaces is considered to be of a
binary type. However, we observed that this property is not followed by the
existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this
prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization
through the volumetric rendering function does not facilitate easy integration
of the desired prior. Instead, we observed that the opacity of ternary-type
(TT) is well supported. In this work, we study why ternary-type opacity is
well-suited and desired for the task at hand. In particular, we provide
theoretical insights into the process of jointly optimizing radiance and
opacity through the volumetric rendering process. Through exhaustive
experiments on benchmark datasets, we validate our claim and provide insights
into the optimization process, which we believe will unleash the potential of
RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple
yet novel visual odometry scheme that uses a hybrid combination of volumetric
and warping-based image renderings. More specifically, the proposed hybrid
odometry (HO) additionally uses image warping-based coarse odometry, leading up
to an order of magnitude final speed-up. Furthermore, we show that the proposed
TT and HO well complement each other, offering state-of-the-art results on
benchmark datasets in terms of both speed and accuracy.";Junru Lin<author:sep>Asen Nachkov<author:sep>Songyou Peng<author:sep>Luc Van Gool<author:sep>Danda Pani Paudel;http://arxiv.org/pdf/2312.13332v2;cs.CV;;nerf
2312.13308v1;http://arxiv.org/abs/2312.13308v1;2023-12-20;SWAGS: Sampling Windows Adaptively for Dynamic 3D Gaussian Splatting;"Novel view synthesis has shown rapid progress recently, with methods capable
of producing evermore photo-realistic results. 3D Gaussian Splatting has
emerged as a particularly promising method, producing high-quality renderings
of static scenes and enabling interactive viewing at real-time frame rates.
However, it is currently limited to static scenes only. In this work, we extend
3D Gaussian Splatting to reconstruct dynamic scenes. We model the dynamics of a
scene using a tunable MLP, which learns the deformation field from a canonical
space to a set of 3D Gaussians per frame. To disentangle the static and dynamic
parts of the scene, we learn a tuneable parameter for each Gaussian, which
weighs the respective MLP parameters to focus attention on the dynamic parts.
This improves the model's ability to capture dynamics in scenes with an
imbalance of static to dynamic regions. To handle scenes of arbitrary length
whilst maintaining high rendering quality, we introduce an adaptive window
sampling strategy to partition the sequence into windows based on the amount of
movement in the sequence. We train a separate dynamic Gaussian Splatting model
for each window, allowing the canonical representation to change, thus enabling
the reconstruction of scenes with significant geometric or topological changes.
Temporal consistency is enforced using a fine-tuning step with self-supervising
consistency loss on randomly sampled novel views. As a result, our method
produces high-quality renderings of general dynamic scenes with competitive
quantitative performance, which can be viewed in real-time with our dynamic
interactive viewer.";Richard Shaw<author:sep>Jifei Song<author:sep>Arthur Moreau<author:sep>Michal Nazarczuk<author:sep>Sibi Catley-Chandar<author:sep>Helisa Dhamo<author:sep>Eduardo Perez-Pellitero;http://arxiv.org/pdf/2312.13308v1;cs.CV;;gaussian splatting
2312.13471v1;http://arxiv.org/abs/2312.13471v1;2023-12-20;NeRF-VO: Real-Time Sparse Visual Odometry with Neural Radiance Fields;"We introduce a novel monocular visual odometry (VO) system, NeRF-VO, that
integrates learning-based sparse visual odometry for low-latency camera
tracking and a neural radiance scene representation for sophisticated dense
reconstruction and novel view synthesis. Our system initializes camera poses
using sparse visual odometry and obtains view-dependent dense geometry priors
from a monocular depth prediction network. We harmonize the scale of poses and
dense geometry, treating them as supervisory cues to train a neural implicit
scene representation. NeRF-VO demonstrates exceptional performance in both
photometric and geometric fidelity of the scene representation by jointly
optimizing a sliding window of keyframed poses and the underlying dense
geometry, which is accomplished through training the radiance field with volume
rendering. We surpass state-of-the-art methods in pose estimation accuracy,
novel view synthesis fidelity, and dense reconstruction quality across a
variety of synthetic and real-world datasets, while achieving a higher camera
tracking frequency and consuming less GPU memory.";Jens Naumann<author:sep>Binbin Xu<author:sep>Stefan Leutenegger<author:sep>Xingxing Zuo;http://arxiv.org/pdf/2312.13471v1;cs.CV;10 tables, 4 figures;nerf
2312.13150v1;http://arxiv.org/abs/2312.13150v1;2023-12-20;Splatter Image: Ultra-Fast Single-View 3D Reconstruction;"We introduce the Splatter Image, an ultra-fast approach for monocular 3D
object reconstruction which operates at 38 FPS. Splatter Image is based on
Gaussian Splatting, which has recently brought real-time rendering, fast
training, and excellent scaling to multi-view reconstruction. For the first
time, we apply Gaussian Splatting in a monocular reconstruction setting. Our
approach is learning-based, and, at test time, reconstruction only requires the
feed-forward evaluation of a neural network. The main innovation of Splatter
Image is the surprisingly straightforward design: it uses a 2D image-to-image
network to map the input image to one 3D Gaussian per pixel. The resulting
Gaussians thus have the form of an image, the Splatter Image. We further extend
the method to incorporate more than one image as input, which we do by adding
cross-view attention. Owning to the speed of the renderer (588 FPS), we can use
a single GPU for training while generating entire images at each iteration in
order to optimize perceptual metrics like LPIPS. On standard benchmarks, we
demonstrate not only fast reconstruction but also better results than recent
and much more expensive baselines in terms of PSNR, LPIPS, and other metrics.";Stanislaw Szymanowicz<author:sep>Christian Rupprecht<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2312.13150v1;cs.CV;"Project page: https://szymanowiczs.github.io/splatter-image.html .
  Code: https://github.com/szymanowiczs/splatter-image";gaussian splatting
2312.12726v1;http://arxiv.org/abs/2312.12726v1;2023-12-20;Reducing Shape-Radiance Ambiguity in Radiance Fields with a Closed-Form  Color Estimation Method;"Neural radiance field (NeRF) enables the synthesis of cutting-edge realistic
novel view images of a 3D scene. It includes density and color fields to model
the shape and radiance of a scene, respectively. Supervised by the photometric
loss in an end-to-end training manner, NeRF inherently suffers from the
shape-radiance ambiguity problem, i.e., it can perfectly fit training views but
does not guarantee decoupling the two fields correctly. To deal with this
issue, existing works have incorporated prior knowledge to provide an
independent supervision signal for the density field, including total variation
loss, sparsity loss, distortion loss, etc. These losses are based on general
assumptions about the density field, e.g., it should be smooth, sparse, or
compact, which are not adaptive to a specific scene. In this paper, we propose
a more adaptive method to reduce the shape-radiance ambiguity. The key is a
rendering method that is only based on the density field. Specifically, we
first estimate the color field based on the density field and posed images in a
closed form. Then NeRF's rendering process can proceed. We address the problems
in estimating the color field, including occlusion and non-uniformly
distributed views. Afterward, it is applied to regularize NeRF's density field.
As our regularization is guided by photometric loss, it is more adaptive
compared to existing ones. Experimental results show that our method improves
the density field of NeRF both qualitatively and quantitatively. Our code is
available at https://github.com/qihangGH/Closed-form-color-field.";Qihang Fang<author:sep>Yafei Song<author:sep>Keqiang Li<author:sep>Liefeng Bo;http://arxiv.org/pdf/2312.12726v1;cs.CV;This work has been published in NeurIPS 2023;nerf
2312.13324v1;http://arxiv.org/abs/2312.13324v1;2023-12-20;ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors;"We introduce ShowRoom3D, a three-stage approach for generating high-quality
3D room-scale scenes from texts. Previous methods using 2D diffusion priors to
optimize neural radiance fields for generating room-scale scenes have shown
unsatisfactory quality. This is primarily attributed to the limitations of 2D
priors lacking 3D awareness and constraints in the training methodology. In
this paper, we utilize a 3D diffusion prior, MVDiffusion, to optimize the 3D
room-scale scene. Our contributions are in two aspects. Firstly, we propose a
progressive view selection process to optimize NeRF. This involves dividing the
training process into three stages, gradually expanding the camera sampling
scope. Secondly, we propose the pose transformation method in the second stage.
It will ensure MVDiffusion provide the accurate view guidance. As a result,
ShowRoom3D enables the generation of rooms with improved structural integrity,
enhanced clarity from any view, reduced content repetition, and higher
consistency across different perspectives. Extensive experiments demonstrate
that our method, significantly outperforms state-of-the-art approaches by a
large margin in terms of user study.";Weijia Mao<author:sep>Yan-Pei Cao<author:sep>Jia-Wei Liu<author:sep>Zhongcong Xu<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2312.13324v1;cs.CV;;nerf
2312.12036v2;http://arxiv.org/abs/2312.12036v2;2023-12-19;LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks  in Cluttered Tabletop Environments;"Instructing a robot to complete an everyday task within our homes has been a
long-standing challenge for robotics. While recent progress in
language-conditioned imitation learning and offline reinforcement learning has
demonstrated impressive performance across a wide range of tasks, they are
typically limited to short-horizon tasks -- not reflective of those a home
robot would be expected to complete. While existing architectures have the
potential to learn these desired behaviours, the lack of the necessary
long-horizon, multi-step datasets for real robotic systems poses a significant
challenge. To this end, we present the Long-Horizon Manipulation (LHManip)
dataset comprising 200 episodes, demonstrating 20 different manipulation tasks
via real robot teleoperation. The tasks entail multiple sub-tasks, including
grasping, pushing, stacking and throwing objects in highly cluttered
environments. Each task is paired with a natural language instruction and
multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the
dataset comprises 176,278 observation-action pairs which form part of the Open
X-Embodiment dataset. The full LHManip dataset is made publicly available at
https://github.com/fedeceola/LHManip.";Federico Ceola<author:sep>Lorenzo Natale<author:sep>Niko Sünderhauf<author:sep>Krishan Rana;http://arxiv.org/pdf/2312.12036v2;cs.RO;Submitted to IJRR;nerf
2312.12122v1;http://arxiv.org/abs/2312.12122v1;2023-12-19;ZS-SRT: An Efficient Zero-Shot Super-Resolution Training Method for  Neural Radiance Fields;"Neural Radiance Fields (NeRF) have achieved great success in the task of
synthesizing novel views that preserve the same resolution as the training
views. However, it is challenging for NeRF to synthesize high-quality
high-resolution novel views with low-resolution training data. To solve this
problem, we propose a zero-shot super-resolution training framework for NeRF.
This framework aims to guide the NeRF model to synthesize high-resolution novel
views via single-scene internal learning rather than requiring any external
high-resolution training data. Our approach consists of two stages. First, we
learn a scene-specific degradation mapping by performing internal learning on a
pretrained low-resolution coarse NeRF. Second, we optimize a super-resolution
fine NeRF by conducting inverse rendering with our mapping function so as to
backpropagate the gradients from low-resolution 2D space into the
super-resolution 3D sampling space. Then, we further introduce a temporal
ensemble strategy in the inference phase to compensate for the scene estimation
errors. Our method is featured on two points: (1) it does not consume
high-resolution views or additional scene data to train super-resolution NeRF;
(2) it can speed up the training process by adopting a coarse-to-fine strategy.
By conducting extensive experiments on public datasets, we have qualitatively
and quantitatively demonstrated the effectiveness of our method.";Xiang Feng<author:sep>Yongbo He<author:sep>Yubo Wang<author:sep>Chengkai Wang<author:sep>Zhenzhong Kuang<author:sep>Jiajun Ding<author:sep>Feiwei Qin<author:sep>Jun Yu<author:sep>Jianping Fan;http://arxiv.org/pdf/2312.12122v1;cs.CV;;nerf
2312.12337v2;http://arxiv.org/abs/2312.12337v2;2023-12-19;pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable  Generalizable 3D Reconstruction;"We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D
radiance fields parameterized by 3D Gaussian primitives from pairs of images.
Our model features real-time and memory-efficient rendering for scalable
training as well as fast 3D reconstruction at inference time. To overcome local
minima inherent to sparse and locally supported representations, we predict a
dense probability distribution over 3D and sample Gaussian means from that
probability distribution. We make this sampling operation differentiable via a
reparameterization trick, allowing us to back-propagate gradients through the
Gaussian splatting representation. We benchmark our method on wide-baseline
novel view synthesis on the real-world RealEstate10k and ACID datasets, where
we outperform state-of-the-art light field transformers and accelerate
rendering by 2.5 orders of magnitude while reconstructing an interpretable and
editable 3D radiance field.";David Charatan<author:sep>Sizhe Li<author:sep>Andrea Tagliasacchi<author:sep>Vincent Sitzmann;http://arxiv.org/pdf/2312.12337v2;cs.CV;Project page: https://dcharatan.github.io/pixelsplat;gaussian splatting
2312.11774v1;http://arxiv.org/abs/2312.11774v1;2023-12-19;Text-Image Conditioned Diffusion for Consistent Text-to-3D Generation;"By lifting the pre-trained 2D diffusion models into Neural Radiance Fields
(NeRFs), text-to-3D generation methods have made great progress. Many
state-of-the-art approaches usually apply score distillation sampling (SDS) to
optimize the NeRF representations, which supervises the NeRF optimization with
pre-trained text-conditioned 2D diffusion models such as Imagen. However, the
supervision signal provided by such pre-trained diffusion models only depends
on text prompts and does not constrain the multi-view consistency. To inject
the cross-view consistency into diffusion priors, some recent works finetune
the 2D diffusion model with multi-view data, but still lack fine-grained view
coherence. To tackle this challenge, we incorporate multi-view image conditions
into the supervision signal of NeRF optimization, which explicitly enforces
fine-grained view consistency. With such stronger supervision, our proposed
text-to-3D method effectively mitigates the generation of floaters (due to
excessive densities) and completely empty spaces (due to insufficient
densities). Our quantitative evaluations on the T$^3$Bench dataset demonstrate
that our method achieves state-of-the-art performance over existing text-to-3D
methods. We will make the code publicly available.";Yuze He<author:sep>Yushi Bai<author:sep>Matthieu Lin<author:sep>Jenny Sheng<author:sep>Yubin Hu<author:sep>Qi Wang<author:sep>Yu-Hui Wen<author:sep>Yong-Jin Liu;http://arxiv.org/pdf/2312.11774v1;cs.CV;;nerf
2312.11841v3;http://arxiv.org/abs/2312.11841v3;2023-12-19;MixRT: Mixed Neural Representations For Real-Time NeRF Rendering;"Neural Radiance Field (NeRF) has emerged as a leading technique for novel
view synthesis, owing to its impressive photorealistic reconstruction and
rendering capability. Nevertheless, achieving real-time NeRF rendering in
large-scale scenes has presented challenges, often leading to the adoption of
either intricate baked mesh representations with a substantial number of
triangles or resource-intensive ray marching in baked representations. We
challenge these conventions, observing that high-quality geometry, represented
by meshes with substantial triangles, is not necessary for achieving
photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF
representation that includes a low-quality mesh, a view-dependent displacement
map, and a compressed NeRF model. This design effectively harnesses the
capabilities of existing graphics hardware, thus enabling real-time NeRF
rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering
framework, our proposed MixRT attains real-time rendering speeds on edge
devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),
better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360
datasets), and a smaller storage size (less than 80% compared to
state-of-the-art methods).";Chaojian Li<author:sep>Bichen Wu<author:sep>Peter Vajda<author:sep> Yingyan<author:sep> Lin;http://arxiv.org/pdf/2312.11841v3;cs.CV;Accepted by 3DV'24. Project Page: https://licj15.github.io/MixRT/;nerf
2312.13299v1;http://arxiv.org/abs/2312.13299v1;2023-12-19;Compact 3D Scene Representation via Self-Organizing Gaussian Grids;"3D Gaussian Splatting has recently emerged as a highly promising technique
for modeling of static 3D scenes. In contrast to Neural Radiance Fields, it
utilizes efficient rasterization allowing for very fast rendering at
high-quality. However, the storage size is significantly higher, which hinders
practical deployment, e.g.~on resource constrained devices. In this paper, we
introduce a compact scene representation organizing the parameters of 3D
Gaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a
drastic reduction in storage requirements without compromising visual quality
during rendering. Central to our idea is the explicit exploitation of
perceptual redundancies present in natural scenes. In essence, the inherent
nature of a scene allows for numerous permutations of Gaussian parameters to
equivalently represent it. To this end, we propose a novel highly parallel
algorithm that regularly arranges the high-dimensional Gaussian parameters into
a 2D grid while preserving their neighborhood structure. During training, we
further enforce local smoothness between the sorted parameters in the grid. The
uncompressed Gaussians use the same structure as 3DGS, ensuring a seamless
integration with established renderers. Our method achieves a reduction factor
of 8x to 26x in size for complex scenes with no increase in training time,
marking a substantial leap forward in the domain of 3D scene distribution and
consumption. Additional information can be found on our project page:
https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/";Wieland Morgenstern<author:sep>Florian Barthel<author:sep>Anna Hilsmann<author:sep>Peter Eisert;http://arxiv.org/pdf/2312.13299v1;cs.CV;;gaussian splatting
2312.11458v1;http://arxiv.org/abs/2312.11458v1;2023-12-18;GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View  Synthesis;"We propose a method for dynamic scene reconstruction using deformable 3D
Gaussians that is tailored for monocular video. Building upon the efficiency of
Gaussian splatting, our approach extends the representation to accommodate
dynamic elements via a deformable set of Gaussians residing in a canonical
space, and a time-dependent deformation field defined by a multi-layer
perceptron (MLP). Moreover, under the assumption that most natural scenes have
large regions that remain static, we allow the MLP to focus its
representational power by additionally including a static Gaussian point cloud.
The concatenated dynamic and static point clouds form the input for the
Gaussian Splatting rasterizer, enabling real-time rendering. The differentiable
pipeline is optimized end-to-end with a self-supervised rendering loss. Our
method achieves results that are comparable to state-of-the-art dynamic neural
radiance field methods while allowing much faster optimization and rendering.
Project website: https://lynl7130.github.io/gaufre/index.html";Yiqing Liang<author:sep>Numair Khan<author:sep>Zhengqin Li<author:sep>Thu Nguyen-Phuoc<author:sep>Douglas Lanman<author:sep>James Tompkin<author:sep>Lei Xiao;http://arxiv.org/pdf/2312.11458v1;cs.CV;10 pages, 8 figures, 4 tables;gaussian splatting
2312.10921v1;http://arxiv.org/abs/2312.10921v1;2023-12-18;AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head  Synthesis;"Audio-driven talking head synthesis is a promising topic with wide
applications in digital human, film making and virtual reality. Recent
NeRF-based approaches have shown superiority in quality and fidelity compared
to previous studies. However, when it comes to few-shot talking head
generation, a practical scenario where only few seconds of talking video is
available for one identity, two limitations emerge: 1) they either have no base
model, which serves as a facial prior for fast convergence, or ignore the
importance of audio when building the prior; 2) most of them overlook the
degree of correlation between different face regions and audio, e.g., mouth is
audio related, while ear is audio independent. In this paper, we present Audio
Enhanced Neural Radiance Field (AE-NeRF) to tackle the above issues, which can
generate realistic portraits of a new speaker with fewshot dataset.
Specifically, we introduce an Audio Aware Aggregation module into the feature
fusion stage of the reference scheme, where the weight is determined by the
similarity of audio between reference and target image. Then, an Audio-Aligned
Face Generation strategy is proposed to model the audio related and audio
independent regions respectively, with a dual-NeRF framework. Extensive
experiments have shown AE-NeRF surpasses the state-of-the-art on image
fidelity, audio-lip synchronization, and generalization ability, even in
limited training set or training iterations.";Dongze Li<author:sep>Kang Zhao<author:sep>Wei Wang<author:sep>Bo Peng<author:sep>Yingya Zhang<author:sep>Jing Dong<author:sep>Tieniu Tan;http://arxiv.org/pdf/2312.10921v1;cs.CV;Accepted by AAAI 2024;nerf
2312.11461v1;http://arxiv.org/abs/2312.11461v1;2023-12-18;GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning;"Gaussian splatting has emerged as a powerful 3D representation that harnesses
the advantages of both explicit (mesh) and implicit (NeRF) 3D representations.
In this paper, we seek to leverage Gaussian splatting to generate realistic
animatable avatars from textual descriptions, addressing the limitations (e.g.,
flexibility and efficiency) imposed by mesh or NeRF-based representations.
However, a naive application of Gaussian splatting cannot generate high-quality
animatable avatars and suffers from learning instability; it also cannot
capture fine avatar geometries and often leads to degenerate body parts. To
tackle these problems, we first propose a primitive-based 3D Gaussian
representation where Gaussians are defined inside pose-driven primitives to
facilitate animation. Second, to stabilize and amortize the learning of
millions of Gaussians, we propose to use neural implicit fields to predict the
Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries
and extract detailed meshes, we propose a novel SDF-based implicit mesh
learning approach for 3D Gaussians that regularizes the underlying geometries
and extracts highly detailed textured meshes. Our proposed method, GAvatar,
enables the large-scale generation of diverse animatable avatars using only
text prompts. GAvatar significantly surpasses existing methods in terms of both
appearance and geometry quality, and achieves extremely fast rendering (100
fps) at 1K resolution.";Ye Yuan<author:sep>Xueting Li<author:sep>Yangyi Huang<author:sep>Shalini De Mello<author:sep>Koki Nagano<author:sep>Jan Kautz<author:sep>Umar Iqbal;http://arxiv.org/pdf/2312.11461v1;cs.CV;Project website: https://nvlabs.github.io/GAvatar;gaussian splatting<tag:sep>nerf
2312.10649v1;http://arxiv.org/abs/2312.10649v1;2023-12-17;PNeRFLoc: Visual Localization with Point-based Neural Radiance Fields;"Due to the ability to synthesize high-quality novel views, Neural Radiance
Fields (NeRF) have been recently exploited to improve visual localization in a
known environment. However, the existing methods mostly utilize NeRFs for data
augmentation to improve the regression model training, and the performance on
novel viewpoints and appearances is still limited due to the lack of geometric
constraints. In this paper, we propose a novel visual localization framework,
\ie, PNeRFLoc, based on a unified point-based representation. On the one hand,
PNeRFLoc supports the initial pose estimation by matching 2D and 3D feature
points as traditional structure-based methods; on the other hand, it also
enables pose refinement with novel view synthesis using rendering-based
optimization. Specifically, we propose a novel feature adaption module to close
the gaps between the features for visual localization and neural rendering. To
improve the efficacy and efficiency of neural rendering-based optimization, we
also develop an efficient rendering-based framework with a warping loss
function. Furthermore, several robustness techniques are developed to handle
illumination changes and dynamic objects for outdoor scenarios. Experiments
demonstrate that PNeRFLoc performs the best on synthetic data when the NeRF
model can be well learned and performs on par with the SOTA method on the
visual localization benchmark datasets.";Boming Zhao<author:sep>Luwei Yang<author:sep>Mao Mao<author:sep>Hujun Bao<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2312.10649v1;cs.CV;Accepted to AAAI 2024;nerf
2312.10422v2;http://arxiv.org/abs/2312.10422v2;2023-12-16;Learning Dense Correspondence for NeRF-Based Face Reenactment;"Face reenactment is challenging due to the need to establish dense
correspondence between various face representations for motion transfer. Recent
studies have utilized Neural Radiance Field (NeRF) as fundamental
representation, which further enhanced the performance of multi-view face
reenactment in photo-realism and 3D consistency. However, establishing dense
correspondence between different face NeRFs is non-trivial, because implicit
representations lack ground-truth correspondence annotations like mesh-based 3D
parametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning
3DMM space with NeRF-based face representations can realize motion control, it
is sub-optimal for their limited face-only modeling and low identity fidelity.
Therefore, we are inspired to ask: Can we learn the dense correspondence
between different NeRF-based face representations without a 3D parametric model
prior? To address this challenge, we propose a novel framework, which adopts
tri-planes as fundamental NeRF representation and decomposes face tri-planes
into three components: canonical tri-planes, identity deformations, and motion.
In terms of motion control, our key contribution is proposing a Plane
Dictionary (PlaneDict) module, which efficiently maps the motion conditions to
a linear weighted addition of learnable orthogonal plane bases. To the best of
our knowledge, our framework is the first method that achieves one-shot
multi-view face reenactment without a 3D parametric model prior. Extensive
experiments demonstrate that we produce better results in fine-grained motion
control and identity preservation than previous methods.";Songlin Yang<author:sep>Wei Wang<author:sep>Yushi Lan<author:sep>Xiangyu Fan<author:sep>Bo Peng<author:sep>Lei Yang<author:sep>Jing Dong;http://arxiv.org/pdf/2312.10422v2;cs.CV;"Accepted by Proceedings of the AAAI Conference on Artificial
  Intelligence, 2024";nerf
2312.10034v1;http://arxiv.org/abs/2312.10034v1;2023-12-15;SlimmeRF: Slimmable Radiance Fields;"Neural Radiance Field (NeRF) and its variants have recently emerged as
successful methods for novel view synthesis and 3D scene reconstruction.
However, most current NeRF models either achieve high accuracy using large
model sizes, or achieve high memory-efficiency by trading off accuracy. This
limits the applicable scope of any single model, since high-accuracy models
might not fit in low-memory devices, and memory-efficient models might not
satisfy high-quality requirements. To this end, we present SlimmeRF, a model
that allows for instant test-time trade-offs between model size and accuracy
through slimming, thus making the model simultaneously suitable for scenarios
with different computing budgets. We achieve this through a newly proposed
algorithm named Tensorial Rank Incrementation (TRaIn) which increases the rank
of the model's tensorial representation gradually during training. We also
observe that our model allows for more effective trade-offs in sparse-view
scenarios, at times even achieving higher accuracy after being slimmed. We
credit this to the fact that erroneous information such as floaters tend to be
stored in components corresponding to higher ranks. Our implementation is
available at https://github.com/Shiran-Yuan/SlimmeRF.";Shiran Yuan<author:sep>Hao Zhao;http://arxiv.org/pdf/2312.10034v1;cs.CV;"3DV 2024 Oral, Project Page: https://shiran-yuan.github.io/SlimmeRF/,
  Code: https://github.com/Shiran-Yuan/SlimmeRF/";nerf
2312.09558v1;http://arxiv.org/abs/2312.09558v1;2023-12-15;Towards Transferable Targeted 3D Adversarial Attack in the Physical  World;"Compared with transferable untargeted attacks, transferable targeted
adversarial attacks could specify the misclassification categories of
adversarial samples, posing a greater threat to security-critical tasks. In the
meanwhile, 3D adversarial samples, due to their potential of multi-view
robustness, can more comprehensively identify weaknesses in existing deep
learning systems, possessing great application value. However, the field of
transferable targeted 3D adversarial attacks remains vacant. The goal of this
work is to develop a more effective technique that could generate transferable
targeted 3D adversarial examples, filling the gap in this field. To achieve
this goal, we design a novel framework named TT3D that could rapidly
reconstruct from few multi-view images into Transferable Targeted 3D textured
meshes. While existing mesh-based texture optimization methods compute
gradients in the high-dimensional mesh space and easily fall into local optima,
leading to unsatisfactory transferability and distinct distortions, TT3D
innovatively performs dual optimization towards both feature grid and
Multi-layer Perceptron (MLP) parameters in the grid-based NeRF space, which
significantly enhances black-box transferability while enjoying naturalness.
Experimental results show that TT3D not only exhibits superior cross-model
transferability but also maintains considerable adaptability across different
renders and vision tasks. More importantly, we produce 3D adversarial examples
with 3D printing techniques in the real world and verify their robust
performance under various scenarios.";Yao Huang<author:sep>Yinpeng Dong<author:sep>Shouwei Ruan<author:sep>Xiao Yang<author:sep>Hang Su<author:sep>Xingxing Wei;http://arxiv.org/pdf/2312.09558v1;cs.CV;11 pages, 7 figures;nerf
2312.09780v1;http://arxiv.org/abs/2312.09780v1;2023-12-15;RANRAC: Robust Neural Scene Representations via Random Ray Consensus;"We introduce RANRAC, a robust reconstruction algorithm for 3D objects
handling occluded and distracted images, which is a particularly challenging
scenario that prior robust reconstruction methods cannot deal with. Our
solution supports single-shot reconstruction by involving light-field networks,
and is also applicable to photo-realistic, robust, multi-view reconstruction
from real-world images based on neural radiance fields. While the algorithm
imposes certain limitations on the scene representation and, thereby, the
supported scene types, it reliably detects and excludes inconsistent
perspectives, resulting in clean images without floating artifacts. Our
solution is based on a fuzzy adaption of the random sample consensus paradigm,
enabling its application to large scale models. We interpret the minimal number
of samples to determine the model parameters as a tunable hyperparameter. This
is applicable, as a cleaner set of samples improves reconstruction quality.
Further, this procedure also handles outliers. Especially for conditioned
models, it can result in the same local minimum in the latent space as would be
obtained with a completely clean set. We report significant improvements for
novel-view synthesis in occluded scenarios, of up to 8dB PSNR compared to the
baseline.";Benno Buschmann<author:sep>Andreea Dogaru<author:sep>Elmar Eisemann<author:sep>Michael Weinmann<author:sep>Bernhard Egger;http://arxiv.org/pdf/2312.09780v1;cs.CV;;
2312.11535v2;http://arxiv.org/abs/2312.11535v2;2023-12-15;Customize-It-3D: High-Quality 3D Creation from A Single Image Using  Subject-Specific Knowledge Prior;"In this paper, we present a novel two-stage approach that fully utilizes the
information provided by the reference image to establish a customized knowledge
prior for image-to-3D generation. While previous approaches primarily rely on a
general diffusion prior, which struggles to yield consistent results with the
reference image, we propose a subject-specific and multi-modal diffusion model.
This model not only aids NeRF optimization by considering the shading mode for
improved geometry but also enhances texture from the coarse results to achieve
superior refinement. Both aspects contribute to faithfully aligning the 3D
content with the subject. Extensive experiments showcase the superiority of our
method, Customize-It-3D, outperforming previous works by a substantial margin.
It produces faithful 360-degree reconstructions with impressive visual quality,
making it well-suited for various applications, including text-to-3D creation.";Nan Huang<author:sep>Ting Zhang<author:sep>Yuhui Yuan<author:sep>Dong Chen<author:sep>Shanghang Zhang;http://arxiv.org/pdf/2312.11535v2;cs.CV;Project Page: https://nnanhuang.github.io/projects/customize-it-3d/;nerf
2312.09913v1;http://arxiv.org/abs/2312.09913v1;2023-12-15;LAENeRF: Local Appearance Editing for Neural Radiance Fields;"Due to the omnipresence of Neural Radiance Fields (NeRFs), the interest
towards editable implicit 3D representations has surged over the last years.
However, editing implicit or hybrid representations as used for NeRFs is
difficult due to the entanglement of appearance and geometry encoded in the
model parameters. Despite these challenges, recent research has shown first
promising steps towards photorealistic and non-photorealistic appearance edits.
The main open issues of related work include limited interactivity, a lack of
support for local edits and large memory requirements, rendering them less
useful in practice. We address these limitations with LAENeRF, a unified
framework for photorealistic and non-photorealistic appearance editing of
NeRFs. To tackle local editing, we leverage a voxel grid as starting point for
region selection. We learn a mapping from expected ray terminations to final
output color, which can optionally be supervised by a style loss, resulting in
a framework which can perform photorealistic and non-photorealistic appearance
editing of selected regions. Relying on a single point per ray for our mapping,
we limit memory requirements and enable fast optimization. To guarantee
interactivity, we compose the output color using a set of learned, modifiable
base colors, composed with additive layer mixing. Compared to concurrent work,
LAENeRF enables recoloring and stylization while keeping processing time low.
Furthermore, we demonstrate that our approach surpasses baseline methods both
quantitatively and qualitatively.";Lukas Radl<author:sep>Michael Steiner<author:sep>Andreas Kurz<author:sep>Markus Steinberger;http://arxiv.org/pdf/2312.09913v1;cs.CV;Project website: https://r4dl.github.io/LAENeRF/;nerf
2312.09682v1;http://arxiv.org/abs/2312.09682v1;2023-12-15;Exploring the Feasibility of Generating Realistic 3D Models of  Endangered Species Using DreamGaussian: An Analysis of Elevation Angle's  Impact on Model Generation;"Many species face the threat of extinction. It's important to study these
species and gather information about them as much as possible to preserve
biodiversity. Due to the rarity of endangered species, there is a limited
amount of data available, making it difficult to apply data requiring
generative AI methods to this domain. We aim to study the feasibility of
generating consistent and real-like 3D models of endangered animals using
limited data. Such a phenomenon leads us to utilize zero-shot stable diffusion
models that can generate a 3D model out of a single image of the target
species. This paper investigates the intricate relationship between elevation
angle and the output quality of 3D model generation, focusing on the innovative
approach presented in DreamGaussian. DreamGaussian, a novel framework utilizing
Generative Gaussian Splatting along with novel mesh extraction and refinement
algorithms, serves as the focal point of our study. We conduct a comprehensive
analysis, analyzing the effect of varying elevation angles on DreamGaussian's
ability to reconstruct 3D scenes accurately. Through an empirical evaluation,
we demonstrate how changes in elevation angle impact the generated images'
spatial coherence, structural integrity, and perceptual realism. We observed
that giving a correct elevation angle with the input image significantly
affects the result of the generated 3D model. We hope this study to be
influential for the usability of AI to preserve endangered animals; while the
penultimate aim is to obtain a model that can output biologically consistent 3D
models via small samples, the qualitative interpretation of an existing
state-of-the-art model such as DreamGaussian will be a step forward in our
goal.";Selcuk Anil Karatopak<author:sep>Deniz Sen;http://arxiv.org/pdf/2312.09682v1;cs.CV;;gaussian splatting
2312.11537v2;http://arxiv.org/abs/2312.11537v2;2023-12-15;FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple  Super-Resolution Pipeline;"Super-resolution (SR) techniques have recently been proposed to upscale the
outputs of neural radiance fields (NeRF) and generate high-quality images with
enhanced inference speeds. However, existing NeRF+SR methods increase training
overhead by using extra input features, loss functions, and/or expensive
training procedures such as knowledge distillation. In this paper, we aim to
leverage SR for efficiency gains without costly training or architectural
changes. Specifically, we build a simple NeRF+SR pipeline that directly
combines existing modules, and we propose a lightweight augmentation technique,
random patch sampling, for training. Compared to existing NeRF+SR methods, our
pipeline mitigates the SR computing overhead and can be trained up to 23x
faster, making it feasible to run on consumer devices such as the Apple
MacBook. Experiments show our pipeline can upscale NeRF outputs by 2-4x while
maintaining high quality, increasing inference speeds by up to 18x on an NVIDIA
V100 GPU and 12.8x on an M1 Pro chip. We conclude that SR can be a simple but
effective technique for improving the efficiency of NeRF models for consumer
devices.";Chien-Yu Lin<author:sep>Qichen Fu<author:sep>Thomas Merth<author:sep>Karren Yang<author:sep>Anurag Ranjan;http://arxiv.org/pdf/2312.11537v2;cs.CV;WACV 2024 (Oral);nerf
2312.09743v1;http://arxiv.org/abs/2312.09743v1;2023-12-15;SLS4D: Sparse Latent Space for 4D Novel View Synthesis;"Neural radiance field (NeRF) has achieved great success in novel view
synthesis and 3D representation for static scenarios. Existing dynamic NeRFs
usually exploit a locally dense grid to fit the deformation field; however,
they fail to capture the global dynamics and concomitantly yield models of
heavy parameters. We observe that the 4D space is inherently sparse. Firstly,
the deformation field is sparse in spatial but dense in temporal due to the
continuity of of motion. Secondly, the radiance field is only valid on the
surface of the underlying scene, usually occupying a small fraction of the
whole space. We thus propose to represent the 4D scene using a learnable sparse
latent space, a.k.a. SLS4D. Specifically, SLS4D first uses dense learnable time
slot features to depict the temporal space, from which the deformation field is
fitted with linear multi-layer perceptions (MLP) to predict the displacement of
a 3D position at any time. It then learns the spatial features of a 3D position
using another sparse latent space. This is achieved by learning the adaptive
weights of each latent code with the attention mechanism. Extensive experiments
demonstrate the effectiveness of our SLS4D: it achieves the best 4D novel view
synthesis using only about $6\%$ parameters of the most recent work.";Qi-Yuan Feng<author:sep>Hao-Xiang Chen<author:sep>Qun-Ce Xu<author:sep>Tai-Jiang Mu;http://arxiv.org/pdf/2312.09743v1;cs.CV;10 pages, 6 figures;nerf
2312.09147v2;http://arxiv.org/abs/2312.09147v2;2023-12-14;Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D  Reconstruction with Transformers;"Recent advancements in 3D reconstruction from single images have been driven
by the evolution of generative models. Prominent among these are methods based
on Score Distillation Sampling (SDS) and the adaptation of diffusion models in
the 3D domain. Despite their progress, these techniques often face limitations
due to slow optimization or rendering processes, leading to extensive training
and optimization times. In this paper, we introduce a novel approach for
single-view reconstruction that efficiently generates a 3D model from a single
image via feed-forward inference. Our method utilizes two transformer-based
networks, namely a point decoder and a triplane decoder, to reconstruct 3D
objects using a hybrid Triplane-Gaussian intermediate representation. This
hybrid representation strikes a balance, achieving a faster rendering speed
compared to implicit representations while simultaneously delivering superior
rendering quality than explicit representations. The point decoder is designed
for generating point clouds from single images, offering an explicit
representation which is then utilized by the triplane decoder to query Gaussian
features for each point. This design choice addresses the challenges associated
with directly regressing explicit 3D Gaussian attributes characterized by their
non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to
enable rapid rendering through splatting. Both decoders are built upon a
scalable, transformer-based architecture and have been efficiently trained on
large-scale 3D datasets. The evaluations conducted on both synthetic datasets
and real-world images demonstrate that our method not only achieves higher
quality but also ensures a faster runtime in comparison to previous
state-of-the-art techniques. Please see our project page at
https://zouzx.github.io/TriplaneGaussian/.";Zi-Xin Zou<author:sep>Zhipeng Yu<author:sep>Yuan-Chen Guo<author:sep>Yangguang Li<author:sep>Ding Liang<author:sep>Yan-Pei Cao<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2312.09147v2;cs.CV;Project Page: https://zouzx.github.io/TriplaneGaussian/;gaussian splatting
2312.09313v2;http://arxiv.org/abs/2312.09313v2;2023-12-14;LatentEditor: Text Driven Local Editing of 3D Scenes;"While neural fields have made significant strides in view synthesis and scene
reconstruction, editing them poses a formidable challenge due to their implicit
encoding of geometry and texture information from multi-view inputs. In this
paper, we introduce \textsc{LatentEditor}, an innovative framework designed to
empower users with the ability to perform precise and locally controlled
editing of neural fields using text prompts. Leveraging denoising diffusion
models, we successfully embed real-world scenes into the latent space,
resulting in a faster and more adaptable NeRF backbone for editing compared to
traditional methods. To enhance editing precision, we introduce a delta score
to calculate the 2D mask in the latent space that serves as a guide for local
modifications while preserving irrelevant regions. Our novel pixel-level
scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the
disparity between IP2P conditional and unconditional noise predictions in the
latent space. The edited latents conditioned on the 2D masks are then
iteratively updated in the training set to achieve 3D local editing. Our
approach achieves faster editing speeds and superior output quality compared to
existing 3D editing models, bridging the gap between textual instructions and
high-quality 3D scene editing in latent space. We show the superiority of our
approach on four benchmark 3D datasets, LLFF, IN2N, NeRFStudio and NeRF-Art.";Umar Khalid<author:sep>Hasan Iqbal<author:sep>Nazmul Karim<author:sep>Jing Hua<author:sep>Chen Chen;http://arxiv.org/pdf/2312.09313v2;cs.CV;Project Page: https://latenteditor.github.io/;nerf
2312.09243v1;http://arxiv.org/abs/2312.09243v1;2023-12-14;OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural  Radiance Fields;"As a fundamental task of vision-based perception, 3D occupancy prediction
reconstructs 3D structures of surrounding environments. It provides detailed
information for autonomous driving planning and navigation. However, most
existing methods heavily rely on the LiDAR point clouds to generate occupancy
ground truth, which is not available in the vision-based system. In this paper,
we propose an OccNeRF method for self-supervised multi-camera occupancy
prediction. Different from bounded 3D occupancy labels, we need to consider
unbounded scenes with raw image supervision. To solve the issue, we
parameterize the reconstructed occupancy fields and reorganize the sampling
strategy. The neural rendering is adopted to convert occupancy fields to
multi-camera depth maps, supervised by multi-frame photometric consistency.
Moreover, for semantic occupancy prediction, we design several strategies to
polish the prompts and filter the outputs of a pretrained open-vocabulary 2D
segmentation model. Extensive experiments for both self-supervised depth
estimation and semantic occupancy prediction tasks on nuScenes dataset
demonstrate the effectiveness of our method.";Chubin Zhang<author:sep>Juncheng Yan<author:sep>Yi Wei<author:sep>Jiaxin Li<author:sep>Li Liu<author:sep>Yansong Tang<author:sep>Yueqi Duan<author:sep>Jiwen Lu;http://arxiv.org/pdf/2312.09243v1;cs.CV;Code: https://github.com/LinShan-Bin/OccNeRF;nerf
2312.09228v2;http://arxiv.org/abs/2312.09228v2;2023-12-14;3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting;"We introduce an approach that creates animatable human avatars from monocular
videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural
radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image
synthesis but often require days of training, and are extremely slow at
inference time. Recently, the community has explored fast grid structures for
efficient training of clothed avatars. Albeit being extremely fast at training,
these methods can barely achieve an interactive rendering frame rate with
around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a
non-rigid deformation network to reconstruct animatable clothed human avatars
that can be trained within 30 minutes and rendered at real-time frame rates
(50+ FPS). Given the explicit nature of our representation, we further
introduce as-isometric-as-possible regularizations on both the Gaussian mean
vectors and the covariance matrices, enhancing the generalization of our model
on highly articulated unseen poses. Experimental results show that our method
achieves comparable and even better performance compared to state-of-the-art
approaches on animatable avatar creation from a monocular input, while being
400x and 250x faster in training and inference, respectively.";Zhiyin Qian<author:sep>Shaofei Wang<author:sep>Marko Mihajlovic<author:sep>Andreas Geiger<author:sep>Siyu Tang;http://arxiv.org/pdf/2312.09228v2;cs.CV;Project page: https://neuralbodies.github.io/3DGS-Avatar;gaussian splatting<tag:sep>nerf
2312.09095v2;http://arxiv.org/abs/2312.09095v2;2023-12-14;ColNeRF: Collaboration for Generalizable Sparse Input Neural Radiance  Field;"Neural Radiance Fields (NeRF) have demonstrated impressive potential in
synthesizing novel views from dense input, however, their effectiveness is
challenged when dealing with sparse input. Existing approaches that incorporate
additional depth or semantic supervision can alleviate this issue to an extent.
However, the process of supervision collection is not only costly but also
potentially inaccurate, leading to poor performance and generalization ability
in diverse scenarios. In our work, we introduce a novel model: the
Collaborative Neural Radiance Fields (ColNeRF) designed to work with sparse
input. The collaboration in ColNeRF includes both the cooperation between
sparse input images and the cooperation between the output of the neural
radiation field. Through this, we construct a novel collaborative module that
aligns information from various views and meanwhile imposes self-supervised
constraints to ensure multi-view consistency in both geometry and appearance. A
Collaborative Cross-View Volume Integration module (CCVI) is proposed to
capture complex occlusions and implicitly infer the spatial location of
objects. Moreover, we introduce self-supervision of target rays projected in
multiple directions to ensure geometric and color consistency in adjacent
regions. Benefiting from the collaboration at the input and output ends,
ColNeRF is capable of capturing richer and more generalized scene
representation, thereby facilitating higher-quality results of the novel view
synthesis. Extensive experiments demonstrate that ColNeRF outperforms
state-of-the-art sparse input generalizable NeRF methods. Furthermore, our
approach exhibits superiority in fine-tuning towards adapting to new scenes,
achieving competitive performance compared to per-scene optimized NeRF-based
methods while significantly reducing computational costs. Our code is available
at: https://github.com/eezkni/ColNeRF.";Zhangkai Ni<author:sep>Peiqi Yang<author:sep>Wenhan Yang<author:sep>Hanli Wang<author:sep>Lin Ma<author:sep>Sam Kwong;http://arxiv.org/pdf/2312.09095v2;cs.CV;;nerf
2312.09305v1;http://arxiv.org/abs/2312.09305v1;2023-12-14;Stable Score Distillation for High-Quality 3D Generation;"Score Distillation Sampling (SDS) has exhibited remarkable performance in
conditional 3D content generation. However, a comprehensive understanding of
the SDS formulation is still lacking, hindering the development of 3D
generation. In this work, we present an interpretation of SDS as a combination
of three functional components: mode-disengaging, mode-seeking and
variance-reducing terms, and analyze the properties of each. We show that
problems such as over-smoothness and color-saturation result from the intrinsic
deficiency of the supervision terms and reveal that the variance-reducing term
introduced by SDS is sub-optimal. Additionally, we shed light on the adoption
of large Classifier-Free Guidance (CFG) scale for 3D generation. Based on the
analysis, we propose a simple yet effective approach named Stable Score
Distillation (SSD) which strategically orchestrates each term for high-quality
3D generation. Extensive experiments validate the efficacy of our approach,
demonstrating its ability to generate high-fidelity 3D content without
succumbing to issues such as over-smoothness and over-saturation, even under
low CFG conditions with the most challenging NeRF representation.";Boshi Tang<author:sep>Jianan Wang<author:sep>Zhiyong Wu<author:sep>Lei Zhang;http://arxiv.org/pdf/2312.09305v1;cs.CV;;nerf
2312.09031v1;http://arxiv.org/abs/2312.09031v1;2023-12-14;iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via  Comparing and Matching;"We present a method named iComMa to address the 6D pose estimation problem in
computer vision. The conventional pose estimation methods typically rely on the
target's CAD model or necessitate specific network training tailored to
particular object classes. Some existing methods address mesh-free 6D pose
estimation by employing the inversion of a Neural Radiance Field (NeRF), aiming
to overcome the aforementioned constraints. However, it still suffers from
adverse initializations. By contrast, we model the pose estimation as the
problem of inverting the 3D Gaussian Splatting (3DGS) with both the comparing
and matching loss. In detail, a render-and-compare strategy is adopted for the
precise estimation of poses. Additionally, a matching module is designed to
enhance the model's robustness against adverse initializations by minimizing
the distances between 2D keypoints. This framework systematically incorporates
the distinctive characteristics and inherent rationale of render-and-compare
and matching-based approaches. This comprehensive consideration equips the
framework to effectively address a broader range of intricate and challenging
scenarios, including instances with substantial angular deviations, all while
maintaining a high level of prediction accuracy. Experimental results
demonstrate the superior precision and robustness of our proposed jointly
optimized framework when evaluated on synthetic and complex real-world data in
challenging scenarios.";Yuan Sun<author:sep>Xuan Wang<author:sep>Yunfan Zhang<author:sep>Jie Zhang<author:sep>Caigui Jiang<author:sep>Yu Guo<author:sep>Fei Wang;http://arxiv.org/pdf/2312.09031v1;cs.CV;10 pages, 5 figures;gaussian splatting<tag:sep>nerf
2312.09249v1;http://arxiv.org/abs/2312.09249v1;2023-12-14;ZeroRF: Fast Sparse View 360° Reconstruction with Zero Pretraining;"We present ZeroRF, a novel per-scene optimization method addressing the
challenge of sparse view 360{\deg} reconstruction in neural field
representations. Current breakthroughs like Neural Radiance Fields (NeRF) have
demonstrated high-fidelity image synthesis but struggle with sparse input
views. Existing methods, such as Generalizable NeRFs and per-scene optimization
approaches, face limitations in data dependency, computational cost, and
generalization across diverse scenarios. To overcome these challenges, we
propose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior into
a factorized NeRF representation. Unlike traditional methods, ZeroRF
parametrizes feature grids with a neural network generator, enabling efficient
sparse view 360{\deg} reconstruction without any pretraining or additional
regularization. Extensive experiments showcase ZeroRF's versatility and
superiority in terms of both quality and speed, achieving state-of-the-art
results on benchmark datasets. ZeroRF's significance extends to applications in
3D content generation and editing. Project page:
https://sarahweiii.github.io/zerorf/";Ruoxi Shi<author:sep>Xinyue Wei<author:sep>Cheng Wang<author:sep>Hao Su;http://arxiv.org/pdf/2312.09249v1;cs.CV;Project page: https://sarahweiii.github.io/zerorf/;nerf
2312.09093v2;http://arxiv.org/abs/2312.09093v2;2023-12-14;Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption;"The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered
methodology, entangling the aspects of illumination and material reflectance
into emission solely from 3D points. This simplified rendering approach
presents challenges in accurately modeling images captured under adverse
lighting conditions, such as low light or over-exposure. Motivated by the
ancient Greek emission theory that posits visual perception as a result of rays
emanating from the eyes, we slightly refine the conventional NeRF framework to
train NeRF under challenging light conditions and generate normal-light
condition novel views unsupervised. We introduce the concept of a ""Concealing
Field,"" which assigns transmittance values to the surrounding air to account
for illumination effects. In dark scenarios, we assume that object emissions
maintain a standard lighting level but are attenuated as they traverse the air
during the rendering process. Concealing Field thus compel NeRF to learn
reasonable density and colour estimations for objects even in dimly lit
situations. Similarly, the Concealing Field can mitigate over-exposed emissions
during the rendering stage. Furthermore, we present a comprehensive multi-view
dataset captured under challenging illumination conditions for evaluation. Our
code and dataset available at https://github.com/cuiziteng/Aleth-NeRF";Ziteng Cui<author:sep>Lin Gu<author:sep>Xiao Sun<author:sep>Xianzheng Ma<author:sep>Yu Qiao<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2312.09093v2;cs.CV;"AAAI 2024, code available at https://github.com/cuiziteng/Aleth-NeRF
  Modified version of previous paper arXiv:2303.05807";nerf
2312.08692v1;http://arxiv.org/abs/2312.08692v1;2023-12-14;SpectralNeRF: Physically Based Spectral Rendering with Neural Radiance  Field;"In this paper, we propose SpectralNeRF, an end-to-end Neural Radiance Field
(NeRF)-based architecture for high-quality physically based rendering from a
novel spectral perspective. We modify the classical spectral rendering into two
main steps, 1) the generation of a series of spectrum maps spanning different
wavelengths, 2) the combination of these spectrum maps for the RGB output. Our
SpectralNeRF follows these two steps through the proposed multi-layer
perceptron (MLP)-based architecture (SpectralMLP) and Spectrum Attention UNet
(SAUNet). Given the ray origin and the ray direction, the SpectralMLP
constructs the spectral radiance field to obtain spectrum maps of novel views,
which are then sent to the SAUNet to produce RGB images of white-light
illumination. Applying NeRF to build up the spectral rendering is a more
physically-based way from the perspective of ray-tracing. Further, the spectral
radiance fields decompose difficult scenes and improve the performance of
NeRF-based methods. Comprehensive experimental results demonstrate the proposed
SpectralNeRF is superior to recent NeRF-based methods when synthesizing new
views on synthetic and real datasets. The codes and datasets are available at
https://github.com/liru0126/SpectralNeRF.";Ru Li<author:sep>Jia Liu<author:sep>Guanghui Liu<author:sep>Shengping Zhang<author:sep>Bing Zeng<author:sep>Shuaicheng Liu;http://arxiv.org/pdf/2312.08692v1;cs.CV;Accepted by AAAI 2024;nerf
2312.09005v1;http://arxiv.org/abs/2312.09005v1;2023-12-14;Scene 3-D Reconstruction System in Scattering Medium;"The research on neural radiance fields for new view synthesis has experienced
explosive growth with the development of new models and extensions. The NERF
algorithm, suitable for underwater scenes or scattering media, is also
evolving. Existing underwater 3D reconstruction systems still face challenges
such as extensive training time and low rendering efficiency. This paper
proposes an improved underwater 3D reconstruction system to address these
issues and achieve rapid, high-quality 3D reconstruction.To begin with, we
enhance underwater videos captured by a monocular camera to correct the poor
image quality caused by the physical properties of the water medium while
ensuring consistency in enhancement across adjacent frames. Subsequently, we
perform keyframe selection on the video frames to optimize resource utilization
and eliminate the impact of dynamic objects on the reconstruction results. The
selected keyframes, after pose estimation using COLMAP, undergo a
three-dimensional reconstruction improvement process using neural radiance
fields based on multi-resolution hash coding for model construction and
rendering.";Zhuoyifan Zhang<author:sep>Lu Zhang<author:sep>Liang Wang<author:sep>Haoming Wu;http://arxiv.org/pdf/2312.09005v1;cs.CV;;nerf
2312.08892v1;http://arxiv.org/abs/2312.08892v1;2023-12-14;VaLID: Variable-Length Input Diffusion for Novel View Synthesis;"Novel View Synthesis (NVS), which tries to produce a realistic image at the
target view given source view images and their corresponding poses, is a
fundamental problem in 3D Vision. As this task is heavily under-constrained,
some recent work, like Zero123, tries to solve this problem with generative
modeling, specifically using pre-trained diffusion models. Although this
strategy generalizes well to new scenes, compared to neural radiance
field-based methods, it offers low levels of flexibility. For example, it can
only accept a single-view image as input, despite realistic applications often
offering multiple input images. This is because the source-view images and
corresponding poses are processed separately and injected into the model at
different stages. Thus it is not trivial to generalize the model into
multi-view source images, once they are available. To solve this issue, we try
to process each pose image pair separately and then fuse them as a unified
visual representation which will be injected into the model to guide image
synthesis at the target-views. However, inconsistency and computation costs
increase as the number of input source-view images increases. To solve these
issues, the Multi-view Cross Former module is proposed which maps
variable-length input data to fix-size output data. A two-stage training
strategy is introduced to further improve the efficiency during training time.
Qualitative and quantitative evaluation over multiple datasets demonstrates the
effectiveness of the proposed method against previous approaches. The code will
be released according to the acceptance.";Shijie Li<author:sep>Farhad G. Zanjani<author:sep>Haitam Ben Yahia<author:sep>Yuki M. Asano<author:sep>Juergen Gall<author:sep>Amirhossein Habibian;http://arxiv.org/pdf/2312.08892v1;cs.CV;paper and supplementary material;
2312.08760v1;http://arxiv.org/abs/2312.08760v1;2023-12-14;CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental  Learning;"Neural Radiance Fields (NeRF) have demonstrated impressive performance in
novel view synthesis. However, NeRF and most of its variants still rely on
traditional complex pipelines to provide extrinsic and intrinsic camera
parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF,
directly treat camera parameters as learnable and estimate them through
differential volume rendering. However, these methods work for forward-looking
scenes with slight motions and fail to tackle the rotation scenario in
practice. To overcome this limitation, we propose a novel \underline{c}amera
parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally
reconstructs 3D representations and recovers the camera parameters inspired by
incremental structure from motion (SfM). Given a sequence of images, CF-NeRF
estimates the camera parameters of images one by one and reconstructs the scene
through initialization, implicit localization, and implicit optimization. To
evaluate our method, we use a challenging real-world dataset NeRFBuster which
provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF
is robust to camera rotation and achieves state-of-the-art results without
providing prior information and constraints.";Qingsong Yan<author:sep>Qiang Wang<author:sep>Kaiyong Zhao<author:sep>Jie Chen<author:sep>Bo Li<author:sep>Xiaowen Chu<author:sep>Fei Deng;http://arxiv.org/pdf/2312.08760v1;cs.CV;"Accepted at the Thirty-Eighth AAAI Conference on Artificial
  Intelligence (AAAI24)";nerf
2312.07920v1;http://arxiv.org/abs/2312.07920v1;2023-12-13;DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic  Autonomous Driving Scenes;"We present DrivingGaussian, an efficient and effective framework for
surrounding dynamic autonomous driving scenes. For complex scenes with moving
objects, we first sequentially and progressively model the static background of
the entire scene with incremental static 3D Gaussians. We then leverage a
composite dynamic Gaussian graph to handle multiple moving objects,
individually reconstructing each object and restoring their accurate positions
and occlusion relationships within the scene. We further use a LiDAR prior for
Gaussian Splatting to reconstruct scenes with greater details and maintain
panoramic consistency. DrivingGaussian outperforms existing methods in driving
scene reconstruction and enables photorealistic surround-view synthesis with
high-fidelity and multi-camera consistency. The source code and trained models
will be released.";Xiaoyu Zhou<author:sep>Zhiwei Lin<author:sep>Xiaojun Shan<author:sep>Yongtao Wang<author:sep>Deqing Sun<author:sep>Ming-Hsuan Yang;http://arxiv.org/pdf/2312.07920v1;cs.CV;;gaussian splatting
2312.08118v1;http://arxiv.org/abs/2312.08118v1;2023-12-13;Neural Radiance Fields for Transparent Object Using Visual Hull;"Unlike opaque object, novel view synthesis of transparent object is a
challenging task, because transparent object refracts light of background
causing visual distortions on the transparent object surface along the
viewpoint change. Recently introduced Neural Radiance Fields (NeRF) is a view
synthesis method. Thanks to its remarkable performance improvement, lots of
following applications based on NeRF in various topics have been developed.
However, if an object with a different refractive index is included in a scene
such as transparent object, NeRF shows limited performance because refracted
light ray at the surface of the transparent object is not appropriately
considered. To resolve the problem, we propose a NeRF-based method consisting
of the following three steps: First, we reconstruct a three-dimensional shape
of a transparent object using visual hull. Second, we simulate the refraction
of the rays inside of the transparent object according to Snell's law. Last, we
sample points through refracted rays and put them into NeRF. Experimental
evaluation results demonstrate that our method addresses the limitation of
conventional NeRF with transparent objects.";Heechan Yoon<author:sep>Seungkyu Lee;http://arxiv.org/pdf/2312.08118v1;cs.CV;;nerf
2312.08094v1;http://arxiv.org/abs/2312.08094v1;2023-12-13;3DGEN: A GAN-based approach for generating novel 3D models from image  data;"The recent advances in text and image synthesis show a great promise for the
future of generative models in creative fields. However, a less explored area
is the one of 3D model generation, with a lot of potential applications to game
design, video production, and physical product design. In our paper, we present
3DGEN, a model that leverages the recent work on both Neural Radiance Fields
for object reconstruction and GAN-based image generation. We show that the
proposed architecture can generate plausible meshes for objects of the same
category as the training images and compare the resulting meshes with the
state-of-the-art baselines, leading to visible uplifts in generation quality.";Antoine Schnepf<author:sep>Flavian Vasile<author:sep>Ugo Tanielian;http://arxiv.org/pdf/2312.08094v1;cs.CV;"Submitted to NeurIPS 2022 Machine Learning for Creativity and Design
  Workshop";
2312.08012v1;http://arxiv.org/abs/2312.08012v1;2023-12-13;uSF: Learning Neural Semantic Field with Uncertainty;"Recently, there has been an increased interest in NeRF methods which
reconstruct differentiable representation of three-dimensional scenes. One of
the main limitations of such methods is their inability to assess the
confidence of the model in its predictions. In this paper, we propose a new
neural network model for the formation of extended vector representations,
called uSF, which allows the model to predict not only color and semantic label
of each point, but also estimate the corresponding values of uncertainty. We
show that with a small number of images available for training, a model
quantifying uncertainty performs better than a model without such
functionality. Code of the uSF approach is publicly available at
https://github.com/sevashasla/usf/.";Vsevolod Skorokhodov<author:sep>Darya Drozdova<author:sep>Dmitry Yudin;http://arxiv.org/pdf/2312.08012v1;cs.CV;12 pages, 4 figures;nerf
2312.08136v1;http://arxiv.org/abs/2312.08136v1;2023-12-13;ProNeRF: Learning Efficient Projection-Aware Ray Sampling for  Fine-Grained Implicit Neural Radiance Fields;"Recent advances in neural rendering have shown that, albeit slow, implicit
compact models can learn a scene's geometries and view-dependent appearances
from multiple views. To maintain such a small memory footprint but achieve
faster inference times, recent works have adopted `sampler' networks that
adaptively sample a small subset of points along each ray in the implicit
neural radiance fields. Although these methods achieve up to a 10$\times$
reduction in rendering time, they still suffer from considerable quality
degradation compared to the vanilla NeRF. In contrast, we propose ProNeRF,
which provides an optimal trade-off between memory footprint (similar to NeRF),
speed (faster than HyperReel), and quality (better than K-Planes). ProNeRF is
equipped with a novel projection-aware sampling (PAS) network together with a
new training strategy for ray exploration and exploitation, allowing for
efficient fine-grained particle sampling. Our ProNeRF yields state-of-the-art
metrics, being 15-23x faster with 0.65dB higher PSNR than NeRF and yielding
0.95dB higher PSNR than the best published sampler-based method, HyperReel. Our
exploration and exploitation training strategy allows ProNeRF to learn the full
scenes' color and density distributions while also learning efficient ray
sampling focused on the highest-density regions. We provide extensive
experimental results that support the effectiveness of our method on the widely
adopted forward-facing and 360 datasets, LLFF and Blender, respectively.";Juan Luis Gonzalez Bello<author:sep>Minh-Quan Viet Bui<author:sep>Munchurl Kim;http://arxiv.org/pdf/2312.08136v1;cs.CV;"Visit our project website at
  https://kaist-viclab.github.io/pronerf-site/";nerf
2312.06946v1;http://arxiv.org/abs/2312.06946v1;2023-12-12;WaterHE-NeRF: Water-ray Tracing Neural Radiance Fields for Underwater  Scene Reconstruction;"Neural Radiance Field (NeRF) technology demonstrates immense potential in
novel viewpoint synthesis tasks, due to its physics-based volumetric rendering
process, which is particularly promising in underwater scenes. Addressing the
limitations of existing underwater NeRF methods in handling light attenuation
caused by the water medium and the lack of real Ground Truth (GT) supervision,
this study proposes WaterHE-NeRF. We develop a new water-ray tracing field by
Retinex theory that precisely encodes color, density, and illuminance
attenuation in three-dimensional space. WaterHE-NeRF, through its illuminance
attenuation mechanism, generates both degraded and clear multi-view images and
optimizes image restoration by combining reconstruction loss with Wasserstein
distance. Additionally, the use of histogram equalization (HE) as pseudo-GT
enhances the network's accuracy in preserving original details and color
distribution. Extensive experiments on real underwater datasets and synthetic
datasets validate the effectiveness of WaterHE-NeRF. Our code will be made
publicly available.";Jingchun Zhou<author:sep>Tianyu Liang<author:sep>Zongxin He<author:sep>Dehuan Zhang<author:sep>Weishi Zhang<author:sep>Xianping Fu<author:sep>Chongyi Li;http://arxiv.org/pdf/2312.06946v1;cs.CV;;nerf
2312.07246v1;http://arxiv.org/abs/2312.07246v1;2023-12-12;Unifying Correspondence, Pose and NeRF for Pose-Free Novel View  Synthesis from Stereo Pairs;"This work delves into the task of pose-free novel view synthesis from stereo
pairs, a challenging and pioneering task in 3D vision. Our innovative
framework, unlike any before, seamlessly integrates 2D correspondence matching,
camera pose estimation, and NeRF rendering, fostering a synergistic enhancement
of these tasks. We achieve this through designing an architecture that utilizes
a shared representation, which serves as a foundation for enhanced 3D geometry
understanding. Capitalizing on the inherent interplay between the tasks, our
unified framework is trained end-to-end with the proposed training strategy to
improve overall model accuracy. Through extensive evaluations across diverse
indoor and outdoor scenes from two real-world datasets, we demonstrate that our
approach achieves substantial improvement over previous methodologies,
especially in scenarios characterized by extreme viewpoint changes and the
absence of accurate camera poses.";Sunghwan Hong<author:sep>Jaewoo Jung<author:sep>Heeseong Shin<author:sep>Jiaolong Yang<author:sep>Seungryong Kim<author:sep>Chong Luo;http://arxiv.org/pdf/2312.07246v1;cs.CV;Project page: https://ku-cvlab.github.io/CoPoNeRF/;nerf
2312.07504v1;http://arxiv.org/abs/2312.07504v1;2023-12-12;COLMAP-Free 3D Gaussian Splatting;"While neural rendering has led to impressive advances in scene reconstruction
and novel view synthesis, it relies heavily on accurately pre-computed camera
poses. To relax this constraint, multiple efforts have been made to train
Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the
implicit representations of NeRFs provide extra challenges to optimize the 3D
structure and camera poses at the same time. On the other hand, the recently
proposed 3D Gaussian Splatting provides new opportunities given its explicit
point cloud representations. This paper leverages both the explicit geometric
representation and the continuity of the input video stream to perform novel
view synthesis without any SfM preprocessing. We process the input frames in a
sequential manner and progressively grow the 3D Gaussians set by taking one
input frame at a time, without the need to pre-compute the camera poses. Our
method significantly improves over previous approaches in view synthesis and
camera pose estimation under large motion changes. Our project page is
https://oasisyang.github.io/colmap-free-3dgs";Yang Fu<author:sep>Sifei Liu<author:sep>Amey Kulkarni<author:sep>Jan Kautz<author:sep>Alexei A. Efros<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2312.07504v1;cs.CV;Project Page: https://oasisyang.github.io/colmap-free-3dgs;gaussian splatting<tag:sep>nerf
2312.06439v1;http://arxiv.org/abs/2312.06439v1;2023-12-11;DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior;"3D generation has raised great attention in recent years. With the success of
text-to-image diffusion models, the 2D-lifting technique becomes a promising
route to controllable 3D generation. However, these methods tend to present
inconsistent geometry, which is also known as the Janus problem. We observe
that the problem is caused mainly by two aspects, i.e., viewpoint bias in 2D
diffusion models and overfitting of the optimization objective. To address it,
we propose a two-stage 2D-lifting framework, namely DreamControl, which
optimizes coarse NeRF scenes as 3D self-prior and then generates fine-grained
objects with control-based score distillation. Specifically, adaptive viewpoint
sampling and boundary integrity metric are proposed to ensure the consistency
of generated priors. The priors are then regarded as input conditions to
maintain reasonable geometries, in which conditional LoRA and weighted score
are further proposed to optimize detailed textures. DreamControl can generate
high-quality 3D content in terms of both geometry consistency and texture
fidelity. Moreover, our control-based optimization guidance is applicable to
more downstream tasks, including user-guided generation and 3D animation. The
project page is available at https://github.com/tyhuang0428/DreamControl.";Tianyu Huang<author:sep>Yihan Zeng<author:sep>Zhilu Zhang<author:sep>Wan Xu<author:sep>Hang Xu<author:sep>Songcen Xu<author:sep>Rynson W. H. Lau<author:sep>Wangmeng Zuo;http://arxiv.org/pdf/2312.06439v1;cs.CV;;nerf
2312.06657v1;http://arxiv.org/abs/2312.06657v1;2023-12-11;Learning Naturally Aggregated Appearance for Efficient 3D Editing;"Neural radiance fields, which represent a 3D scene as a color field and a
density field, have demonstrated great progress in novel view synthesis yet are
unfavorable for editing due to the implicitness. In view of such a deficiency,
we propose to replace the color field with an explicit 2D appearance
aggregation, also called canonical image, with which users can easily customize
their 3D editing via 2D image processing. To avoid the distortion effect and
facilitate convenient editing, we complement the canonical image with a
projection field that maps 3D points onto 2D pixels for texture lookup. This
field is carefully initialized with a pseudo canonical camera model and
optimized with offset regularity to ensure naturalness of the aggregated
appearance. Extensive experimental results on three datasets suggest that our
representation, dubbed AGAP, well supports various ways of 3D editing (e.g.,
stylization, interactive drawing, and content extraction) with no need of
re-optimization for each case, demonstrating its generalizability and
efficiency. Project page is available at https://felixcheng97.github.io/AGAP/.";Ka Leong Cheng<author:sep>Qiuyu Wang<author:sep>Zifan Shi<author:sep>Kecheng Zheng<author:sep>Yinghao Xu<author:sep>Hao Ouyang<author:sep>Qifeng Chen<author:sep>Yujun Shen;http://arxiv.org/pdf/2312.06657v1;cs.CV;"Project Webpage: https://felixcheng97.github.io/AGAP/, Code:
  https://github.com/felixcheng97/AGAP";
2312.05283v1;http://arxiv.org/abs/2312.05283v1;2023-12-11;Nuvo: Neural UV Mapping for Unruly 3D Representations;"Existing UV mapping algorithms are designed to operate on well-behaved
meshes, instead of the geometry representations produced by state-of-the-art 3D
reconstruction and generation techniques. As such, applying these methods to
the volume densities recovered by neural radiance fields and related techniques
(or meshes triangulated from such fields) results in texture atlases that are
too fragmented to be useful for tasks such as view synthesis or appearance
editing. We present a UV mapping method designed to operate on geometry
produced by 3D reconstruction and generation techniques. Instead of computing a
mapping defined on a mesh's vertices, our method Nuvo uses a neural field to
represent a continuous UV mapping, and optimizes it to be a valid and
well-behaved mapping for just the set of visible points, i.e. only points that
affect the scene's appearance. We show that our model is robust to the
challenges posed by ill-behaved geometry, and that it produces editable UV
mappings that can represent detailed appearance.";Pratul P. Srinivasan<author:sep>Stephan J. Garbin<author:sep>Dor Verbin<author:sep>Jonathan T. Barron<author:sep>Ben Mildenhall;http://arxiv.org/pdf/2312.05283v1;cs.CV;Project page at https://pratulsrinivasan.github.io/nuvo;
2312.06741v1;http://arxiv.org/abs/2312.06741v1;2023-12-11;Gaussian Splatting SLAM;"We present the first application of 3D Gaussian Splatting to incremental 3D
reconstruction using a single moving monocular or RGB-D camera. Our
Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps,
utilises Gaussians as the only 3D representation, unifying the required
representation for accurate, efficient tracking, mapping, and high-quality
rendering. Several innovations are required to continuously reconstruct 3D
scenes with high fidelity from a live camera. First, to move beyond the
original 3DGS algorithm, which requires accurate poses from an offline
Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using
direct optimisation against the 3D Gaussians, and show that this enables fast
and robust tracking with a wide basin of convergence. Second, by utilising the
explicit nature of the Gaussians, we introduce geometric verification and
regularisation to handle the ambiguities occurring in incremental 3D dense
reconstruction. Finally, we introduce a full SLAM system which not only
achieves state-of-the-art results in novel view synthesis and trajectory
estimation, but also reconstruction of tiny and even transparent objects.";Hidenobu Matsuki<author:sep>Riku Murai<author:sep>Paul H. J. Kelly<author:sep>Andrew J. Davison;http://arxiv.org/pdf/2312.06741v1;cs.CV;"First two authors contributed equally to this work. Project Page:
  https://rmurai.co.uk/projects/GaussianSplattingSLAM/ Video:
  https://www.youtube.com/watch?v=x604ghp9R_Q&ab_channel=DysonRoboticsLaboratoryatImperialCollege";gaussian splatting
2312.06642v1;http://arxiv.org/abs/2312.06642v1;2023-12-11;CorresNeRF: Image Correspondence Priors for Neural Radiance Fields;"Neural Radiance Fields (NeRFs) have achieved impressive results in novel view
synthesis and surface reconstruction tasks. However, their performance suffers
under challenging scenarios with sparse input views. We present CorresNeRF, a
novel method that leverages image correspondence priors computed by
off-the-shelf methods to supervise NeRF training. We design adaptive processes
for augmentation and filtering to generate dense and high-quality
correspondences. The correspondences are then used to regularize NeRF training
via the correspondence pixel reprojection and depth loss terms. We evaluate our
methods on novel view synthesis and surface reconstruction tasks with
density-based and SDF-based NeRF models on different datasets. Our method
outperforms previous methods in both photometric and geometric metrics. We show
that this simple yet effective technique of using correspondence priors can be
applied as a plug-and-play module across different NeRF variants. The project
page is at https://yxlao.github.io/corres-nerf.";Yixing Lao<author:sep>Xiaogang Xu<author:sep>Zhipeng Cai<author:sep>Xihui Liu<author:sep>Hengshuang Zhao;http://arxiv.org/pdf/2312.06642v1;cs.CV;;nerf
2401.08633v1;http://arxiv.org/abs/2401.08633v1;2023-12-11;Creating Visual Effects with Neural Radiance Fields;"We present a pipeline for integrating NeRFs into traditional compositing VFX
pipelines using Nerfstudio, an open-source framework for training and rendering
NeRFs. Our approach involves using Blender, a widely used open-source 3D
creation software, to align camera paths and composite NeRF renders with meshes
and other NeRFs, allowing for seamless integration of NeRFs into traditional
VFX pipelines. Our NeRF Blender add-on allows for more controlled camera
trajectories of photorealistic scenes, compositing meshes and other
environmental effects with NeRFs, and compositing multiple NeRFs in a single
scene.This approach of generating NeRF aligned camera paths can be adapted to
other 3D tool sets and workflows, enabling a more seamless integration of NeRFs
into visual effects and film production. Documentation can be found here:
https://docs.nerf.studio/extensions/blender_addon.html";Cyrus Vachha;http://arxiv.org/pdf/2401.08633v1;cs.CV;2 pages, 4 figures;nerf
2312.06713v1;http://arxiv.org/abs/2312.06713v1;2023-12-10;TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint  Video;"Neural Radiance Fields (NeRF) revolutionize the realm of visual media by
providing photorealistic Free-Viewpoint Video (FVV) experiences, offering
viewers unparalleled immersion and interactivity. However, the technology's
significant storage requirements and the computational complexity involved in
generation and rendering currently limit its broader application. To close this
gap, this paper presents Temporal Tri-Plane Radiance Fields (TeTriRF), a novel
technology that significantly reduces the storage size for Free-Viewpoint Video
(FVV) while maintaining low-cost generation and rendering. TeTriRF introduces a
hybrid representation with tri-planes and voxel grids to support scaling up to
long-duration sequences and scenes with complex motions or rapid changes. We
propose a group training scheme tailored to achieving high training efficiency
and yielding temporally consistent, low-entropy scene representations.
Leveraging these properties of the representations, we introduce a compression
pipeline with off-the-shelf video codecs, achieving an order of magnitude less
storage size compared to the state-of-the-art. Our experiments demonstrate that
TeTriRF can achieve competitive quality with a higher compression rate.";Minye Wu<author:sep>Zehao Wang<author:sep>Georgios Kouros<author:sep>Tinne Tuytelaars;http://arxiv.org/pdf/2312.06713v1;cs.CV;13 pages, 11 figures;nerf
2312.05873v1;http://arxiv.org/abs/2312.05873v1;2023-12-10;Learning for CasADi: Data-driven Models in Numerical Optimization;"While real-world problems are often challenging to analyze analytically, deep
learning excels in modeling complex processes from data. Existing optimization
frameworks like CasADi facilitate seamless usage of solvers but face challenges
when integrating learned process models into numerical optimizations. To
address this gap, we present the Learning for CasADi (L4CasADi) framework,
enabling the seamless integration of PyTorch-learned models with CasADi for
efficient and potentially hardware-accelerated numerical optimization. The
applicability of L4CasADi is demonstrated with two tutorial examples: First, we
optimize a fish's trajectory in a turbulent river for energy efficiency where
the turbulent flow is represented by a PyTorch model. Second, we demonstrate
how an implicit Neural Radiance Field environment representation can be easily
leveraged for optimal control with L4CasADi. L4CasADi, along with examples and
documentation, is available under MIT license at
https://github.com/Tim-Salzmann/l4casadi";Tim Salzmann<author:sep>Jon Arrizabalaga<author:sep>Joel Andersson<author:sep>Marco Pavone<author:sep>Markus Ryll;http://arxiv.org/pdf/2312.05873v1;eess.SY;;
2312.05855v1;http://arxiv.org/abs/2312.05855v1;2023-12-10;NeVRF: Neural Video-based Radiance Fields for Long-duration Sequences;"Adopting Neural Radiance Fields (NeRF) to long-duration dynamic sequences has
been challenging. Existing methods struggle to balance between quality and
storage size and encounter difficulties with complex scene changes such as
topological changes and large motions. To tackle these issues, we propose a
novel neural video-based radiance fields (NeVRF) representation. NeVRF marries
neural radiance field with image-based rendering to support photo-realistic
novel view synthesis on long-duration dynamic inward-looking scenes. We
introduce a novel multi-view radiance blending approach to predict radiance
directly from multi-view videos. By incorporating continual learning
techniques, NeVRF can efficiently reconstruct frames from sequential data
without revisiting previous frames, enabling long-duration free-viewpoint
video. Furthermore, with a tailored compression approach, NeVRF can compactly
represent dynamic scenes, making dynamic radiance fields more practical in
real-world scenarios. Our extensive experiments demonstrate the effectiveness
of NeVRF in enabling long-duration sequence rendering, sequential data
reconstruction, and compact data storage.";Minye Wu<author:sep>Tinne Tuytelaars;http://arxiv.org/pdf/2312.05855v1;cs.CV;11 pages, 12 figures;nerf
2312.05748v1;http://arxiv.org/abs/2312.05748v1;2023-12-10;IL-NeRF: Incremental Learning for Neural Radiance Fields with Camera  Pose Alignment;"Neural radiance fields (NeRF) is a promising approach for generating
photorealistic images and representing complex scenes. However, when processing
data sequentially, it can suffer from catastrophic forgetting, where previous
data is easily forgotten after training with new data. Existing incremental
learning methods using knowledge distillation assume that continuous data
chunks contain both 2D images and corresponding camera pose parameters,
pre-estimated from the complete dataset. This poses a paradox as the necessary
camera pose must be estimated from the entire dataset, even though the data
arrives sequentially and future chunks are inaccessible. In contrast, we focus
on a practical scenario where camera poses are unknown. We propose IL-NeRF, a
novel framework for incremental NeRF training, to address this challenge.
IL-NeRF's key idea lies in selecting a set of past camera poses as references
to initialize and align the camera poses of incoming image data. This is
followed by a joint optimization of camera poses and replay-based NeRF
distillation. Our experiments on real-world indoor and outdoor scenes show that
IL-NeRF handles incremental NeRF training and outperforms the baselines by up
to $54.04\%$ in rendering quality.";Letian Zhang<author:sep>Ming Li<author:sep>Chen Chen<author:sep>Jie Xu;http://arxiv.org/pdf/2312.05748v1;cs.CV;;nerf
2312.05941v1;http://arxiv.org/abs/2312.05941v1;2023-12-10;ASH: Animatable Gaussian Splats for Efficient and Photoreal Human  Rendering;"Real-time rendering of photorealistic and controllable human avatars stands
as a cornerstone in Computer Vision and Graphics. While recent advances in
neural implicit rendering have unlocked unprecedented photorealism for digital
avatars, real-time performance has mostly been demonstrated for static scenes
only. To address this, we propose ASH, an animatable Gaussian splatting
approach for photorealistic rendering of dynamic humans in real-time. We
parameterize the clothed human as animatable 3D Gaussians, which can be
efficiently splatted into image space to generate the final rendering. However,
naively learning the Gaussian parameters in 3D space poses a severe challenge
in terms of compute. Instead, we attach the Gaussians onto a deformable
character model, and learn their parameters in 2D texture space, which allows
leveraging efficient 2D convolutional architectures that easily scale with the
required number of Gaussians. We benchmark ASH with competing methods on
pose-controllable avatars, demonstrating that our method outperforms existing
real-time methods by a large margin and shows comparable or even better results
than offline methods.";Haokai Pang<author:sep>Heming Zhu<author:sep>Adam Kortylewski<author:sep>Christian Theobalt<author:sep>Marc Habermann;http://arxiv.org/pdf/2312.05941v1;cs.CV;"13 pages, 7 figures. For project page, see
  https://vcai.mpi-inf.mpg.de/projects/ash/";gaussian splatting
2312.05664v1;http://arxiv.org/abs/2312.05664v1;2023-12-09;CoGS: Controllable Gaussian Splatting;"Capturing and re-animating the 3D structure of articulated objects present
significant barriers. On one hand, methods requiring extensively calibrated
multi-view setups are prohibitively complex and resource-intensive, limiting
their practical applicability. On the other hand, while single-camera Neural
Radiance Fields (NeRFs) offer a more streamlined approach, they have excessive
training and rendering costs. 3D Gaussian Splatting would be a suitable
alternative but for two reasons. Firstly, existing methods for 3D dynamic
Gaussians require synchronized multi-view cameras, and secondly, the lack of
controllability in dynamic scenarios. We present CoGS, a method for
Controllable Gaussian Splatting, that enables the direct manipulation of scene
elements, offering real-time control of dynamic scenes without the prerequisite
of pre-computing control signals. We evaluated CoGS using both synthetic and
real-world datasets that include dynamic objects that differ in degree of
difficulty. In our evaluations, CoGS consistently outperformed existing dynamic
and controllable neural representations in terms of visual fidelity.";Heng Yu<author:sep>Joel Julin<author:sep>Zoltán Á. Milacski<author:sep>Koichiro Niinuma<author:sep>László A. Jeni;http://arxiv.org/pdf/2312.05664v1;cs.CV;10 pages, in submission;gaussian splatting<tag:sep>nerf
2312.05572v1;http://arxiv.org/abs/2312.05572v1;2023-12-09;R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid  Landmarks Encoding and Progressive Multilayer Conditioning;"Dynamic NeRFs have recently garnered growing attention for 3D talking
portrait synthesis. Despite advances in rendering speed and visual quality,
challenges persist in enhancing efficiency and effectiveness. We present
R2-Talker, an efficient and effective framework enabling realistic real-time
talking head synthesis. Specifically, using multi-resolution hash grids, we
introduce a novel approach for encoding facial landmarks as conditional
features. This approach losslessly encodes landmark structures as conditional
features, decoupling input diversity, and conditional spaces by mapping
arbitrary landmarks to a unified feature space. We further propose a scheme of
progressive multilayer conditioning in the NeRF rendering pipeline for
effective conditional feature fusion. Our new approach has the following
advantages as demonstrated by extensive experiments compared with the
state-of-the-art works: 1) The lossless input encoding enables acquiring more
precise features, yielding superior visual quality. The decoupling of inputs
and conditional spaces improves generalizability. 2) The fusing of conditional
features and MLP outputs at each MLP layer enhances conditional impact,
resulting in more accurate lip synthesis and better visual quality. 3) It
compactly structures the fusion of conditional features, significantly
enhancing computational efficiency.";Zhiling Ye<author:sep>LiangGuo Zhang<author:sep>Dingheng Zeng<author:sep>Quan Lu<author:sep>Ning Jiang;http://arxiv.org/pdf/2312.05572v1;cs.CV;;nerf
2312.06686v1;http://arxiv.org/abs/2312.06686v1;2023-12-09;Robo360: A 3D Omnispective Multi-Material Robotic Manipulation Dataset;"Building robots that can automate labor-intensive tasks has long been the
core motivation behind the advancements in computer vision and the robotics
community. Recent interest in leveraging 3D algorithms, particularly neural
fields, has led to advancements in robot perception and physical understanding
in manipulation scenarios. However, the real world's complexity poses
significant challenges. To tackle these challenges, we present Robo360, a
dataset that features robotic manipulation with a dense view coverage, which
enables high-quality 3D neural representation learning, and a diverse set of
objects with various physical and optical properties and facilitates research
in various object manipulation and physical world modeling tasks. We confirm
the effectiveness of our dataset using existing dynamic NeRF and evaluate its
potential in learning multi-view policies. We hope that Robo360 can open new
research directions yet to be explored at the intersection of understanding the
physical world in 3D and robot control.";Litian Liang<author:sep>Liuyu Bian<author:sep>Caiwei Xiao<author:sep>Jialin Zhang<author:sep>Linghao Chen<author:sep>Isabella Liu<author:sep>Fanbo Xiang<author:sep>Zhiao Huang<author:sep>Hao Su;http://arxiv.org/pdf/2312.06686v1;cs.CV;;nerf
2312.04820v1;http://arxiv.org/abs/2312.04820v1;2023-12-08;Learn to Optimize Denoising Scores for 3D Generation: A Unified and  Improved Diffusion Prior on NeRF and 3D Gaussian Splatting;"We propose a unified framework aimed at enhancing the diffusion priors for 3D
generation tasks. Despite the critical importance of these tasks, existing
methodologies often struggle to generate high-caliber results. We begin by
examining the inherent limitations in previous diffusion priors. We identify a
divergence between the diffusion priors and the training procedures of
diffusion models that substantially impairs the quality of 3D generation. To
address this issue, we propose a novel, unified framework that iteratively
optimizes both the 3D model and the diffusion prior. Leveraging the different
learnable parameters of the diffusion prior, our approach offers multiple
configurations, affording various trade-offs between performance and
implementation complexity. Notably, our experimental results demonstrate that
our method markedly surpasses existing techniques, establishing new
state-of-the-art in the realm of text-to-3D generation. Furthermore, our
approach exhibits impressive performance on both NeRF and the newly introduced
3D Gaussian Splatting backbones. Additionally, our framework yields insightful
contributions to the understanding of recent score distillation methods, such
as the VSD and DDS loss.";Xiaofeng Yang<author:sep>Yiwen Chen<author:sep>Cheng Chen<author:sep>Chi Zhang<author:sep>Yi Xu<author:sep>Xulei Yang<author:sep>Fayao Liu<author:sep>Guosheng Lin;http://arxiv.org/pdf/2312.04820v1;cs.CV;;gaussian splatting<tag:sep>nerf
2312.04784v1;http://arxiv.org/abs/2312.04784v1;2023-12-08;Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular  Video;"Recent advancements in 3D avatar generation excel with multi-view supervision
for photorealistic models. However, monocular counterparts lag in quality
despite broader applicability. We propose ReCaLab to close this gap. ReCaLab is
a fully-differentiable pipeline that learns high-fidelity 3D human avatars from
just a single RGB video. A pose-conditioned deformable NeRF is optimized to
volumetrically represent a human subject in canonical T-pose. The canonical
representation is then leveraged to efficiently associate viewpoint-agnostic
textures using 2D-3D correspondences. This enables to separately generate
albedo and shading which jointly compose an RGB prediction. The design allows
to control intermediate results for human pose, body shape, texture, and
lighting with text prompts. An image-conditioned diffusion model thereby helps
to animate appearance and pose of the 3D avatar to create video sequences with
previously unseen human motion. Extensive experiments show that ReCaLab
outperforms previous monocular approaches in terms of image quality for image
synthesis tasks. ReCaLab even outperforms multi-view methods that leverage up
to 19x more synchronized videos for the task of novel pose rendering. Moreover,
natural language offers an intuitive user interface for creative manipulation
of 3D human avatars.";Yuchen Rao<author:sep>Eduardo Perez Pellitero<author:sep>Benjamin Busam<author:sep>Yiren Zhou<author:sep>Jifei Song;http://arxiv.org/pdf/2312.04784v1;cs.CV;Video link: https://youtu.be/Oz83z1es2J4;nerf
2312.05330v1;http://arxiv.org/abs/2312.05330v1;2023-12-08;Multi-view Inversion for 3D-aware Generative Adversarial Networks;"Current 3D GAN inversion methods for human heads typically use only one
single frontal image to reconstruct the whole 3D head model. This leaves out
meaningful information when multi-view data or dynamic videos are available.
Our method builds on existing state-of-the-art 3D GAN inversion techniques to
allow for consistent and simultaneous inversion of multiple views of the same
subject. We employ a multi-latent extension to handle inconsistencies present
in dynamic face videos to re-synthesize consistent 3D representations from the
sequence. As our method uses additional information about the target subject,
we observe significant enhancements in both geometric accuracy and image
quality, particularly when rendering from wide viewing angles. Moreover, we
demonstrate the editability of our inverted 3D renderings, which distinguishes
them from NeRF-based scene reconstructions.";Florian Barthel<author:sep>Anna Hilsmann<author:sep>Peter Eisert;http://arxiv.org/pdf/2312.05330v1;cs.CV;;nerf
2312.05311v1;http://arxiv.org/abs/2312.05311v1;2023-12-08;360° Volumetric Portrait Avatar;"We propose 360{\deg} Volumetric Portrait (3VP) Avatar, a novel method for
reconstructing 360{\deg} photo-realistic portrait avatars of human subjects
solely based on monocular video inputs. State-of-the-art monocular avatar
reconstruction methods rely on stable facial performance capturing. However,
the common usage of 3DMM-based facial tracking has its limits; side-views can
hardly be captured and it fails, especially, for back-views, as required inputs
like facial landmarks or human parsing masks are missing. This results in
incomplete avatar reconstructions that only cover the frontal hemisphere. In
contrast to this, we propose a template-based tracking of the torso, head and
facial expressions which allows us to cover the appearance of a human subject
from all sides. Thus, given a sequence of a subject that is rotating in front
of a single camera, we train a neural volumetric representation based on neural
radiance fields. A key challenge to construct this representation is the
modeling of appearance changes, especially, in the mouth region (i.e., lips and
teeth). We, therefore, propose a deformation-field-based blend basis which
allows us to interpolate between different appearance states. We evaluate our
approach on captured real-world data and compare against state-of-the-art
monocular reconstruction methods. In contrast to those, our method is the first
monocular technique that reconstructs an entire 360{\deg} avatar.";Jalees Nehvi<author:sep>Berna Kabadayi<author:sep>Julien Valentin<author:sep>Justus Thies;http://arxiv.org/pdf/2312.05311v1;cs.CV;Project page: https://jalees018.github.io/3VP-Avatar/;
2312.05239v1;http://arxiv.org/abs/2312.05239v1;2023-12-08;SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational  Score Distillation;"Despite their ability to generate high-resolution and diverse images from
text prompts, text-to-image diffusion models often suffer from slow iterative
sampling processes. Model distillation is one of the most effective directions
to accelerate these models. However, previous distillation methods fail to
retain the generation quality while requiring a significant amount of images
for training, either from real data or synthetically generated by the teacher
model. In response to this limitation, we present a novel image-free
distillation scheme named $\textbf{SwiftBrush}$. Drawing inspiration from
text-to-3D synthesis, in which a 3D neural radiance field that aligns with the
input prompt can be obtained from a 2D text-to-image diffusion prior via a
specialized loss without the use of any 3D data ground-truth, our approach
re-purposes that same loss for distilling a pretrained multi-step text-to-image
model to a student network that can generate high-fidelity images with just a
single inference step. In spite of its simplicity, our model stands as one of
the first one-step text-to-image generators that can produce images of
comparable quality to Stable Diffusion without reliance on any training image
data. Remarkably, SwiftBrush achieves an FID score of $\textbf{16.67}$ and a
CLIP score of $\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive
results or even substantially surpassing existing state-of-the-art distillation
techniques.";Thuan Hoang Nguyen<author:sep>Anh Tran;http://arxiv.org/pdf/2312.05239v1;cs.CV;Project Page: https://thuanz123.github.io/swiftbrush/;
2312.05161v1;http://arxiv.org/abs/2312.05161v1;2023-12-08;TriHuman : A Real-time and Controllable Tri-plane Representation for  Detailed Human Geometry and Appearance Synthesis;"Creating controllable, photorealistic, and geometrically detailed digital
doubles of real humans solely from video data is a key challenge in Computer
Graphics and Vision, especially when real-time performance is required. Recent
methods attach a neural radiance field (NeRF) to an articulated structure,
e.g., a body model or a skeleton, to map points into a pose canonical space
while conditioning the NeRF on the skeletal pose. These approaches typically
parameterize the neural field with a multi-layer perceptron (MLP) leading to a
slow runtime. To address this drawback, we propose TriHuman a novel
human-tailored, deformable, and efficient tri-plane representation, which
achieves real-time performance, state-of-the-art pose-controllable geometry
synthesis as well as photorealistic rendering quality. At the core, we
non-rigidly warp global ray samples into our undeformed tri-plane texture
space, which effectively addresses the problem of global points being mapped to
the same tri-plane locations. We then show how such a tri-plane feature
representation can be conditioned on the skeletal motion to account for dynamic
appearance and geometry changes. Our results demonstrate a clear step towards
higher quality in terms of geometry and appearance modeling of humans as well
as runtime performance.";Heming Zhu<author:sep>Fangneng Zhan<author:sep>Christian Theobalt<author:sep>Marc Habermann;http://arxiv.org/pdf/2312.05161v1;cs.CV;;nerf
2312.04143v1;http://arxiv.org/abs/2312.04143v1;2023-12-07;Towards 4D Human Video Stylization;"We present a first step towards 4D (3D and time) human video stylization,
which addresses style transfer, novel view synthesis and human animation within
a unified framework. While numerous video stylization methods have been
developed, they are often restricted to rendering images in specific viewpoints
of the input video, lacking the capability to generalize to novel views and
novel poses in dynamic scenes. To overcome these limitations, we leverage
Neural Radiance Fields (NeRFs) to represent videos, conducting stylization in
the rendered feature space. Our innovative approach involves the simultaneous
representation of both the human subject and the surrounding scene using two
NeRFs. This dual representation facilitates the animation of human subjects
across various poses and novel viewpoints. Specifically, we introduce a novel
geometry-guided tri-plane representation, significantly enhancing feature
representation robustness compared to direct tri-plane optimization. Following
the video reconstruction, stylization is performed within the NeRFs' rendered
feature space. Extensive experiments demonstrate that the proposed method
strikes a superior balance between stylized textures and temporal coherence,
surpassing existing approaches. Furthermore, our framework uniquely extends its
capabilities to accommodate novel poses and viewpoints, making it a versatile
tool for creative human video stylization.";Tiantian Wang<author:sep>Xinxin Zuo<author:sep>Fangzhou Mu<author:sep>Jian Wang<author:sep>Ming-Hsuan Yang;http://arxiv.org/pdf/2312.04143v1;cs.CV;Under Review;nerf
2312.04654v1;http://arxiv.org/abs/2312.04654v1;2023-12-07;NeuSD: Surface Completion with Multi-View Text-to-Image Diffusion;"We present a novel method for 3D surface reconstruction from multiple images
where only a part of the object of interest is captured. Our approach builds on
two recent developments: surface reconstruction using neural radiance fields
for the reconstruction of the visible parts of the surface, and guidance of
pre-trained 2D diffusion models in the form of Score Distillation Sampling
(SDS) to complete the shape in unobserved regions in a plausible manner. We
introduce three components. First, we suggest employing normal maps as a pure
geometric representation for SDS instead of color renderings which are
entangled with the appearance information. Second, we introduce the freezing of
the SDS noise during training which results in more coherent gradients and
better convergence. Third, we propose Multi-View SDS as a way to condition the
generation of the non-observable part of the surface without fine-tuning or
making changes to the underlying 2D Stable Diffusion model. We evaluate our
approach on the BlendedMVS dataset demonstrating significant qualitative and
quantitative improvements over competing methods.";Savva Ignatyev<author:sep>Daniil Selikhanovych<author:sep>Oleg Voynov<author:sep>Yiqun Wang<author:sep>Peter Wonka<author:sep>Stamatios Lefkimmiatis<author:sep>Evgeny Burnaev;http://arxiv.org/pdf/2312.04654v1;cs.CV;;
2312.04106v1;http://arxiv.org/abs/2312.04106v1;2023-12-07;Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial  Reconstruction;"Neural radiance fields (NeRF) typically require a complete set of images
taken from multiple camera perspectives to accurately reconstruct geometric
details. However, this approach raise significant privacy concerns in the
context of facial reconstruction. The critical need for privacy protection
often leads invidividuals to be reluctant in sharing their facial images, due
to fears of potential misuse or security risks. Addressing these concerns, we
propose a method that leverages privacy-preserving images for reconstructing 3D
head geometry within the NeRF framework. Our method stands apart from
traditional facial reconstruction techniques as it does not depend on RGB
information from images containing sensitive facial data. Instead, it
effectively generates plausible facial geometry using a series of
identity-obscured inputs, thereby protecting facial privacy.";Jiayi Kong<author:sep>Baixin Xu<author:sep>Xurui Song<author:sep>Chen Qian<author:sep>Jun Luo<author:sep>Ying He;http://arxiv.org/pdf/2312.04106v1;cs.CV;;nerf
2312.04558v1;http://arxiv.org/abs/2312.04558v1;2023-12-07;MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar;"The ability to animate photo-realistic head avatars reconstructed from
monocular portrait video sequences represents a crucial step in bridging the
gap between the virtual and real worlds. Recent advancements in head avatar
techniques, including explicit 3D morphable meshes (3DMM), point clouds, and
neural implicit representation have been exploited for this ongoing research.
However, 3DMM-based methods are constrained by their fixed topologies,
point-based approaches suffer from a heavy training burden due to the extensive
quantity of points involved, and the last ones suffer from limitations in
deformation flexibility and rendering efficiency. In response to these
challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head
Avatar), a novel approach that harnesses 3D Gaussian point representation
coupled with a Gaussian deformation field to learn explicit head avatars from
monocular portrait videos. We define our head avatars with Gaussian points
characterized by adaptable shapes, enabling flexible topology. These points
exhibit movement with a Gaussian deformation field in alignment with the target
pose and expression of a person, facilitating efficient deformation.
Additionally, the Gaussian points have controllable shape, size, color, and
opacity combined with Gaussian splatting, allowing for efficient training and
rendering. Experiments demonstrate the superior performance of our method,
which achieves state-of-the-art results among previous methods.";Yufan Chen<author:sep>Lizhen Wang<author:sep>Qijing Li<author:sep>Hongjiang Xiao<author:sep>Shengping Zhang<author:sep>Hongxun Yao<author:sep>Yebin Liu;http://arxiv.org/pdf/2312.04558v1;cs.CV;"The link to our projectpage is
  https://yufan1012.github.io/MonoGaussianAvatar";gaussian splatting
2312.04527v1;http://arxiv.org/abs/2312.04527v1;2023-12-07;Correspondences of the Third Kind: Camera Pose Estimation from Object  Reflection;"Computer vision has long relied on two kinds of correspondences: pixel
correspondences in images and 3D correspondences on object surfaces. Is there
another kind, and if there is, what can they do for us? In this paper, we
introduce correspondences of the third kind we call reflection correspondences
and show that they can help estimate camera pose by just looking at objects
without relying on the background. Reflection correspondences are point
correspondences in the reflected world, i.e., the scene reflected by the object
surface. The object geometry and reflectance alters the scene geometrically and
radiometrically, respectively, causing incorrect pixel correspondences.
Geometry recovered from each image is also hampered by distortions, namely
generalized bas-relief ambiguity, leading to erroneous 3D correspondences. We
show that reflection correspondences can resolve the ambiguities arising from
these distortions. We introduce a neural correspondence estimator and a RANSAC
algorithm that fully leverages all three kinds of correspondences for robust
and accurate joint camera pose and object shape estimation just from the object
appearance. The method expands the horizon of numerous downstream tasks,
including camera pose estimation for appearance modeling (e.g., NeRF) and
motion estimation of reflective objects (e.g., cars on the road), to name a
few, as it relieves the requirement of overlapping background.";Kohei Yamashita<author:sep>Vincent Lepetit<author:sep>Ko Nishino;http://arxiv.org/pdf/2312.04527v1;cs.CV;;nerf
2312.04651v1;http://arxiv.org/abs/2312.04651v1;2023-12-07;VOODOO 3D: Volumetric Portrait Disentanglement for One-Shot 3D Head  Reenactment;"We present a 3D-aware one-shot head reenactment method based on a fully
volumetric neural disentanglement framework for source appearance and driver
expressions. Our method is real-time and produces high-fidelity and
view-consistent output, suitable for 3D teleconferencing systems based on
holographic displays. Existing cutting-edge 3D-aware reenactment methods often
use neural radiance fields or 3D meshes to produce view-consistent appearance
encoding, but, at the same time, they rely on linear face models, such as 3DMM,
to achieve its disentanglement with facial expressions. As a result, their
reenactment results often exhibit identity leakage from the driver or have
unnatural expressions. To address these problems, we propose a neural
self-supervised disentanglement approach that lifts both the source image and
driver video frame into a shared 3D volumetric representation based on
tri-planes. This representation can then be freely manipulated with expression
tri-planes extracted from the driving images and rendered from an arbitrary
view using neural radiance fields. We achieve this disentanglement via
self-supervised learning on a large in-the-wild video dataset. We further
introduce a highly effective fine-tuning approach to improve the
generalizability of the 3D lifting using the same real-world data. We
demonstrate state-of-the-art performance on a wide range of datasets, and also
showcase high-quality 3D-aware head reenactment on highly challenging and
diverse subjects, including non-frontal head poses and complex expressions for
both source and driver.";Phong Tran<author:sep>Egor Zakharov<author:sep>Long-Nhat Ho<author:sep>Anh Tuan Tran<author:sep>Liwen Hu<author:sep>Hao Li;http://arxiv.org/pdf/2312.04651v1;cs.CV;;
2312.04337v1;http://arxiv.org/abs/2312.04337v1;2023-12-07;Multi-View Unsupervised Image Generation with Cross Attention Guidance;"The growing interest in novel view synthesis, driven by Neural Radiance Field
(NeRF) models, is hindered by scalability issues due to their reliance on
precisely annotated multi-view images. Recent models address this by
fine-tuning large text2image diffusion models on synthetic multi-view data.
Despite robust zero-shot generalization, they may need post-processing and can
face quality issues due to the synthetic-real domain gap. This paper introduces
a novel pipeline for unsupervised training of a pose-conditioned diffusion
model on single-category datasets. With the help of pretrained self-supervised
Vision Transformers (DINOv2), we identify object poses by clustering the
dataset through comparing visibility and locations of specific object parts.
The pose-conditioned diffusion model, trained on pose labels, and equipped with
cross-frame attention at inference time ensures cross-view consistency, that is
further aided by our novel hard-attention guidance. Our model, MIRAGE,
surpasses prior work in novel view synthesis on real images. Furthermore,
MIRAGE is robust to diverse textures and geometries, as demonstrated with our
experiments on synthetic images generated with pretrained Stable Diffusion.";Llukman Cerkezi<author:sep>Aram Davtyan<author:sep>Sepehr Sameni<author:sep>Paolo Favaro;http://arxiv.org/pdf/2312.04337v1;cs.CV;;nerf
2312.04564v1;http://arxiv.org/abs/2312.04564v1;2023-12-07;EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS;"Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view
scene synthesis. It addresses the challenges of lengthy training times and slow
rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,
differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time
rendering and accelerated training. They, however, demand substantial memory
resources for both training and storage, as they require millions of Gaussians
in their point cloud representation for each scene. We present a technique
utilizing quantized embeddings to significantly reduce memory storage
requirements and a coarse-to-fine training strategy for a faster and more
stable optimization of the Gaussian point clouds. Our approach results in scene
representations with fewer Gaussians and quantized representations, leading to
faster training times and rendering speeds for real-time rendering of high
resolution scenes. We reduce memory by more than an order of magnitude all
while maintaining the reconstruction quality. We validate the effectiveness of
our approach on a variety of datasets and scenes preserving the visual quality
while consuming 10-20x less memory and faster training/inference speed. Project
page and code is available https://efficientgaussian.github.io";Sharath Girish<author:sep>Kamal Gupta<author:sep>Abhinav Shrivastava;http://arxiv.org/pdf/2312.04564v1;cs.CV;"Website: https://efficientgaussian.github.io Code:
  https://github.com/Sharath-girish/efficientgaussian";gaussian splatting<tag:sep>nerf
2312.04565v1;http://arxiv.org/abs/2312.04565v1;2023-12-07;MuRF: Multi-Baseline Radiance Fields;"We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward
approach to solving sparse view synthesis under multiple different baseline
settings (small and large baselines, and different number of input views). To
render a target novel view, we discretize the 3D space into planes parallel to
the target image plane, and accordingly construct a target view frustum volume.
Such a target volume representation is spatially aligned with the target view,
which effectively aggregates relevant information from the input views for
high-quality rendering. It also facilitates subsequent radiance field
regression with a convolutional network thanks to its axis-aligned nature. The
3D context modeled by the convolutional network enables our method to synthesis
sharper scene structures than prior works. Our MuRF achieves state-of-the-art
performance across multiple different baseline settings and diverse scenarios
ranging from simple objects (DTU) to complex indoor and outdoor scenes
(RealEstate10K and LLFF). We also show promising zero-shot generalization
abilities on the Mip-NeRF 360 dataset, demonstrating the general applicability
of MuRF.";Haofei Xu<author:sep>Anpei Chen<author:sep>Yuedong Chen<author:sep>Christos Sakaridis<author:sep>Yulun Zhang<author:sep>Marc Pollefeys<author:sep>Andreas Geiger<author:sep>Fisher Yu;http://arxiv.org/pdf/2312.04565v1;cs.CV;Project page: https://haofeixu.github.io/murf/;nerf
2312.03357v1;http://arxiv.org/abs/2312.03357v1;2023-12-06;RING-NeRF: A Versatile Architecture based on Residual Implicit Neural  Grids;"Since their introduction, Neural Fields have become very popular for 3D
reconstruction and new view synthesis. Recent researches focused on
accelerating the process, as well as improving the robustness to variation of
the observation distance and limited number of supervised viewpoints. However,
those approaches often led to dedicated solutions that cannot be easily
combined. To tackle this issue, we introduce a new simple but efficient
architecture named RING-NeRF, based on Residual Implicit Neural Grids, that
provides a control on the level of detail of the mapping function between the
scene and the latent spaces. Associated with a distance-aware forward mapping
mechanism and a continuous coarse-to-fine reconstruction process, our versatile
architecture demonstrates both fast training and state-of-the-art performances
in terms of: (1) anti-aliased rendering, (2) reconstruction quality from few
supervised viewpoints, and (3) robustness in the absence of appropriate
scene-specific initialization for SDF-based NeRFs. We also demonstrate that our
architecture can dynamically add grids to increase the details of the
reconstruction, opening the way to adaptive reconstruction.";Doriand Petit<author:sep>Steve Bourgeois<author:sep>Dumitru Pavel<author:sep>Vincent Gay-Bellile<author:sep>Florian Chabot<author:sep>Loic Barthe;http://arxiv.org/pdf/2312.03357v1;cs.CV;;nerf
2312.03203v1;http://arxiv.org/abs/2312.03203v1;2023-12-06;Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled  Feature Fields;"3D scene representations have gained immense popularity in recent years.
Methods that use Neural Radiance fields are versatile for traditional tasks
such as novel view synthesis. In recent times, some work has emerged that aims
to extend the functionality of NeRF beyond view synthesis, for semantically
aware tasks such as editing and segmentation using 3D feature field
distillation from 2D foundation models. However, these methods have two major
limitations: (a) they are limited by the rendering speed of NeRF pipelines, and
(b) implicitly represented feature fields suffer from continuity artifacts
reducing feature quality. Recently, 3D Gaussian Splatting has shown
state-of-the-art performance on real-time radiance field rendering. In this
work, we go one step further: in addition to radiance field rendering, we
enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D
foundation model distillation. This translation is not straightforward: naively
incorporating feature fields in the 3DGS framework leads to warp-level
divergence. We propose architectural and training changes to efficiently avert
this problem. Our proposed method is general, and our experiments showcase
novel view semantic segmentation, language-guided editing and segment anything
through learning feature fields from state-of-the-art 2D foundation models such
as SAM and CLIP-LSeg. Across experiments, our distillation method is able to
provide comparable or better results, while being significantly faster to both
train and render. Additionally, to the best of our knowledge, we are the first
method to enable point and bounding-box prompting for radiance field
manipulation, by leveraging the SAM model. Project website at:
https://feature-3dgs.github.io/";Shijie Zhou<author:sep>Haoran Chang<author:sep>Sicheng Jiang<author:sep>Zhiwen Fan<author:sep>Zehao Zhu<author:sep>Dejia Xu<author:sep>Pradyumna Chari<author:sep>Suya You<author:sep>Zhangyang Wang<author:sep>Achuta Kadambi;http://arxiv.org/pdf/2312.03203v1;cs.CV;;gaussian splatting<tag:sep>nerf
2312.03372v1;http://arxiv.org/abs/2312.03372v1;2023-12-06;Evaluating the point cloud of individual trees generated from images  based on Neural Radiance fields (NeRF) method;"Three-dimensional (3D) reconstruction of trees has always been a key task in
precision forestry management and research. Due to the complex branch
morphological structure of trees themselves and the occlusions from tree stems,
branches and foliage, it is difficult to recreate a complete three-dimensional
tree model from a two-dimensional image by conventional photogrammetric
methods. In this study, based on tree images collected by various cameras in
different ways, the Neural Radiance Fields (NeRF) method was used for
individual tree reconstruction and the exported point cloud models are compared
with point cloud derived from photogrammetric reconstruction and laser scanning
methods. The results show that the NeRF method performs well in individual tree
3D reconstruction, as it has higher successful reconstruction rate, better
reconstruction in the canopy area, it requires less amount of images as input.
Compared with photogrammetric reconstruction method, NeRF has significant
advantages in reconstruction efficiency and is adaptable to complex scenes, but
the generated point cloud tends to be noisy and low resolution. The accuracy of
tree structural parameters (tree height and diameter at breast height)
extracted from the photogrammetric point cloud is still higher than those of
derived from the NeRF point cloud. The results of this study illustrate the
great potential of NeRF method for individual tree reconstruction, and it
provides new ideas and research directions for 3D reconstruction and
visualization of complex forest scenes.";Hongyu Huang<author:sep>Guoji Tian<author:sep>Chongcheng Chen;http://arxiv.org/pdf/2312.03372v1;cs.CV;"25 pages; 6 figures";nerf
2312.03431v1;http://arxiv.org/abs/2312.03431v1;2023-12-06;Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle;"We introduce Gaussian-Flow, a novel point-based approach for fast dynamic
scene reconstruction and real-time rendering from both multi-view and monocular
videos. In contrast to the prevalent NeRF-based approaches hampered by slow
training and rendering speeds, our approach harnesses recent advancements in
point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain
Deformation Model (DDDM) is proposed to explicitly model attribute deformations
of each Gaussian point, where the time-dependent residual of each attribute is
captured by a polynomial fitting in the time domain, and a Fourier series
fitting in the frequency domain. The proposed DDDM is capable of modeling
complex scene deformations across long video footage, eliminating the need for
training separate 3DGS for each frame or introducing an additional implicit
neural field to model 3D dynamics. Moreover, the explicit deformation modeling
for discretized Gaussian points ensures ultra-fast training and rendering of a
4D scene, which is comparable to the original 3DGS designed for static 3D
reconstruction. Our proposed approach showcases a substantial efficiency
improvement, achieving a $5\times$ faster training speed compared to the
per-frame 3DGS modeling. In addition, quantitative results demonstrate that the
proposed Gaussian-Flow significantly outperforms previous leading methods in
novel view rendering quality. Project page:
https://nju-3dv.github.io/projects/Gaussian-Flow";Youtian Lin<author:sep>Zuozhuo Dai<author:sep>Siyu Zhu<author:sep>Yao Yao;http://arxiv.org/pdf/2312.03431v1;cs.CV;;gaussian splatting<tag:sep>nerf
2312.10070v1;http://arxiv.org/abs/2312.10070v1;2023-12-06;Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting;"We present a new dense simultaneous localization and mapping (SLAM) method
that uses Gaussian splats as a scene representation. The new representation
enables interactive-time reconstruction and photo-realistic rendering of
real-world and synthetic scenes. We propose novel strategies for seeding and
optimizing Gaussian splats to extend their use from multiview offline scenarios
to sequential monocular RGBD input data setups. In addition, we extend Gaussian
splats to encode geometry and experiment with tracking against this scene
representation. Our method achieves state-of-the-art rendering quality on both
real-world and synthetic datasets while being competitive in reconstruction
performance and runtime.";Vladimir Yugay<author:sep>Yue Li<author:sep>Theo Gevers<author:sep>Martin R. Oswald;http://arxiv.org/pdf/2312.10070v1;cs.CV;;gaussian splatting
2312.03461v2;http://arxiv.org/abs/2312.03461v2;2023-12-06;HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian  Splatting;"We have recently seen tremendous progress in photo-real human modeling and
rendering. Yet, efficiently rendering realistic human performance and
integrating it into the rasterization pipeline remains challenging. In this
paper, we present HiFi4G, an explicit and compact Gaussian-based approach for
high-fidelity human performance rendering from dense footage. Our core
intuition is to marry the 3D Gaussian representation with non-rigid tracking,
achieving a compact and compression-friendly representation. We first propose a
dual-graph mechanism to obtain motion priors, with a coarse deformation graph
for effective initialization and a fine-grained Gaussian graph to enforce
subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with
adaptive spatial-temporal regularizers to effectively balance the non-rigid
prior and Gaussian updating. We also present a companion compression scheme
with residual compensation for immersive experiences on various platforms. It
achieves a substantial compression rate of approximately 25 times, with less
than 2MB of storage per frame. Extensive experiments demonstrate the
effectiveness of our approach, which significantly outperforms existing
approaches in terms of optimization speed, rendering quality, and storage
overhead.";Yuheng Jiang<author:sep>Zhehao Shen<author:sep>Penghao Wang<author:sep>Zhuo Su<author:sep>Yu Hong<author:sep>Yingliang Zhang<author:sep>Jingyi Yu<author:sep>Lan Xu;http://arxiv.org/pdf/2312.03461v2;cs.CV;;
2312.03266v1;http://arxiv.org/abs/2312.03266v1;2023-12-06;SO-NeRF: Active View Planning for NeRF using Surrogate Objectives;"Despite the great success of Neural Radiance Fields (NeRF), its
data-gathering process remains vague with only a general rule of thumb of
sampling as densely as possible. The lack of understanding of what actually
constitutes good views for NeRF makes it difficult to actively plan a sequence
of views that yield the maximal reconstruction quality. We propose Surrogate
Objectives for Active Radiance Fields (SOAR), which is a set of interpretable
functions that evaluates the goodness of views using geometric and photometric
visual cues - surface coverage, geometric complexity, textural complexity, and
ray diversity. Moreover, by learning to infer the SOAR scores from a deep
network, SOARNet, we are able to effectively select views in mere seconds
instead of hours, without the need for prior visits to all the candidate views
or training any radiance field during such planning. Our experiments show
SOARNet outperforms the baselines with $\sim$80x speed-up while achieving
better or comparable reconstruction qualities. We finally show that SOAR is
model-agnostic, thus it generalizes across fully neural-implicit to fully
explicit approaches.";Keifer Lee<author:sep>Shubham Gupta<author:sep>Sunglyoung Kim<author:sep>Bhargav Makwana<author:sep>Chao Chen<author:sep>Chen Feng;http://arxiv.org/pdf/2312.03266v1;cs.CV;13 pages;nerf
2312.03869v1;http://arxiv.org/abs/2312.03869v1;2023-12-06;Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion;"This paper presents a novel approach to inpainting 3D regions of a scene,
given masked multi-view images, by distilling a 2D diffusion model into a
learned 3D scene representation (e.g. a NeRF). Unlike 3D generative methods
that explicitly condition the diffusion model on camera pose or multi-view
information, our diffusion model is conditioned only on a single masked 2D
image. Nevertheless, we show that this 2D diffusion model can still serve as a
generative prior in a 3D multi-view reconstruction problem where we optimize a
NeRF using a combination of score distillation sampling and NeRF reconstruction
losses. Predicted depth is used as additional supervision to encourage accurate
geometry. We compare our approach to 3D inpainting methods that focus on object
removal. Because our method can generate content to fill any 3D masked region,
we additionally demonstrate 3D object completion, 3D object replacement, and 3D
scene completion.";Kira Prabhu<author:sep>Jane Wu<author:sep>Lynn Tsai<author:sep>Peter Hedman<author:sep>Dan B Goldman<author:sep>Ben Poole<author:sep>Michael Broxton;http://arxiv.org/pdf/2312.03869v1;cs.CV;;nerf
2312.03420v1;http://arxiv.org/abs/2312.03420v1;2023-12-06;Artist-Friendly Relightable and Animatable Neural Heads;"An increasingly common approach for creating photo-realistic digital avatars
is through the use of volumetric neural fields. The original neural radiance
field (NeRF) allowed for impressive novel view synthesis of static heads when
trained on a set of multi-view images, and follow up methods showed that these
neural representations can be extended to dynamic avatars. Recently, new
variants also surpassed the usual drawback of baked-in illumination in neural
representations, showing that static neural avatars can be relit in any
environment. In this work we simultaneously tackle both the motion and
illumination problem, proposing a new method for relightable and animatable
neural heads. Our method builds on a proven dynamic avatar approach based on a
mixture of volumetric primitives, combined with a recently-proposed lightweight
hardware setup for relightable neural fields, and includes a novel architecture
that allows relighting dynamic neural avatars performing unseen expressions in
any environment, even with nearfield illumination and viewpoints.";Yingyan Xu<author:sep>Prashanth Chandran<author:sep>Sebastian Weiss<author:sep>Markus Gross<author:sep>Gaspard Zoss<author:sep>Derek Bradley;http://arxiv.org/pdf/2312.03420v1;cs.CV;;nerf
2312.02963v1;http://arxiv.org/abs/2312.02963v1;2023-12-05;MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human  Captures;"In this era, the success of large language models and text-to-image models
can be attributed to the driving force of large-scale datasets. However, in the
realm of 3D vision, while remarkable progress has been made with models trained
on large-scale synthetic and real-captured object data like Objaverse and
MVImgNet, a similar level of progress has not been observed in the domain of
human-centric tasks partially due to the lack of a large-scale human dataset.
Existing datasets of high-fidelity 3D human capture continue to be mid-sized
due to the significant challenges in acquiring large-scale high-quality 3D
human data. To bridge this gap, we present MVHumanNet, a dataset that comprises
multi-view human action sequences of 4,500 human identities. The primary focus
of our work is on collecting human data that features a large number of diverse
identities and everyday clothing using a multi-view human capture system, which
facilitates easily scalable data collection. Our dataset contains 9,000 daily
outfits, 60,000 motion sequences and 645 million frames with extensive
annotations, including human masks, camera parameters, 2D and 3D keypoints,
SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the
potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot
studies on view-consistent action recognition, human NeRF reconstruction,
text-driven view-unconstrained human image generation, as well as 2D
view-unconstrained human image and 3D avatar generation. Extensive experiments
demonstrate the performance improvements and effective applications enabled by
the scale provided by MVHumanNet. As the current largest-scale 3D human
dataset, we hope that the release of MVHumanNet data with annotations will
foster further innovations in the domain of 3D human-centric tasks at scale.";Zhangyang Xiong<author:sep>Chenghong Li<author:sep>Kenkun Liu<author:sep>Hongjie Liao<author:sep>Jianqiao Hu<author:sep>Junyi Zhu<author:sep>Shuliang Ning<author:sep>Lingteng Qiu<author:sep>Chongjie Wang<author:sep>Shijie Wang<author:sep>Shuguang Cui<author:sep>Xiaoguang Han;http://arxiv.org/pdf/2312.02963v1;cs.CV;Project page: https://x-zhangyang.github.io/MVHumanNet/;nerf
2312.02973v1;http://arxiv.org/abs/2312.02973v1;2023-12-05;GauHuman: Articulated Gaussian Splatting from Monocular Human Videos;"We present, GauHuman, a 3D human model with Gaussian Splatting for both fast
training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with
existing NeRF-based implicit representation modelling frameworks demanding
hours of training and seconds of rendering per frame. Specifically, GauHuman
encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians
from canonical space to posed space with linear blend skinning (LBS), in which
effective pose and LBS refinement modules are designed to learn fine details of
3D humans under negligible computational cost. Moreover, to enable fast
optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human
prior, while splitting/cloning via KL divergence guidance, along with a novel
merge operation for further speeding up. Extensive experiments on ZJU_Mocap and
MonoCap datasets demonstrate that GauHuman achieves state-of-the-art
performance quantitatively and qualitatively with fast training and real-time
rendering speed. Notably, without sacrificing rendering quality, GauHuman can
fast model the 3D human performer with ~13k 3D Gaussians.";Shoukang Hu<author:sep>Ziwei Liu;http://arxiv.org/pdf/2312.02973v1;cs.CV;"project page: https://skhu101.github.io/GauHuman/; code:
  https://github.com/skhu101/GauHuman";gaussian splatting<tag:sep>nerf
2312.02434v1;http://arxiv.org/abs/2312.02434v1;2023-12-05;FINER: Flexible spectral-bias tuning in Implicit NEural Representation  by Variable-periodic Activation Functions;"Implicit Neural Representation (INR), which utilizes a neural network to map
coordinate inputs to corresponding attributes, is causing a revolution in the
field of signal processing. However, current INR techniques suffer from a
restricted capability to tune their supported frequency set, resulting in
imperfect performance when representing complex signals with multiple
frequencies. We have identified that this frequency-related problem can be
greatly alleviated by introducing variable-periodic activation functions, for
which we propose FINER. By initializing the bias of the neural network within
different ranges, sub-functions with various frequencies in the
variable-periodic function are selected for activation. Consequently, the
supported frequency set of FINER can be flexibly tuned, leading to improved
performance in signal representation. We demonstrate the capabilities of FINER
in the contexts of 2D image fitting, 3D signed distance field representation,
and 5D neural radiance fields optimization, and we show that it outperforms
existing INRs.";Zhen Liu<author:sep>Hao Zhu<author:sep>Qi Zhang<author:sep>Jingde Fu<author:sep>Weibing Deng<author:sep>Zhan Ma<author:sep>Yanwen Guo<author:sep>Xun Cao;http://arxiv.org/pdf/2312.02434v1;cs.CV;10 pages, 9 figures;
2312.02568v1;http://arxiv.org/abs/2312.02568v1;2023-12-05;Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent;"This paper explores promptable NeRF generation (e.g., text prompt or single
image prompt) for direct conditioning and fast generation of NeRF parameters
for the underlying 3D scenes, thus undoing complex intermediate steps while
providing full 3D generation with conditional control. Unlike previous
diffusion-CLIP-based pipelines that involve tedious per-prompt optimizations,
Prompt2NeRF-PIL is capable of generating a variety of 3D objects with a single
forward pass, leveraging a pre-trained implicit latent space of NeRF
parameters. Furthermore, in zero-shot tasks, our experiments demonstrate that
the NeRFs produced by our method serve as semantically informative
initializations, significantly accelerating the inference process of existing
prompt-to-NeRF methods. Specifically, we will show that our approach speeds up
the text-to-NeRF model DreamFusion and the 3D reconstruction speed of the
image-to-NeRF method Zero-1-to-3 by 3 to 5 times.";Jianmeng Liu<author:sep>Yuyao Zhang<author:sep>Zeyuan Meng<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2312.02568v1;cs.CV;;nerf
2312.02902v1;http://arxiv.org/abs/2312.02902v1;2023-12-05;HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting;"3D head animation has seen major quality and runtime improvements over the
last few years, particularly empowered by the advances in differentiable
rendering and neural radiance fields. Real-time rendering is a highly desirable
goal for real-world applications. We propose HeadGaS, the first model to use 3D
Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper
we introduce a hybrid model that extends the explicit representation from 3DGS
with a base of learnable latent features, which can be linearly blended with
low-dimensional parameters from parametric head models to obtain
expression-dependent final color and opacity values. We demonstrate that
HeadGaS delivers state-of-the-art results in real-time inference frame rates,
which surpasses baselines by up to ~2dB, while accelerating rendering speed by
over x10.";Helisa Dhamo<author:sep>Yinyu Nie<author:sep>Arthur Moreau<author:sep>Jifei Song<author:sep>Richard Shaw<author:sep>Yiren Zhou<author:sep>Eduardo Pérez-Pellitero;http://arxiv.org/pdf/2312.02902v1;cs.CV;;gaussian splatting
2312.03160v1;http://arxiv.org/abs/2312.03160v1;2023-12-05;HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces;"Neural radiance fields provide state-of-the-art view synthesis quality but
tend to be slow to render. One reason is that they make use of volume
rendering, thus requiring many samples (and model queries) per ray at render
time. Although this representation is flexible and easy to optimize, most
real-world objects can be modeled more efficiently with surfaces instead of
volumes, requiring far fewer samples per ray. This observation has spurred
considerable progress in surface representations such as signed distance
functions, but these may struggle to model semi-opaque and thin structures. We
propose a method, HybridNeRF, that leverages the strengths of both
representations by rendering most objects as surfaces while modeling the
(typically) small fraction of challenging regions volumetrically. We evaluate
HybridNeRF against the challenging Eyeful Tower dataset along with other
commonly used view synthesis datasets. When comparing to state-of-the-art
baselines, including recent rasterization-based approaches, we improve error
rates by 15-30% while achieving real-time framerates (at least 36 FPS) for
virtual-reality resolutions (2Kx2K).";Haithem Turki<author:sep>Vasu Agrawal<author:sep>Samuel Rota Bulò<author:sep>Lorenzo Porzi<author:sep>Peter Kontschieder<author:sep>Deva Ramanan<author:sep>Michael Zollhöfer<author:sep>Christian Richardt;http://arxiv.org/pdf/2312.03160v1;cs.CV;Project page: https://haithemturki.com/hybrid-nerf/;nerf
2312.02981v1;http://arxiv.org/abs/2312.02981v1;2023-12-05;ReconFusion: 3D Reconstruction with Diffusion Priors;"3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at
rendering photorealistic novel views of complex scenes. However, recovering a
high-quality NeRF typically requires tens to hundreds of input images,
resulting in a time-consuming capture process. We present ReconFusion to
reconstruct real-world scenes using only a few photos. Our approach leverages a
diffusion prior for novel view synthesis, trained on synthetic and multiview
datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel
camera poses beyond those captured by the set of input images. Our method
synthesizes realistic geometry and texture in underconstrained regions while
preserving the appearance of observed regions. We perform an extensive
evaluation across various real-world datasets, including forward-facing and
360-degree scenes, demonstrating significant performance improvements over
previous few-view NeRF reconstruction approaches.";Rundi Wu<author:sep>Ben Mildenhall<author:sep>Philipp Henzler<author:sep>Keunhong Park<author:sep>Ruiqi Gao<author:sep>Daniel Watson<author:sep>Pratul P. Srinivasan<author:sep>Dor Verbin<author:sep>Jonathan T. Barron<author:sep>Ben Poole<author:sep>Aleksander Holynski;http://arxiv.org/pdf/2312.02981v1;cs.CV;Project page: https://reconfusion.github.io/;nerf
2312.02751v2;http://arxiv.org/abs/2312.02751v2;2023-12-05;C-NERF: Representing Scene Changes as Directional Consistency  Difference-based NeRF;"In this work, we aim to detect the changes caused by object variations in a
scene represented by the neural radiance fields (NeRFs). Given an arbitrary
view and two sets of scene images captured at different timestamps, we can
predict the scene changes in that view, which has significant potential
applications in scene monitoring and measuring. We conducted preliminary
studies and found that such an exciting task cannot be easily achieved by
utilizing existing NeRFs and 2D change detection methods with many false or
missing detections. The main reason is that the 2D change detection is based on
the pixel appearance difference between spatial-aligned image pairs and
neglects the stereo information in the NeRF. To address the limitations, we
propose the C-NERF to represent scene changes as directional consistency
difference-based NeRF, which mainly contains three modules. We first perform
the spatial alignment of two NeRFs captured before and after changes. Then, we
identify the change points based on the direction-consistent constraint; that
is, real change points have similar change representations across view
directions, but fake change points do not. Finally, we design the change map
rendering process based on the built NeRFs and can generate the change map of
an arbitrarily specified view direction. To validate the effectiveness, we
build a new dataset containing ten scenes covering diverse scenarios with
different changing objects. Our approach surpasses state-of-the-art 2D change
detection and NeRF-based methods by a significant margin.";Rui Huang<author:sep>Binbin Jiang<author:sep>Qingyi Zhao<author:sep>William Wang<author:sep>Yuxiang Zhang<author:sep>Qing Guo;http://arxiv.org/pdf/2312.02751v2;cs.CV;;nerf
2312.02970v1;http://arxiv.org/abs/2312.02970v1;2023-12-05;Alchemist: Parametric Control of Material Properties with Diffusion  Models;"We propose a method to control material attributes of objects like roughness,
metallic, albedo, and transparency in real images. Our method capitalizes on
the generative prior of text-to-image models known for photorealism, employing
a scalar value and instructions to alter low-level material properties.
Addressing the lack of datasets with controlled material attributes, we
generated an object-centric synthetic dataset with physically-based materials.
Fine-tuning a modified pre-trained text-to-image model on this synthetic
dataset enables us to edit material properties in real-world images while
preserving all other attributes. We show the potential application of our model
to material edited NeRFs.";Prafull Sharma<author:sep>Varun Jampani<author:sep>Yuanzhen Li<author:sep>Xuhui Jia<author:sep>Dmitry Lagun<author:sep>Fredo Durand<author:sep>William T. Freeman<author:sep>Mark Matthews;http://arxiv.org/pdf/2312.02970v1;cs.CV;;nerf
2312.02255v1;http://arxiv.org/abs/2312.02255v1;2023-12-04;Re-Nerfing: Enforcing Geometric Constraints on Neural Radiance Fields  through Novel Views Synthesis;"Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis
capabilities even in large-scale, unbounded scenes, albeit requiring hundreds
of views or introducing artifacts in sparser settings. Their optimization
suffers from shape-radiance ambiguities wherever only a small visual overlap is
available. This leads to erroneous scene geometry and artifacts. In this paper,
we propose Re-Nerfing, a simple and general multi-stage approach that leverages
NeRF's own view synthesis to address these limitations. With Re-Nerfing, we
increase the scene's coverage and enhance the geometric consistency of novel
views as follows: First, we train a NeRF with the available views. Then, we use
the optimized NeRF to synthesize pseudo-views next to the original ones to
simulate a stereo or trifocal setup. Finally, we train a second NeRF with both
original and pseudo views while enforcing structural, epipolar constraints via
the newly synthesized images. Extensive experiments on the mip-NeRF 360 dataset
show the effectiveness of Re-Nerfing across denser and sparser input scenarios,
bringing improvements to the state-of-the-art Zip-NeRF, even when trained with
all views.";Felix Tristram<author:sep>Stefano Gasperini<author:sep>Federico Tombari<author:sep>Nassir Navab<author:sep>Benjamin Busam;http://arxiv.org/pdf/2312.02255v1;cs.CV;Code will be released upon acceptance;nerf
2312.14937v2;http://arxiv.org/abs/2312.14937v2;2023-12-04;SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes;"Novel view synthesis for dynamic scenes is still a challenging problem in
computer vision and graphics. Recently, Gaussian splatting has emerged as a
robust technique to represent static scenes and enable high-quality and
real-time novel view synthesis. Building upon this technique, we propose a new
representation that explicitly decomposes the motion and appearance of dynamic
scenes into sparse control points and dense Gaussians, respectively. Our key
idea is to use sparse control points, significantly fewer in number than the
Gaussians, to learn compact 6 DoF transformation bases, which can be locally
interpolated through learned interpolation weights to yield the motion field of
3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF
transformations for each control point, which reduces learning complexities,
enhances learning abilities, and facilitates obtaining temporal and spatial
coherent motion patterns. Then, we jointly learn the 3D Gaussians, the
canonical space locations of control points, and the deformation MLP to
reconstruct the appearance, geometry, and dynamics of 3D scenes. During
learning, the location and number of control points are adaptively adjusted to
accommodate varying motion complexities in different regions, and an ARAP loss
following the principle of as rigid as possible is developed to enforce spatial
continuity and local rigidity of learned motions. Finally, thanks to the
explicit sparse motion representation and its decomposition from appearance,
our method can enable user-controlled motion editing while retaining
high-fidelity appearances. Extensive experiments demonstrate that our approach
outperforms existing approaches on novel view synthesis with a high rendering
speed and enables novel appearance-preserved motion editing applications.
Project page: https://yihua7.github.io/SC-GS-web/";Yi-Hua Huang<author:sep>Yang-Tian Sun<author:sep>Ziyi Yang<author:sep>Xiaoyang Lyu<author:sep>Yan-Pei Cao<author:sep>Xiaojuan Qi;http://arxiv.org/pdf/2312.14937v2;cs.CV;Code link: https://github.com/yihua7/SC-GS;gaussian splatting
2312.02155v1;http://arxiv.org/abs/2312.02155v1;2023-12-04;GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for  Real-time Human Novel View Synthesis;"We present a new approach, termed GPS-Gaussian, for synthesizing novel views
of a character in a real-time manner. The proposed method enables 2K-resolution
rendering under a sparse-view camera setting. Unlike the original Gaussian
Splatting or neural implicit rendering methods that necessitate per-subject
optimizations, we introduce Gaussian parameter maps defined on the source views
and regress directly Gaussian Splatting properties for instant novel view
synthesis without any fine-tuning or optimization. To this end, we train our
Gaussian parameter regression module on a large amount of human scan data,
jointly with a depth estimation module to lift 2D parameter maps to 3D space.
The proposed framework is fully differentiable and experiments on several
datasets demonstrate that our method outperforms state-of-the-art methods while
achieving an exceeding rendering speed.";Shunyuan Zheng<author:sep>Boyao Zhou<author:sep>Ruizhi Shao<author:sep>Boning Liu<author:sep>Shengping Zhang<author:sep>Liqiang Nie<author:sep>Yebin Liu;http://arxiv.org/pdf/2312.02155v1;cs.CV;The link to our projectpage is https://shunyuanzheng.github.io;gaussian splatting
2312.02069v1;http://arxiv.org/abs/2312.02069v1;2023-12-04;GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians;"We introduce GaussianAvatars, a new method to create photorealistic head
avatars that are fully controllable in terms of expression, pose, and
viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian
splats that are rigged to a parametric morphable face model. This combination
facilitates photorealistic rendering while allowing for precise animation
control via the underlying parametric model, e.g., through expression transfer
from a driving sequence or by manually changing the morphable model parameters.
We parameterize each splat by a local coordinate frame of a triangle and
optimize for explicit displacement offset to obtain a more accurate geometric
representation. During avatar reconstruction, we jointly optimize for the
morphable model parameters and Gaussian splat parameters in an end-to-end
fashion. We demonstrate the animation capabilities of our photorealistic avatar
in several challenging scenarios. For instance, we show reenactments from a
driving video, where our method outperforms existing works by a significant
margin.";Shenhan Qian<author:sep>Tobias Kirschstein<author:sep>Liam Schoneveld<author:sep>Davide Davoli<author:sep>Simon Giebenhain<author:sep>Matthias Nießner;http://arxiv.org/pdf/2312.02069v1;cs.CV;Project page: https://shenhanqian.github.io/gaussian-avatars;
2312.02362v1;http://arxiv.org/abs/2312.02362v1;2023-12-04;PointNeRF++: A multi-scale, point-based Neural Radiance Field;"Point clouds offer an attractive source of information to complement images
in neural scene representations, especially when few images are available.
Neural rendering methods based on point clouds do exist, but they do not
perform well when the point cloud quality is low -- e.g., sparse or incomplete,
which is often the case with real-world data. We overcome these problems with a
simple representation that aggregates point clouds at multiple scale levels
with sparse voxel grids at different resolutions. To deal with point cloud
sparsity, we average across multiple scale levels -- but only among those that
are valid, i.e., that have enough neighboring points in proximity to the ray of
a pixel. To help model areas without points, we add a global voxel at the
coarsest scale, thus unifying ""classical"" and point-based NeRF formulations. We
validate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets,
outperforming the state of the art by a significant margin.";Weiwei Sun<author:sep>Eduard Trulls<author:sep>Yang-Che Tseng<author:sep>Sneha Sambandam<author:sep>Gopal Sharma<author:sep>Andrea Tagliasacchi<author:sep>Kwang Moo Yi;http://arxiv.org/pdf/2312.02362v1;cs.CV;;nerf
2312.02350v1;http://arxiv.org/abs/2312.02350v1;2023-12-04;Calibrated Uncertainties for Neural Radiance Fields;"Neural Radiance Fields have achieved remarkable results for novel view
synthesis but still lack a crucial component: precise measurement of
uncertainty in their predictions. Probabilistic NeRF methods have tried to
address this, but their output probabilities are not typically accurately
calibrated, and therefore do not capture the true confidence levels of the
model. Calibration is a particularly challenging problem in the sparse-view
setting, where additional held-out data is unavailable for fitting a calibrator
that generalizes to the test distribution. In this paper, we introduce the
first method for obtaining calibrated uncertainties from NeRF models. Our
method is based on a robust and efficient metric to calculate per-pixel
uncertainties from the predictive posterior distribution. We propose two
techniques that eliminate the need for held-out data. The first, based on patch
sampling, involves training two NeRF models for each scene. The second is a
novel meta-calibrator that only requires the training of one NeRF model. Our
proposed approach for obtaining calibrated uncertainties achieves
state-of-the-art uncertainty in the sparse-view setting while maintaining image
quality. We further demonstrate our method's effectiveness in applications such
as view enhancement and next-best view selection.";Niki Amini-Naieni<author:sep>Tomas Jakab<author:sep>Andrea Vedaldi<author:sep>Ronald Clark;http://arxiv.org/pdf/2312.02350v1;cs.CV;;nerf
2312.01689v2;http://arxiv.org/abs/2312.01689v2;2023-12-04;Fast and accurate sparse-view CBCT reconstruction using meta-learned  neural attenuation field and hash-encoding regularization;"Cone beam computed tomography (CBCT) is an emerging medical imaging technique
to visualize the internal anatomical structures of patients. During a CBCT
scan, several projection images of different angles or views are collectively
utilized to reconstruct a tomographic image. However, reducing the number of
projections in a CBCT scan while preserving the quality of a reconstructed
image is challenging due to the nature of an ill-posed inverse problem.
Recently, a neural attenuation field (NAF) method was proposed by adopting a
neural radiance field algorithm as a new way for CBCT reconstruction,
demonstrating fast and promising results using only 50 views. However,
decreasing the number of projections is still preferable to reduce potential
radiation exposure, and a faster reconstruction time is required considering a
typical scan time. In this work, we propose a fast and accurate sparse-view
CBCT reconstruction (FACT) method to provide better reconstruction quality and
faster optimization speed in the minimal number of view acquisitions ($<$ 50
views). In the FACT method, we meta-trained a neural network and a hash-encoder
using a few scans (= 15), and a new regularization technique is utilized to
reconstruct the details of an anatomical structure. In conclusion, we have
shown that the FACT method produced better, and faster reconstruction results
over the other conventional algorithms based on CBCT scans of different body
parts (chest, head, and abdomen) and CT vendors (Siemens, Phillips, and GE).";Heejun Shin<author:sep>Taehee Kim<author:sep>Jongho Lee<author:sep>Se Young Chun<author:sep>Seungryung Cho<author:sep>Dongmyung Shin;http://arxiv.org/pdf/2312.01689v2;eess.IV;;
2312.02015v1;http://arxiv.org/abs/2312.02015v1;2023-12-04;ColonNeRF: Neural Radiance Fields for High-Fidelity Long-Sequence  Colonoscopy Reconstruction;"Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.
However, accurate long-sequence colonoscopy reconstruction faces three major
challenges: (1) dissimilarity among segments of the colon due to its meandering
and convoluted shape; (2) co-existence of simple and intricately folded
geometry structures; (3) sparse viewpoints due to constrained camera
trajectories. To tackle these challenges, we introduce a new reconstruction
framework based on neural radiance field (NeRF), named ColonNeRF, which
leverages neural rendering for novel view synthesis of long-sequence
colonoscopy. Specifically, to reconstruct the entire colon in a piecewise
manner, our ColonNeRF introduces a region division and integration module,
effectively reducing shape dissimilarity and ensuring geometric consistency in
each segment. To learn both the simple and complex geometry in a unified
framework, our ColonNeRF incorporates a multi-level fusion module that
progressively models the colon regions from easy to hard. Additionally, to
overcome the challenges from sparse views, we devise a DensiNet module for
densifying camera poses under the guidance of semantic consistency. We conduct
extensive experiments on both synthetic and real-world datasets to evaluate our
ColonNeRF. Quantitatively, our ColonNeRF outperforms existing methods on two
benchmarks over four evaluation metrics. Notably, our LPIPS-ALEX scores exhibit
a substantial increase of about 67%-85% on the SimCol-to-3D dataset.
Qualitatively, our reconstruction visualizations show much clearer textures and
more accurate geometric details. These sufficiently demonstrate our superior
performance over the state-of-the-art methods.";Yufei Shi<author:sep>Beijia Lu<author:sep>Jia-Wei Liu<author:sep>Ming Li<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2312.02015v1;cs.CV;for Project Page, see https://showlab.github.io/ColonNeRF/;nerf
2312.01663v1;http://arxiv.org/abs/2312.01663v1;2023-12-04;Customize your NeRF: Adaptive Source Driven 3D Scene Editing via  Local-Global Iterative Training;"In this paper, we target the adaptive source driven 3D scene editing task by
proposing a CustomNeRF model that unifies a text description or a reference
image as the editing prompt. However, obtaining desired editing results
conformed with the editing prompt is nontrivial since there exist two
significant challenges, including accurate editing of only foreground regions
and multi-view consistency given a single-view reference image. To tackle the
first challenge, we propose a Local-Global Iterative Editing (LGIE) training
scheme that alternates between foreground region editing and full-image
editing, aimed at foreground-only manipulation while preserving the background.
For the second challenge, we also design a class-guided regularization that
exploits class priors within the generation model to alleviate the
inconsistency problem among different views in image-driven editing. Extensive
experiments show that our CustomNeRF produces precise editing results under
various real scenes for both text- and image-driven settings.";Runze He<author:sep>Shaofei Huang<author:sep>Xuecheng Nie<author:sep>Tianrui Hui<author:sep>Luoqi Liu<author:sep>Jiao Dai<author:sep>Jizhong Han<author:sep>Guanbin Li<author:sep>Si Liu;http://arxiv.org/pdf/2312.01663v1;cs.CV;14 pages, 13 figures, project website: https://customnerf.github.io/;nerf
2312.02121v1;http://arxiv.org/abs/2312.02121v1;2023-12-04;Mathematical Supplement for the $\texttt{gsplat}$ Library;"This report provides the mathematical details of the gsplat library, a
modular toolbox for efficient differentiable Gaussian splatting, as proposed by
Kerbl et al. It provides a self-contained reference for the computations
involved in the forward and backward passes of differentiable Gaussian
splatting. To facilitate practical usage and development, we provide a user
friendly Python API that exposes each component of the forward and backward
passes in rasterization at github.com/nerfstudio-project/gsplat .";Vickie Ye<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2312.02121v1;cs.MS;Find the library at: https://docs.gsplat.studio/;gaussian splatting<tag:sep>nerf
2312.02137v1;http://arxiv.org/abs/2312.02137v1;2023-12-04;MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D  Gaussians;"Understanding how we grasp objects with our hands has important applications
in areas like robotics and mixed reality. However, this challenging problem
requires accurate modeling of the contact between hands and objects. To capture
grasps, existing methods use skeletons, meshes, or parametric models that can
cause misalignments resulting in inaccurate contacts. We present MANUS, a
method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians.
We build a novel articulated 3D Gaussians representation that extends 3D
Gaussian splatting for high-fidelity representation of articulating hands.
Since our representation uses Gaussian primitives, it enables us to efficiently
and accurately estimate contacts between the hand and the object. For the most
accurate results, our method requires tens of camera views that current
datasets do not provide. We therefore build MANUS-Grasps, a new dataset that
contains hand-object grasps viewed from 53 cameras across 30+ scenes, 3
subjects, and comprising over 7M frames. In addition to extensive qualitative
results, we also show that our method outperforms others on a quantitative
contact evaluation method that uses paint transfer from the object to the hand.";Chandradeep Pokhariya<author:sep>Ishaan N Shah<author:sep>Angela Xing<author:sep>Zekun Li<author:sep>Kefan Chen<author:sep>Avinash Sharma<author:sep>Srinath Sridhar;http://arxiv.org/pdf/2312.02137v1;cs.CV;;gaussian splatting
2312.02157v1;http://arxiv.org/abs/2312.02157v1;2023-12-04;Mesh-Guided Neural Implicit Field Editing;"Neural implicit fields have emerged as a powerful 3D representation for
reconstructing and rendering photo-realistic views, yet they possess limited
editability. Conversely, explicit 3D representations, such as polygonal meshes,
offer ease of editing but may not be as suitable for rendering high-quality
novel views. To harness the strengths of both representations, we propose a new
approach that employs a mesh as a guiding mechanism in editing the neural
radiance field. We first introduce a differentiable method using marching
tetrahedra for polygonal mesh extraction from the neural implicit field and
then design a differentiable color extractor to assign colors obtained from the
volume renderings to this extracted mesh. This differentiable colored mesh
allows gradient back-propagation from the explicit mesh to the implicit fields,
empowering users to easily manipulate the geometry and color of neural implicit
fields. To enhance user control from coarse-grained to fine-grained levels, we
introduce an octree-based structure into its optimization. This structure
prioritizes the edited regions and the surface part, making our method achieve
fine-grained edits to the neural implicit field and accommodate various user
modifications, including object additions, component removals, specific area
deformations, and adjustments to local and global colors. Through extensive
experiments involving diverse scenes and editing operations, we have
demonstrated the capabilities and effectiveness of our method. Our project page
is: \url{https://cassiepython.github.io/MNeuEdit/}";Can Wang<author:sep>Mingming He<author:sep>Menglei Chai<author:sep>Dongdong Chen<author:sep>Jing Liao;http://arxiv.org/pdf/2312.02157v1;cs.CV;Project page: https://cassiepython.github.io/MNeuEdit/;
2312.02135v1;http://arxiv.org/abs/2312.02135v1;2023-12-04;Fast View Synthesis of Casual Videos;"Novel view synthesis from an in-the-wild video is difficult due to challenges
like scene dynamics and lack of parallax. While existing methods have shown
promising results with implicit neural radiance fields, they are slow to train
and render. This paper revisits explicit video representations to synthesize
high-quality novel views from a monocular video efficiently. We treat static
and dynamic video content separately. Specifically, we build a global static
scene model using an extended plane-based scene representation to synthesize
temporally coherent novel video. Our plane-based scene representation is
augmented with spherical harmonics and displacement maps to capture
view-dependent effects and model non-planar complex surface geometry. We opt to
represent the dynamic content as per-frame point clouds for efficiency. While
such representations are inconsistency-prone, minor temporal inconsistencies
are perceptually masked due to motion. We develop a method to quickly estimate
such a hybrid video representation and render novel views in real time. Our
experiments show that our method can render high-quality novel views from an
in-the-wild video with comparable quality to state-of-the-art methods while
being 100x faster in training and enabling real-time rendering.";Yao-Chih Lee<author:sep>Zhoutong Zhang<author:sep>Kevin Blackburn-Matzen<author:sep>Simon Niklaus<author:sep>Jianming Zhang<author:sep>Jia-Bin Huang<author:sep>Feng Liu;http://arxiv.org/pdf/2312.02135v1;cs.CV;Project page: https://casual-fvs.github.io/;
2312.01407v1;http://arxiv.org/abs/2312.01407v1;2023-12-03;VideoRF: Rendering Dynamic Radiance Fields as 2D Feature Video Streams;"Neural Radiance Fields (NeRFs) excel in photorealistically rendering static
scenes. However, rendering dynamic, long-duration radiance fields on ubiquitous
devices remains challenging, due to data storage and computational constraints.
In this paper, we introduce VideoRF, the first approach to enable real-time
streaming and rendering of dynamic radiance fields on mobile platforms. At the
core is a serialized 2D feature image stream representing the 4D radiance field
all in one. We introduce a tailored training scheme directly applied to this 2D
domain to impose the temporal and spatial redundancy of the feature image
stream. By leveraging the redundancy, we show that the feature image stream can
be efficiently compressed by 2D video codecs, which allows us to exploit video
hardware accelerators to achieve real-time decoding. On the other hand, based
on the feature image stream, we propose a novel rendering pipeline for VideoRF,
which has specialized space mappings to query radiance properties efficiently.
Paired with a deferred shading model, VideoRF has the capability of real-time
rendering on mobile devices thanks to its efficiency. We have developed a
real-time interactive player that enables online streaming and rendering of
dynamic scenes, offering a seamless and immersive free-viewpoint experience
across a range of devices, from desktops to mobile phones.";Liao Wang<author:sep>Kaixin Yao<author:sep>Chengcheng Guo<author:sep>Zhirui Zhang<author:sep>Qiang Hu<author:sep>Jingyi Yu<author:sep>Lan Xu<author:sep>Minye Wu;http://arxiv.org/pdf/2312.01407v1;cs.CV;Project page, see https://aoliao12138.github.io/VideoRF;nerf
2312.02218v1;http://arxiv.org/abs/2312.02218v1;2023-12-03;WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance  Fields;"Dynamic Neural Radiance Fields (Dynamic NeRF) enhance NeRF technology to
model moving scenes. However, they are resource intensive and challenging to
compress. To address this issue, this paper presents WavePlanes, a fast and
more compact explicit model. We propose a multi-scale space and space-time
feature plane representation using N-level 2-D wavelet coefficients. The
inverse discrete wavelet transform reconstructs N feature signals at varying
detail, which are linearly decoded to approximate the color and density of
volumes in a 4-D grid. Exploiting the sparsity of wavelet coefficients, we
compress a Hash Map containing only non-zero coefficients and their locations
on each plane. This results in a compressed model size of ~12 MB. Compared with
state-of-the-art plane-based models, WavePlanes is up to 15x smaller, less
computationally demanding and achieves comparable results in as little as one
hour of training - without requiring custom CUDA code or high performance
computing resources. Additionally, we propose new feature fusion schemes that
work as well as previously proposed schemes while providing greater
interpretability. Our code is available at:
https://github.com/azzarelli/waveplanes/";Adrian Azzarelli<author:sep>Nantheera Anantrasirichai<author:sep>David R Bull;http://arxiv.org/pdf/2312.02218v1;cs.CV;;nerf
2312.01531v1;http://arxiv.org/abs/2312.01531v1;2023-12-03;SANeRF-HQ: Segment Anything for NeRF in High Quality;"Recently, the Segment Anything Model (SAM) has showcased remarkable
capabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has
gained popularity as a method for various 3D problems beyond novel view
synthesis. Though there exist initial attempts to incorporate these two methods
into 3D segmentation, they face the challenge of accurately and consistently
segmenting objects in complex scenarios. In this paper, we introduce the
Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality
3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for
open-world object segmentation guided by user-supplied prompts, while
leveraging NeRF to aggregate information from different viewpoints. To overcome
the aforementioned challenges, we employ density field and RGB similarity to
enhance the accuracy of segmentation boundary during the aggregation.
Emphasizing on segmentation accuracy, we evaluate our method quantitatively on
multiple NeRF datasets where high-quality ground-truths are available or
manually annotated. SANeRF-HQ shows a significant quality improvement over
previous state-of-the-art methods in NeRF object segmentation, provides higher
flexibility for object localization, and enables more consistent object
segmentation across multiple views. Additional information can be found at
https://lyclyc52.github.io/SANeRF-HQ/.";Yichen Liu<author:sep>Benran Hu<author:sep>Chi-Keung Tang<author:sep>Yu-Wing Tai;http://arxiv.org/pdf/2312.01531v1;cs.CV;;nerf
2312.01003v2;http://arxiv.org/abs/2312.01003v2;2023-12-02;Self-Evolving Neural Radiance Fields;"Recently, neural radiance field (NeRF) has shown remarkable performance in
novel view synthesis and 3D reconstruction. However, it still requires abundant
high-quality images, limiting its applicability in real-world scenarios. To
overcome this limitation, recent works have focused on training NeRF only with
sparse viewpoints by giving additional regularizations, often called few-shot
NeRF. We observe that due to the under-constrained nature of the task, solely
using additional regularization is not enough to prevent the model from
overfitting to sparse viewpoints. In this paper, we propose a novel framework,
dubbed Self-Evolving Neural Radiance Fields (SE-NeRF), that applies a
self-training framework to NeRF to address these problems. We formulate
few-shot NeRF into a teacher-student framework to guide the network to learn a
more robust representation of the scene by training the student with additional
pseudo labels generated from the teacher. By distilling ray-level pseudo labels
using distinct distillation schemes for reliable and unreliable rays obtained
with our novel reliability estimation method, we enable NeRF to learn a more
accurate and robust geometry of the 3D scene. We show and evaluate that
applying our self-training framework to existing models improves the quality of
the rendered images and achieves state-of-the-art performance in multiple
settings.";Jaewoo Jung<author:sep>Jisang Han<author:sep>Jiwon Kang<author:sep>Seongchan Kim<author:sep>Min-Seop Kwak<author:sep>Seungryong Kim;http://arxiv.org/pdf/2312.01003v2;cs.CV;"34 pages, 21 figures Our project page can be found at :
  https://ku-cvlab.github.io/SE-NeRF/";nerf
2312.02202v1;http://arxiv.org/abs/2312.02202v1;2023-12-02;Volumetric Rendering with Baked Quadrature Fields;"We propose a novel Neural Radiance Field (NeRF) representation for non-opaque
scenes that allows fast inference by utilizing textured polygons. Despite the
high-quality novel view rendering that NeRF provides, a critical limitation is
that it relies on volume rendering that can be computationally expensive and
does not utilize the advancements in modern graphics hardware. Existing methods
for this problem fall short when it comes to modelling volumetric effects as
they rely purely on surface rendering. We thus propose to model the scene with
polygons, which can then be used to obtain the quadrature points required to
model volumetric effects, and also their opacity and colour from the texture.
To obtain such polygonal mesh, we train a specialized field whose
zero-crossings would correspond to the quadrature points when volume rendering,
and perform marching cubes on this field. We then rasterize the polygons and
utilize the fragment shaders to obtain the final colour image. Our method
allows rendering on various devices and easy integration with existing graphics
frameworks while keeping the benefits of volume rendering alive.";Gopal Sharma<author:sep>Daniel Rebain<author:sep>Kwang Moo Yi<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2312.02202v1;cs.GR;;nerf
2312.02189v1;http://arxiv.org/abs/2312.02189v1;2023-12-02;StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D;"In the realm of text-to-3D generation, utilizing 2D diffusion models through
score distillation sampling (SDS) frequently leads to issues such as blurred
appearances and multi-faced geometry, primarily due to the intrinsically noisy
nature of the SDS loss. Our analysis identifies the core of these challenges as
the interaction among noise levels in the 2D diffusion process, the
architecture of the diffusion network, and the 3D model representation. To
overcome these limitations, we present StableDreamer, a methodology
incorporating three advances. First, inspired by InstructNeRF2NeRF, we
formalize the equivalence of the SDS generative prior and a simple supervised
L2 reconstruction loss. This finding provides a novel tool to debug SDS, which
we use to show the impact of time-annealing noise levels on reducing
multi-faced geometries. Second, our analysis shows that while image-space
diffusion contributes to geometric precision, latent-space diffusion is crucial
for vivid color rendition. Based on this observation, StableDreamer introduces
a two-stage training strategy that effectively combines these aspects,
resulting in high-fidelity 3D models. Third, we adopt an anisotropic 3D
Gaussians representation, replacing Neural Radiance Fields (NeRFs), to enhance
the overall quality, reduce memory usage during training, and accelerate
rendering speeds, and better capture semi-transparent objects. StableDreamer
reduces multi-face geometries, generates fine details, and converges stably.";Pengsheng Guo<author:sep>Hans Hao<author:sep>Adam Caccavale<author:sep>Zhongzheng Ren<author:sep>Edward Zhang<author:sep>Qi Shan<author:sep>Aditya Sankar<author:sep>Alexander G. Schwing<author:sep>Alex Colburn<author:sep>Fangchang Ma;http://arxiv.org/pdf/2312.02189v1;cs.CV;;nerf
2401.05345v1;http://arxiv.org/abs/2401.05345v1;2023-12-01;DISTWAR: Fast Differentiable Rendering on Raster-based Rendering  Pipelines;"Differentiable rendering is a technique used in an important emerging class
of visual computing applications that involves representing a 3D scene as a
model that is trained from 2D images using gradient descent. Recent works (e.g.
3D Gaussian Splatting) use a rasterization pipeline to enable rendering high
quality photo-realistic imagery at high speeds from these learned 3D models.
These methods have been demonstrated to be very promising, providing
state-of-art quality for many important tasks. However, training a model to
represent a scene is still a time-consuming task even when using powerful GPUs.
In this work, we observe that the gradient computation phase during training is
a significant bottleneck on GPUs due to the large number of atomic operations
that need to be processed. These atomic operations overwhelm atomic units in
the L2 partitions causing stalls. To address this challenge, we leverage the
observations that during the gradient computation: (1) for most warps, all
threads atomically update the same memory locations; and (2) warps generate
varying amounts of atomic traffic (since some threads may be inactive). We
propose DISTWAR, a software-approach to accelerate atomic operations based on
two key ideas: First, we enable warp-level reduction of threads at the SM
sub-cores using registers to leverage the locality in intra-warp atomic
updates. Second, we distribute the atomic computation between the warp-level
reduction at the SM and the L2 atomic units to increase the throughput of
atomic computation. Warps with many threads performing atomic updates to the
same memory locations are scheduled at the SM, and the rest using L2 atomic
units. We implement DISTWAR using existing warp-level primitives. We evaluate
DISTWAR on widely used raster-based differentiable rendering workloads. We
demonstrate significant speedups of 2.44x on average (up to 5.7x).";Sankeerth Durvasula<author:sep>Adrian Zhao<author:sep>Fan Chen<author:sep>Ruofan Liang<author:sep>Pawan Kumar Sanjaya<author:sep>Nandita Vijaykumar;http://arxiv.org/pdf/2401.05345v1;cs.CV;;gaussian splatting
2312.00860v1;http://arxiv.org/abs/2312.00860v1;2023-12-01;Segment Any 3D Gaussians;"Interactive 3D segmentation in radiance fields is an appealing task since its
importance in 3D scene understanding and manipulation. However, existing
methods face challenges in either achieving fine-grained, multi-granularity
segmentation or contending with substantial computational overhead, inhibiting
real-time interaction. In this paper, we introduce Segment Any 3D GAussians
(SAGA), a novel 3D interactive segmentation approach that seamlessly blends a
2D segmentation foundation model with 3D Gaussian Splatting (3DGS), a recent
breakthrough of radiance fields. SAGA efficiently embeds multi-granularity 2D
segmentation results generated by the segmentation foundation model into 3D
Gaussian point features through well-designed contrastive training. Evaluation
on existing benchmarks demonstrates that SAGA can achieve competitive
performance with state-of-the-art methods. Moreover, SAGA achieves
multi-granularity segmentation and accommodates various prompts, including
points, scribbles, and 2D masks. Notably, SAGA can finish the 3D segmentation
within milliseconds, achieving nearly 1000x acceleration compared to previous
SOTA. The project page is at https://jumpat.github.io/SAGA.";Jiazhong Cen<author:sep>Jiemin Fang<author:sep>Chen Yang<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wei Shen<author:sep>Qi Tian;http://arxiv.org/pdf/2312.00860v1;cs.CV;Work in progress. Project page: https://jumpat.github.io/SAGA;gaussian splatting
2312.00846v1;http://arxiv.org/abs/2312.00846v1;2023-12-01;NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting  Guidance;"Existing neural implicit surface reconstruction methods have achieved
impressive performance in multi-view 3D reconstruction by leveraging explicit
geometry priors such as depth maps or point clouds as regularization. However,
the reconstruction results still lack fine details because of the over-smoothed
depth map or sparse point cloud. In this work, we propose a neural implicit
surface reconstruction pipeline with guidance from 3D Gaussian Splatting to
recover highly detailed surfaces. The advantage of 3D Gaussian Splatting is
that it can generate dense point clouds with detailed structure. Nonetheless, a
naive adoption of 3D Gaussian Splatting can fail since the generated points are
the centers of 3D Gaussians that do not necessarily lie on the surface. We thus
introduce a scale regularizer to pull the centers close to the surface by
enforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine
the point cloud from 3D Gaussians Splatting with the normal priors from the
surface predicted by neural implicit models instead of using a fixed set of
points as guidance. Consequently, the quality of surface reconstruction
improves from the guidance of the more accurate 3D Gaussian splatting. By
jointly optimizing the 3D Gaussian Splatting and the neural implicit model, our
approach benefits from both representations and generates complete surfaces
with intricate details. Experiments on Tanks and Temples verify the
effectiveness of our proposed method.";Hanlin Chen<author:sep>Chen Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2312.00846v1;cs.CV;;gaussian splatting
2312.00732v1;http://arxiv.org/abs/2312.00732v1;2023-12-01;Gaussian Grouping: Segment and Edit Anything in 3D Scenes;"The recent Gaussian Splatting achieves high-quality and real-time novel-view
synthesis of the 3D scenes. However, it is solely concentrated on the
appearance and geometry modeling, while lacking in fine-grained object-level
scene understanding. To address this issue, we propose Gaussian Grouping, which
extends Gaussian Splatting to jointly reconstruct and segment anything in
open-world 3D scenes. We augment each Gaussian with a compact Identity
Encoding, allowing the Gaussians to be grouped according to their object
instance or stuff membership in the 3D scene. Instead of resorting to expensive
3D labels, we supervise the Identity Encodings during the differentiable
rendering by leveraging the 2D mask predictions by SAM, along with introduced
3D spatial consistency regularization. Comparing to the implicit NeRF
representation, we show that the discrete and grouped 3D Gaussians can
reconstruct, segment and edit anything in 3D with high visual quality, fine
granularity and efficiency. Based on Gaussian Grouping, we further propose a
local Gaussian Editing scheme, which shows efficacy in versatile scene editing
applications, including 3D object removal, inpainting, colorization and scene
recomposition. Our code and models will be at
https://github.com/lkeab/gaussian-grouping.";Mingqiao Ye<author:sep>Martin Danelljan<author:sep>Fisher Yu<author:sep>Lei Ke;http://arxiv.org/pdf/2312.00732v1;cs.CV;"We propose Gaussian Grouping, which extends Gaussian Splatting to
  fine-grained open-world 3D scene understanding. Github:
  https://github.com/lkeab/gaussian-grouping";gaussian splatting<tag:sep>nerf
2312.00451v1;http://arxiv.org/abs/2312.00451v1;2023-12-01;FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting;"Novel view synthesis from limited observations remains an important and
persistent task. However, high efficiency in existing NeRF-based few-shot view
synthesis is often compromised to obtain an accurate 3D representation. To
address this challenge, we propose a few-shot view synthesis framework based on
3D Gaussian Splatting that enables real-time and photo-realistic view synthesis
with as few as three training views. The proposed method, dubbed FSGS, handles
the extremely sparse initialized SfM points with a thoughtfully designed
Gaussian Unpooling process. Our method iteratively distributes new Gaussians
around the most representative locations, subsequently infilling local details
in vacant areas. We also integrate a large-scale pre-trained monocular depth
estimator within the Gaussians optimization process, leveraging online
augmented views to guide the geometric optimization towards an optimal
solution. Starting from sparse points observed from limited input viewpoints,
our FSGS can accurately grow into unseen regions, comprehensively covering the
scene and boosting the rendering quality of novel views. Overall, FSGS achieves
state-of-the-art performance in both accuracy and rendering efficiency across
diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website:
https://zehaozhu.github.io/FSGS/.";Zehao Zhu<author:sep>Zhiwen Fan<author:sep>Yifan Jiang<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2312.00451v1;cs.CV;Project page: https://zehaozhu.github.io/FSGS/;gaussian splatting<tag:sep>nerf
2311.18311v1;http://arxiv.org/abs/2311.18311v1;2023-11-30;Anisotropic Neural Representation Learning for High-Quality Neural  Rendering;"Neural radiance fields (NeRFs) have achieved impressive view synthesis
results by learning an implicit volumetric representation from multi-view
images. To project the implicit representation into an image, NeRF employs
volume rendering that approximates the continuous integrals of rays as an
accumulation of the colors and densities of the sampled points. Although this
approximation enables efficient rendering, it ignores the direction information
in point intervals, resulting in ambiguous features and limited reconstruction
quality. In this paper, we propose an anisotropic neural representation
learning method that utilizes learnable view-dependent features to improve
scene representation and reconstruction. We model the volumetric function as
spherical harmonic (SH)-guided anisotropic features, parameterized by
multilayer perceptrons, facilitating ambiguity elimination while preserving the
rendering efficiency. To achieve robust scene reconstruction without anisotropy
overfitting, we regularize the energy of the anisotropic features during
training. Our method is flexiable and can be plugged into NeRF-based
frameworks. Extensive experiments show that the proposed representation can
boost the rendering quality of various NeRFs and achieve state-of-the-art
rendering performance on both synthetic and real-world scenes.";Y. Wang<author:sep>J. Xu<author:sep>Y. Zeng<author:sep>Y. Gong;http://arxiv.org/pdf/2311.18311v1;cs.CV;;nerf
2311.18608v1;http://arxiv.org/abs/2311.18608v1;2023-11-30;Contrastive Denoising Score for Text-guided Latent Diffusion Image  Editing;"With the remarkable advent of text-to-image diffusion models, image editing
methods have become more diverse and continue to evolve. A promising recent
approach in this realm is Delta Denoising Score (DDS) - an image editing
technique based on Score Distillation Sampling (SDS) framework that leverages
the rich generative prior of text-to-image diffusion models. However, relying
solely on the difference between scoring functions is insufficient for
preserving specific structural elements from the original image, a crucial
aspect of image editing. Inspired by the similarity and importance differences
between DDS and the contrastive learning for unpaired image-to-image
translation (CUT), here we present an embarrassingly simple yet very powerful
modification of DDS, called Contrastive Denoising Score (CDS), for latent
diffusion models (LDM). Specifically, to enforce structural correspondence
between the input and output while maintaining the controllability of contents,
we introduce a straightforward approach to regulate structural consistency
using CUT loss within the DDS framework. To calculate this loss, instead of
employing auxiliary networks, we utilize the intermediate features of LDM, in
particular, those from the self-attention layers, which possesses rich spatial
information. Our approach enables zero-shot image-to-image translation and
neural radiance field (NeRF) editing, achieving a well-balanced interplay
between maintaining the structural details and transforming content.
Qualitative results and comparisons demonstrates the effectiveness of our
proposed method. Project page with code is available at
https://hyelinnam.github.io/CDS/.";Hyelin Nam<author:sep>Gihyun Kwon<author:sep>Geon Yeong Park<author:sep>Jong Chul Ye;http://arxiv.org/pdf/2311.18608v1;cs.CV;Project page: https://hyelinnam.github.io/CDS/;nerf
2311.18491v1;http://arxiv.org/abs/2311.18491v1;2023-11-30;ZeST-NeRF: Using temporal aggregation for Zero-Shot Temporal NeRFs;"In the field of media production, video editing techniques play a pivotal
role. Recent approaches have had great success at performing novel view image
synthesis of static scenes. But adding temporal information adds an extra layer
of complexity. Previous models have focused on implicitly representing static
and dynamic scenes using NeRF. These models achieve impressive results but are
costly at training and inference time. They overfit an MLP to describe the
scene implicitly as a function of position. This paper proposes ZeST-NeRF, a
new approach that can produce temporal NeRFs for new scenes without retraining.
We can accurately reconstruct novel views using multi-view synthesis techniques
and scene flow-field estimation, trained only with unrelated scenes. We
demonstrate how existing state-of-the-art approaches from a range of fields
cannot adequately solve this new task and demonstrate the efficacy of our
solution. The resulting network improves quantitatively by 15% and produces
significantly better visual results.";Violeta Menéndez González<author:sep>Andrew Gilbert<author:sep>Graeme Phillipson<author:sep>Stephen Jolly<author:sep>Simon Hadfield;http://arxiv.org/pdf/2311.18491v1;cs.CV;VUA BMVC 2023;nerf
2311.18288v1;http://arxiv.org/abs/2311.18288v1;2023-11-30;CosAvatar: Consistent and Animatable Portrait Video Tuning with Text  Prompt;"Recently, text-guided digital portrait editing has attracted more and more
attentions. However, existing methods still struggle to maintain consistency
across time, expression, and view or require specific data prerequisites. To
solve these challenging problems, we propose CosAvatar, a high-quality and
user-friendly framework for portrait tuning. With only monocular video and text
instructions as input, we can produce animatable portraits with both temporal
and 3D consistency. Different from methods that directly edit in the 2D domain,
we employ a dynamic NeRF-based 3D portrait representation to model both the
head and torso. We alternate between editing the video frames' dataset and
updating the underlying 3D portrait until the edited frames reach 3D
consistency. Additionally, we integrate the semantic portrait priors to enhance
the edited results, allowing precise modifications in specified semantic areas.
Extensive results demonstrate that our proposed method can not only accurately
edit portrait styles or local attributes based on text instructions but also
support expressive animation driven by a source video.";Haiyao Xiao<author:sep>Chenglai Zhong<author:sep>Xuan Gao<author:sep>Yudong Guo<author:sep>Juyong Zhang;http://arxiv.org/pdf/2311.18288v1;cs.CV;Project page: https://ustc3dv.github.io/CosAvatar/;nerf
2312.00112v1;http://arxiv.org/abs/2312.00112v1;2023-11-30;DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis  with 3D Gaussian Splatting;"Accurately and efficiently modeling dynamic scenes and motions is considered
so challenging a task due to temporal dynamics and motion complexity. To
address these challenges, we propose DynMF, a compact and efficient
representation that decomposes a dynamic scene into a few neural trajectories.
We argue that the per-point motions of a dynamic scene can be decomposed into a
small set of explicit or learned trajectories. Our carefully designed neural
framework consisting of a tiny set of learned basis queried only in time allows
for rendering speed similar to 3D Gaussian Splatting, surpassing 120 FPS, while
at the same time, requiring only double the storage compared to static scenes.
Our neural representation adequately constrains the inherently underconstrained
motion field of a dynamic scene leading to effective and fast optimization.
This is done by biding each point to motion coefficients that enforce the
per-point sharing of basis trajectories. By carefully applying a sparsity loss
to the motion coefficients, we are able to disentangle the motions that
comprise the scene, independently control them, and generate novel motion
combinations that have never been seen before. We can reach state-of-the-art
render quality within just 5 minutes of training and in less than half an hour,
we can synthesize novel views of dynamic scenes with superior photorealistic
quality. Our representation is interpretable, efficient, and expressive enough
to offer real-time view synthesis of complex dynamic scene motions, in
monocular and multi-view scenarios.";Agelos Kratimenos<author:sep>Jiahui Lei<author:sep>Kostas Daniilidis;http://arxiv.org/pdf/2312.00112v1;cs.CV;Project page: https://agelosk.github.io/dynmf/;gaussian splatting
2312.00206v1;http://arxiv.org/abs/2312.00206v1;2023-11-30;SparseGS: Real-Time 360° Sparse View Synthesis using Gaussian  Splatting;"The problem of novel view synthesis has grown significantly in popularity
recently with the introduction of Neural Radiance Fields (NeRFs) and other
implicit scene representation methods. A recent advance, 3D Gaussian Splatting
(3DGS), leverages an explicit representation to achieve real-time rendering
with high-quality results. However, 3DGS still requires an abundance of
training views to generate a coherent scene representation. In few shot
settings, similar to NeRF, 3DGS tends to overfit to training views, causing
background collapse and excessive floaters, especially as the number of
training views are reduced. We propose a method to enable training coherent
3DGS-based radiance fields of 360 scenes from sparse training views. We find
that using naive depth priors is not sufficient and integrate depth priors with
generative and explicit constraints to reduce background collapse, remove
floaters, and enhance consistency from unseen viewpoints. Experiments show that
our method outperforms base 3DGS by up to 30.5% and NeRF-based methods by up to
15.6% in LPIPS on the MipNeRF-360 dataset with substantially less training and
inference cost.";Haolin Xiong<author:sep>Sairisheek Muttukuru<author:sep>Rishi Upadhyay<author:sep>Pradyumna Chari<author:sep>Achuta Kadambi;http://arxiv.org/pdf/2312.00206v1;cs.CV;"The main text spans eight pages, followed by two pages of references
  and four pages of supplementary materials";gaussian splatting<tag:sep>nerf
2312.00252v1;http://arxiv.org/abs/2312.00252v1;2023-11-30;PyNeRF: Pyramidal Neural Radiance Fields;"Neural Radiance Fields (NeRFs) can be dramatically accelerated by spatial
grid representations. However, they do not explicitly reason about scale and so
introduce aliasing artifacts when reconstructing scenes captured at different
camera distances. Mip-NeRF and its extensions propose scale-aware renderers
that project volumetric frustums rather than point samples but such approaches
rely on positional encodings that are not readily compatible with grid methods.
We propose a simple modification to grid-based models by training model heads
at different spatial grid resolutions. At render time, we simply use coarser
grids to render samples that cover larger volumes. Our method can be easily
applied to existing accelerated NeRF methods and significantly improves
rendering quality (reducing error rates by 20-90% across synthetic and
unbounded real-world scenes) while incurring minimal performance overhead (as
each model head is quick to evaluate). Compared to Mip-NeRF, we reduce error
rates by 20% while training over 60x faster.";Haithem Turki<author:sep>Michael Zollhöfer<author:sep>Christian Richardt<author:sep>Deva Ramanan;http://arxiv.org/pdf/2312.00252v1;cs.CV;Neurips 2023 Project page: https://haithemturki.com/pynerf/;nerf
2401.06143v1;http://arxiv.org/abs/2401.06143v1;2023-11-30;Redefining Recon: Bridging Gaps with UAVs, 360 degree Cameras, and  Neural Radiance Fields;"In the realm of digital situational awareness during disaster situations,
accurate digital representations, like 3D models, play an indispensable role.
To ensure the safety of rescue teams, robotic platforms are often deployed to
generate these models. In this paper, we introduce an innovative approach that
synergizes the capabilities of compact Unmaned Arial Vehicles (UAVs), smaller
than 30 cm, equipped with 360 degree cameras and the advances of Neural
Radiance Fields (NeRFs). A NeRF, a specialized neural network, can deduce a 3D
representation of any scene using 2D images and then synthesize it from various
angles upon request. This method is especially tailored for urban environments
which have experienced significant destruction, where the structural integrity
of buildings is compromised to the point of barring entry-commonly observed
post-earthquakes and after severe fires. We have tested our approach through
recent post-fire scenario, underlining the efficacy of NeRFs even in
challenging outdoor environments characterized by water, snow, varying light
conditions, and reflective surfaces.";Hartmut Surmann<author:sep>Niklas Digakis<author:sep>Jan-Nicklas Kremer<author:sep>Julien Meine<author:sep>Max Schulte<author:sep>Niklas Voigt;http://arxiv.org/pdf/2401.06143v1;cs.CV;"6 pages, published at IEEE International Symposium on
  Safety,Security,and Rescue Robotics SSRR2023 in FUKUSHIMA, November 13-15
  2023";nerf
2312.00109v1;http://arxiv.org/abs/2312.00109v1;2023-11-30;Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering;"Neural rendering methods have significantly advanced photo-realistic 3D scene
rendering in various academic and industrial applications. The recent 3D
Gaussian Splatting method has achieved the state-of-the-art rendering quality
and speed combining the benefits of both primitive-based representations and
volumetric representations. However, it often leads to heavily redundant
Gaussians that try to fit every training view, neglecting the underlying scene
geometry. Consequently, the resulting model becomes less robust to significant
view changes, texture-less area and lighting effects. We introduce Scaffold-GS,
which uses anchor points to distribute local 3D Gaussians, and predicts their
attributes on-the-fly based on viewing direction and distance within the view
frustum. Anchor growing and pruning strategies are developed based on the
importance of neural Gaussians to reliably improve the scene coverage. We show
that our method effectively reduces redundant Gaussians while delivering
high-quality rendering. We also demonstrates an enhanced capability to
accommodate scenes with varying levels-of-detail and view-dependent
observations, without sacrificing the rendering speed.";Tao Lu<author:sep>Mulin Yu<author:sep>Linning Xu<author:sep>Yuanbo Xiangli<author:sep>Limin Wang<author:sep>Dahua Lin<author:sep>Bo Dai;http://arxiv.org/pdf/2312.00109v1;cs.CV;Project page: https://city-super.github.io/scaffold-gs/;gaussian splatting
2311.18159v1;http://arxiv.org/abs/2311.18159v1;2023-11-30;Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector  Quantization;"3D Gaussian Splatting is a new method for modeling and rendering 3D radiance
fields that achieves much faster learning and rendering time compared to SOTA
NeRF methods. However, it comes with a drawback in the much larger storage
demand compared to NeRF methods since it needs to store the parameters for
several 3D Gaussians. We notice that many Gaussians may share similar
parameters, so we introduce a simple vector quantization method based on
\kmeans algorithm to quantize the Gaussian parameters. Then, we store the small
codebook along with the index of the code for each Gaussian. Moreover, we
compress the indices further by sorting them and using a method similar to
run-length encoding. We do extensive experiments on standard benchmarks as well
as a new benchmark which is an order of magnitude larger than the standard
benchmarks. We show that our simple yet effective method can reduce the storage
cost for the original 3D Gaussian Splatting method by a factor of almost
$20\times$ with a very small drop in the quality of rendered images.";KL Navaneet<author:sep>Kossar Pourahmadi Meibodi<author:sep>Soroush Abbasi Koohpayegani<author:sep>Hamed Pirsiavash;http://arxiv.org/pdf/2311.18159v1;cs.CV;Code is available at https://github.com/UCDvision/compact3d;gaussian splatting<tag:sep>nerf
2311.18561v1;http://arxiv.org/abs/2311.18561v1;2023-11-30;Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and  Real-time Rendering;"Modeling dynamic, large-scale urban scenes is challenging due to their highly
intricate geometric structures and unconstrained dynamics in both space and
time. Prior methods often employ high-level architectural priors, separating
static and dynamic elements, resulting in suboptimal capture of their
synergistic interactions. To address this challenge, we present a unified
representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon
the efficient 3D Gaussian splatting technique, originally designed for static
scene representation, by introducing periodic vibration-based temporal
dynamics. This innovation enables PVG to elegantly and uniformly represent the
characteristics of various objects and elements in dynamic urban scenes. To
enhance temporally coherent representation learning with sparse training data,
we introduce a novel flow-based temporal smoothing mechanism and a
position-aware adaptive control strategy. Extensive experiments on Waymo Open
Dataset and KITTI benchmarks demonstrate that PVG surpasses state-of-the-art
alternatives in both reconstruction and novel view synthesis for both dynamic
and static scenes. Notably, PVG achieves this without relying on manually
labeled object bounding boxes or expensive optical flow estimation. Moreover,
PVG exhibits 50/6000-fold acceleration in training/rendering over the best
alternative.";Yurui Chen<author:sep>Chun Gu<author:sep>Junzhe Jiang<author:sep>Xiatian Zhu<author:sep>Li Zhang;http://arxiv.org/pdf/2311.18561v1;cs.CV;Project page: https://fudan-zvg.github.io/PVG/;gaussian splatting
2312.00583v1;http://arxiv.org/abs/2312.00583v1;2023-11-30;MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly  Deformable Scenes;"Accurate 3D tracking in highly deformable scenes with occlusions and shadows
can facilitate new applications in robotics, augmented reality, and generative
AI. However, tracking under these conditions is extremely challenging due to
the ambiguity that arises with large deformations, shadows, and occlusions. We
introduce MD-Splatting, an approach for simultaneous 3D tracking and novel view
synthesis, using video captures of a dynamic scene from various camera poses.
MD-Splatting builds on recent advances in Gaussian splatting, a method that
learns the properties of a large number of Gaussians for state-of-the-art and
fast novel view synthesis. MD-Splatting learns a deformation function to
project a set of Gaussians with non-metric, thus canonical, properties into
metric space. The deformation function uses a neural-voxel encoding and a
multilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow
scalar. We enforce physics-inspired regularization terms based on local
rigidity, conservation of momentum, and isometry, which leads to trajectories
with smaller trajectory errors. MD-Splatting achieves high-quality 3D tracking
on highly deformable scenes with shadows and occlusions. Compared to
state-of-the-art, we improve 3D tracking by an average of 23.9 %, while
simultaneously achieving high-quality novel view synthesis. With sufficient
texture such as in scene 6, MD-Splatting achieves a median tracking error of
3.39 mm on a cloth of 1 x 1 meters in size. Project website:
https://md-splatting.github.io/.";Bardienus P. Duisterhof<author:sep>Zhao Mandi<author:sep>Yunchao Yao<author:sep>Jia-Wei Liu<author:sep>Mike Zheng Shou<author:sep>Shuran Song<author:sep>Jeffrey Ichnowski;http://arxiv.org/pdf/2312.00583v1;cs.CV;;gaussian splatting
2312.00588v1;http://arxiv.org/abs/2312.00588v1;2023-11-30;LucidDreaming: Controllable Object-Centric 3D Generation;"With the recent development of generative models, Text-to-3D generations have
also seen significant growth. Nonetheless, achieving precise control over 3D
generation continues to be an arduous task, as using text to control often
leads to missing objects and imprecise locations. Contemporary strategies for
enhancing controllability in 3D generation often entail the introduction of
additional parameters, such as customized diffusion models. This often induces
hardness in adapting to different diffusion models or creating distinct
objects.
  In this paper, we present LucidDreaming as an effective pipeline capable of
fine-grained control over 3D generation. It requires only minimal input of 3D
bounding boxes, which can be deduced from a simple text prompt using a Large
Language Model. Specifically, we propose clipped ray sampling to separately
render and optimize objects with user specifications. We also introduce
object-centric density blob bias, fostering the separation of generated
objects. With individual rendering and optimizing of objects, our method excels
not only in controlled content generation from scratch but also within the
pre-trained NeRF scenes. In such scenarios, existing generative approaches
often disrupt the integrity of the original scene, and current editing methods
struggle to synthesize new content in empty spaces. We show that our method
exhibits remarkable adaptability across a spectrum of mainstream Score
Distillation Sampling-based 3D generation frameworks, and achieves superior
alignment of 3D content when compared to baseline approaches. We also provide a
dataset of prompts with 3D bounding boxes, benchmarking 3D spatial
controllability.";Zhaoning Wang<author:sep>Ming Li<author:sep>Chen Chen;http://arxiv.org/pdf/2312.00588v1;cs.CV;;nerf
2311.17590v1;http://arxiv.org/abs/2311.17590v1;2023-11-29;SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis;"Achieving high synchronization in the synthesis of realistic, speech-driven
talking head videos presents a significant challenge. Traditional Generative
Adversarial Networks (GAN) struggle to maintain consistent facial identity,
while Neural Radiance Fields (NeRF) methods, although they can address this
issue, often produce mismatched lip movements, inadequate facial expressions,
and unstable head poses. A lifelike talking head requires synchronized
coordination of subject identity, lip movements, facial expressions, and head
poses. The absence of these synchronizations is a fundamental flaw, leading to
unrealistic and artificial outcomes. To address the critical issue of
synchronization, identified as the ""devil"" in creating realistic talking heads,
we introduce SyncTalk. This NeRF-based method effectively maintains subject
identity, enhancing synchronization and realism in talking head synthesis.
SyncTalk employs a Face-Sync Controller to align lip movements with speech and
innovatively uses a 3D facial blendshape model to capture accurate facial
expressions. Our Head-Sync Stabilizer optimizes head poses, achieving more
natural head movements. The Portrait-Sync Generator restores hair details and
blends the generated head with the torso for a seamless visual experience.
Extensive experiments and user studies demonstrate that SyncTalk outperforms
state-of-the-art methods in synchronization and realism. We recommend watching
the supplementary video: https://ziqiaopeng.github.io/synctalk";Ziqiao Peng<author:sep>Wentao Hu<author:sep>Yue Shi<author:sep>Xiangyu Zhu<author:sep>Xiaomei Zhang<author:sep>Hao Zhao<author:sep>Jun He<author:sep>Hongyan Liu<author:sep>Zhaoxin Fan;http://arxiv.org/pdf/2311.17590v1;cs.CV;11 pages, 5 figures;nerf
2311.17910v1;http://arxiv.org/abs/2311.17910v1;2023-11-29;HUGS: Human Gaussian Splats;"Recent advances in neural rendering have improved both training and rendering
times by orders of magnitude. While these methods demonstrate state-of-the-art
quality and speed, they are designed for photogrammetry of static scenes and do
not generalize well to freely moving humans in the environment. In this work,
we introduce Human Gaussian Splats (HUGS) that represents an animatable human
together with the scene using 3D Gaussian Splatting (3DGS). Our method takes
only a monocular video with a small number of (50-100) frames, and it
automatically learns to disentangle the static scene and a fully animatable
human avatar within 30 minutes. We utilize the SMPL body model to initialize
the human Gaussians. To capture details that are not modeled by SMPL (e.g.
cloth, hairs), we allow the 3D Gaussians to deviate from the human body model.
Utilizing 3D Gaussians for animated humans brings new challenges, including the
artifacts created when articulating the Gaussians. We propose to jointly
optimize the linear blend skinning weights to coordinate the movements of
individual Gaussians during animation. Our approach enables novel-pose
synthesis of human and novel view synthesis of both the human and the scene. We
achieve state-of-the-art rendering quality with a rendering speed of 60 FPS
while being ~100x faster to train over previous work. Our code will be
announced here: https://github.com/apple/ml-hugs";Muhammed Kocabas<author:sep>Jen-Hao Rick Chang<author:sep>James Gabriel<author:sep>Oncel Tuzel<author:sep>Anurag Ranjan;http://arxiv.org/pdf/2311.17910v1;cs.CV;;gaussian splatting
2311.17907v1;http://arxiv.org/abs/2311.17907v1;2023-11-29;CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting;"With the onset of diffusion-based generative models and their ability to
generate text-conditioned images, content generation has received a massive
invigoration. Recently, these models have been shown to provide useful guidance
for the generation of 3D graphics assets. However, existing work in
text-conditioned 3D generation faces fundamental constraints: (i) inability to
generate detailed, multi-object scenes, (ii) inability to textually control
multi-object configurations, and (iii) physically realistic scene composition.
In this work, we propose CG3D, a method for compositionally generating scalable
3D assets that resolves these constraints. We find that explicit Gaussian
radiance fields, parameterized to allow for compositions of objects, possess
the capability to enable semantically and physically consistent scenes. By
utilizing a guidance framework built around this explicit representation, we
show state of the art results, capable of even exceeding the guiding diffusion
model in terms of object combinations and physics accuracy.";Alexander Vilesov<author:sep>Pradyumna Chari<author:sep>Achuta Kadambi;http://arxiv.org/pdf/2311.17907v1;cs.CV;;gaussian splatting
2311.17754v1;http://arxiv.org/abs/2311.17754v1;2023-11-29;Cinematic Behavior Transfer via NeRF-based Differentiable Filming;"In the evolving landscape of digital media and video production, the precise
manipulation and reproduction of visual elements like camera movements and
character actions are highly desired. Existing SLAM methods face limitations in
dynamic scenes and human pose estimation often focuses on 2D projections,
neglecting 3D statuses. To address these issues, we first introduce a reverse
filming behavior estimation technique. It optimizes camera trajectories by
leveraging NeRF as a differentiable renderer and refining SMPL tracks. We then
introduce a cinematic transfer pipeline that is able to transfer various shot
types to a new 2D video or a 3D virtual environment. The incorporation of 3D
engine workflow enables superior rendering and control abilities, which also
achieves a higher rating in the user study.";Xuekun Jiang<author:sep>Anyi Rao<author:sep>Jingbo Wang<author:sep>Dahua Lin<author:sep>Bo Dai;http://arxiv.org/pdf/2311.17754v1;cs.CV;"Project Page:
  https://virtualfilmstudio.github.io/projects/cinetransfer";nerf
2311.17917v1;http://arxiv.org/abs/2311.17917v1;2023-11-29;AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text;"We study the problem of creating high-fidelity and animatable 3D avatars from
only textual descriptions. Existing text-to-avatar methods are either limited
to static avatars which cannot be animated or struggle to generate animatable
avatars with promising quality and precise pose control. To address these
limitations, we propose AvatarStudio, a coarse-to-fine generative model that
generates explicit textured 3D meshes for animatable human avatars.
Specifically, AvatarStudio begins with a low-resolution NeRF-based
representation for coarse generation, followed by incorporating SMPL-guided
articulation into the explicit mesh representation to support avatar animation
and high resolution rendering. To ensure view consistency and pose
controllability of the resulting avatars, we introduce a 2D diffusion model
conditioned on DensePose for Score Distillation Sampling supervision. By
effectively leveraging the synergy between the articulated mesh representation
and the DensePose-conditional diffusion model, AvatarStudio can create
high-quality avatars from text that are ready for animation, significantly
outperforming previous methods. Moreover, it is competent for many
applications, e.g., multimodal avatar animations and style-guided avatar
creation. For more results, please refer to our project page:
http://jeff95.me/projects/avatarstudio.html";Jianfeng Zhang<author:sep>Xuanmeng Zhang<author:sep>Huichao Zhang<author:sep>Jun Hao Liew<author:sep>Chenxu Zhang<author:sep>Yi Yang<author:sep>Jiashi Feng;http://arxiv.org/pdf/2311.17917v1;cs.GR;Project page at http://jeff95.me/projects/avatarstudio.html;nerf
2311.17874v1;http://arxiv.org/abs/2311.17874v1;2023-11-29;FisherRF: Active View Selection and Uncertainty Quantification for  Radiance Fields using Fisher Information;"This study addresses the challenging problem of active view selection and
uncertainty quantification within the domain of Radiance Fields. Neural
Radiance Fields (NeRF) have greatly advanced image rendering and
reconstruction, but the limited availability of 2D images poses uncertainties
stemming from occlusions, depth ambiguities, and imaging errors. Efficiently
selecting informative views becomes crucial, and quantifying NeRF model
uncertainty presents intricate challenges. Existing approaches either depend on
model architecture or are based on assumptions regarding density distributions
that are not generally applicable. By leveraging Fisher Information, we
efficiently quantify observed information within Radiance Fields without ground
truth data. This can be used for the next best view selection and pixel-wise
uncertainty quantification. Our method overcomes existing limitations on model
architecture and effectiveness, achieving state-of-the-art results in both view
selection and uncertainty quantification, demonstrating its potential to
advance the field of Radiance Fields. Our method with the 3D Gaussian Splatting
backend could perform view selections at 70 fps.";Wen Jiang<author:sep>Boshu Lei<author:sep>Kostas Daniilidis;http://arxiv.org/pdf/2311.17874v1;cs.CV;Project page: https://jiangwenpl.github.io/FisherRF/;gaussian splatting<tag:sep>nerf
2311.17977v1;http://arxiv.org/abs/2311.17977v1;2023-11-29;GaussianShader: 3D Gaussian Splatting with Shading Functions for  Reflective Surfaces;"The advent of neural 3D Gaussians has recently brought about a revolution in
the field of neural rendering, facilitating the generation of high-quality
renderings at real-time speeds. However, the explicit and discrete
representation encounters challenges when applied to scenes featuring
reflective surfaces. In this paper, we present GaussianShader, a novel method
that applies a simplified shading function on 3D Gaussians to enhance the
neural rendering in scenes with reflective surfaces while preserving the
training and rendering efficiency. The main challenge in applying the shading
function lies in the accurate normal estimation on discrete 3D Gaussians.
Specifically, we proposed a novel normal estimation framework based on the
shortest axis directions of 3D Gaussians with a delicately designed loss to
make the consistency between the normals and the geometries of Gaussian
spheres. Experiments show that GaussianShader strikes a commendable balance
between efficiency and visual quality. Our method surpasses Gaussian Splatting
in PSNR on specular object datasets, exhibiting an improvement of 1.57dB. When
compared to prior works handling reflective surfaces, such as Ref-NeRF, our
optimization time is significantly accelerated (23h vs. 0.58h). Please click on
our project website to see more results.";Yingwenqi Jiang<author:sep>Jiadong Tu<author:sep>Yuan Liu<author:sep>Xifeng Gao<author:sep>Xiaoxiao Long<author:sep>Wenping Wang<author:sep>Yuexin Ma;http://arxiv.org/pdf/2311.17977v1;cs.CV;13 pages, 11 figures, refrences added;gaussian splatting<tag:sep>nerf
2311.17332v1;http://arxiv.org/abs/2311.17332v1;2023-11-29;NeRFTAP: Enhancing Transferability of Adversarial Patches on Face  Recognition using Neural Radiance Fields;"Face recognition (FR) technology plays a crucial role in various
applications, but its vulnerability to adversarial attacks poses significant
security concerns. Existing research primarily focuses on transferability to
different FR models, overlooking the direct transferability to victim's face
images, which is a practical threat in real-world scenarios. In this study, we
propose a novel adversarial attack method that considers both the
transferability to the FR model and the victim's face image, called NeRFTAP.
Leveraging NeRF-based 3D-GAN, we generate new view face images for the source
and target subjects to enhance transferability of adversarial patches. We
introduce a style consistency loss to ensure the visual similarity between the
adversarial UV map and the target UV map under a 0-1 mask, enhancing the
effectiveness and naturalness of the generated adversarial face images.
Extensive experiments and evaluations on various FR models demonstrate the
superiority of our approach over existing attack techniques. Our work provides
valuable insights for enhancing the robustness of FR systems in practical
adversarial settings.";Xiaoliang Liu<author:sep>Furao Shen<author:sep>Feng Han<author:sep>Jian Zhao<author:sep>Changhai Nie;http://arxiv.org/pdf/2311.17332v1;cs.CV;;nerf
2311.16504v1;http://arxiv.org/abs/2311.16504v1;2023-11-28;Rethinking Directional Integration in Neural Radiance Fields;"Recent works use the Neural radiance field (NeRF) to perform multi-view 3D
reconstruction, providing a significant leap in rendering photorealistic
scenes. However, despite its efficacy, NeRF exhibits limited capability of
learning view-dependent effects compared to light field rendering or
image-based view synthesis. To that end, we introduce a modification to the
NeRF rendering equation which is as simple as a few lines of code change for
any NeRF variations, while greatly improving the rendering quality of
view-dependent effects. By swapping the integration operator and the direction
decoder network, we only integrate the positional features along the ray and
move the directional terms out of the integration, resulting in a
disentanglement of the view-dependent and independent components. The modified
equation is equivalent to the classical volumetric rendering in ideal cases on
object surfaces with Dirac densities. Furthermore, we prove that with the
errors caused by network approximation and numerical integration, our rendering
equation exhibits better convergence properties with lower error accumulations
compared to the classical NeRF. We also show that the modified equation can be
interpreted as light field rendering with learned ray embeddings. Experiments
on different NeRF variations show consistent improvements in the quality of
view-dependent effects with our simple modification.";Congyue Deng<author:sep>Jiawei Yang<author:sep>Leonidas Guibas<author:sep>Yue Wang;http://arxiv.org/pdf/2311.16504v1;cs.CV;;nerf
2311.16854v2;http://arxiv.org/abs/2311.16854v2;2023-11-28;A Unified Approach for Text- and Image-guided 4D Scene Generation;"Large-scale diffusion generative models are greatly simplifying image, video
and 3D asset creation from user-provided text prompts and images. However, the
challenging problem of text-to-4D dynamic 3D scene generation with diffusion
guidance remains largely unexplored. We propose Dream-in-4D, which features a
novel two-stage approach for text-to-4D synthesis, leveraging (1) 3D and 2D
diffusion guidance to effectively learn a high-quality static 3D asset in the
first stage; (2) a deformable neural radiance field that explicitly
disentangles the learned static asset from its deformation, preserving quality
during motion learning; and (3) a multi-resolution feature grid for the
deformation field with a displacement total variation loss to effectively learn
motion with video diffusion guidance in the second stage. Through a user
preference study, we demonstrate that our approach significantly advances image
and motion quality, 3D consistency and text fidelity for text-to-4D generation
compared to baseline approaches. Thanks to its motion-disentangled
representation, Dream-in-4D can also be easily adapted for controllable
generation where appearance is defined by one or multiple images, without the
need to modify the motion learning stage. Thus, our method offers, for the
first time, a unified approach for text-to-4D, image-to-4D and personalized 4D
generation tasks.";Yufeng Zheng<author:sep>Xueting Li<author:sep>Koki Nagano<author:sep>Sifei Liu<author:sep>Karsten Kreis<author:sep>Otmar Hilliges<author:sep>Shalini De Mello;http://arxiv.org/pdf/2311.16854v2;cs.CV;Project page: https://research.nvidia.com/labs/nxp/dream-in-4d/;
2311.16737v1;http://arxiv.org/abs/2311.16737v1;2023-11-28;Point'n Move: Interactive Scene Object Manipulation on Gaussian  Splatting Radiance Fields;"We propose Point'n Move, a method that achieves interactive scene object
manipulation with exposed region inpainting. Interactivity here further comes
from intuitive object selection and real-time editing. To achieve this, we
adopt Gaussian Splatting Radiance Field as the scene representation and fully
leverage its explicit nature and speed advantage. Its explicit representation
formulation allows us to devise a 2D prompt points to 3D mask dual-stage
self-prompting segmentation algorithm, perform mask refinement and merging,
minimize change as well as provide good initialization for scene inpainting and
perform editing in real-time without per-editing training, all leads to
superior quality and performance. We test our method by performing editing on
both forward-facing and 360 scenes. We also compare our method against existing
scene object removal methods, showing superior quality despite being more
capable and having a speed advantage.";Jiajun Huang<author:sep>Hongchuan Yu;http://arxiv.org/pdf/2311.16737v1;cs.CV;;gaussian splatting
2311.17119v1;http://arxiv.org/abs/2311.17119v1;2023-11-28;Continuous Pose for Monocular Cameras in Neural Implicit Representation;"In this paper, we showcase the effectiveness of optimizing monocular camera
poses as a continuous function of time. The camera poses are represented using
an implicit neural function which maps the given time to the corresponding
camera pose. The mapped camera poses are then used for the downstream tasks
where joint camera pose optimization is also required. While doing so, the
network parameters -- that implicitly represent camera poses -- are optimized.
We exploit the proposed method in four diverse experimental settings, namely,
(1) NeRF from noisy poses; (2) NeRF from asynchronous Events; (3) Visual
Simultaneous Localization and Mapping (vSLAM); and (4) vSLAM with IMUs. In all
four settings, the proposed method performs significantly better than the
compared baselines and the state-of-the-art methods. Additionally, using the
assumption of continuous motion, changes in pose may actually live in a
manifold that has lower than 6 degrees of freedom (DOF) is also realized. We
call this low DOF motion representation as the \emph{intrinsic motion} and use
the approach in vSLAM settings, showing impressive camera tracking performance.";Qi Ma<author:sep>Danda Pani Paudel<author:sep>Ajad Chhatkuli<author:sep>Luc Van Gool;http://arxiv.org/pdf/2311.17119v1;cs.CV;;nerf
2311.17061v1;http://arxiv.org/abs/2311.17061v1;2023-11-28;HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting;"Realistic 3D human generation from text prompts is a desirable yet
challenging task. Existing methods optimize 3D representations like mesh or
neural fields via score distillation sampling (SDS), which suffers from
inadequate fine details or excessive training time. In this paper, we propose
an efficient yet effective framework, HumanGaussian, that generates
high-quality 3D humans with fine-grained geometry and realistic appearance. Our
key insight is that 3D Gaussian Splatting is an efficient renderer with
periodic Gaussian shrinkage or growing, where such adaptive density control can
be naturally guided by intrinsic human structures. Specifically, 1) we first
propose a Structure-Aware SDS that simultaneously optimizes human appearance
and geometry. The multi-modal score function from both RGB and depth space is
leveraged to distill the Gaussian densification and pruning process. 2)
Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS
into a noisier generative score and a cleaner classifier score, which well
addresses the over-saturation issue. The floating artifacts are further
eliminated based on Gaussian size in a prune-only phase to enhance generation
smoothness. Extensive experiments demonstrate the superior efficiency and
competitive quality of our framework, rendering vivid 3D humans under diverse
scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian";Xian Liu<author:sep>Xiaohang Zhan<author:sep>Jiaxiang Tang<author:sep>Ying Shan<author:sep>Gang Zeng<author:sep>Dahua Lin<author:sep>Xihui Liu<author:sep>Ziwei Liu;http://arxiv.org/pdf/2311.17061v1;cs.CV;Project Page: https://alvinliu0.github.io/projects/HumanGaussian;gaussian splatting
2311.17089v1;http://arxiv.org/abs/2311.17089v1;2023-11-28;Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering;"3D Gaussians have recently emerged as a highly efficient representation for
3D reconstruction and rendering. Despite its high rendering quality and speed
at high resolutions, they both deteriorate drastically when rendered at lower
resolutions or from far away camera position. During low resolution or far away
rendering, the pixel size of the image can fall below the Nyquist frequency
compared to the screen size of each splatted 3D Gaussian and leads to aliasing
effect. The rendering is also drastically slowed down by the sequential alpha
blending of more splatted Gaussians per pixel. To address these issues, we
propose a multi-scale 3D Gaussian splatting algorithm, which maintains
Gaussians at different scales to represent the same scene. Higher-resolution
images are rendered with more small Gaussians, and lower-resolution images are
rendered with fewer larger Gaussians. With similar training time, our algorithm
can achieve 13\%-66\% PSNR and 160\%-2400\% rendering speed improvement at
4$\times$-128$\times$ scale rendering on Mip-NeRF360 dataset compared to the
single scale 3D Gaussian splatting.";Zhiwen Yan<author:sep>Weng Fei Low<author:sep>Yu Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2311.17089v1;cs.CV;;gaussian splatting<tag:sep>nerf
2311.16671v1;http://arxiv.org/abs/2311.16671v1;2023-11-28;SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry,  Illumination, and Material Estimation;"We present a novel approach for digitizing real-world objects by estimating
their geometry, material properties, and environmental lighting from a set of
posed images with fixed lighting. Our method incorporates into Neural Radiance
Field (NeRF) pipelines the split sum approximation used with image-based
lighting for real-time physical-based rendering. We propose modeling the
scene's lighting with a single scene-specific MLP representing pre-integrated
image-based lighting at arbitrary resolutions. We achieve accurate modeling of
pre-integrated lighting by exploiting a novel regularizer based on efficient
Monte Carlo sampling. Additionally, we propose a new method of supervising
self-occlusion predictions by exploiting a similar regularizer based on Monte
Carlo sampling. Experimental results demonstrate the efficiency and
effectiveness of our approach in estimating scene geometry, material
properties, and lighting. Our method is capable of attaining state-of-the-art
relighting quality after only ${\sim}1$ hour of training in a single NVIDIA
A100 GPU.";Jesus Zarzar<author:sep>Bernard Ghanem;http://arxiv.org/pdf/2311.16671v1;cs.CV;;nerf
2311.16592v1;http://arxiv.org/abs/2311.16592v1;2023-11-28;RGBGrasp: Image-based Object Grasping by Capturing Multiple Views during  Robot Arm Movement with Neural Radiance Fields;"Robotic research encounters a significant hurdle when it comes to the
intricate task of grasping objects that come in various shapes, materials, and
textures. Unlike many prior investigations that heavily leaned on specialized
point-cloud cameras or abundant RGB visual data to gather 3D insights for
object-grasping missions, this paper introduces a pioneering approach called
RGBGrasp. This method depends on a limited set of RGB views to perceive the 3D
surroundings containing transparent and specular objects and achieve accurate
grasping. Our method utilizes pre-trained depth prediction models to establish
geometry constraints, enabling precise 3D structure estimation, even under
limited view conditions. Finally, we integrate hash encoding and a proposal
sampler strategy to significantly accelerate the 3D reconstruction process.
These innovations significantly enhance the adaptability and effectiveness of
our algorithm in real-world scenarios. Through comprehensive experimental
validation, we demonstrate that RGBGrasp achieves remarkable success across a
wide spectrum of object-grasping scenarios, establishing it as a promising
solution for real-world robotic manipulation tasks. The demo of our method can
be found on: https://sites.google.com/view/rgbgrasp";Chang Liu<author:sep>Kejian Shi<author:sep>Kaichen Zhou<author:sep>Haoxiao Wang<author:sep>Jiyao Zhang<author:sep>Hao Dong;http://arxiv.org/pdf/2311.16592v1;cs.RO;;
2311.16664v1;http://arxiv.org/abs/2311.16664v1;2023-11-28;DGNR: Density-Guided Neural Point Rendering of Large Driving Scenes;"Despite the recent success of Neural Radiance Field (NeRF), it is still
challenging to render large-scale driving scenes with long trajectories,
particularly when the rendering quality and efficiency are in high demand.
Existing methods for such scenes usually involve with spatial warping,
geometric supervision from zero-shot normal or depth estimation, or scene
division strategies, where the synthesized views are often blurry or fail to
meet the requirement of efficient rendering. To address the above challenges,
this paper presents a novel framework that learns a density space from the
scenes to guide the construction of a point-based renderer, dubbed as DGNR
(Density-Guided Neural Rendering). In DGNR, geometric priors are no longer
needed, which can be intrinsically learned from the density space through
volumetric rendering. Specifically, we make use of a differentiable renderer to
synthesize images from the neural density features obtained from the learned
density space. A density-based fusion module and geometric regularization are
proposed to optimize the density space. By conducting experiments on a widely
used autonomous driving dataset, we have validated the effectiveness of DGNR in
synthesizing photorealistic driving scenes and achieving real-time capable
rendering.";Zhuopeng Li<author:sep>Chenming Wu<author:sep>Liangjun Zhang<author:sep>Jianke Zhu;http://arxiv.org/pdf/2311.16664v1;cs.CV;;nerf
2311.16937v1;http://arxiv.org/abs/2311.16937v1;2023-11-28;The Sky's the Limit: Re-lightable Outdoor Scenes via a Sky-pixel  Constrained Illumination Prior and Outside-In Visibility;"Inverse rendering of outdoor scenes from unconstrained image collections is a
challenging task, particularly illumination/albedo ambiguities and occlusion of
the illumination environment (shadowing) caused by geometry. However, there are
many cues in an image that can aid in the disentanglement of geometry, albedo
and shadows. We exploit the fact that any sky pixel provides a direct
measurement of distant lighting in the corresponding direction and, via a
neural illumination prior, a statistical cue as to the remaining illumination
environment. We also introduce a novel `outside-in' method for computing
differentiable sky visibility based on a neural directional distance function.
This is efficient and can be trained in parallel with the neural scene
representation, allowing gradients from appearance loss to flow from shadows to
influence estimation of illumination and geometry. Our method estimates
high-quality albedo, geometry, illumination and sky visibility, achieving
state-of-the-art results on the NeRF-OSR relighting benchmark. Our code and
models can be found https://github.com/JADGardner/neusky";James A. D. Gardner<author:sep>Evgenii Kashin<author:sep>Bernhard Egger<author:sep>William A. P. Smith;http://arxiv.org/pdf/2311.16937v1;cs.CV;;nerf
2311.17113v1;http://arxiv.org/abs/2311.17113v1;2023-11-28;Human Gaussian Splatting: Real-time Rendering of Animatable Avatars;"This work addresses the problem of real-time rendering of photorealistic
human body avatars learned from multi-view videos. While the classical
approaches to model and render virtual humans generally use a textured mesh,
recent research has developed neural body representations that achieve
impressive visual quality. However, these models are difficult to render in
real-time and their quality degrades when the character is animated with body
poses different than the training observations. We propose the first animatable
human model based on 3D Gaussian Splatting, that has recently emerged as a very
efficient alternative to neural radiance fields. Our body is represented by a
set of gaussian primitives in a canonical space which are deformed in a coarse
to fine approach that combines forward skinning and local non-rigid refinement.
We describe how to learn our Human Gaussian Splatting (\OURS) model in an
end-to-end fashion from multi-view observations, and evaluate it against the
state-of-the-art approaches for novel pose synthesis of clothed body. Our
method presents a PSNR 1.5dbB better than the state-of-the-art on THuman4
dataset while being able to render at 20fps or more.";Arthur Moreau<author:sep>Jifei Song<author:sep>Helisa Dhamo<author:sep>Richard Shaw<author:sep>Yiren Zhou<author:sep>Eduardo Pérez-Pellitero;http://arxiv.org/pdf/2311.17113v1;cs.CV;;gaussian splatting
2311.17245v3;http://arxiv.org/abs/2311.17245v3;2023-11-28;LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and  200+ FPS;"Recent advancements in real-time neural rendering using point-based
techniques have paved the way for the widespread adoption of 3D
representations. However, foundational approaches like 3D Gaussian Splatting
come with a substantial storage overhead caused by growing the SfM points to
millions, often demanding gigabyte-level disk space for a single unbounded
scene, posing significant scalability challenges and hindering the splatting
efficiency.
  To address this challenge, we introduce LightGaussian, a novel method
designed to transform 3D Gaussians into a more efficient and compact format.
Drawing inspiration from the concept of Network Pruning, LightGaussian
identifies Gaussians that are insignificant in contributing to the scene
reconstruction and adopts a pruning and recovery process, effectively reducing
redundancy in Gaussian counts while preserving visual effects. Additionally,
LightGaussian employs distillation and pseudo-view augmentation to distill
spherical harmonics to a lower degree, allowing knowledge transfer to more
compact representations while maintaining reflectance. Furthermore, we propose
a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in
lower bitwidth representations with minimal accuracy losses.
  In summary, LightGaussian achieves an averaged compression rate over 15x
while boosting the FPS from 139 to 215, enabling an efficient representation of
complex scenes on Mip-NeRF 360, Tank and Temple datasets.
  Project website: https://lightgaussian.github.io/";Zhiwen Fan<author:sep>Kevin Wang<author:sep>Kairun Wen<author:sep>Zehao Zhu<author:sep>Dejia Xu<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2311.17245v3;cs.CV;16pages, 8figures;gaussian splatting<tag:sep>nerf
2311.16945v2;http://arxiv.org/abs/2311.16945v2;2023-11-28;UC-NeRF: Neural Radiance Field for Under-Calibrated Multi-view Cameras  in Autonomous Driving;"Multi-camera setups find widespread use across various applications, such as
autonomous driving, as they greatly expand sensing capabilities. Despite the
fast development of Neural radiance field (NeRF) techniques and their wide
applications in both indoor and outdoor scenes, applying NeRF to multi-camera
systems remains very challenging. This is primarily due to the inherent
under-calibration issues in multi-camera setup, including inconsistent imaging
effects stemming from separately calibrated image signal processing units in
diverse cameras, and system errors arising from mechanical vibrations during
driving that affect relative camera poses. In this paper, we present UC-NeRF, a
novel method tailored for novel view synthesis in under-calibrated multi-view
camera systems. Firstly, we propose a layer-based color correction to rectify
the color inconsistency in different image regions. Second, we propose virtual
warping to generate more viewpoint-diverse but color-consistent virtual views
for color correction and 3D recovery. Finally, a spatiotemporally constrained
pose refinement is designed for more robust and accurate pose calibration in
multi-camera systems. Our method not only achieves state-of-the-art performance
of novel view synthesis in multi-camera setups, but also effectively
facilitates depth estimation in large-scale outdoor scenes with the synthesized
novel views.";Kai Cheng<author:sep>Xiaoxiao Long<author:sep>Wei Yin<author:sep>Jin Wang<author:sep>Zhiqiang Wu<author:sep>Yuexin Ma<author:sep>Kaixuan Wang<author:sep>Xiaozhi Chen<author:sep>Xuejin Chen;http://arxiv.org/pdf/2311.16945v2;cs.CV;"See the project page for code, data:
  https://kcheng1021.github.io/ucnerf.github.io";nerf
2311.17116v3;http://arxiv.org/abs/2311.17116v3;2023-11-28;REF$^2$-NeRF: Reflection and Refraction aware Neural Radiance Field;"Recently, significant progress has been made in the study of methods for 3D
reconstruction from multiple images using implicit neural representations,
exemplified by the neural radiance field (NeRF) method. Such methods, which are
based on volume rendering, can model various light phenomena, and various
extended methods have been proposed to accommodate different scenes and
situations. However, when handling scenes with multiple glass objects, e.g.,
objects in a glass showcase, modeling the target scene accurately has been
challenging due to the presence of multiple reflection and refraction effects.
Thus, this paper proposes a NeRF-based modeling method for scenes containing a
glass case. In the proposed method, refraction and reflection are modeled using
elements that are dependent and independent of the viewer's perspective. This
approach allows us to estimate the surfaces where refraction occurs, i.e.,
glass surfaces, and enables the separation and modeling of both direct and
reflected light components. Compared to existing methods, the proposed method
enables more accurate modeling of both glass refraction and the overall scene.";Wooseok Kim<author:sep>Taiki Fukiage<author:sep>Takeshi Oishi;http://arxiv.org/pdf/2311.17116v3;cs.CV;10 pages, 8 figures, 2 tables;nerf
2311.16657v1;http://arxiv.org/abs/2311.16657v1;2023-11-28;SCALAR-NeRF: SCAlable LARge-scale Neural Radiance Fields for Scene  Reconstruction;"In this work, we introduce SCALAR-NeRF, a novel framework tailored for
scalable large-scale neural scene reconstruction. We structure the neural
representation as an encoder-decoder architecture, where the encoder processes
3D point coordinates to produce encoded features, and the decoder generates
geometric values that include volume densities of signed distances and colors.
Our approach first trains a coarse global model on the entire image dataset.
Subsequently, we partition the images into smaller blocks using KMeans with
each block being modeled by a dedicated local model. We enhance the overlapping
regions across different blocks by scaling up the bounding boxes of each local
block. Notably, the decoder from the global model is shared across distinct
blocks and therefore promoting alignment in the feature space of local
encoders. We propose an effective and efficient methodology to fuse the outputs
from these local models to attain the final reconstruction. Employing this
refined coarse-to-fine strategy, our method outperforms state-of-the-art NeRF
methods and demonstrates scalability for large-scale scene reconstruction. The
code will be available on our project page at
https://aibluefisher.github.io/SCALAR-NeRF/";Yu Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2311.16657v1;cs.CV;Project Page: https://aibluefisher.github.io/SCALAR-NeRF;nerf
2311.16493v1;http://arxiv.org/abs/2311.16493v1;2023-11-27;Mip-Splatting: Alias-free 3D Gaussian Splatting;"Recently, 3D Gaussian Splatting has demonstrated impressive novel view
synthesis results, reaching high fidelity and efficiency. However, strong
artifacts can be observed when changing the sampling rate, \eg, by changing
focal length or camera distance. We find that the source for this phenomenon
can be attributed to the lack of 3D frequency constraints and the usage of a 2D
dilation filter. To address this problem, we introduce a 3D smoothing filter
which constrains the size of the 3D Gaussian primitives based on the maximal
sampling frequency induced by the input views, eliminating high-frequency
artifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mip
filter, which simulates a 2D box filter, effectively mitigates aliasing and
dilation issues. Our evaluation, including scenarios such a training on
single-scale images and testing on multiple scales, validates the effectiveness
of our approach.";Zehao Yu<author:sep>Anpei Chen<author:sep>Binbin Huang<author:sep>Torsten Sattler<author:sep>Andreas Geiger;http://arxiv.org/pdf/2311.16493v1;cs.CV;Project page: https://niujinshuchong.github.io/mip-splatting/;gaussian splatting
2311.16499v1;http://arxiv.org/abs/2311.16499v1;2023-11-27;Deceptive-Human: Prompt-to-NeRF 3D Human Generation with 3D-Consistent  Synthetic Images;"This paper presents Deceptive-Human, a novel Prompt-to-NeRF framework
capitalizing state-of-the-art control diffusion models (e.g., ControlNet) to
generate a high-quality controllable 3D human NeRF. Different from direct 3D
generative approaches, e.g., DreamFusion and DreamHuman, Deceptive-Human
employs a progressive refinement technique to elevate the reconstruction
quality. This is achieved by utilizing high-quality synthetic human images
generated through the ControlNet with view-consistent loss. Our method is
versatile and readily extensible, accommodating multimodal inputs, including a
text prompt and additional data such as 3D mesh, poses, and seed images. The
resulting 3D human NeRF model empowers the synthesis of highly photorealistic
novel views from 360-degree perspectives. The key to our Deceptive-Human for
hallucinating multi-view consistent synthetic human images lies in our
progressive finetuning strategy. This strategy involves iteratively enhancing
views using the provided multimodal inputs at each intermediate step to improve
the human NeRF model. Within this iterative refinement process, view-dependent
appearances are systematically eliminated to prevent interference with the
underlying density estimation. Extensive qualitative and quantitative
experimental comparison shows that our deceptive human models achieve
state-of-the-art application quality.";Shiu-hong Kao<author:sep>Xinhang Liu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2311.16499v1;cs.CV;Github project: https://github.com/DanielSHKao/DeceptiveHuman;nerf
2311.16096v1;http://arxiv.org/abs/2311.16096v1;2023-11-27;Animatable Gaussians: Learning Pose-dependent Gaussian Maps for  High-fidelity Human Avatar Modeling;"Modeling animatable human avatars from RGB videos is a long-standing and
challenging problem. Recent works usually adopt MLP-based neural radiance
fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to
regress pose-dependent garment details. To this end, we introduce Animatable
Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D
Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians
with the animatable avatar, we learn a parametric template from the input
videos, and then parameterize the template on two front \& back canonical
Gaussian maps where each pixel represents a 3D Gaussian. The learned template
is adaptive to the wearing garments for modeling looser clothes like dresses.
Such template-guided 2D parameterization enables us to employ a powerful
StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling
detailed dynamic appearances. Furthermore, we introduce a pose projection
strategy for better generalization given novel poses. Overall, our method can
create lifelike avatars with dynamic, realistic and generalized appearances.
Experiments show that our method outperforms other state-of-the-art approaches.
Code: https://github.com/lizhe00/AnimatableGaussians";Zhe Li<author:sep>Zerong Zheng<author:sep>Lizhen Wang<author:sep>Yebin Liu;http://arxiv.org/pdf/2311.16096v1;cs.CV;"Projectpage: https://animatable-gaussians.github.io/, Code:
  https://github.com/lizhe00/AnimatableGaussians";gaussian splatting<tag:sep>nerf
2311.16482v2;http://arxiv.org/abs/2311.16482v2;2023-11-27;Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple  Human Avatars;"Neural radiance fields are capable of reconstructing high-quality drivable
human avatars but are expensive to train and render. To reduce consumption, we
propose Animatable 3D Gaussian, which learns human avatars from input images
and poses. We extend 3D Gaussians to dynamic human scenes by modeling a set of
skinned 3D Gaussians and a corresponding skeleton in canonical space and
deforming 3D Gaussians to posed space according to the input poses. We
introduce hash-encoded shape and appearance to speed up training and propose
time-dependent ambient occlusion to achieve high-quality reconstructions in
scenes containing complex motions and dynamic shadows. On both novel view
synthesis and novel pose synthesis tasks, our method outperforms existing
methods in terms of training time, rendering speed, and reconstruction quality.
Our method can be easily extended to multi-human scenes and achieve comparable
novel view synthesis results on a scene with ten people in only 25 seconds of
training.";Yang Liu<author:sep>Xiang Huang<author:sep>Minghan Qin<author:sep>Qinwei Lin<author:sep>Haoqian Wang;http://arxiv.org/pdf/2311.16482v2;cs.CV;;
2311.16037v1;http://arxiv.org/abs/2311.16037v1;2023-11-27;GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions;"Recently, impressive results have been achieved in 3D scene editing with text
instructions based on a 2D diffusion model. However, current diffusion models
primarily generate images by predicting noise in the latent space, and the
editing is usually applied to the whole image, which makes it challenging to
perform delicate, especially localized, editing for 3D scenes. Inspired by
recent 3D Gaussian splatting, we propose a systematic framework, named
GaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text
instructions. Benefiting from the explicit property of 3D Gaussians, we design
a series of techniques to achieve delicate editing. Specifically, we first
extract the region of interest (RoI) corresponding to the text instruction,
aligning it to 3D Gaussians. The Gaussian RoI is further used to control the
editing process. Our framework can achieve more delicate and precise editing of
3D scenes than previous methods while enjoying much faster training speed, i.e.
within 20 minutes on a single V100 GPU, more than twice as fast as
Instruct-NeRF2NeRF (45 minutes -- 2 hours).";Jiemin Fang<author:sep>Junjie Wang<author:sep>Xiaopeng Zhang<author:sep>Lingxi Xie<author:sep>Qi Tian;http://arxiv.org/pdf/2311.16037v1;cs.CV;Project page: https://GaussianEditor.github.io;gaussian splatting<tag:sep>nerf
2311.16043v1;http://arxiv.org/abs/2311.16043v1;2023-11-27;Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF  Decomposition and Ray Tracing;"We present a novel differentiable point-based rendering framework for
material and lighting decomposition from multi-view images, enabling editing,
ray-tracing, and real-time relighting of the 3D point cloud. Specifically, a 3D
scene is represented as a set of relightable 3D Gaussian points, where each
point is additionally associated with a normal direction, BRDF parameters, and
incident lights from different directions. To achieve robust lighting
estimation, we further divide incident lights of each point into global and
local components, as well as view-dependent visibilities. The 3D scene is
optimized through the 3D Gaussian Splatting technique while BRDF and lighting
are decomposed by physically-based differentiable rendering. Moreover, we
introduce an innovative point-based ray-tracing approach based on the bounding
volume hierarchy for efficient visibility baking, enabling real-time rendering
and relighting of 3D Gaussian points with accurate shadow effects. Extensive
experiments demonstrate improved BRDF estimation and novel view rendering
results compared to state-of-the-art material estimation approaches. Our
framework showcases the potential to revolutionize the mesh-based graphics
pipeline with a relightable, traceable, and editable rendering pipeline solely
based on point cloud. Project
page:https://nju-3dv.github.io/projects/Relightable3DGaussian/.";Jian Gao<author:sep>Chun Gu<author:sep>Youtian Lin<author:sep>Hao Zhu<author:sep>Xun Cao<author:sep>Li Zhang<author:sep>Yao Yao;http://arxiv.org/pdf/2311.16043v1;cs.CV;;gaussian splatting
2311.15803v2;http://arxiv.org/abs/2311.15803v2;2023-11-27;SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using  Neural Radiance Fields;"In rapidly-evolving domains such as autonomous driving, the use of multiple
sensors with different modalities is crucial to ensure high operational
precision and stability. To correctly exploit the provided information by each
sensor in a single common frame, it is essential for these sensors to be
accurately calibrated. In this paper, we leverage the ability of Neural
Radiance Fields (NeRF) to represent different sensors modalities in a common
volumetric representation to achieve robust and accurate spatio-temporal sensor
calibration. By designing a partitioning approach based on the visible part of
the scene for each sensor, we formulate the calibration problem using only the
overlapping areas. This strategy results in a more robust and accurate
calibration that is less prone to failure. We demonstrate that our approach
works on outdoor urban scenes by validating it on multiple established driving
datasets. Results show that our method is able to get better accuracy and
robustness compared to existing methods.";Quentin Herau<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Luis Roldão<author:sep>Dzmitry Tsishkou<author:sep>Cyrille Migniot<author:sep>Pascal Vasseur<author:sep>Cédric Demonceaux;http://arxiv.org/pdf/2311.15803v2;cs.CV;"Paper + Supplementary, under review. Project page:
  https://qherau.github.io/SOAC/";nerf
2311.15510v1;http://arxiv.org/abs/2311.15510v1;2023-11-27;CaesarNeRF: Calibrated Semantic Representation for Few-shot  Generalizable Neural Rendering;"Generalizability and few-shot learning are key challenges in Neural Radiance
Fields (NeRF), often due to the lack of a holistic understanding in pixel-level
rendering. We introduce CaesarNeRF, an end-to-end approach that leverages
scene-level CAlibratEd SemAntic Representation along with pixel-level
representations to advance few-shot, generalizable neural rendering,
facilitating a holistic understanding without compromising high-quality
details. CaesarNeRF explicitly models pose differences of reference views to
combine scene-level semantic representations, providing a calibrated holistic
understanding. This calibration process aligns various viewpoints with precise
location and is further enhanced by sequential refinement to capture varying
details. Extensive experiments on public datasets, including LLFF, Shiny,
mip-NeRF 360, and MVImgNet, show that CaesarNeRF delivers state-of-the-art
performance across varying numbers of reference views, proving effective even
with a single reference image. The project page of this work can be found at
https://haidongz-usc.github.io/project/caesarnerf.";Haidong Zhu<author:sep>Tianyu Ding<author:sep>Tianyi Chen<author:sep>Ilya Zharkov<author:sep>Ram Nevatia<author:sep>Luming Liang;http://arxiv.org/pdf/2311.15510v1;cs.CV;;nerf
2311.15637v1;http://arxiv.org/abs/2311.15637v1;2023-11-27;PaintNeSF: Artistic Creation of Stylized Scenes with Vectorized 3D  Strokes;"We present Paint Neural Stroke Field (PaintNeSF), a novel technique to
generate stylized images of a 3D scene at arbitrary novel views from multi-view
2D images. Different from existing methods which apply stylization to trained
neural radiance fields at the voxel level, our approach draws inspiration from
image-to-painting methods, simulating the progressive painting process of human
artwork with vector strokes. We develop a palette of stylized 3D strokes from
basic primitives and splines, and consider the 3D scene stylization task as a
multi-view reconstruction process based on these 3D stroke primitives. Instead
of directly searching for the parameters of these 3D strokes, which would be
too costly, we introduce a differentiable renderer that allows optimizing
stroke parameters using gradient descent, and propose a training scheme to
alleviate the vanishing gradient issue. The extensive evaluation demonstrates
that our approach effectively synthesizes 3D scenes with significant geometric
and aesthetic stylization while maintaining a consistent appearance across
different views. Our method can be further integrated with style loss and
image-text contrastive models to extend its applications, including color
transfer and text-driven 3D scene drawing.";Hao-Bin Duan<author:sep>Miao Wang<author:sep>Yan-Xun Li<author:sep>Yong-Liang Yang;http://arxiv.org/pdf/2311.15637v1;cs.CV;;
2311.16473v2;http://arxiv.org/abs/2311.16473v2;2023-11-26;GS-IR: 3D Gaussian Splatting for Inverse Rendering;"We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian
Splatting (GS) that leverages forward mapping volume rendering to achieve
photorealistic novel view synthesis and relighting results. Unlike previous
works that use implicit neural representations and volume rendering (e.g.
NeRF), which suffer from low expressive power and high computational
complexity, we extend GS, a top-performance representation for novel view
synthesis, to estimate scene geometry, surface material, and environment
illumination from multi-view images captured under unknown lighting conditions.
There are two main problems when introducing GS to inverse rendering: 1) GS
does not support producing plausible normal natively; 2) forward mapping (e.g.
rasterization and splatting) cannot trace the occlusion like backward mapping
(e.g. ray tracing). To address these challenges, our GS-IR proposes an
efficient optimization scheme that incorporates a depth-derivation-based
regularization for normal estimation and a baking-based occlusion to model
indirect lighting. The flexible and expressive GS representation allows us to
achieve fast and compact geometry reconstruction, photorealistic novel view
synthesis, and effective physically-based rendering. We demonstrate the
superiority of our method over baseline methods through qualitative and
quantitative evaluations on various challenging scenes.";Zhihao Liang<author:sep>Qi Zhang<author:sep>Ying Feng<author:sep>Ying Shan<author:sep>Kui Jia;http://arxiv.org/pdf/2311.16473v2;cs.CV;;gaussian splatting<tag:sep>nerf
2311.15439v1;http://arxiv.org/abs/2311.15439v1;2023-11-26;Efficient Encoding of Graphics Primitives with Simplex-based Structures;"Grid-based structures are commonly used to encode explicit features for
graphics primitives such as images, signed distance functions (SDF), and neural
radiance fields (NeRF) due to their simple implementation. However, in
$n$-dimensional space, calculating the value of a sampled point requires
interpolating the values of its $2^n$ neighboring vertices. The exponential
scaling with dimension leads to significant computational overheads. To address
this issue, we propose a simplex-based approach for encoding graphics
primitives. The number of vertices in a simplex-based structure increases
linearly with dimension, making it a more efficient and generalizable
alternative to grid-based representations. Using the non-axis-aligned
simplicial structure property, we derive and prove a coordinate transformation,
simplicial subdivision, and barycentric interpolation scheme for efficient
sampling, which resembles transformation procedures in the simplex noise
algorithm. Finally, we use hash tables to store multiresolution features of all
interest points in the simplicial grid, which are passed into a tiny fully
connected neural network to parameterize graphics primitives. We implemented a
detailed simplex-based structure encoding algorithm in C++ and CUDA using the
methods outlined in our approach. In the 2D image fitting task, the proposed
method is capable of fitting a giga-pixel image with 9.4% less time compared to
the baseline method proposed by instant-ngp, while maintaining the same quality
and compression rate. In the volumetric rendering setup, we observe a maximum
41.2% speedup when the samples are dense enough.";Yibo Wen<author:sep>Yunfan Yang;http://arxiv.org/pdf/2311.15439v1;cs.CV;10 pages, 8 figures;nerf
2311.15291v1;http://arxiv.org/abs/2311.15291v1;2023-11-26;Obj-NeRF: Extract Object NeRFs from Multi-view Images;"Neural Radiance Fields (NeRFs) have demonstrated remarkable effectiveness in
novel view synthesis within 3D environments. However, extracting a radiance
field of one specific object from multi-view images encounters substantial
challenges due to occlusion and background complexity, thereby presenting
difficulties in downstream applications such as NeRF editing and 3D mesh
extraction. To solve this problem, in this paper, we propose Obj-NeRF, a
comprehensive pipeline that recovers the 3D geometry of a specific object from
multi-view images using a single prompt. This method combines the 2D
segmentation capabilities of the Segment Anything Model (SAM) in conjunction
with the 3D reconstruction ability of NeRF. Specifically, we first obtain
multi-view segmentation for the indicated object using SAM with a single
prompt. Then, we use the segmentation images to supervise NeRF construction,
integrating several effective techniques. Additionally, we construct a large
object-level NeRF dataset containing diverse objects, which can be useful in
various downstream tasks. To demonstrate the practicality of our method, we
also apply Obj-NeRF to various applications, including object removal,
rotation, replacement, and recoloring.";Zhiyi Li<author:sep>Lihe Ding<author:sep>Tianfan Xue;http://arxiv.org/pdf/2311.15291v1;cs.CV;;nerf
2311.15260v2;http://arxiv.org/abs/2311.15260v2;2023-11-26;NeuRAD: Neural Rendering for Autonomous Driving;"Neural radiance fields (NeRFs) have gained popularity in the autonomous
driving (AD) community. Recent methods show NeRFs' potential for closed-loop
simulation, enabling testing of AD systems, and as an advanced training data
augmentation technique. However, existing methods often require long training
times, dense semantic supervision, or lack generalizability. This, in turn,
hinders the application of NeRFs for AD at scale. In this paper, we propose
NeuRAD, a robust novel view synthesis method tailored to dynamic AD data. Our
method features simple network design, extensive sensor modeling for both
camera and lidar -- including rolling shutter, beam divergence and ray dropping
-- and is applicable to multiple datasets out of the box. We verify its
performance on five popular AD datasets, achieving state-of-the-art performance
across the board. To encourage further development, we will openly release the
NeuRAD source code. See https://github.com/georghess/NeuRAD .";Adam Tonderski<author:sep>Carl Lindström<author:sep>Georg Hess<author:sep>William Ljungbergh<author:sep>Lennart Svensson<author:sep>Christoffer Petersson;http://arxiv.org/pdf/2311.15260v2;cs.CV;;nerf
2311.14603v1;http://arxiv.org/abs/2311.14603v1;2023-11-24;Animate124: Animating One Image to 4D Dynamic Scene;"We introduce Animate124 (Animate-one-image-to-4D), the first work to animate
a single in-the-wild image into 3D video through textual motion descriptions,
an underexplored problem with significant applications. Our 4D generation
leverages an advanced 4D grid dynamic Neural Radiance Field (NeRF) model,
optimized in three distinct stages using multiple diffusion priors. Initially,
a static model is optimized using the reference image, guided by 2D and 3D
diffusion priors, which serves as the initialization for the dynamic NeRF.
Subsequently, a video diffusion model is employed to learn the motion specific
to the subject. However, the object in the 3D videos tends to drift away from
the reference image over time. This drift is mainly due to the misalignment
between the text prompt and the reference image in the video diffusion model.
In the final stage, a personalized diffusion prior is therefore utilized to
address the semantic drift. As the pioneering image-text-to-4D generation
framework, our method demonstrates significant advancements over existing
baselines, evidenced by comprehensive quantitative and qualitative assessments.";Yuyang Zhao<author:sep>Zhiwen Yan<author:sep>Enze Xie<author:sep>Lanqing Hong<author:sep>Zhenguo Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2311.14603v1;cs.CV;Project Page: https://animate124.github.io;nerf
2311.14521v4;http://arxiv.org/abs/2311.14521v4;2023-11-24;GaussianEditor: Swift and Controllable 3D Editing with Gaussian  Splatting;"3D editing plays a crucial role in many areas such as gaming and virtual
reality. Traditional 3D editing methods, which rely on representations like
meshes and point clouds, often fall short in realistically depicting complex
scenes. On the other hand, methods based on implicit 3D representations, like
Neural Radiance Field (NeRF), render complex scenes effectively but suffer from
slow processing speeds and limited control over specific scene areas. In
response to these challenges, our paper presents GaussianEditor, an innovative
and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D
representation. GaussianEditor enhances precision and control in editing
through our proposed Gaussian semantic tracing, which traces the editing target
throughout the training process. Additionally, we propose Hierarchical Gaussian
splatting (HGS) to achieve stabilized and fine results under stochastic
generative guidance from 2D diffusion models. We also develop editing
strategies for efficient object removal and integration, a challenging task for
existing methods. Our comprehensive experiments demonstrate GaussianEditor's
superior control, efficacy, and rapid performance, marking a significant
advancement in 3D editing. Project Page:
https://buaacyw.github.io/gaussian-editor/";Yiwen Chen<author:sep>Zilong Chen<author:sep>Chi Zhang<author:sep>Feng Wang<author:sep>Xiaofeng Yang<author:sep>Yikai Wang<author:sep>Zhongang Cai<author:sep>Lei Yang<author:sep>Huaping Liu<author:sep>Guosheng Lin;http://arxiv.org/pdf/2311.14521v4;cs.CV;"Project Page: https://buaacyw.github.io/gaussian-editor/ Code:
  https://github.com/buaacyw/GaussianEditor";gaussian splatting<tag:sep>nerf
2311.14153v1;http://arxiv.org/abs/2311.14153v1;2023-11-23;Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC  using Tube-Guided Data Augmentation and NeRFs;"Imitation learning (IL) can train computationally-efficient sensorimotor
policies from a resource-intensive Model Predictive Controller (MPC), but it
often requires many samples, leading to long training times or limited
robustness. To address these issues, we combine IL with a variant of robust MPC
that accounts for process and sensing uncertainties, and we design a data
augmentation (DA) strategy that enables efficient learning of vision-based
policies. The proposed DA method, named Tube-NeRF, leverages Neural Radiance
Fields (NeRFs) to generate novel synthetic images, and uses properties of the
robust MPC (the tube) to select relevant views and to efficiently compute the
corresponding actions. We tailor our approach to the task of localization and
trajectory tracking on a multirotor, by learning a visuomotor policy that
generates control actions using images from the onboard camera as only source
of horizontal position. Our evaluations numerically demonstrate learning of a
robust visuomotor policy with an 80-fold increase in demonstration efficiency
and a 50% reduction in training time over current IL methods. Additionally, our
policies successfully transfer to a real multirotor, achieving accurate
localization and low tracking errors despite large disturbances, with an
onboard inference time of only 1.5 ms.";Andrea Tagliabue<author:sep>Jonathan P. How;http://arxiv.org/pdf/2311.14153v1;cs.RO;"Video: https://youtu.be/_W5z33ZK1m4. Evolved paper from our previous
  work: arXiv:2210.10127";nerf
2311.13750v2;http://arxiv.org/abs/2311.13750v2;2023-11-23;Towards Transferable Multi-modal Perception Representation Learning for  Autonomy: NeRF-Supervised Masked AutoEncoder;"This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. We hope
this study can inspire exploration of more general multi-modal representation
learning for autonomous agents.";Xiaohao Xu;http://arxiv.org/pdf/2311.13750v2;cs.CV;;nerf
2311.14208v1;http://arxiv.org/abs/2311.14208v1;2023-11-23;ECRF: Entropy-Constrained Neural Radiance Fields Compression with  Frequency Domain Optimization;"Explicit feature-grid based NeRF models have shown promising results in terms
of rendering quality and significant speed-up in training. However, these
methods often require a significant amount of data to represent a single scene
or object. In this work, we present a compression model that aims to minimize
the entropy in the frequency domain in order to effectively reduce the data
size. First, we propose using the discrete cosine transform (DCT) on the
tensorial radiance fields to compress the feature-grid. This feature-grid is
transformed into coefficients, which are then quantized and entropy encoded,
following a similar approach to the traditional video coding pipeline.
Furthermore, to achieve a higher level of sparsity, we propose using an entropy
parameterization technique for the frequency domain, specifically for DCT
coefficients of the feature-grid. Since the transformed coefficients are
optimized during the training phase, the proposed model does not require any
fine-tuning or additional information. Our model only requires a lightweight
compression pipeline for encoding and decoding, making it easier to apply
volumetric radiance field methods for real-world applications. Experimental
results demonstrate that our proposed frequency domain entropy model can
achieve superior compression performance across various datasets. The source
code will be made publicly available.";Soonbin Lee<author:sep>Fangwen Shu<author:sep>Yago Sanchez<author:sep>Thomas Schierl<author:sep>Cornelius Hellge;http://arxiv.org/pdf/2311.14208v1;cs.CV;10 pages, 6 figures, 4 tables;nerf
2311.13831v1;http://arxiv.org/abs/2311.13831v1;2023-11-23;Posterior Distillation Sampling;"We introduce Posterior Distillation Sampling (PDS), a novel optimization
method for parametric image editing based on diffusion models. Existing
optimization-based methods, which leverage the powerful 2D prior of diffusion
models to handle various parametric images, have mainly focused on generation.
Unlike generation, editing requires a balance between conforming to the target
attribute and preserving the identity of the source content. Recent 2D image
editing methods have achieved this balance by leveraging the stochastic latent
encoded in the generative process of diffusion models. To extend the editing
capabilities of diffusion models shown in pixel space to parameter space, we
reformulate the 2D image editing method into an optimization form named PDS.
PDS matches the stochastic latents of the source and the target, enabling the
sampling of targets in diverse parameter spaces that align with a desired
attribute while maintaining the source's identity. We demonstrate that this
optimization resembles running a generative process with the target attribute,
but aligning this process with the trajectory of the source's generative
process. Extensive editing results in Neural Radiance Fields and Scalable
Vector Graphics representations demonstrate that PDS is capable of sampling
targets to fulfill the aforementioned balance across various parameter spaces.";Juil Koo<author:sep>Chanho Park<author:sep>Minhyuk Sung;http://arxiv.org/pdf/2311.13831v1;cs.CV;Project page: https://posterior-distillation-sampling.github.io/;
2311.13297v1;http://arxiv.org/abs/2311.13297v1;2023-11-22;Retargeting Visual Data with Deformation Fields;"Seam carving is an image editing method that enable content-aware resizing,
including operations like removing objects. However, the seam-finding strategy
based on dynamic programming or graph-cut limits its applications to broader
visual data formats and degrees of freedom for editing. Our observation is that
describing the editing and retargeting of images more generally by a
displacement field yields a generalisation of content-aware deformations. We
propose to learn a deformation with a neural network that keeps the output
plausible while trying to deform it only in places with low information
content. This technique applies to different kinds of visual data, including
images, 3D scenes given as neural radiance fields, or even polygon meshes.
Experiments conducted on different visual data show that our method achieves
better content-aware retargeting compared to previous methods.";Tim Elsner<author:sep>Julia Berger<author:sep>Tong Wu<author:sep>Victor Czech<author:sep>Lin Gao<author:sep>Leif Kobbelt;http://arxiv.org/pdf/2311.13297v1;cs.CV;;
2311.13681v1;http://arxiv.org/abs/2311.13681v1;2023-11-22;Compact 3D Gaussian Representation for Radiance Field;"Neural Radiance Fields (NeRFs) have demonstrated remarkable potential in
capturing complex 3D scenes with high fidelity. However, one persistent
challenge that hinders the widespread adoption of NeRFs is the computational
bottleneck due to the volumetric rendering. On the other hand, 3D Gaussian
splatting (3DGS) has recently emerged as an alternative representation that
leverages a 3D Gaussisan-based representation and adopts the rasterization
pipeline to render the images rather than volumetric rendering, achieving very
fast rendering speed and promising image quality. However, a significant
drawback arises as 3DGS entails a substantial number of 3D Gaussians to
maintain the high fidelity of the rendered images, which requires a large
amount of memory and storage. To address this critical issue, we place a
specific emphasis on two key objectives: reducing the number of Gaussian points
without sacrificing performance and compressing the Gaussian attributes, such
as view-dependent color and covariance. To this end, we propose a learnable
mask strategy that significantly reduces the number of Gaussians while
preserving high performance. In addition, we propose a compact but effective
representation of view-dependent color by employing a grid-based neural field
rather than relying on spherical harmonics. Finally, we learn codebooks to
compactly represent the geometric attributes of Gaussian by vector
quantization. In our extensive experiments, we consistently show over
10$\times$ reduced storage and enhanced rendering speed, while maintaining the
quality of the scene representation, compared to 3DGS. Our work provides a
comprehensive framework for 3D scene representation, achieving high
performance, fast training, compactness, and real-time rendering. Our project
page is available at https://maincold2.github.io/c3dgs/.";Joo Chan Lee<author:sep>Daniel Rho<author:sep>Xiangyu Sun<author:sep>Jong Hwan Ko<author:sep>Eunbyung Park;http://arxiv.org/pdf/2311.13681v1;cs.CV;Project page: http://maincold2.github.io/c3dgs/;nerf
2311.13384v2;http://arxiv.org/abs/2311.13384v2;2023-11-22;LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes;"With the widespread usage of VR devices and contents, demands for 3D scene
generation techniques become more popular. Existing 3D scene generation models,
however, limit the target scene to specific domain, primarily due to their
training strategies using 3D scan dataset that is far from the real-world. To
address such limitation, we propose LucidDreamer, a domain-free scene
generation pipeline by fully leveraging the power of existing large-scale
diffusion-based generative model. Our LucidDreamer has two alternate steps:
Dreaming and Alignment. First, to generate multi-view consistent images from
inputs, we set the point cloud as a geometrical guideline for each image
generation. Specifically, we project a portion of point cloud to the desired
view and provide the projection as a guidance for inpainting using the
generative model. The inpainted images are lifted to 3D space with estimated
depth maps, composing a new points. Second, to aggregate the new points into
the 3D scene, we propose an aligning algorithm which harmoniously integrates
the portions of newly generated 3D scenes. The finally obtained 3D scene serves
as initial points for optimizing Gaussian splats. LucidDreamer produces
Gaussian splats that are highly-detailed compared to the previous 3D scene
generation methods, with no constraint on domain of the target scene. Project
page: https://luciddreamer-cvlab.github.io/";Jaeyoung Chung<author:sep>Suyoung Lee<author:sep>Hyeongjin Nam<author:sep>Jaerin Lee<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2311.13384v2;cs.CV;Project page: https://luciddreamer-cvlab.github.io/;gaussian splatting
2311.13404v2;http://arxiv.org/abs/2311.13404v2;2023-11-22;Animatable 3D Gaussians for High-fidelity Synthesis of Human Motions;"We present a novel animatable 3D Gaussian model for rendering high-fidelity
free-view human motions in real time. Compared to existing NeRF-based methods,
the model owns better capability in synthesizing high-frequency details without
the jittering problem across video frames. The core of our model is a novel
augmented 3D Gaussian representation, which attaches each Gaussian with a
learnable code. The learnable code serves as a pose-dependent appearance
embedding for refining the erroneous appearance caused by geometric
transformation of Gaussians, based on which an appearance refinement model is
learned to produce residual Gaussian properties to match the appearance in
target pose. To force the Gaussians to learn the foreground human only without
background interference, we further design a novel alpha loss to explicitly
constrain the Gaussians within the human body. We also propose to jointly
optimize the human joint parameters to improve the appearance accuracy. The
animatable 3D Gaussian model can be learned with shallow MLPs, so new human
motions can be synthesized in real time (66 fps on avarage). Experiments show
that our model has superior performance over NeRF-based methods.";Keyang Ye<author:sep>Tianjia Shao<author:sep>Kun Zhou;http://arxiv.org/pdf/2311.13404v2;cs.CV;"Some experiment data is wrong. The expression of the paper in
  introduction and abstract is incorrect. Some graphs have inappropriate
  descriptions";nerf
2311.13168v1;http://arxiv.org/abs/2311.13168v1;2023-11-22;3D Face Style Transfer with a Hybrid Solution of NeRF and Mesh  Rasterization;"Style transfer for human face has been widely researched in recent years.
Majority of the existing approaches work in 2D image domain and have 3D
inconsistency issue when applied on different viewpoints of the same face. In
this paper, we tackle the problem of 3D face style transfer which aims at
generating stylized novel views of a 3D human face with multi-view consistency.
We propose to use a neural radiance field (NeRF) to represent 3D human face and
combine it with 2D style transfer to stylize the 3D face. We find that directly
training a NeRF on stylized images from 2D style transfer brings in 3D
inconsistency issue and causes blurriness. On the other hand, training a NeRF
jointly with 2D style transfer objectives shows poor convergence due to the
identity and head pose gap between style image and content image. It also poses
challenge in training time and memory due to the need of volume rendering for
full image to apply style transfer loss functions. We therefore propose a
hybrid framework of NeRF and mesh rasterization to combine the benefits of high
fidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our
framework consists of three stages: 1. Training a NeRF model on input face
images to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF
model and optimizing it with style transfer objectives via differentiable
rasterization; 3. Training a new color network in NeRF conditioned on a style
embedding to enable arbitrary style transfer to the 3D face. Experiment results
show that our approach generates high quality face style transfer with great 3D
consistency, while also enabling a flexible style control.";Jianwei Feng<author:sep>Prateek Singhal;http://arxiv.org/pdf/2311.13168v1;cs.CV;;nerf
2311.13617v1;http://arxiv.org/abs/2311.13617v1;2023-11-22;Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to  3D Prior with Progressive Learning;"We present Boosting3D, a multi-stage single image-to-3D generation method
that can robustly generate reasonable 3D objects in different data domains. The
point of this work is to solve the view consistency problem in single
image-guided 3D generation by modeling a reasonable geometric structure. For
this purpose, we propose to utilize better 3D prior to training the NeRF. More
specifically, we train an object-level LoRA for the target object using
original image and the rendering output of NeRF. And then we train the LoRA and
NeRF using a progressive training strategy. The LoRA and NeRF will boost each
other while training. After the progressive training, the LoRA learns the 3D
information of the generated object and eventually turns to an object-level 3D
prior. In the final stage, we extract the mesh from the trained NeRF and use
the trained LoRA to optimize the structure and appearance of the mesh. The
experiments demonstrate the effectiveness of the proposed method. Boosting3D
learns object-specific 3D prior which is beyond the ability of pre-trained
diffusion priors and achieves state-of-the-art performance in the single
image-to-3d generation task.";Kai Yu<author:sep>Jinlin Liu<author:sep>Mengyang Feng<author:sep>Miaomiao Cui<author:sep>Xuansong Xie;http://arxiv.org/pdf/2311.13617v1;cs.CV;8 pages, 7 figures, 1 table;nerf
2311.13099v1;http://arxiv.org/abs/2311.13099v1;2023-11-22;PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF;"We show that physics-based simulations can be seamlessly integrated with NeRF
to generate high-quality elastodynamics of real-world objects. Unlike existing
methods, we discretize nonlinear hyperelasticity in a meshless way, obviating
the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh
or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed
to capture nonlinear dynamics and large deformation on the implicit model. Such
meshless integration enables versatile simulations of complex and codimensional
shapes. We adaptively place the least-square kernels according to the NeRF
density field to significantly reduce the complexity of the nonlinear
simulation. As a result, physically realistic animations can be conveniently
synthesized using our method for a wide range of hyperelastic materials at an
interactive rate. For more information, please visit our project page at
https://fytalon.github.io/pienerf/.";Yutao Feng<author:sep>Yintong Shang<author:sep>Xuan Li<author:sep>Tianjia Shao<author:sep>Chenfanfu Jiang<author:sep>Yin Yang;http://arxiv.org/pdf/2311.13099v1;cs.CV;;nerf
2311.13398v3;http://arxiv.org/abs/2311.13398v3;2023-11-22;Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot  Images;"In this paper, we present a method to optimize Gaussian splatting with a
limited number of images while avoiding overfitting. Representing a 3D scene by
combining numerous Gaussian splats has yielded outstanding visual quality.
However, it tends to overfit the training views when only a small number of
images are available. To address this issue, we introduce a dense depth map as
a geometry guide to mitigate overfitting. We obtained the depth map using a
pre-trained monocular depth estimation model and aligning the scale and offset
using sparse COLMAP feature points. The adjusted depth aids in the color-based
optimization of 3D Gaussian splatting, mitigating floating artifacts, and
ensuring adherence to geometric constraints. We verify the proposed method on
the NeRF-LLFF dataset with varying numbers of few images. Our approach
demonstrates robust geometry compared to the original method that relies solely
on images. Project page: robot0321.github.io/DepthRegGS";Jaeyoung Chung<author:sep>Jeongtaek Oh<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2311.13398v3;cs.CV;"10 pages, 5 figures; Project page: robot0321.github.io/DepthRegGS";gaussian splatting<tag:sep>nerf
2311.12897v1;http://arxiv.org/abs/2311.12897v1;2023-11-21;An Efficient 3D Gaussian Representation for Monocular/Multi-view Dynamic  Scenes;"In novel view synthesis of scenes from multiple input views, 3D Gaussian
splatting emerges as a viable alternative to existing radiance field
approaches, delivering great visual quality and real-time rendering. While
successful in static scenes, the present advancement of 3D Gaussian
representation, however, faces challenges in dynamic scenes in terms of memory
consumption and the need for numerous observations per time step, due to the
onus of storing 3D Gaussian parameters per time step. In this study, we present
an efficient 3D Gaussian representation tailored for dynamic scenes in which we
define positions and rotations as functions of time while leaving other
time-invariant properties of the static 3D Gaussian unchanged. Notably, our
representation reduces memory usage, which is consistent regardless of the
input sequence length. Additionally, it mitigates the risk of overfitting
observed frames by accounting for temporal changes. The optimization of our
Gaussian representation based on image and flow reconstruction results in a
powerful framework for dynamic scene view synthesis in both monocular and
multi-view cases. We obtain the highest rendering speed of $118$ frames per
second (FPS) at a resolution of $1352 \times 1014$ with a single GPU, showing
the practical usability and effectiveness of our proposed method in dynamic
scene rendering scenarios.";Kai Katsumata<author:sep>Duc Minh Vo<author:sep>Hideki Nakayama;http://arxiv.org/pdf/2311.12897v1;cs.GR;10 pages, 10 figures;
2311.12775v3;http://arxiv.org/abs/2311.12775v3;2023-11-21;SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh  Reconstruction and High-Quality Mesh Rendering;"We propose a method to allow precise and extremely fast mesh extraction from
3D Gaussian Splatting. Gaussian Splatting has recently become very popular as
it yields realistic rendering while being significantly faster to train than
NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D
gaussians as these gaussians tend to be unorganized after optimization and no
method has been proposed so far. Our first key contribution is a regularization
term that encourages the gaussians to align well with the surface of the scene.
We then introduce a method that exploits this alignment to extract a mesh from
the Gaussians using Poisson reconstruction, which is fast, scalable, and
preserves details, in contrast to the Marching Cubes algorithm usually applied
to extract meshes from Neural SDFs. Finally, we introduce an optional
refinement strategy that binds gaussians to the surface of the mesh, and
jointly optimizes these Gaussians and the mesh through Gaussian splatting
rendering. This enables easy editing, sculpting, rigging, animating,
compositing and relighting of the Gaussians using traditional softwares by
manipulating the mesh instead of the gaussians themselves. Retrieving such an
editable mesh for realistic rendering is done within minutes with our method,
compared to hours with the state-of-the-art methods on neural SDFs, while
providing a better rendering quality. Our project page is the following:
https://anttwo.github.io/sugar/";Antoine Guédon<author:sep>Vincent Lepetit;http://arxiv.org/pdf/2311.12775v3;cs.GR;"We identified a minor typographical error in Equation 6; We updated
  the paper accordingly. Project Webpage: https://anttwo.github.io/sugar/";gaussian splatting<tag:sep>nerf
2311.12490v1;http://arxiv.org/abs/2311.12490v1;2023-11-21;Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields;"Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity
scene reconstruction for novel view synthesis. However, NeRF requires hundreds
of network evaluations per pixel to approximate a volume rendering integral,
making it slow to train. Caching NeRFs into explicit data structures can
effectively enhance rendering speed but at the cost of higher memory usage. To
address these issues, we present Hyb-NeRF, a novel neural radiance field with a
multi-resolution hybrid encoding that achieves efficient neural modeling and
fast rendering, which also allows for high-quality novel view synthesis. The
key idea of Hyb-NeRF is to represent the scene using different encoding
strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits
memory-efficiency learnable positional features at coarse resolutions and the
fast optimization speed and local details of hash-based feature grids at fine
resolutions. In addition, to further boost performance, we embed cone
tracing-based features in our learnable positional encoding that eliminates
encoding ambiguity and reduces aliasing artifacts. Extensive experiments on
both synthetic and real-world datasets show that Hyb-NeRF achieves faster
rendering speed with better rending quality and even a lower memory footprint
in comparison to previous state-of-the-art methods.";Yifan Wang<author:sep>Yi Gong<author:sep>Yuan Zeng;http://arxiv.org/pdf/2311.12490v1;cs.CV;WACV2024;nerf
2311.11700v3;http://arxiv.org/abs/2311.11700v3;2023-11-20;GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting;"In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D
Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
system. It facilitates a better balance between efficiency and accuracy.
Compared to recent SLAM methods employing neural implicit representations, our
method utilizes a real-time differentiable splatting rendering pipeline that
offers significant speedup to map optimization and RGB-D re-rendering.
Specifically, we propose an adaptive expansion strategy that adds new or
deletes noisy 3D Gaussian in order to efficiently reconstruct new observed
scene geometry and improve the mapping of previously observed areas. This
strategy is essential to extend 3D Gaussian representation to reconstruct the
whole scene rather than synthesize a static object in existing methods.
Moreover, in the pose tracking process, an effective coarse-to-fine technique
is designed to select reliable 3D Gaussian representations to optimize camera
pose, resulting in runtime reduction and robust estimation. Our method achieves
competitive performance compared with existing state-of-the-art real-time
methods on the Replica, TUM-RGBD datasets. The source code will be released
soon.";Chi Yan<author:sep>Delin Qu<author:sep>Dong Wang<author:sep>Dan Xu<author:sep>Zhigang Wang<author:sep>Bin Zhao<author:sep>Xuelong Li;http://arxiv.org/pdf/2311.11700v3;cs.CV;;gaussian splatting
2311.11845v1;http://arxiv.org/abs/2311.11845v1;2023-11-20;Entangled View-Epipolar Information Aggregation for Generalizable Neural  Radiance Fields;"Generalizable NeRF can directly synthesize novel views across new scenes,
eliminating the need for scene-specific retraining in vanilla NeRF. A critical
enabling factor in these approaches is the extraction of a generalizable 3D
representation by aggregating source-view features. In this paper, we propose
an Entangled View-Epipolar Information Aggregation method dubbed EVE-NeRF.
Different from existing methods that consider cross-view and along-epipolar
information independently, EVE-NeRF conducts the view-epipolar feature
aggregation in an entangled manner by injecting the scene-invariant appearance
continuity and geometry consistency priors to the aggregation process. Our
approach effectively mitigates the potential lack of inherent geometric and
appearance constraint resulting from one-dimensional interactions, thus further
boosting the 3D representation generalizablity. EVE-NeRF attains
state-of-the-art performance across various evaluation scenarios. Extensive
experiments demonstate that, compared to prevailing single-dimensional
aggregation, the entangled network excels in the accuracy of 3D scene geometry
and appearance reconstruction.Our project page is
https://github.com/tatakai1/EVENeRF.";Zhiyuan Min<author:sep>Yawei Luo<author:sep>Wei Yang<author:sep>Yuesong Wang<author:sep>Yi Yang;http://arxiv.org/pdf/2311.11845v1;cs.CV;;nerf
2311.11863v1;http://arxiv.org/abs/2311.11863v1;2023-11-20;GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene  Understanding;"Applying NeRF to downstream perception tasks for scene understanding and
representation is becoming increasingly popular. Most existing methods treat
semantic prediction as an additional rendering task, \textit{i.e.}, the ""label
rendering"" task, to build semantic NeRFs. However, by rendering
semantic/instance labels per pixel without considering the contextual
information of the rendered image, these methods usually suffer from unclear
boundary segmentation and abnormal segmentation of pixels within an object. To
solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel
pipeline that makes the widely used segmentation model and NeRF work compatibly
under a unified framework, for facilitating context-aware 3D scene perception.
To accomplish this goal, we introduce transformers to aggregate radiance as
well as semantic embedding fields jointly for novel views and facilitate the
joint volumetric rendering of both fields. In addition, we propose two
self-distillation mechanisms, i.e., the Semantic Distill Loss and the
Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality
of the semantic field and the maintenance of geometric consistency. In
evaluation, we conduct experimental comparisons under two perception tasks
(\textit{i.e.} semantic and instance segmentation) using both synthetic and
real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\%,
11.76\%, and 8.47\% on generalized semantic segmentation, finetuning semantic
segmentation, and instance segmentation, respectively.";Hao Li<author:sep>Dingwen Zhang<author:sep>Yalun Dai<author:sep>Nian Liu<author:sep>Lechao Cheng<author:sep>Jingfeng Li<author:sep>Jingdong Wang<author:sep>Junwei Han;http://arxiv.org/pdf/2311.11863v1;cs.CV;;nerf
2311.11284v3;http://arxiv.org/abs/2311.11284v3;2023-11-19;LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval  Score Matching;"The recent advancements in text-to-3D generation mark a significant milestone
in generative models, unlocking new possibilities for creating imaginative 3D
assets across various real-world scenarios. While recent advancements in
text-to-3D generation have shown promise, they often fall short in rendering
detailed and high-quality 3D models. This problem is especially prevalent as
many methods base themselves on Score Distillation Sampling (SDS). This paper
identifies a notable deficiency in SDS, that it brings inconsistent and
low-quality updating direction for the 3D model, causing the over-smoothing
effect. To address this, we propose a novel approach called Interval Score
Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes
interval-based score matching to counteract over-smoothing. Furthermore, we
incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline.
Extensive experiments show that our model largely outperforms the
state-of-the-art in quality and training efficiency.";Yixun Liang<author:sep>Xin Yang<author:sep>Jiantao Lin<author:sep>Haodong Li<author:sep>Xiaogang Xu<author:sep>Yingcong Chen;http://arxiv.org/pdf/2311.11284v3;cs.CV;"The first two authors contributed equally to this work. Our code will
  be available at: https://github.com/EnVision-Research/LucidDreamer";gaussian splatting
2311.11221v1;http://arxiv.org/abs/2311.11221v1;2023-11-19;GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion  Probabilistic Models with Structured Noise;"Text-to-3D, known for its efficient generation methods and expansive creative
potential, has garnered significant attention in the AIGC domain. However, the
amalgamation of Nerf and 2D diffusion models frequently yields oversaturated
images, posing severe limitations on downstream industrial applications due to
the constraints of pixelwise rendering method. Gaussian splatting has recently
superseded the traditional pointwise sampling technique prevalent in NeRF-based
methodologies, revolutionizing various aspects of 3D reconstruction. This paper
introduces a novel text to 3D content generation framework based on Gaussian
splatting, enabling fine control over image saturation through individual
Gaussian sphere transparencies, thereby producing more realistic images. The
challenge of achieving multi-view consistency in 3D generation significantly
impedes modeling complexity and accuracy. Taking inspiration from SJC, we
explore employing multi-view noise distributions to perturb images generated by
3D Gaussian splatting, aiming to rectify inconsistencies in multi-view
geometry. We ingeniously devise an efficient method to generate noise that
produces Gaussian noise from diverse viewpoints, all originating from a shared
noise source. Furthermore, vanilla 3D Gaussian-based generation tends to trap
models in local minima, causing artifacts like floaters, burrs, or
proliferative elements. To mitigate these issues, we propose the variational
Gaussian splatting technique to enhance the quality and stability of 3D
appearance. To our knowledge, our approach represents the first comprehensive
utilization of Gaussian splatting across the entire spectrum of 3D content
generation processes.";Xinhai Li<author:sep>Huaibin Wang<author:sep>Kuo-Kun Tseng;http://arxiv.org/pdf/2311.11221v1;cs.CV;;gaussian splatting<tag:sep>nerf
2311.10959v1;http://arxiv.org/abs/2311.10959v1;2023-11-18;Structure-Aware Sparse-View X-ray 3D Reconstruction;"X-ray, known for its ability to reveal internal structures of objects, is
expected to provide richer information for 3D reconstruction than visible
light. Yet, existing neural radiance fields (NeRF) algorithms overlook this
important nature of X-ray, leading to their limitations in capturing structural
contents of imaged objects. In this paper, we propose a framework,
Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view
X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer
(Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal
structures of objects in 3D space by modeling the dependencies within each line
segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray
sampling strategy to extract contextual and geometric information in 2D
projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray
applications. Experiments on X3D show that SAX-NeRF surpasses previous
NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT
reconstruction. Code, models, and data will be released at
https://github.com/caiyuanhao1998/SAX-NeRF";Yuanhao Cai<author:sep>Jiahao Wang<author:sep>Alan Yuille<author:sep>Zongwei Zhou<author:sep>Angtian Wang;http://arxiv.org/pdf/2311.10959v1;eess.IV;;nerf
2311.11016v1;http://arxiv.org/abs/2311.11016v1;2023-11-18;SNI-SLAM: Semantic Neural Implicit SLAM;"We propose SNI-SLAM, a semantic SLAM system utilizing neural implicit
representation, that simultaneously performs accurate semantic mapping,
high-quality surface reconstruction, and robust camera tracking. In this
system, we introduce hierarchical semantic representation to allow multi-level
semantic comprehension for top-down structured semantic mapping of the scene.
In addition, to fully utilize the correlation between multiple attributes of
the environment, we integrate appearance, geometry and semantic features
through cross-attention for feature collaboration. This strategy enables a more
multifaceted understanding of the environment, thereby allowing SNI-SLAM to
remain robust even when single attribute is defective. Then, we design an
internal fusion-based decoder to obtain semantic, RGB, Truncated Signed
Distance Field (TSDF) values from multi-level features for accurate decoding.
Furthermore, we propose a feature loss to update the scene representation at
the feature level. Compared with low-level losses such as RGB loss and depth
loss, our feature loss is capable of guiding the network optimization on a
higher-level. Our SNI-SLAM method demonstrates superior performance over all
recent NeRF-based SLAM methods in terms of mapping and tracking accuracy on
Replica and ScanNet datasets, while also showing excellent capabilities in
accurate semantic segmentation and real-time semantic mapping.";Siting Zhu<author:sep>Guangming Wang<author:sep>Hermann Blum<author:sep>Jiuming Liu<author:sep>Liang Song<author:sep>Marc Pollefeys<author:sep>Hesheng Wang;http://arxiv.org/pdf/2311.11016v1;cs.RO;;nerf
2311.12059v1;http://arxiv.org/abs/2311.12059v1;2023-11-18;Towards Function Space Mesh Watermarking: Protecting the Copyright of  Signed Distance Fields;"The signed distance field (SDF) represents 3D geometries in continuous
function space. Due to its continuous nature, explicit 3D models (e.g., meshes)
can be extracted from it at arbitrary resolution, which means losing the SDF is
equivalent to losing the mesh. Recent research has shown meshes can also be
extracted from SDF-enhanced neural radiance fields (NeRF). Such a signal raises
an alarm that any implicit neural representation with SDF enhancement can
extract the original mesh, which indicates identifying the SDF's intellectual
property becomes an urgent issue. This paper proposes FuncMark, a robust and
invisible watermarking method to protect the copyright of signed distance
fields by leveraging analytic on-surface deformations to embed binary watermark
messages. Such deformation can survive isosurfacing and thus be inherited by
the extracted meshes for further watermark message decoding. Our method can
recover the message with high-resolution meshes extracted from SDFs and detect
the watermark even when mesh vertices are extremely sparse. Furthermore, our
method is robust even when various distortions (including remeshing) are
encountered. Extensive experiments demonstrate that our \tool significantly
outperforms state-of-the-art approaches and the message is still detectable
even when only 50 vertex samples are given.";Xingyu Zhu<author:sep>Guanhui Ye<author:sep>Chengdong Dong<author:sep>Xiapu Luo<author:sep>Xuetao Wei;http://arxiv.org/pdf/2311.12059v1;cs.CV;;nerf
2311.10812v1;http://arxiv.org/abs/2311.10812v1;2023-11-17;SplatArmor: Articulated Gaussian splatting for animatable humans from  monocular RGB videos;"We propose SplatArmor, a novel approach for recovering detailed and
animatable human models by `armoring' a parameterized body model with 3D
Gaussians. Our approach represents the human as a set of 3D Gaussians within a
canonical space, whose articulation is defined by extending the skinning of the
underlying SMPL geometry to arbitrary locations in the canonical space. To
account for pose-dependent effects, we introduce a SE(3) field, which allows us
to capture both the location and anisotropy of the Gaussians. Furthermore, we
propose the use of a neural color field to provide color regularization and 3D
supervision for the precise positioning of these Gaussians. We show that
Gaussian splatting provides an interesting alternative to neural rendering
based methods by leverging a rasterization primitive without facing any of the
non-differentiability and optimization challenges typically faced in such
approaches. The rasterization paradigms allows us to leverage forward skinning,
and does not suffer from the ambiguities associated with inverse skinning and
warping. We show compelling results on the ZJU MoCap and People Snapshot
datasets, which underscore the effectiveness of our method for controllable
human synthesis.";Rohit Jena<author:sep>Ganesh Subramanian Iyer<author:sep>Siddharth Choudhary<author:sep>Brandon Smith<author:sep>Pratik Chaudhari<author:sep>James Gee;http://arxiv.org/pdf/2311.10812v1;cs.CV;;gaussian splatting
2311.10523v1;http://arxiv.org/abs/2311.10523v1;2023-11-17;Removing Adverse Volumetric Effects From Trained Neural Radiance Fields;"While the use of neural radiance fields (NeRFs) in different challenging
settings has been explored, only very recently have there been any
contributions that focus on the use of NeRF in foggy environments. We argue
that the traditional NeRF models are able to replicate scenes filled with fog
and propose a method to remove the fog when synthesizing novel views. By
calculating the global contrast of a scene, we can estimate a density threshold
that, when applied, removes all visible fog. This makes it possible to use NeRF
as a way of rendering clear views of objects of interest located in fog-filled
environments. Additionally, to benchmark performance on such scenes, we
introduce a new dataset that expands some of the original synthetic NeRF scenes
through the addition of fog and natural environments. The code, dataset, and
video results can be found on our project page: https://vegardskui.com/fognerf/";Andreas L. Teigen<author:sep>Mauhing Yip<author:sep>Victor P. Hamran<author:sep>Vegard Skui<author:sep>Annette Stahl<author:sep>Rudolf Mester;http://arxiv.org/pdf/2311.10523v1;cs.CV;"This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible";nerf
2401.02436v1;http://arxiv.org/abs/2401.02436v1;2023-11-17;Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis;"Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian
splat representation has been introduced for novel view synthesis from sparse
image sets. Making such representations suitable for applications like network
streaming and rendering on low-power devices requires significantly reduced
memory consumption as well as improved rendering efficiency. We propose a
compressed 3D Gaussian splat representation that utilizes sensitivity-aware
vector clustering with quantization-aware training to compress directional
colors and Gaussian parameters. The learned codebooks have low bitrates and
achieve a compression rate of up to $31\times$ on real-world scenes with only
minimal degradation of visual quality. We demonstrate that the compressed splat
representation can be efficiently rendered with hardware rasterization on
lightweight GPUs at up to $4\times$ higher framerates than reported via an
optimized GPU compute pipeline. Extensive experiments across multiple datasets
demonstrate the robustness and rendering speed of the proposed approach.";Simon Niedermayr<author:sep>Josef Stumpfegger<author:sep>Rüdiger Westermann;http://arxiv.org/pdf/2401.02436v1;cs.CV;;gaussian splatting
2311.09646v1;http://arxiv.org/abs/2311.09646v1;2023-11-16;Reconstructing Continuous Light Field From Single Coded Image;"We propose a method for reconstructing a continuous light field of a target
scene from a single observed image. Our method takes the best of two worlds:
joint aperture-exposure coding for compressive light-field acquisition, and a
neural radiance field (NeRF) for view synthesis. Joint aperture-exposure coding
implemented in a camera enables effective embedding of 3-D scene information
into an observed image, but in previous works, it was used only for
reconstructing discretized light-field views. NeRF-based neural rendering
enables high quality view synthesis of a 3-D scene from continuous viewpoints,
but when only a single image is given as the input, it struggles to achieve
satisfactory quality. Our method integrates these two techniques into an
efficient and end-to-end trainable pipeline. Trained on a wide variety of
scenes, our method can reconstruct continuous light fields accurately and
efficiently without any test time optimization. To our knowledge, this is the
first work to bridge two worlds: camera design for efficiently acquiring 3-D
information and neural rendering.";Yuya Ishikawa<author:sep>Keita Takahashi<author:sep>Chihiro Tsutake<author:sep>Toshiaki Fujii;http://arxiv.org/pdf/2311.09646v1;cs.CV;;nerf
2311.09806v2;http://arxiv.org/abs/2311.09806v2;2023-11-16;EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction  on Mobile Devices;"Reconstructing real-world 3D objects has numerous applications in computer
vision, such as virtual reality, video games, and animations. Ideally, 3D
reconstruction methods should generate high-fidelity results with 3D
consistency in real-time. Traditional methods match pixels between images using
photo-consistency constraints or learned features, while differentiable
rendering methods like Neural Radiance Fields (NeRF) use differentiable volume
rendering or surface-based representation to generate high-fidelity scenes.
However, these methods require excessive runtime for rendering, making them
impractical for daily applications. To address these challenges, we present
$\textbf{EvaSurf}$, an $\textbf{E}$fficient $\textbf{V}$iew-$\textbf{A}$ware
implicit textured $\textbf{Surf}$ace reconstruction method on mobile devices.
In our method, we first employ an efficient surface-based model with a
multi-view supervision module to ensure accurate mesh reconstruction. To enable
high-fidelity rendering, we learn an implicit texture embedded with a set of
Gaussian lobes to capture view-dependent information. Furthermore, with the
explicit geometry and the implicit texture, we can employ a lightweight neural
shader to reduce the expense of computation and further support real-time
rendering on common mobile devices. Extensive experiments demonstrate that our
method can reconstruct high-quality appearance and accurate mesh on both
synthetic and real-world datasets. Moreover, our method can be trained in just
1-2 hours using a single GPU and run on mobile devices at over 40 FPS (Frames
Per Second), with a final package required for rendering taking up only 40-50
MB.";Jingnan Gao<author:sep>Zhuo Chen<author:sep>Yichao Yan<author:sep>Bowen Pan<author:sep>Zhe Wang<author:sep>Jiangjing Lyu<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2311.09806v2;cs.CV;Project Page: http://g-1nonly.github.io/EvaSurf-Website/;nerf
2311.10091v1;http://arxiv.org/abs/2311.10091v1;2023-11-16;Adaptive Shells for Efficient Neural Radiance Field Rendering;"Neural radiance fields achieve unprecedented quality for novel view
synthesis, but their volumetric formulation remains expensive, requiring a huge
number of samples to render high-resolution images. Volumetric encodings are
essential to represent fuzzy geometry such as foliage and hair, and they are
well-suited for stochastic optimization. Yet, many scenes ultimately consist
largely of solid surfaces which can be accurately rendered by a single sample
per pixel. Based on this insight, we propose a neural radiance formulation that
smoothly transitions between volumetric- and surface-based rendering, greatly
accelerating rendering speed and even improving visual fidelity. Our method
constructs an explicit mesh envelope which spatially bounds a neural volumetric
representation. In solid regions, the envelope nearly converges to a surface
and can often be rendered with a single sample. To this end, we generalize the
NeuS formulation with a learned spatially-varying kernel size which encodes the
spread of the density, fitting a wide kernel to volume-like regions and a tight
kernel to surface-like regions. We then extract an explicit mesh of a narrow
band around the surface, with width determined by the kernel size, and
fine-tune the radiance field within this band. At inference time, we cast rays
against the mesh and evaluate the radiance field only within the enclosed
region, greatly reducing the number of samples required. Experiments show that
our approach enables efficient rendering at very high fidelity. We also
demonstrate that the extracted envelope enables downstream applications such as
animation and simulation.";Zian Wang<author:sep>Tianchang Shen<author:sep>Merlin Nimier-David<author:sep>Nicholas Sharp<author:sep>Jun Gao<author:sep>Alexander Keller<author:sep>Sanja Fidler<author:sep>Thomas Müller<author:sep>Zan Gojcic;http://arxiv.org/pdf/2311.10091v1;cs.CV;"SIGGRAPH Asia 2023. Project page:
  research.nvidia.com/labs/toronto-ai/adaptive-shells/";
2311.09221v1;http://arxiv.org/abs/2311.09221v1;2023-11-15;Single-Image 3D Human Digitization with Shape-Guided Diffusion;"We present an approach to generate a 360-degree view of a person with a
consistent, high-resolution appearance from a single input image. NeRF and its
variants typically require videos or images from different viewpoints. Most
existing approaches taking monocular input either rely on ground-truth 3D scans
for supervision or lack 3D consistency. While recent 3D generative models show
promise of 3D consistent human digitization, these approaches do not generalize
well to diverse clothing appearances, and the results lack photorealism. Unlike
existing work, we utilize high-capacity 2D diffusion models pretrained for
general image synthesis tasks as an appearance prior of clothed humans. To
achieve better 3D consistency while retaining the input identity, we
progressively synthesize multiple views of the human in the input image by
inpainting missing regions with shape-guided diffusion conditioned on
silhouette and surface normal. We then fuse these synthesized multi-view images
via inverse rendering to obtain a fully textured high-resolution 3D mesh of the
given person. Experiments show that our approach outperforms prior methods and
achieves photorealistic 360-degree synthesis of a wide range of clothed humans
with complex textures from a single image.";Badour AlBahar<author:sep>Shunsuke Saito<author:sep>Hung-Yu Tseng<author:sep>Changil Kim<author:sep>Johannes Kopf<author:sep>Jia-Bin Huang;http://arxiv.org/pdf/2311.09221v1;cs.CV;SIGGRAPH Asia 2023. Project website: https://human-sgd.github.io/;nerf
2311.09217v1;http://arxiv.org/abs/2311.09217v1;2023-11-15;DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction  Model;"We propose \textbf{DMV3D}, a novel 3D generation approach that uses a
transformer-based 3D large reconstruction model to denoise multi-view
diffusion. Our reconstruction model incorporates a triplane NeRF representation
and can denoise noisy multi-view images via NeRF reconstruction and rendering,
achieving single-stage 3D generation in $\sim$30s on single A100 GPU. We train
\textbf{DMV3D} on large-scale multi-view image datasets of highly diverse
objects using only image reconstruction losses, without accessing 3D assets. We
demonstrate state-of-the-art results for the single-image reconstruction
problem where probabilistic modeling of unseen object parts is required for
generating diverse reconstructions with sharp textures. We also show
high-quality text-to-3D generation results outperforming previous 3D diffusion
models. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .";Yinghao Xu<author:sep>Hao Tan<author:sep>Fujun Luan<author:sep>Sai Bi<author:sep>Peng Wang<author:sep>Jiahao Li<author:sep>Zifan Shi<author:sep>Kalyan Sunkavalli<author:sep>Gordon Wetzstein<author:sep>Zexiang Xu<author:sep>Kai Zhang;http://arxiv.org/pdf/2311.09217v1;cs.CV;Project Page: https://justimyhxu.github.io/projects/dmv3d/;nerf
2311.09077v2;http://arxiv.org/abs/2311.09077v2;2023-11-15;Spiking NeRF: Representing the Real-World Geometry by a Discontinuous  Representation;"A crucial reason for the success of existing NeRF-based methods is to build a
neural density field for the geometry representation via multiple perceptron
layers (MLPs). MLPs are continuous functions, however, real geometry or density
field is frequently discontinuous at the interface between the air and the
surface. Such a contrary brings the problem of unfaithful geometry
representation. To this end, this paper proposes spiking NeRF, which leverages
spiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural
Network (SNN) framework to build a discontinuous density field for faithful
geometry representation. Specifically, we first demonstrate the reason why
continuous density fields will bring inaccuracy. Then, we propose to use the
spiking neurons to build a discontinuous density field. We conduct a
comprehensive analysis for the problem of existing spiking neuron models and
then provide the numerical relationship between the parameter of the spiking
neuron and the theoretical accuracy of geometry. Based on this, we propose a
bounded spiking neuron to build the discontinuous density field. Our method
achieves SOTA performance. The source code and the supplementary material are
available at https://github.com/liaozhanfeng/Spiking-NeRF.";Zhanfeng Liao<author:sep>Qian Zheng<author:sep>Yan Liu<author:sep>Gang Pan;http://arxiv.org/pdf/2311.09077v2;cs.CV;;nerf
2311.08581v1;http://arxiv.org/abs/2311.08581v1;2023-11-14;Drivable 3D Gaussian Avatars;"We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable
model for human bodies rendered with Gaussian splats. Current photorealistic
drivable avatars require either accurate 3D registrations during training,
dense input images during testing, or both. The ones based on neural radiance
fields also tend to be prohibitively slow for telepresence applications. This
work uses the recently presented 3D Gaussian Splatting (3DGS) technique to
render realistic humans at real-time framerates, using dense calibrated
multi-view videos as input. To deform those primitives, we depart from the
commonly used point deformation method of linear blend skinning (LBS) and use a
classic volumetric deformation method: cage deformations. Given their smaller
size, we drive these deformations with joint angles and keypoints, which are
more suitable for communication applications. Our experiments on nine subjects
with varied body shapes, clothes, and motions obtain higher-quality results
than state-of-the-art methods when using the same training and test data.";Wojciech Zielonka<author:sep>Timur Bagautdinov<author:sep>Shunsuke Saito<author:sep>Michael Zollhöfer<author:sep>Justus Thies<author:sep>Javier Romero;http://arxiv.org/pdf/2311.08581v1;cs.CV;Website: https://zielon.github.io/d3ga/;gaussian splatting
2311.07044v1;http://arxiv.org/abs/2311.07044v1;2023-11-13;$L_0$-Sampler: An $L_{0}$ Model Guided Volume Sampling for NeRF;"Since being proposed, Neural Radiance Fields (NeRF) have achieved great
success in related tasks, mainly adopting the hierarchical volume sampling
(HVS) strategy for volume rendering. However, the HVS of NeRF approximates
distributions using piecewise constant functions, which provides a relatively
rough estimation. Based on the observation that a well-trained weight function
$w(t)$ and the $L_0$ distance between points and the surface have very high
similarity, we propose $L_0$-Sampler by incorporating the $L_0$ model into
$w(t)$ to guide the sampling process. Specifically, we propose to use piecewise
exponential functions rather than piecewise constant functions for
interpolation, which can not only approximate quasi-$L_0$ weight distributions
along rays quite well but also can be easily implemented with few lines of code
without additional computational burden. Stable performance improvements can be
achieved by applying $L_0$-Sampler to NeRF and its related tasks like 3D
reconstruction. Code is available at https://ustc3dv.github.io/L0-Sampler/ .";Liangchen Li<author:sep>Juyong Zhang;http://arxiv.org/pdf/2311.07044v1;cs.CV;Project page: https://ustc3dv.github.io/L0-Sampler/;nerf
2311.06455v1;http://arxiv.org/abs/2311.06455v1;2023-11-11;Aria-NeRF: Multimodal Egocentric View Synthesis;"We seek to accelerate research in developing rich, multimodal scene models
trained from egocentric data, based on differentiable volumetric ray-tracing
inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like
model from an egocentric image sequence plays a pivotal role in understanding
human behavior and holds diverse applications within the realms of VR/AR. Such
egocentric NeRF-like models may be used as realistic simulations, contributing
significantly to the advancement of intelligent agents capable of executing
tasks in the real-world. The future of egocentric view synthesis may lead to
novel environment representations going beyond today's NeRFs by augmenting
visual data with multimodal sensors such as IMU for egomotion tracking, audio
sensors to capture surface texture and human language context, and eye-gaze
trackers to infer human attention patterns in the scene. To support and
facilitate the development and evaluation of egocentric multimodal scene
modeling, we present a comprehensive multimodal egocentric video dataset. This
dataset offers a comprehensive collection of sensory data, featuring RGB
images, eye-tracking camera footage, audio recordings from a microphone,
atmospheric pressure readings from a barometer, positional coordinates from
GPS, connectivity details from Wi-Fi and Bluetooth, and information from
dual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The
dataset was collected with the Meta Aria Glasses wearable device platform. The
diverse data modalities and the real-world context captured within this dataset
serve as a robust foundation for furthering our understanding of human behavior
and enabling more immersive and intelligent experiences in the realms of VR,
AR, and robotics.";Jiankai Sun<author:sep>Jianing Qiu<author:sep>Chuanyang Zheng<author:sep>John Tucker<author:sep>Javier Yu<author:sep>Mac Schwager;http://arxiv.org/pdf/2311.06455v1;cs.CV;;nerf
2311.06214v2;http://arxiv.org/abs/2311.06214v2;2023-11-10;Instant3D: Fast Text-to-3D with Sparse-View Generation and Large  Reconstruction Model;"Text-to-3D with diffusion models has achieved remarkable progress in recent
years. However, existing methods either rely on score distillation-based
optimization which suffer from slow inference, low diversity and Janus
problems, or are feed-forward methods that generate low-quality results due to
the scarcity of 3D training data. In this paper, we propose Instant3D, a novel
method that generates high-quality and diverse 3D assets from text prompts in a
feed-forward manner. We adopt a two-stage paradigm, which first generates a
sparse set of four structured and consistent views from text in one shot with a
fine-tuned 2D text-to-image diffusion model, and then directly regresses the
NeRF from the generated images with a novel transformer-based sparse-view
reconstructor. Through extensive experiments, we demonstrate that our method
can generate diverse 3D assets of high visual quality within 20 seconds, which
is two orders of magnitude faster than previous optimization-based methods that
can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.";Jiahao Li<author:sep>Hao Tan<author:sep>Kai Zhang<author:sep>Zexiang Xu<author:sep>Fujun Luan<author:sep>Yinghao Xu<author:sep>Yicong Hong<author:sep>Kalyan Sunkavalli<author:sep>Greg Shakhnarovich<author:sep>Sai Bi;http://arxiv.org/pdf/2311.06214v2;cs.CV;Project webpage: https://jiahao.ai/instant3d/;nerf
2311.05958v1;http://arxiv.org/abs/2311.05958v1;2023-11-10;A Neural Height-Map Approach for the Binocular Photometric Stereo  Problem;"In this work we propose a novel, highly practical, binocular photometric
stereo (PS) framework, which has same acquisition speed as single view PS,
however significantly improves the quality of the estimated geometry.
  As in recent neural multi-view shape estimation frameworks such as NeRF,
SIREN and inverse graphics approaches to multi-view photometric stereo (e.g.
PS-NeRF) we formulate shape estimation task as learning of a differentiable
surface and texture representation by minimising surface normal discrepancy for
normals estimated from multiple varying light images for two views as well as
discrepancy between rendered surface intensity and observed images. Our method
differs from typical multi-view shape estimation approaches in two key ways.
First, our surface is represented not as a volume but as a neural heightmap
where heights of points on a surface are computed by a deep neural network.
Second, instead of predicting an average intensity as PS-NeRF or introducing
lambertian material assumptions as Guo et al., we use a learnt BRDF and perform
near-field per point intensity rendering.
  Our method achieves the state-of-the-art performance on the DiLiGenT-MV
dataset adapted to binocular stereo setup as well as a new binocular
photometric stereo dataset - LUCES-ST.";Fotios Logothetis<author:sep>Ignas Budvytis<author:sep>Roberto Cipolla;http://arxiv.org/pdf/2311.05958v1;cs.CV;WACV 2024;nerf
2311.05836v4;http://arxiv.org/abs/2311.05836v4;2023-11-10;UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical  Neural Radiance Fields;"In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.";Jing Hu<author:sep>Qinrui Fan<author:sep>Shu Hu<author:sep>Siwei Lyu<author:sep>Xi Wu<author:sep>Xin Wang;http://arxiv.org/pdf/2311.05836v4;eess.IV;;nerf
2311.06211v1;http://arxiv.org/abs/2311.06211v1;2023-11-10;ASSIST: Interactive Scene Nodes for Scalable and Realistic Indoor  Simulation;"We present ASSIST, an object-wise neural radiance field as a panoptic
representation for compositional and realistic simulation. Central to our
approach is a novel scene node data structure that stores the information of
each object in a unified fashion, allowing online interaction in both intra-
and cross-scene settings. By incorporating a differentiable neural network
along with the associated bounding box and semantic features, the proposed
structure guarantees user-friendly interaction on independent objects to scale
up novel view simulation. Objects in the scene can be queried, added,
duplicated, deleted, transformed, or swapped simply through mouse/keyboard
controls or language instructions. Experiments demonstrate the efficacy of the
proposed method, where scaled realistic simulation can be achieved through
interactive editing and compositional rendering, with color images, depth
images, and panoptic segmentation masks generated in a 3D consistent manner.";Zhide Zhong<author:sep>Jiakai Cao<author:sep>Songen Gu<author:sep>Sirui Xie<author:sep>Weibo Gao<author:sep>Liyi Luo<author:sep>Zike Yan<author:sep>Hao Zhao<author:sep>Guyue Zhou;http://arxiv.org/pdf/2311.06211v1;cs.CV;;
2311.05461v1;http://arxiv.org/abs/2311.05461v1;2023-11-09;Control3D: Towards Controllable Text-to-3D Generation;"Recent remarkable advances in large-scale text-to-image diffusion models have
inspired a significant breakthrough in text-to-3D generation, pursuing 3D
content creation solely from a given text prompt. However, existing text-to-3D
techniques lack a crucial ability in the creative process: interactively
control and shape the synthetic 3D contents according to users' desired
specifications (e.g., sketch). To alleviate this issue, we present the first
attempt for text-to-3D generation conditioning on the additional hand-drawn
sketch, namely Control3D, which enhances controllability for users. In
particular, a 2D conditioned diffusion model (ControlNet) is remoulded to guide
the learning of 3D scene parameterized as NeRF, encouraging each view of 3D
scene aligned with the given text prompt and hand-drawn sketch. Moreover, we
exploit a pre-trained differentiable photo-to-sketch model to directly estimate
the sketch of the rendered image over synthetic 3D scene. Such estimated sketch
along with each sampled view is further enforced to be geometrically consistent
with the given sketch, pursuing better controllable text-to-3D generation.
Through extensive experiments, we demonstrate that our proposal can generate
accurate and faithful 3D scenes that align closely with the input text prompts
and sketches.";Yang Chen<author:sep>Yingwei Pan<author:sep>Yehao Li<author:sep>Ting Yao<author:sep>Tao Mei;http://arxiv.org/pdf/2311.05461v1;cs.CV;ACM Multimedia 2023;nerf
2311.05230v1;http://arxiv.org/abs/2311.05230v1;2023-11-09;ConRad: Image Constrained Radiance Fields for 3D Generation from a  Single Image;"We present a novel method for reconstructing 3D objects from a single RGB
image. Our method leverages the latest image generation models to infer the
hidden 3D structure while remaining faithful to the input image. While existing
methods obtain impressive results in generating 3D models from text prompts,
they do not provide an easy approach for conditioning on input RGB data.
Na\""ive extensions of these methods often lead to improper alignment in
appearance between the input image and the 3D reconstructions. We address these
challenges by introducing Image Constrained Radiance Fields (ConRad), a novel
variant of neural radiance fields. ConRad is an efficient 3D representation
that explicitly captures the appearance of an input image in one viewpoint. We
propose a training algorithm that leverages the single RGB image in conjunction
with pretrained Diffusion Models to optimize the parameters of a ConRad
representation. Extensive experiments show that ConRad representations can
simplify preservation of image details while producing a realistic 3D
reconstruction. Compared to existing state-of-the-art baselines, we show that
our 3D reconstructions remain more faithful to the input and produce more
consistent 3D models while demonstrating significantly improved quantitative
performance on a ShapeNet object benchmark.";Senthil Purushwalkam<author:sep>Nikhil Naik;http://arxiv.org/pdf/2311.05230v1;cs.CV;Advances in Neural Information Processing Systems (NeurIPS 2023);
2311.05521v2;http://arxiv.org/abs/2311.05521v2;2023-11-09;BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis;"Synthesizing photorealistic 4D human head avatars from videos is essential
for VR/AR, telepresence, and video game applications. Although existing Neural
Radiance Fields (NeRF)-based methods achieve high-fidelity results, the
computational expense limits their use in real-time applications. To overcome
this limitation, we introduce BakedAvatar, a novel representation for real-time
neural head avatar synthesis, deployable in a standard polygon rasterization
pipeline. Our approach extracts deformable multi-layer meshes from learned
isosurfaces of the head and computes expression-, pose-, and view-dependent
appearances that can be baked into static textures for efficient rasterization.
We thus propose a three-stage pipeline for neural head avatar synthesis, which
includes learning continuous deformation, manifold, and radiance fields,
extracting layered meshes and textures, and fine-tuning texture details with
differential rasterization. Experimental results demonstrate that our
representation generates synthesis results of comparable quality to other
state-of-the-art methods while significantly reducing the inference time
required. We further showcase various head avatar synthesis results from
monocular videos, including view synthesis, face reenactment, expression
editing, and pose editing, all at interactive frame rates.";Hao-Bin Duan<author:sep>Miao Wang<author:sep>Jin-Chuan Shi<author:sep>Xu-Chuan Chen<author:sep>Yan-Pei Cao;http://arxiv.org/pdf/2311.05521v2;cs.GR;"ACM Transactions on Graphics (SIGGRAPH Asia 2023). Project Page:
  https://buaavrcg.github.io/BakedAvatar";nerf
2311.05289v1;http://arxiv.org/abs/2311.05289v1;2023-11-09;VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for  Enhanced Indoor View Synthesis;"Creating high-quality view synthesis is essential for immersive applications
but continues to be problematic, particularly in indoor environments and for
real-time deployment. Current techniques frequently require extensive
computational time for both training and rendering, and often produce
less-than-ideal 3D representations due to inadequate geometric structuring. To
overcome this, we introduce VoxNeRF, a novel approach that leverages volumetric
representations to enhance the quality and efficiency of indoor view synthesis.
Firstly, VoxNeRF constructs a structured scene geometry and converts it into a
voxel-based representation. We employ multi-resolution hash grids to adaptively
capture spatial features, effectively managing occlusions and the intricate
geometry of indoor scenes. Secondly, we propose a unique voxel-guided efficient
sampling technique. This innovation selectively focuses computational resources
on the most relevant portions of ray segments, substantially reducing
optimization time. We validate our approach against three public indoor
datasets and demonstrate that VoxNeRF outperforms state-of-the-art methods.
Remarkably, it achieves these gains while reducing both training and rendering
times, surpassing even Instant-NGP in speed and bringing the technology closer
to real-time.";Sen Wang<author:sep>Wei Zhang<author:sep>Stefano Gasperini<author:sep>Shun-Cheng Wu<author:sep>Nassir Navab;http://arxiv.org/pdf/2311.05289v1;cs.CV;8 pages, 4 figures;nerf
2311.04521v1;http://arxiv.org/abs/2311.04521v1;2023-11-08;Learning Robust Multi-Scale Representation for Neural Radiance Fields  from Unposed Images;"We introduce an improved solution to the neural image-based rendering problem
in computer vision. Given a set of images taken from a freely moving camera at
train time, the proposed approach could synthesize a realistic image of the
scene from a novel viewpoint at test time. The key ideas presented in this
paper are (i) Recovering accurate camera parameters via a robust pipeline from
unposed day-to-day images is equally crucial in neural novel view synthesis
problem; (ii) It is rather more practical to model object's content at
different resolutions since dramatic camera motion is highly likely in
day-to-day unposed images. To incorporate the key ideas, we leverage the
fundamentals of scene rigidity, multi-scale neural scene representation, and
single-image depth prediction. Concretely, the proposed approach makes the
camera parameters as learnable in a neural fields-based modeling framework. By
assuming per view depth prediction is given up to scale, we constrain the
relative pose between successive frames. From the relative poses, absolute
camera pose estimation is modeled via a graph-neural network-based multiple
motion averaging within the multi-scale neural-fields network, leading to a
single loss function. Optimizing the introduced loss function provides camera
intrinsic, extrinsic, and image rendering from unposed images. We demonstrate,
with examples, that for a unified framework to accurately model multiscale
neural scene representation from day-to-day acquired unposed multi-view images,
it is equally essential to have precise camera-pose estimates within the scene
representation framework. Without considering robustness measures in the camera
pose estimation pipeline, modeling for multi-scale aliasing artifacts can be
counterproductive. We present extensive experiments on several benchmark
datasets to demonstrate the suitability of our approach.";Nishant Jain<author:sep>Suryansh Kumar<author:sep>Luc Van Gool;http://arxiv.org/pdf/2311.04521v1;cs.CV;"Accepted for publication at International Journal of Computer Vision
  (IJCV). Draft info: 22 pages, 12 figures and 14 tables";
2311.04400v1;http://arxiv.org/abs/2311.04400v1;2023-11-08;LRM: Large Reconstruction Model for Single Image to 3D;"We propose the first Large Reconstruction Model (LRM) that predicts the 3D
model of an object from a single input image within just 5 seconds. In contrast
to many previous methods that are trained on small-scale datasets such as
ShapeNet in a category-specific fashion, LRM adopts a highly scalable
transformer-based architecture with 500 million learnable parameters to
directly predict a neural radiance field (NeRF) from the input image. We train
our model in an end-to-end manner on massive multi-view data containing around
1 million objects, including both synthetic renderings from Objaverse and real
captures from MVImgNet. This combination of a high-capacity model and
large-scale training data empowers our model to be highly generalizable and
produce high-quality 3D reconstructions from various testing inputs including
real-world in-the-wild captures and images from generative models. Video demos
and interactable 3D meshes can be found on this website:
https://yiconghong.me/LRM/.";Yicong Hong<author:sep>Kai Zhang<author:sep>Jiuxiang Gu<author:sep>Sai Bi<author:sep>Yang Zhou<author:sep>Difan Liu<author:sep>Feng Liu<author:sep>Kalyan Sunkavalli<author:sep>Trung Bui<author:sep>Hao Tan;http://arxiv.org/pdf/2311.04400v1;cs.CV;23 pages;nerf
2311.04154v1;http://arxiv.org/abs/2311.04154v1;2023-11-07;High-fidelity 3D Reconstruction of Plants using Neural Radiance Field;"Accurate reconstruction of plant phenotypes plays a key role in optimising
sustainable farming practices in the field of Precision Agriculture (PA).
Currently, optical sensor-based approaches dominate the field, but the need for
high-fidelity 3D reconstruction of crops and plants in unstructured
agricultural environments remains challenging. Recently, a promising
development has emerged in the form of Neural Radiance Field (NeRF), a novel
method that utilises neural density fields. This technique has shown impressive
performance in various novel vision synthesis tasks, but has remained
relatively unexplored in the agricultural context. In our study, we focus on
two fundamental tasks within plant phenotyping: (1) the synthesis of 2D
novel-view images and (2) the 3D reconstruction of crop and plant models. We
explore the world of neural radiance fields, in particular two SOTA methods:
Instant-NGP, which excels in generating high-quality images with impressive
training and inference speed, and Instant-NSR, which improves the reconstructed
geometry by incorporating the Signed Distance Function (SDF) during training.
In particular, we present a novel plant phenotype dataset comprising real plant
images from production environments. This dataset is a first-of-its-kind
initiative aimed at comprehensively exploring the advantages and limitations of
NeRF in agricultural contexts. Our experimental results show that NeRF
demonstrates commendable performance in the synthesis of novel-view images and
is able to achieve reconstruction results that are competitive with Reality
Capture, a leading commercial software for 3D Multi-View Stereo (MVS)-based
reconstruction. However, our study also highlights certain drawbacks of NeRF,
including relatively slow training speeds, performance limitations in cases of
insufficient sampling, and challenges in obtaining geometry quality in complex
setups.";Kewei Hu<author:sep>Ying Wei<author:sep>Yaoqiang Pan<author:sep>Hanwen Kang<author:sep>Chao Chen;http://arxiv.org/pdf/2311.04154v1;cs.CV;;nerf
2311.03965v1;http://arxiv.org/abs/2311.03965v1;2023-11-07;Fast Sun-aligned Outdoor Scene Relighting based on TensoRF;"In this work, we introduce our method of outdoor scene relighting for Neural
Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF).
SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun,
thereby achieving a simplified workflow that eliminates the need for
environment maps. Our sun-alignment strategy is motivated by the insight that
shadows, unlike viewpoint-dependent albedo, are determined by light direction.
We directly use the sun direction as an input during shadow generation,
simplifying the requirements of the inference process significantly. Moreover,
SR-TensoRF leverages the training efficiency of TensoRF by incorporating our
proposed cubemap concept, resulting in notable acceleration in both training
and rendering processes compared to existing methods.";Yeonjin Chang<author:sep>Yearim Kim<author:sep>Seunghyeon Seo<author:sep>Jung Yi<author:sep>Nojun Kwak;http://arxiv.org/pdf/2311.03965v1;cs.CV;WACV 2024;nerf
2311.04246v2;http://arxiv.org/abs/2311.04246v2;2023-11-07;ADFactory: An Effective Framework for Generalizing Optical Flow with  Nerf;"A significant challenge facing current optical flow methods is the difficulty
in generalizing them well to the real world. This is mainly due to the high
cost of hand-crafted datasets, and existing self-supervised methods are limited
by indirect loss and occlusions, resulting in fuzzy outcomes. To address this
challenge, we introduce a novel optical flow training framework: automatic data
factory (ADF). ADF only requires RGB images as input to effectively train the
optical flow network on the target data domain. Specifically, we use advanced
Nerf technology to reconstruct scenes from photo groups collected by a
monocular camera, and then calculate optical flow labels between camera pose
pairs based on the rendering results. To eliminate erroneous labels caused by
defects in the scene reconstructed by Nerf, we screened the generated labels
from multiple aspects, such as optical flow matching accuracy, radiation field
confidence, and depth consistency. The filtered labels can be directly used for
network supervision. Experimentally, the generalization ability of ADF on KITTI
surpasses existing self-supervised optical flow and monocular scene flow
algorithms. In addition, ADF achieves impressive results in real-world
zero-point generalization evaluations and surpasses most supervised methods.";Han Ling;http://arxiv.org/pdf/2311.04246v2;cs.CV;8 pages;nerf
2311.03784v2;http://arxiv.org/abs/2311.03784v2;2023-11-07;UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields;"Neural Radiance Field (NeRF) has enabled novel view synthesis with high
fidelity given images and camera poses. Subsequent works even succeeded in
eliminating the necessity of pose priors by jointly optimizing NeRF and camera
pose. However, these works are limited to relatively simple settings such as
photometrically consistent and occluder-free image collections or a sequence of
images from a video. So they have difficulty handling unconstrained images with
varying illumination and transient occluders. In this paper, we propose
$\textbf{UP-NeRF}$ ($\textbf{U}$nconstrained $\textbf{P}$ose-prior-free
$\textbf{Ne}$ural $\textbf{R}$adiance $\textbf{F}$ields) to optimize NeRF with
unconstrained image collections without camera pose prior. We tackle these
challenges with surrogate tasks that optimize color-insensitive feature fields
and a separate module for transient occluders to block their influence on pose
estimation. In addition, we introduce a candidate head to enable more robust
pose estimation and transient-aware depth supervision to minimize the effect of
incorrect prior. Our experiments verify the superior performance of our method
compared to the baselines including BARF and its variants in a challenging
internet photo collection, $\textit{Phototourism}$ dataset.";Injae Kim<author:sep>Minhyuk Choi<author:sep>Hyunwoo J. Kim;http://arxiv.org/pdf/2311.03784v2;cs.CV;"Neural Information Processing Systems (NeurIPS), 2023. The code is
  available at https://github.com/mlvlab/UP-NeRF";nerf
2311.02848v1;http://arxiv.org/abs/2311.02848v1;2023-11-06;Consistent4D: Consistent 360° Dynamic Object Generation from  Monocular Video;"In this paper, we present Consistent4D, a novel approach for generating 4D
dynamic objects from uncalibrated monocular videos. Uniquely, we cast the
360-degree dynamic object reconstruction as a 4D generation problem,
eliminating the need for tedious multi-view data collection and camera
calibration. This is achieved by leveraging the object-level 3D-aware image
diffusion model as the primary supervision signal for training Dynamic Neural
Radiance Fields (DyNeRF). Specifically, we propose a Cascade DyNeRF to
facilitate stable convergence and temporal continuity under the supervision
signal which is discrete along the time axis. To achieve spatial and temporal
consistency, we further introduce an Interpolation-driven Consistency Loss. It
is optimized by minimizing the discrepancy between rendered frames from DyNeRF
and interpolated frames from a pre-trained video interpolation model. Extensive
experiments show that our Consistent4D can perform competitively to prior art
alternatives, opening up new possibilities for 4D dynamic object generation
from monocular videos, whilst also demonstrating advantage for conventional
text-to-3D generation tasks. Our project page is
https://consistent4d.github.io/.";Yanqin Jiang<author:sep>Li Zhang<author:sep>Jin Gao<author:sep>Weimin Hu<author:sep>Yao Yao;http://arxiv.org/pdf/2311.02848v1;cs.CV;Technique report. Project page: https://consistent4d.github.io/;nerf
2311.03140v1;http://arxiv.org/abs/2311.03140v1;2023-11-06;Animating NeRFs from Texture Space: A Framework for Pose-Dependent  Rendering of Human Performances;"Creating high-quality controllable 3D human models from multi-view RGB videos
poses a significant challenge. Neural radiance fields (NeRFs) have demonstrated
remarkable quality in reconstructing and free-viewpoint rendering of static as
well as dynamic scenes. The extension to a controllable synthesis of dynamic
human performances poses an exciting research question. In this paper, we
introduce a novel NeRF-based framework for pose-dependent rendering of human
performances. In our approach, the radiance field is warped around an SMPL body
mesh, thereby creating a new surface-aligned representation. Our representation
can be animated through skeletal joint parameters that are provided to the NeRF
in addition to the viewpoint for pose dependent appearances. To achieve this,
our representation includes the corresponding 2D UV coordinates on the mesh
texture map and the distance between the query point and the mesh. To enable
efficient learning despite mapping ambiguities and random visual variations, we
introduce a novel remapping process that refines the mapped coordinates.
Experiments demonstrate that our approach results in high-quality renderings
for novel-view and novel-pose synthesis.";Paul Knoll<author:sep>Wieland Morgenstern<author:sep>Anna Hilsmann<author:sep>Peter Eisert;http://arxiv.org/pdf/2311.03140v1;cs.CV;;nerf
2311.02826v1;http://arxiv.org/abs/2311.02826v1;2023-11-06;InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image;"With the success of Neural Radiance Field (NeRF) in 3D-aware portrait
editing, a variety of works have achieved promising results regarding both
quality and 3D consistency. However, these methods heavily rely on per-prompt
optimization when handling natural language as editing instructions. Due to the
lack of labeled human face 3D datasets and effective architectures, the area of
human-instructed 3D-aware editing for open-world portraits in an end-to-end
manner remains under-explored. To solve this problem, we propose an end-to-end
diffusion-based framework termed InstructPix2NeRF, which enables instructed
3D-aware portrait editing from a single open-world image with human
instructions. At its core lies a conditional latent 3D diffusion process that
lifts 2D editing to 3D space by learning the correlation between the paired
images' difference and the instructions via triplet data. With the help of our
proposed token position randomization strategy, we could even achieve
multi-semantic editing through one single pass with the portrait identity
well-preserved. Besides, we further propose an identity consistency module that
directly modulates the extracted identity signals into our diffusion process,
which increases the multi-view 3D identity consistency. Extensive experiments
verify the effectiveness of our method and show its superiority against strong
baselines quantitatively and qualitatively.";Jianhui Li<author:sep>Shilong Liu<author:sep>Zidong Liu<author:sep>Yikai Wang<author:sep>Kaiwen Zheng<author:sep>Jinghui Xu<author:sep>Jianmin Li<author:sep>Jun Zhu;http://arxiv.org/pdf/2311.02826v1;cs.CV;https://github.com/mybabyyh/InstructPix2NeRF;nerf
2311.03484v1;http://arxiv.org/abs/2311.03484v1;2023-11-06;Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM  and Next Best View Planning;"Aerial mapping systems are important for many surveying applications (e.g.,
industrial inspection or agricultural monitoring). Semi-autonomous mapping with
GPS-guided aerial platforms that fly preplanned missions is already widely
available but fully autonomous systems can significantly improve efficiency.
Autonomously mapping complex 3D structures requires a system that performs
online mapping and mission planning. This paper presents Osprey, an autonomous
aerial mapping system with state-of-the-art multi-session mapping capabilities.
It enables a non-expert operator to specify a bounded target area that the
aerial platform can then map autonomously, over multiple flights if necessary.
Field experiments with Osprey demonstrate that this system can achieve greater
map coverage of large industrial sites than manual surveys with a pilot-flown
aerial platform or a terrestrial laser scanner (TLS). Three sites, with a total
ground coverage of $7085$ m$^2$ and a maximum height of $27$ m, were mapped in
separate missions using $112$ minutes of autonomous flight time. True colour
maps were created from images captured by Osprey using pointcloud and NeRF
reconstruction methods. These maps provide useful data for structural
inspection tasks.";Rowan Border<author:sep>Nived Chebrolu<author:sep>Yifu Tao<author:sep>Jonathan D. Gammell<author:sep>Maurice Fallon;http://arxiv.org/pdf/2311.03484v1;cs.RO;"Submitted to Field Robotics, Manuscript #FR-23-0016. 25 pages, 15
  figures, 3 tables. Video available at
  https://www.youtube.com/watch?v=CVIXu2qUQJ8";nerf
2311.03345v1;http://arxiv.org/abs/2311.03345v1;2023-11-06;Long-Term Invariant Local Features via Implicit Cross-Domain  Correspondences;"Modern learning-based visual feature extraction networks perform well in
intra-domain localization, however, their performance significantly declines
when image pairs are captured across long-term visual domain variations, such
as different seasonal and daytime variations. In this paper, our first
contribution is a benchmark to investigate the performance impact of long-term
variations on visual localization. We conduct a thorough analysis of the
performance of current state-of-the-art feature extraction networks under
various domain changes and find a significant performance gap between intra-
and cross-domain localization. We investigate different methods to close this
gap by improving the supervision of modern feature extractor networks. We
propose a novel data-centric method, Implicit Cross-Domain Correspondences
(iCDC). iCDC represents the same environment with multiple Neural Radiance
Fields, each fitting the scene under individual visual domains. It utilizes the
underlying 3D representations to generate accurate correspondences across
different long-term visual conditions. Our proposed method enhances
cross-domain localization performance, significantly reducing the performance
gap. When evaluated on popular long-term localization benchmarks, our trained
networks consistently outperform existing methods. This work serves as a
substantial stride toward more robust visual localization pipelines for
long-term deployments, and opens up research avenues in the development of
long-term invariant descriptors.";Zador Pataki<author:sep>Mohammad Altillawi<author:sep>Menelaos Kanakis<author:sep>Rémi Pautrat<author:sep>Fengyi Shen<author:sep>Ziyuan Liu<author:sep>Luc Van Gool<author:sep>Marc Pollefeys;http://arxiv.org/pdf/2311.03345v1;cs.CV;14 pages + 5 pages appendix, 13 figures;
2311.02542v1;http://arxiv.org/abs/2311.02542v1;2023-11-05;VR-NeRF: High-Fidelity Virtualized Walkable Spaces;"We present an end-to-end system for the high-fidelity capture, model
reconstruction, and real-time rendering of walkable spaces in virtual reality
using neural radiance fields. To this end, we designed and built a custom
multi-camera rig to densely capture walkable spaces in high fidelity and with
multi-view high dynamic range images in unprecedented quality and density. We
extend instant neural graphics primitives with a novel perceptual color space
for learning accurate HDR appearance, and an efficient mip-mapping mechanism
for level-of-detail rendering with anti-aliasing, while carefully optimizing
the trade-off between quality and speed. Our multi-GPU renderer enables
high-fidelity volume rendering of our neural radiance field model at the full
VR resolution of dual 2K$\times$2K at 36 Hz on our custom demo machine. We
demonstrate the quality of our results on our challenging high-fidelity
datasets, and compare our method and datasets to existing baselines. We release
our dataset on our project website.";Linning Xu<author:sep>Vasu Agrawal<author:sep>William Laney<author:sep>Tony Garcia<author:sep>Aayush Bansal<author:sep>Changil Kim<author:sep>Samuel Rota Bulò<author:sep>Lorenzo Porzi<author:sep>Peter Kontschieder<author:sep>Aljaž Božič<author:sep>Dahua Lin<author:sep>Michael Zollhöfer<author:sep>Christian Richardt;http://arxiv.org/pdf/2311.02542v1;cs.CV;"SIGGRAPH Asia 2023; Project page: https://vr-nerf.github.io";nerf
2311.01659v1;http://arxiv.org/abs/2311.01659v1;2023-11-03;Efficient Cloud Pipelines for Neural Radiance Fields;"Since their introduction in 2020, Neural Radiance Fields (NeRFs) have taken
the computer vision community by storm. They provide a multi-view
representation of a scene or object that is ideal for eXtended Reality (XR)
applications and for creative endeavors such as virtual production, as well as
change detection operations in geospatial analytics. The computational cost of
these generative AI models is quite high, however, and the construction of
cloud pipelines to generate NeRFs is neccesary to realize their potential in
client applications. In this paper, we present pipelines on a high performance
academic computing cluster and compare it with a pipeline implemented on
Microsoft Azure. Along the way, we describe some uses of NeRFs in enabling
novel user interaction scenarios.";Derek Jacoby<author:sep>Donglin Xu<author:sep>Weder Ribas<author:sep>Minyi Xu<author:sep>Ting Liu<author:sep>Vishwanath Jayaraman<author:sep>Mengdi Wei<author:sep>Emma De Blois<author:sep>Yvonne Coady;http://arxiv.org/pdf/2311.01659v1;cs.CV;;nerf
2311.01653v1;http://arxiv.org/abs/2311.01653v1;2023-11-03;INeAT: Iterative Neural Adaptive Tomography;"Computed Tomography (CT) with its remarkable capability for three-dimensional
imaging from multiple projections, enjoys a broad range of applications in
clinical diagnosis, scientific observation, and industrial detection. Neural
Adaptive Tomography (NeAT) is a recently proposed 3D rendering method based on
neural radiance field for CT, and it demonstrates superior performance compared
to traditional methods. However, it still faces challenges when dealing with
the substantial perturbations and pose shifts encountered in CT scanning
processes. Here, we propose a neural rendering method for CT reconstruction,
named Iterative Neural Adaptive Tomography (INeAT), which incorporates
iterative posture optimization to effectively counteract the influence of
posture perturbations in data, particularly in cases involving significant
posture variations. Through the implementation of a posture feedback
optimization strategy, INeAT iteratively refines the posture corresponding to
the input images based on the reconstructed 3D volume. We demonstrate that
INeAT achieves artifact-suppressed and resolution-enhanced reconstruction in
scenarios with significant pose disturbances. Furthermore, we show that our
INeAT maintains comparable reconstruction performance to stable-state
acquisitions even using data from unstable-state acquisitions, which
significantly reduces the time required for CT scanning and relaxes the
stringent requirements on imaging hardware systems, underscoring its immense
potential for applications in short-time and low-cost CT technology.";Bo Xiong<author:sep>Changqing Su<author:sep>Zihan Lin<author:sep>You Zhou<author:sep>Zhaofei Yu;http://arxiv.org/pdf/2311.01653v1;eess.IV;;
2311.01815v2;http://arxiv.org/abs/2311.01815v2;2023-11-03;Estimating 3D Uncertainty Field: Quantifying Uncertainty for Neural  Radiance Fields;"Current methods based on Neural Radiance Fields (NeRF) significantly lack the
capacity to quantify uncertainty in their predictions, particularly on the
unseen space including the occluded and outside scene content. This limitation
hinders their extensive applications in robotics, where the reliability of
model predictions has to be considered for tasks such as robotic exploration
and planning in unknown environments. To address this, we propose a novel
approach to estimate a 3D Uncertainty Field based on the learned incomplete
scene geometry, which explicitly identifies these unseen regions. By
considering the accumulated transmittance along each camera ray, our
Uncertainty Field infers 2D pixel-wise uncertainty, exhibiting high values for
rays directly casting towards occluded or outside the scene content. To
quantify the uncertainty on the learned surface, we model a stochastic radiance
field. Our experiments demonstrate that our approach is the only one that can
explicitly reason about high uncertainty both on 3D unseen regions and its
involved 2D rendered pixels, compared with recent methods. Furthermore, we
illustrate that our designed uncertainty field is ideally suited for real-world
robotics tasks, such as next-best-view selection.";Jianxiong Shen<author:sep>Ruijie Ren<author:sep>Adria Ruiz<author:sep>Francesc Moreno-Noguer;http://arxiv.org/pdf/2311.01815v2;cs.CV;;nerf
2311.01842v1;http://arxiv.org/abs/2311.01842v1;2023-11-03;A Neural Radiance Field-Based Architecture for Intelligent Multilayered  View Synthesis;"A mobile ad hoc network is made up of a number of wireless portable nodes
that spontaneously come together en route for establish a transitory network
with no need for any central management. A mobile ad hoc network (MANET) is
made up of a sizable and reasonably dense community of mobile nodes that travel
across any terrain and rely solely on wireless interfaces for communication,
not on any well before centralized management. Furthermore, routing be supposed
to offer a method for instantly delivering data across a network between any
two nodes. Finding the best packet routing from across infrastructure is the
major issue, though. The proposed protocol's major goal is to identify the
least-expensive nominal capacity acquisition that assures the transportation of
realistic transport that ensures its durability in the event of any node
failure. This study suggests the Optimized Route Selection via Red Imported
Fire Ants (RIFA) Strategy as a way to improve on-demand source routing systems.
Predicting Route Failure and energy Utilization is used to pick the path during
the routing phase. Proposed work assess the results of the comparisons based on
performance parameters like as energy usage, packet delivery rate (PDR), and
end-to-end (E2E) delay. The outcome demonstrates that the proposed strategy is
preferable and increases network lifetime while lowering node energy
consumption and typical E2E delay under the majority of network performance
measures and factors.";D. Dhinakaran<author:sep>S. M. Udhaya Sankar<author:sep>G. Elumalai<author:sep>N. Jagadish kumar;http://arxiv.org/pdf/2311.01842v1;cs.NI;;
2311.01773v1;http://arxiv.org/abs/2311.01773v1;2023-11-03;PDF: Point Diffusion Implicit Function for Large-scale Scene Neural  Representation;"Recent advances in implicit neural representations have achieved impressive
results by sampling and fusing individual points along sampling rays in the
sampling space. However, due to the explosively growing sampling space, finely
representing and synthesizing detailed textures remains a challenge for
unbounded large-scale outdoor scenes. To alleviate the dilemma of using
individual points to perceive the entire colossal space, we explore learning
the surface distribution of the scene to provide structural priors and reduce
the samplable space and propose a Point Diffusion implicit Function, PDF, for
large-scale scene neural representation. The core of our method is a
large-scale point cloud super-resolution diffusion module that enhances the
sparse point cloud reconstructed from several training images into a dense
point cloud as an explicit prior. Then in the rendering stage, only sampling
points with prior points within the sampling radius are retained. That is, the
sampling space is reduced from the unbounded space to the scene surface.
Meanwhile, to fill in the background of the scene that cannot be provided by
point clouds, the region sampling based on Mip-NeRF 360 is employed to model
the background representation. Expensive experiments have demonstrated the
effectiveness of our method for large-scale scene novel view synthesis, which
outperforms relevant state-of-the-art baselines.";Yuhan Ding<author:sep>Fukun Yin<author:sep>Jiayuan Fan<author:sep>Hui Li<author:sep>Xin Chen<author:sep>Wen Liu<author:sep>Chongshan Lu<author:sep>Gang YU<author:sep>Tao Chen;http://arxiv.org/pdf/2311.01773v1;cs.CV;Accepted to NeurIPS 2023;nerf
2311.01065v1;http://arxiv.org/abs/2311.01065v1;2023-11-02;Novel View Synthesis from a Single RGBD Image for Indoor Scenes;"In this paper, we propose an approach for synthesizing novel view images from
a single RGBD (Red Green Blue-Depth) input. Novel view synthesis (NVS) is an
interesting computer vision task with extensive applications. Methods using
multiple images has been well-studied, exemplary ones include training
scene-specific Neural Radiance Fields (NeRF), or leveraging multi-view stereo
(MVS) and 3D rendering pipelines. However, both are either computationally
intensive or non-generalizable across different scenes, limiting their
practical value. Conversely, the depth information embedded in RGBD images
unlocks 3D potential from a singular view, simplifying NVS. The widespread
availability of compact, affordable stereo cameras, and even LiDARs in
contemporary devices like smartphones, makes capturing RGBD images more
accessible than ever. In our method, we convert an RGBD image into a point
cloud and render it from a different viewpoint, then formulate the NVS task
into an image translation problem. We leveraged generative adversarial networks
to style-transfer the rendered image, achieving a result similar to a
photograph taken from the new perspective. We explore both unsupervised
learning using CycleGAN and supervised learning with Pix2Pix, and demonstrate
the qualitative results. Our method circumvents the limitations of traditional
multi-image techniques, holding significant promise for practical, real-time
applications in NVS.";Congrui Hetang<author:sep>Yuping Wang;http://arxiv.org/pdf/2311.01065v1;cs.CV;"2nd International Conference on Image Processing, Computer Vision and
  Machine Learning, November 2023";nerf
2310.20685v1;http://arxiv.org/abs/2310.20685v1;2023-10-31;NeRF Revisited: Fixing Quadrature Instability in Volume Rendering;"Neural radiance fields (NeRF) rely on volume rendering to synthesize novel
views. Volume rendering requires evaluating an integral along each ray, which
is numerically approximated with a finite sum that corresponds to the exact
integral along the ray under piecewise constant volume density. As a
consequence, the rendered result is unstable w.r.t. the choice of samples along
the ray, a phenomenon that we dub quadrature instability. We propose a
mathematically principled solution by reformulating the sample-based rendering
equation so that it corresponds to the exact integral under piecewise linear
volume density. This simultaneously resolves multiple issues: conflicts between
samples along different rays, imprecise hierarchical sampling, and
non-differentiability of quantiles of ray termination distances w.r.t. model
parameters. We demonstrate several benefits over the classical sample-based
rendering equation, such as sharper textures, better geometric reconstruction,
and stronger depth supervision. Our proposed formulation can be also be used as
a drop-in replacement to the volume rendering equation of existing NeRF-based
methods. Our project page can be found at pl-nerf.github.io.";Mikaela Angelina Uy<author:sep>Kiyohiro Nakayama<author:sep>Guandao Yang<author:sep>Rahul Krishna Thomas<author:sep>Leonidas Guibas<author:sep>Ke Li;http://arxiv.org/pdf/2310.20685v1;cs.CV;Neurips 2023;nerf
2310.20710v1;http://arxiv.org/abs/2310.20710v1;2023-10-31;FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance  Fields by Analyzing and Enhancing Fourier PlenOctrees;"Fourier PlenOctrees have shown to be an efficient representation for
real-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many
advantages, this method suffers from artifacts introduced by the involved
compression when combining it with recent state-of-the-art techniques for
training the static per-frame NeRF models. In this paper, we perform an
in-depth analysis of these artifacts and leverage the resulting insights to
propose an improved representation. In particular, we present a novel density
encoding that adapts the Fourier-based compression to the characteristics of
the transfer function used by the underlying volume rendering procedure and
leads to a substantial reduction of artifacts in the dynamic model.
Furthermore, we show an augmentation of the training data that relaxes the
periodicity assumption of the compression. We demonstrate the effectiveness of
our enhanced Fourier PlenOctrees in the scope of quantitative and qualitative
evaluations on synthetic and real-world scenes.";Saskia Rabich<author:sep>Patrick Stotko<author:sep>Reinhard Klein;http://arxiv.org/pdf/2310.20710v1;cs.CV;;nerf
2310.19464v1;http://arxiv.org/abs/2310.19464v1;2023-10-30;Generative Neural Fields by Mixtures of Neural Implicit Functions;"We propose a novel approach to learning the generative neural fields
represented by linear combinations of implicit basis networks. Our algorithm
learns basis networks in the form of implicit neural representations and their
coefficients in a latent space by either conducting meta-learning or adopting
auto-decoding paradigms. The proposed method easily enlarges the capacity of
generative neural fields by increasing the number of basis networks while
maintaining the size of a network for inference to be small through their
weighted model averaging. Consequently, sampling instances using the model is
efficient in terms of latency and memory footprint. Moreover, we customize
denoising diffusion probabilistic model for a target task to sample latent
mixture coefficients, which allows our final model to generate unseen data
effectively. Experiments show that our approach achieves competitive generation
performance on diverse benchmarks for images, voxel data, and NeRF scenes
without sophisticated designs for specific modalities and domains.";Tackgeun You<author:sep>Mijeong Kim<author:sep>Jungtaek Kim<author:sep>Bohyung Han;http://arxiv.org/pdf/2310.19464v1;cs.LG;;nerf
2311.16127v1;http://arxiv.org/abs/2311.16127v1;2023-10-30;SeamlessNeRF: Stitching Part NeRFs with Gradient Propagation;"Neural Radiance Fields (NeRFs) have emerged as promising digital mediums of
3D objects and scenes, sparking a surge in research to extend the editing
capabilities in this domain. The task of seamless editing and merging of
multiple NeRFs, resembling the ``Poisson blending'' in 2D image editing,
remains a critical operation that is under-explored by existing work. To fill
this gap, we propose SeamlessNeRF, a novel approach for seamless appearance
blending of multiple NeRFs. In specific, we aim to optimize the appearance of a
target radiance field in order to harmonize its merge with a source field. We
propose a well-tailored optimization procedure for blending, which is
constrained by 1) pinning the radiance color in the intersecting boundary area
between the source and target fields and 2) maintaining the original gradient
of the target. Extensive experiments validate that our approach can effectively
propagate the source appearance from the boundary area to the entire target
field through the gradients. To the best of our knowledge, SeamlessNeRF is the
first work that introduces gradient-guided appearance editing to radiance
fields, offering solutions for seamless stitching of 3D objects represented in
NeRFs.";Bingchen Gong<author:sep>Yuehao Wang<author:sep>Xiaoguang Han<author:sep>Qi Dou;http://arxiv.org/pdf/2311.16127v1;cs.CV;"To appear in SIGGRAPH Asia 2023. Project website is accessible at
  https://sites.google.com/view/seamlessnerf";nerf
2310.19441v1;http://arxiv.org/abs/2310.19441v1;2023-10-30;Dynamic Gaussian Splatting from Markerless Motion Capture can  Reconstruct Infants Movements;"Easy access to precise 3D tracking of movement could benefit many aspects of
rehabilitation. A challenge to achieving this goal is that while there are many
datasets and pretrained algorithms for able-bodied adults, algorithms trained
on these datasets often fail to generalize to clinical populations including
people with disabilities, infants, and neonates. Reliable movement analysis of
infants and neonates is important as spontaneous movement behavior is an
important indicator of neurological function and neurodevelopmental disability,
which can help guide early interventions. We explored the application of
dynamic Gaussian splatting to sparse markerless motion capture (MMC) data. Our
approach leverages semantic segmentation masks to focus on the infant,
significantly improving the initialization of the scene. Our results
demonstrate the potential of this method in rendering novel views of scenes and
tracking infant movements. This work paves the way for advanced movement
analysis tools that can be applied to diverse clinical populations, with a
particular emphasis on early detection in infants.";R. James Cotton<author:sep>Colleen Peyton;http://arxiv.org/pdf/2310.19441v1;cs.CV;;gaussian splatting
2310.18999v2;http://arxiv.org/abs/2310.18999v2;2023-10-29;DynPoint: Dynamic Neural Point For View Synthesis;"The introduction of neural radiance fields has greatly improved the
effectiveness of view synthesis for monocular videos. However, existing
algorithms face difficulties when dealing with uncontrolled or lengthy
scenarios, and require extensive training time specific to each new scenario.
To tackle these limitations, we propose DynPoint, an algorithm designed to
facilitate the rapid synthesis of novel views for unconstrained monocular
videos. Rather than encoding the entirety of the scenario information into a
latent representation, DynPoint concentrates on predicting the explicit 3D
correspondence between neighboring frames to realize information aggregation.
Specifically, this correspondence prediction is achieved through the estimation
of consistent depth and scene flow information across frames. Subsequently, the
acquired correspondence is utilized to aggregate information from multiple
reference frames to a target frame, by constructing hierarchical neural point
clouds. The resulting framework enables swift and accurate view synthesis for
desired views of target frames. The experimental results obtained demonstrate
the considerable acceleration of training time achieved - typically an order of
magnitude - by our proposed method while yielding comparable outcomes compared
to prior approaches. Furthermore, our method exhibits strong robustness in
handling long-duration videos without learning a canonical representation of
video content.";Kaichen Zhou<author:sep>Jia-Xing Zhong<author:sep>Sangyun Shin<author:sep>Kai Lu<author:sep>Yiyuan Yang<author:sep>Andrew Markham<author:sep>Niki Trigoni;http://arxiv.org/pdf/2310.18999v2;cs.CV;;
2310.18917v2;http://arxiv.org/abs/2310.18917v2;2023-10-29;TiV-NeRF: Tracking and Mapping via Time-Varying Representation with  Dynamic Neural Radiance Fields;"Previous attempts to integrate Neural Radiance Fields (NeRF) into
Simultaneous Localization and Mapping (SLAM) framework either rely on the
assumption of static scenes or treat dynamic objects as outliers. However, most
of real-world scenarios is dynamic. In this paper, we propose a time-varying
representation to track and reconstruct the dynamic scenes. Our system
simultaneously maintains two processes, tracking process and mapping process.
For tracking process, the entire input images are uniformly sampled and
training of the RGB images are self-supervised. For mapping process, we
leverage know masks to differentiate dynamic objects and static backgrounds,
and we apply distinct sampling strategies for two types of areas. The
parameters optimization for both processes are made up by two stages, the first
stage associates time with 3D positions to convert the deformation field to the
canonical field. And the second associates time with 3D positions in canonical
field to obtain colors and Signed Distance Function (SDF). Besides, We propose
a novel keyframe selection strategy based on the overlapping rate. We evaluate
our approach on two publicly available synthetic datasets and validate that our
method is more effective compared to current state-of-the-art dynamic mapping
methods.";Chengyao Duan<author:sep>Zhiliu Yang;http://arxiv.org/pdf/2310.18917v2;cs.CV;;nerf
2310.18846v1;http://arxiv.org/abs/2310.18846v1;2023-10-28;INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings;"Implicit Neural Representations (INRs) have revolutionized signal
representation by leveraging neural networks to provide continuous and smooth
representations of complex data. However, existing INRs face limitations in
capturing fine-grained details, handling noise, and adapting to diverse signal
types. To address these challenges, we introduce INCODE, a novel approach that
enhances the control of the sinusoidal-based activation function in INRs using
deep prior knowledge. INCODE comprises a harmonizer network and a composer
network, where the harmonizer network dynamically adjusts key parameters of the
activation function. Through a task-specific pre-trained model, INCODE adapts
the task-specific parameters to optimize the representation process. Our
approach not only excels in representation, but also extends its prowess to
tackle complex tasks such as audio, image, and 3D shape reconstructions, as
well as intricate challenges such as neural radiance fields (NeRFs), and
inverse problems, including denoising, super-resolution, inpainting, and CT
reconstruction. Through comprehensive experiments, INCODE demonstrates its
superiority in terms of robustness, accuracy, quality, and convergence rate,
broadening the scope of signal representation. Please visit the project's
website for details on the proposed method and access to the code.";Amirhossein Kazerouni<author:sep>Reza Azad<author:sep>Alireza Hosseini<author:sep>Dorit Merhof<author:sep>Ulas Bagci;http://arxiv.org/pdf/2310.18846v1;cs.CV;Accepted at WACV 2024 conference;nerf
2310.17994v1;http://arxiv.org/abs/2310.17994v1;2023-10-27;ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Real Image;"We introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel view
synthesis for in-the-wild scenes. While existing methods are designed for
single objects with masked backgrounds, we propose new techniques to address
challenges introduced by in-the-wild multi-object scenes with complex
backgrounds. Specifically, we train a generative prior on a mixture of data
sources that capture object-centric, indoor, and outdoor scenes. To address
issues from data mixture such as depth-scale ambiguity, we propose a novel
camera conditioning parameterization and normalization scheme. Further, we
observe that Score Distillation Sampling (SDS) tends to truncate the
distribution of complex backgrounds during distillation of 360-degree scenes,
and propose ""SDS anchoring"" to improve the diversity of synthesized novel
views. Our model sets a new state-of-the-art result in LPIPS on the DTU dataset
in the zero-shot setting, even outperforming methods specifically trained on
DTU. We further adapt the challenging Mip-NeRF 360 dataset as a new benchmark
for single-image novel view synthesis, and demonstrate strong performance in
this setting. Our code and data are at http://kylesargent.github.io/zeronvs/";Kyle Sargent<author:sep>Zizhang Li<author:sep>Tanmay Shah<author:sep>Charles Herrmann<author:sep>Hong-Xing Yu<author:sep>Yunzhi Zhang<author:sep>Eric Ryan Chan<author:sep>Dmitry Lagun<author:sep>Li Fei-Fei<author:sep>Deqing Sun<author:sep>Jiajun Wu;http://arxiv.org/pdf/2310.17994v1;cs.CV;17 pages;nerf
2310.17880v1;http://arxiv.org/abs/2310.17880v1;2023-10-27;Reconstructive Latent-Space Neural Radiance Fields for Efficient 3D  Scene Representations;"Neural Radiance Fields (NeRFs) have proven to be powerful 3D representations,
capable of high quality novel view synthesis of complex scenes. While NeRFs
have been applied to graphics, vision, and robotics, problems with slow
rendering speed and characteristic visual artifacts prevent adoption in many
use cases. In this work, we investigate combining an autoencoder (AE) with a
NeRF, in which latent features (instead of colours) are rendered and then
convolutionally decoded. The resulting latent-space NeRF can produce novel
views with higher quality than standard colour-space NeRFs, as the AE can
correct certain visual artifacts, while rendering over three times faster. Our
work is orthogonal to other techniques for improving NeRF efficiency. Further,
we can control the tradeoff between efficiency and image quality by shrinking
the AE architecture, achieving over 13 times faster rendering with only a small
drop in performance. We hope that our approach can form the basis of an
efficient, yet high-fidelity, 3D scene representation for downstream tasks,
especially when retaining differentiability is useful, as in many robotics
scenarios requiring continual learning.";Tristan Aumentado-Armstrong<author:sep>Ashkan Mirzaei<author:sep>Marcus A. Brubaker<author:sep>Jonathan Kelly<author:sep>Alex Levinshtein<author:sep>Konstantinos G. Derpanis<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2310.17880v1;cs.CV;;nerf
2310.17075v2;http://arxiv.org/abs/2310.17075v2;2023-10-26;HyperFields: Towards Zero-Shot Generation of NeRFs from Text;"We introduce HyperFields, a method for generating text-conditioned Neural
Radiance Fields (NeRFs) with a single forward pass and (optionally) some
fine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns
a smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF
distillation training, which distills scenes encoded in individual NeRFs into
one dynamic hypernetwork. These techniques enable a single network to fit over
a hundred unique scenes. We further demonstrate that HyperFields learns a more
general map between text and NeRFs, and consequently is capable of predicting
novel in-distribution and out-of-distribution scenes -- either zero-shot or
with a few finetuning steps. Finetuning HyperFields benefits from accelerated
convergence thanks to the learned general map, and is capable of synthesizing
novel scenes 5 to 10 times faster than existing neural optimization-based
methods. Our ablation experiments show that both the dynamic architecture and
NeRF distillation are critical to the expressivity of HyperFields.";Sudarshan Babu<author:sep>Richard Liu<author:sep>Avery Zhou<author:sep>Michael Maire<author:sep>Greg Shakhnarovich<author:sep>Rana Hanocka;http://arxiv.org/pdf/2310.17075v2;cs.CV;Project page: https://threedle.github.io/hyperfields/;nerf
2310.16831v2;http://arxiv.org/abs/2310.16831v2;2023-10-25;PERF: Panoramic Neural Radiance Field from a Single Panorama;"Neural Radiance Field (NeRF) has achieved substantial progress in novel view
synthesis given multi-view images. Recently, some works have attempted to train
a NeRF from a single image with 3D priors. They mainly focus on a limited field
of view with a few occlusions, which greatly limits their scalability to
real-world 360-degree panoramic scenarios with large-size occlusions. In this
paper, we present PERF, a 360-degree novel view synthesis framework that trains
a panoramic neural radiance field from a single panorama. Notably, PERF allows
3D roaming in a complex scene without expensive and tedious image collection.
To achieve this goal, we propose a novel collaborative RGBD inpainting method
and a progressive inpainting-and-erasing method to lift up a 360-degree 2D
scene to a 3D scene. Specifically, we first predict a panoramic depth map as
initialization given a single panorama and reconstruct visible 3D regions with
volume rendering. Then we introduce a collaborative RGBD inpainting approach
into a NeRF for completing RGB images and depth maps from random views, which
is derived from an RGB Stable Diffusion model and a monocular depth estimator.
Finally, we introduce an inpainting-and-erasing strategy to avoid inconsistent
geometry between a newly-sampled view and reference views. The two components
are integrated into the learning of NeRFs in a unified optimization framework
and achieve promising results. Extensive experiments on Replica and a new
dataset PERF-in-the-wild demonstrate the superiority of our PERF over
state-of-the-art methods. Our PERF can be widely used for real-world
applications, such as panorama-to-3D, text-to-3D, and 3D scene stylization
applications. Project page and code are available at
https://perf-project.github.io/ and https://github.com/perf-project/PeRF.";Guangcong Wang<author:sep>Peng Wang<author:sep>Zhaoxi Chen<author:sep>Wenping Wang<author:sep>Chen Change Loy<author:sep>Ziwei Liu;http://arxiv.org/pdf/2310.16831v2;cs.CV;"Project Page: https://perf-project.github.io/ , Code:
  https://github.com/perf-project/PeRF";nerf
2310.16255v1;http://arxiv.org/abs/2310.16255v1;2023-10-25;UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception;"Tremendous variations coupled with large degrees of freedom in UAV-based
imaging conditions lead to a significant lack of data in adequately learning
UAV-based perception models. Using various synthetic renderers in conjunction
with perception models is prevalent to create synthetic data to augment the
learning in the ground-based imaging domain. However, severe challenges in the
austere UAV-based domain require distinctive solutions to image synthesis for
data augmentation. In this work, we leverage recent advancements in neural
rendering to improve static and dynamic novelview UAV-based image synthesis,
especially from high altitudes, capturing salient scene attributes. Finally, we
demonstrate a considerable performance boost is achieved when a state-ofthe-art
detection model is optimized primarily on hybrid sets of real and synthetic
data instead of the real or synthetic data separately.";Christopher Maxey<author:sep>Jaehoon Choi<author:sep>Hyungtae Lee<author:sep>Dinesh Manocha<author:sep>Heesung Kwon;http://arxiv.org/pdf/2310.16255v1;cs.CV;Video Link: https://www.youtube.com/watch?v=ucPzbPLqqpI;nerf
2310.16858v2;http://arxiv.org/abs/2310.16858v2;2023-10-25;4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance  Fields via Semantic Distillation;"This paper targets interactive object-level editing (e.g., deletion,
recoloring, transformation, composition) in dynamic scenes. Recently, some
methods aiming for flexible editing static scenes represented by neural
radiance field (NeRF) have shown impressive synthesis quality, while similar
capabilities in time-variant dynamic scenes remain limited. To solve this
problem, we propose 4D-Editor, an interactive semantic-driven editing
framework, allowing editing multiple objects in a dynamic NeRF with user
strokes on a single frame. We propose an extension to the original dynamic NeRF
by incorporating a hybrid semantic feature distillation to maintain
spatial-temporal consistency after editing. In addition, we design Recursive
Selection Refinement that significantly boosts object segmentation accuracy
within a dynamic NeRF to aid the editing process. Moreover, we develop
Multi-view Reprojection Inpainting to fill holes caused by incomplete scene
capture after editing. Extensive experiments and editing examples on real-world
demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs.
Project page: https://patrickddj.github.io/4D-Editor";Dadong Jiang<author:sep>Zhihui Ke<author:sep>Xiaobo Zhou<author:sep>Xidong Shi;http://arxiv.org/pdf/2310.16858v2;cs.CV;Project page: https://patrickddj.github.io/4D-Editor;nerf
2310.16832v2;http://arxiv.org/abs/2310.16832v2;2023-10-25;LightSpeed: Light and Fast Neural Light Fields on Mobile Devices;"Real-time novel-view image synthesis on mobile devices is prohibitive due to
the limited computational power and storage. Using volumetric rendering
methods, such as NeRF and its derivatives, on mobile devices is not suitable
due to the high computational cost of volumetric rendering. On the other hand,
recent advances in neural light field representations have shown promising
real-time view synthesis results on mobile devices. Neural light field methods
learn a direct mapping from a ray representation to the pixel color. The
current choice of ray representation is either stratified ray sampling or
Plucker coordinates, overlooking the classic light slab (two-plane)
representation, the preferred representation to interpolate between light field
views. In this work, we find that using the light slab representation is an
efficient representation for learning a neural light field. More importantly,
it is a lower-dimensional ray representation enabling us to learn the 4D ray
space using feature grids which are significantly faster to train and render.
Although mostly designed for frontal views, we show that the light-slab
representation can be further extended to non-frontal scenes using a
divide-and-conquer strategy. Our method offers superior rendering quality
compared to previous light field methods and achieves a significantly improved
trade-off between rendering quality and speed.";Aarush Gupta<author:sep>Junli Cao<author:sep>Chaoyang Wang<author:sep>Ju Hu<author:sep>Sergey Tulyakov<author:sep>Jian Ren<author:sep>László A Jeni;http://arxiv.org/pdf/2310.16832v2;cs.CV;"Project Page: http://lightspeed-r2l.github.io/ . Add camera ready
  version";nerf
2310.16383v1;http://arxiv.org/abs/2310.16383v1;2023-10-25;Open-NeRF: Towards Open Vocabulary NeRF Decomposition;"In this paper, we address the challenge of decomposing Neural Radiance Fields
(NeRF) into objects from an open vocabulary, a critical task for object
manipulation in 3D reconstruction and view synthesis. Current techniques for
NeRF decomposition involve a trade-off between the flexibility of processing
open-vocabulary queries and the accuracy of 3D segmentation. We present,
Open-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage
large-scale, off-the-shelf, segmentation models like the Segment Anything Model
(SAM) and introduce an integrate-and-distill paradigm with hierarchical
embeddings to achieve both the flexibility of open-vocabulary querying and 3D
segmentation accuracy. Open-NeRF first utilizes large-scale foundation models
to generate hierarchical 2D mask proposals from varying viewpoints. These
proposals are then aligned via tracking approaches and integrated within the 3D
space and subsequently distilled into the 3D field. This process ensures
consistent recognition and granularity of objects from different viewpoints,
even in challenging scenarios involving occlusion and indistinct features. Our
experimental results show that the proposed Open-NeRF outperforms
state-of-the-art methods such as LERF \cite{lerf} and FFD \cite{ffd} in
open-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF
decomposition, guided by open-vocabulary queries, enabling novel applications
in robotics and vision-language interaction in open-world 3D scenes.";Hao Zhang<author:sep>Fang Li<author:sep>Narendra Ahuja;http://arxiv.org/pdf/2310.16383v1;cs.CV;Accepted by WACV 2024;nerf
2310.15504v1;http://arxiv.org/abs/2310.15504v1;2023-10-24;Cross-view Self-localization from Synthesized Scene-graphs;"Cross-view self-localization is a challenging scenario of visual place
recognition in which database images are provided from sparse viewpoints.
Recently, an approach for synthesizing database images from unseen viewpoints
using NeRF (Neural Radiance Fields) technology has emerged with impressive
performance. However, synthesized images provided by these techniques are often
of lower quality than the original images, and furthermore they significantly
increase the storage cost of the database. In this study, we explore a new
hybrid scene model that combines the advantages of view-invariant appearance
features computed from raw images and view-dependent spatial-semantic features
computed from synthesized images. These two types of features are then fused
into scene graphs, and compressively learned and recognized by a graph neural
network. The effectiveness of the proposed method was verified using a novel
cross-view self-localization dataset with many unseen views generated using a
photorealistic Habitat simulator.";Ryogo Yamamoto<author:sep>Kanji Tanaka;http://arxiv.org/pdf/2310.15504v1;cs.CV;5 pages, 5 figures, technical report;nerf
2310.14695v1;http://arxiv.org/abs/2310.14695v1;2023-10-23;CAwa-NeRF: Instant Learning of Compression-Aware NeRF Features;"Modeling 3D scenes by volumetric feature grids is one of the promising
directions of neural approximations to improve Neural Radiance Fields (NeRF).
Instant-NGP (INGP) introduced multi-resolution hash encoding from a lookup
table of trainable feature grids which enabled learning high-quality neural
graphics primitives in a matter of seconds. However, this improvement came at
the cost of higher storage size. In this paper, we address this challenge by
introducing instant learning of compression-aware NeRF features (CAwa-NeRF),
that allows exporting the zip compressed feature grids at the end of the model
training with a negligible extra time overhead without changing neither the
storage architecture nor the parameters used in the original INGP paper.
Nonetheless, the proposed method is not limited to INGP but could also be
adapted to any model. By means of extensive simulations, our proposed instant
learning pipeline can achieve impressive results on different kinds of static
scenes such as single object masked background scenes and real-life scenes
captured in our studio. In particular, for single object masked background
scenes CAwa-NeRF compresses the feature grids down to 6% (1.2 MB) of the
original size without any loss in the PSNR (33 dB) or down to 2.4% (0.53 MB)
with a slight virtual loss (32.31 dB).";Omnia Mahmoud<author:sep>Théo Ladune<author:sep>Matthieu Gendrin;http://arxiv.org/pdf/2310.14695v1;cs.CV;10 pages, 9 figures;nerf
2310.14487v1;http://arxiv.org/abs/2310.14487v1;2023-10-23;VQ-NeRF: Vector Quantization Enhances Implicit Neural Representations;"Recent advancements in implicit neural representations have contributed to
high-fidelity surface reconstruction and photorealistic novel view synthesis.
However, the computational complexity inherent in these methodologies presents
a substantial impediment, constraining the attainable frame rates and
resolutions in practical applications. In response to this predicament, we
propose VQ-NeRF, an effective and efficient pipeline for enhancing implicit
neural representations via vector quantization. The essence of our method
involves reducing the sampling space of NeRF to a lower resolution and
subsequently reinstating it to the original size utilizing a pre-trained VAE
decoder, thereby effectively mitigating the sampling time bottleneck
encountered during rendering. Although the codebook furnishes representative
features, reconstructing fine texture details of the scene remains challenging
due to high compression rates. To overcome this constraint, we design an
innovative multi-scale NeRF sampling scheme that concurrently optimizes the
NeRF model at both compressed and original scales to enhance the network's
ability to preserve fine details. Furthermore, we incorporate a semantic loss
function to improve the geometric fidelity and semantic coherence of our 3D
reconstructions. Extensive experiments demonstrate the effectiveness of our
model in achieving the optimal trade-off between rendering quality and
efficiency. Evaluation on the DTU, BlendMVS, and H3DS datasets confirms the
superior performance of our approach.";Yiying Yang<author:sep>Wen Liu<author:sep>Fukun Yin<author:sep>Xin Chen<author:sep>Gang Yu<author:sep>Jiayuan Fan<author:sep>Tao Chen;http://arxiv.org/pdf/2310.14487v1;cs.CV;"Submitted to the 38th Annual AAAI Conference on Artificial
  Intelligence";nerf
2310.13356v2;http://arxiv.org/abs/2310.13356v2;2023-10-20;Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos;"Recent advancements in 4D scene reconstruction using neural radiance fields
(NeRF) have demonstrated the ability to represent dynamic scenes from
multi-view videos. However, they fail to reconstruct the dynamic scenes and
struggle to fit even the training views in unsynchronized settings. It happens
because they employ a single latent embedding for a frame while the multi-view
images at the same frame were actually captured at different moments. To
address this limitation, we introduce time offsets for individual
unsynchronized videos and jointly optimize the offsets with NeRF. By design,
our method is applicable for various baselines and improves them with large
margins. Furthermore, finding the offsets naturally works as synchronizing the
videos without manual effort. Experiments are conducted on the common Plenoptic
Video Dataset and a newly built Unsynchronized Dynamic Blender Dataset to
verify the performance of our method. Project page:
https://seoha-kim.github.io/sync-nerf";Seoha Kim<author:sep>Jeongmin Bae<author:sep>Youngsik Yun<author:sep>Hahyun Lee<author:sep>Gun Bang<author:sep>Youngjung Uh;http://arxiv.org/pdf/2310.13356v2;cs.CV;AAAI 2024, Project page: https://seoha-kim.github.io/sync-nerf;nerf
2310.13670v1;http://arxiv.org/abs/2310.13670v1;2023-10-20;ManifoldNeRF: View-dependent Image Feature Supervision for Few-shot  Neural Radiance Fields;"Novel view synthesis has recently made significant progress with the advent
of Neural Radiance Fields (NeRF). DietNeRF is an extension of NeRF that aims to
achieve this task from only a few images by introducing a new loss function for
unknown viewpoints with no input images. The loss function assumes that a
pre-trained feature extractor should output the same feature even if input
images are captured at different viewpoints since the images contain the same
object. However, while that assumption is ideal, in reality, it is known that
as viewpoints continuously change, also feature vectors continuously change.
Thus, the assumption can harm training. To avoid this harmful training, we
propose ManifoldNeRF, a method for supervising feature vectors at unknown
viewpoints using interpolated features from neighboring known viewpoints. Since
the method provides appropriate supervision for each unknown viewpoint by the
interpolated features, the volume representation is learned better than
DietNeRF. Experimental results show that the proposed method performs better
than others in a complex scene. We also experimented with several subsets of
viewpoints from a set of viewpoints and identified an effective set of
viewpoints for real environments. This provided a basic policy of viewpoint
patterns for real-world application. The code is available at
https://github.com/haganelego/ManifoldNeRF_BMVC2023";Daiju Kanaoka<author:sep>Motoharu Sonogashira<author:sep>Hakaru Tamukoh<author:sep>Yasutomo Kawanishi;http://arxiv.org/pdf/2310.13670v1;cs.CV;Accepted by BMVC2023;nerf
2310.13263v1;http://arxiv.org/abs/2310.13263v1;2023-10-20;UE4-NeRF:Neural Radiance Field for Real-Time Rendering of Large-Scale  Scene;"Neural Radiance Fields (NeRF) is a novel implicit 3D reconstruction method
that shows immense potential and has been gaining increasing attention. It
enables the reconstruction of 3D scenes solely from a set of photographs.
However, its real-time rendering capability, especially for interactive
real-time rendering of large-scale scenes, still has significant limitations.
To address these challenges, in this paper, we propose a novel neural rendering
system called UE4-NeRF, specifically designed for real-time rendering of
large-scale scenes. We partitioned each large scene into different sub-NeRFs.
In order to represent the partitioned independent scene, we initialize
polygonal meshes by constructing multiple regular octahedra within the scene
and the vertices of the polygonal faces are continuously optimized during the
training process. Drawing inspiration from Level of Detail (LOD) techniques, we
trained meshes of varying levels of detail for different observation levels.
Our approach combines with the rasterization pipeline in Unreal Engine 4 (UE4),
achieving real-time rendering of large-scale scenes at 4K resolution with a
frame rate of up to 43 FPS. Rendering within UE4 also facilitates scene editing
in subsequent stages. Furthermore, through experiments, we have demonstrated
that our method achieves rendering quality comparable to state-of-the-art
approaches. Project page: https://jamchaos.github.io/UE4-NeRF/.";Jiaming Gu<author:sep>Minchao Jiang<author:sep>Hongsheng Li<author:sep>Xiaoyuan Lu<author:sep>Guangming Zhu<author:sep>Syed Afaq Ali Shah<author:sep>Liang Zhang<author:sep>Mohammed Bennamoun;http://arxiv.org/pdf/2310.13263v1;cs.CV;Accepted by NeurIPS2023;nerf
2310.11645v1;http://arxiv.org/abs/2310.11645v1;2023-10-18;Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos  using NeRFs;"Given that a conventional laparoscope only provides a two-dimensional (2-D)
view, the detection and diagnosis of medical ailments can be challenging. To
overcome the visual constraints associated with laparoscopy, the use of
laparoscopic images and videos to reconstruct the three-dimensional (3-D)
anatomical structure of the abdomen has proven to be a promising approach.
Neural Radiance Fields (NeRFs) have recently gained attention thanks to their
ability to generate photorealistic images from a 3-D static scene, thus
facilitating a more comprehensive exploration of the abdomen through the
synthesis of new views. This distinguishes NeRFs from alternative methods such
as Simultaneous Localization and Mapping (SLAM) and depth estimation. In this
paper, we present a comprehensive examination of NeRFs in the context of
laparoscopy surgical videos, with the goal of rendering abdominal scenes in
3-D. Although our experimental results are promising, the proposed approach
encounters substantial challenges, which require further exploration in future
research.";Khoa Tuan Nguyen<author:sep>Francesca Tozzi<author:sep>Nikdokht Rashidian<author:sep>Wouter Willaert<author:sep>Joris Vankerschaver<author:sep>Wesley De Neve;http://arxiv.org/pdf/2310.11645v1;cs.CV;"The Version of Record of this contribution is published in MLMI 2023
  Part I, and is available online at
  https://doi.org/10.1007/978-3-031-45673-2_9";nerf
2310.11864v3;http://arxiv.org/abs/2310.11864v3;2023-10-18;VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector  Quantization;"We propose VQ-NeRF, a two-branch neural network model that incorporates
Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes.
Conventional neural reflectance fields use only continuous representations to
model 3D scenes, despite the fact that objects are typically composed of
discrete materials in reality. This lack of discretization can result in noisy
material decomposition and complicated material editing. To address these
limitations, our model consists of a continuous branch and a discrete branch.
The continuous branch follows the conventional pipeline to predict decomposed
materials, while the discrete branch uses the VQ mechanism to quantize
continuous materials into individual ones. By discretizing the materials, our
model can reduce noise in the decomposition process and generate a segmentation
map of discrete materials. Specific materials can be easily selected for
further editing by clicking on the corresponding area of the segmentation
outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy
to predict the number of materials in a scene, which reduces redundancy in the
material segmentation process. To improve usability, we also develop an
interactive interface to further assist material editing. We evaluate our model
on both computer-generated and real-world scenes, demonstrating its superior
performance. To the best of our knowledge, our model is the first to enable
discrete material editing in 3D scenes.";Hongliang Zhong<author:sep>Jingbo Zhang<author:sep>Jing Liao;http://arxiv.org/pdf/2310.11864v3;cs.CV;"Accepted by TVCG. Project Page:
  https://jtbzhl.github.io/VQ-NeRF.github.io/";nerf
2310.10209v1;http://arxiv.org/abs/2310.10209v1;2023-10-16;Self-supervised Fetal MRI 3D Reconstruction Based on Radiation Diffusion  Generation Model;"Although the use of multiple stacks can handle slice-to-volume motion
correction and artifact removal problems, there are still several problems: 1)
The slice-to-volume method usually uses slices as input, which cannot solve the
problem of uniform intensity distribution and complementarity in regions of
different fetal MRI stacks; 2) The integrity of 3D space is not considered,
which adversely affects the discrimination and generation of globally
consistent information in fetal MRI; 3) Fetal MRI with severe motion artifacts
in the real-world cannot achieve high-quality super-resolution reconstruction.
To address these issues, we propose a novel fetal brain MRI high-quality volume
reconstruction method, called the Radiation Diffusion Generation Model (RDGM).
It is a self-supervised generation method, which incorporates the idea of
Neural Radiation Field (NeRF) based on the coordinate generation and diffusion
model based on super-resolution generation. To solve regional intensity
heterogeneity in different directions, we use a pre-trained transformer model
for slice registration, and then, a new regionally Consistent Implicit Neural
Representation (CINR) network sub-module is proposed. CINR can generate the
initial volume by combining a coordinate association map of two different
coordinate mapping spaces. To enhance volume global consistency and
discrimination, we introduce the Volume Diffusion Super-resolution Generation
(VDSG) mechanism. The global intensity discriminant generation from
volume-to-volume is carried out using the idea of diffusion generation, and
CINR becomes the deviation intensity generation network of the volume-to-volume
diffusion model. Finally, the experimental results on real-world fetal brain
MRI stacks demonstrate the state-of-the-art performance of our method.";Junpeng Tan<author:sep>Xin Zhang<author:sep>Yao Lv<author:sep>Xiangmin Xu<author:sep>Gang Li;http://arxiv.org/pdf/2310.10209v1;eess.IV;;nerf
2310.10624v2;http://arxiv.org/abs/2310.10624v2;2023-10-16;DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and  View-Change Human-Centric Video Editing;"Despite recent progress in diffusion-based video editing, existing methods
are limited to short-length videos due to the contradiction between long-range
consistency and frame-wise editing. Prior attempts to address this challenge by
introducing video-2D representations encounter significant difficulties with
large-scale motion- and view-change videos, especially in human-centric
scenarios. To overcome this, we propose to introduce the dynamic Neural
Radiance Fields (NeRF) as the innovative video representation, where the
editing can be performed in the 3D spaces and propagated to the entire video
via the deformation field. To provide consistent and controllable editing, we
propose the image-based video-NeRF editing pipeline with a set of innovative
designs, including multi-view multi-pose Score Distillation Sampling (SDS) from
both the 2D personalized diffusion prior and 3D diffusion prior, reconstruction
losses, text-guided local parts super-resolution, and style transfer. Extensive
experiments demonstrate that our method, dubbed as DynVideo-E, significantly
outperforms SOTA approaches on two challenging datasets by a large margin of
50% ~ 95% for human preference. Code will be released at
https://showlab.github.io/DynVideo-E/.";Jia-Wei Liu<author:sep>Yan-Pei Cao<author:sep>Jay Zhangjie Wu<author:sep>Weijia Mao<author:sep>Yuchao Gu<author:sep>Rui Zhao<author:sep>Jussi Keppo<author:sep>Ying Shan<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2310.10624v2;cs.CV;Project Page: https://showlab.github.io/DynVideo-E/;nerf
2310.10642v2;http://arxiv.org/abs/2310.10642v2;2023-10-16;Real-time Photorealistic Dynamic Scene Representation and Rendering with  4D Gaussian Splatting;"Reconstructing dynamic 3D scenes from 2D images and generating diverse views
over time is challenging due to scene complexity and temporal dynamics. Despite
advancements in neural implicit models, limitations persist: (i) Inadequate
Scene Structure: Existing methods struggle to reveal the spatial and temporal
structure of dynamic scenes from directly learning the complex 6D plenoptic
function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element
deformation becomes impractical for complex dynamics. To address these issues,
we consider the spacetime as an entirety and propose to approximate the
underlying spatio-temporal 4D volume of a dynamic scene by optimizing a
collection of 4D primitives, with explicit geometry and appearance modeling.
Learning to optimize the 4D primitives enables us to synthesize novel views at
any desired time with our tailored rendering routine. Our model is conceptually
simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that
can rotate arbitrarily in space and time, as well as view-dependent and
time-evolved appearance represented by the coefficient of 4D spherindrical
harmonics. This approach offers simplicity, flexibility for variable-length
video and end-to-end training, and efficient real-time rendering, making it
suitable for capturing complex dynamic scene motions. Experiments across
various benchmarks, including monocular and multi-view scenarios, demonstrate
our 4DGS model's superior visual quality and efficiency.";Zeyu Yang<author:sep>Hongye Yang<author:sep>Zijie Pan<author:sep>Xiatian Zhu<author:sep>Li Zhang;http://arxiv.org/pdf/2310.10642v2;cs.CV;ICLR 2024;gaussian splatting
2310.10650v1;http://arxiv.org/abs/2310.10650v1;2023-10-16;TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through  Neural Radiance Fields;"Implicit representations like Neural Radiance Fields (NeRF) showed impressive
results for photorealistic rendering of complex scenes with fine details.
However, ideal or near-perfectly specular reflecting objects such as mirrors,
which are often encountered in various indoor scenes, impose ambiguities and
inconsistencies in the representation of the reconstructed scene leading to
severe artifacts in the synthesized renderings. In this paper, we present a
novel reflection tracing method tailored for the involved volume rendering
within NeRF that takes these mirror-like objects into account while avoiding
the cost of straightforward but expensive extensions through standard path
tracing. By explicitly modeling the reflection behavior using physically
plausible materials and estimating the reflected radiance with Monte-Carlo
methods within the volume rendering formulation, we derive efficient strategies
for importance sampling and the transmittance computation along rays from only
few samples. We show that our novel method enables the training of consistent
representations of such challenging scenes and achieves superior results in
comparison to previous state-of-the-art approaches.";Leif Van Holland<author:sep>Ruben Bliersbach<author:sep>Jan U. Müller<author:sep>Patrick Stotko<author:sep>Reinhard Klein;http://arxiv.org/pdf/2310.10650v1;cs.CV;;nerf
2310.09892v1;http://arxiv.org/abs/2310.09892v1;2023-10-15;Active Perception using Neural Radiance Fields;"We study active perception from first principles to argue that an autonomous
agent performing active perception should maximize the mutual information that
past observations posses about future ones. Doing so requires (a) a
representation of the scene that summarizes past observations and the ability
to update this representation to incorporate new observations (state estimation
and mapping), (b) the ability to synthesize new observations of the scene (a
generative model), and (c) the ability to select control trajectories that
maximize predictive information (planning). This motivates a neural radiance
field (NeRF)-like representation which captures photometric, geometric and
semantic properties of the scene grounded. This representation is well-suited
to synthesizing new observations from different viewpoints. And thereby, a
sampling-based planner can be used to calculate the predictive information from
synthetic observations along dynamically-feasible trajectories. We use active
perception for exploring cluttered indoor environments and employ a notion of
semantic uncertainty to check for the successful completion of an exploration
task. We demonstrate these ideas via simulation in realistic 3D indoor
environments.";Siming He<author:sep>Christopher D. Hsu<author:sep>Dexter Ong<author:sep>Yifei Simon Shao<author:sep>Pratik Chaudhari;http://arxiv.org/pdf/2310.09892v1;cs.RO;;nerf
2310.09965v1;http://arxiv.org/abs/2310.09965v1;2023-10-15;ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context;"Neural Radiance Fields (NeRFs) have recently emerged as a popular option for
photo-realistic object capture due to their ability to faithfully capture
high-fidelity volumetric content even from handheld video input. Although much
research has been devoted to efficient optimization leading to real-time
training and rendering, options for interactive editing NeRFs remain limited.
We present a very simple but effective neural network architecture that is fast
and efficient while maintaining a low memory footprint. This architecture can
be incrementally guided through user-friendly image-based edits. Our
representation allows straightforward object selection via semantic feature
distillation at the training stage. More importantly, we propose a local
3D-aware image context to facilitate view-consistent image editing that can
then be distilled into fine-tuned NeRFs, via geometric and appearance
adjustments. We evaluate our setup on a variety of examples to demonstrate
appearance and geometric edits and report 10-30x speedup over concurrent work
focusing on text-guided NeRF editing. Video results can be seen on our project
webpage at https://proteusnerf.github.io.";Binglun Wang<author:sep>Niladri Shekhar Dutt<author:sep>Niloy J. Mitra;http://arxiv.org/pdf/2310.09965v1;cs.CV;;nerf
2310.09776v1;http://arxiv.org/abs/2310.09776v1;2023-10-15;CBARF: Cascaded Bundle-Adjusting Neural Radiance Fields from Imperfect  Camera Poses;"Existing volumetric neural rendering techniques, such as Neural Radiance
Fields (NeRF), face limitations in synthesizing high-quality novel views when
the camera poses of input images are imperfect. To address this issue, we
propose a novel 3D reconstruction framework that enables simultaneous
optimization of camera poses, dubbed CBARF (Cascaded Bundle-Adjusting NeRF).In
a nutshell, our framework optimizes camera poses in a coarse-to-fine manner and
then reconstructs scenes based on the rectified poses. It is observed that the
initialization of camera poses has a significant impact on the performance of
bundle-adjustment (BA). Therefore, we cascade multiple BA modules at different
scales to progressively improve the camera poses. Meanwhile, we develop a
neighbor-replacement strategy to further optimize the results of BA in each
stage. In this step, we introduce a novel criterion to effectively identify
poorly estimated camera poses. Then we replace them with the poses of
neighboring cameras, thus further eliminating the impact of inaccurate camera
poses. Once camera poses have been optimized, we employ a density voxel grid to
generate high-quality 3D reconstructed scenes and images in novel views.
Experimental results demonstrate that our CBARF model achieves state-of-the-art
performance in both pose optimization and novel view synthesis, especially in
the existence of large camera pose noise.";Hongyu Fu<author:sep>Xin Yu<author:sep>Lincheng Li<author:sep>Li Zhang;http://arxiv.org/pdf/2310.09776v1;cs.CV;;nerf
2310.08528v2;http://arxiv.org/abs/2310.08528v2;2023-10-12;4D Gaussian Splatting for Real-Time Dynamic Scene Rendering;"Representing and rendering dynamic scenes has been an important but
challenging task. Especially, to accurately model complex motions, high
efficiency is usually hard to guarantee. To achieve real-time dynamic scene
rendering while also enjoying high training and storage efficiency, we propose
4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes
rather than applying 3D-GS for each individual frame. In 4D-GS, a novel
explicit representation containing both 3D Gaussians and 4D neural voxels is
proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is
proposed to efficiently build Gaussian features from 4D neural voxels and then
a lightweight MLP is applied to predict Gaussian deformations at novel
timestamps. Our 4D-GS method achieves real-time rendering under high
resolutions, 82 FPS at an 800$\times$800 resolution on an RTX 3090 GPU while
maintaining comparable or better quality than previous state-of-the-art
methods. More demos and code are available at
https://guanjunwu.github.io/4dgs/.";Guanjun Wu<author:sep>Taoran Yi<author:sep>Jiemin Fang<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wei Wei<author:sep>Wenyu Liu<author:sep>Qi Tian<author:sep>Xinggang Wang;http://arxiv.org/pdf/2310.08528v2;cs.CV;Project page: https://guanjunwu.github.io/4dgs/;gaussian splatting
2310.08529v2;http://arxiv.org/abs/2310.08529v2;2023-10-12;GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging  2D and 3D Diffusion Models;"In recent times, the generation of 3D assets from text prompts has shown
impressive results. Both 2D and 3D diffusion models can help generate decent 3D
objects based on prompts. 3D diffusion models have good 3D consistency, but
their quality and generalization are limited as trainable 3D data is expensive
and hard to obtain. 2D diffusion models enjoy strong abilities of
generalization and fine generation, but 3D consistency is hard to guarantee.
This paper attempts to bridge the power from the two types of diffusion models
via the recent explicit and efficient 3D Gaussian splatting representation. A
fast 3D object generation framework, named as GaussianDreamer, is proposed,
where the 3D diffusion model provides priors for initialization and the 2D
diffusion model enriches the geometry and appearance. Operations of noisy point
growing and color perturbation are introduced to enhance the initialized
Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D
avatar within 15 minutes on one GPU, much faster than previous methods, while
the generated instances can be directly rendered in real time. Demos and code
are available at https://taoranyi.com/gaussiandreamer/.";Taoran Yi<author:sep>Jiemin Fang<author:sep>Junjie Wang<author:sep>Guanjun Wu<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wenyu Liu<author:sep>Qi Tian<author:sep>Xinggang Wang;http://arxiv.org/pdf/2310.08529v2;cs.CV;Project page: https://taoranyi.com/gaussiandreamer/;gaussian splatting
2310.07179v1;http://arxiv.org/abs/2310.07179v1;2023-10-11;rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera;"Novel view synthesis of satellite images holds a wide range of practical
applications. While recent advances in the Neural Radiance Field have
predominantly targeted pin-hole cameras, and models for satellite cameras often
demand sufficient input views. This paper presents rpcPRF, a Multiplane Images
(MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC).
Unlike coordinate-based neural radiance fields in need of sufficient views of
one scene, our model is applicable to single or few inputs and performs well on
images from unseen scenes. To enable generalization across scenes, we propose
to use reprojection supervision to induce the predicted MPI to learn the
correct geometry between the 3D coordinates and the images. Moreover, we remove
the stringent requirement of dense depth supervision from deep
multiview-stereo-based methods by introducing rendering techniques of radiance
fields. rpcPRF combines the superiority of implicit representations and the
advantages of the RPC model, to capture the continuous altitude space while
learning the 3D structure. Given an RGB image and its corresponding RPC, the
end-to-end model learns to synthesize the novel view with a new RPC and
reconstruct the altitude of the scene. When multiple views are provided as
inputs, rpcPRF exerts extra supervision provided by the extra views. On the TLC
dataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF
outperforms state-of-the-art nerf-based methods by a significant margin in
terms of image fidelity, reconstruction accuracy, and efficiency, for both
single-view and multiview task.";Tongtong Zhang<author:sep>Yuanxiang Li;http://arxiv.org/pdf/2310.07179v1;cs.CV;;nerf
2310.07449v2;http://arxiv.org/abs/2310.07449v2;2023-10-11;PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction;"Neural surface reconstruction is sensitive to the camera pose noise, even if
state-of-the-art pose estimators like COLMAP or ARKit are used. More
importantly, existing Pose-NeRF joint optimisation methods have struggled to
improve pose accuracy in challenging real-world scenarios. To overcome the
challenges, we introduce the pose residual field (\textbf{PoRF}), a novel
implicit representation that uses an MLP for regressing pose updates. This is
more robust than the conventional pose parameter optimisation due to parameter
sharing that leverages global information over the entire sequence.
Furthermore, we propose an epipolar geometry loss to enhance the supervision
that leverages the correspondences exported from COLMAP results without the
extra computational overhead. Our method yields promising results. On the DTU
dataset, we reduce the rotation error by 78\% for COLMAP poses, leading to the
decreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the
MobileBrick dataset that contains casually captured unbounded 360-degree
videos, our method refines ARKit poses and improves the reconstruction F1 score
from 69.18 to 75.67, outperforming that with the dataset provided ground-truth
pose (75.14). These achievements demonstrate the efficacy of our approach in
refining camera poses and improving the accuracy of neural surface
reconstruction in real-world scenarios.";Jia-Wang Bian<author:sep>Wenjing Bian<author:sep>Victor Adrian Prisacariu<author:sep>Philip Torr;http://arxiv.org/pdf/2310.07449v2;cs.CV;Under review;nerf
2310.07916v2;http://arxiv.org/abs/2310.07916v2;2023-10-11;Dynamic Appearance Particle Neural Radiance Field;"Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D
scenes. Dynamic NeRFs extend this model by capturing time-varying elements,
typically using deformation fields. The existing dynamic NeRFs employ a similar
Eulerian representation for both light radiance and deformation fields. This
leads to a close coupling of appearance and motion and lacks a physical
interpretation. In this work, we propose Dynamic Appearance Particle Neural
Radiance Field (DAP-NeRF), which introduces particle-based representation to
model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists
of superposition of a static field and a dynamic field. The dynamic field is
quantised as a collection of {\em appearance particles}, which carries the
visual information of a small dynamic element in the scene and is equipped with
a motion model. All components, including the static field, the visual features
and motion models of the particles, are learned from monocular videos without
any prior geometric knowledge of the scene. We develop an efficient
computational framework for the particle-based model. We also construct a new
dataset to evaluate motion modelling. Experimental results show that DAP-NeRF
is an effective technique to capture not only the appearance but also the
physically meaningful motions in a 3D dynamic scene.";Ancheng Lin<author:sep>Jun Li;http://arxiv.org/pdf/2310.07916v2;cs.CV;;nerf
2310.06984v1;http://arxiv.org/abs/2310.06984v1;2023-10-10;Leveraging Neural Radiance Fields for Uncertainty-Aware Visual  Localization;"As a promising fashion for visual localization, scene coordinate regression
(SCR) has seen tremendous progress in the past decade. Most recent methods
usually adopt neural networks to learn the mapping from image pixels to 3D
scene coordinates, which requires a vast amount of annotated training data. We
propose to leverage Neural Radiance Fields (NeRF) to generate training samples
for SCR. Despite NeRF's efficiency in rendering, many of the rendered data are
polluted by artifacts or only contain minimal information gain, which can
hinder the regression accuracy or bring unnecessary computational costs with
redundant data. These challenges are addressed in three folds in this paper:
(1) A NeRF is designed to separately predict uncertainties for the rendered
color and depth images, which reveal data reliability at the pixel level. (2)
SCR is formulated as deep evidential learning with epistemic uncertainty, which
is used to evaluate information gain and scene coordinate quality. (3) Based on
the three arts of uncertainties, a novel view selection policy is formed that
significantly improves data efficiency. Experiments on public datasets
demonstrate that our method could select the samples that bring the most
information gain and promote the performance with the highest efficiency.";Le Chen<author:sep>Weirong Chen<author:sep>Rui Wang<author:sep>Marc Pollefeys;http://arxiv.org/pdf/2310.06984v1;cs.CV;8 pages, 5 figures;nerf
2310.06275v1;http://arxiv.org/abs/2310.06275v1;2023-10-10;High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying  Expression Conditioned Neural Radiance Field;"One crucial aspect of 3D head avatar reconstruction lies in the details of
facial expressions. Although recent NeRF-based photo-realistic 3D head avatar
methods achieve high-quality avatar rendering, they still encounter challenges
retaining intricate facial expression details because they overlook the
potential of specific expression variations at different spatial positions when
conditioning the radiance field. Motivated by this observation, we introduce a
novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained
by a simple MLP-based generation network, encompassing both spatial positional
features and global expression information. Benefiting from rich and diverse
information of the SVE at different positions, the proposed SVE-conditioned
neural radiance field can deal with intricate facial expressions and achieve
realistic rendering and geometry details of high-fidelity 3D head avatars.
Additionally, to further elevate the geometric and rendering quality, we
introduce a new coarse-to-fine training strategy, including a geometry
initialization strategy at the coarse stage and an adaptive importance sampling
strategy at the fine stage. Extensive experiments indicate that our method
outperforms other state-of-the-art (SOTA) methods in rendering and geometry
quality on mobile phone-collected and public datasets.";Minghan Qin<author:sep>Yifan Liu<author:sep>Yuelang Xu<author:sep>Xiaochen Zhao<author:sep>Yebin Liu<author:sep>Haoqian Wang;http://arxiv.org/pdf/2310.06275v1;cs.CV;9 pages, 5 figures;nerf
2310.05391v1;http://arxiv.org/abs/2310.05391v1;2023-10-09;Neural Impostor: Editing Neural Radiance Fields with Explicit Shape  Manipulation;"Neural Radiance Fields (NeRF) have significantly advanced the generation of
highly realistic and expressive 3D scenes. However, the task of editing NeRF,
particularly in terms of geometry modification, poses a significant challenge.
This issue has obstructed NeRF's wider adoption across various applications. To
tackle the problem of efficiently editing neural implicit fields, we introduce
Neural Impostor, a hybrid representation incorporating an explicit tetrahedral
mesh alongside a multigrid implicit field designated for each tetrahedron
within the explicit mesh. Our framework bridges the explicit shape manipulation
and the geometric editing of implicit fields by utilizing multigrid barycentric
coordinate encoding, thus offering a pragmatic solution to deform, composite,
and generate neural implicit fields while maintaining a complex volumetric
appearance. Furthermore, we propose a comprehensive pipeline for editing neural
implicit fields based on a set of explicit geometric editing operations. We
show the robustness and adaptability of our system through diverse examples and
experiments, including the editing of both synthetic objects and real captured
data. Finally, we demonstrate the authoring process of a hybrid
synthetic-captured object utilizing a variety of editing operations,
underlining the transformative potential of Neural Impostor in the field of 3D
content creation and manipulation.";Ruiyang Liu<author:sep>Jinxu Xiang<author:sep>Bowen Zhao<author:sep>Ran Zhang<author:sep>Jingyi Yu<author:sep>Changxi Zheng;http://arxiv.org/pdf/2310.05391v1;cs.GR;Accepted at Pacific Graphics 2023 and Computer Graphics Forum;nerf
2310.05837v1;http://arxiv.org/abs/2310.05837v1;2023-10-09;A Real-time Method for Inserting Virtual Objects into Neural Radiance  Fields;"We present the first real-time method for inserting a rigid virtual object
into a neural radiance field, which produces realistic lighting and shadowing
effects, as well as allows interactive manipulation of the object. By
exploiting the rich information about lighting and geometry in a NeRF, our
method overcomes several challenges of object insertion in augmented reality.
For lighting estimation, we produce accurate, robust and 3D spatially-varying
incident lighting that combines the near-field lighting from NeRF and an
environment lighting to account for sources not covered by the NeRF. For
occlusion, we blend the rendered virtual object with the background scene using
an opacity map integrated from the NeRF. For shadows, with a precomputed field
of spherical signed distance field, we query the visibility term for any point
around the virtual object, and cast soft, detailed shadows onto 3D surfaces.
Compared with state-of-the-art techniques, our approach can insert virtual
object into scenes with superior fidelity, and has a great potential to be
further applied to augmented reality systems.";Keyang Ye<author:sep>Hongzhi Wu<author:sep>Xin Tong<author:sep>Kun Zhou;http://arxiv.org/pdf/2310.05837v1;cs.CV;;nerf
2310.05133v1;http://arxiv.org/abs/2310.05133v1;2023-10-08;Geometry Aware Field-to-field Transformations for 3D Semantic  Segmentation;"We present a novel approach to perform 3D semantic segmentation solely from
2D supervision by leveraging Neural Radiance Fields (NeRFs). By extracting
features along a surface point cloud, we achieve a compact representation of
the scene which is sample-efficient and conducive to 3D reasoning. Learning
this feature space in an unsupervised manner via masked autoencoding enables
few-shot segmentation. Our method is agnostic to the scene parameterization,
working on scenes fit with any type of NeRF.";Dominik Hollidt<author:sep>Clinton Wang<author:sep>Polina Golland<author:sep>Marc Pollefeys;http://arxiv.org/pdf/2310.05133v1;cs.CV;8 pages;nerf
2310.05134v1;http://arxiv.org/abs/2310.05134v1;2023-10-08;LocoNeRF: A NeRF-based Approach for Local Structure from Motion for  Precise Localization;"Visual localization is a critical task in mobile robotics, and researchers
are continuously developing new approaches to enhance its efficiency. In this
article, we propose a novel approach to improve the accuracy of visual
localization using Structure from Motion (SfM) techniques. We highlight the
limitations of global SfM, which suffers from high latency, and the challenges
of local SfM, which requires large image databases for accurate reconstruction.
To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as
opposed to image databases, to cut down on the space required for storage. We
suggest that sampling reference images around the prior query position can lead
to further improvements. We evaluate the accuracy of our proposed method
against ground truth obtained using LIDAR and Advanced Lidar Odometry and
Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM
with COLMAP in the conducted experiments. Our proposed method achieves an
accuracy of 0.068 meters compared to the ground truth, which is slightly lower
than the most advanced method COLMAP, which has an accuracy of 0.022 meters.
However, the size of the database required for COLMAP is 400 megabytes, whereas
the size of our NeRF model is only 160 megabytes. Finally, we perform an
ablation study to assess the impact of using reference images from the NeRF
reconstruction.";Artem Nenashev<author:sep>Mikhail Kurenkov<author:sep>Andrei Potapov<author:sep>Iana Zhura<author:sep>Maksim Katerishich<author:sep>Dzmitry Tsetserukou;http://arxiv.org/pdf/2310.05134v1;cs.CV;;nerf
2310.04152v1;http://arxiv.org/abs/2310.04152v1;2023-10-06;Improving Neural Radiance Field using Near-Surface Sampling with Point  Cloud Generation;"Neural radiance field (NeRF) is an emerging view synthesis method that
samples points in a three-dimensional (3D) space and estimates their existence
and color probabilities. The disadvantage of NeRF is that it requires a long
training time since it samples many 3D points. In addition, if one samples
points from occluded regions or in the space where an object is unlikely to
exist, the rendering quality of NeRF can be degraded. These issues can be
solved by estimating the geometry of 3D scene. This paper proposes a
near-surface sampling framework to improve the rendering quality of NeRF. To
this end, the proposed method estimates the surface of a 3D object using depth
images of the training set and sampling is performed around there only. To
obtain depth information on a novel view, the paper proposes a 3D point cloud
generation method and a simple refining method for projected depth from a point
cloud. Experimental results show that the proposed near-surface sampling NeRF
framework can significantly improve the rendering quality, compared to the
original NeRF and a state-of-the-art depth-based NeRF method. In addition, one
can significantly accelerate the training time of a NeRF model with the
proposed near-surface sampling framework.";Hye Bin Yoo<author:sep>Hyun Min Han<author:sep>Sung Soo Hwang<author:sep>Il Yong Chun;http://arxiv.org/pdf/2310.04152v1;cs.CV;13 figures, 2 tables;nerf
2310.03563v1;http://arxiv.org/abs/2310.03563v1;2023-10-05;BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance  Fields;"We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which
defines the image pose estimation problem as a NeRF based iterative linear
optimization. NeRFs are novel neural space representation models that can
synthesize photorealistic novel views of real-world scenes or objects. Our
contributions are as follows: we extend the localization optimization objective
with a depth-based loss function, we introduce a multi-image based loss
function where a sequence of images with known relative poses are used without
increasing the computational complexity, we omit hierarchical sampling during
volumetric rendering, meaning only the coarse model is used for pose
estimation, and we how that by extending the sampling interval convergence can
be achieved even or higher initial pose estimate errors. With the proposed
modifications the convergence speed is significantly improved, and the basin of
convergence is substantially extended.";Ágoston István Csehi<author:sep>Csaba Máté Józsa;http://arxiv.org/pdf/2310.03563v1;cs.CV;Accepted to Nerf4ADR workshop of ICCV23 conference;nerf
2310.03375v1;http://arxiv.org/abs/2310.03375v1;2023-10-05;Point-Based Radiance Fields for Controllable Human Motion Synthesis;"This paper proposes a novel controllable human motion synthesis method for
fine-level deformation based on static point-based radiance fields. Although
previous editable neural radiance field methods can generate impressive results
on novel-view synthesis and allow naive deformation, few algorithms can achieve
complex 3D human editing such as forward kinematics. Our method exploits the
explicit point cloud to train the static 3D scene and apply the deformation by
encoding the point cloud translation using a deformation MLP. To make sure the
rendering result is consistent with the canonical space training, we estimate
the local rotation using SVD and interpolate the per-point rotation to the
query view direction of the pre-trained radiance field. Extensive experiments
show that our approach can significantly outperform the state-of-the-art on
fine-level complex deformation which can be generalized to other 3D characters
besides humans.";Haitao Yu<author:sep>Deheng Zhang<author:sep>Peiyuan Xie<author:sep>Tianyi Zhang;http://arxiv.org/pdf/2310.03375v1;cs.CV;;
2310.03578v1;http://arxiv.org/abs/2310.03578v1;2023-10-05;Targeted Adversarial Attacks on Generalizable Neural Radiance Fields;"Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for
3D scene representation and rendering. These data-driven models can learn to
synthesize high-quality images from sparse 2D observations, enabling realistic
and interactive scene reconstructions. However, the growing usage of NeRFs in
critical applications such as augmented reality, robotics, and virtual
environments could be threatened by adversarial attacks.
  In this paper we present how generalizable NeRFs can be attacked by both
low-intensity adversarial attacks and adversarial patches, where the later
could be robust enough to be used in real world applications. We also
demonstrate targeted attacks, where a specific, predefined output scene is
generated by these attack with success.";Andras Horvath<author:sep>Csaba M. Jozsa;http://arxiv.org/pdf/2310.03578v1;cs.LG;;nerf
2310.03125v1;http://arxiv.org/abs/2310.03125v1;2023-10-04;Shielding the Unseen: Privacy Protection through Poisoning NeRF with  Spatial Deformation;"In this paper, we introduce an innovative method of safeguarding user privacy
against the generative capabilities of Neural Radiance Fields (NeRF) models.
Our novel poisoning attack method induces changes to observed views that are
imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to
accurately reconstruct a 3D scene. To achieve this, we devise a bi-level
optimization algorithm incorporating a Projected Gradient Descent (PGD)-based
spatial deformation. We extensively test our approach on two common NeRF
benchmark datasets consisting of 29 real-world scenes with high-quality images.
Our results compellingly demonstrate that our privacy-preserving method
significantly impairs NeRF's performance across these benchmark datasets.
Additionally, we show that our method is adaptable and versatile, functioning
across various perturbation strengths and NeRF architectures. This work offers
valuable insights into NeRF's vulnerabilities and emphasizes the need to
account for such potential privacy risks when developing robust 3D scene
reconstruction algorithms. Our study contributes to the larger conversation
surrounding responsible AI and generative machine learning, aiming to protect
user privacy and respect creative ownership in the digital age.";Yihan Wu<author:sep>Brandon Y. Feng<author:sep>Heng Huang;http://arxiv.org/pdf/2310.03125v1;cs.CV;;nerf
2310.02712v1;http://arxiv.org/abs/2310.02712v1;2023-10-04;ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space  NeRF;"Recently, there has been a significant advancement in text-to-image diffusion
models, leading to groundbreaking performance in 2D image generation. These
advancements have been extended to 3D models, enabling the generation of novel
3D objects from textual descriptions. This has evolved into NeRF editing
methods, which allow the manipulation of existing 3D objects through textual
conditioning. However, existing NeRF editing techniques have faced limitations
in their performance due to slow training speeds and the use of loss functions
that do not adequately consider editing. To address this, here we present a
novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding
real-world scenes into the latent space of the latent diffusion model (LDM)
through a unique refinement layer. This approach enables us to obtain a NeRF
backbone that is not only faster but also more amenable to editing compared to
traditional image space NeRF editing. Furthermore, we propose an improved loss
function tailored for editing by migrating the delta denoising score (DDS)
distillation loss, originally used in 2D image editing to the three-dimensional
domain. This novel loss function surpasses the well-known score distillation
sampling (SDS) loss in terms of suitability for editing purposes. Our
experimental results demonstrate that ED-NeRF achieves faster editing speed
while producing improved output quality compared to state-of-the-art 3D editing
models.";Jangho Park<author:sep>Gihyun Kwon<author:sep>Jong Chul Ye;http://arxiv.org/pdf/2310.02712v1;cs.CV;;nerf
2310.02687v2;http://arxiv.org/abs/2310.02687v2;2023-10-04;USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields;"Neural Radiance Fields (NeRF) has received much attention recently due to its
impressive capability to represent 3D scene and synthesize novel view images.
Existing works usually assume that the input images are captured by a global
shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied
to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter
effect would also affect the accuracy of the camera pose estimation (e.g. via
COLMAP), which further prevents the success of NeRF algorithm with RS images.
In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance
Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and
recover accurate camera motion trajectory simultaneously under the framework of
NeRF, by modeling the physical image formation process of a RS camera.
Experimental results demonstrate that USB-NeRF achieves better performance
compared to prior works, in terms of RS effect removal, novel view image
synthesis as well as camera motion estimation. Furthermore, our algorithm can
also be used to recover high-fidelity high frame-rate global shutter video from
a sequence of RS images.";Moyang Li<author:sep>Peng Wang<author:sep>Lingzhe Zhao<author:sep>Bangyan Liao<author:sep>Peidong Liu;http://arxiv.org/pdf/2310.02687v2;cs.CV;;nerf
2310.02977v1;http://arxiv.org/abs/2310.02977v1;2023-10-04;T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation;"Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.";Yuze He<author:sep>Yushi Bai<author:sep>Matthieu Lin<author:sep>Wang Zhao<author:sep>Yubin Hu<author:sep>Jenny Sheng<author:sep>Ran Yi<author:sep>Juanzi Li<author:sep>Yong-Jin Liu;http://arxiv.org/pdf/2310.02977v1;cs.CV;16 pages, 11 figures;nerf
2310.03015v1;http://arxiv.org/abs/2310.03015v1;2023-10-04;Efficient-3DiM: Learning a Generalizable Single-image Novel-view  Synthesizer in One Day;"The task of novel view synthesis aims to generate unseen perspectives of an
object or scene from a limited set of input images. Nevertheless, synthesizing
novel views from a single image still remains a significant challenge in the
realm of computer vision. Previous approaches tackle this problem by adopting
mesh prediction, multi-plain image construction, or more advanced techniques
such as neural radiance fields. Recently, a pre-trained diffusion model that is
specifically designed for 2D image synthesis has demonstrated its capability in
producing photorealistic novel views, if sufficiently optimized on a 3D
finetuning task. Although the fidelity and generalizability are greatly
improved, training such a powerful diffusion model requires a vast volume of
training data and model parameters, resulting in a notoriously long time and
high computational costs. To tackle this issue, we propose Efficient-3DiM, a
simple but effective framework to learn a single-image novel-view synthesizer.
Motivated by our in-depth analysis of the inference process of diffusion
models, we propose several pragmatic strategies to reduce the training overhead
to a manageable scale, including a crafted timestep sampling strategy, a
superior 3D feature extractor, and an enhanced training scheme. When combined,
our framework is able to reduce the total training time from 10 days to less
than 1 day, significantly accelerating the training process under the same
computational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive
experiments are conducted to demonstrate the efficiency and generalizability of
our proposed method.";Yifan Jiang<author:sep>Hao Tang<author:sep>Jen-Hao Rick Chang<author:sep>Liangchen Song<author:sep>Zhangyang Wang<author:sep>Liangliang Cao;http://arxiv.org/pdf/2310.03015v1;cs.CV;;
2310.02437v2;http://arxiv.org/abs/2310.02437v2;2023-10-03;EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields;"We present EvDNeRF, a pipeline for generating event data and training an
event-based dynamic NeRF, for the purpose of faithfully reconstructing
eventstreams on scenes with rigid and non-rigid deformations that may be too
fast to capture with a standard camera. Event cameras register asynchronous
per-pixel brightness changes at MHz rates with high dynamic range, making them
ideal for observing fast motion with almost no motion blur. Neural radiance
fields (NeRFs) offer visual-quality geometric-based learnable rendering, but
prior work with events has only considered reconstruction of static scenes. Our
EvDNeRF can predict eventstreams of dynamic scenes from a static or moving
viewpoint between any desired timestamps, thereby allowing it to be used as an
event-based simulator for a given scene. We show that by training on varied
batch sizes of events, we can improve test-time predictions of events at fine
time resolutions, outperforming baselines that pair standard dynamic NeRFs with
event generators. We release our simulated and real datasets, as well as code
for multi-view event-based data generation and the training and evaluation of
EvDNeRF models (https://github.com/anish-bhattacharya/EvDNeRF).";Anish Bhattacharya<author:sep>Ratnesh Madaan<author:sep>Fernando Cladera<author:sep>Sai Vemprala<author:sep>Rogerio Bonatti<author:sep>Kostas Daniilidis<author:sep>Ashish Kapoor<author:sep>Vijay Kumar<author:sep>Nikolai Matni<author:sep>Jayesh K. Gupta;http://arxiv.org/pdf/2310.02437v2;cs.CV;16 pages, 20 figures, 2 tables;nerf
2310.01821v1;http://arxiv.org/abs/2310.01821v1;2023-10-03;MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural  Radiance Fields;"Neural radiance fields (NeRFs) have shown impressive results for novel view
synthesis. However, they depend on the repetitive use of a single-input
single-output multilayer perceptron (SISO MLP) that maps 3D coordinates and
view direction to the color and volume density in a sample-wise manner, which
slows the rendering. We propose a multi-input multi-output NeRF (MIMO-NeRF)
that reduces the number of MLPs running by replacing the SISO MLP with a MIMO
MLP and conducting mappings in a group-wise manner. One notable challenge with
this approach is that the color and volume density of each point can differ
according to a choice of input coordinates in a group, which can lead to some
notable ambiguity. We also propose a self-supervised learning method that
regularizes the MIMO MLP with multiple fast reformulated MLPs to alleviate this
ambiguity without using pretrained models. The results of a comprehensive
experimental evaluation including comparative and ablation studies are
presented to show that MIMO-NeRF obtains a good trade-off between speed and
quality with a reasonable training time. We then demonstrate that MIMO-NeRF is
compatible with and complementary to previous advancements in NeRFs by applying
it to two representative fast NeRFs, i.e., a NeRF with sample reduction
(DONeRF) and a NeRF with alternative representations (TensoRF).";Takuhiro Kaneko;http://arxiv.org/pdf/2310.01821v1;cs.CV;"Accepted to ICCV 2023. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/mimo-nerf/";nerf
2310.01881v1;http://arxiv.org/abs/2310.01881v1;2023-10-03;Adaptive Multi-NeRF: Exploit Efficient Parallelism in Adaptive Multiple  Scale Neural Radiance Field Rendering;"Recent advances in Neural Radiance Fields (NeRF) have demonstrated
significant potential for representing 3D scene appearances as implicit neural
networks, enabling the synthesis of high-fidelity novel views. However, the
lengthy training and rendering process hinders the widespread adoption of this
promising technique for real-time rendering applications. To address this
issue, we present an effective adaptive multi-NeRF method designed to
accelerate the neural rendering process for large scenes with unbalanced
workloads due to varying scene complexities.
  Our method adaptively subdivides scenes into axis-aligned bounding boxes
using a tree hierarchy approach, assigning smaller NeRFs to different-sized
subspaces based on the complexity of each scene portion. This ensures the
underlying neural representation is specific to a particular part of the scene.
We optimize scene subdivision by employing a guidance density grid, which
balances representation capability for each Multilayer Perceptron (MLP).
Consequently, samples generated by each ray can be sorted and collected for
parallel inference, achieving a balanced workload suitable for small MLPs with
consistent dimensions for regular and GPU-friendly computations. We aosl
demonstrated an efficient NeRF sampling strategy that intrinsically adapts to
increase parallelism, utilization, and reduce kernel calls, thereby achieving
much higher GPU utilization and accelerating the rendering process.";Tong Wang<author:sep>Shuichi Kurabayashi;http://arxiv.org/pdf/2310.01881v1;cs.CV;;nerf
2310.00874v1;http://arxiv.org/abs/2310.00874v1;2023-10-02;PC-NeRF: Parent-Child Neural Radiance Fields under Partial Sensor Data  Loss in Autonomous Driving Environments;"Reconstructing large-scale 3D scenes is essential for autonomous vehicles,
especially when partial sensor data is lost. Although the recently developed
neural radiance fields (NeRF) have shown compelling results in implicit
representations, the large-scale 3D scene reconstruction using partially lost
LiDAR point cloud data still needs to be explored. To bridge this gap, we
propose a novel 3D scene reconstruction framework called parent-child neural
radiance field (PC-NeRF). The framework comprises two modules, the parent NeRF
and the child NeRF, to simultaneously optimize scene-level, segment-level, and
point-level scene representations. Sensor data can be utilized more efficiently
by leveraging the segment-level representation capabilities of child NeRFs, and
an approximate volumetric representation of the scene can be quickly obtained
even with limited observations. With extensive experiments, our proposed
PC-NeRF is proven to achieve high-precision 3D reconstruction in large-scale
scenes. Moreover, PC-NeRF can effectively tackle situations where partial
sensor data is lost and has high deployment efficiency with limited training
time. Our approach implementation and the pre-trained models will be available
at https://github.com/biter0088/pc-nerf.";Xiuzhong Hu<author:sep>Guangming Xiong<author:sep>Zheng Zang<author:sep>Peng Jia<author:sep>Yuxuan Han<author:sep>Junyi Ma;http://arxiv.org/pdf/2310.00874v1;cs.CV;;nerf
2310.00684v1;http://arxiv.org/abs/2310.00684v1;2023-10-01;How Many Views Are Needed to Reconstruct an Unknown Object Using NeRF?;"Neural Radiance Fields (NeRFs) are gaining significant interest for online
active object reconstruction due to their exceptional memory efficiency and
requirement for only posed RGB inputs. Previous NeRF-based view planning
methods exhibit computational inefficiency since they rely on an iterative
paradigm, consisting of (1) retraining the NeRF when new images arrive; and (2)
planning a path to the next best view only. To address these limitations, we
propose a non-iterative pipeline based on the Prediction of the Required number
of Views (PRV). The key idea behind our approach is that the required number of
views to reconstruct an object depends on its complexity. Therefore, we design
a deep neural network, named PRVNet, to predict the required number of views,
allowing us to tailor the data acquisition based on the object complexity and
plan a globally shortest path. To train our PRVNet, we generate supervision
labels using the ShapeNet dataset. Simulated experiments show that our
PRV-based view planning method outperforms baselines, achieving good
reconstruction quality while significantly reducing movement cost and planning
time. We further justify the generalization ability of our approach in a
real-world experiment.";Sicong Pan<author:sep>Liren Jin<author:sep>Hao Hu<author:sep>Marija Popović<author:sep>Maren Bennewitz;http://arxiv.org/pdf/2310.00684v1;cs.RO;Submitted to ICRA 2024;nerf
2310.00530v3;http://arxiv.org/abs/2310.00530v3;2023-10-01;Multi-tiling Neural Radiance Field (NeRF) -- Geometric Assessment on  Large-scale Aerial Datasets;"Neural Radiance Fields (NeRF) offer the potential to benefit 3D
reconstruction tasks, including aerial photogrammetry. However, the scalability
and accuracy of the inferred geometry are not well-documented for large-scale
aerial assets,since such datasets usually result in very high memory
consumption and slow convergence.. In this paper, we aim to scale the NeRF on
large-scael aerial datasets and provide a thorough geometry assessment of NeRF.
Specifically, we introduce a location-specific sampling technique as well as a
multi-camera tiling (MCT) strategy to reduce memory consumption during image
loading for RAM, representation training for GPU memory, and increase the
convergence rate within tiles. MCT decomposes a large-frame image into multiple
tiled images with different camera models, allowing these small-frame images to
be fed into the training process as needed for specific locations without a
loss of accuracy. We implement our method on a representative approach,
Mip-NeRF, and compare its geometry performance with threephotgrammetric MVS
pipelines on two typical aerial datasets against LiDAR reference data. Both
qualitative and quantitative results suggest that the proposed NeRF approach
produces better completeness and object details than traditional approaches,
although as of now, it still falls short in terms of accuracy.";Ningli Xu<author:sep>Rongjun Qin<author:sep>Debao Huang<author:sep>Fabio Remondino;http://arxiv.org/pdf/2310.00530v3;cs.CV;9 Figure;nerf
2310.00249v1;http://arxiv.org/abs/2310.00249v1;2023-09-30;MMPI: a Flexible Radiance Field Representation by Multiple Multi-plane  Images Blending;"This paper presents a flexible representation of neural radiance fields based
on multi-plane images (MPI), for high-quality view synthesis of complex scenes.
MPI with Normalized Device Coordinate (NDC) parameterization is widely used in
NeRF learning for its simple definition, easy calculation, and powerful ability
to represent unbounded scenes. However, existing NeRF works that adopt MPI
representation for novel view synthesis can only handle simple forward-facing
unbounded scenes, where the input cameras are all observing in similar
directions with small relative translations. Hence, extending these MPI-based
methods to more complex scenes like large-range or even 360-degree scenes is
very challenging. In this paper, we explore the potential of MPI and show that
MPI can synthesize high-quality novel views of complex scenes with diverse
camera distributions and view directions, which are not only limited to simple
forward-facing scenes. Our key idea is to encode the neural radiance field with
multiple MPIs facing different directions and blend them with an adaptive
blending operation. For each region of the scene, the blending operation gives
larger blending weights to those advantaged MPIs with stronger local
representation abilities while giving lower weights to those with weaker
representation abilities. Such blending operation automatically modulates the
multiple MPIs to appropriately represent the diverse local density and color
information. Experiments on the KITTI dataset and ScanNet dataset demonstrate
that our proposed MMPI synthesizes high-quality images from diverse camera pose
distributions and is fast to train, outperforming the previous fast-training
NeRF methods for novel view synthesis. Moreover, we show that MMPI can encode
extremely long trajectories and produce novel view renderings, demonstrating
its potential in applications like autonomous driving.";Yuze He<author:sep>Peng Wang<author:sep>Yubin Hu<author:sep>Wang Zhao<author:sep>Ran Yi<author:sep>Yong-Jin Liu<author:sep>Wenping Wang;http://arxiv.org/pdf/2310.00249v1;cs.CV;;nerf
2309.17450v1;http://arxiv.org/abs/2309.17450v1;2023-09-29;Multi-task View Synthesis with Neural Radiance Fields;"Multi-task visual learning is a critical aspect of computer vision. Current
research, however, predominantly concentrates on the multi-task dense
prediction setting, which overlooks the intrinsic 3D world and its multi-view
consistent structures, and lacks the capability for versatile imagination. In
response to these limitations, we present a novel problem setting -- multi-task
view synthesis (MTVS), which reinterprets multi-task prediction as a set of
novel-view synthesis tasks for multiple scene properties, including RGB. To
tackle the MTVS problem, we propose MuvieNeRF, a framework that incorporates
both multi-task and cross-view knowledge to simultaneously synthesize multiple
scene properties. MuvieNeRF integrates two key modules, the Cross-Task
Attention (CTA) and Cross-View Attention (CVA) modules, enabling the efficient
use of information across multiple views and tasks. Extensive evaluation on
both synthetic and realistic benchmarks demonstrates that MuvieNeRF is capable
of simultaneously synthesizing different scene properties with promising visual
quality, even outperforming conventional discriminative models in various
settings. Notably, we show that MuvieNeRF exhibits universal applicability
across a range of NeRF backbones. Our code is available at
https://github.com/zsh2000/MuvieNeRF.";Shuhong Zheng<author:sep>Zhipeng Bao<author:sep>Martial Hebert<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2309.17450v1;cs.CV;ICCV 2023, Website: https://zsh2000.github.io/mtvs.github.io/;nerf
2309.17390v1;http://arxiv.org/abs/2309.17390v1;2023-09-29;Forward Flow for Novel View Synthesis of Dynamic Scenes;"This paper proposes a neural radiance field (NeRF) approach for novel view
synthesis of dynamic scenes using forward warping. Existing methods often adopt
a static NeRF to represent the canonical space, and render dynamic images at
other time steps by mapping the sampled 3D points back to the canonical space
with the learned backward flow field. However, this backward flow field is
non-smooth and discontinuous, which is difficult to be fitted by commonly used
smooth motion models. To address this problem, we propose to estimate the
forward flow field and directly warp the canonical radiance field to other time
steps. Such forward flow field is smooth and continuous within the object
region, which benefits the motion model learning. To achieve this goal, we
represent the canonical radiance field with voxel grids to enable efficient
forward warping, and propose a differentiable warping process, including an
average splatting operation and an inpaint network, to resolve the many-to-one
and one-to-many mapping issues. Thorough experiments show that our method
outperforms existing methods in both novel view rendering and motion modeling,
demonstrating the effectiveness of our forward flow motion modeling. Project
page: https://npucvr.github.io/ForwardFlowDNeRF";Xiang Guo<author:sep>Jiadai Sun<author:sep>Yuchao Dai<author:sep>Guanying Chen<author:sep>Xiaoqing Ye<author:sep>Xiao Tan<author:sep>Errui Ding<author:sep>Yumeng Zhang<author:sep>Jingdong Wang;http://arxiv.org/pdf/2309.17390v1;cs.CV;"Accepted by ICCV2023 as oral. Project page:
  https://npucvr.github.io/ForwardFlowDNeRF";nerf
2309.17128v1;http://arxiv.org/abs/2309.17128v1;2023-09-29;HAvatar: High-fidelity Head Avatar via Facial Model Conditioned Neural  Radiance Field;"The problem of modeling an animatable 3D human head avatar under light-weight
setups is of significant importance but has not been well solved. Existing 3D
representations either perform well in the realism of portrait images synthesis
or the accuracy of expression control, but not both. To address the problem, we
introduce a novel hybrid explicit-implicit 3D representation, Facial Model
Conditioned Neural Radiance Field, which integrates the expressiveness of NeRF
and the prior information from the parametric template. At the core of our
representation, a synthetic-renderings-based condition method is proposed to
fuse the prior information from the parametric model into the implicit field
without constraining its topological flexibility. Besides, based on the hybrid
representation, we properly overcome the inconsistent shape issue presented in
existing methods and improve the animation stability. Moreover, by adopting an
overall GAN-based architecture using an image-to-image translation network, we
achieve high-resolution, realistic and view-consistent synthesis of dynamic
head appearance. Experiments demonstrate that our method can achieve
state-of-the-art performance for 3D head avatar animation compared with
previous methods.";Xiaochen Zhao<author:sep>Lizhen Wang<author:sep>Jingxiang Sun<author:sep>Hongwen Zhang<author:sep>Jinli Suo<author:sep>Yebin Liu;http://arxiv.org/pdf/2309.17128v1;cs.CV;;nerf
2309.16110v1;http://arxiv.org/abs/2309.16110v1;2023-09-28;Learning Effective NeRFs and SDFs Representations with 3D Generative  Adversarial Networks for 3D Object Generation: Technical Report for ICCV 2023  OmniObject3D Challenge;"In this technical report, we present a solution for 3D object generation of
ICCV 2023 OmniObject3D Challenge. In recent years, 3D object generation has
made great process and achieved promising results, but it remains a challenging
task due to the difficulty of generating complex, textured and high-fidelity
results. To resolve this problem, we study learning effective NeRFs and SDFs
representations with 3D Generative Adversarial Networks (GANs) for 3D object
generation. Specifically, inspired by recent works, we use the efficient
geometry-aware 3D GANs as the backbone incorporating with label embedding and
color mapping, which enables to train the model on different taxonomies
simultaneously. Then, through a decoder, we aggregate the resulting features to
generate Neural Radiance Fields (NeRFs) based representations for rendering
high-fidelity synthetic images. Meanwhile, we optimize Signed Distance
Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we
observe that this model can be effectively trained with only a few images of
each object from a variety of classes, instead of using a great number of
images per object or training one model per class. With this pipeline, we can
optimize an effective model for 3D object generation. This solution is one of
the final top-3-place solutions in the ICCV 2023 OmniObject3D Challenge.";Zheyuan Yang<author:sep>Yibo Liu<author:sep>Guile Wu<author:sep>Tongtong Cao<author:sep>Yuan Ren<author:sep>Yang Liu<author:sep>Bingbing Liu;http://arxiv.org/pdf/2309.16110v1;cs.CV;;nerf
2309.16364v2;http://arxiv.org/abs/2309.16364v2;2023-09-28;FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for  Independence-Assumption-Free Uncertainty Estimation;"Neural radiance fields with stochasticity have garnered significant interest
by enabling the sampling of plausible radiance fields and quantifying
uncertainty for downstream tasks. Existing works rely on the independence
assumption of points in the radiance field or the pixels in input views to
obtain tractable forms of the probability density function. However, this
assumption inadvertently impacts performance when dealing with intricate
geometry and texture. In this work, we propose an independence-assumption-free
probabilistic neural radiance field based on Flow-GAN. By combining the
generative capability of adversarial learning and the powerful expressivity of
normalizing flow, our method explicitly models the density-radiance
distribution of the whole scene. We represent our probabilistic NeRF as a
mean-shifted probabilistic residual neural model. Our model is trained without
an explicit likelihood function, thereby avoiding the independence assumption.
Specifically, We downsample the training images with different strides and
centers to form fixed-size patches which are used to train the generator with
patch-based adversarial learning. Through extensive experiments, our method
demonstrates state-of-the-art performance by predicting lower rendering errors
and more reliable uncertainty on both synthetic and real-world datasets.";Songlin Wei<author:sep>Jiazhao Zhang<author:sep>Yang Wang<author:sep>Fanbo Xiang<author:sep>Hao Su<author:sep>He Wang;http://arxiv.org/pdf/2309.16364v2;cs.CV;;nerf
2309.16859v1;http://arxiv.org/abs/2309.16859v1;2023-09-28;Preface: A Data-driven Volumetric Prior for Few-shot Ultra  High-resolution Face Synthesis;"NeRFs have enabled highly realistic synthesis of human faces including
complex appearance and reflectance effects of hair and skin. These methods
typically require a large number of multi-view input images, making the process
hardware intensive and cumbersome, limiting applicability to unconstrained
settings. We propose a novel volumetric human face prior that enables the
synthesis of ultra high-resolution novel views of subjects that are not part of
the prior's training distribution. This prior model consists of an
identity-conditioned NeRF, trained on a dataset of low-resolution multi-view
images of diverse humans with known camera calibration. A simple sparse
landmark-based 3D alignment of the training dataset allows our model to learn a
smooth latent space of geometry and appearance despite a limited number of
training identities. A high-quality volumetric representation of a novel
subject can be obtained by model fitting to 2 or 3 camera views of arbitrary
resolution. Importantly, our method requires as few as two views of casually
captured images as input at inference time.";Marcel C. Bühler<author:sep>Kripasindhu Sarkar<author:sep>Tanmay Shah<author:sep>Gengyan Li<author:sep>Daoye Wang<author:sep>Leonhard Helminger<author:sep>Sergio Orts-Escolano<author:sep>Dmitry Lagun<author:sep>Otmar Hilliges<author:sep>Thabo Beeler<author:sep>Abhimitra Meka;http://arxiv.org/pdf/2309.16859v1;cs.CV;"In Proceedings of the IEEE/CVF International Conference on Computer
  Vision, 2023";nerf
2310.13700v1;http://arxiv.org/abs/2310.13700v1;2023-09-28;Augmenting Heritage: An Open-Source Multiplatform AR Application;"AI NeRF algorithms, capable of cloud processing, have significantly reduced
hardware requirements and processing efficiency in photogrammetry pipelines.
This accessibility has unlocked the potential for museums, charities, and
cultural heritage sites worldwide to leverage mobile devices for artifact
scanning and processing. However, the adoption of augmented reality platforms
often necessitates the installation of proprietary applications on users'
mobile devices, which adds complexity to development and limits global
availability. This paper presents a case study that demonstrates a
cost-effective pipeline for visualizing scanned museum artifacts using mobile
augmented reality, leveraging an open-source embedded solution on a website.";Corrie Green;http://arxiv.org/pdf/2310.13700v1;cs.HC;;nerf
2309.16585v3;http://arxiv.org/abs/2309.16585v3;2023-09-28;Text-to-3D using Gaussian Splatting;"In this paper, we present Gaussian Splatting based text-to-3D generation
(GSGEN), a novel approach for generating high-quality 3D objects. Previous
methods suffer from inaccurate geometry and limited fidelity due to the absence
of 3D prior and proper representation. We leverage 3D Gaussian Splatting, a
recent state-of-the-art representation, to address existing shortcomings by
exploiting the explicit nature that enables the incorporation of 3D prior.
Specifically, our method adopts a progressive optimization strategy, which
includes a geometry optimization stage and an appearance refinement stage. In
geometry optimization, a coarse representation is established under a 3D
geometry prior along with the ordinary 2D SDS loss, ensuring a sensible and
3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an
iterative refinement to enrich details. In this stage, we increase the number
of Gaussians by compactness-based densification to enhance continuity and
improve fidelity. With these designs, our approach can generate 3D content with
delicate details and more accurate geometry. Extensive evaluations demonstrate
the effectiveness of our method, especially for capturing high-frequency
components. Video results are provided at https://gsgen3d.github.io. Our code
is available at https://github.com/gsgen3d/gsgen";Zilong Chen<author:sep>Feng Wang<author:sep>Huaping Liu;http://arxiv.org/pdf/2309.16585v3;cs.CV;"Project page: https://gsgen3d.github.io. Code:
  https://github.com/gsgen3d/gsgen";gaussian splatting
2309.16553v1;http://arxiv.org/abs/2309.16553v1;2023-09-28;MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering  and Beyond;"Neural radiance fields (NeRF) and its subsequent variants have led to
remarkable progress in neural rendering. While most of recent neural rendering
works focus on objects and small-scale scenes, developing neural rendering
methods for city-scale scenes is of great potential in many real-world
applications. However, this line of research is impeded by the absence of a
comprehensive and high-quality dataset, yet collecting such a dataset over real
city-scale scenes is costly, sensitive, and technically difficult. To this end,
we build a large-scale, comprehensive, and high-quality synthetic dataset for
city-scale neural rendering researches. Leveraging the Unreal Engine 5 City
Sample project, we develop a pipeline to easily collect aerial and street city
views, accompanied by ground-truth camera poses and a range of additional data
modalities. Flexible controls over environmental factors like light, weather,
human and car crowd are also available in our pipeline, supporting the need of
various tasks covering city-scale neural rendering and beyond. The resulting
pilot dataset, MatrixCity, contains 67k aerial images and 452k street images
from two city maps of total size $28km^2$. On top of MatrixCity, a thorough
benchmark is also conducted, which not only reveals unique challenges of the
task of city-scale neural rendering, but also highlights potential improvements
for future works. The dataset and code will be publicly available at our
project page: https://city-super.github.io/matrixcity/.";Yixuan Li<author:sep>Lihan Jiang<author:sep>Linning Xu<author:sep>Yuanbo Xiangli<author:sep>Zhenzhi Wang<author:sep>Dahua Lin<author:sep>Bo Dai;http://arxiv.org/pdf/2309.16553v1;cs.CV;"Accepted to ICCV 2023. Project page:
  $\href{https://city-super.github.io/matrixcity/}{this\, https\, URL}$";nerf
2309.16653v1;http://arxiv.org/abs/2309.16653v1;2023-09-28;DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content  Creation;"Recent advances in 3D content creation mostly leverage optimization-based 3D
generation via score distillation sampling (SDS). Though promising results have
been exhibited, these methods often suffer from slow per-sample optimization,
limiting their practical usage. In this paper, we propose DreamGaussian, a
novel 3D content generation framework that achieves both efficiency and quality
simultaneously. Our key insight is to design a generative 3D Gaussian Splatting
model with companioned mesh extraction and texture refinement in UV space. In
contrast to the occupancy pruning used in Neural Radiance Fields, we
demonstrate that the progressive densification of 3D Gaussians converges
significantly faster for 3D generative tasks. To further enhance the texture
quality and facilitate downstream applications, we introduce an efficient
algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning
stage to refine the details. Extensive experiments demonstrate the superior
efficiency and competitive generation quality of our proposed approach.
Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes
from a single-view image, achieving approximately 10 times acceleration
compared to existing methods.";Jiaxiang Tang<author:sep>Jiawei Ren<author:sep>Hang Zhou<author:sep>Ziwei Liu<author:sep>Gang Zeng;http://arxiv.org/pdf/2309.16653v1;cs.CV;project page: https://dreamgaussian.github.io/;gaussian splatting
2309.15526v1;http://arxiv.org/abs/2309.15526v1;2023-09-27;P2I-NET: Mapping Camera Pose to Image via Adversarial Learning for New  View Synthesis in Real Indoor Environments;"Given a new $6DoF$ camera pose in an indoor environment, we study the
challenging problem of predicting the view from that pose based on a set of
reference RGBD views. Existing explicit or implicit 3D geometry construction
methods are computationally expensive while those based on learning have
predominantly focused on isolated views of object categories with regular
geometric structure. Differing from the traditional \textit{render-inpaint}
approach to new view synthesis in the real indoor environment, we propose a
conditional generative adversarial neural network (P2I-NET) to directly predict
the new view from the given pose. P2I-NET learns the conditional distribution
of the images of the environment for establishing the correspondence between
the camera pose and its view of the environment, and achieves this through a
number of innovative designs in its architecture and training lost function.
Two auxiliary discriminator constraints are introduced for enforcing the
consistency between the pose of the generated image and that of the
corresponding real world image in both the latent feature space and the real
world pose space. Additionally a deep convolutional neural network (CNN) is
introduced to further reinforce this consistency in the pixel space. We have
performed extensive new view synthesis experiments on real indoor datasets.
Results show that P2I-NET has superior performance against a number of NeRF
based strong baseline models. In particular, we show that P2I-NET is 40 to 100
times faster than these competitor techniques while synthesising similar
quality images. Furthermore, we contribute a new publicly available indoor
environment dataset containing 22 high resolution RGBD videos where each frame
also has accurate camera pose parameters.";Xujie Kang<author:sep>Kanglin Liu<author:sep>Jiang Duan<author:sep>Yuanhao Gong<author:sep>Guoping Qiu;http://arxiv.org/pdf/2309.15526v1;cs.CV;;nerf
2309.15426v1;http://arxiv.org/abs/2309.15426v1;2023-09-27;NeuRBF: A Neural Fields Representation with Adaptive Radial Basis  Functions;"We present a novel type of neural fields that uses general radial bases for
signal representation. State-of-the-art neural fields typically rely on
grid-based representations for storing local neural features and N-dimensional
linear kernels for interpolating features at continuous query points. The
spatial positions of their neural features are fixed on grid nodes and cannot
well adapt to target signals. Our method instead builds upon general radial
bases with flexible kernel position and shape, which have higher spatial
adaptivity and can more closely fit target signals. To further improve the
channel-wise capacity of radial basis functions, we propose to compose them
with multi-frequency sinusoid functions. This technique extends a radial basis
to multiple Fourier radial bases of different frequency bands without requiring
extra parameters, facilitating the representation of details. Moreover, by
marrying adaptive radial bases with grid-based ones, our hybrid combination
inherits both adaptivity and interpolation smoothness. We carefully designed
weighting schemes to let radial bases adapt to different types of signals
effectively. Our experiments on 2D image and 3D signed distance field
representation demonstrate the higher accuracy and compactness of our method
than prior arts. When applied to neural radiance field reconstruction, our
method achieves state-of-the-art rendering quality, with small model size and
comparable training speed.";Zhang Chen<author:sep>Zhong Li<author:sep>Liangchen Song<author:sep>Lele Chen<author:sep>Jingyi Yu<author:sep>Junsong Yuan<author:sep>Yi Xu;http://arxiv.org/pdf/2309.15426v1;cs.CV;"Accepted to ICCV 2023 Oral. Project page:
  https://oppo-us-research.github.io/NeuRBF-website/";
2309.15329v1;http://arxiv.org/abs/2309.15329v1;2023-09-27;BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction  using Neural Radiance Fields;"Reconstruction of deformable scenes from endoscopic videos is important for
many applications such as intraoperative navigation, surgical visual
perception, and robotic surgery. It is a foundational requirement for realizing
autonomous robotic interventions for minimally invasive surgery. However,
previous approaches in this domain have been limited by their modular nature
and are confined to specific camera and scene settings. Our work adopts the
Neural Radiance Fields (NeRF) approach to learning 3D implicit representations
of scenes that are both dynamic and deformable over time, and furthermore with
unknown camera poses. We demonstrate this approach on endoscopic surgical
scenes from robotic surgery. This work removes the constraints of known camera
poses and overcomes the drawbacks of the state-of-the-art unstructured dynamic
scene reconstruction technique, which relies on the static part of the scene
for accurate reconstruction. Through several experimental datasets, we
demonstrate the versatility of our proposed model to adapt to diverse camera
and scene settings, and show its promise for both current and future robotic
surgical systems.";Shreya Saha<author:sep>Sainan Liu<author:sep>Shan Lin<author:sep>Jingpei Lu<author:sep>Michael Yip;http://arxiv.org/pdf/2309.15329v1;cs.CV;;nerf
2309.14800v1;http://arxiv.org/abs/2309.14800v1;2023-09-26;3D Density-Gradient based Edge Detection on Neural Radiance Fields  (NeRFs) for Geometric Reconstruction;"Generating geometric 3D reconstructions from Neural Radiance Fields (NeRFs)
is of great interest. However, accurate and complete reconstructions based on
the density values are challenging. The network output depends on input data,
NeRF network configuration and hyperparameter. As a result, the direct usage of
density values, e.g. via filtering with global density thresholds, usually
requires empirical investigations. Under the assumption that the density
increases from non-object to object area, the utilization of density gradients
from relative values is evident. As the density represents a position-dependent
parameter it can be handled anisotropically, therefore processing of the
voxelized 3D density field is justified. In this regard, we address geometric
3D reconstructions based on density gradients, whereas the gradients result
from 3D edge detection filters of the first and second derivatives, namely
Sobel, Canny and Laplacian of Gaussian. The gradients rely on relative
neighboring density values in all directions, thus are independent from
absolute magnitudes. Consequently, gradient filters are able to extract edges
along a wide density range, almost independent from assumptions and empirical
investigations. Our approach demonstrates the capability to achieve geometric
3D reconstructions with high geometric accuracy on object surfaces and
remarkable object completeness. Notably, Canny filter effectively eliminates
gaps, delivers a uniform point density, and strikes a favorable balance between
correctness and completeness across the scenes.";Miriam Jäger<author:sep>Boris Jutzi;http://arxiv.org/pdf/2309.14800v1;cs.CV;"8 pages, 4 figures, 2 tables. Will be published in the ISPRS The
  International Archives of Photogrammetry, Remote Sensing and Spatial
  Information Sciences";nerf
2309.14010v1;http://arxiv.org/abs/2309.14010v1;2023-09-25;Variational Inference for Scalable 3D Object-centric Learning;"We tackle the task of scalable unsupervised object-centric representation
learning on 3D scenes. Existing approaches to object-centric representation
learning show limitations in generalizing to larger scenes as their learning
processes rely on a fixed global coordinate system. In contrast, we propose to
learn view-invariant 3D object representations in localized object coordinate
systems. To this end, we estimate the object pose and appearance representation
separately and explicitly map object representations across views while
maintaining object identities. We adopt an amortized variational inference
pipeline that can process sequential input and scalably update object latent
distributions online. To handle large-scale scenes with a varying number of
objects, we further introduce a Cognitive Map that allows the registration and
query of objects on a per-scene global map to achieve scalable representation
learning. We explore the object-centric neural radiance field (NeRF) as our 3D
scene representation, which is jointly modeled within our unsupervised
object-centric learning framework. Experimental results on synthetic and real
datasets show that our proposed method can infer and maintain object-centric
representations of 3D scenes and outperforms previous models.";Tianyu Wang<author:sep>Kee Siong Ng<author:sep>Miaomiao Liu;http://arxiv.org/pdf/2309.14010v1;cs.CV;;nerf
2309.14291v1;http://arxiv.org/abs/2309.14291v1;2023-09-25;Tiled Multiplane Images for Practical 3D Photography;"The task of synthesizing novel views from a single image has useful
applications in virtual reality and mobile computing, and a number of
approaches to the problem have been proposed in recent years. A Multiplane
Image (MPI) estimates the scene as a stack of RGBA layers, and can model
complex appearance effects, anti-alias depth errors and synthesize soft edges
better than methods that use textured meshes or layered depth images. And
unlike neural radiance fields, an MPI can be efficiently rendered on graphics
hardware. However, MPIs are highly redundant and require a large number of
depth layers to achieve plausible results. Based on the observation that the
depth complexity in local image regions is lower than that over the entire
image, we split an MPI into many small, tiled regions, each with only a few
depth planes. We call this representation a Tiled Multiplane Image (TMPI). We
propose a method for generating a TMPI with adaptive depth planes for
single-view 3D photography in the wild. Our synthesized results are comparable
to state-of-the-art single-view MPI methods while having lower computational
overhead.";Numair Khan<author:sep>Douglas Lanman<author:sep>Lei Xiao;http://arxiv.org/pdf/2309.14291v1;cs.CV;ICCV 2023;
2309.14293v3;http://arxiv.org/abs/2309.14293v3;2023-09-25;NAS-NeRF: Generative Neural Architecture Search for Neural Radiance  Fields;"Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their high computational complexity limits deployability. While existing
neural-based solutions strive for efficiency, they use one-size-fits-all
architectures regardless of scene complexity. The same architecture may be
unnecessarily large for simple scenes but insufficient for complex ones. Thus,
there is a need to dynamically optimize the neural network component of NeRFs
to achieve a balance between computational complexity and specific targets for
synthesis quality. We introduce NAS-NeRF, a generative neural architecture
search strategy that generates compact, scene-specialized NeRF architectures by
balancing architecture complexity and target synthesis quality metrics. Our
method incorporates constraints on target metrics and budgets to guide the
search towards architectures tailored for each scene. Experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made
publicly available at https://saeejithnair.github.io/NAS-NeRF.";Saeejith Nair<author:sep>Yuhao Chen<author:sep>Mohammad Javad Shafiee<author:sep>Alexander Wong;http://arxiv.org/pdf/2309.14293v3;cs.CV;8 pages;nerf
2309.13607v2;http://arxiv.org/abs/2309.13607v2;2023-09-24;MM-NeRF: Multimodal-Guided 3D Multi-Style Transfer of Neural Radiance  Field;"3D style transfer aims to generate stylized views of 3D scenes with specified
styles, which requires high-quality generating and keeping multi-view
consistency. Existing methods still suffer the challenges of high-quality
stylization with texture details and stylization with multimodal guidance. In
this paper, we reveal that the common training method of stylization with NeRF,
which generates stylized multi-view supervision by 2D style transfer models,
causes the same object in supervision to show various states (color tone,
details, etc.) in different views, leading NeRF to tend to smooth the texture
details, further resulting in low-quality rendering for 3D multi-style
transfer. To tackle these problems, we propose a novel Multimodal-guided 3D
Multi-style transfer of NeRF, termed MM-NeRF. First, MM-NeRF projects
multimodal guidance into a unified space to keep the multimodal styles
consistency and extracts multimodal features to guide the 3D stylization.
Second, a novel multi-head learning scheme is proposed to relieve the
difficulty of learning multi-style transfer, and a multi-view style consistent
loss is proposed to track the inconsistency of multi-view supervision data.
Finally, a novel incremental learning mechanism to generalize MM-NeRF to any
new style with small costs. Extensive experiments on several real-world
datasets show that MM-NeRF achieves high-quality 3D multi-style stylization
with multimodal guidance, and keeps multi-view consistency and style
consistency between multimodal guidance. Codes will be released.";Zijiang Yang<author:sep>Zhongwei Qiu<author:sep>Chang Xu<author:sep>Dongmei Fu;http://arxiv.org/pdf/2309.13607v2;cs.CV;;nerf
2309.13240v1;http://arxiv.org/abs/2309.13240v1;2023-09-23;NeRF-Enhanced Outpainting for Faithful Field-of-View Extrapolation;"In various applications, such as robotic navigation and remote visual
assistance, expanding the field of view (FOV) of the camera proves beneficial
for enhancing environmental perception. Unlike image outpainting techniques
aimed solely at generating aesthetically pleasing visuals, these applications
demand an extended view that faithfully represents the scene. To achieve this,
we formulate a new problem of faithful FOV extrapolation that utilizes a set of
pre-captured images as prior knowledge of the scene. To address this problem,
we present a simple yet effective solution called NeRF-Enhanced Outpainting
(NEO) that uses extended-FOV images generated through NeRF to train a
scene-specific image outpainting model. To assess the performance of NEO, we
conduct comprehensive evaluations on three photorealistic datasets and one
real-world dataset. Extensive experiments on the benchmark datasets showcase
the robustness and potential of our method in addressing this challenge. We
believe our work lays a strong foundation for future exploration within the
research community.";Rui Yu<author:sep>Jiachen Liu<author:sep>Zihan Zhou<author:sep>Sharon X. Huang;http://arxiv.org/pdf/2309.13240v1;cs.CV;;nerf
2309.13039v1;http://arxiv.org/abs/2309.13039v1;2023-09-22;NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular  Objects with Neural Refractive-Reflective Fields;"Neural radiance fields (NeRF) have revolutionized the field of image-based
view synthesis. However, NeRF uses straight rays and fails to deal with
complicated light path changes caused by refraction and reflection. This
prevents NeRF from successfully synthesizing transparent or specular objects,
which are ubiquitous in real-world robotics and A/VR applications. In this
paper, we introduce the refractive-reflective field. Taking the object
silhouette as input, we first utilize marching tetrahedra with a progressive
encoding to reconstruct the geometry of non-Lambertian objects and then model
refraction and reflection effects of the object in a unified framework using
Fresnel terms. Meanwhile, to achieve efficient and effective anti-aliasing, we
propose a virtual cone supersampling technique. We benchmark our method on
different shapes, backgrounds and Fresnel terms on both real-world and
synthetic datasets. We also qualitatively and quantitatively benchmark the
rendering results of various editing applications, including material editing,
object replacement/insertion, and environment illumination estimation. Codes
and data are publicly available at https://github.com/dawning77/NeRRF.";Xiaoxue Chen<author:sep>Junchen Liu<author:sep>Hao Zhao<author:sep>Guyue Zhou<author:sep>Ya-Qin Zhang;http://arxiv.org/pdf/2309.13039v1;cs.CV;;nerf
2309.12642v1;http://arxiv.org/abs/2309.12642v1;2023-09-22;RHINO: Regularizing the Hash-based Implicit Neural Representation;"The use of Implicit Neural Representation (INR) through a hash-table has
demonstrated impressive effectiveness and efficiency in characterizing
intricate signals. However, current state-of-the-art methods exhibit
insufficient regularization, often yielding unreliable and noisy results during
interpolations. We find that this issue stems from broken gradient flow between
input coordinates and indexed hash-keys, where the chain rule attempts to model
discrete hash-keys, rather than the continuous coordinates. To tackle this
concern, we introduce RHINO, in which a continuous analytical function is
incorporated to facilitate regularization by connecting the input coordinate
and the network additionally without modifying the architecture of current
hash-based INRs. This connection ensures a seamless backpropagation of
gradients from the network's output back to the input coordinates, thereby
enhancing regularization. Our experimental results not only showcase the
broadened regularization capability across different hash-based INRs like DINER
and Instant NGP, but also across a variety of tasks such as image fitting,
representation of signed distance functions, and optimization of 5D static / 6D
dynamic neural radiance fields. Notably, RHINO outperforms current
state-of-the-art techniques in both quality and speed, affirming its
superiority.";Hao Zhu<author:sep>Fengyi Liu<author:sep>Qi Zhang<author:sep>Xun Cao<author:sep>Zhan Ma;http://arxiv.org/pdf/2309.12642v1;cs.CV;17 pages, 11 figures;
2309.13101v2;http://arxiv.org/abs/2309.13101v2;2023-09-22;Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene  Reconstruction;"Implicit neural representation has paved the way for new approaches to
dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic
neural rendering methods rely heavily on these implicit representations, which
frequently struggle to capture the intricate details of objects in the scene.
Furthermore, implicit methods have difficulty achieving real-time rendering in
general dynamic scenes, limiting their use in a variety of tasks. To address
the issues, we propose a deformable 3D Gaussians Splatting method that
reconstructs scenes using 3D Gaussians and learns them in canonical space with
a deformation field to model monocular dynamic scenes. We also introduce an
annealing smoothing training mechanism with no extra overhead, which can
mitigate the impact of inaccurate poses on the smoothness of time interpolation
tasks in real-world datasets. Through a differential Gaussian rasterizer, the
deformable 3D Gaussians not only achieve higher rendering quality but also
real-time rendering speed. Experiments show that our method outperforms
existing methods significantly in terms of both rendering quality and speed,
making it well-suited for tasks such as novel-view synthesis, time
interpolation, and real-time rendering.";Ziyi Yang<author:sep>Xinyu Gao<author:sep>Wen Zhou<author:sep>Shaohui Jiao<author:sep>Yuqing Zhang<author:sep>Xiaogang Jin;http://arxiv.org/pdf/2309.13101v2;cs.CV;;
2309.11966v1;http://arxiv.org/abs/2309.11966v1;2023-09-21;NeuralLabeling: A versatile toolset for labeling vision datasets using  Neural Radiance Fields;"We present NeuralLabeling, a labeling approach and toolset for annotating a
scene using either bounding boxes or meshes and generating segmentation masks,
affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth
maps and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as
renderer, allowing labeling to be performed using 3D spatial tools while
incorporating geometric clues such as occlusions, relying only on images
captured from multiple viewpoints as input. To demonstrate the applicability of
NeuralLabeling to a practical problem in robotics, we added ground truth depth
maps to 30000 frames of transparent object RGB and noisy depth maps of glasses
placed in a dishwasher captured using an RGBD sensor, yielding the
Dishwasher30k dataset. We show that training a simple deep neural network with
supervision using the annotated depth maps yields a higher reconstruction
performance than training with the previously applied weakly supervised
approach.";Floris Erich<author:sep>Naoya Chiba<author:sep>Yusuke Yoshiyasu<author:sep>Noriaki Ando<author:sep>Ryo Hanai<author:sep>Yukiyasu Domae;http://arxiv.org/pdf/2309.11966v1;cs.CV;"8 pages, project website:
  https://florise.github.io/neural_labeling_web/";nerf
2309.12183v1;http://arxiv.org/abs/2309.12183v1;2023-09-21;ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average  Texture and Mesh Encoding;"In 3D human shape and pose estimation from a monocular video, models trained
with limited labeled data cannot generalize well to videos with occlusion,
which is common in the wild videos. The recent human neural rendering
approaches focusing on novel view synthesis initialized by the off-the-shelf
human shape and pose methods have the potential to correct the initial human
shape. However, the existing methods have some drawbacks such as, erroneous in
handling occlusion, sensitive to inaccurate human segmentation, and ineffective
loss computation due to the non-regularized opacity field. To address these
problems, we introduce ORTexME, an occlusion-robust temporal method that
utilizes temporal information from the input video to better regularize the
occluded body parts. While our ORTexME is based on NeRF, to determine the
reliable regions for the NeRF ray sampling, we utilize our novel average
texture learning approach to learn the average appearance of a person, and to
infer a mask based on the average texture. In addition, to guide the
opacity-field updates in NeRF to suppress blur and noise, we propose the use of
human body mesh. The quantitative evaluation demonstrates that our method
achieves significant improvement on the challenging multi-person 3DPW dataset,
where our method achieves 1.8 P-MPJPE error reduction. The SOTA rendering-based
methods fail and enlarge the error up to 5.6 on the same dataset.";Yu Cheng<author:sep>Bo Wang<author:sep>Robby T. Tan;http://arxiv.org/pdf/2309.12183v1;cs.CV;8 pages, 8 figures;nerf
2309.11698v1;http://arxiv.org/abs/2309.11698v1;2023-09-21;Rendering stable features improves sampling-based localisation with  Neural radiance fields;"Neural radiance fields (NeRFs) are a powerful tool for implicit scene
representations, allowing for differentiable rendering and the ability to make
predictions about previously unseen viewpoints. From a robotics perspective,
there has been growing interest in object and scene-based localisation using
NeRFs, with a number of recent works relying on sampling-based or Monte-Carlo
localisation schemes. Unfortunately, these can be extremely computationally
expensive, requiring multiple network forward passes to infer camera or object
pose. To alleviate this, a variety of sampling strategies have been applied,
many relying on keypoint recognition techniques from classical computer vision.
This work conducts a systematic empirical comparison of these approaches and
shows that in contrast to conventional feature matching approaches for
geometry-based localisation, sampling-based localisation using NeRFs benefits
significantly from stable features. Results show that rendering stable features
can result in a tenfold reduction in the number of forward passes required, a
significant speed improvement.";Boxuan Zhang<author:sep>Lindsay Kleeman<author:sep>Michael Burke;http://arxiv.org/pdf/2309.11698v1;cs.RO;;nerf
2309.11767v1;http://arxiv.org/abs/2309.11767v1;2023-09-21;Fast Satellite Tensorial Radiance Field for Multi-date Satellite Imagery  of Large Size;"Existing NeRF models for satellite images suffer from slow speeds, mandatory
solar information as input, and limitations in handling large satellite images.
In response, we present SatensoRF, which significantly accelerates the entire
process while employing fewer parameters for satellite imagery of large size.
Besides, we observed that the prevalent assumption of Lambertian surfaces in
neural radiance fields falls short for vegetative and aquatic elements. In
contrast to the traditional hierarchical MLP-based scene representation, we
have chosen a multiscale tensor decomposition approach for color, volume
density, and auxiliary variables to model the lightfield with specular color.
Additionally, to rectify inconsistencies in multi-date imagery, we incorporate
total variation loss to restore the density tensor field and treat the problem
as a denosing task.To validate our approach, we conducted assessments of
SatensoRF using subsets from the spacenet multi-view dataset, which includes
both multi-date and single-date multi-view RGB images. Our results clearly
demonstrate that SatensoRF surpasses the state-of-the-art Sat-NeRF series in
terms of novel view synthesis performance. Significantly, SatensoRF requires
fewer parameters for training, resulting in faster training and inference
speeds and reduced computational demands.";Tongtong Zhang<author:sep>Yuanxiang Li;http://arxiv.org/pdf/2309.11767v1;cs.CV;;nerf
2309.11747v1;http://arxiv.org/abs/2309.11747v1;2023-09-21;MarkNerf:Watermarking for Neural Radiance Field;"A watermarking algorithm is proposed in this paper to address the copyright
protection issue of implicit 3D models. The algorithm involves embedding
watermarks into the images in the training set through an embedding network,
and subsequently utilizing the NeRF model for 3D modeling. A copyright verifier
is employed to generate a backdoor image by providing a secret perspective as
input to the neural radiation field. Subsequently, a watermark extractor is
devised using the hyperparameterization method of the neural network to extract
the embedded watermark image from that perspective. In a black box scenario, if
there is a suspicion that the 3D model has been used without authorization, the
verifier can extract watermarks from a secret perspective to verify network
copyright. Experimental results demonstrate that the proposed algorithm
effectively safeguards the copyright of 3D models. Furthermore, the extracted
watermarks exhibit favorable visual effects and demonstrate robust resistance
against various types of noise attacks.";Lifeng Chen<author:sep>Jia Liu<author:sep>Yan Ke<author:sep>Wenquan Sun<author:sep>Weina Dong<author:sep>Xiaozhong Pan;http://arxiv.org/pdf/2309.11747v1;cs.CR;;nerf
2309.11281v2;http://arxiv.org/abs/2309.11281v2;2023-09-20;Language-driven Object Fusion into Neural Radiance Fields with  Pose-Conditioned Dataset Updates;"Neural radiance field is an emerging rendering method that generates
high-quality multi-view consistent images from a neural scene representation
and volume rendering. Although neural radiance field-based techniques are
robust for scene reconstruction, their ability to add or remove objects remains
limited. This paper proposes a new language-driven approach for object
manipulation with neural radiance fields through dataset updates. Specifically,
to insert a new foreground object represented by a set of multi-view images
into a background radiance field, we use a text-to-image diffusion model to
learn and generate combined images that fuse the object of interest into the
given background across views. These combined images are then used for refining
the background radiance field so that we can render view-consistent images
containing both the object and the background. To ensure view consistency, we
propose a dataset updates strategy that prioritizes radiance field training
with camera views close to the already-trained views prior to propagating the
training to remaining views. We show that under the same dataset updates
strategy, we can easily adapt our method for object insertion using data from
text-to-3D models as well as object removal. Experimental results show that our
method generates photorealistic images of the edited scenes, and outperforms
state-of-the-art methods in 3D reconstruction and neural radiance field
blending.";Ka Chun Shum<author:sep>Jaeyeon Kim<author:sep>Binh-Son Hua<author:sep>Duc Thanh Nguyen<author:sep>Sai-Kit Yeung;http://arxiv.org/pdf/2309.11281v2;cs.CV;;
2309.10987v3;http://arxiv.org/abs/2309.10987v3;2023-09-20;SpikingNeRF: Making Bio-inspired Neural Networks See through the Real  World;"Spiking neural networks (SNNs) have been thriving on numerous tasks to
leverage their promising energy efficiency and exploit their potentialities as
biologically plausible intelligence. Meanwhile, the Neural Radiance Fields
(NeRF) render high-quality 3D scenes with massive energy consumption, but few
works delve into the energy-saving solution with a bio-inspired approach. In
this paper, we propose SpikingNeRF, which aligns the radiance ray with the
temporal dimension of SNN, to naturally accommodate the SNN to the
reconstruction of Radiance Fields. Thus, the computation turns into a
spike-based, multiplication-free manner, reducing the energy consumption. In
SpikingNeRF, each sampled point on the ray is matched onto a particular time
step, and represented in a hybrid manner where the voxel grids are maintained
as well. Based on the voxel grids, sampled points are determined whether to be
masked for better training and inference. However, this operation also incurs
irregular temporal length. We propose the temporal padding strategy to tackle
the masked samples to maintain regular temporal length, i.e., regular tensors,
and the temporal condensing strategy to form a denser data structure for
hardware-friendly computation. Extensive experiments on various datasets
demonstrate that our method reduces the 70.79% energy consumption on average
and obtains comparable synthesis quality with the ANN baseline.";Xingting Yao<author:sep>Qinghao Hu<author:sep>Tielong Liu<author:sep>Zitao Mo<author:sep>Zeyu Zhu<author:sep>Zhengyang Zhuge<author:sep>Jian Cheng;http://arxiv.org/pdf/2309.10987v3;cs.NE;;nerf
2309.11525v2;http://arxiv.org/abs/2309.11525v2;2023-09-20;Light Field Diffusion for Single-View Novel View Synthesis;"Single-view novel view synthesis, the task of generating images from new
viewpoints based on a single reference image, is an important but challenging
task in computer vision. Recently, Denoising Diffusion Probabilistic Model
(DDPM) has become popular in this area due to its strong ability to generate
high-fidelity images. However, current diffusion-based methods directly rely on
camera pose matrices as viewing conditions, globally and implicitly introducing
3D constraints. These methods may suffer from inconsistency among generated
images from different perspectives, especially in regions with intricate
textures and structures. In this work, we present Light Field Diffusion (LFD),
a conditional diffusion-based model for single-view novel view synthesis.
Unlike previous methods that employ camera pose matrices, LFD transforms the
camera view information into light field encoding and combines it with the
reference image. This design introduces local pixel-wise constraints within the
diffusion models, thereby encouraging better multi-view consistency.
Experiments on several datasets show that our LFD can efficiently generate
high-fidelity images and maintain better 3D consistency even in intricate
regions. Our method can generate images with higher quality than NeRF-based
models, and we obtain sample quality similar to other diffusion-based models
but with only one-third of the model size.";Yifeng Xiong<author:sep>Haoyu Ma<author:sep>Shanlin Sun<author:sep>Kun Han<author:sep>Xiaohui Xie;http://arxiv.org/pdf/2309.11525v2;cs.CV;;nerf
2309.11627v1;http://arxiv.org/abs/2309.11627v1;2023-09-20;GenLayNeRF: Generalizable Layered Representations with 3D Model  Alignment for Multi-Human View Synthesis;"Novel view synthesis (NVS) of multi-human scenes imposes challenges due to
the complex inter-human occlusions. Layered representations handle the
complexities by dividing the scene into multi-layered radiance fields, however,
they are mainly constrained to per-scene optimization making them inefficient.
Generalizable human view synthesis methods combine the pre-fitted 3D human
meshes with image features to reach generalization, yet they are mainly
designed to operate on single-human scenes. Another drawback is the reliance on
multi-step optimization techniques for parametric pre-fitting of the 3D body
models that suffer from misalignment with the images in sparse view settings
causing hallucinations in synthesized views. In this work, we propose,
GenLayNeRF, a generalizable layered scene representation for free-viewpoint
rendering of multiple human subjects which requires no per-scene optimization
and very sparse views as input. We divide the scene into multi-human layers
anchored by the 3D body meshes. We then ensure pixel-level alignment of the
body models with the input views through a novel end-to-end trainable module
that carries out iterative parametric correction coupled with multi-view
feature fusion to produce aligned 3D models. For NVS, we extract point-wise
image-aligned and human-anchored features which are correlated and fused using
self-attention and cross-attention modules. We augment low-level RGB values
into the features with an attention-based RGB fusion module. To evaluate our
approach, we construct two multi-human view synthesis datasets; DeepMultiSyn
and ZJU-MultiHuman. The results indicate that our proposed approach outperforms
generalizable and non-human per-scene NeRF methods while performing at par with
layered per-scene methods without test time optimization.";Youssef Abdelkareem<author:sep>Shady Shehata<author:sep>Fakhri Karray;http://arxiv.org/pdf/2309.11627v1;cs.CV;Accepted to GCPR 2023;nerf
2309.11009v2;http://arxiv.org/abs/2309.11009v2;2023-09-20;Controllable Dynamic Appearance for Neural 3D Portraits;"Recent advances in Neural Radiance Fields (NeRFs) have made it possible to
reconstruct and reanimate dynamic portrait scenes with control over head-pose,
facial expressions and viewing direction. However, training such models assumes
photometric consistency over the deformed region e.g. the face must be evenly
lit as it deforms with changing head-pose and facial expression. Such
photometric consistency across frames of a video is hard to maintain, even in
studio environments, thus making the created reanimatable neural portraits
prone to artifacts during reanimation. In this work, we propose CoDyNeRF, a
system that enables the creation of fully controllable 3D portraits in
real-world capture conditions. CoDyNeRF learns to approximate illumination
dependent effects via a dynamic appearance model in the canonical space that is
conditioned on predicted surface normals and the facial expressions and
head-pose deformations. The surface normals prediction is guided using 3DMM
normals that act as a coarse prior for the normals of the human head, where
direct prediction of normals is hard due to rigid and non-rigid deformations
induced by head-pose and facial expression changes. Using only a
smartphone-captured short video of a subject for training, we demonstrate the
effectiveness of our method on free view synthesis of a portrait scene with
explicit head pose and expression controls, and realistic lighting effects. The
project page can be found here:
http://shahrukhathar.github.io/2023/08/22/CoDyNeRF.html";ShahRukh Athar<author:sep>Zhixin Shu<author:sep>Zexiang Xu<author:sep>Fujun Luan<author:sep>Sai Bi<author:sep>Kalyan Sunkavalli<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2309.11009v2;cs.CV;;nerf
2309.10503v1;http://arxiv.org/abs/2309.10503v1;2023-09-19;Steganography for Neural Radiance Fields by Backdooring;"The utilization of implicit representation for visual data (such as images,
videos, and 3D models) has recently gained significant attention in computer
vision research. In this letter, we propose a novel model steganography scheme
with implicit neural representation. The message sender leverages Neural
Radiance Fields (NeRF) and its viewpoint synthesis capabilities by introducing
a viewpoint as a key. The NeRF model generates a secret viewpoint image, which
serves as a backdoor. Subsequently, we train a message extractor using
overfitting to establish a one-to-one mapping between the secret message and
the secret viewpoint image. The sender delivers the trained NeRF model and the
message extractor to the receiver over the open channel, and the receiver
utilizes the key shared by both parties to obtain the rendered image in the
secret view from the NeRF model, and then obtains the secret message through
the message extractor. The inherent complexity of the viewpoint information
prevents attackers from stealing the secret message accurately. Experimental
results demonstrate that the message extractor trained in this letter achieves
high-capacity steganography with fast performance, achieving a 100\% accuracy
in message extraction. Furthermore, the extensive viewpoint key space of NeRF
ensures the security of the steganography scheme.";Weina Dong<author:sep>Jia Liu<author:sep>Yan Ke<author:sep>Lifeng Chen<author:sep>Wenquan Sun<author:sep>Xiaozhong Pan;http://arxiv.org/pdf/2309.10503v1;cs.CR;6 pages, 7 figures;nerf
2309.10684v1;http://arxiv.org/abs/2309.10684v1;2023-09-19;Locally Stylized Neural Radiance Fields;"In recent years, there has been increasing interest in applying stylization
on 3D scenes from a reference style image, in particular onto neural radiance
fields (NeRF). While performing stylization directly on NeRF guarantees
appearance consistency over arbitrary novel views, it is a challenging problem
to guide the transfer of patterns from the style image onto different parts of
the NeRF scene. In this work, we propose a stylization framework for NeRF based
on local style transfer. In particular, we use a hash-grid encoding to learn
the embedding of the appearance and geometry components, and show that the
mapping defined by the hash table allows us to control the stylization to a
certain extent. Stylization is then achieved by optimizing the appearance
branch while keeping the geometry branch fixed. To support local style
transfer, we propose a new loss function that utilizes a segmentation network
and bipartite matching to establish region correspondences between the style
image and the content images obtained from volume rendering. Our experiments
show that our method yields plausible stylization results with novel view
synthesis while having flexible controllability via manipulating and
customizing the region correspondences.";Hong-Wing Pang<author:sep>Binh-Son Hua<author:sep>Sai-Kit Yeung;http://arxiv.org/pdf/2309.10684v1;cs.CV;ICCV 2023;nerf
2309.09502v1;http://arxiv.org/abs/2309.09502v1;2023-09-18;RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering  Supervision;"3D occupancy prediction holds significant promise in the fields of robot
perception and autonomous driving, which quantifies 3D scenes into grid cells
with semantic labels. Recent works mainly utilize complete occupancy labels in
3D voxel space for supervision. However, the expensive annotation process and
sometimes ambiguous labels have severely constrained the usability and
scalability of 3D occupancy models. To address this, we present RenderOcc, a
novel paradigm for training 3D occupancy models only using 2D labels.
Specifically, we extract a NeRF-style 3D volume representation from multi-view
images, and employ volume rendering techniques to establish 2D renderings, thus
enabling direct 3D supervision from 2D semantics and depth labels.
Additionally, we introduce an Auxiliary Ray method to tackle the issue of
sparse viewpoints in autonomous driving scenarios, which leverages sequential
frames to construct comprehensive 2D rendering for each object. To our best
knowledge, RenderOcc is the first attempt to train multi-view 3D occupancy
models only using 2D labels, reducing the dependence on costly 3D occupancy
annotations. Extensive experiments demonstrate that RenderOcc achieves
comparable performance to models fully supervised with 3D labels, underscoring
the significance of this approach in real-world applications.";Mingjie Pan<author:sep>Jiaming Liu<author:sep>Renrui Zhang<author:sep>Peixiang Huang<author:sep>Xiaoqi Li<author:sep>Li Liu<author:sep>Shanghang Zhang;http://arxiv.org/pdf/2309.09502v1;cs.CV;;nerf
2309.10011v2;http://arxiv.org/abs/2309.10011v2;2023-09-18;Instant Photorealistic Style Transfer: A Lightweight and Adaptive  Approach;"In this paper, we propose an Instant Photorealistic Style Transfer (IPST)
approach, designed to achieve instant photorealistic style transfer on
super-resolution inputs without the need for pre-training on pair-wise datasets
or imposing extra constraints. Our method utilizes a lightweight StyleNet to
enable style transfer from a style image to a content image while preserving
non-color information. To further enhance the style transfer process, we
introduce an instance-adaptive optimization to prioritize the photorealism of
outputs and accelerate the convergence of the style network, leading to a rapid
training completion within seconds. Moreover, IPST is well-suited for
multi-frame style transfer tasks, as it retains temporal and multi-view
consistency of the multi-frame inputs such as video and Neural Radiance Field
(NeRF). Experimental results demonstrate that IPST requires less GPU memory
usage, offers faster multi-frame transfer speed, and generates photorealistic
outputs, making it a promising solution for various photorealistic transfer
applications.";Rong Liu<author:sep>Enyu Zhao<author:sep>Zhiyuan Liu<author:sep>Andrew Feng<author:sep>Scott John Easley;http://arxiv.org/pdf/2309.10011v2;cs.CV;8 pages (reference excluded), 6 figures, 4 tables;nerf
2309.09295v1;http://arxiv.org/abs/2309.09295v1;2023-09-17;NeRF-VINS: A Real-time Neural Radiance Field Map-based Visual-Inertial  Navigation System;"Achieving accurate, efficient, and consistent localization within an a priori
environment map remains a fundamental challenge in robotics and computer
vision. Conventional map-based keyframe localization often suffers from
sub-optimal viewpoints due to limited field of view (FOV), thus degrading its
performance. To address this issue, in this paper, we design a real-time
tightly-coupled Neural Radiance Fields (NeRF)-aided visual-inertial navigation
system (VINS), termed NeRF-VINS. By effectively leveraging NeRF's potential to
synthesize novel views, essential for addressing limited viewpoints, the
proposed NeRF-VINS optimally fuses IMU and monocular image measurements along
with synthetically rendered images within an efficient filter-based framework.
This tightly coupled integration enables 3D motion tracking with bounded error.
We extensively compare the proposed NeRF-VINS against the state-of-the-art
methods that use prior map information, which is shown to achieve superior
performance. We also demonstrate the proposed method is able to perform
real-time estimation at 15 Hz, on a resource-constrained Jetson AGX Orin
embedded platform with impressive accuracy.";Saimouli Katragadda<author:sep>Woosik Lee<author:sep>Yuxiang Peng<author:sep>Patrick Geneva<author:sep>Chuchu Chen<author:sep>Chao Guo<author:sep>Mingyang Li<author:sep>Guoquan Huang;http://arxiv.org/pdf/2309.09295v1;cs.RO;6 pages, 7 figures;nerf
2309.08927v1;http://arxiv.org/abs/2309.08927v1;2023-09-16;DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic  NeRF;"Dynamic reconstruction with neural radiance fields (NeRF) requires accurate
camera poses. These are often hard to retrieve with existing
structure-from-motion (SfM) pipelines as both camera and scene content can
change. We propose DynaMoN that leverages simultaneous localization and mapping
(SLAM) jointly with motion masking to handle dynamic scene content. Our robust
SLAM-based tracking module significantly accelerates the training process of
the dynamic NeRF while improving the quality of synthesized views at the same
time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and
the DyCheck's iPhone dataset, three real-world datasets, shows the advantages
of DynaMoN both for camera pose estimation and novel view synthesis.";Mert Asim Karaoglu<author:sep>Hannah Schieber<author:sep>Nicolas Schischka<author:sep>Melih Görgülü<author:sep>Florian Grötzner<author:sep>Alexander Ladikos<author:sep>Daniel Roth<author:sep>Nassir Navab<author:sep>Benjamin Busam;http://arxiv.org/pdf/2309.08927v1;cs.CV;6 pages, 4 figures;nerf
2309.08523v2;http://arxiv.org/abs/2309.08523v2;2023-09-15;Breathing New Life into 3D Assets with Generative Repainting;"Diffusion-based text-to-image models ignited immense attention from the
vision community, artists, and content creators. Broad adoption of these models
is due to significant improvement in the quality of generations and efficient
conditioning on various modalities, not just text. However, lifting the rich
generative priors of these 2D models into 3D is challenging. Recent works have
proposed various pipelines powered by the entanglement of diffusion models and
neural fields. We explore the power of pretrained 2D diffusion models and
standard 3D neural radiance fields as independent, standalone tools and
demonstrate their ability to work together in a non-learned fashion. Such
modularity has the intrinsic advantage of eased partial upgrades, which became
an important property in such a fast-paced domain. Our pipeline accepts any
legacy renderable geometry, such as textured or untextured meshes, orchestrates
the interaction between 2D generative refinement and 3D consistency enforcement
tools, and outputs a painted input geometry in several formats. We conduct a
large-scale study on a wide range of objects and categories from the
ShapeNetSem dataset and demonstrate the advantages of our approach, both
qualitatively and quantitatively. Project page:
https://www.obukhov.ai/repainting_3d_assets";Tianfu Wang<author:sep>Menelaos Kanakis<author:sep>Konrad Schindler<author:sep>Luc Van Gool<author:sep>Anton Obukhov;http://arxiv.org/pdf/2309.08523v2;cs.CV;;
2309.08596v1;http://arxiv.org/abs/2309.08596v1;2023-09-15;Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion;"Event cameras offer many advantages over standard cameras due to their
distinctive principle of operation: low power, low latency, high temporal
resolution and high dynamic range. Nonetheless, the success of many downstream
visual applications also hinges on an efficient and effective scene
representation, where Neural Radiance Field (NeRF) is seen as the leading
candidate. Such promise and potential of event cameras and NeRF inspired recent
works to investigate on the reconstruction of NeRF from moving event cameras.
However, these works are mainly limited in terms of the dependence on dense and
low-noise event streams, as well as generalization to arbitrary contrast
threshold values and camera speed profiles. In this work, we propose Robust
e-NeRF, a novel method to directly and robustly reconstruct NeRFs from moving
event cameras under various real-world conditions, especially from sparse and
noisy events generated under non-uniform motion. It consists of two key
components: a realistic event generation model that accounts for various
intrinsic parameters (e.g. time-independent, asymmetric threshold and
refractory period) and non-idealities (e.g. pixel-to-pixel threshold
variation), as well as a complementary pair of normalized reconstruction losses
that can effectively generalize to arbitrary speed profiles and intrinsic
parameter values without such prior knowledge. Experiments on real and novel
realistically simulated sequences verify our effectiveness. Our code, synthetic
dataset and improved event simulator are public.";Weng Fei Low<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2309.08596v1;cs.CV;"Accepted to ICCV 2023. Project website is accessible at
  https://wengflow.github.io/robust-e-nerf";nerf
2309.08416v2;http://arxiv.org/abs/2309.08416v2;2023-09-15;Deformable Neural Radiance Fields using RGB and Event Cameras;"Modeling Neural Radiance Fields for fast-moving deformable objects from
visual data alone is a challenging problem. A major issue arises due to the
high deformation and low acquisition rates. To address this problem, we propose
to use event cameras that offer very fast acquisition of visual change in an
asynchronous manner. In this work, we develop a novel method to model the
deformable neural radiance fields using RGB and event cameras. The proposed
method uses the asynchronous stream of events and calibrated sparse RGB frames.
In our setup, the camera pose at the individual events required to integrate
them into the radiance fields remains unknown. Our method jointly optimizes
these poses and the radiance field. This happens efficiently by leveraging the
collection of events at once and actively sampling the events during learning.
Experiments conducted on both realistically rendered graphics and real-world
datasets demonstrate a significant benefit of the proposed method over the
state-of-the-art and the compared baseline.
  This shows a promising direction for modeling deformable neural radiance
fields in real-world dynamic scenes.";Qi Ma<author:sep>Danda Pani Paudel<author:sep>Ajad Chhatkuli<author:sep>Luc Van Gool;http://arxiv.org/pdf/2309.08416v2;cs.CV;;
2309.08040v1;http://arxiv.org/abs/2309.08040v1;2023-09-14;Gradient based Grasp Pose Optimization on a NeRF that Approximates Grasp  Success;"Current robotic grasping methods often rely on estimating the pose of the
target object, explicitly predicting grasp poses, or implicitly estimating
grasp success probabilities. In this work, we propose a novel approach that
directly maps gripper poses to their corresponding grasp success values,
without considering objectness. Specifically, we leverage a Neural Radiance
Field (NeRF) architecture to learn a scene representation and use it to train a
grasp success estimator that maps each pose in the robot's task space to a
grasp success value. We employ this learned estimator to tune its inputs, i.e.,
grasp poses, by gradient-based optimization to obtain successful grasp poses.
Contrary to other NeRF-based methods which enhance existing grasp pose
estimation approaches by relying on NeRF's rendering capabilities or directly
estimate grasp poses in a discretized space using NeRF's scene representation
capabilities, our approach uniquely sidesteps both the need for rendering and
the limitation of discretization. We demonstrate the effectiveness of our
approach on four simulated 3DoF (Degree of Freedom) robotic grasping tasks and
show that it can generalize to novel objects. Our best model achieves an
average translation error of 3mm from valid grasp poses. This work opens the
door for future research to apply our approach to higher DoF grasps and
real-world scenarios.";Gergely Sóti<author:sep>Björn Hein<author:sep>Christian Wurll;http://arxiv.org/pdf/2309.08040v1;cs.RO;;nerf
2309.07846v2;http://arxiv.org/abs/2309.07846v2;2023-09-14;MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image  Acquisition Systems;"Neural Radiance Fields (NeRF) employ multi-view images for 3D scene
representation and have shown remarkable performance. As one of the primary
sources of multi-view images, multi-camera systems encounter challenges such as
varying intrinsic parameters and frequent pose changes. Most previous
NeRF-based methods often assume a global unique camera and seldom consider
scenarios with multiple cameras. Besides, some pose-robust methods still remain
susceptible to suboptimal solutions when poses are poor initialized. In this
paper, we propose MC-NeRF, a method can jointly optimize both intrinsic and
extrinsic parameters for bundle-adjusting Neural Radiance Fields. Firstly, we
conduct a theoretical analysis to tackle the degenerate case and coupling issue
that arise from the joint optimization between intrinsic and extrinsic
parameters. Secondly, based on the proposed solutions, we introduce an
efficient calibration image acquisition scheme for multi-camera systems,
including the design of calibration object. Lastly, we present a global
end-to-end network with training sequence that enables the regression of
intrinsic and extrinsic parameters, along with the rendering network. Moreover,
most existing datasets are designed for unique camera, we create a new dataset
that includes four different styles of multi-camera acquisition systems,
allowing readers to generate custom datasets. Experiments confirm the
effectiveness of our method when each image corresponds to different camera
parameters. Specifically, we adopt up to 110 images with 110 different
intrinsic and extrinsic parameters, to achieve 3D scene representation without
providing initial poses. The Code and supplementary materials are available at
https://in2-viaun.github.io/MC-NeRF.";Yu Gao<author:sep>Lutong Su<author:sep>Hao Liang<author:sep>Yufeng Yue<author:sep>Yi Yang<author:sep>Mengyin Fu;http://arxiv.org/pdf/2309.07846v2;cs.CV;This manuscript is currently under review;nerf
2309.07752v1;http://arxiv.org/abs/2309.07752v1;2023-09-14;DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for  High-Fidelity Talking Portrait Synthesis;"In this paper, we present the decomposed triplane-hash neural radiance fields
(DT-NeRF), a framework that significantly improves the photorealistic rendering
of talking faces and achieves state-of-the-art results on key evaluation
datasets. Our architecture decomposes the facial region into two specialized
triplanes: one specialized for representing the mouth, and the other for the
broader facial features. We introduce audio features as residual terms and
integrate them as query vectors into our model through an audio-mouth-face
transformer. Additionally, our method leverages the capabilities of Neural
Radiance Fields (NeRF) to enrich the volumetric representation of the entire
face through additive volumetric rendering techniques. Comprehensive
experimental evaluations corroborate the effectiveness and superiority of our
proposed approach.";Yaoyu Su<author:sep>Shaohui Wang<author:sep>Haoqian Wang;http://arxiv.org/pdf/2309.07752v1;cs.CV;5 pages, 5 figures. Submitted to ICASSP 2024;nerf
2309.07640v2;http://arxiv.org/abs/2309.07640v2;2023-09-14;Indoor Scene Reconstruction with Fine-Grained Details Using Hybrid  Representation and Normal Prior Enhancement;"The reconstruction of indoor scenes from multi-view RGB images is challenging
due to the coexistence of flat and texture-less regions alongside delicate and
fine-grained regions. Recent methods leverage neural radiance fields aided by
predicted surface normal priors to recover the scene geometry. These methods
excel in producing complete and smooth results for floor and wall areas.
However, they struggle to capture complex surfaces with high-frequency
structures due to the inadequate neural representation and the inaccurately
predicted normal priors. This work aims to reconstruct high-fidelity surfaces
with fine-grained details by addressing the above limitations. To improve the
capacity of the implicit representation, we propose a hybrid architecture to
represent low-frequency and high-frequency regions separately. To enhance the
normal priors, we introduce a simple yet effective image sharpening and
denoising technique, coupled with a network that estimates the pixel-wise
uncertainty of the predicted surface normal vectors. Identifying such
uncertainty can prevent our model from being misled by unreliable surface
normal supervisions that hinder the accurate reconstruction of intricate
geometries. Experiments on the benchmark datasets show that our method
outperforms existing methods in terms of reconstruction quality. Furthermore,
the proposed method also generalizes well to real-world indoor scenarios
captured by our hand-held mobile phones. Our code is publicly available at:
https://github.com/yec22/Fine-Grained-Indoor-Recon.";Sheng Ye<author:sep>Yubin Hu<author:sep>Matthieu Lin<author:sep>Yu-Hui Wen<author:sep>Wang Zhao<author:sep>Yong-Jin Liu<author:sep>Wenping Wang;http://arxiv.org/pdf/2309.07640v2;cs.CV;;
2309.07668v1;http://arxiv.org/abs/2309.07668v1;2023-09-14;CoRF : Colorizing Radiance Fields using Knowledge Distillation;"Neural radiance field (NeRF) based methods enable high-quality novel-view
synthesis for multi-view images. This work presents a method for synthesizing
colorized novel views from input grey-scale multi-view images. When we apply
image or video-based colorization methods on the generated grey-scale novel
views, we observe artifacts due to inconsistency across views. Training a
radiance field network on the colorized grey-scale image sequence also does not
solve the 3D consistency issue. We propose a distillation based method to
transfer color knowledge from the colorization networks trained on natural
images to the radiance field network. Specifically, our method uses the
radiance field network as a 3D representation and transfers knowledge from
existing 2D colorization methods. The experimental results demonstrate that the
proposed method produces superior colorized novel views for indoor and outdoor
scenes while maintaining cross-view consistency than baselines. Further, we
show the efficacy of our method on applications like colorization of radiance
field network trained from 1.) Infra-Red (IR) multi-view images and 2.) Old
grey-scale multi-view image sequences.";Ankit Dhiman<author:sep>R Srinath<author:sep>Srinjay Sarkar<author:sep>Lokesh R Boregowda<author:sep>R Venkatesh Babu;http://arxiv.org/pdf/2309.07668v1;cs.CV;AI3DCC @ ICCV 2023;nerf
2310.12987v1;http://arxiv.org/abs/2310.12987v1;2023-09-14;Spec-NeRF: Multi-spectral Neural Radiance Fields;"We propose Multi-spectral Neural Radiance Fields(Spec-NeRF) for jointly
reconstructing a multispectral radiance field and spectral sensitivity
functions(SSFs) of the camera from a set of color images filtered by different
filters. The proposed method focuses on modeling the physical imaging process,
and applies the estimated SSFs and radiance field to synthesize novel views of
multispectral scenes. In this method, the data acquisition requires only a
low-cost trichromatic camera and several off-the-shelf color filters, making it
more practical than using specialized 3D scanning and spectral imaging
equipment. Our experiments on both synthetic and real scenario datasets
demonstrate that utilizing filtered RGB images with learnable NeRF and SSFs can
achieve high fidelity and promising spectral reconstruction while retaining the
inherent capability of NeRF to comprehend geometric structures. Code is
available at https://github.com/CPREgroup/SpecNeRF-v2.";Jiabao Li<author:sep>Yuqi Li<author:sep>Ciliang Sun<author:sep>Chong Wang<author:sep>Jinhui Xiang;http://arxiv.org/pdf/2310.12987v1;eess.IV;;nerf
2309.06802v1;http://arxiv.org/abs/2309.06802v1;2023-09-13;Dynamic NeRFs for Soccer Scenes;"The long-standing problem of novel view synthesis has many applications,
notably in sports broadcasting. Photorealistic novel view synthesis of soccer
actions, in particular, is of enormous interest to the broadcast industry. Yet
only a few industrial solutions have been proposed, and even fewer that achieve
near-broadcast quality of the synthetic replays. Except for their setup of
multiple static cameras around the playfield, the best proprietary systems
disclose close to no information about their inner workings. Leveraging
multiple static cameras for such a task indeed presents a challenge rarely
tackled in the literature, for a lack of public datasets: the reconstruction of
a large-scale, mostly static environment, with small, fast-moving elements.
Recently, the emergence of neural radiance fields has induced stunning progress
in many novel view synthesis applications, leveraging deep learning principles
to produce photorealistic results in the most challenging settings. In this
work, we investigate the feasibility of basing a solution to the task on
dynamic NeRFs, i.e., neural models purposed to reconstruct general dynamic
content. We compose synthetic soccer environments and conduct multiple
experiments using them, identifying key components that help reconstruct soccer
scenes with dynamic NeRFs. We show that, although this approach cannot fully
meet the quality requirements for the target application, it suggests promising
avenues toward a cost-efficient, automatic solution. We also make our work
dataset and code publicly available, with the goal to encourage further efforts
from the research community on the task of novel view synthesis for dynamic
soccer scenes. For code, data, and video results, please see
https://soccernerfs.isach.be.";Sacha Lewin<author:sep>Maxime Vandegar<author:sep>Thomas Hoyoux<author:sep>Olivier Barnich<author:sep>Gilles Louppe;http://arxiv.org/pdf/2309.06802v1;cs.CV;"Accepted at the 6th International ACM Workshop on Multimedia Content
  Analysis in Sports. 8 pages, 9 figures. Project page:
  https://soccernerfs.isach.be";nerf
2309.07125v1;http://arxiv.org/abs/2309.07125v1;2023-09-13;Text-Guided Generation and Editing of Compositional 3D Avatars;"Our goal is to create a realistic 3D facial avatar with hair and accessories
using only a text description. While this challenge has attracted significant
recent interest, existing methods either lack realism, produce unrealistic
shapes, or do not support editing, such as modifications to the hairstyle. We
argue that existing methods are limited because they employ a monolithic
modeling approach, using a single representation for the head, face, hair, and
accessories. Our observation is that the hair and face, for example, have very
different structural qualities that benefit from different representations.
Building on this insight, we generate avatars with a compositional model, in
which the head, face, and upper body are represented with traditional 3D
meshes, and the hair, clothing, and accessories with neural radiance fields
(NeRF). The model-based mesh representation provides a strong geometric prior
for the face region, improving realism while enabling editing of the person's
appearance. By using NeRFs to represent the remaining components, our method is
able to model and synthesize parts with complex geometry and appearance, such
as curly hair and fluffy scarves. Our novel system synthesizes these
high-quality compositional avatars from text descriptions. The experimental
results demonstrate that our method, Text-guided generation and Editing of
Compositional Avatars (TECA), produces avatars that are more realistic than
those of recent methods while being editable because of their compositional
nature. For example, our TECA enables the seamless transfer of compositional
features like hairstyles, scarves, and other accessories between avatars. This
capability supports applications such as virtual try-on.";Hao Zhang<author:sep>Yao Feng<author:sep>Peter Kulits<author:sep>Yandong Wen<author:sep>Justus Thies<author:sep>Michael J. Black;http://arxiv.org/pdf/2309.07125v1;cs.CV;Home page: https://yfeng95.github.io/teca;nerf
2309.06030v1;http://arxiv.org/abs/2309.06030v1;2023-09-12;Federated Learning for Large-Scale Scene Modeling with Neural Radiance  Fields;"We envision a system to continuously build and maintain a map based on
earth-scale neural radiance fields (NeRF) using data collected from vehicles
and drones in a lifelong learning manner. However, existing large-scale
modeling by NeRF has problems in terms of scalability and maintainability when
modeling earth-scale environments. Therefore, to address these problems, we
propose a federated learning pipeline for large-scale modeling with NeRF. We
tailor the model aggregation pipeline in federated learning for NeRF, thereby
allowing local updates of NeRF. In the aggregation step, the accuracy of the
clients' global pose is critical. Thus, we also propose global pose alignment
to align the noisy global pose of clients before the aggregation step. In
experiments, we show the effectiveness of the proposed pose alignment and the
federated learning pipeline on the large-scale scene dataset, Mill19.";Teppei Suzuki;http://arxiv.org/pdf/2309.06030v1;cs.CV;;nerf
2309.06441v1;http://arxiv.org/abs/2309.06441v1;2023-09-12;Learning Disentangled Avatars with Hybrid 3D Representations;"Tremendous efforts have been made to learn animatable and photorealistic
human avatars. Towards this end, both explicit and implicit 3D representations
are heavily studied for a holistic modeling and capture of the whole human
(e.g., body, clothing, face and hair), but neither representation is an optimal
choice in terms of representation efficacy since different parts of the human
avatar have different modeling desiderata. For example, meshes are generally
not suitable for modeling clothing and hair. Motivated by this, we present
Disentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit
3D representations. DELTA takes a monocular RGB video as input, and produces a
human avatar with separate body and clothing/hair layers. Specifically, we
demonstrate two important applications for DELTA. For the first one, we
consider the disentanglement of the human body and clothing and in the second,
we disentangle the face and hair. To do so, DELTA represents the body or face
with an explicit mesh-based parametric 3D model and the clothing or hair with
an implicit neural radiance field. To make this possible, we design an
end-to-end differentiable renderer that integrates meshes into volumetric
rendering, enabling DELTA to learn directly from monocular videos without any
3D supervision. Finally, we show that how these two applications can be easily
combined to model full-body avatars, such that the hair, face, body and
clothing can be fully disentangled yet jointly rendered. Such a disentanglement
enables hair and clothing transfer to arbitrary body shapes. We empirically
validate the effectiveness of DELTA's disentanglement by demonstrating its
promising performance on disentangled reconstruction, virtual clothing try-on
and hairstyle transfer. To facilitate future research, we also release an
open-sourced pipeline for the study of hybrid human avatar modeling.";Yao Feng<author:sep>Weiyang Liu<author:sep>Timo Bolkart<author:sep>Jinlong Yang<author:sep>Marc Pollefeys<author:sep>Michael J. Black;http://arxiv.org/pdf/2309.06441v1;cs.CV;"home page: https://yfeng95.github.io/delta. arXiv admin note: text
  overlap with arXiv:2210.01868";
2309.05339v1;http://arxiv.org/abs/2309.05339v1;2023-09-11;PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D  representations for agricultural robotics;"Precise scene understanding is key for most robot monitoring and intervention
tasks in agriculture. In this work we present PAg-NeRF which is a novel
NeRF-based system that enables 3D panoptic scene understanding. Our
representation is trained using an image sequence with noisy robot odometry
poses and automatic panoptic predictions with inconsistent IDs between frames.
Despite this noisy input, our system is able to output scene geometry,
photo-realistic renders and 3D consistent panoptic representations with
consistent instance IDs. We evaluate this novel system in a very challenging
horticultural scenario and in doing so demonstrate an end-to-end trainable
system that can make use of noisy robot poses rather than precise poses that
have to be pre-calculated. Compared to a baseline approach the peak signal to
noise ratio is improved from 21.34dB to 23.37dB while the panoptic quality
improves from 56.65% to 70.08%. Furthermore, our approach is faster and can be
tuned to improve inference time by more than a factor of 2 while being memory
efficient with approximately 12 times fewer parameters.";Claus Smitt<author:sep>Michael Halstead<author:sep>Patrick Zimmer<author:sep>Thomas Läbe<author:sep>Esra Guclu<author:sep>Cyrill Stachniss<author:sep>Chris McCool;http://arxiv.org/pdf/2309.05339v1;cs.RO;;nerf
2309.05028v1;http://arxiv.org/abs/2309.05028v1;2023-09-10;SC-NeRF: Self-Correcting Neural Radiance Field with Sparse Views;"In recent studies, the generalization of neural radiance fields for novel
view synthesis task has been widely explored. However, existing methods are
limited to objects and indoor scenes. In this work, we extend the
generalization task to outdoor scenes, trained only on object-level datasets.
This approach presents two challenges. Firstly, the significant distributional
shift between training and testing scenes leads to black artifacts in rendering
results. Secondly, viewpoint changes in outdoor scenes cause ghosting or
missing regions in rendered images. To address these challenges, we propose a
geometric correction module and an appearance correction module based on
multi-head attention mechanisms. We normalize rendered depth and combine it
with light direction as query in the attention mechanism. Our network
effectively corrects varying scene structures and geometric features in outdoor
scenes, generalizing well from object-level to unseen outdoor scenes.
Additionally, we use appearance correction module to correct appearance
features, preventing rendering artifacts like blank borders and ghosting due to
viewpoint changes. By combining these modules, our approach successfully
tackles the challenges of outdoor scene generalization, producing high-quality
rendering results. When evaluated on four datasets (Blender, DTU, LLFF,
Spaces), our network outperforms previous methods. Notably, compared to
MVSNeRF, our network improves average PSNR from 19.369 to 25.989, SSIM from
0.838 to 0.889, and reduces LPIPS from 0.265 to 0.224 on Spaces outdoor scenes.";Liang Song<author:sep>Guangming Wang<author:sep>Jiuming Liu<author:sep>Zhenyang Fu<author:sep>Yanzi Miao<author:sep> Hesheng;http://arxiv.org/pdf/2309.05028v1;cs.CV;;nerf
2309.04917v3;http://arxiv.org/abs/2309.04917v3;2023-09-10;Editing 3D Scenes via Text Prompts without Retraining;"Numerous diffusion models have recently been applied to image synthesis and
editing. However, editing 3D scenes is still in its early stages. It poses
various challenges, such as the requirement to design specific methods for
different editing types, retraining new models for various 3D scenes, and the
absence of convenient human interaction during editing. To tackle these issues,
we introduce a text-driven editing method, termed DN2N, which allows for the
direct acquisition of a NeRF model with universal editing capabilities,
eliminating the requirement for retraining. Our method employs off-the-shelf
text-based editing models of 2D images to modify the 3D scene images, followed
by a filtering process to discard poorly edited images that disrupt 3D
consistency. We then consider the remaining inconsistency as a problem of
removing noise perturbation, which can be solved by generating training data
with similar perturbation characteristics for training. We further propose
cross-view regularization terms to help the generalized NeRF model mitigate
these perturbations. Our text-driven method allows users to edit a 3D scene
with their desired description, which is more friendly, intuitive, and
practical than prior works. Empirical results show that our method achieves
multiple editing types, including but not limited to appearance editing,
weather transition, material changing, and style transfer. Most importantly,
our method generalizes well with editing abilities shared among a set of model
parameters without requiring a customized editing model for some specific
scenes, thus inferring novel views with editing effects directly from user
input. The project website is available at https://sk-fun.fun/DN2N";Shuangkang Fang<author:sep>Yufeng Wang<author:sep>Yi Yang<author:sep>Yi-Hsuan Tsai<author:sep>Wenrui Ding<author:sep>Shuchang Zhou<author:sep>Ming-Hsuan Yang;http://arxiv.org/pdf/2309.04917v3;cs.CV;Project Website: https://sk-fun.fun/DN2N;nerf
2309.04750v1;http://arxiv.org/abs/2309.04750v1;2023-09-09;Mirror-Aware Neural Humans;"Human motion capture either requires multi-camera systems or is unreliable
using single-view input due to depth ambiguities. Meanwhile, mirrors are
readily available in urban environments and form an affordable alternative by
recording two views with only a single camera. However, the mirror setting
poses the additional challenge of handling occlusions of real and mirror image.
Going beyond existing mirror approaches for 3D human pose estimation, we
utilize mirrors for learning a complete body model, including shape and dense
appearance. Our main contributions are extending articulated neural radiance
fields to include a notion of a mirror, making it sample-efficient over
potential occlusion regions. Together, our contributions realize a
consumer-level 3D motion capture system that starts from off-the-shelf 2D poses
by automatically calibrating the camera, estimating mirror orientation, and
subsequently lifting 2D keypoint detections to 3D skeleton pose that is used to
condition the mirror-aware NeRF. We empirically demonstrate the benefit of
learning a body model and accounting for occlusion in challenging mirror
scenes.";Daniel Ajisafe<author:sep>James Tang<author:sep>Shih-Yang Su<author:sep>Bastian Wandt<author:sep>Helge Rhodin;http://arxiv.org/pdf/2309.04750v1;cs.CV;"Project website:
  https://danielajisafe.github.io/mirror-aware-neural-humans/";nerf
2309.04581v1;http://arxiv.org/abs/2309.04581v1;2023-09-08;Dynamic Mesh-Aware Radiance Fields;"Embedding polygonal mesh assets within photorealistic Neural Radience Fields
(NeRF) volumes, such that they can be rendered and their dynamics simulated in
a physically consistent manner with the NeRF, is under-explored from the system
perspective of integrating NeRF into the traditional graphics pipeline. This
paper designs a two-way coupling between mesh and NeRF during rendering and
simulation. We first review the light transport equations for both mesh and
NeRF, then distill them into an efficient algorithm for updating radiance and
throughput along a cast ray with an arbitrary number of bounces. To resolve the
discrepancy between the linear color space that the path tracer assumes and the
sRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range
(HDR) images. We also present a strategy to estimate light sources and cast
shadows on the NeRF. Finally, we consider how the hybrid surface-volumetric
formulation can be efficiently integrated with a high-performance physics
simulator that supports cloth, rigid and soft bodies. The full rendering and
simulation system can be run on a GPU at interactive rates. We show that a
hybrid system approach outperforms alternatives in visual realism for mesh
insertion, because it allows realistic light transport from volumetric NeRF
media onto surfaces, which affects the appearance of reflective/refractive
surfaces and illumination of diffuse surfaces informed by the dynamic scene.";Yi-Ling Qiao<author:sep>Alexander Gao<author:sep>Yiran Xu<author:sep>Yue Feng<author:sep>Jia-Bin Huang<author:sep>Ming C. Lin;http://arxiv.org/pdf/2309.04581v1;cs.GR;ICCV 2023;nerf
2309.04410v1;http://arxiv.org/abs/2309.04410v1;2023-09-08;DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields;"In this paper, we address the challenging problem of 3D toonification, which
involves transferring the style of an artistic domain onto a target 3D face
with stylized geometry and texture. Although fine-tuning a pre-trained 3D GAN
on the artistic domain can produce reasonable performance, this strategy has
limitations in the 3D domain. In particular, fine-tuning can deteriorate the
original GAN latent space, which affects subsequent semantic editing, and
requires independent optimization and storage for each new style, limiting
flexibility and efficient deployment. To overcome these challenges, we propose
DeformToon3D, an effective toonification framework tailored for hierarchical 3D
GAN. Our approach decomposes 3D toonification into subproblems of geometry and
texture stylization to better preserve the original latent space. Specifically,
we devise a novel StyleField that predicts conditional 3D deformation to align
a real-space NeRF to the style space for geometry stylization. Thanks to the
StyleField formulation, which already handles geometry stylization well,
texture stylization can be achieved conveniently via adaptive style mixing that
injects information of the artistic domain into the decoder of the pre-trained
3D GAN. Due to the unique design, our method enables flexible style degree
control and shape-texture-specific style swap. Furthermore, we achieve
efficient training without any real-world 2D-3D training pairs but proxy
samples synthesized from off-the-shelf 2D toonification models.";Junzhe Zhang<author:sep>Yushi Lan<author:sep>Shuai Yang<author:sep>Fangzhou Hong<author:sep>Quan Wang<author:sep>Chai Kiat Yeo<author:sep>Ziwei Liu<author:sep>Chen Change Loy;http://arxiv.org/pdf/2309.04410v1;cs.CV;"ICCV 2023. Code: https://github.com/junzhezhang/DeformToon3D Project
  page: https://www.mmlab-ntu.com/project/deformtoon3d/";nerf
2309.03933v1;http://arxiv.org/abs/2309.03933v1;2023-09-07;BluNF: Blueprint Neural Field;"Neural Radiance Fields (NeRFs) have revolutionized scene novel view
synthesis, offering visually realistic, precise, and robust implicit
reconstructions. While recent approaches enable NeRF editing, such as object
removal, 3D shape modification, or material property manipulation, the manual
annotation prior to such edits makes the process tedious. Additionally,
traditional 2D interaction tools lack an accurate sense of 3D space, preventing
precise manipulation and editing of scenes. In this paper, we introduce a novel
approach, called Blueprint Neural Field (BluNF), to address these editing
issues. BluNF provides a robust and user-friendly 2D blueprint, enabling
intuitive scene editing. By leveraging implicit neural representation, BluNF
constructs a blueprint of a scene using prior semantic and depth information.
The generated blueprint allows effortless editing and manipulation of NeRF
representations. We demonstrate BluNF's editability through an intuitive
click-and-change mechanism, enabling 3D manipulations, such as masking,
appearance modification, and object removal. Our approach significantly
contributes to visual content creation, paving the way for further research in
this area.";Robin Courant<author:sep>Xi Wang<author:sep>Marc Christie<author:sep>Vicky Kalogeiton;http://arxiv.org/pdf/2309.03933v1;cs.CV;"ICCV-W (AI3DCC) 2023. Project page with videos and code:
  https://www.lix.polytechnique.fr/vista/projects/2023_iccvw_courant/";nerf
2309.03550v1;http://arxiv.org/abs/2309.03550v1;2023-09-07;Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance  Fields using Geometry-Guided Text-to-Image Diffusion Model;"Recent advances in diffusion models such as ControlNet have enabled
geometrically controllable, high-fidelity text-to-image generation. However,
none of them addresses the question of adding such controllability to
text-to-3D generation. In response, we propose Text2Control3D, a controllable
text-to-3D avatar generation method whose facial expression is controllable
given a monocular video casually captured with hand-held camera. Our main
strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF)
optimized with a set of controlled viewpoint-aware images that we generate from
ControlNet, whose condition input is the depth map extracted from the input
video. When generating the viewpoint-aware images, we utilize cross-reference
attention to inject well-controlled, referential facial expression and
appearance via cross attention. We also conduct low-pass filtering of Gaussian
latent of the diffusion model in order to ameliorate the viewpoint-agnostic
texture problem we observed from our empirical analysis, where the
viewpoint-aware images contain identical textures on identical pixel positions
that are incomprehensible in 3D. Finally, to train NeRF with the images that
are viewpoint-aware yet are not strictly consistent in geometry, our approach
considers per-image geometric variation as a view of deformation from a shared
3D canonical space. Consequently, we construct the 3D avatar in a canonical
space of deformable NeRF by learning a set of per-image deformation via
deformation field table. We demonstrate the empirical results and discuss the
effectiveness of our method.";Sungwon Hwang<author:sep>Junha Hyung<author:sep>Jaegul Choo;http://arxiv.org/pdf/2309.03550v1;cs.CV;Project page: https://text2control3d.github.io/;nerf
2309.03955v2;http://arxiv.org/abs/2309.03955v2;2023-09-07;SimpleNeRF: Regularizing Sparse Input Neural Radiance Fields with  Simpler Solutions;"Neural Radiance Fields (NeRF) show impressive performance for the
photorealistic free-view rendering of scenes. However, NeRFs require dense
sampling of images in the given scene, and their performance degrades
significantly when only a sparse set of views are available. Researchers have
found that supervising the depth estimated by the NeRF helps train it
effectively with fewer views. The depth supervision is obtained either using
classical approaches or neural networks pre-trained on a large dataset. While
the former may provide only sparse supervision, the latter may suffer from
generalization issues. As opposed to the earlier approaches, we seek to learn
the depth supervision by designing augmented models and training them along
with the NeRF. We design augmented models that encourage simpler solutions by
exploring the role of positional encoding and view-dependent radiance in
training the few-shot NeRF. The depth estimated by these simpler models is used
to supervise the NeRF depth estimates. Since the augmented models can be
inaccurate in certain regions, we design a mechanism to choose only reliable
depth estimates for supervision. Finally, we add a consistency loss between the
coarse and fine multi-layer perceptrons of the NeRF to ensure better
utilization of hierarchical sampling. We achieve state-of-the-art
view-synthesis performance on two popular datasets by employing the above
regularizations. The source code for our model can be found on our project
page: https://nagabhushansn95.github.io/publications/2023/SimpleNeRF.html";Nagabhushan Somraj<author:sep>Adithyan Karanayil<author:sep>Rajiv Soundararajan;http://arxiv.org/pdf/2309.03955v2;cs.CV;SIGGRAPH Asia 2023;nerf
2309.03160v2;http://arxiv.org/abs/2309.03160v2;2023-09-06;ResFields: Residual Neural Fields for Spatiotemporal Signals;"Neural fields, a category of neural networks trained to represent
high-frequency signals, have gained significant attention in recent years due
to their impressive performance in modeling complex 3D data, especially large
neural signed distance (SDFs) or radiance fields (NeRFs) via a single
multi-layer perceptron (MLP). However, despite the power and simplicity of
representing signals with an MLP, these methods still face challenges when
modeling large and complex temporal signals due to the limited capacity of
MLPs. In this paper, we propose an effective approach to address this
limitation by incorporating temporal residual layers into neural fields, dubbed
ResFields, a novel class of networks specifically designed to effectively
represent complex temporal signals. We conduct a comprehensive analysis of the
properties of ResFields and propose a matrix factorization technique to reduce
the number of trainable parameters and enhance generalization capabilities.
Importantly, our formulation seamlessly integrates with existing techniques and
consistently improves results across various challenging tasks: 2D video
approximation, dynamic shape modeling via temporal SDFs, and dynamic NeRF
reconstruction. Lastly, we demonstrate the practical utility of ResFields by
showcasing its effectiveness in capturing dynamic 3D scenes from sparse sensory
inputs of a lightweight capture system.";Marko Mihajlovic<author:sep>Sergey Prokudin<author:sep>Marc Pollefeys<author:sep>Siyu Tang;http://arxiv.org/pdf/2309.03160v2;cs.CV;Project page and code at https://markomih.github.io/ResFields/;nerf
2309.03185v1;http://arxiv.org/abs/2309.03185v1;2023-09-06;Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields;"Neural Radiance Fields (NeRFs) have shown promise in applications like view
synthesis and depth estimation, but learning from multiview images faces
inherent uncertainties. Current methods to quantify them are either heuristic
or computationally demanding. We introduce BayesRays, a post-hoc framework to
evaluate uncertainty in any pre-trained NeRF without modifying the training
process. Our method establishes a volumetric uncertainty field using spatial
perturbations and a Bayesian Laplace approximation. We derive our algorithm
statistically and show its superior performance in key metrics and
applications. Additional results available at: https://bayesrays.github.io.";Lily Goli<author:sep>Cody Reading<author:sep>Silvia Sellán<author:sep>Alec Jacobson<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2309.03185v1;cs.CV;;nerf
2309.01811v2;http://arxiv.org/abs/2309.01811v2;2023-09-04;Instant Continual Learning of Neural Radiance Fields;"Neural radiance fields (NeRFs) have emerged as an effective method for
novel-view synthesis and 3D scene reconstruction. However, conventional
training methods require access to all training views during scene
optimization. This assumption may be prohibitive in continual learning
scenarios, where new data is acquired in a sequential manner and a continuous
update of the NeRF is desired, as in automotive or remote sensing applications.
When naively trained in such a continual setting, traditional scene
representation frameworks suffer from catastrophic forgetting, where previously
learned knowledge is corrupted after training on new data. Prior works in
alleviating forgetting with NeRFs suffer from low reconstruction quality and
high latency, making them impractical for real-world application. We propose a
continual learning framework for training NeRFs that leverages replay-based
methods combined with a hybrid explicit--implicit scene representation. Our
method outperforms previous methods in reconstruction quality when trained in a
continual setting, while having the additional benefit of being an order of
magnitude faster.";Ryan Po<author:sep>Zhengyang Dong<author:sep>Alexander W. Bergman<author:sep>Gordon Wetzstein;http://arxiv.org/pdf/2309.01811v2;cs.CV;For project page please visit https://ryanpo.com/icngp/;nerf
2309.01351v1;http://arxiv.org/abs/2309.01351v1;2023-09-04;Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF;"Deep neural networks (DNNs) have been proven extremely susceptible to
adversarial examples, which raises special safety-critical concerns for
DNN-based autonomous driving stacks (i.e., 3D object detection). Although there
are extensive works on image-level attacks, most are restricted to 2D pixel
spaces, and such attacks are not always physically realistic in our 3D world.
Here we present Adv3D, the first exploration of modeling adversarial examples
as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic
appearances and 3D accurate generation, yielding a more realistic and
realizable adversarial example. We train our adversarial NeRF by minimizing the
surrounding objects' confidence predicted by 3D detectors on the training set.
Then we evaluate Adv3D on the unseen validation set and show that it can cause
a large performance reduction when rendering NeRF in any sampled pose. To
generate physically realizable adversarial examples, we propose primitive-aware
sampling and semantic-guided regularization that enable 3D patch attacks with
camouflage adversarial texture. Experimental results demonstrate that the
trained adversarial NeRF generalizes well to different poses, scenes, and 3D
detectors. Finally, we provide a defense method to our attacks that involves
adversarial training through data augmentation. Project page:
https://len-li.github.io/adv3d-web";Leheng Li<author:sep>Qing Lian<author:sep>Ying-Cong Chen;http://arxiv.org/pdf/2309.01351v1;cs.CV;;nerf
2309.00277v1;http://arxiv.org/abs/2309.00277v1;2023-09-01;SparseSat-NeRF: Dense Depth Supervised Neural Radiance Fields for Sparse  Satellite Images;"Digital surface model generation using traditional multi-view stereo matching
(MVS) performs poorly over non-Lambertian surfaces, with asynchronous
acquisitions, or at discontinuities. Neural radiance fields (NeRF) offer a new
paradigm for reconstructing surface geometries using continuous volumetric
representation. NeRF is self-supervised, does not require ground truth geometry
for training, and provides an elegant way to include in its representation
physical parameters about the scene, thus potentially remedying the challenging
scenarios where MVS fails. However, NeRF and its variants require many views to
produce convincing scene's geometries which in earth observation satellite
imaging is rare. In this paper we present SparseSat-NeRF (SpS-NeRF) - an
extension of Sat-NeRF adapted to sparse satellite views. SpS-NeRF employs dense
depth supervision guided by crosscorrelation similarity metric provided by
traditional semi-global MVS matching. We demonstrate the effectiveness of our
approach on stereo and tri-stereo Pleiades 1B/WorldView-3 images, and compare
against NeRF and Sat-NeRF. The code is available at
https://github.com/LulinZhang/SpS-NeRF";Lulin Zhang<author:sep>Ewelina Rupnik;http://arxiv.org/pdf/2309.00277v1;cs.CV;ISPRS Annals 2023;nerf
2308.16576v3;http://arxiv.org/abs/2308.16576v3;2023-08-31;GHuNeRF: Generalizable Human NeRF from a Monocular Video;"In this paper, we tackle the challenging task of learning a generalizable
human NeRF model from a monocular video. Although existing generalizable human
NeRFs have achieved impressive results, they require muti-view images or videos
which might not be always available. On the other hand, some works on
free-viewpoint rendering of human from monocular videos cannot be generalized
to unseen identities. In view of these limitations, we propose GHuNeRF to learn
a generalizable human NeRF model from a monocular video of the human performer.
We first introduce a visibility-aware aggregation scheme to compute vertex-wise
features, which is used to construct a 3D feature volume. The feature volume
can only represent the overall geometry of the human performer with
insufficient accuracy due to the limited resolution. To solve this, we further
enhance the volume feature with temporally aligned point-wise features using an
attention mechanism. Finally, the enhanced feature is used for predicting
density and color for each sampled point. A surface-guided sampling strategy is
also adopted to improve the efficiency for both training and inference. We
validate our approach on the widely-used ZJU-MoCap dataset, where we achieve
comparable performance with existing multi-view video based approaches. We also
test on the monocular People-Snapshot dataset and achieve better performance
than existing works when only monocular video is used. Our code is available at
the project website.";Chen Li<author:sep>Jiahao Lin<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2308.16576v3;cs.CV;Add in more baseline for comparison;nerf
2308.16041v1;http://arxiv.org/abs/2308.16041v1;2023-08-30;From Pixels to Portraits: A Comprehensive Survey of Talking Head  Generation Techniques and Applications;"Recent advancements in deep learning and computer vision have led to a surge
of interest in generating realistic talking heads. This paper presents a
comprehensive survey of state-of-the-art methods for talking head generation.
We systematically categorises them into four main approaches: image-driven,
audio-driven, video-driven and others (including neural radiance fields (NeRF),
and 3D-based methods). We provide an in-depth analysis of each method,
highlighting their unique contributions, strengths, and limitations.
Furthermore, we thoroughly compare publicly available models, evaluating them
on key aspects such as inference time and human-rated quality of the generated
outputs. Our aim is to provide a clear and concise overview of the current
landscape in talking head generation, elucidating the relationships between
different approaches and identifying promising directions for future research.
This survey will serve as a valuable reference for researchers and
practitioners interested in this rapidly evolving field.";Shreyank N Gowda<author:sep>Dheeraj Pandey<author:sep>Shashank Narayana Gowda;http://arxiv.org/pdf/2308.16041v1;cs.CV;;nerf
2308.15733v1;http://arxiv.org/abs/2308.15733v1;2023-08-30;Drone-NeRF: Efficient NeRF Based 3D Scene Reconstruction for Large-Scale  Drone Survey;"Neural rendering has garnered substantial attention owing to its capacity for
creating realistic 3D scenes. However, its applicability to extensive scenes
remains challenging, with limitations in effectiveness. In this work, we
propose the Drone-NeRF framework to enhance the efficient reconstruction of
unbounded large-scale scenes suited for drone oblique photography using Neural
Radiance Fields (NeRF). Our approach involves dividing the scene into uniform
sub-blocks based on camera position and depth visibility. Sub-scenes are
trained in parallel using NeRF, then merged for a complete scene. We refine the
model by optimizing camera poses and guiding NeRF with a uniform sampler.
Integrating chosen samples enhances accuracy. A hash-coded fusion MLP
accelerates density representation, yielding RGB and Depth outputs. Our
framework accounts for sub-scene constraints, reduces parallel-training noise,
handles shadow occlusion, and merges sub-regions for a polished rendering
result. This Drone-NeRF framework demonstrates promising capabilities in
addressing challenges related to scene complexity, rendering efficiency, and
accuracy in drone-obtained imagery.";Zhihao Jia<author:sep>Bing Wang<author:sep>Changhao Chen;http://arxiv.org/pdf/2308.15733v1;cs.CV;15 pages, 7 figures, in submission;nerf
2308.15547v1;http://arxiv.org/abs/2308.15547v1;2023-08-29;Efficient Ray Sampling for Radiance Fields Reconstruction;"Accelerating neural radiance fields training is of substantial practical
value, as the ray sampling strategy profoundly impacts network convergence.
More efficient ray sampling can thus directly enhance existing NeRF models'
training efficiency. We therefore propose a novel ray sampling approach for
neural radiance fields that improves training efficiency while retaining
photorealistic rendering results. First, we analyze the relationship between
the pixel loss distribution of sampled rays and rendering quality. This reveals
redundancy in the original NeRF's uniform ray sampling. Guided by this finding,
we develop a sampling method leveraging pixel regions and depth boundaries. Our
main idea is to sample fewer rays in training views, yet with each ray more
informative for scene fitting. Sampling probability increases in pixel areas
exhibiting significant color and depth variation, greatly reducing wasteful
rays from other regions without sacrificing precision. Through this method, not
only can the convergence of the network be accelerated, but the spatial
geometry of a scene can also be perceived more accurately. Rendering outputs
are enhanced, especially for texture-complex regions. Experiments demonstrate
that our method significantly outperforms state-of-the-art techniques on public
benchmark datasets.";Shilei Sun<author:sep>Ming Liu<author:sep>Zhongyi Fan<author:sep>Yuxue Liu<author:sep>Chengwei Lv<author:sep>Liquan Dong<author:sep>Lingqin Kong;http://arxiv.org/pdf/2308.15547v1;cs.CV;15 pages;nerf
2308.15049v1;http://arxiv.org/abs/2308.15049v1;2023-08-29;Pose-Free Neural Radiance Fields via Implicit Pose Regularization;"Pose-free neural radiance fields (NeRF) aim to train NeRF with unposed
multi-view images and it has achieved very impressive success in recent years.
Most existing works share the pipeline of training a coarse pose estimator with
rendered images at first, followed by a joint optimization of estimated poses
and neural radiance field. However, as the pose estimator is trained with only
rendered images, the pose estimation is usually biased or inaccurate for real
images due to the domain gap between real images and rendered images, leading
to poor robustness for the pose estimation of real images and further local
minima in joint optimization. We design IR-NeRF, an innovative pose-free NeRF
that introduces implicit pose regularization to refine pose estimator with
unposed real images and improve the robustness of the pose estimation for real
images. With a collection of 2D images of a specific scene, IR-NeRF constructs
a scene codebook that stores scene features and captures the scene-specific
pose distribution implicitly as priors. Thus, the robustness of pose estimation
can be promoted with the scene priors according to the rationale that a 2D real
image can be well reconstructed from the scene codebook only when its estimated
pose lies within the pose distribution. Extensive experiments show that IR-NeRF
achieves superior novel view synthesis and outperforms the state-of-the-art
consistently across multiple synthetic and real datasets.";Jiahui Zhang<author:sep>Fangneng Zhan<author:sep>Yingchen Yu<author:sep>Kunhao Liu<author:sep>Rongliang Wu<author:sep>Xiaoqin Zhang<author:sep>Ling Shao<author:sep>Shijian Lu;http://arxiv.org/pdf/2308.15049v1;cs.CV;Accepted by ICCV2023;nerf
2308.14816v1;http://arxiv.org/abs/2308.14816v1;2023-08-28;CLNeRF: Continual Learning Meets NeRF;"Novel view synthesis aims to render unseen views given a set of calibrated
images. In practical applications, the coverage, appearance or geometry of the
scene may change over time, with new images continuously being captured.
Efficiently incorporating such continuous change is an open challenge. Standard
NeRF benchmarks only involve scene coverage expansion. To study other practical
scene changes, we propose a new dataset, World Across Time (WAT), consisting of
scenes that change in appearance and geometry over time. We also propose a
simple yet effective method, CLNeRF, which introduces continual learning (CL)
to Neural Radiance Fields (NeRFs). CLNeRF combines generative replay and the
Instant Neural Graphics Primitives (NGP) architecture to effectively prevent
catastrophic forgetting and efficiently update the model when new data arrives.
We also add trainable appearance and geometry embeddings to NGP, allowing a
single compact model to handle complex scene changes. Without the need to store
historical images, CLNeRF trained sequentially over multiple scans of a
changing scene performs on-par with the upper bound model trained on all scans
at once. Compared to other CL baselines CLNeRF performs much better across
standard benchmarks and WAT. The source code, and the WAT dataset are available
at https://github.com/IntelLabs/CLNeRF. Video presentation is available at:
https://youtu.be/nLRt6OoDGq0?si=8yD6k-8MMBJInQPs";Zhipeng Cai<author:sep>Matthias Mueller;http://arxiv.org/pdf/2308.14816v1;cs.CV;Accepted to ICCV 2023;nerf
2308.14737v1;http://arxiv.org/abs/2308.14737v1;2023-08-28;Flexible Techniques for Differentiable Rendering with 3D Gaussians;"Fast, reliable shape reconstruction is an essential ingredient in many
computer vision applications. Neural Radiance Fields demonstrated that
photorealistic novel view synthesis is within reach, but was gated by
performance requirements for fast reconstruction of real scenes and objects.
Several recent approaches have built on alternative shape representations, in
particular, 3D Gaussians. We develop extensions to these renderers, such as
integrating differentiable optical flow, exporting watertight meshes and
rendering per-ray normals. Additionally, we show how two of the recent methods
are interoperable with each other. These reconstructions are quick, robust, and
easily performed on GPU or CPU. For code and visual examples, see
https://leonidk.github.io/fmb-plus";Leonid Keselman<author:sep>Martial Hebert;http://arxiv.org/pdf/2308.14737v1;cs.CV;;
2308.14383v1;http://arxiv.org/abs/2308.14383v1;2023-08-28;Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a  Light-Weight ToF Sensor;"Light-weight time-of-flight (ToF) depth sensors are compact and
cost-efficient, and thus widely used on mobile devices for tasks such as
autofocus and obstacle detection. However, due to the sparse and noisy depth
measurements, these sensors have rarely been considered for dense geometry
reconstruction. In this work, we present the first dense SLAM system with a
monocular camera and a light-weight ToF sensor. Specifically, we propose a
multi-modal implicit scene representation that supports rendering both the
signals from the RGB camera and light-weight ToF sensor which drives the
optimization by comparing with the raw sensor inputs. Moreover, in order to
guarantee successful pose tracking and reconstruction, we exploit a predicted
depth as an intermediate supervision and develop a coarse-to-fine optimization
strategy for efficient learning of the implicit representation. At last, the
temporal information is explicitly exploited to deal with the noisy signals
from light-weight ToF sensors to improve the accuracy and robustness of the
system. Experiments demonstrate that our system well exploits the signals of
light-weight ToF sensors and achieves competitive results both on camera
tracking and dense scene reconstruction. Project page:
\url{https://zju3dv.github.io/tof_slam/}.";Xinyang Liu<author:sep>Yijin Li<author:sep>Yanbin Teng<author:sep>Hujun Bao<author:sep>Guofeng Zhang<author:sep>Yinda Zhang<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2308.14383v1;cs.CV;"Accepted to ICCV 2023 (Oral). Project Page:
  https://zju3dv.github.io/tof_slam/";
2308.14152v1;http://arxiv.org/abs/2308.14152v1;2023-08-27;Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code  Diffusion using Transformers;"Generating 3D images of complex objects conditionally from a few 2D views is
a difficult synthesis problem, compounded by issues such as domain gap and
geometric misalignment. For instance, a unified framework such as Generative
Adversarial Networks cannot achieve this unless they explicitly define both a
domain-invariant and geometric-invariant joint latent distribution, whereas
Neural Radiance Fields are generally unable to handle both issues as they
optimize at the pixel level. By contrast, we propose a simple and novel 2D to
3D synthesis approach based on conditional diffusion with vector-quantized
codes. Operating in an information-rich code space enables high-resolution 3D
synthesis via full-coverage attention across the views. Specifically, we
generate the 3D codes (e.g. for CT images) conditional on previously generated
3D codes and the entire codebook of two 2D views (e.g. 2D X-rays). Qualitative
and quantitative results demonstrate state-of-the-art performance over
specialized methods across varied evaluation criteria, including fidelity
metrics such as density, coverage, and distortion metrics for two complex
volumetric imagery datasets from in real-world scenarios.";Abril Corona-Figueroa<author:sep>Sam Bond-Taylor<author:sep>Neelanjan Bhowmik<author:sep>Yona Falinie A. Gaus<author:sep>Toby P. Breckon<author:sep>Hubert P. H. Shum<author:sep>Chris G. Willcocks;http://arxiv.org/pdf/2308.14152v1;cs.CV;Camera-ready version for ICCV 2023;
2308.14078v2;http://arxiv.org/abs/2308.14078v2;2023-08-27;Sparse3D: Distilling Multiview-Consistent Diffusion for Object  Reconstruction from Sparse Views;"Reconstructing 3D objects from extremely sparse views is a long-standing and
challenging problem. While recent techniques employ image diffusion models for
generating plausible images at novel viewpoints or for distilling pre-trained
diffusion priors into 3D representations using score distillation sampling
(SDS), these methods often struggle to simultaneously achieve high-quality,
consistent, and detailed results for both novel-view synthesis (NVS) and
geometry. In this work, we present Sparse3D, a novel 3D reconstruction method
tailored for sparse view inputs. Our approach distills robust priors from a
multiview-consistent diffusion model to refine a neural radiance field.
Specifically, we employ a controller that harnesses epipolar features from
input views, guiding a pre-trained diffusion model, such as Stable Diffusion,
to produce novel-view images that maintain 3D consistency with the input. By
tapping into 2D priors from powerful image diffusion models, our integrated
model consistently delivers high-quality results, even when faced with
open-world objects. To address the blurriness introduced by conventional SDS,
we introduce the category-score distillation sampling (C-SDS) to enhance
detail. We conduct experiments on CO3DV2 which is a multi-view dataset of
real-world objects. Both quantitative and qualitative evaluations demonstrate
that our approach outperforms previous state-of-the-art works on the metrics
regarding NVS and geometry reconstruction.";Zi-Xin Zou<author:sep>Weihao Cheng<author:sep>Yan-Pei Cao<author:sep>Shi-Sheng Huang<author:sep>Ying Shan<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2308.14078v2;cs.CV;;
2308.13897v1;http://arxiv.org/abs/2308.13897v1;2023-08-26;InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules;"Generalizing Neural Radiance Fields (NeRF) to new scenes is a significant
challenge that existing approaches struggle to address without extensive
modifications to vanilla NeRF framework. We introduce InsertNeRF, a method for
INStilling gEneRalizabiliTy into NeRF. By utilizing multiple plug-and-play
HyperNet modules, InsertNeRF dynamically tailors NeRF's weights to specific
reference scenes, transforming multi-scale sampling-aware features into
scene-specific representations. This novel design allows for more accurate and
efficient representations of complex appearances and geometries. Experiments
show that this method not only achieves superior generalization performance but
also provides a flexible pathway for integration with other NeRF-like systems,
even in sparse input settings. Code will be available
https://github.com/bbbbby-99/InsertNeRF.";Yanqi Bao<author:sep>Tianyu Ding<author:sep>Jing Huo<author:sep>Wenbin Li<author:sep>Yuxin Li<author:sep>Yang Gao;http://arxiv.org/pdf/2308.13897v1;cs.CV;;nerf
2308.13404v1;http://arxiv.org/abs/2308.13404v1;2023-08-25;Relighting Neural Radiance Fields with Shadow and Highlight Hints;"This paper presents a novel neural implicit radiance representation for free
viewpoint relighting from a small set of unstructured photographs of an object
lit by a moving point light source different from the view position. We express
the shape as a signed distance function modeled by a multi layer perceptron. In
contrast to prior relightable implicit neural representations, we do not
disentangle the different reflectance components, but model both the local and
global reflectance at each point by a second multi layer perceptron that, in
addition, to density features, the current position, the normal (from the
signed distace function), view direction, and light position, also takes shadow
and highlight hints to aid the network in modeling the corresponding high
frequency light transport effects. These hints are provided as a suggestion,
and we leave it up to the network to decide how to incorporate these in the
final relit result. We demonstrate and validate our neural implicit
representation on synthetic and real scenes exhibiting a wide variety of
shapes, material properties, and global illumination light transport.";Chong Zeng<author:sep>Guojun Chen<author:sep>Yue Dong<author:sep>Pieter Peers<author:sep>Hongzhi Wu<author:sep>Xin Tong;http://arxiv.org/pdf/2308.13404v1;cs.CV;"Accepted to SIGGRAPH 2023. Author's version. Project page:
  https://nrhints.github.io/";
2309.00014v2;http://arxiv.org/abs/2309.00014v2;2023-08-24;Improving NeRF Quality by Progressive Camera Placement for Unrestricted  Navigation in Complex Environments;"Neural Radiance Fields, or NeRFs, have drastically improved novel view
synthesis and 3D reconstruction for rendering. NeRFs achieve impressive results
on object-centric reconstructions, but the quality of novel view synthesis with
free-viewpoint navigation in complex environments (rooms, houses, etc) is often
problematic. While algorithmic improvements play an important role in the
resulting quality of novel view synthesis, in this work, we show that because
optimizing a NeRF is inherently a data-driven process, good quality data play a
fundamental role in the final quality of the reconstruction. As a consequence,
it is critical to choose the data samples -- in this case the cameras -- in a
way that will eventually allow the optimization to converge to a solution that
allows free-viewpoint navigation with good quality. Our main contribution is an
algorithm that efficiently proposes new camera placements that improve visual
quality with minimal assumptions. Our solution can be used with any NeRF model
and outperforms baselines and similar work.";Georgios Kopanas<author:sep>George Drettakis;http://arxiv.org/pdf/2309.00014v2;cs.CV;;nerf
2308.12560v1;http://arxiv.org/abs/2308.12560v1;2023-08-24;NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects;"We propose a novel-view augmentation (NOVA) strategy to train NeRFs for
photo-realistic 3D composition of dynamic objects in a static scene. Compared
to prior work, our framework significantly reduces blending artifacts when
inserting multiple dynamic objects into a 3D scene at novel views and times;
achieves comparable PSNR without the need for additional ground truth
modalities like optical flow; and overall provides ease, flexibility, and
scalability in neural composition. Our codebase is on GitHub.";Dakshit Agrawal<author:sep>Jiajie Xu<author:sep>Siva Karthik Mustikovela<author:sep>Ioannis Gkioulekas<author:sep>Ashish Shrivastava<author:sep>Yuning Chai;http://arxiv.org/pdf/2308.12560v1;cs.CV;"Accepted for publication in ICCV Computer Vision for Metaverse
  Workshop 2023 (code is available at https://github.com/dakshitagrawal/NoVA)";nerf
2308.11974v2;http://arxiv.org/abs/2308.11974v2;2023-08-23;Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields;"Text-driven localized editing of 3D objects is particularly difficult as
locally mixing the original 3D object with the intended new object and style
effects without distorting the object's form is not a straightforward process.
To address this issue, we propose a novel NeRF-based model, Blending-NeRF,
which consists of two NeRF networks: pretrained NeRF and editable NeRF.
Additionally, we introduce new blending operations that allow Blending-NeRF to
properly edit target regions which are localized by text. By using a pretrained
vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects
with varying colors and densities, modify textures, and remove parts of the
original object. Our extensive experiments demonstrate that Blending-NeRF
produces naturally and locally edited 3D objects from various text prompts. Our
project page is available at https://seokhunchoi.github.io/Blending-NeRF/";Hyeonseop Song<author:sep>Seokhun Choi<author:sep>Hoseok Do<author:sep>Chul Lee<author:sep>Taehyeong Kim;http://arxiv.org/pdf/2308.11974v2;cs.CV;"Accepted to ICCV 2023. The first two authors contributed equally to
  this work";nerf
2308.12452v2;http://arxiv.org/abs/2308.12452v2;2023-08-23;ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for  3D Scene Stylization;"The radiance fields style transfer is an emerging field that has recently
gained popularity as a means of 3D scene stylization, thanks to the outstanding
performance of neural radiance fields in 3D reconstruction and view synthesis.
We highlight a research gap in radiance fields style transfer, the lack of
sufficient perceptual controllability, motivated by the existing concept in the
2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style
transfer framework offering manageable control over perceptual factors, to
systematically explore the perceptual controllability in 3D scene stylization.
Four distinct types of controls - color preservation control, (style pattern)
scale control, spatial (selective stylization area) control, and depth
enhancement control - are proposed and integrated into this framework. Results
from real-world datasets, both quantitative and qualitative, show that the four
types of controls in our ARF-Plus framework successfully accomplish their
corresponding perceptual controls when stylizing 3D scenes. These techniques
work well for individual style inputs as well as for the simultaneous
application of multiple styles within a scene. This unlocks a realm of
limitless possibilities, allowing customized modifications of stylization
effects and flexible merging of the strengths of different styles, ultimately
enabling the creation of novel and eye-catching stylistic effects on 3D scenes.";Wenzhao Li<author:sep>Tianhao Wu<author:sep>Fangcheng Zhong<author:sep>Cengiz Oztireli;http://arxiv.org/pdf/2308.12452v2;cs.CV;;
2308.11951v3;http://arxiv.org/abs/2308.11951v3;2023-08-23;Pose Modulated Avatars from Video;"It is now possible to reconstruct dynamic human motion and shape from a
sparse set of cameras using Neural Radiance Fields (NeRF) driven by an
underlying skeleton. However, a challenge remains to model the deformation of
cloth and skin in relation to skeleton pose. Unlike existing avatar models that
are learned implicitly or rely on a proxy surface, our approach is motivated by
the observation that different poses necessitate unique frequency assignments.
Neglecting this distinction yields noisy artifacts in smooth areas or blurs
fine-grained texture and shape details in sharp regions. We develop a
two-branch neural network that is adaptive and explicit in the frequency
domain. The first branch is a graph neural network that models correlations
among body parts locally, taking skeleton pose as input. The second branch
combines these correlation features to a set of global frequencies and then
modulates the feature encoding. Our experiments demonstrate that our network
outperforms state-of-the-art methods in terms of preserving details and
generalization capabilities.";Chunjin Song<author:sep>Bastian Wandt<author:sep>Helge Rhodin;http://arxiv.org/pdf/2308.11951v3;cs.CV;;nerf
2308.11774v1;http://arxiv.org/abs/2308.11774v1;2023-08-22;SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene  Reconstruction by Neural Radiance Field (NeRF);"The accurate reconstruction of surgical scenes from surgical videos is
critical for various applications, including intraoperative navigation and
image-guided robotic surgery automation. However, previous approaches, mainly
relying on depth estimation, have limited effectiveness in reconstructing
surgical scenes with moving surgical tools. To address this limitation and
provide accurate 3D position prediction for surgical tools in all frames, we
propose a novel approach called SAMSNeRF that combines Segment Anything Model
(SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates
accurate segmentation masks of surgical tools using SAM, which guides the
refinement of the dynamic surgical scene reconstruction by NeRF. Our
experimental results on public endoscopy surgical videos demonstrate that our
approach successfully reconstructs high-fidelity dynamic surgical scenes and
accurately reflects the spatial information of surgical tools. Our proposed
approach can significantly enhance surgical navigation and automation by
providing surgeons with accurate 3D position information of surgical tools
during surgery.The source code will be released soon.";Ange Lou<author:sep>Yamin Li<author:sep>Xing Yao<author:sep>Yike Zhang<author:sep>Jack Noble;http://arxiv.org/pdf/2308.11774v1;cs.CV;;nerf
2308.11130v1;http://arxiv.org/abs/2308.11130v1;2023-08-22;Efficient View Synthesis with Neural Radiance Distribution Field;"Recent work on Neural Radiance Fields (NeRF) has demonstrated significant
advances in high-quality view synthesis. A major limitation of NeRF is its low
rendering efficiency due to the need for multiple network forwardings to render
a single pixel. Existing methods to improve NeRF either reduce the number of
required samples or optimize the implementation to accelerate the network
forwarding. Despite these efforts, the problem of multiple sampling persists
due to the intrinsic representation of radiance fields. In contrast, Neural
Light Fields (NeLF) reduce the computation cost of NeRF by querying only one
single network forwarding per pixel. To achieve a close visual quality to NeRF,
existing NeLF methods require significantly larger network capacities which
limits their rendering efficiency in practice. In this work, we propose a new
representation called Neural Radiance Distribution Field (NeRDF) that targets
efficient view synthesis in real-time. Specifically, we use a small network
similar to NeRF while preserving the rendering speed with a single network
forwarding per pixel as in NeLF. The key is to model the radiance distribution
along each ray with frequency basis and predict frequency weights using the
network. Pixel values are then computed via volume rendering on radiance
distributions. Experiments show that our proposed method offers a better
trade-off among speed, quality, and network size than existing methods: we
achieve a ~254x speed-up over NeRF with similar network size, with only a
marginal performance decline. Our project page is at
yushuang-wu.github.io/NeRDF.";Yushuang Wu<author:sep>Xiao Li<author:sep>Jinglu Wang<author:sep>Xiaoguang Han<author:sep>Shuguang Cui<author:sep>Yan Lu;http://arxiv.org/pdf/2308.11130v1;cs.CV;Accepted by ICCV2023;nerf
2308.11793v1;http://arxiv.org/abs/2308.11793v1;2023-08-22;Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer  with Mixture-of-View-Experts;"Cross-scene generalizable NeRF models, which can directly synthesize novel
views of unseen scenes, have become a new spotlight of the NeRF field. Several
existing attempts rely on increasingly end-to-end ""neuralized"" architectures,
i.e., replacing scene representation and/or rendering modules with performant
neural networks such as transformers, and turning novel view synthesis into a
feed-forward inference pipeline. While those feedforward ""neuralized""
architectures still do not fit diverse scenes well out of the box, we propose
to bridge them with the powerful Mixture-of-Experts (MoE) idea from large
language models (LLMs), which has demonstrated superior generalization ability
by balancing between larger overall model capacity and flexible per-instance
specialization. Starting from a recent generalizable NeRF architecture called
GNT, we first demonstrate that MoE can be neatly plugged in to enhance the
model. We further customize a shared permanent expert and a geometry-aware
consistency loss to enforce cross-scene consistency and spatial smoothness
respectively, which are essential for generalizable view synthesis. Our
proposed model, dubbed GNT with Mixture-of-View-Experts (GNT-MOVE), has
experimentally shown state-of-the-art results when transferring to unseen
scenes, indicating remarkably better cross-scene generalization in both
zero-shot and few-shot settings. Our codes are available at
https://github.com/VITA-Group/GNT-MOVE.";Wenyan Cong<author:sep>Hanxue Liang<author:sep>Peihao Wang<author:sep>Zhiwen Fan<author:sep>Tianlong Chen<author:sep>Mukund Varma<author:sep>Yi Wang<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2308.11793v1;cs.CV;Accepted by ICCV2023;nerf
2308.11198v1;http://arxiv.org/abs/2308.11198v1;2023-08-22;Novel-view Synthesis and Pose Estimation for Hand-Object Interaction  from Sparse Views;"Hand-object interaction understanding and the barely addressed novel view
synthesis are highly desired in the immersive communication, whereas it is
challenging due to the high deformation of hand and heavy occlusions between
hand and object. In this paper, we propose a neural rendering and pose
estimation system for hand-object interaction from sparse views, which can also
enable 3D hand-object interaction editing. We share the inspiration from recent
scene understanding work that shows a scene specific model built beforehand can
significantly improve and unblock vision tasks especially when inputs are
sparse, and extend it to the dynamic hand-object interaction scenario and
propose to solve the problem in two stages. We first learn the shape and
appearance prior knowledge of hands and objects separately with the neural
representation at the offline stage. During the online stage, we design a
rendering-based joint model fitting framework to understand the dynamic
hand-object interaction with the pre-built hand and object models as well as
interaction priors, which thereby overcomes penetration and separation issues
between hand and object and also enables novel view synthesis. In order to get
stable contact during the hand-object interaction process in a sequence, we
propose a stable contact loss to make the contact region to be consistent.
Experiments demonstrate that our method outperforms the state-of-the-art
methods. Code and dataset are available in project webpage
https://iscas3dv.github.io/HO-NeRF.";Wentian Qu<author:sep>Zhaopeng Cui<author:sep>Yinda Zhang<author:sep>Chenyu Meng<author:sep>Cuixia Ma<author:sep>Xiaoming Deng<author:sep>Hongan Wang;http://arxiv.org/pdf/2308.11198v1;cs.CV;;nerf
2308.10902v2;http://arxiv.org/abs/2308.10902v2;2023-08-21;CamP: Camera Preconditioning for Neural Radiance Fields;"Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D
scene reconstructions of objects and large-scale scenes. However, NeRFs require
accurate camera parameters as input -- inaccurate camera parameters result in
blurry renderings. Extrinsic and intrinsic camera parameters are usually
estimated using Structure-from-Motion (SfM) methods as a pre-processing step to
NeRF, but these techniques rarely yield perfect estimates. Thus, prior works
have proposed jointly optimizing camera parameters alongside a NeRF, but these
methods are prone to local minima in challenging settings. In this work, we
analyze how different camera parameterizations affect this joint optimization
problem, and observe that standard parameterizations exhibit large differences
in magnitude with respect to small perturbations, which can lead to an
ill-conditioned optimization problem. We propose using a proxy problem to
compute a whitening transform that eliminates the correlation between camera
parameters and normalizes their effects, and we propose to use this transform
as a preconditioner for the camera parameters during joint optimization. Our
preconditioned camera optimization significantly improves reconstruction
quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE)
by 67% compared to state-of-the-art NeRF approaches that do not optimize for
cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint
optimization approaches using the camera parameterization of SCNeRF. Our
approach is easy to implement, does not significantly increase runtime, can be
applied to a wide variety of camera parameterizations, and can
straightforwardly be incorporated into other NeRF-like models.";Keunhong Park<author:sep>Philipp Henzler<author:sep>Ben Mildenhall<author:sep>Jonathan T. Barron<author:sep>Ricardo Martin-Brualla;http://arxiv.org/pdf/2308.10902v2;cs.CV;SIGGRAPH Asia 2023, Project page: https://camp-nerf.github.io;nerf
2308.10337v1;http://arxiv.org/abs/2308.10337v1;2023-08-20;Strata-NeRF : Neural Radiance Fields for Stratified Scenes;"Neural Radiance Field (NeRF) approaches learn the underlying 3D
representation of a scene and generate photo-realistic novel views with high
fidelity. However, most proposed settings concentrate on modelling a single
object or a single level of a scene. However, in the real world, we may capture
a scene at multiple levels, resulting in a layered capture. For example,
tourists usually capture a monument's exterior structure before capturing the
inner structure. Modelling such scenes in 3D with seamless switching between
levels can drastically improve immersive experiences. However, most existing
techniques struggle in modelling such scenes. We propose Strata-NeRF, a single
neural radiance field that implicitly captures a scene with multiple levels.
Strata-NeRF achieves this by conditioning the NeRFs on Vector Quantized (VQ)
latent representations which allow sudden changes in scene structure. We
evaluate the effectiveness of our approach in multi-layered synthetic dataset
comprising diverse scenes and then further validate its generalization on the
real-world RealEstate10K dataset. We find that Strata-NeRF effectively captures
stratified scenes, minimizes artifacts, and synthesizes high-fidelity views
compared to existing approaches.";Ankit Dhiman<author:sep>Srinath R<author:sep>Harsh Rangwani<author:sep>Rishubh Parihar<author:sep>Lokesh R Boregowda<author:sep>Srinath Sridhar<author:sep>R Venkatesh Babu;http://arxiv.org/pdf/2308.10337v1;cs.CV;ICCV 2023, Project Page: https://ankitatiisc.github.io/Strata-NeRF/;nerf
2308.10001v1;http://arxiv.org/abs/2308.10001v1;2023-08-19;AltNeRF: Learning Robust Neural Radiance Field via Alternating  Depth-Pose Optimization;"Neural Radiance Fields (NeRF) have shown promise in generating realistic
novel views from sparse scene images. However, existing NeRF approaches often
encounter challenges due to the lack of explicit 3D supervision and imprecise
camera poses, resulting in suboptimal outcomes. To tackle these issues, we
propose AltNeRF -- a novel framework designed to create resilient NeRF
representations using self-supervised monocular depth estimation (SMDE) from
monocular videos, without relying on known camera poses. SMDE in AltNeRF
masterfully learns depth and pose priors to regulate NeRF training. The depth
prior enriches NeRF's capacity for precise scene geometry depiction, while the
pose prior provides a robust starting point for subsequent pose refinement.
Moreover, we introduce an alternating algorithm that harmoniously melds NeRF
outputs into SMDE through a consistence-driven mechanism, thus enhancing the
integrity of depth priors. This alternation empowers AltNeRF to progressively
refine NeRF representations, yielding the synthesis of realistic novel views.
Additionally, we curate a distinctive dataset comprising indoor videos captured
via mobile devices. Extensive experiments showcase the compelling capabilities
of AltNeRF in generating high-fidelity and robust novel views that closely
resemble reality.";Kun Wang<author:sep>Zhiqiang Yan<author:sep>Huang Tian<author:sep>Zhenyu Zhang<author:sep>Xiang Li<author:sep>Jun Li<author:sep>Jian Yang;http://arxiv.org/pdf/2308.10001v1;cs.CV;;nerf
2308.10122v1;http://arxiv.org/abs/2308.10122v1;2023-08-19;HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision  Mitigation;"Neural radiance fields (NeRF) have garnered significant attention, with
recent works such as Instant-NGP accelerating NeRF training and evaluation
through a combination of hashgrid-based positional encoding and neural
networks. However, effectively leveraging the spatial sparsity of 3D scenes
remains a challenge. To cull away unnecessary regions of the feature grid,
existing solutions rely on prior knowledge of object shape or periodically
estimate object shape during training by repeated model evaluations, which are
costly and wasteful.
  To address this issue, we propose HollowNeRF, a novel compression solution
for hashgrid-based NeRF which automatically sparsifies the feature grid during
the training phase. Instead of directly compressing dense features, HollowNeRF
trains a coarse 3D saliency mask that guides efficient feature pruning, and
employs an alternating direction method of multipliers (ADMM) pruner to
sparsify the 3D saliency mask during training. By exploiting the sparsity in
the 3D scene to redistribute hash collisions, HollowNeRF improves rendering
quality while using a fraction of the parameters of comparable state-of-the-art
solutions, leading to a better cost-accuracy trade-off. Our method delivers
comparable rendering quality to Instant-NGP, while utilizing just 31% of the
parameters. In addition, our solution can achieve a PSNR accuracy gain of up to
1dB using only 56% of the parameters.";Xiufeng Xie<author:sep>Riccardo Gherardi<author:sep>Zhihong Pan<author:sep>Stephen Huang;http://arxiv.org/pdf/2308.10122v1;cs.CV;Accepted to ICCV 2023;nerf
2308.09894v1;http://arxiv.org/abs/2308.09894v1;2023-08-19;Semantic-Human: Neural Rendering of Humans from Monocular Video with  Human Parsing;"The neural rendering of humans is a topic of great research significance.
However, previous works mostly focus on achieving photorealistic details,
neglecting the exploration of human parsing. Additionally, classical semantic
work are all limited in their ability to efficiently represent fine results in
complex motions. Human parsing is inherently related to radiance
reconstruction, as similar appearance and geometry often correspond to similar
semantic part. Furthermore, previous works often design a motion field that
maps from the observation space to the canonical space, while it tends to
exhibit either underfitting or overfitting, resulting in limited
generalization. In this paper, we present Semantic-Human, a novel method that
achieves both photorealistic details and viewpoint-consistent human parsing for
the neural rendering of humans. Specifically, we extend neural radiance fields
(NeRF) to jointly encode semantics, appearance and geometry to achieve accurate
2D semantic labels using noisy pseudo-label supervision. Leveraging the
inherent consistency and smoothness properties of NeRF, Semantic-Human achieves
consistent human parsing in both continuous and novel views. We also introduce
constraints derived from the SMPL surface for the motion field and
regularization for the recovered volumetric geometry. We have evaluated the
model using the ZJU-MoCap dataset, and the obtained highly competitive results
demonstrate the effectiveness of our proposed Semantic-Human. We also showcase
various compelling applications, including label denoising, label synthesis and
image editing, and empirically validate its advantageous properties.";Jie Zhang<author:sep>Pengcheng Shi<author:sep>Zaiwang Gu<author:sep>Yiyang Zhou<author:sep>Zhi Wang;http://arxiv.org/pdf/2308.09894v1;cs.CV;;nerf
2308.09386v1;http://arxiv.org/abs/2308.09386v1;2023-08-18;DReg-NeRF: Deep Registration for Neural Radiance Fields;"Although Neural Radiance Fields (NeRF) is popular in the computer vision
community recently, registering multiple NeRFs has yet to gain much attention.
Unlike the existing work, NeRF2NeRF, which is based on traditional optimization
methods and needs human annotated keypoints, we propose DReg-NeRF to solve the
NeRF registration problem on object-centric scenes without human intervention.
After training NeRF models, our DReg-NeRF first extracts features from the
occupancy grid in NeRF. Subsequently, our DReg-NeRF utilizes a transformer
architecture with self-attention and cross-attention layers to learn the
relations between pairwise NeRF blocks. In contrast to state-of-the-art (SOTA)
point cloud registration methods, the decoupled correspondences are supervised
by surface fields without any ground truth overlapping labels. We construct a
novel view synthesis dataset with 1,700+ 3D objects obtained from Objaverse to
train our network. When evaluated on the test set, our proposed method beats
the SOTA point cloud registration methods by a large margin, with a mean
$\text{RPE}=9.67^{\circ}$ and a mean $\text{RTE}=0.038$.
  Our code is available at https://github.com/AIBluefisher/DReg-NeRF.";Yu Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2308.09386v1;cs.CV;Accepted at ICCV 2023;nerf
2308.09421v2;http://arxiv.org/abs/2308.09421v2;2023-08-18;MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection;"In the field of monocular 3D detection, it is common practice to utilize
scene geometric clues to enhance the detector's performance. However, many
existing works adopt these clues explicitly such as estimating a depth map and
back-projecting it into 3D space. This explicit methodology induces sparsity in
3D representations due to the increased dimensionality from 2D to 3D, and leads
to substantial information loss, especially for distant and occluded objects.
To alleviate this issue, we propose MonoNeRD, a novel detection framework that
can infer dense 3D geometry and occupancy. Specifically, we model scenes with
Signed Distance Functions (SDF), facilitating the production of dense 3D
representations. We treat these representations as Neural Radiance Fields
(NeRF) and then employ volume rendering to recover RGB images and depth maps.
To the best of our knowledge, this work is the first to introduce volume
rendering for M3D, and demonstrates the potential of implicit reconstruction
for image-based 3D perception. Extensive experiments conducted on the KITTI-3D
benchmark and Waymo Open Dataset demonstrate the effectiveness of MonoNeRD.
Codes are available at https://github.com/cskkxjk/MonoNeRD.";Junkai Xu<author:sep>Liang Peng<author:sep>Haoran Cheng<author:sep>Hao Li<author:sep>Wei Qian<author:sep>Ke Li<author:sep>Wenxiao Wang<author:sep>Deng Cai;http://arxiv.org/pdf/2308.09421v2;cs.CV;Accepted by ICCV 2023;nerf
2308.08947v1;http://arxiv.org/abs/2308.08947v1;2023-08-17;Watch Your Steps: Local Image and Scene Editing by Text Instructions;"Denoising diffusion models have enabled high-quality image generation and
editing. We present a method to localize the desired edit region implicit in a
text instruction. We leverage InstructPix2Pix (IP2P) and identify the
discrepancy between IP2P predictions with and without the instruction. This
discrepancy is referred to as the relevance map. The relevance map conveys the
importance of changing each pixel to achieve the edits, and is used to to guide
the modifications. This guidance ensures that the irrelevant pixels remain
unchanged. Relevance maps are further used to enhance the quality of
text-guided editing of 3D scenes in the form of neural radiance fields. A field
is trained on relevance maps of training views, denoted as the relevance field,
defining the 3D region within which modifications should be made. We perform
iterative updates on the training views guided by rendered relevance maps from
the relevance field. Our method achieves state-of-the-art performance on both
image and NeRF editing tasks. Project page:
https://ashmrz.github.io/WatchYourSteps/";Ashkan Mirzaei<author:sep>Tristan Aumentado-Armstrong<author:sep>Marcus A. Brubaker<author:sep>Jonathan Kelly<author:sep>Alex Levinshtein<author:sep>Konstantinos G. Derpanis<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2308.08947v1;cs.CV;Project page: https://ashmrz.github.io/WatchYourSteps/;nerf
2308.08854v1;http://arxiv.org/abs/2308.08854v1;2023-08-17;Language-enhanced RNR-Map: Querying Renderable Neural Radiance Field  maps with natural language;"We present Le-RNR-Map, a Language-enhanced Renderable Neural Radiance map for
Visual Navigation with natural language query prompts. The recently proposed
RNR-Map employs a grid structure comprising latent codes positioned at each
pixel. These latent codes, which are derived from image observation, enable: i)
image rendering given a camera pose, since they are converted to Neural
Radiance Field; ii) image navigation and localization with astonishing
accuracy. On top of this, we enhance RNR-Map with CLIP-based embedding latent
codes, allowing natural language search without additional label data. We
evaluate the effectiveness of this map in single and multi-object searches. We
also investigate its compatibility with a Large Language Model as an
""affordance query resolver"". Code and videos are available at
https://intelligolabs.github.io/Le-RNR-Map/";Francesco Taioli<author:sep>Federico Cunico<author:sep>Federico Girella<author:sep>Riccardo Bologna<author:sep>Alessandro Farinelli<author:sep>Marco Cristani;http://arxiv.org/pdf/2308.08854v1;cs.CV;Accepted at ICCVW23 VLAR;
2308.08530v3;http://arxiv.org/abs/2308.08530v3;2023-08-16;Ref-DVGO: Reflection-Aware Direct Voxel Grid Optimization for an  Improved Quality-Efficiency Trade-Off in Reflective Scene Reconstruction;"Neural Radiance Fields (NeRFs) have revolutionized the field of novel view
synthesis, demonstrating remarkable performance. However, the modeling and
rendering of reflective objects remain challenging problems. Recent methods
have shown significant improvements over the baselines in handling reflective
scenes, albeit at the expense of efficiency. In this work, we aim to strike a
balance between efficiency and quality. To this end, we investigate an
implicit-explicit approach based on conventional volume rendering to enhance
the reconstruction quality and accelerate the training and rendering processes.
We adopt an efficient density-based grid representation and reparameterize the
reflected radiance in our pipeline. Our proposed reflection-aware approach
achieves a competitive quality efficiency trade-off compared to competing
methods. Based on our experimental results, we propose and discuss hypotheses
regarding the factors influencing the results of density-based methods for
reconstructing reflective objects. The source code is available at
https://github.com/gkouros/ref-dvgo.";Georgios Kouros<author:sep>Minye Wu<author:sep>Shubham Shrivastava<author:sep>Sushruth Nagesh<author:sep>Punarjay Chakravarty<author:sep>Tinne Tuytelaars;http://arxiv.org/pdf/2308.08530v3;cs.CV;5 pages, 4 figures, 3 tables, ICCV TRICKY 2023 Workshop;nerf
2308.08258v1;http://arxiv.org/abs/2308.08258v1;2023-08-16;SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes;"Existing methods for the 4D reconstruction of general, non-rigidly deforming
objects focus on novel-view synthesis and neglect correspondences. However,
time consistency enables advanced downstream tasks like 3D editing, motion
analysis, or virtual-asset creation. We propose SceNeRFlow to reconstruct a
general, non-rigid scene in a time-consistent manner. Our dynamic-NeRF method
takes multi-view RGB videos and background images from static cameras with
known camera parameters as input. It then reconstructs the deformations of an
estimated canonical model of the geometry and appearance in an online fashion.
Since this canonical model is time-invariant, we obtain correspondences even
for long-term, long-range motions. We employ neural scene representations to
parametrize the components of our method. Like prior dynamic-NeRF methods, we
use a backwards deformation model. We find non-trivial adaptations of this
model necessary to handle larger motions: We decompose the deformations into a
strongly regularized coarse component and a weakly regularized fine component,
where the coarse component also extends the deformation field into the space
surrounding the object, which enables tracking over time. We show
experimentally that, unlike prior work that only handles small motion, our
method enables the reconstruction of studio-scale motions.";Edith Tretschk<author:sep>Vladislav Golyanik<author:sep>Michael Zollhoefer<author:sep>Aljaz Bozic<author:sep>Christoph Lassner<author:sep>Christian Theobalt;http://arxiv.org/pdf/2308.08258v1;cs.CV;Project page: https://vcai.mpi-inf.mpg.de/projects/scenerflow/;nerf
2308.07032v1;http://arxiv.org/abs/2308.07032v1;2023-08-14;S3IM: Stochastic Structural SIMilarity and Its Unreasonable  Effectiveness for Neural Fields;"Recently, Neural Radiance Field (NeRF) has shown great success in rendering
novel-view images of a given scene by learning an implicit representation with
only posed RGB images. NeRF and relevant neural field methods (e.g., neural
surface representation) typically optimize a point-wise loss and make
point-wise predictions, where one data point corresponds to one pixel.
Unfortunately, this line of research failed to use the collective supervision
of distant pixels, although it is known that pixels in an image or scene can
provide rich structural information. To the best of our knowledge, we are the
first to design a nonlocal multiplex training paradigm for NeRF and relevant
neural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss
that processes multiple data points as a whole set instead of process multiple
inputs independently. Our extensive experiments demonstrate the unreasonable
effectiveness of S3IM in improving NeRF and neural surface representation for
nearly free. The improvements of quality metrics can be particularly
significant for those relatively difficult tasks: e.g., the test MSE loss
unexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view
synthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance
reduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is
consistently robust even with sparse inputs, corrupted images, and dynamic
scenes.";Zeke Xie<author:sep>Xindi Yang<author:sep>Yujie Yang<author:sep>Qi Sun<author:sep>Yixiang Jiang<author:sep>Haoran Wang<author:sep>Yunfeng Cai<author:sep>Mingming Sun;http://arxiv.org/pdf/2308.07032v1;cs.CV;"ICCV 2023 main conference. Code: https://github.com/Madaoer/S3IM. 14
  pages, 5 figures, 17 tables";nerf
2308.07118v2;http://arxiv.org/abs/2308.07118v2;2023-08-14;Neural radiance fields in the industrial and robotics domain:  applications, research opportunities and use cases;"The proliferation of technologies, such as extended reality (XR), has
increased the demand for high-quality three-dimensional (3D) graphical
representations. Industrial 3D applications encompass computer-aided design
(CAD), finite element analysis (FEA), scanning, and robotics. However, current
methods employed for industrial 3D representations suffer from high
implementation costs and reliance on manual human input for accurate 3D
modeling. To address these challenges, neural radiance fields (NeRFs) have
emerged as a promising approach for learning 3D scene representations based on
provided training 2D images. Despite a growing interest in NeRFs, their
potential applications in various industrial subdomains are still unexplored.
In this paper, we deliver a comprehensive examination of NeRF industrial
applications while also providing direction for future research endeavors. We
also present a series of proof-of-concept experiments that demonstrate the
potential of NeRFs in the industrial domain. These experiments include
NeRF-based video compression techniques and using NeRFs for 3D motion
estimation in the context of collision avoidance. In the video compression
experiment, our results show compression savings up to 48\% and 74\% for
resolutions of 1920x1080 and 300x168, respectively. The motion estimation
experiment used a 3D animation of a robotic arm to train Dynamic-NeRF (D-NeRF)
and achieved an average peak signal-to-noise ratio (PSNR) of disparity map with
the value of 23 dB and an structural similarity index measure (SSIM) 0.97.";Eugen Šlapak<author:sep>Enric Pardo<author:sep>Matúš Dopiriak<author:sep>Taras Maksymyuk<author:sep>Juraj Gazda;http://arxiv.org/pdf/2308.07118v2;cs.RO;;nerf
2308.05970v1;http://arxiv.org/abs/2308.05970v1;2023-08-11;Focused Specific Objects NeRF;"Most NeRF-based models are designed for learning the entire scene, and
complex scenes can lead to longer learning times and poorer rendering effects.
This paper utilizes scene semantic priors to make improvements in fast
training, allowing the network to focus on the specific targets and not be
affected by complex backgrounds. The training speed can be increased by 7.78
times with better rendering effect, and small to medium sized targets can be
rendered faster. In addition, this improvement applies to all NeRF-based
models. Considering the inherent multi-view consistency and smoothness of NeRF,
this paper also studies weak supervision by sparsely sampling negative ray
samples. With this method, training can be further accelerated and rendering
quality can be maintained. Finally, this paper extends pixel semantic and color
rendering formulas and proposes a new scene editing technique that can achieve
unique displays of the specific semantic targets or masking them in rendering.
To address the problem of unsupervised regions incorrect inferences in the
scene, we also designed a self-supervised loop that combines morphological
operations and clustering.";Yuesong Li<author:sep>Feng Pan<author:sep>Helong Yan<author:sep>Xiuli Xin<author:sep>Xiaoxue Feng;http://arxiv.org/pdf/2308.05970v1;cs.CV;17 pages,32 figures;nerf
2308.05939v1;http://arxiv.org/abs/2308.05939v1;2023-08-11;VERF: Runtime Monitoring of Pose Estimation with Neural Radiance Fields;"We present VERF, a collection of two methods (VERF-PnP and VERF-Light) for
providing runtime assurance on the correctness of a camera pose estimate of a
monocular camera without relying on direct depth measurements. We leverage the
ability of NeRF (Neural Radiance Fields) to render novel RGB perspectives of a
scene. We only require as input the camera image whose pose is being estimated,
an estimate of the camera pose we want to monitor, and a NeRF model containing
the scene pictured by the camera. We can then predict if the pose estimate is
within a desired distance from the ground truth and justify our prediction with
a level of confidence. VERF-Light does this by rendering a viewpoint with NeRF
at the estimated pose and estimating its relative offset to the sensor image up
to scale. Since scene scale is unknown, the approach renders another auxiliary
image and reasons over the consistency of the optical flows across the three
images. VERF-PnP takes a different approach by rendering a stereo pair of
images with NeRF and utilizing the Perspective-n-Point (PnP) algorithm. We
evaluate both methods on the LLFF dataset, on data from a Unitree A1 quadruped
robot, and on data collected from Blue Origin's sub-orbital New Shepard rocket
to demonstrate the effectiveness of the proposed pose monitoring method across
a range of scene scales. We also show monitoring can be completed in under half
a second on a 3090 GPU.";Dominic Maggio<author:sep>Courtney Mario<author:sep>Luca Carlone;http://arxiv.org/pdf/2308.05939v1;cs.RO;;nerf
2308.04669v4;http://arxiv.org/abs/2308.04669v4;2023-08-09;A General Implicit Framework for Fast NeRF Composition and Rendering;"A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.";Xinyu Gao<author:sep>Ziyi Yang<author:sep>Yunlu Zhao<author:sep>Yuxiang Sun<author:sep>Xiaogang Jin<author:sep>Changqing Zou;http://arxiv.org/pdf/2308.04669v4;cs.CV;AAAI 2024;nerf
2308.04826v2;http://arxiv.org/abs/2308.04826v2;2023-08-09;WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields;"Neural Radiance Field (NeRF) has shown impressive performance in novel view
synthesis via implicit scene representation. However, it usually suffers from
poor scalability as requiring densely sampled images for each new scene.
Several studies have attempted to mitigate this problem by integrating
Multi-View Stereo (MVS) technique into NeRF while they still entail a
cumbersome fine-tuning process for new scenes. Notably, the rendering quality
will drop severely without this fine-tuning process and the errors mainly
appear around the high-frequency features. In the light of this observation, we
design WaveNeRF, which integrates wavelet frequency decomposition into MVS and
NeRF to achieve generalizable yet high-quality synthesis without any per-scene
optimization. To preserve high-frequency information when generating 3D feature
volumes, WaveNeRF builds Multi-View Stereo in the Wavelet domain by integrating
the discrete wavelet transform into the classical cascade MVS, which
disentangles high-frequency information explicitly. With that, disentangled
frequency features can be injected into classic NeRF via a novel hybrid neural
renderer to yield faithful high-frequency details, and an intuitive
frequency-guided sampling strategy can be designed to suppress artifacts around
high-frequency regions. Extensive experiments over three widely studied
benchmarks show that WaveNeRF achieves superior generalizable radiance field
modeling when only given three images as input.";Muyu Xu<author:sep>Fangneng Zhan<author:sep>Jiahui Zhang<author:sep>Yingchen Yu<author:sep>Xiaoqin Zhang<author:sep>Christian Theobalt<author:sep>Ling Shao<author:sep>Shijian Lu;http://arxiv.org/pdf/2308.04826v2;cs.CV;"Accepted to ICCV 2023. Project website:
  https://mxuai.github.io/WaveNeRF/";nerf
2308.04079v1;http://arxiv.org/abs/2308.04079v1;2023-08-08;3D Gaussian Splatting for Real-Time Radiance Field Rendering;"Radiance Field methods have recently revolutionized novel-view synthesis of
scenes captured with multiple photos or videos. However, achieving high visual
quality still requires neural networks that are costly to train and render,
while recent faster methods inevitably trade off speed for quality. For
unbounded and complete scenes (rather than isolated objects) and 1080p
resolution rendering, no current method can achieve real-time display rates. We
introduce three key elements that allow us to achieve state-of-the-art visual
quality while maintaining competitive training times and importantly allow
high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution.
First, starting from sparse points produced during camera calibration, we
represent the scene with 3D Gaussians that preserve desirable properties of
continuous volumetric radiance fields for scene optimization while avoiding
unnecessary computation in empty space; Second, we perform interleaved
optimization/density control of the 3D Gaussians, notably optimizing
anisotropic covariance to achieve an accurate representation of the scene;
Third, we develop a fast visibility-aware rendering algorithm that supports
anisotropic splatting and both accelerates training and allows realtime
rendering. We demonstrate state-of-the-art visual quality and real-time
rendering on several established datasets.";Bernhard Kerbl<author:sep>Georgios Kopanas<author:sep>Thomas Leimkühler<author:sep>George Drettakis;http://arxiv.org/pdf/2308.04079v1;cs.GR;https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/;gaussian splatting
2308.04413v1;http://arxiv.org/abs/2308.04413v1;2023-08-08;Digging into Depth Priors for Outdoor Neural Radiance Fields;"Neural Radiance Fields (NeRF) have demonstrated impressive performance in
vision and graphics tasks, such as novel view synthesis and immersive reality.
However, the shape-radiance ambiguity of radiance fields remains a challenge,
especially in the sparse viewpoints setting. Recent work resorts to integrating
depth priors into outdoor NeRF training to alleviate the issue. However, the
criteria for selecting depth priors and the relative merits of different priors
have not been thoroughly investigated. Moreover, the relative merits of
selecting different approaches to use the depth priors is also an unexplored
problem. In this paper, we provide a comprehensive study and evaluation of
employing depth priors to outdoor neural radiance fields, covering common depth
sensing technologies and most application ways. Specifically, we conduct
extensive experiments with two representative NeRF methods equipped with four
commonly-used depth priors and different depth usages on two widely used
outdoor datasets. Our experimental results reveal several interesting findings
that can potentially benefit practitioners and researchers in training their
NeRF models with depth priors. Project Page:
https://cwchenwang.github.io/outdoor-nerf-depth";Chen Wang<author:sep>Jiadai Sun<author:sep>Lina Liu<author:sep>Chenming Wu<author:sep>Zhelun Shen<author:sep>Dayan Wu<author:sep>Yuchao Dai<author:sep>Liangjun Zhang;http://arxiv.org/pdf/2308.04413v1;cs.CV;"Accepted to ACM MM 2023. Project Page:
  https://cwchenwang.github.io/outdoor-nerf-depth";nerf
2308.03280v1;http://arxiv.org/abs/2308.03280v1;2023-08-07;Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with  Whitted-Style Ray Tracing;"Recently, Neural Radiance Fields (NeRF) has exhibited significant success in
novel view synthesis, surface reconstruction, etc. However, since no physical
reflection is considered in its rendering pipeline, NeRF mistakes the
reflection in the mirror as a separate virtual scene, leading to the inaccurate
reconstruction of the mirror and multi-view inconsistent reflections in the
mirror. In this paper, we present a novel neural rendering framework, named
Mirror-NeRF, which is able to learn accurate geometry and reflection of the
mirror and support various scene manipulation applications with mirrors, such
as adding new objects or mirrors into the scene and synthesizing the
reflections of these new objects in mirrors, controlling mirror roughness, etc.
To achieve this goal, we propose a unified radiance field by introducing the
reflection probability and tracing rays following the light transport model of
Whitted Ray Tracing, and also develop several techniques to facilitate the
learning process. Experiments and comparisons on both synthetic and real
datasets demonstrate the superiority of our method. The code and supplementary
material are available on the project webpage:
https://zju3dv.github.io/Mirror-NeRF/.";Junyi Zeng<author:sep>Chong Bao<author:sep>Rui Chen<author:sep>Zilong Dong<author:sep>Guofeng Zhang<author:sep>Hujun Bao<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2308.03280v1;cs.CV;"Accepted to ACM Multimedia 2023. Project Page:
  https://zju3dv.github.io/Mirror-NeRF/";nerf
2308.02751v2;http://arxiv.org/abs/2308.02751v2;2023-08-05;NeRFs: The Search for the Best 3D Representation;"Neural Radiance Fields or NeRFs have become the representation of choice for
problems in view synthesis or image-based rendering, as well as in many other
applications across computer graphics and vision, and beyond. At their core,
NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of
meshes, disparity maps, multiplane images or even voxel grids, they represent
the scene as a continuous volume, with volumetric parameters like
view-dependent radiance and volume density obtained by querying a neural
network. The NeRF representation has now been widely used, with thousands of
papers extending or building on it every year, multiple authors and websites
providing overviews and surveys, and numerous industrial applications and
startup companies. In this article, we briefly review the NeRF representation,
and describe the three decades-long quest to find the best 3D representation
for view synthesis and related problems, culminating in the NeRF papers. We
then describe new developments in terms of NeRF representations and make some
observations and insights regarding the future of 3D representations.";Ravi Ramamoorthi;http://arxiv.org/pdf/2308.02751v2;cs.CV;"Updated based on feedback in-person and via e-mail at SIGGRAPH 2023.
  In particular, I have added references and discussion of seminal SIGGRAPH
  image-based rendering papers, and better put the recent Kerbl et al. work in
  context, with more references";nerf
2308.02908v1;http://arxiv.org/abs/2308.02908v1;2023-08-05;Where and How: Mitigating Confusion in Neural Radiance Fields from  Sparse Inputs;"Neural Radiance Fields from Sparse input} (NeRF-S) have shown great potential
in synthesizing novel views with a limited number of observed viewpoints.
However, due to the inherent limitations of sparse inputs and the gap between
non-adjacent views, rendering results often suffer from over-fitting and foggy
surfaces, a phenomenon we refer to as ""CONFUSION"" during volume rendering. In
this paper, we analyze the root cause of this confusion and attribute it to two
fundamental questions: ""WHERE"" and ""HOW"". To this end, we present a novel
learning framework, WaH-NeRF, which effectively mitigates confusion by tackling
the following challenges: (i)""WHERE"" to Sample? in NeRF-S -- we introduce a
Deformable Sampling strategy and a Weight-based Mutual Information Loss to
address sample-position confusion arising from the limited number of
viewpoints; and (ii) ""HOW"" to Predict? in NeRF-S -- we propose a
Semi-Supervised NeRF learning Paradigm based on pose perturbation and a
Pixel-Patch Correspondence Loss to alleviate prediction confusion caused by the
disparity between training and testing viewpoints. By integrating our proposed
modules and loss functions, WaH-NeRF outperforms previous methods under the
NeRF-S setting. Code is available https://github.com/bbbbby-99/WaH-NeRF.";Yanqi Bao<author:sep>Yuxin Li<author:sep>Jing Huo<author:sep>Tianyu Ding<author:sep>Xinyue Liang<author:sep>Wenbin Li<author:sep>Yang Gao;http://arxiv.org/pdf/2308.02908v1;cs.CV;"Accepted In Proceedings of the 31st ACM International Conference on
  Multimedia (MM' 23)";nerf
2308.02840v1;http://arxiv.org/abs/2308.02840v1;2023-08-05;Learning Unified Decompositional and Compositional NeRF for Editable  Novel View Synthesis;"Implicit neural representations have shown powerful capacity in modeling
real-world 3D scenes, offering superior performance in novel view synthesis. In
this paper, we target a more challenging scenario, i.e., joint scene novel view
synthesis and editing based on implicit neural scene representations.
State-of-the-art methods in this direction typically consider building separate
networks for these two tasks (i.e., view synthesis and editing). Thus, the
modeling of interactions and correlations between these two tasks is very
limited, which, however, is critical for learning high-quality scene
representations. To tackle this problem, in this paper, we propose a unified
Neural Radiance Field (NeRF) framework to effectively perform joint scene
decomposition and composition for modeling real-world scenes. The decomposition
aims at learning disentangled 3D representations of different objects and the
background, allowing for scene editing, while scene composition models an
entire scene representation for novel view synthesis. Specifically, with a
two-stage NeRF framework, we learn a coarse stage for predicting a global
radiance field as guidance for point sampling, and in the second fine-grained
stage, we perform scene decomposition by a novel one-hot object radiance field
regularization module and a pseudo supervision via inpainting to handle
ambiguous background regions occluded by objects. The decomposed object-level
radiance fields are further composed by using activations from the
decomposition module. Extensive quantitative and qualitative results show the
effectiveness of our method for scene decomposition and composition,
outperforming state-of-the-art methods for both novel-view synthesis and
editing tasks.";Yuxin Wang<author:sep>Wayne Wu<author:sep>Dan Xu;http://arxiv.org/pdf/2308.02840v1;cs.CV;ICCV2023, Project Page: https://w-ted.github.io/publications/udc-nerf;nerf
2308.02191v1;http://arxiv.org/abs/2308.02191v1;2023-08-04;ES-MVSNet: Efficient Framework for End-to-end Self-supervised Multi-View  Stereo;"Compared to the multi-stage self-supervised multi-view stereo (MVS) method,
the end-to-end (E2E) approach has received more attention due to its concise
and efficient training pipeline. Recent E2E self-supervised MVS approaches have
integrated third-party models (such as optical flow models, semantic
segmentation models, NeRF models, etc.) to provide additional consistency
constraints, which grows GPU memory consumption and complicates the model's
structure and training pipeline. In this work, we propose an efficient
framework for end-to-end self-supervised MVS, dubbed ES-MVSNet. To alleviate
the high memory consumption of current E2E self-supervised MVS frameworks, we
present a memory-efficient architecture that reduces memory usage by 43%
without compromising model performance. Furthermore, with the novel design of
asymmetric view selection policy and region-aware depth consistency, we achieve
state-of-the-art performance among E2E self-supervised MVS methods, without
relying on third-party models for additional consistency signals. Extensive
experiments on DTU and Tanks&Temples benchmarks demonstrate that the proposed
ES-MVSNet approach achieves state-of-the-art performance among E2E
self-supervised MVS methods and competitive performance to many supervised and
multi-stage self-supervised methods.";Qiang Zhou<author:sep>Chaohui Yu<author:sep>Jingliang Li<author:sep>Yuang Liu<author:sep>Jing Wang<author:sep>Zhibin Wang;http://arxiv.org/pdf/2308.02191v1;cs.CV;arXiv admin note: text overlap with arXiv:2203.03949 by other authors;nerf
2308.01262v2;http://arxiv.org/abs/2308.01262v2;2023-08-02;Incorporating Season and Solar Specificity into Renderings made by a  NeRF Architecture using Satellite Images;"As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar
angle into account in a NeRF-based framework for rendering a scene from a novel
viewpoint using satellite images for training. Our work extends those
contributions and shows how one can make the renderings season-specific. Our
main challenge was creating a Neural Radiance Field (NeRF) that could render
seasonal features independently of viewing angle and solar angle while still
being able to render shadows. We teach our network to render seasonal features
by introducing one more input variable -- time of the year. However, the small
training datasets typical of satellite imagery can introduce ambiguities in
cases where shadows are present in the same location for every image of a
particular season. We add additional terms to the loss function to discourage
the network from using seasonal features for accounting for shadows. We show
the performance of our network on eight Areas of Interest containing images
captured by the Maxar WorldView-3 satellite. This evaluation includes tests
measuring the ability of our framework to accurately render novel views,
generate height maps, predict shadows, and specify seasonal features
independently from shadows. Our ablation studies justify the choices made for
network design parameters.";Michael Gableman<author:sep>Avinash Kak;http://arxiv.org/pdf/2308.01262v2;cs.CV;18 pages, 17 figures, 10 tables;nerf
2308.00773v3;http://arxiv.org/abs/2308.00773v3;2023-08-01;High-Fidelity Eye Animatable Neural Radiance Fields for Human Face;"Face rendering using neural radiance fields (NeRF) is a rapidly developing
research area in computer vision. While recent methods primarily focus on
controlling facial attributes such as identity and expression, they often
overlook the crucial aspect of modeling eyeball rotation, which holds
importance for various downstream tasks. In this paper, we aim to learn a face
NeRF model that is sensitive to eye movements from multi-view images. We
address two key challenges in eye-aware face NeRF learning: how to effectively
capture eyeball rotation for training and how to construct a manifold for
representing eyeball rotation. To accomplish this, we first fit FLAME, a
well-established parametric face model, to the multi-view images considering
multi-view consistency. Subsequently, we introduce a new Dynamic Eye-aware NeRF
(DeNeRF). DeNeRF transforms 3D points from different views into a canonical
space to learn a unified face NeRF model. We design an eye deformation field
for the transformation, including rigid transformation, e.g., eyeball rotation,
and non-rigid transformation. Through experiments conducted on the ETH-XGaze
dataset, we demonstrate that our model is capable of generating high-fidelity
images with accurate eyeball rotation and non-rigid periocular deformation,
even under novel viewing angles. Furthermore, we show that utilizing the
rendered images can effectively enhance gaze estimation performance.";Hengfei Wang<author:sep>Zhongqun Zhang<author:sep>Yihua Cheng<author:sep>Hyung Jin Chang;http://arxiv.org/pdf/2308.00773v3;cs.CV;BMVC2023 Oral;nerf
2308.00214v2;http://arxiv.org/abs/2308.00214v2;2023-08-01;Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned  Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF);"Many tasks performed in image-guided, mini-invasive, medical procedures can
be cast as pose estimation problems, where an X-ray projection is utilized to
reach a target in 3D space. Expanding on recent advances in the differentiable
rendering of optically reflective materials, we introduce new methods for pose
estimation of radiolucent objects using X-ray projections, and we demonstrate
the critical role of optimal view synthesis in performing this task. We first
develop an algorithm (DiffDRR) that efficiently computes Digitally
Reconstructed Radiographs (DRRs) and leverages automatic differentiation within
TensorFlow. Pose estimation is performed by iterative gradient descent using a
loss function that quantifies the similarity of the DRR synthesized from a
randomly initialized pose and the true fluoroscopic image at the target pose.
We propose two novel methods for high-fidelity view synthesis, Neural Tuned
Tomography (NeTT) and masked Neural Radiance Fields (mNeRF). Both methods rely
on classic Cone-Beam Computerized Tomography (CBCT); NeTT directly optimizes
the CBCT densities, while the non-zero values of mNeRF are constrained by a 3D
mask of the anatomic region segmented from CBCT. We demonstrate that both NeTT
and mNeRF distinctly improve pose estimation within our framework. By defining
a successful pose estimate to be a 3D angle error of less than 3 deg, we find
that NeTT and mNeRF can achieve similar results, both with overall success
rates more than 93%. However, the computational cost of NeTT is significantly
lower than mNeRF in both training and pose estimation. Furthermore, we show
that a NeTT trained for a single subject can generalize to synthesize
high-fidelity DRRs and ensure robust pose estimations for all other subjects.
Therefore, we suggest that NeTT is an attractive option for robust pose
estimation using fluoroscopic projections.";Chaochao Zhou<author:sep>Syed Hasib Akhter Faruqui<author:sep>Abhinav Patel<author:sep>Ramez N. Abdalla<author:sep>Michael C. Hurley<author:sep>Ali Shaibani<author:sep>Matthew B. Potts<author:sep>Babak S. Jahromi<author:sep>Leon Cho<author:sep>Sameer A. Ansari<author:sep>Donald R. Cantrell;http://arxiv.org/pdf/2308.00214v2;cs.CV;;nerf
2308.00462v3;http://arxiv.org/abs/2308.00462v3;2023-08-01;Context-Aware Talking-Head Video Editing;"Talking-head video editing aims to efficiently insert, delete, and substitute
the word of a pre-recorded video through a text transcript editor. The key
challenge for this task is obtaining an editing model that generates new
talking-head video clips which simultaneously have accurate lip synchronization
and motion smoothness. Previous approaches, including 3DMM-based (3D Morphable
Model) methods and NeRF-based (Neural Radiance Field) methods, are sub-optimal
in that they either require minutes of source videos and days of training time
or lack the disentangled control of verbal (e.g., lip motion) and non-verbal
(e.g., head pose and expression) representations for video clip insertion. In
this work, we fully utilize the video context to design a novel framework for
talking-head video editing, which achieves efficiency, disentangled motion
control, and sequential smoothness. Specifically, we decompose this framework
to motion prediction and motion-conditioned rendering: (1) We first design an
animation prediction module that efficiently obtains smooth and lip-sync motion
sequences conditioned on the driven speech. This module adopts a
non-autoregressive network to obtain context prior and improve the prediction
efficiency, and it learns a speech-animation mapping prior with better
generalization to novel speech from a multi-identity video dataset. (2) We then
introduce a neural rendering module to synthesize the photo-realistic and
full-head video frames given the predicted motion sequence. This module adopts
a pre-trained head topology and uses only few frames for efficient fine-tuning
to obtain a person-specific rendering model. Extensive experiments demonstrate
that our method efficiently achieves smoother editing results with higher image
quality and lip accuracy using less data than previous methods.";Songlin Yang<author:sep>Wei Wang<author:sep>Jun Ling<author:sep>Bo Peng<author:sep>Xu Tan<author:sep>Jing Dong;http://arxiv.org/pdf/2308.00462v3;cs.MM;The version of this paper needs to be further improved;nerf
2307.15333v1;http://arxiv.org/abs/2307.15333v1;2023-07-28;Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF;"The explicit neural radiance field (NeRF) has gained considerable interest
for its efficient training and fast inference capabilities, making it a
promising direction such as virtual reality and gaming. In particular,
PlenOctree (POT)[1], an explicit hierarchical multi-scale octree
representation, has emerged as a structural and influential framework. However,
POT's fixed structure for direct optimization is sub-optimal as the scene
complexity evolves continuously with updates to cached color and density,
necessitating refining the sampling distribution to capture signal complexity
accordingly. To address this issue, we propose the dynamic PlenOctree DOT,
which adaptively refines the sample distribution to adjust to changing scene
complexity. Specifically, DOT proposes a concise yet novel hierarchical feature
fusion strategy during the iterative rendering process. Firstly, it identifies
the regions of interest through training signals to ensure adaptive and
efficient refinement. Next, rather than directly filtering out valueless nodes,
DOT introduces the sampling and pruning operations for octrees to aggregate
features, enabling rapid parameter learning. Compared with POT, our DOT
outperforms it by enhancing visual quality, reducing over $55.15$/$68.84\%$
parameters, and providing 1.7/1.9 times FPS for NeRF-synthetic and Tanks $\&$
Temples, respectively. Project homepage:https://vlislab22.github.io/DOT.
  [1] Yu, Alex, et al. ""Plenoctrees for real-time rendering of neural radiance
fields."" Proceedings of the IEEE/CVF International Conference on Computer
Vision. 2021.";Haotian Bai<author:sep>Yiqi Lin<author:sep>Yize Chen<author:sep>Lin Wang;http://arxiv.org/pdf/2307.15333v1;cs.CV;Accepted by ICCV2023;nerf
2308.03772v1;http://arxiv.org/abs/2308.03772v1;2023-07-27;Improved Neural Radiance Fields Using Pseudo-depth and Fusion;"Since the advent of Neural Radiance Fields, novel view synthesis has received
tremendous attention. The existing approach for the generalization of radiance
field reconstruction primarily constructs an encoding volume from nearby source
images as additional inputs. However, these approaches cannot efficiently
encode the geometric information of real scenes with various scale
objects/structures. In this work, we propose constructing multi-scale encoding
volumes and providing multi-scale geometry information to NeRF models. To make
the constructed volumes as close as possible to the surfaces of objects in the
scene and the rendered depth more accurate, we propose to perform depth
prediction and radiance field reconstruction simultaneously. The predicted
depth map will be used to supervise the rendered depth, narrow the depth range,
and guide points sampling. Finally, the geometric information contained in
point volume features may be inaccurate due to occlusion, lighting, etc. To
this end, we propose enhancing the point volume feature from depth-guided
neighbor feature fusion. Experiments demonstrate the superior performance of
our method in both novel view synthesis and dense geometry modeling without
per-scene optimization.";Jingliang Li<author:sep>Qiang Zhou<author:sep>Chaohui Yu<author:sep>Zhengda Lu<author:sep>Jun Xiao<author:sep>Zhibin Wang<author:sep>Fan Wang;http://arxiv.org/pdf/2308.03772v1;cs.CV;;nerf
2307.14981v2;http://arxiv.org/abs/2307.14981v2;2023-07-27;MapNeRF: Incorporating Map Priors into Neural Radiance Fields for  Driving View Simulation;"Simulating camera sensors is a crucial task in autonomous driving. Although
neural radiance fields are exceptional at synthesizing photorealistic views in
driving simulations, they still fail to generate extrapolated views. This paper
proposes to incorporate map priors into neural radiance fields to synthesize
out-of-trajectory driving views with semantic road consistency. The key insight
is that map information can be utilized as a prior to guiding the training of
the radiance fields with uncertainty. Specifically, we utilize the coarse
ground surface as uncertain information to supervise the density field and warp
depth with uncertainty from unknown camera poses to ensure multi-view
consistency. Experimental results demonstrate that our approach can produce
semantic consistency in deviated views for vehicle camera simulation. The
supplementary video can be viewed at https://youtu.be/jEQWr-Rfh3A.";Chenming Wu<author:sep>Jiadai Sun<author:sep>Zhelun Shen<author:sep>Liangjun Zhang;http://arxiv.org/pdf/2307.14981v2;cs.CV;"Accepted by IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2023";nerf
2307.14620v1;http://arxiv.org/abs/2307.14620v1;2023-07-27;NeRF-Det: Learning Geometry-Aware Volumetric Representation for  Multi-View 3D Object Detection;"We present NeRF-Det, a novel method for indoor 3D detection with posed RGB
images as input. Unlike existing indoor 3D detection methods that struggle to
model scene geometry, our method makes novel use of NeRF in an end-to-end
manner to explicitly estimate 3D geometry, thereby improving 3D detection
performance. Specifically, to avoid the significant extra latency associated
with per-scene optimization of NeRF, we introduce sufficient geometry priors to
enhance the generalizability of NeRF-MLP. Furthermore, we subtly connect the
detection and NeRF branches through a shared MLP, enabling an efficient
adaptation of NeRF to detection and yielding geometry-aware volumetric
representations for 3D detection. Our method outperforms state-of-the-arts by
3.9 mAP and 3.1 mAP on the ScanNet and ARKITScenes benchmarks, respectively. We
provide extensive analysis to shed light on how NeRF-Det works. As a result of
our joint-training design, NeRF-Det is able to generalize well to unseen scenes
for object detection, view synthesis, and depth estimation tasks without
requiring per-scene optimization. Code is available at
\url{https://github.com/facebookresearch/NeRF-Det}.";Chenfeng Xu<author:sep>Bichen Wu<author:sep>Ji Hou<author:sep>Sam Tsai<author:sep>Ruilong Li<author:sep>Jialiang Wang<author:sep>Wei Zhan<author:sep>Zijian He<author:sep>Peter Vajda<author:sep>Kurt Keutzer<author:sep>Masayoshi Tomizuka;http://arxiv.org/pdf/2307.14620v1;cs.CV;Accepted by ICCV 2023;nerf
2307.15058v1;http://arxiv.org/abs/2307.15058v1;2023-07-27;MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous  Driving;"Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is
widely recognized that realistic sensor simulation will play a critical role in
solving remaining corner cases by simulating them. To this end, we propose an
autonomous driving simulator based upon neural radiance fields (NeRFs).
Compared with existing works, ours has three notable features: (1)
Instance-aware. Our simulator models the foreground instances and background
environments separately with independent networks so that the static (e.g.,
size and appearance) and dynamic (e.g., trajectory) properties of instances can
be controlled separately. (2) Modular. Our simulator allows flexible switching
between different modern NeRF-related backbones, sampling strategies, input
modalities, etc. We expect this modular design to boost academic progress and
industrial deployment of NeRF-based autonomous driving simulation. (3)
Realistic. Our simulator set new state-of-the-art photo-realism results given
the best module selection. Our simulator will be open-sourced while most of our
counterparts are not. Project page: https://open-air-sun.github.io/mars/.";Zirui Wu<author:sep>Tianyu Liu<author:sep>Liyi Luo<author:sep>Zhide Zhong<author:sep>Jianteng Chen<author:sep>Hongmin Xiao<author:sep>Chao Hou<author:sep>Haozhe Lou<author:sep>Yuantao Chen<author:sep>Runyi Yang<author:sep>Yuxin Huang<author:sep>Xiaoyu Ye<author:sep>Zike Yan<author:sep>Yongliang Shi<author:sep>Yiyi Liao<author:sep>Hao Zhao;http://arxiv.org/pdf/2307.15058v1;cs.CV;"CICAI 2023, project page with code:
  https://open-air-sun.github.io/mars/";nerf
2307.15131v2;http://arxiv.org/abs/2307.15131v2;2023-07-27;Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields;"With the popularity of implicit neural representations, or neural radiance
fields (NeRF), there is a pressing need for editing methods to interact with
the implicit 3D models for tasks like post-processing reconstructed scenes and
3D content creation. While previous works have explored NeRF editing from
various perspectives, they are restricted in editing flexibility, quality, and
speed, failing to offer direct editing response and instant preview. The key
challenge is to conceive a locally editable neural representation that can
directly reflect the editing instructions and update instantly. To bridge the
gap, we propose a new interactive editing method and system for implicit
representations, called Seal-3D, which allows users to edit NeRF models in a
pixel-level and free manner with a wide range of NeRF-like backbone and preview
the editing effects instantly. To achieve the effects, the challenges are
addressed by our proposed proxy function mapping the editing instructions to
the original space of NeRF models in the teacher model and a two-stage training
strategy for the student model with local pretraining and global finetuning. A
NeRF editing system is built to showcase various editing types. Our system can
achieve compelling editing effects with an interactive speed of about 1 second.";Xiangyu Wang<author:sep>Jingsen Zhu<author:sep>Qi Ye<author:sep>Yuchi Huo<author:sep>Yunlong Ran<author:sep>Zhihua Zhong<author:sep>Jiming Chen;http://arxiv.org/pdf/2307.15131v2;cs.CV;"Accepted by ICCV2023. Project Page:
  https://windingwind.github.io/seal-3d/ Code:
  https://github.com/windingwind/seal-3d/";nerf
2307.13908v1;http://arxiv.org/abs/2307.13908v1;2023-07-26;Points-to-3D: Bridging the Gap between Sparse Points and  Shape-Controllable Text-to-3D Generation;"Text-to-3D generation has recently garnered significant attention, fueled by
2D diffusion models trained on billions of image-text pairs. Existing methods
primarily rely on score distillation to leverage the 2D diffusion priors to
supervise the generation of 3D models, e.g., NeRF. However, score distillation
is prone to suffer the view inconsistency problem, and implicit NeRF modeling
can also lead to an arbitrary shape, thus leading to less realistic and
uncontrollable 3D generation. In this work, we propose a flexible framework of
Points-to-3D to bridge the gap between sparse yet freely available 3D points
and realistic shape-controllable 3D generation by distilling the knowledge from
both 2D and 3D diffusion models. The core idea of Points-to-3D is to introduce
controllable sparse 3D points to guide the text-to-3D generation. Specifically,
we use the sparse point cloud generated from the 3D diffusion model, Point-E,
as the geometric prior, conditioned on a single reference image. To better
utilize the sparse 3D points, we propose an efficient point cloud guidance loss
to adaptively drive the NeRF's geometry to align with the shape of the sparse
3D points. In addition to controlling the geometry, we propose to optimize the
NeRF for a more view-consistent appearance. To be specific, we perform score
distillation to the publicly available 2D image diffusion model ControlNet,
conditioned on text as well as depth map of the learned compact geometry.
Qualitative and quantitative comparisons demonstrate that Points-to-3D improves
view consistency and achieves good shape controllability for text-to-3D
generation. Points-to-3D provides users with a new way to improve and control
text-to-3D generation.";Chaohui Yu<author:sep>Qiang Zhou<author:sep>Jingliang Li<author:sep>Zhe Zhang<author:sep>Zhibin Wang<author:sep>Fan Wang;http://arxiv.org/pdf/2307.13908v1;cs.CV;Accepted by ACMMM 2023;nerf
2307.12909v1;http://arxiv.org/abs/2307.12909v1;2023-07-24;Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields;"Recently, the editing of neural radiance fields (NeRFs) has gained
considerable attention, but most prior works focus on static scenes while
research on the appearance editing of dynamic scenes is relatively lacking. In
this paper, we propose a novel framework to edit the local appearance of
dynamic NeRFs by manipulating pixels in a single frame of training video.
Specifically, to locally edit the appearance of dynamic NeRFs while preserving
unedited regions, we introduce a local surface representation of the edited
region, which can be inserted into and rendered along with the original NeRF
and warped to arbitrary other frames through a learned invertible motion
representation network. By employing our method, users without professional
expertise can easily add desired content to the appearance of a dynamic scene.
We extensively evaluate our approach on various scenes and show that our
approach achieves spatially and temporally consistent editing results. Notably,
our approach is versatile and applicable to different variants of dynamic NeRF
representations.";Shangzhan Zhang<author:sep>Sida Peng<author:sep>Yinji ShenTu<author:sep>Qing Shuai<author:sep>Tianrun Chen<author:sep>Kaicheng Yu<author:sep>Hujun Bao<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2307.12909v1;cs.CV;project page: https://dyn-e.github.io/;nerf
2307.12718v1;http://arxiv.org/abs/2307.12718v1;2023-07-24;CarPatch: A Synthetic Benchmark for Radiance Field Evaluation on Vehicle  Components;"Neural Radiance Fields (NeRFs) have gained widespread recognition as a highly
effective technique for representing 3D reconstructions of objects and scenes
derived from sets of images. Despite their efficiency, NeRF models can pose
challenges in certain scenarios such as vehicle inspection, where the lack of
sufficient data or the presence of challenging elements (e.g. reflections)
strongly impact the accuracy of the reconstruction. To this aim, we introduce
CarPatch, a novel synthetic benchmark of vehicles. In addition to a set of
images annotated with their intrinsic and extrinsic camera parameters, the
corresponding depth maps and semantic segmentation masks have been generated
for each view. Global and part-based metrics have been defined and used to
evaluate, compare, and better characterize some state-of-the-art techniques.
The dataset is publicly released at
https://aimagelab.ing.unimore.it/go/carpatch and can be used as an evaluation
guide and as a baseline for future work on this challenging topic.";Davide Di Nucci<author:sep>Alessandro Simoni<author:sep>Matteo Tomei<author:sep>Luca Ciuffreda<author:sep>Roberto Vezzani<author:sep>Rita Cucchiara;http://arxiv.org/pdf/2307.12718v1;cs.CV;Accepted at ICIAP2023;nerf
2307.12291v1;http://arxiv.org/abs/2307.12291v1;2023-07-23;TransHuman: A Transformer-based Human Representation for Generalizable  Neural Human Rendering;"In this paper, we focus on the task of generalizable neural human rendering
which trains conditional Neural Radiance Fields (NeRF) from multi-view videos
of different characters. To handle the dynamic human motion, previous methods
have primarily used a SparseConvNet (SPC)-based human representation to process
the painted SMPL. However, such SPC-based representation i) optimizes under the
volatile observation space which leads to the pose-misalignment between
training and inference stages, and ii) lacks the global relationships among
human parts that is critical for handling the incomplete painted SMPL. Tackling
these issues, we present a brand-new framework named TransHuman, which learns
the painted SMPL under the canonical space and captures the global
relationships between human parts with transformers. Specifically, TransHuman
is mainly composed of Transformer-based Human Encoding (TransHE), Deformable
Partial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI).
TransHE first processes the painted SMPL under the canonical space via
transformers for capturing the global relationships between human parts. Then,
DPaRF binds each output token with a deformable radiance field for encoding the
query point under the observation space. Finally, the FDI is employed to
further integrate fine-grained information from reference images. Extensive
experiments on ZJU-MoCap and H36M show that our TransHuman achieves a
significantly new state-of-the-art performance with high efficiency. Project
page: https://pansanity666.github.io/TransHuman/";Xiao Pan<author:sep>Zongxin Yang<author:sep>Jianxin Ma<author:sep>Chang Zhou<author:sep>Yi Yang;http://arxiv.org/pdf/2307.12291v1;cs.CV;Accepted by ICCV 2023;nerf
2307.11418v3;http://arxiv.org/abs/2307.11418v3;2023-07-21;FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural  Radiance Fields;"As recent advances in Neural Radiance Fields (NeRF) have enabled
high-fidelity 3D face reconstruction and novel view synthesis, its manipulation
also became an essential task in 3D vision. However, existing manipulation
methods require extensive human labor, such as a user-provided semantic mask
and manual attribute search unsuitable for non-expert users. Instead, our
approach is designed to require a single text to manipulate a face
reconstructed with NeRF. To do so, we first train a scene manipulator, a latent
code-conditional deformable NeRF, over a dynamic scene to control a face
deformation using the latent code. However, representing a scene deformation
with a single latent code is unfavorable for compositing local deformations
observed in different instances. As so, our proposed Position-conditional
Anchor Compositor (PAC) learns to represent a manipulated scene with spatially
varying latent codes. Their renderings with the scene manipulator are then
optimized to yield high cosine similarity to a target text in CLIP embedding
space for text-driven manipulation. To the best of our knowledge, our approach
is the first to address the text-driven manipulation of a face reconstructed
with NeRF. Extensive results, comparisons, and ablation studies demonstrate the
effectiveness of our approach.";Sungwon Hwang<author:sep>Junha Hyung<author:sep>Daejin Kim<author:sep>Min-Jung Kim<author:sep>Jaegul Choo;http://arxiv.org/pdf/2307.11418v3;cs.CV;ICCV 2023 project page at https://faceclipnerf.github.io;nerf
2307.11335v1;http://arxiv.org/abs/2307.11335v1;2023-07-21;Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural  Radiance Fields;"Despite the tremendous progress in neural radiance fields (NeRF), we still
face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF
presents fine-detailed and anti-aliased renderings but takes days for training,
while Instant-ngp can accomplish the reconstruction in a few minutes but
suffers from blurring or aliasing when rendering at various distances or
resolutions due to ignoring the sampling area. To this end, we propose a novel
Tri-Mip encoding that enables both instant reconstruction and anti-aliased
high-fidelity rendering for neural radiance fields. The key is to factorize the
pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can
efficiently perform 3D area sampling by taking advantage of 2D pre-filtered
feature maps, which significantly elevates the rendering quality without
sacrificing efficiency. To cope with the novel Tri-Mip representation, we
propose a cone-casting rendering technique to efficiently sample anti-aliased
3D features with the Tri-Mip encoding considering both pixel imaging and
observing distance. Extensive experiments on both synthetic and real-world
datasets demonstrate our method achieves state-of-the-art rendering quality and
reconstruction speed while maintaining a compact representation that reduces
25% model size compared against Instant-ngp.";Wenbo Hu<author:sep>Yuling Wang<author:sep>Lin Ma<author:sep>Bangbang Yang<author:sep>Lin Gao<author:sep>Xiao Liu<author:sep>Yuewen Ma;http://arxiv.org/pdf/2307.11335v1;cs.CV;"Accepted to ICCV 2023 Project page:
  https://wbhu.github.io/projects/Tri-MipRF";nerf
2307.11526v2;http://arxiv.org/abs/2307.11526v2;2023-07-21;CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields;"Neural Radiance Fields (NeRF) have the potential to be a major representation
of media. Since training a NeRF has never been an easy task, the protection of
its model copyright should be a priority. In this paper, by analyzing the pros
and cons of possible copyright protection solutions, we propose to protect the
copyright of NeRF models by replacing the original color representation in NeRF
with a watermarked color representation. Then, a distortion-resistant rendering
scheme is designed to guarantee robust message extraction in 2D renderings of
NeRF. Our proposed method can directly protect the copyright of NeRF models
while maintaining high rendering quality and bit accuracy when compared among
optional solutions.";Ziyuan Luo<author:sep>Qing Guo<author:sep>Ka Chun Cheung<author:sep>Simon See<author:sep>Renjie Wan;http://arxiv.org/pdf/2307.11526v2;cs.CV;11 pages, 6 figures, accepted by ICCV 2023 non-camera-ready version;nerf
2307.10664v1;http://arxiv.org/abs/2307.10664v1;2023-07-20;Lighting up NeRF via Unsupervised Decomposition and Enhancement;"Neural Radiance Field (NeRF) is a promising approach for synthesizing novel
views, given a set of images and the corresponding camera poses of a scene.
However, images photographed from a low-light scene can hardly be used to train
a NeRF model to produce high-quality results, due to their low pixel
intensities, heavy noise, and color distortion. Combining existing low-light
image enhancement methods with NeRF methods also does not work well due to the
view inconsistency caused by the individual 2D enhancement process. In this
paper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to
enhance the scene representation and synthesize normal-light novel views
directly from sRGB low-light images in an unsupervised manner. The core of our
approach is a decomposition of radiance field learning, which allows us to
enhance the illumination, reduce noise and correct the distorted colors jointly
with the NeRF optimization process. Our method is able to produce novel view
images with proper lighting and vivid colors and details, given a collection of
camera-finished low dynamic range (8-bits/channel) images from a low-light
scene. Experiments demonstrate that our method outperforms existing low-light
enhancement methods and NeRF methods.";Haoyuan Wang<author:sep>Xiaogang Xu<author:sep>Ke Xu<author:sep>Rynson WH. Lau;http://arxiv.org/pdf/2307.10664v1;cs.CV;ICCV 2023. Project website: https://whyy.site/paper/llnerf;nerf
2307.10776v1;http://arxiv.org/abs/2307.10776v1;2023-07-20;Urban Radiance Field Representation with Deformable Neural Mesh  Primitives;"Neural Radiance Fields (NeRFs) have achieved great success in the past few
years. However, most current methods still require intensive resources due to
ray marching-based rendering. To construct urban-level radiance fields
efficiently, we design Deformable Neural Mesh Primitive~(DNMP), and propose to
parameterize the entire scene with such primitives. The DNMP is a flexible and
compact neural variant of classic mesh representation, which enjoys both the
efficiency of rasterization-based rendering and the powerful neural
representation capability for photo-realistic image synthesis. Specifically, a
DNMP consists of a set of connected deformable mesh vertices with paired vertex
features to parameterize the geometry and radiance information of a local area.
To constrain the degree of freedom for optimization and lower the storage
budgets, we enforce the shape of each primitive to be decoded from a relatively
low-dimensional latent space. The rendering colors are decoded from the vertex
features (interpolated with rasterization) by a view-dependent MLP. The DNMP
provides a new paradigm for urban-level scene representation with appealing
properties: $(1)$ High-quality rendering. Our method achieves leading
performance for novel view synthesis in urban scenarios. $(2)$ Low
computational costs. Our representation enables fast rendering (2.07ms/1k
pixels) and low peak memory usage (110MB/1k pixels). We also present a
lightweight version that can run 33$\times$ faster than vanilla NeRFs, and
comparable to the highly-optimized Instant-NGP (0.61 vs 0.71ms/1k pixels).
Project page: \href{https://dnmp.github.io/}{https://dnmp.github.io/}.";Fan Lu<author:sep>Yan Xu<author:sep>Guang Chen<author:sep>Hongsheng Li<author:sep>Kwan-Yee Lin<author:sep>Changjun Jiang;http://arxiv.org/pdf/2307.10776v1;cs.CV;Accepted to ICCV2023;nerf
2307.09860v1;http://arxiv.org/abs/2307.09860v1;2023-07-19;Magic NeRF Lens: Interactive Fusion of Neural Radiance Fields for  Virtual Facility Inspection;"Large industrial facilities such as particle accelerators and nuclear power
plants are critical infrastructures for scientific research and industrial
processes. These facilities are complex systems that not only require regular
maintenance and upgrades but are often inaccessible to humans due to various
safety hazards. Therefore, a virtual reality (VR) system that can quickly
replicate real-world remote environments to provide users with a high level of
spatial and situational awareness is crucial for facility maintenance planning.
However, the exact 3D shapes of these facilities are often too complex to be
accurately modeled with geometric primitives through the traditional
rasterization pipeline.
  In this work, we develop Magic NeRF Lens, an interactive framework to support
facility inspection in immersive VR using neural radiance fields (NeRF) and
volumetric rendering. We introduce a novel data fusion approach that combines
the complementary strengths of volumetric rendering and geometric
rasterization, allowing a NeRF model to be merged with other conventional 3D
data, such as a computer-aided design model. We develop two novel 3D magic lens
effects to optimize NeRF rendering by exploiting the properties of human vision
and context-aware visualization. We demonstrate the high usability of our
framework and methods through a technical benchmark, a visual search user
study, and expert reviews. In addition, the source code of our VR NeRF
framework is made publicly available for future research and development.";Ke Li<author:sep>Susanne Schmidt<author:sep>Tim Rolff<author:sep>Reinhard Bacher<author:sep>Wim Leemans<author:sep>Frank Steinicke;http://arxiv.org/pdf/2307.09860v1;cs.GR;"This work has been submitted to the IEEE TVCG for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible";nerf
2307.09323v2;http://arxiv.org/abs/2307.09323v2;2023-07-18;Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking  Portrait Synthesis;"This paper presents ER-NeRF, a novel conditional Neural Radiance Fields
(NeRF) based architecture for talking portrait synthesis that can concurrently
achieve fast convergence, real-time rendering, and state-of-the-art performance
with small model size. Our idea is to explicitly exploit the unequal
contribution of spatial regions to guide talking portrait modeling.
Specifically, to improve the accuracy of dynamic head reconstruction, a compact
and expressive NeRF-based Tri-Plane Hash Representation is introduced by
pruning empty spatial regions with three planar hash encoders. For speech
audio, we propose a Region Attention Module to generate region-aware condition
feature via an attention mechanism. Different from existing methods that
utilize an MLP-based encoder to learn the cross-modal relation implicitly, the
attention mechanism builds an explicit connection between audio features and
spatial regions to capture the priors of local motions. Moreover, a direct and
fast Adaptive Pose Encoding is introduced to optimize the head-torso separation
problem by mapping the complex transformation of the head pose into spatial
coordinates. Extensive experiments demonstrate that our method renders better
high-fidelity and audio-lips synchronized talking portrait videos, with
realistic details and high efficiency compared to previous methods.";Jiahe Li<author:sep>Jiawei Zhang<author:sep>Xiao Bai<author:sep>Jun Zhou<author:sep>Lin Gu;http://arxiv.org/pdf/2307.09323v2;cs.CV;"Accepted by ICCV 2023. Project page:
  https://fictionarry.github.io/ER-NeRF/";nerf
2307.09070v1;http://arxiv.org/abs/2307.09070v1;2023-07-18;PixelHuman: Animatable Neural Radiance Fields from Few Images;"In this paper, we propose PixelHuman, a novel human rendering model that
generates animatable human scenes from a few images of a person with unseen
identity, views, and poses. Previous work have demonstrated reasonable
performance in novel view and pose synthesis, but they rely on a large number
of images to train and are trained per scene from videos, which requires
significant amount of time to produce animatable scenes from unseen human
images. Our method differs from existing methods in that it can generalize to
any input image for animatable human synthesis. Given a random pose sequence,
our method synthesizes each target scene using a neural radiance field that is
conditioned on a canonical representation and pose-aware pixel-aligned
features, both of which can be obtained through deformation fields learned in a
data-driven manner. Our experiments show that our method achieves
state-of-the-art performance in multiview and novel pose synthesis from
few-shot images.";Gyumin Shim<author:sep>Jaeseong Lee<author:sep>Junha Hyung<author:sep>Jaegul Choo;http://arxiv.org/pdf/2307.09070v1;cs.CV;8 pages;
2307.09153v2;http://arxiv.org/abs/2307.09153v2;2023-07-18;OPHAvatars: One-shot Photo-realistic Head Avatars;"We propose a method for synthesizing photo-realistic digital avatars from
only one portrait as the reference. Given a portrait, our method synthesizes a
coarse talking head video using driving keypoints features. And with the coarse
video, our method synthesizes a coarse talking head avatar with a deforming
neural radiance field. With rendered images of the coarse avatar, our method
updates the low-quality images with a blind face restoration model. With
updated images, we retrain the avatar for higher quality. After several
iterations, our method can synthesize a photo-realistic animatable 3D neural
head avatar. The motivation of our method is deformable neural radiance field
can eliminate the unnatural distortion caused by the image2video method. Our
method outperforms state-of-the-art methods in quantitative and qualitative
studies on various subjects.";Shaoxu Li;http://arxiv.org/pdf/2307.09153v2;cs.CV;code: https://github.com/lsx0101/OPHAvatars;
2307.08093v2;http://arxiv.org/abs/2307.08093v2;2023-07-16;Cross-Ray Neural Radiance Fields for Novel-view Synthesis from  Unconstrained Image Collections;"Neural Radiance Fields (NeRF) is a revolutionary approach for rendering
scenes by sampling a single ray per pixel and it has demonstrated impressive
capabilities in novel-view synthesis from static scene images. However, in
practice, we usually need to recover NeRF from unconstrained image collections,
which poses two challenges: 1) the images often have dynamic changes in
appearance because of different capturing time and camera settings; 2) the
images may contain transient objects such as humans and cars, leading to
occlusion and ghosting artifacts. Conventional approaches seek to address these
challenges by locally utilizing a single ray to synthesize a color of a pixel.
In contrast, humans typically perceive appearance and objects by globally
utilizing information across multiple pixels. To mimic the perception process
of humans, in this paper, we propose Cross-Ray NeRF (CR-NeRF) that leverages
interactive information across multiple rays to synthesize occlusion-free novel
views with the same appearances as the images. Specifically, to model varying
appearances, we first propose to represent multiple rays with a novel cross-ray
feature and then recover the appearance by fusing global statistics, i.e.,
feature covariance of the rays and the image appearance. Moreover, to avoid
occlusion introduced by transient objects, we propose a transient objects
handler and introduce a grid sampling strategy for masking out the transient
objects. We theoretically find that leveraging correlation across multiple rays
promotes capturing more global information. Moreover, extensive experimental
results on large real-world datasets verify the effectiveness of CR-NeRF.";Yifan Yang<author:sep>Shuhai Zhang<author:sep>Zixiong Huang<author:sep>Yubing Zhang<author:sep>Mingkui Tan;http://arxiv.org/pdf/2307.08093v2;cs.CV;ICCV 2023 Oral;nerf
2307.07729v1;http://arxiv.org/abs/2307.07729v1;2023-07-15;Improving NeRF with Height Data for Utilization of GIS Data;"Neural Radiance Fields (NeRF) has been applied to various tasks related to
representations of 3D scenes. Most studies based on NeRF have focused on a
small object, while a few studies have tried to reconstruct large-scale scenes
although these methods tend to require large computational cost. For the
application of NeRF to large-scale scenes, a method based on NeRF is proposed
in this paper to effectively use height data which can be obtained from GIS
(Geographic Information System). For this purpose, the scene space was divided
into multiple objects and a background using the height data to represent them
with separate neural networks. In addition, an adaptive sampling method is also
proposed by using the height data. As a result, the accuracy of image rendering
was improved with faster training speed.";Hinata Aoki<author:sep>Takao Yamanaka;http://arxiv.org/pdf/2307.07729v1;cs.CV;ICIP2023;nerf
2307.07125v2;http://arxiv.org/abs/2307.07125v2;2023-07-14;CeRF: Convolutional Neural Radiance Fields for New View Synthesis with  Derivatives of Ray Modeling;"In recent years, novel view synthesis has gained popularity in generating
high-fidelity images. While demonstrating superior performance in the task of
synthesizing novel views, the majority of these methods are still based on the
conventional multi-layer perceptron for scene embedding. Furthermore, light
field models suffer from geometric blurring during pixel rendering, while
radiance field-based volume rendering methods have multiple solutions for a
certain target of density distribution integration. To address these issues, we
introduce the Convolutional Neural Radiance Fields to model the derivatives of
radiance along rays. Based on 1D convolutional operations, our proposed method
effectively extracts potential ray representations through a structured neural
network architecture. Besides, with the proposed ray modeling, a proposed
recurrent module is employed to solve geometric ambiguity in the fully neural
rendering process. Extensive experiments demonstrate the promising results of
our proposed model compared with existing state-of-the-art methods.";Xiaoyan Yang<author:sep>Dingbo Lu<author:sep>Yang Li<author:sep>Chenhui Li<author:sep>Changbo Wang;http://arxiv.org/pdf/2307.07125v2;cs.CV;16 pages, 11 figures;
2307.09555v1;http://arxiv.org/abs/2307.09555v1;2023-07-14;Transient Neural Radiance Fields for Lidar View Synthesis and 3D  Reconstruction;"Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling
scene appearance and geometry from multiview imagery. Recent work has also
begun to explore how to use additional supervision from lidar or depth sensor
measurements in the NeRF framework. However, previous lidar-supervised NeRFs
focus on rendering conventional camera imagery and use lidar-derived point
cloud data as auxiliary supervision; thus, they fail to incorporate the
underlying image formation model of the lidar. Here, we propose a novel method
for rendering transient NeRFs that take as input the raw, time-resolved photon
count histograms measured by a single-photon lidar system, and we seek to
render such histograms from novel views. Different from conventional NeRFs, the
approach relies on a time-resolved version of the volume rendering equation to
render the lidar measurements and capture transient light transport phenomena
at picosecond timescales. We evaluate our method on a first-of-its-kind dataset
of simulated and captured transient multiview scans from a prototype
single-photon lidar. Overall, our work brings NeRFs to a new dimension of
imaging at transient timescales, newly enabling rendering of transient imagery
from novel views. Additionally, we show that our approach recovers improved
geometry and conventional appearance compared to point cloud-based supervision
when training on few input viewpoints. Transient NeRFs may be especially useful
for applications which seek to simulate raw lidar measurements for downstream
tasks in autonomous driving, robotics, and remote sensing.";Anagh Malik<author:sep>Parsa Mirdehghan<author:sep>Sotiris Nousias<author:sep>Kiriakos N. Kutulakos<author:sep>David B. Lindell;http://arxiv.org/pdf/2307.09555v1;cs.CV;;nerf
2307.05087v1;http://arxiv.org/abs/2307.05087v1;2023-07-11;SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View  Representation;"SAR images are highly sensitive to observation configurations, and they
exhibit significant variations across different viewing angles, making it
challenging to represent and learn their anisotropic features. As a result,
deep learning methods often generalize poorly across different view angles.
Inspired by the concept of neural radiance fields (NeRF), this study combines
SAR imaging mechanisms with neural networks to propose a novel NeRF model for
SAR image generation. Following the mapping and projection pinciples, a set of
SAR images is modeled implicitly as a function of attenuation coefficients and
scattering intensities in the 3D imaging space through a differentiable
rendering equation. SAR-NeRF is then constructed to learn the distribution of
attenuation coefficients and scattering intensities of voxels, where the
vectorized form of 3D voxel SAR rendering equation and the sampling
relationship between the 3D space voxels and the 2D view ray grids are
analytically derived. Through quantitative experiments on various datasets, we
thoroughly assess the multi-view representation and generalization capabilities
of SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset can
significantly improve SAR target classification performance under few-shot
learning setup, where a 10-type classification accuracy of 91.6\% can be
achieved by using only 12 images per class.";Zhengxin Lei<author:sep>Feng Xu<author:sep>Jiangtao Wei<author:sep>Feng Cai<author:sep>Feng Wang<author:sep>Ya-Qiu Jin;http://arxiv.org/pdf/2307.05087v1;cs.CV;;nerf
2307.03441v1;http://arxiv.org/abs/2307.03441v1;2023-07-07;NOFA: NeRF-based One-shot Facial Avatar Reconstruction;"3D facial avatar reconstruction has been a significant research topic in
computer graphics and computer vision, where photo-realistic rendering and
flexible controls over poses and expressions are necessary for many related
applications. Recently, its performance has been greatly improved with the
development of neural radiance fields (NeRF). However, most existing NeRF-based
facial avatars focus on subject-specific reconstruction and reenactment,
requiring multi-shot images containing different views of the specific subject
for training, and the learned model cannot generalize to new identities,
limiting its further applications. In this work, we propose a one-shot 3D
facial avatar reconstruction framework that only requires a single source image
to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking
generalization ability and missing multi-view information, we leverage the
generative prior of 3D GAN and develop an efficient encoder-decoder network to
reconstruct the canonical neural volume of the source image, and further
propose a compensation network to complement facial details. To enable
fine-grained control over facial dynamics, we propose a deformation field to
warp the canonical volume into driven expressions. Through extensive
experimental comparisons, we achieve superior synthesis results compared to
several state-of-the-art methods.";Wangbo Yu<author:sep>Yanbo Fan<author:sep>Yong Zhang<author:sep>Xuan Wang<author:sep>Fei Yin<author:sep>Yunpeng Bai<author:sep>Yan-Pei Cao<author:sep>Ying Shan<author:sep>Yang Wu<author:sep>Zhongqian Sun<author:sep>Baoyuan Wu;http://arxiv.org/pdf/2307.03441v1;cs.CV;;nerf
2307.03404v2;http://arxiv.org/abs/2307.03404v2;2023-07-07;RGB-D Mapping and Tracking in a Plenoxel Radiance Field;"The widespread adoption of Neural Radiance Fields (NeRFs) have ensured
significant advances in the domain of novel view synthesis in recent years.
These models capture a volumetric radiance field of a scene, creating highly
convincing, dense, photorealistic models through the use of simple,
differentiable rendering equations. Despite their popularity, these algorithms
suffer from severe ambiguities in visual data inherent to the RGB sensor, which
means that although images generated with view synthesis can visually appear
very believable, the underlying 3D model will often be wrong. This considerably
limits the usefulness of these models in practical applications like Robotics
and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise
would be of significant value. In this paper, we present the vital differences
between view synthesis models and 3D reconstruction models. We also comment on
why a depth sensor is essential for modeling accurate geometry in general
outward-facing scenes using the current paradigm of novel view synthesis
methods. Focusing on the structure-from-motion task, we practically demonstrate
this need by extending the Plenoxel radiance field model: Presenting an
analytical differential approach for dense mapping and tracking with radiance
fields based on RGB-D data without a neural network. Our method achieves
state-of-the-art results in both mapping and tracking tasks, while also being
faster than competing neural network-based approaches. The code is available
at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git";Andreas L. Teigen<author:sep>Yeonsoo Park<author:sep>Annette Stahl<author:sep>Rudolf Mester;http://arxiv.org/pdf/2307.03404v2;cs.CV;"2024 IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV) *The first two authors contributed equally to this paper";nerf
2306.17843v2;http://arxiv.org/abs/2306.17843v2;2023-06-30;Magic123: One Image to High-Quality 3D Object Generation Using Both 2D  and 3D Diffusion Priors;"We present Magic123, a two-stage coarse-to-fine approach for high-quality,
textured 3D meshes generation from a single unposed image in the wild using
both2D and 3D priors. In the first stage, we optimize a neural radiance field
to produce a coarse geometry. In the second stage, we adopt a memory-efficient
differentiable mesh representation to yield a high-resolution mesh with a
visually appealing texture. In both stages, the 3D content is learned through
reference view supervision and novel views guided by a combination of 2D and 3D
diffusion priors. We introduce a single trade-off parameter between the 2D and
3D priors to control exploration (more imaginative) and exploitation (more
precise) of the generated geometry. Additionally, we employ textual inversion
and monocular depth regularization to encourage consistent appearances across
views and to prevent degenerate solutions, respectively. Magic123 demonstrates
a significant improvement over previous image-to-3D techniques, as validated
through extensive experiments on synthetic benchmarks and diverse real-world
images. Our code, models, and generated 3D assets are available at
https://github.com/guochengqian/Magic123.";Guocheng Qian<author:sep>Jinjie Mai<author:sep>Abdullah Hamdi<author:sep>Jian Ren<author:sep>Aliaksandr Siarohin<author:sep>Bing Li<author:sep>Hsin-Ying Lee<author:sep>Ivan Skorokhodov<author:sep>Peter Wonka<author:sep>Sergey Tulyakov<author:sep>Bernard Ghanem;http://arxiv.org/pdf/2306.17843v2;cs.CV;webpage: https://guochengqian.github.io/project/magic123/;
2306.17624v2;http://arxiv.org/abs/2306.17624v2;2023-06-30;Sphere2Vec: A General-Purpose Location Representation Learning over a  Spherical Surface for Large-Scale Geospatial Predictions;"Generating learning-friendly representations for points in space is a
fundamental and long-standing problem in ML. Recently, multi-scale encoding
schemes (such as Space2Vec and NeRF) were proposed to directly encode any point
in 2D/3D Euclidean space as a high-dimensional vector, and has been
successfully applied to various geospatial prediction and generative tasks.
However, all current 2D and 3D location encoders are designed to model point
distances in Euclidean space. So when applied to large-scale real-world GPS
coordinate datasets, which require distance metric learning on the spherical
surface, both types of models can fail due to the map projection distortion
problem (2D) and the spherical-to-Euclidean distance approximation error (3D).
To solve these problems, we propose a multi-scale location encoder called
Sphere2Vec which can preserve spherical distances when encoding point
coordinates on a spherical surface. We developed a unified view of
distance-reserving encoding on spheres based on the DFS. We also provide
theoretical proof that the Sphere2Vec preserves the spherical surface distance
between any two points, while existing encoding schemes do not. Experiments on
20 synthetic datasets show that Sphere2Vec can outperform all baseline models
on all these datasets with up to 30.8% error rate reduction. We then apply
Sphere2Vec to three geo-aware image classification tasks - fine-grained species
recognition, Flickr image recognition, and remote sensing image classification.
Results on 7 real-world datasets show the superiority of Sphere2Vec over
multiple location encoders on all three tasks. Further analysis shows that
Sphere2Vec outperforms other location encoder models, especially in the polar
regions and data-sparse areas because of its nature for spherical surface
distance preservation. Code and data are available at
https://gengchenmai.github.io/sphere2vec-website/.";Gengchen Mai<author:sep>Yao Xuan<author:sep>Wenyun Zuo<author:sep>Yutong He<author:sep>Jiaming Song<author:sep>Stefano Ermon<author:sep>Krzysztof Janowicz<author:sep>Ni Lao;http://arxiv.org/pdf/2306.17624v2;cs.CV;"30 Pages, 16 figures. Accepted to ISPRS Journal of Photogrammetry and
  Remote Sensing";nerf
2306.17723v4;http://arxiv.org/abs/2306.17723v4;2023-06-30;FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis;"Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis
with its remarkable quality of rendered images and simple architecture.
Although NeRF has been developed in various directions improving continuously
its performance, the necessity of a dense set of multi-view images still exists
as a stumbling block to progress for practical application. In this work, we
propose FlipNeRF, a novel regularization method for few-shot novel view
synthesis by utilizing our proposed flipped reflection rays. The flipped
reflection rays are explicitly derived from the input ray directions and
estimated normal vectors, and play a role of effective additional training rays
while enabling to estimate more accurate surface normals and learn the 3D
geometry effectively. Since the surface normal and the scene depth are both
derived from the estimated densities along a ray, the accurate surface normal
leads to more exact depth estimation, which is a key factor for few-shot novel
view synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Loss
and Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate more
reliable outputs with reducing floating artifacts effectively across the
different scene structures, and enhance the feature-level consistency between
the pair of the rays cast toward the photo-consistent pixels without any
additional feature extractor, respectively. Our FlipNeRF achieves the SOTA
performance on the multiple benchmarks across all the scenarios.";Seunghyeon Seo<author:sep>Yeonjin Chang<author:sep>Nojun Kwak;http://arxiv.org/pdf/2306.17723v4;cs.CV;ICCV 2023. Project Page: https://shawn615.github.io/flipnerf/;nerf
2306.16928v1;http://arxiv.org/abs/2306.16928v1;2023-06-29;One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape  Optimization;"Single image 3D reconstruction is an important but challenging task that
requires extensive knowledge of our natural world. Many existing methods solve
this problem by optimizing a neural radiance field under the guidance of 2D
diffusion models but suffer from lengthy optimization time, 3D inconsistency
results, and poor geometry. In this work, we propose a novel method that takes
a single image of any object as input and generates a full 360-degree 3D
textured mesh in a single feed-forward pass. Given a single image, we first use
a view-conditioned 2D diffusion model, Zero123, to generate multi-view images
for the input view, and then aim to lift them up to 3D space. Since traditional
reconstruction methods struggle with inconsistent multi-view predictions, we
build our 3D reconstruction module upon an SDF-based generalizable neural
surface reconstruction method and propose several critical training strategies
to enable the reconstruction of 360-degree meshes. Without costly
optimizations, our method reconstructs 3D shapes in significantly less time
than existing methods. Moreover, our method favors better geometry, generates
more 3D consistent results, and adheres more closely to the input image. We
evaluate our approach on both synthetic data and in-the-wild images and
demonstrate its superiority in terms of both mesh quality and runtime. In
addition, our approach can seamlessly support the text-to-3D task by
integrating with off-the-shelf text-to-image diffusion models.";Minghua Liu<author:sep>Chao Xu<author:sep>Haian Jin<author:sep>Linghao Chen<author:sep>Mukund Varma T<author:sep>Zexiang Xu<author:sep>Hao Su;http://arxiv.org/pdf/2306.16928v1;cs.CV;project website: one-2-3-45.com;
2306.16541v1;http://arxiv.org/abs/2306.16541v1;2023-06-28;Envisioning a Next Generation Extended Reality Conferencing System with  Efficient Photorealistic Human Rendering;"Meeting online is becoming the new normal. Creating an immersive experience
for online meetings is a necessity towards more diverse and seamless
environments. Efficient photorealistic rendering of human 3D dynamics is the
core of immersive meetings. Current popular applications achieve real-time
conferencing but fall short in delivering photorealistic human dynamics, either
due to limited 2D space or the use of avatars that lack realistic interactions
between participants. Recent advances in neural rendering, such as the Neural
Radiance Field (NeRF), offer the potential for greater realism in metaverse
meetings. However, the slow rendering speed of NeRF poses challenges for
real-time conferencing. We envision a pipeline for a future extended reality
metaverse conferencing system that leverages monocular video acquisition and
free-viewpoint synthesis to enhance data and hardware efficiency. Towards an
immersive conferencing experience, we explore an accelerated NeRF-based
free-viewpoint synthesis algorithm for rendering photorealistic human dynamics
more efficiently. We show that our algorithm achieves comparable rendering
quality while performing training and inference 44.5% and 213% faster than
state-of-the-art methods, respectively. Our exploration provides a design basis
for constructing metaverse conferencing systems that can handle complex
application scenarios, including dynamic scene relighting with customized
themes and multi-user conferencing that harmonizes real-world people into an
extended world.";Chuanyue Shen<author:sep>Letian Zhang<author:sep>Zhangsihao Yang<author:sep>Masood Mortazavi<author:sep>Xiyun Song<author:sep>Liang Peng<author:sep>Heather Yu;http://arxiv.org/pdf/2306.16541v1;cs.CV;Accepted to CVPR 2023 ECV Workshop;nerf
2306.15703v1;http://arxiv.org/abs/2306.15703v1;2023-06-27;Toward a Spectral Foundation Model: An Attention-Based Approach with  Domain-Inspired Fine-Tuning and Wavelength Parameterization;"Astrophysical explorations are underpinned by large-scale stellar
spectroscopy surveys, necessitating a paradigm shift in spectral fitting
techniques. Our study proposes three enhancements to transcend the limitations
of the current spectral emulation models. We implement an attention-based
emulator, adept at unveiling long-range information between wavelength pixels.
We leverage a domain-specific fine-tuning strategy where the model is
pre-trained on spectra with fixed stellar parameters and variable elemental
abundances, followed by fine-tuning on the entire domain. Moreover, by treating
wavelength as an autonomous model parameter, akin to neural radiance fields,
the model can generate spectra on any wavelength grid. In the case with a
training set of O(1000), our approach exceeds current leading methods by a
factor of 5-10 across all metrics.";Tomasz Różański<author:sep>Yuan-Sen Ting<author:sep>Maja Jabłońska;http://arxiv.org/pdf/2306.15703v1;astro-ph.IM;"7 pages, 3 figures, accepted to ICML 2023 Workshop on Machine
  Learning for Astrophysics";
2306.15203v2;http://arxiv.org/abs/2306.15203v2;2023-06-27;Unsupervised Polychromatic Neural Representation for CT Metal Artifact  Reduction;"Emerging neural reconstruction techniques based on tomography (e.g., NeRF,
NeAT, and NeRP) have started showing unique capabilities in medical imaging. In
this work, we present a novel Polychromatic neural representation (Polyner) to
tackle the challenging problem of CT imaging when metallic implants exist
within the human body. CT metal artifacts arise from the drastic variation of
metal's attenuation coefficients at various energy levels of the X-ray
spectrum, leading to a nonlinear metal effect in CT measurements. Recovering CT
images from metal-affected measurements hence poses a complicated nonlinear
inverse problem where empirical models adopted in previous metal artifact
reduction (MAR) approaches lead to signal loss and strongly aliased
reconstructions. Polyner instead models the MAR problem from a nonlinear
inverse problem perspective. Specifically, we first derive a polychromatic
forward model to accurately simulate the nonlinear CT acquisition process.
Then, we incorporate our forward model into the implicit neural representation
to accomplish reconstruction. Lastly, we adopt a regularizer to preserve the
physical properties of the CT images across different energy levels while
effectively constraining the solution space. Our Polyner is an unsupervised
method and does not require any external training data. Experimenting with
multiple datasets shows that our Polyner achieves comparable or better
performance than supervised methods on in-domain datasets while demonstrating
significant performance improvements on out-of-domain datasets. To the best of
our knowledge, our Polyner is the first unsupervised MAR method that
outperforms its supervised counterparts. The code for this work is available
at: https://github.com/iwuqing/Polyner.";Qing Wu<author:sep>Lixuan Chen<author:sep>Ce Wang<author:sep>Hongjiang Wei<author:sep>S. Kevin Zhou<author:sep>Jingyi Yu<author:sep>Yuyao Zhang;http://arxiv.org/pdf/2306.15203v2;eess.IV;Accepted by NeurIPS 2023;nerf
2306.14490v1;http://arxiv.org/abs/2306.14490v1;2023-06-26;TaiChi Action Capture and Performance Analysis with Multi-view RGB  Cameras;"Recent advances in computer vision and deep learning have influenced the
field of sports performance analysis for researchers to track and reconstruct
freely moving humans without any marker attachment. However, there are few
works for vision-based motion capture and intelligent analysis for professional
TaiChi movement. In this paper, we propose a framework for TaiChi performance
capture and analysis with multi-view geometry and artificial intelligence
technology. The main innovative work is as follows: 1) A multi-camera system
suitable for TaiChi motion capture is built and the multi-view TaiChi data is
collected and processed; 2) A combination of traditional visual method and
implicit neural radiance field is proposed to achieve sparse 3D skeleton fusion
and dense 3D surface reconstruction. 3) The normalization modeling of movement
sequences is carried out based on motion transfer, so as to realize TaiChi
performance analysis for different groups. We have carried out evaluation
experiments, and the experimental results have shown the efficiency of our
method.";Jianwei Li<author:sep>Siyu Mo<author:sep>Yanfei Shen;http://arxiv.org/pdf/2306.14490v1;cs.CV;;
2306.12760v2;http://arxiv.org/abs/2306.12760v2;2023-06-22;Blended-NeRF: Zero-Shot Object Generation and Blending in Existing  Neural Radiance Fields;"Editing a local region or a specific object in a 3D scene represented by a
NeRF or consistently blending a new realistic object into the scene is
challenging, mainly due to the implicit nature of the scene representation. We
present Blended-NeRF, a robust and flexible framework for editing a specific
region of interest in an existing NeRF scene, based on text prompts, along with
a 3D ROI box. Our method leverages a pretrained language-image model to steer
the synthesis towards a user-provided text prompt, along with a 3D MLP model
initialized on an existing NeRF scene to generate the object and blend it into
a specified region in the original scene. We allow local editing by localizing
a 3D ROI box in the input scene, and blend the content synthesized inside the
ROI with the existing scene using a novel volumetric blending technique. To
obtain natural looking and view-consistent results, we leverage existing and
new geometric priors and 3D augmentations for improving the visual fidelity of
the final result. We test our framework both qualitatively and quantitatively
on a variety of real 3D scenes and text prompts, demonstrating realistic
multi-view consistent results with much flexibility and diversity compared to
the baselines. Finally, we show the applicability of our framework for several
3D editing applications, including adding new objects to a scene,
removing/replacing/altering existing objects, and texture conversion.";Ori Gordon<author:sep>Omri Avrahami<author:sep>Dani Lischinski;http://arxiv.org/pdf/2306.12760v2;cs.CV;"16 pages, 14 figures. Project page:
  https://www.vision.huji.ac.il/blended-nerf/";nerf
2306.12570v1;http://arxiv.org/abs/2306.12570v1;2023-06-21;Local 3D Editing via 3D Distillation of CLIP Knowledge;"3D content manipulation is an important computer vision task with many
real-world applications (e.g., product design, cartoon generation, and 3D
Avatar editing). Recently proposed 3D GANs can generate diverse photorealistic
3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of
NeRF still remains a challenging problem since the visual quality tends to
degrade after manipulation and suboptimal control handles such as 2D semantic
maps are used for manipulations. While text-guided manipulations have shown
potential in 3D editing, such approaches often lack locality. To overcome these
problems, we propose Local Editing NeRF (LENeRF), which only requires text
inputs for fine-grained and localized manipulation. Specifically, we present
three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field
Network, and the Deformation Network, which are jointly used for local
manipulations of 3D features by estimating a 3D attention field. The 3D
attention field is learned in an unsupervised way, by distilling the zero-shot
mask generation capability of CLIP to the 3D space with multi-view guidance. We
conduct diverse experiments and thorough evaluations both quantitatively and
qualitatively.";Junha Hyung<author:sep>Sungwon Hwang<author:sep>Daejin Kim<author:sep>Hyunji Lee<author:sep>Jaegul Choo;http://arxiv.org/pdf/2306.12570v1;cs.CV;conference: CVPR 2023;nerf
2306.12423v1;http://arxiv.org/abs/2306.12423v1;2023-06-21;Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized  Codebase;"Despite the rapid advance of 3D-aware image synthesis, existing studies
usually adopt a mixture of techniques and tricks, leaving it unclear how each
part contributes to the final performance in terms of generality. Following the
most popular and effective paradigm in this field, which incorporates a neural
radiance field (NeRF) into the generator of a generative adversarial network
(GAN), we build a well-structured codebase, dubbed Carver, through modularizing
the generation process. Such a design allows researchers to develop and replace
each module independently, and hence offers an opportunity to fairly compare
various approaches and recognize their contributions from the module
perspective. The reproduction of a range of cutting-edge algorithms
demonstrates the availability of our modularized codebase. We also perform a
variety of in-depth analyses, such as the comparison across different types of
point feature, the necessity of the tailing upsampler in the generator, the
reliance on the camera pose prior, etc., which deepen our understanding of
existing methods and point out some further directions of the research work. We
release code and models at https://github.com/qiuyu96/Carver to facilitate the
development and evaluation of this field.";Qiuyu Wang<author:sep>Zifan Shi<author:sep>Kecheng Zheng<author:sep>Yinghao Xu<author:sep>Sida Peng<author:sep>Yujun Shen;http://arxiv.org/pdf/2306.12423v1;cs.CV;Code: https://github.com/qiuyu96/Carver;nerf
2306.12422v1;http://arxiv.org/abs/2306.12422v1;2023-06-21;DreamTime: An Improved Optimization Strategy for Text-to-3D Content  Creation;"Text-to-image diffusion models pre-trained on billions of image-text pairs
have recently enabled text-to-3D content creation by optimizing a randomly
initialized Neural Radiance Fields (NeRF) with score distillation. However, the
resultant 3D models exhibit two limitations: (a) quality concerns such as
saturated color and the Janus problem; (b) extremely low diversity comparing to
text-guided image synthesis. In this paper, we show that the conflict between
NeRF optimization process and uniform timestep sampling in score distillation
is the main reason for these limitations. To resolve this conflict, we propose
to prioritize timestep sampling with monotonically non-increasing functions,
which aligns NeRF optimization with the sampling process of diffusion model.
Extensive experiments show that our simple redesign significantly improves
text-to-3D content creation with higher quality and diversity.";Yukun Huang<author:sep>Jianan Wang<author:sep>Yukai Shi<author:sep>Xianbiao Qi<author:sep>Zheng-Jun Zha<author:sep>Lei Zhang;http://arxiv.org/pdf/2306.12422v1;cs.CV;;nerf
2306.11556v1;http://arxiv.org/abs/2306.11556v1;2023-06-20;NeRF synthesis with shading guidance;"The emerging Neural Radiance Field (NeRF) shows great potential in
representing 3D scenes, which can render photo-realistic images from novel view
with only sparse views given. However, utilizing NeRF to reconstruct real-world
scenes requires images from different viewpoints, which limits its practical
application. This problem can be even more pronounced for large scenes. In this
paper, we introduce a new task called NeRF synthesis that utilizes the
structural content of a NeRF patch exemplar to construct a new radiance field
of large size. We propose a two-phase method for synthesizing new scenes that
are continuous in geometry and appearance. We also propose a boundary
constraint method to synthesize scenes of arbitrary size without artifacts.
Specifically, we control the lighting effects of synthesized scenes using
shading guidance instead of decoupling the scene. We have demonstrated that our
method can generate high-quality results with consistent geometry and
appearance, even for scenes with complex lighting. We can also synthesize new
scenes on curved surface with arbitrary lighting effects, which enhances the
practicality of our proposed NeRF synthesis approach.";Chenbin Li<author:sep>Yu Xin<author:sep>Gaoyi Liu<author:sep>Xiang Zeng<author:sep>Ligang Liu;http://arxiv.org/pdf/2306.11556v1;cs.CV;16 pages, 16 figures, accepted by CAD/Graphics 2023(poster);nerf
2306.10350v2;http://arxiv.org/abs/2306.10350v2;2023-06-17;MA-NeRF: Motion-Assisted Neural Radiance Fields for Face Synthesis from  Sparse Images;"We address the problem of photorealistic 3D face avatar synthesis from sparse
images. Existing Parametric models for face avatar reconstruction struggle to
generate details that originate from inputs. Meanwhile, although current
NeRF-based avatar methods provide promising results for novel view synthesis,
they fail to generalize well for unseen expressions. We improve from NeRF and
propose a novel framework that, by leveraging the parametric 3DMM models, can
reconstruct a high-fidelity drivable face avatar and successfully handle the
unseen expressions. At the core of our implementation are structured
displacement feature and semantic-aware learning module. Our structured
displacement feature will introduce the motion prior as an additional
constraints and help perform better for unseen expressions, by constructing
displacement volume. Besides, the semantic-aware learning incorporates
multi-level prior, e.g., semantic embedding, learnable latent code, to lift the
performance to a higher level. Thorough experiments have been doen both
quantitatively and qualitatively to demonstrate the design of our framework,
and our method achieves much better results than the current state-of-the-arts.";Weichen Zhang<author:sep>Xiang Zhou<author:sep>Yukang Cao<author:sep>Wensen Feng<author:sep>Chun Yuan;http://arxiv.org/pdf/2306.10350v2;cs.CV;;nerf
2306.09551v1;http://arxiv.org/abs/2306.09551v1;2023-06-15;Edit-DiffNeRF: Editing 3D Neural Radiance Fields using 2D Diffusion  Model;"Recent research has demonstrated that the combination of pretrained diffusion
models with neural radiance fields (NeRFs) has emerged as a promising approach
for text-to-3D generation. Simply coupling NeRF with diffusion models will
result in cross-view inconsistency and degradation of stylized view syntheses.
To address this challenge, we propose the Edit-DiffNeRF framework, which is
composed of a frozen diffusion model, a proposed delta module to edit the
latent semantic space of the diffusion model, and a NeRF. Instead of training
the entire diffusion for each scene, our method focuses on editing the latent
semantic space in frozen pretrained diffusion models by the delta module. This
fundamental change to the standard diffusion framework enables us to make
fine-grained modifications to the rendered views and effectively consolidate
these instructions in a 3D scene via NeRF training. As a result, we are able to
produce an edited 3D scene that faithfully aligns to input text instructions.
Furthermore, to ensure semantic consistency across different viewpoints, we
propose a novel multi-view semantic consistency loss that extracts a latent
semantic embedding from the input view as a prior, and aim to reconstruct it in
different views. Our proposed method has been shown to effectively edit
real-world 3D scenes, resulting in 25% improvement in the alignment of the
performed 3D edits with text instructions compared to prior work.";Lu Yu<author:sep>Wei Xiang<author:sep>Kang Han;http://arxiv.org/pdf/2306.09551v1;cs.CV;;nerf
2306.09329v1;http://arxiv.org/abs/2306.09329v1;2023-06-15;DreamHuman: Animatable 3D Avatars from Text;"We present DreamHuman, a method to generate realistic animatable 3D human
avatar models solely from textual descriptions. Recent text-to-3D methods have
made considerable strides in generation, but are still lacking in important
aspects. Control and often spatial resolution remain limited, existing methods
produce fixed rather than animated 3D human models, and anthropometric
consistency for complex structures like people remains a challenge. DreamHuman
connects large text-to-image synthesis models, neural radiance fields, and
statistical human body models in a novel modeling and optimization framework.
This makes it possible to generate dynamic 3D human avatars with high-quality
textures and learned, instance-specific, surface deformations. We demonstrate
that our method is capable to generate a wide variety of animatable, realistic
3D human models from text. Our 3D models have diverse appearance, clothing,
skin tones and body shapes, and significantly outperform both generic
text-to-3D approaches and previous text-based 3D avatar generators in visual
fidelity. For more results and animations please check our website at
https://dream-human.github.io.";Nikos Kolotouros<author:sep>Thiemo Alldieck<author:sep>Andrei Zanfir<author:sep>Eduard Gabriel Bazavan<author:sep>Mihai Fieraru<author:sep>Cristian Sminchisescu;http://arxiv.org/pdf/2306.09329v1;cs.CV;Project website at https://dream-human.github.io/;
2306.09349v2;http://arxiv.org/abs/2306.09349v2;2023-06-15;UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video;"We show how to build a model that allows realistic, free-viewpoint renderings
of a scene under novel lighting conditions from video. Our method -- UrbanIR:
Urban Scene Inverse Rendering -- computes an inverse graphics representation
from the video. UrbanIR jointly infers shape, albedo, visibility, and sun and
sky illumination from a single video of unbounded outdoor scenes with unknown
lighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many
views of the same points in typical NeRF-style estimation). As a result,
standard methods produce poor geometry estimates (for example, roofs), and
there are numerous ''floaters''. Errors in inverse graphics inference can
result in strong rendering artifacts. UrbanIR uses novel losses to control
these and other sources of error. UrbanIR uses a novel loss to make very good
estimates of shadow volumes in the original scene. The resulting
representations facilitate controllable editing, delivering photorealistic
free-viewpoint renderings of relit scenes and inserted objects. Qualitative
evaluation demonstrates strong improvements over the state-of-the-art.";Zhi-Hao Lin<author:sep>Bohan Liu<author:sep>Yi-Ting Chen<author:sep>David Forsyth<author:sep>Jia-Bin Huang<author:sep>Anand Bhattad<author:sep>Shenlong Wang;http://arxiv.org/pdf/2306.09349v2;cs.CV;https://urbaninverserendering.github.io/;nerf
2306.07579v1;http://arxiv.org/abs/2306.07579v1;2023-06-13;Parametric Implicit Face Representation for Audio-Driven Facial  Reenactment;"Audio-driven facial reenactment is a crucial technique that has a range of
applications in film-making, virtual avatars and video conferences. Existing
works either employ explicit intermediate face representations (e.g., 2D facial
landmarks or 3D face models) or implicit ones (e.g., Neural Radiance Fields),
thus suffering from the trade-offs between interpretability and expressive
power, hence between controllability and quality of the results. In this work,
we break these trade-offs with our novel parametric implicit face
representation and propose a novel audio-driven facial reenactment framework
that is both controllable and can generate high-quality talking heads.
Specifically, our parametric implicit representation parameterizes the implicit
representation with interpretable parameters of 3D face models, thereby taking
the best of both explicit and implicit methods. In addition, we propose several
new techniques to improve the three components of our framework, including i)
incorporating contextual information into the audio-to-expression parameters
encoding; ii) using conditional image synthesis to parameterize the implicit
representation and implementing it with an innovative tri-plane structure for
efficient learning; iii) formulating facial reenactment as a conditional image
inpainting problem and proposing a novel data augmentation technique to improve
model generalizability. Extensive experiments demonstrate that our method can
generate more realistic results than previous methods with greater fidelity to
the identities and talking styles of speakers.";Ricong Huang<author:sep>Peiwen Lai<author:sep>Yipeng Qin<author:sep>Guanbin Li;http://arxiv.org/pdf/2306.07579v1;cs.CV;CVPR 2023;
2306.07581v2;http://arxiv.org/abs/2306.07581v2;2023-06-13;Binary Radiance Fields;"In this paper, we propose \textit{binary radiance fields} (BiRF), a
storage-efficient radiance field representation employing binary feature
encoding that encodes local features using binary encoding parameters in a
format of either $+1$ or $-1$. This binarization strategy lets us represent the
feature grid with highly compact feature encoding and a dramatic reduction in
storage size. Furthermore, our 2D-3D hybrid feature grid design enhances the
compactness of feature encoding as the 3D grid includes main components while
2D grids capture details. In our experiments, binary radiance field
representation successfully outperforms the reconstruction performance of
state-of-the-art (SOTA) efficient radiance field models with lower storage
allocation. In particular, our model achieves impressive results in static
scene reconstruction, with a PSNR of 32.03 dB for Synthetic-NeRF scenes, 34.48
dB for Synthetic-NSVF scenes, 28.20 dB for Tanks and Temples scenes while only
utilizing 0.5 MB of storage space, respectively. We hope the proposed binary
radiance field representation will make radiance fields more accessible without
a storage bottleneck.";Seungjoo Shin<author:sep>Jaesik Park;http://arxiv.org/pdf/2306.07581v2;cs.CV;"Accepted to NeurIPS 2023. Project page:
  https://seungjooshin.github.io/BiRF";nerf
2306.08068v2;http://arxiv.org/abs/2306.08068v2;2023-06-13;DORSal: Diffusion for Object-centric Representations of Scenes et al;"Recent progress in 3D scene understanding enables scalable learning of
representations across large datasets of diverse scenes. As a consequence,
generalization to unseen scenes and objects, rendering novel views from just a
single or a handful of input images, and controllable scene generation that
supports editing, is now possible. However, training jointly on a large number
of scenes typically compromises rendering quality when compared to single-scene
optimized models such as NeRFs. In this paper, we leverage recent progress in
diffusion models to equip 3D scene representation learning models with the
ability to render high-fidelity novel views, while retaining benefits such as
object-level scene editing to a large degree. In particular, we propose DORSal,
which adapts a video diffusion architecture for 3D scene generation conditioned
on frozen object-centric slot-based representations of scenes. On both complex
synthetic multi-object scenes and on the real-world large-scale Street View
dataset, we show that DORSal enables scalable neural rendering of 3D scenes
with object-level editing and improves upon existing approaches.";Allan Jabri<author:sep>Sjoerd van Steenkiste<author:sep>Emiel Hoogeboom<author:sep>Mehdi S. M. Sajjadi<author:sep>Thomas Kipf;http://arxiv.org/pdf/2306.08068v2;cs.CV;Project page: https://www.sjoerdvansteenkiste.com/dorsal;nerf
2306.06388v3;http://arxiv.org/abs/2306.06388v3;2023-06-10;From NeRFLiX to NeRFLiX++: A General NeRF-Agnostic Restorer Paradigm;"Neural radiance fields (NeRF) have shown great success in novel view
synthesis. However, recovering high-quality details from real-world scenes is
still challenging for the existing NeRF-based approaches, due to the potential
imperfect calibration information and scene representation inaccuracy. Even
with high-quality training frames, the synthetic novel views produced by NeRF
models still suffer from notable rendering artifacts, such as noise and blur.
To address this, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm
that learns a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for deep neural networks. Moreover, beyond the degradation removal,
we propose an inter-viewpoint aggregation framework that fuses highly related
high-quality training images, pushing the performance of cutting-edge NeRF
models to entirely new levels and producing highly photo-realistic synthetic
views. Based on this paradigm, we further present NeRFLiX++ with a stronger
two-stage NeRF degradation simulator and a faster inter-viewpoint mixer,
achieving superior performance with significantly improved computational
efficiency. Notably, NeRFLiX++ is capable of restoring photo-realistic
ultra-high-resolution outputs from noisy low-resolution NeRF-rendered views.
Extensive experiments demonstrate the excellent restoration ability of
NeRFLiX++ on various novel view synthesis benchmarks.";Kun Zhou<author:sep>Wenbo Li<author:sep>Nianjuan Jiang<author:sep>Xiaoguang Han<author:sep>Jiangbo Lu;http://arxiv.org/pdf/2306.06388v3;cs.CV;"17 pages, 17 figures. To appear in TPAMI2023. Project Page:
  https://redrock303.github.io/nerflix_plus/. arXiv admin note: text overlap
  with arXiv:2303.06919";nerf
2306.06359v1;http://arxiv.org/abs/2306.06359v1;2023-06-10;NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance  Fields against Adversarial Perturbations;"Generalizable Neural Radiance Fields (GNeRF) are one of the most promising
real-world solutions for novel view synthesis, thanks to their cross-scene
generalization capability and thus the possibility of instant rendering on new
scenes. While adversarial robustness is essential for real-world applications,
little study has been devoted to understanding its implication on GNeRF. We
hypothesize that because GNeRF is implemented by conditioning on the source
views from new scenes, which are often acquired from the Internet or
third-party providers, there are potential new security concerns regarding its
real-world applications. Meanwhile, existing understanding and solutions for
neural networks' adversarial robustness may not be applicable to GNeRF, due to
its 3D nature and uniquely diverse operations. To this end, we present NeRFool,
which to the best of our knowledge is the first work that sets out to
understand the adversarial robustness of GNeRF. Specifically, NeRFool unveils
the vulnerability patterns and important insights regarding GNeRF's adversarial
robustness. Built upon the above insights gained from NeRFool, we further
develop NeRFool+, which integrates two techniques capable of effectively
attacking GNeRF across a wide range of target views, and provide guidelines for
defending against our proposed attacks. We believe that our NeRFool/NeRFool+
lays the initial foundation for future innovations in developing robust
real-world GNeRF solutions. Our codes are available at:
https://github.com/GATECH-EIC/NeRFool.";Yonggan Fu<author:sep>Ye Yuan<author:sep>Souvik Kundu<author:sep>Shang Wu<author:sep>Shunyao Zhang<author:sep>Yingyan Lin;http://arxiv.org/pdf/2306.06359v1;cs.CV;Accepted by ICML 2023;nerf
2306.06044v2;http://arxiv.org/abs/2306.06044v2;2023-06-09;GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields;"Neural Radiance Fields (NeRF) have shown impressive novel view synthesis
results; nonetheless, even thorough recordings yield imperfections in
reconstructions, for instance due to poorly observed areas or minor lighting
changes. Our goal is to mitigate these imperfections from various sources with
a joint solution: we take advantage of the ability of generative adversarial
networks (GANs) to produce realistic images and use them to enhance realism in
3D scene reconstruction with NeRFs. To this end, we learn the patch
distribution of a scene using an adversarial discriminator, which provides
feedback to the radiance field reconstruction, thus improving realism in a
3D-consistent fashion. Thereby, rendering artifacts are repaired directly in
the underlying 3D representation by imposing multi-view path rendering
constraints. In addition, we condition a generator with multi-resolution NeRF
renderings which is adversarially trained to further improve rendering quality.
We demonstrate that our approach significantly improves rendering quality,
e.g., nearly halving LPIPS scores compared to Nerfacto while at the same time
improving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples.";Barbara Roessle<author:sep>Norman Müller<author:sep>Lorenzo Porzi<author:sep>Samuel Rota Bulò<author:sep>Peter Kontschieder<author:sep>Matthias Nießner;http://arxiv.org/pdf/2306.06044v2;cs.CV;"SIGGRAPH Asia 2023, project page:
  https://barbararoessle.github.io/ganerf , video: https://youtu.be/352ccXWxQVE";nerf
2306.06093v3;http://arxiv.org/abs/2306.06093v3;2023-06-09;HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork;"Neural Radiance Fields (NeRF) have become an increasingly popular
representation to capture high-quality appearance and shape of scenes and
objects. However, learning generalizable NeRF priors over categories of scenes
or objects has been challenging due to the high dimensionality of network
weight space. To address the limitations of existing work on generalization,
multi-view consistency and to improve quality, we propose HyP-NeRF, a latent
conditioning method for learning generalizable category-level NeRF priors using
hypernetworks. Rather than using hypernetworks to estimate only the weights of
a NeRF, we estimate both the weights and the multi-resolution hash encodings
resulting in significant quality gains. To improve quality even further, we
incorporate a denoise and finetune strategy that denoises images rendered from
NeRFs estimated by the hypernetwork and finetunes it while retaining multiview
consistency. These improvements enable us to use HyP-NeRF as a generalizable
prior for multiple downstream tasks including NeRF reconstruction from
single-view or cluttered scenes and text-to-NeRF. We provide qualitative
comparisons and evaluate HyP-NeRF on three tasks: generalization, compression,
and retrieval, demonstrating our state-of-the-art results.";Bipasha Sen<author:sep>Gaurav Singh<author:sep>Aditya Agarwal<author:sep>Rohith Agaram<author:sep>K Madhava Krishna<author:sep>Srinath Sridhar;http://arxiv.org/pdf/2306.06093v3;cs.CV;Project Page: https://hyp-nerf.github.io;nerf
2306.06300v2;http://arxiv.org/abs/2306.06300v2;2023-06-09;NERFBK: A High-Quality Benchmark for NERF-Based 3D Reconstruction;"This paper introduces a new real and synthetic dataset called NeRFBK
specifically designed for testing and comparing NeRF-based 3D reconstruction
algorithms. High-quality 3D reconstruction has significant potential in various
fields, and advancements in image-based algorithms make it essential to
evaluate new advanced techniques. However, gathering diverse data with precise
ground truth is challenging and may not encompass all relevant applications.
The NeRFBK dataset addresses this issue by providing multi-scale, indoor and
outdoor datasets with high-resolution images and videos and camera parameters
for testing and comparing NeRF-based algorithms. This paper presents the design
and creation of the NeRFBK benchmark, various examples and application
scenarios, and highlights its potential for advancing the field of 3D
reconstruction.";Ali Karami<author:sep>Simone Rigon<author:sep>Gabriele Mazzacca<author:sep>Ziyang Yan<author:sep>Fabio Remondino;http://arxiv.org/pdf/2306.06300v2;cs.CV;paper result has problem;nerf
2306.05668v2;http://arxiv.org/abs/2306.05668v2;2023-06-09;RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models;"The emergence of Neural Radiance Fields (NeRF) has promoted the development
of synthesized high-fidelity views of the intricate real world. However, it is
still a very demanding task to repaint the content in NeRF. In this paper, we
propose a novel framework that can take RGB images as input and alter the 3D
content in neural scenes. Our work leverages existing diffusion models to guide
changes in the designated 3D content. Specifically, we semantically select the
target object and a pre-trained diffusion model will guide the NeRF model to
generate new 3D objects, which can improve the editability, diversity, and
application range of NeRF. Experiment results show that our algorithm is
effective for editing 3D objects in NeRF under different text prompts,
including editing appearance, shape, and more. We validate our method on both
real-world datasets and synthetic-world datasets for these editing tasks.
Please visit https://starstesla.github.io/repaintnerf for a better view of our
results.";Xingchen Zhou<author:sep>Ying He<author:sep>F. Richard Yu<author:sep>Jianqiang Li<author:sep>You Li;http://arxiv.org/pdf/2306.05668v2;cs.CV;IJCAI 2023 Accepted (Main Track);nerf
2306.05303v1;http://arxiv.org/abs/2306.05303v1;2023-06-08;Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields;"The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored ""fog"" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF";Qianqiu Tan<author:sep>Tao Liu<author:sep>Yinling Xie<author:sep>Shuwan Yu<author:sep>Baohua Zhang;http://arxiv.org/pdf/2306.05303v1;cs.CV;;nerf
2306.05145v1;http://arxiv.org/abs/2306.05145v1;2023-06-08;Variable Radiance Field for Real-Life Category-Specifc Reconstruction  from Single Image;"Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.";Kun Wang<author:sep>Zhiqiang Yan<author:sep>Zhenyu Zhang<author:sep>Xiang Li<author:sep>Jun Li<author:sep>Jian Yang;http://arxiv.org/pdf/2306.05145v1;cs.CV;;
2306.05410v1;http://arxiv.org/abs/2306.05410v1;2023-06-08;LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs;"A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.";Zezhou Cheng<author:sep>Carlos Esteves<author:sep>Varun Jampani<author:sep>Abhishek Kar<author:sep>Subhransu Maji<author:sep>Ameesh Makadia;http://arxiv.org/pdf/2306.05410v1;cs.CV;Project website: https://people.cs.umass.edu/~zezhoucheng/lu-nerf/;nerf
2306.04166v3;http://arxiv.org/abs/2306.04166v3;2023-06-07;BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives;"Implicit neural representation has emerged as a powerful method for
reconstructing 3D scenes from 2D images. Given a set of camera poses and
associated images, the models can be trained to synthesize novel, unseen views.
In order to expand the use cases for implicit neural representations, we need
to incorporate camera pose estimation capabilities as part of the
representation learning, as this is necessary for reconstructing scenes from
real-world video sequences where cameras are generally not being tracked.
Existing approaches like COLMAP and, most recently, bundle-adjusting neural
radiance field methods often suffer from lengthy processing times. These delays
ranging from hours to days, arise from laborious feature matching, hardware
limitations, dense point sampling, and long training times required by a
multi-layer perceptron structure with a large number of parameters. To address
these challenges, we propose a framework called bundle-adjusting accelerated
neural graphics primitives (BAA-NGP). Our approach leverages accelerated
sampling and hash encoding to expedite both pose refinement/estimation and 3D
scene reconstruction. Experimental results demonstrate that our method achieves
a more than 10 to 20 $\times$ speed improvement in novel view synthesis
compared to other bundle-adjusting neural radiance field methods without
sacrificing the quality of pose estimation. The github repository can be found
here https://github.com/IntelLabs/baa-ngp.";Sainan Liu<author:sep>Shan Lin<author:sep>Jingpei Lu<author:sep>Shreya Saha<author:sep>Alexey Supikov<author:sep>Michael Yip;http://arxiv.org/pdf/2306.04166v3;cs.CV;;
2306.03727v1;http://arxiv.org/abs/2306.03727v1;2023-06-06;Towards Visual Foundational Models of Physical Scenes;"We describe a first step towards learning general-purpose visual
representations of physical scenes using only image prediction as a training
criterion. To do so, we first define ""physical scene"" and show that, even
though different agents may maintain different representations of the same
scene, the underlying physical scene that can be inferred is unique. Then, we
show that NeRFs cannot represent the physical scene, as they lack extrapolation
mechanisms. Those, however, could be provided by Diffusion Models, at least in
theory. To test this hypothesis empirically, NeRFs can be combined with
Diffusion Models, a process we refer to as NeRF Diffusion, used as unsupervised
representations of the physical scene. Our analysis is limited to visual data,
without external grounding mechanisms that can be provided by independent
sensory modalities.";Chethan Parameshwara<author:sep>Alessandro Achille<author:sep>Matthew Trager<author:sep>Xiaolong Li<author:sep>Jiawei Mo<author:sep>Matthew Trager<author:sep>Ashwin Swaminathan<author:sep>CJ Taylor<author:sep>Dheera Venkatraman<author:sep>Xiaohan Fei<author:sep>Stefano Soatto;http://arxiv.org/pdf/2306.03727v1;cs.CV;"TLDR: Physical scenes are equivalence classes of sufficient
  statistics, and can be inferred uniquely by any agent measuring the same
  finite data; We formalize and implement an approach to representation
  learning that overturns ""naive realism"" in favor of an analytical approach of
  Russell and Koenderink. NeRFs cannot capture the physical scenes, but
  combined with Diffusion Models they can";nerf
2306.07349v1;http://arxiv.org/abs/2306.07349v1;2023-06-06;ATT3D: Amortized Text-to-3D Object Synthesis;"Text-to-3D modelling has seen exciting progress by combining generative
text-to-image models with image-to-3D methods like Neural Radiance Fields.
DreamFusion recently achieved high-quality results but requires a lengthy,
per-prompt optimization to create 3D objects. To address this, we amortize
optimization over text prompts by training on many prompts simultaneously with
a unified model, instead of separately. With this, we share computation across
a prompt set, training in less time than per-prompt optimization. Our framework
- Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts to
generalize to unseen setups and smooth interpolations between text for novel
assets and simple animations.";Jonathan Lorraine<author:sep>Kevin Xie<author:sep>Xiaohui Zeng<author:sep>Chen-Hsuan Lin<author:sep>Towaki Takikawa<author:sep>Nicholas Sharp<author:sep>Tsung-Yi Lin<author:sep>Ming-Yu Liu<author:sep>Sanja Fidler<author:sep>James Lucas;http://arxiv.org/pdf/2306.07349v1;cs.LG;22 pages, 20 figures;
2306.03576v1;http://arxiv.org/abs/2306.03576v1;2023-06-06;Human 3D Avatar Modeling with Implicit Neural Representation: A Brief  Survey;"A human 3D avatar is one of the important elements in the metaverse, and the
modeling effect directly affects people's visual experience. However, the human
body has a complex topology and diverse details, so it is often expensive,
time-consuming, and laborious to build a satisfactory model. Recent studies
have proposed a novel method, implicit neural representation, which is a
continuous representation method and can describe objects with arbitrary
topology at arbitrary resolution. Researchers have applied implicit neural
representation to human 3D avatar modeling and obtained more excellent results
than traditional methods. This paper comprehensively reviews the application of
implicit neural representation in human body modeling. First, we introduce
three implicit representations of occupancy field, SDF, and NeRF, and make a
classification of the literature investigated in this paper. Then the
application of implicit modeling methods in the body, hand, and head are
compared and analyzed respectively. Finally, we point out the shortcomings of
current work and provide available suggestions for researchers.";Mingyang Sun<author:sep>Dingkang Yang<author:sep>Dongliang Kou<author:sep>Yang Jiang<author:sep>Weihua Shan<author:sep>Zhe Yan<author:sep>Lihua Zhang;http://arxiv.org/pdf/2306.03576v1;cs.CV;A Brief Survey;nerf
2306.02741v1;http://arxiv.org/abs/2306.02741v1;2023-06-05;ZIGNeRF: Zero-shot 3D Scene Representation with Invertible Generative  Neural Radiance Fields;"Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable
proficiency in synthesizing multi-view images by learning the distribution of a
set of unposed images. Despite the aptitude of existing generative NeRFs in
generating 3D-consistent high-quality random samples within data distribution,
the creation of a 3D representation of a singular input image remains a
formidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative
model that executes zero-shot Generative Adversarial Network (GAN) inversion
for the generation of multi-view images from a single out-of-domain image. The
model is underpinned by a novel inverter that maps out-of-domain images into
the latent code of the generator manifold. Notably, ZIGNeRF is capable of
disentangling the object from the background and executing 3D operations such
as 360-degree rotation or depth and horizontal translation. The efficacy of our
model is validated using multiple real-image datasets: Cats, AFHQ, CelebA,
CelebA-HQ, and CompCars.";Kanghyeok Ko<author:sep>Minhyeok Lee;http://arxiv.org/pdf/2306.02741v1;cs.CV;;nerf
2306.03000v2;http://arxiv.org/abs/2306.03000v2;2023-06-05;BeyondPixels: A Comprehensive Review of the Evolution of Neural Radiance  Fields;"Neural rendering combines ideas from classical computer graphics and machine
learning to synthesize images from real-world observations. NeRF, short for
Neural Radiance Fields, is a recent innovation that uses AI algorithms to
create 3D objects from 2D images. By leveraging an interpolation approach, NeRF
can produce new 3D reconstructed views of complicated scenes. Rather than
directly restoring the whole 3D scene geometry, NeRF generates a volumetric
representation called a ``radiance field,'' which is capable of creating color
and density for every point within the relevant 3D space. The broad appeal and
notoriety of NeRF make it imperative to examine the existing research on the
topic comprehensively. While previous surveys on 3D rendering have primarily
focused on traditional computer vision-based or deep learning-based approaches,
only a handful of them discuss the potential of NeRF. However, such surveys
have predominantly focused on NeRF's early contributions and have not explored
its full potential. NeRF is a relatively new technique continuously being
investigated for its capabilities and limitations. This survey reviews recent
advances in NeRF and categorizes them according to their architectural designs,
especially in the field of novel view synthesis.";AKM Shahariar Azad Rabby<author:sep>Chengcui Zhang;http://arxiv.org/pdf/2306.03000v2;cs.CV;22 page, 1 figure, 5 table;nerf
2306.02903v1;http://arxiv.org/abs/2306.02903v1;2023-06-05;Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions;"We propose a method for synthesizing edited photo-realistic digital avatars
with text instructions. Given a short monocular RGB video and text
instructions, our method uses an image-conditioned diffusion model to edit one
head image and uses the video stylization method to accomplish the editing of
other head images. Through iterative training and update (three times or more),
our method synthesizes edited photo-realistic animatable 3D neural head avatars
with a deformable neural radiance field head synthesis method. In quantitative
and qualitative studies on various subjects, our method outperforms
state-of-the-art methods.";Shaoxu Li;http://arxiv.org/pdf/2306.02903v1;cs.CV;https://github.com/lsx0101/Instruct-Video2Avatar;
2306.03207v2;http://arxiv.org/abs/2306.03207v2;2023-06-05;H2-Mapping: Real-time Dense Mapping Using Hierarchical Hybrid  Representation;"Constructing a high-quality dense map in real-time is essential for robotics,
AR/VR, and digital twins applications. As Neural Radiance Field (NeRF) greatly
improves the mapping performance, in this paper, we propose a NeRF-based
mapping method that enables higher-quality reconstruction and real-time
capability even on edge computers. Specifically, we propose a novel
hierarchical hybrid representation that leverages implicit multiresolution hash
encoding aided by explicit octree SDF priors, describing the scene at different
levels of detail. This representation allows for fast scene geometry
initialization and makes scene geometry easier to learn. Besides, we present a
coverage-maximizing keyframe selection strategy to address the forgetting issue
and enhance mapping quality, particularly in marginal areas. To the best of our
knowledge, our method is the first to achieve high-quality NeRF-based mapping
on edge computers of handheld devices and quadrotors in real-time. Experiments
demonstrate that our method outperforms existing NeRF-based mapping methods in
geometry accuracy, texture realism, and time consumption. The code will be
released at: https://github.com/SYSU-STAR/H2-Mapping";Chenxing Jiang<author:sep>Hanwen Zhang<author:sep>Peize Liu<author:sep>Zehuan Yu<author:sep>Hui Cheng<author:sep>Boyu Zhou<author:sep>Shaojie Shen;http://arxiv.org/pdf/2306.03207v2;cs.RO;Accepted by IEEE Robotics and Automation Letters;nerf
2306.01531v2;http://arxiv.org/abs/2306.01531v2;2023-06-02;PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline  Panoramas;"Achieving an immersive experience enabling users to explore virtual
environments with six degrees of freedom (6DoF) is essential for various
applications such as virtual reality (VR). Wide-baseline panoramas are commonly
used in these applications to reduce network bandwidth and storage
requirements. However, synthesizing novel views from these panoramas remains a
key challenge. Although existing neural radiance field methods can produce
photorealistic views under narrow-baseline and dense image captures, they tend
to overfit the training views when dealing with \emph{wide-baseline} panoramas
due to the difficulty in learning accurate geometry from sparse $360^{\circ}$
views. To address this problem, we propose PanoGRF, Generalizable Spherical
Radiance Fields for Wide-baseline Panoramas, which construct spherical radiance
fields incorporating $360^{\circ}$ scene priors. Unlike generalizable radiance
fields trained on perspective images, PanoGRF avoids the information loss from
panorama-to-perspective conversion and directly aggregates geometry and
appearance features of 3D sample points from each panoramic view based on
spherical projection. Moreover, as some regions of the panorama are only
visible from one view while invisible from others under wide baseline settings,
PanoGRF incorporates $360^{\circ}$ monocular depth priors into spherical depth
estimation to improve the geometry features. Experimental results on multiple
panoramic datasets demonstrate that PanoGRF significantly outperforms
state-of-the-art generalizable view synthesis methods for wide-baseline
panoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay).";Zheng Chen<author:sep>Yan-Pei Cao<author:sep>Yuan-Chen Guo<author:sep>Chen Wang<author:sep>Ying Shan<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2306.01531v2;cs.CV;"accepted to NeurIPS2023; Project Page:
  https://thucz.github.io/PanoGRF/";
2306.00696v1;http://arxiv.org/abs/2306.00696v1;2023-06-01;Analyzing the Internals of Neural Radiance Fields;"Modern Neural Radiance Fields (NeRFs) learn a mapping from position to
volumetric density via proposal network samplers. In contrast to the
coarse-to-fine sampling approach with two NeRFs, this offers significant
potential for speedups using lower network capacity as the task of mapping
spatial coordinates to volumetric density involves no view-dependent effects
and is thus much easier to learn. Given that most of the network capacity is
utilized to estimate radiance, NeRFs could store valuable density information
in their parameters or their deep features. To this end, we take one step back
and analyze large, trained ReLU-MLPs used in coarse-to-fine sampling. We find
that trained NeRFs, Mip-NeRFs and proposal network samplers map samples with
high density to local minima along a ray in activation feature space. We show
how these large MLPs can be accelerated by transforming the intermediate
activations to a weight estimate, without any modifications to the parameters
post-optimization. With our approach, we can reduce the computational
requirements of trained NeRFs by up to 50% with only a slight hit in rendering
quality and no changes to the training protocol or architecture. We evaluate
our approach on a variety of architectures and datasets, showing that our
proposition holds in various settings.";Lukas Radl<author:sep>Andreas Kurz<author:sep>Markus Steinberger;http://arxiv.org/pdf/2306.00696v1;cs.CV;project page: nerfinternals.github.io;nerf
2306.00783v2;http://arxiv.org/abs/2306.00783v2;2023-06-01;FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and  Relighting with Diffusion Models;"The ability to create high-quality 3D faces from a single image has become
increasingly important with wide applications in video conferencing, AR/VR, and
advanced video editing in movie industries. In this paper, we propose Face
Diffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality
Face NeRFs from single images, complete with semantic editing and relighting
capabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly
trained 2D latent-diffusion model, allowing users to manipulate and construct
Face NeRFs in zero-shot learning without the need for explicit 3D data. With
carefully designed illumination and identity preserving loss, as well as
multi-modal pre-training, FaceDNeRF offers users unparalleled control over the
editing process enabling them to create and edit face NeRFs using just
single-view images, text prompts, and explicit target lighting. The advanced
features of FaceDNeRF have been designed to produce more impressive results
than existing 2D editing approaches that rely on 2D segmentation maps for
editable attributes. Experiments show that our FaceDNeRF achieves exceptionally
realistic results and unprecedented flexibility in editing compared with
state-of-the-art 3D face reconstruction and editing methods. Our code will be
available at https://github.com/BillyXYB/FaceDNeRF.";Hao Zhang<author:sep>Yanbo Xu<author:sep>Tianyuan Dai<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2306.00783v2;cs.CV;;nerf
2306.00547v2;http://arxiv.org/abs/2306.00547v2;2023-06-01;AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars;"Capturing and editing full head performances enables the creation of virtual
characters with various applications such as extended reality and media
production. The past few years witnessed a steep rise in the photorealism of
human head avatars. Such avatars can be controlled through different input data
modalities, including RGB, audio, depth, IMUs and others. While these data
modalities provide effective means of control, they mostly focus on editing the
head movements such as the facial expressions, head pose and/or camera
viewpoint. In this paper, we propose AvatarStudio, a text-based method for
editing the appearance of a dynamic full head avatar. Our approach builds on
existing work to capture dynamic performances of human heads using neural
radiance field (NeRF) and edits this representation with a text-to-image
diffusion model. Specifically, we introduce an optimization strategy for
incorporating multiple keyframes representing different camera viewpoints and
time stamps of a video performance into a single diffusion model. Using this
personalized diffusion model, we edit the dynamic NeRF by introducing
view-and-time-aware Score Distillation Sampling (VT-SDS) following a
model-based guidance approach. Our method edits the full head in a canonical
space, and then propagates these edits to remaining time steps via a pretrained
deformation network. We evaluate our method visually and numerically via a user
study, and results show that our method outperforms existing approaches. Our
experiments validate the design choices of our method and highlight that our
edits are genuine, personalized, as well as 3D- and time-consistent.";Mohit Mendiratta<author:sep>Xingang Pan<author:sep>Mohamed Elgharib<author:sep>Kartik Teotia<author:sep>Mallikarjun B R<author:sep>Ayush Tewari<author:sep>Vladislav Golyanik<author:sep>Adam Kortylewski<author:sep>Christian Theobalt;http://arxiv.org/pdf/2306.00547v2;cs.CV;"17 pages, 17 figures. Project page:
  https://vcai.mpi-inf.mpg.de/projects/AvatarStudio/";nerf
2305.20082v2;http://arxiv.org/abs/2305.20082v2;2023-05-31;Control4D: Efficient 4D Portrait Editing with Text;"We introduce Control4D, an innovative framework for editing dynamic 4D
portraits using text instructions. Our method addresses the prevalent
challenges in 4D editing, notably the inefficiencies of existing 4D
representations and the inconsistent editing effect caused by diffusion-based
editors. We first propose GaussianPlanes, a novel 4D representation that makes
Gaussian Splatting more structured by applying plane-based decomposition in 3D
space and time. This enhances both efficiency and robustness in 4D editing.
Furthermore, we propose to leverage a 4D generator to learn a more continuous
generation space from inconsistent edited images produced by the
diffusion-based editor, which effectively improves the consistency and quality
of 4D editing. Comprehensive evaluation demonstrates the superiority of
Control4D, including significantly reduced training time, high-quality
rendering, and spatial-temporal consistency in 4D portrait editing. The link to
our project website is https://control4darxiv.github.io.";Ruizhi Shao<author:sep>Jingxiang Sun<author:sep>Cheng Peng<author:sep>Zerong Zheng<author:sep>Boyao Zhou<author:sep>Hongwen Zhang<author:sep>Yebin Liu;http://arxiv.org/pdf/2305.20082v2;cs.CV;The link to our project website is https://control4darxiv.github.io;gaussian splatting
2305.18766v3;http://arxiv.org/abs/2305.18766v3;2023-05-30;HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion  Guidance;"The advancements in automatic text-to-3D generation have been remarkable.
Most existing methods use pre-trained text-to-image diffusion models to
optimize 3D representations like Neural Radiance Fields (NeRFs) via
latent-space denoising score matching. Yet, these methods often result in
artifacts and inconsistencies across different views due to their suboptimal
optimization approaches and limited understanding of 3D geometry. Moreover, the
inherent constraints of NeRFs in rendering crisp geometry and stable textures
usually lead to a two-stage optimization to attain high-resolution details.
This work proposes holistic sampling and smoothing approaches to achieve
high-quality text-to-3D generation, all in a single-stage optimization. We
compute denoising scores in the text-to-image diffusion model's latent and
image spaces. Instead of randomly sampling timesteps (also referred to as noise
levels in denoising score matching), we introduce a novel timestep annealing
approach that progressively reduces the sampled timestep throughout
optimization. To generate high-quality renderings in a single-stage
optimization, we propose regularization for the variance of z-coordinates along
NeRF rays. To address texture flickering issues in NeRFs, we introduce a kernel
smoothing technique that refines importance sampling weights coarse-to-fine,
ensuring accurate and thorough sampling in high-density regions. Extensive
experiments demonstrate the superiority of our method over previous approaches,
enabling the generation of highly detailed and view-consistent 3D assets
through a single-stage training process.";Junzhe Zhu<author:sep>Peiye Zhuang;http://arxiv.org/pdf/2305.18766v3;cs.CV;Project page: https://hifa-team.github.io/HiFA-site/;nerf
2305.19065v2;http://arxiv.org/abs/2305.19065v2;2023-05-30;Template-free Articulated Neural Point Clouds for Reposable View  Synthesis;"Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when
synthesizing novel views of time-evolving 3D scenes. However, the common
reliance on backward deformation fields makes reanimation of the captured
object poses challenging. Moreover, the state of the art dynamic models are
often limited by low visual fidelity, long reconstruction time or specificity
to narrow application domains. In this paper, we present a novel method
utilizing a point-based representation and Linear Blend Skinning (LBS) to
jointly learn a Dynamic NeRF and an associated skeletal model from even sparse
multi-view video. Our forward-warping approach achieves state-of-the-art visual
fidelity when synthesizing novel views and poses while significantly reducing
the necessary learning time when compared to existing work. We demonstrate the
versatility of our representation on a variety of articulated objects from
common datasets and obtain reposable 3D reconstructions without the need of
object-specific skeletal templates. Code will be made available at
https://github.com/lukasuz/Articulated-Point-NeRF.";Lukas Uzolas<author:sep>Elmar Eisemann<author:sep>Petr Kellnhofer;http://arxiv.org/pdf/2305.19065v2;cs.CV;;nerf
2305.19201v2;http://arxiv.org/abs/2305.19201v2;2023-05-30;DaRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth  Adaptation;"Neural radiance fields (NeRF) shows powerful performance in novel view
synthesis and 3D geometry reconstruction, but it suffers from critical
performance degradation when the number of known viewpoints is drastically
reduced. Existing works attempt to overcome this problem by employing external
priors, but their success is limited to certain types of scenes or datasets.
Employing monocular depth estimation (MDE) networks, pretrained on large-scale
RGB-D datasets, with powerful generalization capability would be a key to
solving this problem: however, using MDE in conjunction with NeRF comes with a
new set of challenges due to various ambiguity problems exhibited by monocular
depths. In this light, we propose a novel framework, dubbed D\""aRF, that
achieves robust NeRF reconstruction with a handful of real-world images by
combining the strengths of NeRF and monocular depth estimation through online
complementary training. Our framework imposes the MDE network's powerful
geometry prior to NeRF representation at both seen and unseen viewpoints to
enhance its robustness and coherence. In addition, we overcome the ambiguity
problems of monocular depths through patch-wise scale-shift fitting and
geometry distillation, which adapts the MDE network to produce depths aligned
accurately with NeRF geometry. Experiments show our framework achieves
state-of-the-art results both quantitatively and qualitatively, demonstrating
consistent and reliable performance in both indoor and outdoor real-world
datasets. Project page is available at https://ku-cvlab.github.io/DaRF/.";Jiuhn Song<author:sep>Seonghoon Park<author:sep>Honggyu An<author:sep>Seokju Cho<author:sep>Min-Seop Kwak<author:sep>Sungjin Cho<author:sep>Seungryong Kim;http://arxiv.org/pdf/2305.19201v2;cs.CV;"To appear at NeurIPS 2023. Project Page:
  https://ku-cvlab.github.io/DaRF/";nerf
2305.18163v1;http://arxiv.org/abs/2305.18163v1;2023-05-29;Compact Real-time Radiance Fields with Neural Codebook;"Reconstructing neural radiance fields with explicit volumetric
representations, demonstrated by Plenoxels, has shown remarkable advantages on
training and rendering efficiency, while grid-based representations typically
induce considerable overhead for storage and transmission. In this work, we
present a simple and effective framework for pursuing compact radiance fields
from the perspective of compression methodology. By exploiting intrinsic
properties exhibiting in grid models, a non-uniform compression stem is
developed to significantly reduce model complexity and a novel parameterized
module, named Neural Codebook, is introduced for better encoding high-frequency
details specific to per-scene models via a fast optimization. Our approach can
achieve over 40 $\times$ reduction on grid model storage with competitive
rendering quality. In addition, the method can achieve real-time rendering
speed with 180 fps, realizing significant advantage on storage cost compared to
real-time rendering methods.";Lingzhi Li<author:sep>Zhongshu Wang<author:sep>Zhen Shen<author:sep>Li Shen<author:sep>Ping Tan;http://arxiv.org/pdf/2305.18163v1;cs.CV;Accepted by ICME 2023;
2305.17916v2;http://arxiv.org/abs/2305.17916v2;2023-05-29;Volume Feature Rendering for Fast Neural Radiance Field Reconstruction;"Neural radiance fields (NeRFs) are able to synthesize realistic novel views
from multi-view images captured from distinct positions and perspectives. In
NeRF's rendering pipeline, neural networks are used to represent a scene
independently or transform queried learnable feature vector of a point to the
expected color or density. With the aid of geometry guides either in occupancy
grids or proposal networks, the number of neural network evaluations can be
reduced from hundreds to dozens in the standard volume rendering framework.
Instead of rendering yielded color after neural network evaluation, we propose
to render the queried feature vectors of a ray first and then transform the
rendered feature vector to the final pixel color by a neural network. This
fundamental change to the standard volume rendering framework requires only one
single neural network evaluation to render a pixel, which substantially lowers
the high computational complexity of the rendering framework attributed to a
large number of neural network evaluations. Consequently, we can use a
comparably larger neural network to achieve a better rendering quality while
maintaining the same training and rendering time costs. Our model achieves the
state-of-the-art rendering quality on both synthetic and real-world datasets
while requiring a training time of several minutes.";Kang Han<author:sep>Wei Xiang<author:sep>Lu Yu;http://arxiv.org/pdf/2305.17916v2;cs.CV;;nerf
2305.18079v3;http://arxiv.org/abs/2305.18079v3;2023-05-29;Towards a Robust Framework for NeRF Evaluation;"Neural Radiance Field (NeRF) research has attracted significant attention
recently, with 3D modelling, virtual/augmented reality, and visual effects
driving its application. While current NeRF implementations can produce high
quality visual results, there is a conspicuous lack of reliable methods for
evaluating them. Conventional image quality assessment methods and analytical
metrics (e.g. PSNR, SSIM, LPIPS etc.) only provide approximate indicators of
performance since they generalise the ability of the entire NeRF pipeline.
Hence, in this paper, we propose a new test framework which isolates the neural
rendering network from the NeRF pipeline and then performs a parametric
evaluation by training and evaluating the NeRF on an explicit radiance field
representation. We also introduce a configurable approach for generating
representations specifically for evaluation purposes. This employs ray-casting
to transform mesh models into explicit NeRF samples, as well as to ""shade""
these representations. Combining these two approaches, we demonstrate how
different ""tasks"" (scenes with different visual effects or learning strategies)
and types of networks (NeRFs and depth-wise implicit neural representations
(INRs)) can be evaluated within this framework. Additionally, we propose a
novel metric to measure task complexity of the framework which accounts for the
visual parameters and the distribution of the spatial data. Our approach offers
the potential to create a comparative objective evaluation framework for NeRF
methods.";Adrian Azzarelli<author:sep>Nantheera Anantrasirichai<author:sep>David R Bull;http://arxiv.org/pdf/2305.18079v3;cs.CV;9 pages, 2 main experiments, 2 additional experiments;nerf
2305.16914v4;http://arxiv.org/abs/2305.16914v4;2023-05-26;PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale  Scene Reconstruction;"Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images
and camera poses for Novel View Synthesis (NVS). Although NeRF can produce
photorealistic results, it often suffers from overfitting to training views,
leading to poor geometry reconstruction, especially in low-texture areas. This
limitation restricts many important applications which require accurate
geometry, such as extrapolated NVS, HD mapping and scene editing. To address
this limitation, we propose a new method to improve NeRF's 3D structure using
only RGB images and semantic maps. Our approach introduces a novel plane
regularization based on Singular Value Decomposition (SVD), that does not rely
on any geometric prior. In addition, we leverage the Structural Similarity
Index Measure (SSIM) in our loss design to properly initialize the volumetric
representation of NeRF. Quantitative and qualitative results show that our
method outperforms popular regularization approaches in accurate geometry
reconstruction for large-scale outdoor scenes and achieves SoTA rendering
quality on the KITTI-360 NVS benchmark.";Fusang Wang<author:sep>Arnaud Louys<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Luis Roldão<author:sep>Dzmitry Tsishkou;http://arxiv.org/pdf/2305.16914v4;cs.CV;Accepted to 3DV 2023;nerf
2305.16411v1;http://arxiv.org/abs/2305.16411v1;2023-05-25;ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image;"Recent advancements in text-to-image generation have enabled significant
progress in zero-shot 3D shape generation. This is achieved by score
distillation, a methodology that uses pre-trained text-to-image diffusion
models to optimize the parameters of a 3D neural presentation, e.g. Neural
Radiance Field (NeRF). While showing promising results, existing methods are
often not able to preserve the geometry of complex shapes, such as human
bodies. To address this challenge, we present ZeroAvatar, a method that
introduces the explicit 3D human body prior to the optimization process.
Specifically, we first estimate and refine the parameters of a parametric human
body from a single image. Then during optimization, we use the posed parametric
body as additional geometry constraint to regularize the diffusion model as
well as the underlying density field. Lastly, we propose a UV-guided texture
regularization term to further guide the completion of texture on invisible
body parts. We show that ZeroAvatar significantly enhances the robustness and
3D consistency of optimization-based image-to-3D avatar generation,
outperforming existing zero-shot image-to-3D methods.";Zhenzhen Weng<author:sep>Zeyu Wang<author:sep>Serena Yeung;http://arxiv.org/pdf/2305.16411v1;cs.CV;;nerf
2305.16233v1;http://arxiv.org/abs/2305.16233v1;2023-05-25;Interactive Segment Anything NeRF with Feature Imitation;"This paper investigates the potential of enhancing Neural Radiance Fields
(NeRF) with semantics to expand their applications. Although NeRF has been
proven useful in real-world applications like VR and digital creation, the lack
of semantics hinders interaction with objects in complex scenes. We propose to
imitate the backbone feature of off-the-shelf perception models to achieve
zero-shot semantic segmentation with NeRF. Our framework reformulates the
segmentation process by directly rendering semantic features and only applying
the decoder from perception models. This eliminates the need for expensive
backbones and benefits 3D consistency. Furthermore, we can project the learned
semantics onto extracted mesh surfaces for real-time interaction. With the
state-of-the-art Segment Anything Model (SAM), our framework accelerates
segmentation by 16 times with comparable mask quality. The experimental results
demonstrate the efficacy and computational advantages of our approach. Project
page: \url{https://me.kiui.moe/san/}.";Xiaokang Chen<author:sep>Jiaxiang Tang<author:sep>Diwen Wan<author:sep>Jingbo Wang<author:sep>Gang Zeng;http://arxiv.org/pdf/2305.16233v1;cs.CV;Technical Report;nerf
2305.16213v2;http://arxiv.org/abs/2305.16213v2;2023-05-25;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with  Variational Score Distillation;"Score distillation sampling (SDS) has shown great promise in text-to-3D
generation by distilling pretrained large-scale text-to-image diffusion models,
but suffers from over-saturation, over-smoothing, and low-diversity problems.
In this work, we propose to model the 3D parameter as a random variable instead
of a constant as in SDS and present variational score distillation (VSD), a
principled particle-based variational framework to explain and address the
aforementioned issues in text-to-3D generation. We show that SDS is a special
case of VSD and leads to poor samples with both small and large CFG weights. In
comparison, VSD works well with various CFG weights as ancestral sampling from
diffusion models and simultaneously improves the diversity and sample quality
with a common CFG weight (i.e., $7.5$). We further present various improvements
in the design space for text-to-3D such as distillation time schedule and
density initialization, which are orthogonal to the distillation algorithm yet
not well explored. Our overall approach, dubbed ProlificDreamer, can generate
high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with
rich structure and complex effects (e.g., smoke and drops). Further,
initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and
photo-realistic. Project page and codes:
https://ml.cs.tsinghua.edu.cn/prolificdreamer/";Zhengyi Wang<author:sep>Cheng Lu<author:sep>Yikai Wang<author:sep>Fan Bao<author:sep>Chongxuan Li<author:sep>Hang Su<author:sep>Jun Zhu;http://arxiv.org/pdf/2305.16213v2;cs.LG;NeurIPS 2023 (Spotlight);nerf
2305.15171v3;http://arxiv.org/abs/2305.15171v3;2023-05-24;Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations  from Diffusion Models;"We introduce Deceptive-NeRF, a novel methodology for few-shot NeRF
reconstruction, which leverages diffusion models to synthesize plausible
pseudo-observations to improve the reconstruction. This approach unfolds
through three key steps: 1) reconstructing a coarse NeRF from sparse input
data; 2) utilizing the coarse NeRF to render images and subsequently generating
pseudo-observations based on them; 3) training a refined NeRF model utilizing
input images augmented with pseudo-observations. We develop a deceptive
diffusion model that adeptly transitions RGB images and depth maps from coarse
NeRFs into photo-realistic pseudo-observations, all while preserving scene
semantics for reconstruction. Furthermore, we propose a progressive strategy
for training the Deceptive-NeRF, using the current NeRF renderings to create
pseudo-observations that enhance the next iteration's NeRF. Extensive
experiments demonstrate that our approach is capable of synthesizing
photo-realistic novel views, even for highly complex scenes with very sparse
inputs. Codes will be released.";Xinhang Liu<author:sep>Jiaben Chen<author:sep>Shiu-hong Kao<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2305.15171v3;cs.CV;;nerf
2305.15094v1;http://arxiv.org/abs/2305.15094v1;2023-05-24;InpaintNeRF360: Text-Guided 3D Inpainting on Unbounded Neural Radiance  Fields;"Neural Radiance Fields (NeRF) can generate highly realistic novel views.
However, editing 3D scenes represented by NeRF across 360-degree views,
particularly removing objects while preserving geometric and photometric
consistency, remains a challenging problem due to NeRF's implicit scene
representation. In this paper, we propose InpaintNeRF360, a unified framework
that utilizes natural language instructions as guidance for inpainting
NeRF-based 3D scenes.Our approach employs a promptable segmentation model by
generating multi-modal prompts from the encoded text for multiview
segmentation. We apply depth-space warping to enforce viewing consistency in
the segmentations, and further refine the inpainted NeRF model using perceptual
priors to ensure visual plausibility. InpaintNeRF360 is capable of
simultaneously removing multiple objects or modifying object appearance based
on text instructions while synthesizing 3D viewing-consistent and
photo-realistic inpainting. Through extensive experiments on both unbounded and
frontal-facing scenes trained through NeRF, we demonstrate the effectiveness of
our approach and showcase its potential to enhance the editability of implicit
radiance fields.";Dongqing Wang<author:sep>Tong Zhang<author:sep>Alaa Abboud<author:sep>Sabine Süsstrunk;http://arxiv.org/pdf/2305.15094v1;cs.CV;;nerf
2305.14831v1;http://arxiv.org/abs/2305.14831v1;2023-05-24;OD-NeRF: Efficient Training of On-the-Fly Dynamic Neural Radiance Fields;"Dynamic neural radiance fields (dynamic NeRFs) have demonstrated impressive
results in novel view synthesis on 3D dynamic scenes. However, they often
require complete video sequences for training followed by novel view synthesis,
which is similar to playing back the recording of a dynamic 3D scene. In
contrast, we propose OD-NeRF to efficiently train and render dynamic NeRFs
on-the-fly which instead is capable of streaming the dynamic scene. When
training on-the-fly, the training frames become available sequentially and the
model is trained and rendered frame-by-frame. The key challenge of efficient
on-the-fly training is how to utilize the radiance field estimated from the
previous frames effectively. To tackle this challenge, we propose: 1) a NeRF
model conditioned on the multi-view projected colors to implicitly track
correspondence between the current and previous frames, and 2) a transition and
update algorithm that leverages the occupancy grid from the last frame to
sample efficiently at the current frame. Our algorithm can achieve an
interactive speed of 6FPS training and rendering on synthetic dynamic scenes
on-the-fly, and a significant speed-up compared to the state-of-the-art on
real-world dynamic scenes.";Zhiwen Yan<author:sep>Chen Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2305.14831v1;cs.CV;;nerf
2305.14093v4;http://arxiv.org/abs/2305.14093v4;2023-05-23;Weakly Supervised 3D Open-vocabulary Segmentation;"Open-vocabulary segmentation of 3D scenes is a fundamental function of human
perception and thus a crucial objective in computer vision research. However,
this task is heavily impeded by the lack of large-scale and diverse 3D
open-vocabulary segmentation datasets for training robust and generalizable
models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation
models helps but it compromises the open-vocabulary feature as the 2D models
are mostly finetuned with close-vocabulary datasets. We tackle the challenges
in 3D open-vocabulary segmentation by exploiting pre-trained foundation models
CLIP and DINO in a weakly supervised manner. Specifically, given only the
open-vocabulary text descriptions of the objects in a scene, we distill the
open-vocabulary multimodal knowledge and object reasoning capability of CLIP
and DINO into a neural radiance field (NeRF), which effectively lifts 2D
features into view-consistent 3D segmentation. A notable aspect of our approach
is that it does not require any manual segmentation annotations for either the
foundation models or the distillation process. Extensive experiments show that
our method even outperforms fully supervised models trained with segmentation
annotations in certain scenes, suggesting that 3D open-vocabulary segmentation
can be effectively learned from 2D images and text-image pairs. Code is
available at \url{https://github.com/Kunhao-Liu/3D-OVS}.";Kunhao Liu<author:sep>Fangneng Zhan<author:sep>Jiahui Zhang<author:sep>Muyu Xu<author:sep>Yingchen Yu<author:sep>Abdulmotaleb El Saddik<author:sep>Christian Theobalt<author:sep>Eric Xing<author:sep>Shijian Lu;http://arxiv.org/pdf/2305.14093v4;cs.CV;Accepted to NeurIPS 2023;nerf
2305.12843v1;http://arxiv.org/abs/2305.12843v1;2023-05-22;Registering Neural Radiance Fields as 3D Density Images;"No significant work has been done to directly merge two partially overlapping
scenes using NeRF representations. Given pre-trained NeRF models of a 3D scene
with partial overlapping, this paper aligns them with a rigid transform, by
generalizing the traditional registration pipeline, that is, key point
detection and point set registration, to operate on 3D density fields. To
describe corner points as key points in 3D, we propose to use universal
pre-trained descriptor-generating neural networks that can be trained and
tested on different scenes. We perform experiments to demonstrate that the
descriptor networks can be conveniently trained using a contrastive learning
strategy. We demonstrate that our method, as a global approach, can effectively
register NeRF models, thus making possible future large-scale NeRF construction
by registering its smaller and overlapping NeRFs captured individually.";Han Jiang<author:sep>Ruoxuan Li<author:sep>Haosen Sun<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2305.12843v1;cs.CV;;nerf
2305.13307v1;http://arxiv.org/abs/2305.13307v1;2023-05-22;NeRFuser: Large-Scale Scene Representation by NeRF Fusion;"A practical benefit of implicit visual representations like Neural Radiance
Fields (NeRFs) is their memory efficiency: large scenes can be efficiently
stored and shared as small neural nets instead of collections of images.
However, operating on these implicit visual data structures requires extending
classical image-based vision techniques (e.g., registration, blending) from
image sets to neural fields. Towards this goal, we propose NeRFuser, a novel
architecture for NeRF registration and blending that assumes only access to
pre-generated NeRFs, and not the potentially large sets of images used to
generate them. We propose registration from re-rendering, a technique to infer
the transformation between NeRFs based on images synthesized from individual
NeRFs. For blending, we propose sample-based inverse distance weighting to
blend visual information at the ray-sample level. We evaluate NeRFuser on
public benchmarks and a self-collected object-centric indoor dataset, showing
the robustness of our method, including to views that are challenging to render
from the individual source NeRFs.";Jiading Fang<author:sep>Shengjie Lin<author:sep>Igor Vasiljevic<author:sep>Vitor Guizilini<author:sep>Rares Ambrus<author:sep>Adrien Gaidon<author:sep>Gregory Shakhnarovich<author:sep>Matthew R. Walter;http://arxiv.org/pdf/2305.13307v1;cs.CV;Code available at https://github.com/ripl/nerfuser;nerf
2305.11588v1;http://arxiv.org/abs/2305.11588v1;2023-05-19;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields;"Text-driven 3D scene generation is widely applicable to video gaming, film
industry, and metaverse applications that have a large demand for 3D scenes.
However, existing text-to-3D generation methods are limited to producing 3D
objects with simple geometries and dreamlike styles that lack realism. In this
work, we present Text2NeRF, which is able to generate a wide range of 3D scenes
with complicated geometric structures and high-fidelity textures purely from a
text prompt. To this end, we adopt NeRF as the 3D representation and leverage a
pre-trained text-to-image diffusion model to constrain the 3D reconstruction of
the NeRF to reflect the scene description. Specifically, we employ the
diffusion model to infer the text-related image as the content prior and use a
monocular depth estimation method to offer the geometric prior. Both content
and geometric priors are utilized to update the NeRF model. To guarantee
textured and geometric consistency between different views, we introduce a
progressive scene inpainting and updating strategy for novel view synthesis of
the scene. Our method requires no additional training data but only a natural
language description of the scene as the input. Extensive experiments
demonstrate that our Text2NeRF outperforms existing methods in producing
photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of
natural language prompts.";Jingbo Zhang<author:sep>Xiaoyu Li<author:sep>Ziyu Wan<author:sep>Can Wang<author:sep>Jing Liao;http://arxiv.org/pdf/2305.11588v1;cs.CV;Homepage: https://eckertzhang.github.io/Text2NeRF.github.io/;nerf
2305.11031v1;http://arxiv.org/abs/2305.11031v1;2023-05-18;ConsistentNeRF: Enhancing Neural Radiance Fields with 3D Consistency for  Sparse View Synthesis;"Neural Radiance Fields (NeRF) has demonstrated remarkable 3D reconstruction
capabilities with dense view images. However, its performance significantly
deteriorates under sparse view settings. We observe that learning the 3D
consistency of pixels among different views is crucial for improving
reconstruction quality in such cases. In this paper, we propose ConsistentNeRF,
a method that leverages depth information to regularize both multi-view and
single-view 3D consistency among pixels. Specifically, ConsistentNeRF employs
depth-derived geometry information and a depth-invariant loss to concentrate on
pixels that exhibit 3D correspondence and maintain consistent depth
relationships. Extensive experiments on recent representative works reveal that
our approach can considerably enhance model performance in sparse view
conditions, achieving improvements of up to 94% in PSNR, 76% in SSIM, and 31%
in LPIPS compared to the vanilla baselines across various benchmarks, including
DTU, NeRF Synthetic, and LLFF.";Shoukang Hu<author:sep>Kaichen Zhou<author:sep>Kaiyu Li<author:sep>Longhui Yu<author:sep>Lanqing Hong<author:sep>Tianyang Hu<author:sep>Zhenguo Li<author:sep>Gim Hee Lee<author:sep>Ziwei Liu;http://arxiv.org/pdf/2305.11031v1;cs.CV;https://github.com/skhu101/ConsistentNeRF;nerf
2305.11167v1;http://arxiv.org/abs/2305.11167v1;2023-05-18;MVPSNet: Fast Generalizable Multi-view Photometric Stereo;"We propose a fast and generalizable solution to Multi-view Photometric Stereo
(MVPS), called MVPSNet. The key to our approach is a feature extraction network
that effectively combines images from the same view captured under multiple
lighting conditions to extract geometric features from shading cues for stereo
matching. We demonstrate these features, termed `Light Aggregated Feature Maps'
(LAFM), are effective for feature matching even in textureless regions, where
traditional multi-view stereo methods fail. Our method produces similar
reconstruction results to PS-NeRF, a state-of-the-art MVPS method that
optimizes a neural network per-scene, while being 411$\times$ faster (105
seconds vs. 12 hours) in inference. Additionally, we introduce a new synthetic
dataset for MVPS, sMVPS, which is shown to be effective to train a
generalizable MVPS method.";Dongxu Zhao<author:sep>Daniel Lichy<author:sep>Pierre-Nicolas Perrin<author:sep>Jan-Michael Frahm<author:sep>Soumyadip Sengupta;http://arxiv.org/pdf/2305.11167v1;cs.CV;;nerf
2305.10579v2;http://arxiv.org/abs/2305.10579v2;2023-05-17;MultiPlaneNeRF: Neural Radiance Field with Non-Trainable Representation;"NeRF is a popular model that efficiently represents 3D objects from 2D
images. However, vanilla NeRF has some important limitations. NeRF must be
trained on each object separately. The training time is long since we encode
the object's shape and color in neural network weights. Moreover, NeRF does not
generalize well to unseen data. In this paper, we present MultiPlaneNeRF -- a
model that simultaneously solves the above problems. Our model works directly
on 2D images. We project 3D points on 2D images to produce non-trainable
representations. The projection step is not parametrized and a very shallow
decoder can efficiently process the representation. Furthermore, we can train
MultiPlaneNeRF on a large data set and force our implicit decoder to generalize
across many objects. Consequently, we can only replace the 2D images (without
additional training) to produce a NeRF representation of the new object. In the
experimental section, we demonstrate that MultiPlaneNeRF achieves results
comparable to state-of-the-art models for synthesizing new views and has
generalization properties. Additionally, MultiPlane decoder can be used as a
component in large generative models like GANs.";Dominik Zimny<author:sep>Artur Kasymov<author:sep>Adam Kania<author:sep>Jacek Tabor<author:sep>Maciej Zięba<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2305.10579v2;cs.CV;;nerf
2305.10503v3;http://arxiv.org/abs/2305.10503v3;2023-05-17;OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation  with Neural Radiance Fields;"The emergence of Neural Radiance Fields (NeRF) for novel view synthesis has
increased interest in 3D scene editing. An essential task in editing is
removing objects from a scene while ensuring visual reasonability and multiview
consistency. However, current methods face challenges such as time-consuming
object labeling, limited capability to remove specific targets, and compromised
rendering quality after removal. This paper proposes a novel object-removing
pipeline, named OR-NeRF, that can remove objects from 3D scenes with user-given
points or text prompts on a single view, achieving better performance in less
time than previous works. Our method spreads user annotations to all views
through 3D geometry and sparse correspondence, ensuring 3D consistency with
less processing burden. Then recent 2D segmentation model Segment-Anything
(SAM) is applied to predict masks, and a 2D inpainting model is used to
generate color supervision. Finally, our algorithm applies depth supervision
and perceptual loss to maintain consistency in geometry and appearance after
object removal. Experimental results demonstrate that our method achieves
better editing quality with less time than previous works, considering both
quality and quantity.";Youtan Yin<author:sep>Zhoujie Fu<author:sep>Fan Yang<author:sep>Guosheng Lin;http://arxiv.org/pdf/2305.10503v3;cs.CV;project site: https://ornerf.github.io/ (codes available);nerf
2305.09761v1;http://arxiv.org/abs/2305.09761v1;2023-05-16;NerfBridge: Bringing Real-time, Online Neural Radiance Field Training to  Robotics;"This work was presented at the IEEE International Conference on Robotics and
Automation 2023 Workshop on Unconventional Spatial Representations.
  Neural radiance fields (NeRFs) are a class of implicit scene representations
that model 3D environments from color images. NeRFs are expressive, and can
model the complex and multi-scale geometry of real world environments, which
potentially makes them a powerful tool for robotics applications. Modern NeRF
training libraries can generate a photo-realistic NeRF from a static data set
in just a few seconds, but are designed for offline use and require a slow pose
optimization pre-computation step.
  In this work we propose NerfBridge, an open-source bridge between the Robot
Operating System (ROS) and the popular Nerfstudio library for real-time, online
training of NeRFs from a stream of images. NerfBridge enables rapid development
of research on applications of NeRFs in robotics by providing an extensible
interface to the efficient training pipelines and model libraries provided by
Nerfstudio. As an example use case we outline a hardware setup that can be used
NerfBridge to train a NeRF from images captured by a camera mounted to a
quadrotor in both indoor and outdoor environments.
  For accompanying video https://youtu.be/EH0SLn-RcDg and code
https://github.com/javieryu/nerf_bridge.";Javier Yu<author:sep>Jun En Low<author:sep>Keiko Nagami<author:sep>Mac Schwager;http://arxiv.org/pdf/2305.09761v1;cs.RO;;nerf
2305.08851v3;http://arxiv.org/abs/2305.08851v3;2023-05-15;MV-Map: Offboard HD-Map Generation with Multi-view Consistency;"While bird's-eye-view (BEV) perception models can be useful for building
high-definition maps (HD-Maps) with less human labor, their results are often
unreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps
from different viewpoints. This is because BEV perception is typically set up
in an 'onboard' manner, which restricts the computation and consequently
prevents algorithms from reasoning multiple views simultaneously. This paper
overcomes these limitations and advocates a more practical 'offboard' HD-Map
generation setup that removes the computation constraints, based on the fact
that HD-Maps are commonly reusable infrastructures built offline in data
centers. To this end, we propose a novel offboard pipeline called MV-Map that
capitalizes multi-view consistency and can handle an arbitrary number of frames
with the key design of a 'region-centric' framework. In MV-Map, the target
HD-Maps are created by aggregating all the frames of onboard predictions,
weighted by the confidence scores assigned by an 'uncertainty network'. To
further enhance multi-view consistency, we augment the uncertainty network with
the global 3D structure optimized by a voxelized neural radiance field
(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map
significantly improves the quality of HD-Maps, further highlighting the
importance of offboard methods for HD-Map generation.";Ziyang Xie<author:sep>Ziqi Pang<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2305.08851v3;cs.CV;ICCV 2023;nerf
2305.08552v1;http://arxiv.org/abs/2305.08552v1;2023-05-15;Curvature-Aware Training for Coordinate Networks;"Coordinate networks are widely used in computer vision due to their ability
to represent signals as compressed, continuous entities. However, training
these networks with first-order optimizers can be slow, hindering their use in
real-time applications. Recent works have opted for shallow voxel-based
representations to achieve faster training, but this sacrifices memory
efficiency. This work proposes a solution that leverages second-order
optimization methods to significantly reduce training times for coordinate
networks while maintaining their compressibility. Experiments demonstrate the
effectiveness of this approach on various signal modalities, such as audio,
images, videos, shape reconstruction, and neural radiance fields.";Hemanth Saratchandran<author:sep>Shin-Fang Chng<author:sep>Sameera Ramasinghe<author:sep>Lachlan MacDonald<author:sep>Simon Lucey;http://arxiv.org/pdf/2305.08552v1;cs.CV;;
2305.07342v1;http://arxiv.org/abs/2305.07342v1;2023-05-12;BundleRecon: Ray Bundle-Based 3D Neural Reconstruction;"With the growing popularity of neural rendering, there has been an increasing
number of neural implicit multi-view reconstruction methods. While many models
have been enhanced in terms of positional encoding, sampling, rendering, and
other aspects to improve the reconstruction quality, current methods do not
fully leverage the information among neighboring pixels during the
reconstruction process. To address this issue, we propose an enhanced model
called BundleRecon. In the existing approaches, sampling is performed by a
single ray that corresponds to a single pixel. In contrast, our model samples a
patch of pixels using a bundle of rays, which incorporates information from
neighboring pixels. Furthermore, we design bundle-based constraints to further
improve the reconstruction quality. Experimental results demonstrate that
BundleRecon is compatible with the existing neural implicit multi-view
reconstruction methods and can improve their reconstruction quality.";Weikun Zhang<author:sep>Jianke Zhu;http://arxiv.org/pdf/2305.07342v1;cs.CV;CVPR 2023 workshop XRNeRF: Advances in NeRF for the Metaverse;
2305.07024v1;http://arxiv.org/abs/2305.07024v1;2023-05-11;SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input  Views;"We study to generate novel views of indoor scenes given sparse input views.
The challenge is to achieve both photorealism and view consistency. We present
SparseGNV: a learning framework that incorporates 3D structures and image
generative models to generate novel views with three modules. The first module
builds a neural point cloud as underlying geometry, providing contextual
information and guidance for the target novel view. The second module utilizes
a transformer-based network to map the scene context and the guidance into a
shared latent space and autoregressively decodes the target view in the form of
discrete image tokens. The third module reconstructs the tokens into the image
of the target view. SparseGNV is trained across a large indoor scene dataset to
learn generalizable priors. Once trained, it can efficiently generate novel
views of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNV
on both real-world and synthetic indoor scenes and demonstrate that it
outperforms state-of-the-art methods based on either neural radiance fields or
conditional image generation.";Weihao Cheng<author:sep>Yan-Pei Cao<author:sep>Ying Shan;http://arxiv.org/pdf/2305.07024v1;cs.CV;10 pages, 6 figures;
2305.06131v2;http://arxiv.org/abs/2305.06131v2;2023-05-10;Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era;"Generative AI (AIGC, a.k.a. AI generated content) has made remarkable
progress in the past few years, among which text-guided content generation is
the most practical one since it enables the interaction between human
instruction and AIGC. Due to the development in text-to-image as well 3D
modeling technologies (like NeRF), text-to-3D has become a newly emerging yet
highly active research field. Our work conducts the first yet comprehensive
survey on text-to-3D to help readers interested in this direction quickly catch
up with its fast development. First, we introduce 3D data representations,
including both Euclidean data and non-Euclidean data. On top of that, we
introduce various foundation technologies as well as summarize how recent works
combine those foundation technologies to realize satisfactory text-to-3D.
Moreover, we summarize how text-to-3D technology is used in various
applications, including avatar generation, texture generation, shape
transformation, and scene generation.";Chenghao Li<author:sep>Chaoning Zhang<author:sep>Atish Waghwase<author:sep>Lik-Hang Lee<author:sep>Francois Rameau<author:sep>Yang Yang<author:sep>Sung-Ho Bae<author:sep>Choong Seon Hong;http://arxiv.org/pdf/2305.06131v2;cs.CV;;nerf
2305.06118v2;http://arxiv.org/abs/2305.06118v2;2023-05-10;NeRF2: Neural Radio-Frequency Radiance Fields;"Although Maxwell discovered the physical laws of electromagnetic waves 160
years ago, how to precisely model the propagation of an RF signal in an
electrically large and complex environment remains a long-standing problem. The
difficulty is in the complex interactions between the RF signal and the
obstacles (e.g., reflection, diffraction, etc.). Inspired by the great success
of using a neural network to describe the optical field in computer vision, we
propose a neural radio-frequency radiance field, NeRF$^\textbf{2}$, which
represents a continuous volumetric scene function that makes sense of an RF
signal's propagation. Particularly, after training with a few signal
measurements, NeRF$^\textbf{2}$ can tell how/what signal is received at any
position when it knows the position of a transmitter. As a physical-layer
neural network, NeRF$^\textbf{2}$ can take advantage of the learned statistic
model plus the physical model of ray tracing to generate a synthetic dataset
that meets the training demands of application-layer artificial neural networks
(ANNs). Thus, we can boost the performance of ANNs by the proposed
turbo-learning, which mixes the true and synthetic datasets to intensify the
training. Our experiment results show that turbo-learning can enhance
performance with an approximate 50% increase. We also demonstrate the power of
NeRF$^\textbf{2}$ in the field of indoor localization and 5G MIMO.";Xiaopeng Zhao<author:sep>Zhenlin An<author:sep>Qingrui Pan<author:sep>Lei Yang;http://arxiv.org/pdf/2305.06118v2;cs.NI;;nerf
2305.06356v2;http://arxiv.org/abs/2305.06356v2;2023-05-10;HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion;"Representing human performance at high-fidelity is an essential building
block in diverse applications, such as film production, computer games or
videoconferencing. To close the gap to production-level quality, we introduce
HumanRF, a 4D dynamic neural scene representation that captures full-body
appearance in motion from multi-view video input, and enables playback from
novel, unseen viewpoints. Our novel representation acts as a dynamic video
encoding that captures fine details at high compression rates by factorizing
space-time into a temporal matrix-vector decomposition. This allows us to
obtain temporally coherent reconstructions of human actors for long sequences,
while representing high-resolution details even in the context of challenging
motion. While most research focuses on synthesizing at resolutions of 4MP or
lower, we address the challenge of operating at 12MP. To this end, we introduce
ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160
cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. We
demonstrate challenges that emerge from using such high-resolution data and
show that our newly introduced HumanRF effectively leverages this data, making
a significant step towards production-level quality novel view synthesis.";Mustafa Işık<author:sep>Martin Rünz<author:sep>Markos Georgopoulos<author:sep>Taras Khakhulin<author:sep>Jonathan Starck<author:sep>Lourdes Agapito<author:sep>Matthias Nießner;http://arxiv.org/pdf/2305.06356v2;cs.CV;"Project webpage: https://synthesiaresearch.github.io/humanrf Dataset
  webpage: https://www.actors-hq.com/ Video:
  https://www.youtube.com/watch?v=OTnhiLLE7io Code:
  https://github.com/synthesiaresearch/humanrf";
2305.05594v1;http://arxiv.org/abs/2305.05594v1;2023-05-09;PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces;"A signed distance function (SDF) parametrized by an MLP is a common
ingredient of neural surface reconstruction. We build on the successful recent
method NeuS to extend it by three new components. The first component is to
borrow the tri-plane representation from EG3D and represent signed distance
fields as a mixture of tri-planes and MLPs instead of representing it with MLPs
only. Using tri-planes leads to a more expressive data structure but will also
introduce noise in the reconstructed surface. The second component is to use a
new type of positional encoding with learnable weights to combat noise in the
reconstruction process. We divide the features in the tri-plane into multiple
frequency scales and modulate them with sin and cos functions of different
frequencies. The third component is to use learnable convolution operations on
the tri-plane features using self-attention convolution to produce features
with different frequency bands. The experiments show that PET-NeuS achieves
high-fidelity surface reconstruction on standard datasets. Following previous
work and using the Chamfer metric as the most important way to measure surface
reconstruction quality, we are able to improve upon the NeuS baseline by 57% on
Nerf-synthetic (0.84 compared to 1.97) and by 15.5% on DTU (0.71 compared to
0.84). The qualitative evaluation reveals how our method can better control the
interference of high-frequency noise. Code available at
\url{https://github.com/yiqun-wang/PET-NeuS}.";Yiqun Wang<author:sep>Ivan Skorokhodov<author:sep>Peter Wonka;http://arxiv.org/pdf/2305.05594v1;cs.CV;"CVPR 2023; 20 Pages; Project page:
  \url{https://github.com/yiqun-wang/PET-NeuS}";nerf
2305.05766v1;http://arxiv.org/abs/2305.05766v1;2023-05-09;Instant-NeRF: Instant On-Device Neural Radiance Field Training via  Algorithm-Accelerator Co-Designed Near-Memory Processing;"Instant on-device Neural Radiance Fields (NeRFs) are in growing demand for
unleashing the promise of immersive AR/VR experiences, but are still limited by
their prohibitive training time. Our profiling analysis reveals a memory-bound
inefficiency in NeRF training. To tackle this inefficiency, near-memory
processing (NMP) promises to be an effective solution, but also faces
challenges due to the unique workloads of NeRFs, including the random hash
table lookup, random point processing sequence, and heterogeneous bottleneck
steps. Therefore, we propose the first NMP framework, Instant-NeRF, dedicated
to enabling instant on-device NeRF training. Experiments on eight datasets
consistently validate the effectiveness of Instant-NeRF.";Yang Zhao<author:sep>Shang Wu<author:sep>Jingqun Zhang<author:sep>Sixu Li<author:sep>Chaojian Li<author:sep>Yingyan Lin;http://arxiv.org/pdf/2305.05766v1;cs.CV;Accepted by DAC 2023;nerf
2305.04789v1;http://arxiv.org/abs/2305.04789v1;2023-05-08;AvatarReX: Real-time Expressive Full-body Avatars;"We present AvatarReX, a new method for learning NeRF-based full-body avatars
from video data. The learnt avatar not only provides expressive control of the
body, hands and the face together, but also supports real-time animation and
rendering. To this end, we propose a compositional avatar representation, where
the body, hands and the face are separately modeled in a way that the
structural prior from parametric mesh templates is properly utilized without
compromising representation flexibility. Furthermore, we disentangle the
geometry and appearance for each part. With these technical designs, we propose
a dedicated deferred rendering pipeline, which can be executed in real-time
framerate to synthesize high-quality free-view images. The disentanglement of
geometry and appearance also allows us to design a two-pass training strategy
that combines volume rendering and surface rendering for network training. In
this way, patch-level supervision can be applied to force the network to learn
sharp appearance details on the basis of geometry estimation. Overall, our
method enables automatic construction of expressive full-body avatars with
real-time rendering capability, and can generate photo-realistic images with
dynamic details for novel body motions and facial expressions.";Zerong Zheng<author:sep>Xiaochen Zhao<author:sep>Hongwen Zhang<author:sep>Boning Liu<author:sep>Yebin Liu;http://arxiv.org/pdf/2305.04789v1;cs.CV;"To appear in SIGGRAPH 2023 Journal Track. Project page at
  https://liuyebin.com/AvatarRex/";nerf
2305.04966v2;http://arxiv.org/abs/2305.04966v2;2023-05-08;NerfAcc: Efficient Sampling Accelerates NeRFs;"Optimizing and rendering Neural Radiance Fields is computationally expensive
due to the vast number of samples required by volume rendering. Recent works
have included alternative sampling approaches to help accelerate their methods,
however, they are often not the focus of the work. In this paper, we
investigate and compare multiple sampling approaches and demonstrate that
improved sampling is generally applicable across NeRF variants under an unified
concept of transmittance estimator. To facilitate future experiments, we
develop NerfAcc, a Python toolbox that provides flexible APIs for incorporating
advanced sampling methods into NeRF related methods. We demonstrate its
flexibility by showing that it can reduce the training time of several recent
NeRF methods by 1.5x to 20x with minimal modifications to the existing
codebase. Additionally, highly customized NeRFs, such as Instant-NGP, can be
implemented in native PyTorch using NerfAcc.";Ruilong Li<author:sep>Hang Gao<author:sep>Matthew Tancik<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2305.04966v2;cs.CV;Website: https://www.nerfacc.com;nerf
2305.04296v1;http://arxiv.org/abs/2305.04296v1;2023-05-07;HashCC: Lightweight Method to Improve the Quality of the Camera-less  NeRF Scene Generation;"Neural Radiance Fields has become a prominent method of scene generation via
view synthesis. A critical requirement for the original algorithm to learn
meaningful scene representation is camera pose information for each image in a
data set. Current approaches try to circumnavigate this assumption with
moderate success, by learning approximate camera positions alongside learning
neural representations of a scene. This requires complicated camera models,
causing a long and complicated training process, or results in a lack of
texture and sharp details in rendered scenes. In this work we introduce Hash
Color Correction (HashCC) -- a lightweight method for improving Neural Radiance
Fields rendered image quality, applicable also in situations where camera
positions for a given set of images are unknown.";Jan Olszewski;http://arxiv.org/pdf/2305.04296v1;cs.CV;;nerf
2305.04268v1;http://arxiv.org/abs/2305.04268v1;2023-05-07;Multi-Space Neural Radiance Fields;"Existing Neural Radiance Fields (NeRF) methods suffer from the existence of
reflective objects, often resulting in blurry or distorted rendering. Instead
of calculating a single radiance field, we propose a multi-space neural
radiance field (MS-NeRF) that represents the scene using a group of feature
fields in parallel sub-spaces, which leads to a better understanding of the
neural network toward the existence of reflective and refractive objects. Our
multi-space scheme works as an enhancement to existing NeRF methods, with only
small computational overheads needed for training and inferring the extra-space
outputs. We demonstrate the superiority and compatibility of our approach using
three representative NeRF-based models, i.e., NeRF, Mip-NeRF, and Mip-NeRF 360.
Comparisons are performed on a novelly constructed dataset consisting of 25
synthetic scenes and 7 real captured scenes with complex reflection and
refraction, all having 360-degree viewpoints. Extensive experiments show that
our approach significantly outperforms the existing single-space NeRF methods
for rendering high-quality scenes concerned with complex light paths through
mirror-like objects. Our code and dataset will be publicly available at
https://zx-yin.github.io/msnerf.";Ze-Xin Yin<author:sep>Jiaxiong Qiu<author:sep>Ming-Ming Cheng<author:sep>Bo Ren;http://arxiv.org/pdf/2305.04268v1;cs.CV;CVPR 2023, 10 pages, 12 figures;nerf
2305.03462v2;http://arxiv.org/abs/2305.03462v2;2023-05-05;General Neural Gauge Fields;"The recent advance of neural fields, such as neural radiance fields, has
significantly pushed the boundary of scene representation learning. Aiming to
boost the computation efficiency and rendering quality of 3D scenes, a popular
line of research maps the 3D coordinate system to another measuring system,
e.g., 2D manifolds and hash tables, for modeling neural fields. The conversion
of coordinate systems can be typically dubbed as \emph{gauge transformation},
which is usually a pre-defined mapping function, e.g., orthogonal projection or
spatial hash function. This begs a question: can we directly learn a desired
gauge transformation along with the neural field in an end-to-end manner? In
this work, we extend this problem to a general paradigm with a taxonomy of
discrete \& continuous cases, and develop a learning framework to jointly
optimize gauge transformations and neural fields. To counter the problem that
the learning of gauge transformations can collapse easily, we derive a general
regularization mechanism from the principle of information conservation during
the gauge transformation. To circumvent the high computation cost in gauge
learning with regularization, we directly derive an information-invariant gauge
transformation which allows to preserve scene information inherently and yield
superior performance. Project: https://fnzhan.com/Neural-Gauge-Fields";Fangneng Zhan<author:sep>Lingjie Liu<author:sep>Adam Kortylewski<author:sep>Christian Theobalt;http://arxiv.org/pdf/2305.03462v2;cs.CV;ICLR 2023;
2305.03176v1;http://arxiv.org/abs/2305.03176v1;2023-05-04;NeRF-QA: Neural Radiance Fields Quality Assessment Database;"This short paper proposes a new database - NeRF-QA - containing 48 videos
synthesized with seven NeRF based methods, along with their perceived quality
scores, resulting from subjective assessment tests; for the videos selection,
both real and synthetic, 360 degrees scenes were considered. This database will
allow to evaluate the suitability, to NeRF based synthesized views, of existing
objective quality metrics and also the development of new quality metrics,
specific for this case.";Pedro Martin<author:sep>António Rodrigues<author:sep>João Ascenso<author:sep>Maria Paula Queluz;http://arxiv.org/pdf/2305.03176v1;cs.MM;;nerf
2305.03027v1;http://arxiv.org/abs/2305.03027v1;2023-05-04;NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads;"We focus on reconstructing high-fidelity radiance fields of human heads,
capturing their animations over time, and synthesizing re-renderings from novel
viewpoints at arbitrary time steps. To this end, we propose a new multi-view
capture setup composed of 16 calibrated machine vision cameras that record
time-synchronized images at 7.1 MP resolution and 73 frames per second. With
our setup, we collect a new dataset of over 4700 high-resolution,
high-framerate sequences of more than 220 human heads, from which we introduce
a new human head reconstruction benchmark. The recorded sequences cover a wide
range of facial dynamics, including head motions, natural expressions,
emotions, and spoken language. In order to reconstruct high-fidelity human
heads, we propose Dynamic Neural Radiance Fields using Hash Ensembles
(NeRSemble). We represent scene dynamics by combining a deformation field and
an ensemble of 3D multi-resolution hash encodings. The deformation field allows
for precise modeling of simple scene movements, while the ensemble of hash
encodings helps to represent complex dynamics. As a result, we obtain radiance
field representations of human heads that capture motion over time and
facilitate re-rendering of arbitrary novel viewpoints. In a series of
experiments, we explore the design choices of our method and demonstrate that
our approach outperforms state-of-the-art dynamic radiance field approaches by
a significant margin.";Tobias Kirschstein<author:sep>Shenhan Qian<author:sep>Simon Giebenhain<author:sep>Tim Walter<author:sep>Matthias Nießner;http://arxiv.org/pdf/2305.03027v1;cs.CV;"Siggraph 2023, Project Page:
  https://tobias-kirschstein.github.io/nersemble/ , Video:
  https://youtu.be/a-OAWqBzldU";
2305.03049v1;http://arxiv.org/abs/2305.03049v1;2023-05-04;NeuralEditor: Editing Neural Radiance Fields via Manipulating Point  Clouds;"This paper proposes NeuralEditor that enables neural radiance fields (NeRFs)
natively editable for general shape editing tasks. Despite their impressive
results on novel-view synthesis, it remains a fundamental challenge for NeRFs
to edit the shape of the scene. Our key insight is to exploit the explicit
point cloud representation as the underlying structure to construct NeRFs,
inspired by the intuitive interpretation of NeRF rendering as a process that
projects or ""plots"" the associated 3D point cloud to a 2D image plane. To this
end, NeuralEditor introduces a novel rendering scheme based on deterministic
integration within K-D tree-guided density-adaptive voxels, which produces both
high-quality rendering results and precise point clouds through optimization.
NeuralEditor then performs shape editing via mapping associated points between
point clouds. Extensive evaluation shows that NeuralEditor achieves
state-of-the-art performance in both shape deformation and scene morphing
tasks. Notably, NeuralEditor supports both zero-shot inference and further
fine-tuning over the edited scene. Our code, benchmark, and demo video are
available at https://immortalco.github.io/NeuralEditor.";Jun-Kun Chen<author:sep>Jipeng Lyu<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2305.03049v1;cs.CV;CVPR 2023;nerf
2305.02756v2;http://arxiv.org/abs/2305.02756v2;2023-05-04;Floaters No More: Radiance Field Gradient Scaling for Improved  Near-Camera Training;"NeRF acquisition typically requires careful choice of near planes for the
different cameras or suffers from background collapse, creating floating
artifacts on the edges of the captured scene. The key insight of this work is
that background collapse is caused by a higher density of samples in regions
near cameras. As a result of this sampling imbalance, near-camera volumes
receive significantly more gradients, leading to incorrect density buildup. We
propose a gradient scaling approach to counter-balance this sampling imbalance,
removing the need for near planes, while preventing background collapse. Our
method can be implemented in a few lines, does not induce any significant
overhead, and is compatible with most NeRF implementations.";Julien Philip<author:sep>Valentin Deschaintre;http://arxiv.org/pdf/2305.02756v2;cs.CV;EGSR 2023;nerf
2305.03043v1;http://arxiv.org/abs/2305.03043v1;2023-05-04;Single-Shot Implicit Morphable Faces with Consistent Texture  Parameterization;"There is a growing demand for the accessible creation of high-quality 3D
avatars that are animatable and customizable. Although 3D morphable models
provide intuitive control for editing and animation, and robustness for
single-view face reconstruction, they cannot easily capture geometric and
appearance details. Methods based on neural implicit representations, such as
signed distance functions (SDF) or neural radiance fields, approach
photo-realism, but are difficult to animate and do not generalize well to
unseen data. To tackle this problem, we propose a novel method for constructing
implicit 3D morphable face models that are both generalizable and intuitive for
editing. Trained from a collection of high-quality 3D scans, our face model is
parameterized by geometry, expression, and texture latent codes with a learned
SDF and explicit UV texture parameterization. Once trained, we can reconstruct
an avatar from a single in-the-wild image by leveraging the learned prior to
project the image into the latent space of our model. Our implicit morphable
face models can be used to render an avatar from novel views, animate facial
expressions by modifying expression codes, and edit textures by directly
painting on the learned UV-texture maps. We demonstrate quantitatively and
qualitatively that our method improves upon photo-realism, geometry, and
expression accuracy compared to state-of-the-art methods.";Connor Z. Lin<author:sep>Koki Nagano<author:sep>Jan Kautz<author:sep>Eric R. Chan<author:sep>Umar Iqbal<author:sep>Leonidas Guibas<author:sep>Gordon Wetzstein<author:sep>Sameh Khamis;http://arxiv.org/pdf/2305.03043v1;cs.CV;"SIGGRAPH 2023, Project Page:
  https://research.nvidia.com/labs/toronto-ai/ssif";
2305.02618v1;http://arxiv.org/abs/2305.02618v1;2023-05-04;Semantic-aware Generation of Multi-view Portrait Drawings;"Neural radiance fields (NeRF) based methods have shown amazing performance in
synthesizing 3D-consistent photographic images, but fail to generate multi-view
portrait drawings. The key is that the basic assumption of these methods -- a
surface point is consistent when rendered from different views -- doesn't hold
for drawings. In a portrait drawing, the appearance of a facial point may
changes when viewed from different angles. Besides, portrait drawings usually
present little 3D information and suffer from insufficient training data. To
combat this challenge, in this paper, we propose a Semantic-Aware GEnerator
(SAGE) for synthesizing multi-view portrait drawings. Our motivation is that
facial semantic labels are view-consistent and correlate with drawing
techniques. We therefore propose to collaboratively synthesize multi-view
semantic maps and the corresponding portrait drawings. To facilitate training,
we design a semantic-aware domain translator, which generates portrait drawings
based on features of photographic faces. In addition, use data augmentation via
synthesis to mitigate collapsed results. We apply SAGE to synthesize multi-view
portrait drawings in diverse artistic styles. Experimental results show that
SAGE achieves significantly superior or highly competitive performance,
compared to existing 3D-aware image synthesis methods. The codes are available
at https://github.com/AiArt-HDU/SAGE.";Biao Ma<author:sep>Fei Gao<author:sep>Chang Jiang<author:sep>Nannan Wang<author:sep>Gang Xu;http://arxiv.org/pdf/2305.02618v1;cs.CV;Accepted by IJCAI 2023;nerf
2305.02463v1;http://arxiv.org/abs/2305.02463v1;2023-05-03;Shap-E: Generating Conditional 3D Implicit Functions;"We present Shap-E, a conditional generative model for 3D assets. Unlike
recent work on 3D generative models which produce a single output
representation, Shap-E directly generates the parameters of implicit functions
that can be rendered as both textured meshes and neural radiance fields. We
train Shap-E in two stages: first, we train an encoder that deterministically
maps 3D assets into the parameters of an implicit function; second, we train a
conditional diffusion model on outputs of the encoder. When trained on a large
dataset of paired 3D and text data, our resulting models are capable of
generating complex and diverse 3D assets in a matter of seconds. When compared
to Point-E, an explicit generative model over point clouds, Shap-E converges
faster and reaches comparable or better sample quality despite modeling a
higher-dimensional, multi-representation output space. We release model
weights, inference code, and samples at https://github.com/openai/shap-e.";Heewoo Jun<author:sep>Alex Nichol;http://arxiv.org/pdf/2305.02463v1;cs.CV;23 pages, 13 figures;
2305.02310v1;http://arxiv.org/abs/2305.02310v1;2023-05-03;Real-Time Radiance Fields for Single-Image Portrait View Synthesis;"We present a one-shot method to infer and render a photorealistic 3D
representation from a single unposed image (e.g., face portrait) in real-time.
Given a single RGB input, our image encoder directly predicts a canonical
triplane representation of a neural radiance field for 3D-aware novel view
synthesis via volume rendering. Our method is fast (24 fps) on consumer
hardware, and produces higher quality results than strong GAN-inversion
baselines that require test-time optimization. To train our triplane encoder
pipeline, we use only synthetic data, showing how to distill the knowledge from
a pretrained 3D GAN into a feedforward encoder. Technical contributions include
a Vision Transformer-based triplane encoder, a camera data augmentation
strategy, and a well-designed loss function for synthetic data training. We
benchmark against the state-of-the-art methods, demonstrating significant
improvements in robustness and image quality in challenging real-world
settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ),
but our algorithm can also be applied in the future to other categories with a
3D-aware image generator.";Alex Trevithick<author:sep>Matthew Chan<author:sep>Michael Stengel<author:sep>Eric R. Chan<author:sep>Chao Liu<author:sep>Zhiding Yu<author:sep>Sameh Khamis<author:sep>Manmohan Chandraker<author:sep>Ravi Ramamoorthi<author:sep>Koki Nagano;http://arxiv.org/pdf/2305.02310v1;cs.CV;Project page: https://research.nvidia.com/labs/nxp/lp3d/;
2305.01190v2;http://arxiv.org/abs/2305.01190v2;2023-05-02;LatentAvatar: Learning Latent Expression Code for Expressive Neural Head  Avatar;"Existing approaches to animatable NeRF-based head avatars are either built
upon face templates or use the expression coefficients of templates as the
driving signal. Despite the promising progress, their performances are heavily
bound by the expression power and the tracking accuracy of the templates. In
this work, we present LatentAvatar, an expressive neural head avatar driven by
latent expression codes. Such latent expression codes are learned in an
end-to-end and self-supervised manner without templates, enabling our method to
get rid of expression and tracking issues. To achieve this, we leverage a
latent head NeRF to learn the person-specific latent expression codes from a
monocular portrait video, and further design a Y-shaped network to learn the
shared latent expression codes of different subjects for cross-identity
reenactment. By optimizing the photometric reconstruction objectives in NeRF,
the latent expression codes are learned to be 3D-aware while faithfully
capturing the high-frequency detailed expressions. Moreover, by learning a
mapping between the latent expression code learned in shared and
person-specific settings, LatentAvatar is able to perform expressive
reenactment between different subjects. Experimental results show that our
LatentAvatar is able to capture challenging expressions and the subtle movement
of teeth and even eyeballs, which outperforms previous state-of-the-art
solutions in both quantitative and qualitative comparisons. Project page:
https://www.liuyebin.com/latentavatar.";Yuelang Xu<author:sep>Hongwen Zhang<author:sep>Lizhen Wang<author:sep>Xiaochen Zhao<author:sep>Han Huang<author:sep>Guojun Qi<author:sep>Yebin Liu;http://arxiv.org/pdf/2305.01190v2;cs.CV;Accepted by SIGGRAPH 2023;nerf
2305.01163v1;http://arxiv.org/abs/2305.01163v1;2023-05-02;Federated Neural Radiance Fields;"The ability of neural radiance fields or NeRFs to conduct accurate 3D
modelling has motivated application of the technique to scene representation.
Previous approaches have mainly followed a centralised learning paradigm, which
assumes that all training images are available on one compute node for
training. In this paper, we consider training NeRFs in a federated manner,
whereby multiple compute nodes, each having acquired a distinct set of
observations of the overall scene, learn a common NeRF in parallel. This
supports the scenario of cooperatively modelling a scene using multiple agents.
Our contribution is the first federated learning algorithm for NeRF, which
splits the training effort across multiple compute nodes and obviates the need
to pool the images at a central node. A technique based on low-rank
decomposition of NeRF layers is introduced to reduce bandwidth consumption to
transmit the model parameters for aggregation. Transferring compressed models
instead of the raw data also contributes to the privacy of the data collecting
agents.";Lachlan Holden<author:sep>Feras Dayoub<author:sep>David Harvey<author:sep>Tat-Jun Chin;http://arxiv.org/pdf/2305.01163v1;cs.CV;10 pages, 7 figures;nerf
2305.01643v2;http://arxiv.org/abs/2305.01643v2;2023-05-02;Neural LiDAR Fields for Novel View Synthesis;"We present Neural Fields for LiDAR (NFL), a method to optimise a neural field
scene representation from LiDAR measurements, with the goal of synthesizing
realistic LiDAR scans from novel viewpoints. NFL combines the rendering power
of neural fields with a detailed, physically motivated model of the LiDAR
sensing process, thus enabling it to accurately reproduce key sensor behaviors
like beam divergence, secondary returns, and ray dropping. We evaluate NFL on
synthetic and real LiDAR scans and show that it outperforms explicit
reconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR
novel view synthesis task. Moreover, we show that the improved realism of the
synthesized views narrows the domain gap to real scans and translates to better
registration and semantic segmentation performance.";Shengyu Huang<author:sep>Zan Gojcic<author:sep>Zian Wang<author:sep>Francis Williams<author:sep>Yoni Kasten<author:sep>Sanja Fidler<author:sep>Konrad Schindler<author:sep>Or Litany;http://arxiv.org/pdf/2305.01643v2;cs.CV;"ICCV 2023 - camera ready. Project page:
  https://research.nvidia.com/labs/toronto-ai/nfl/";nerf
2305.00787v1;http://arxiv.org/abs/2305.00787v1;2023-05-01;GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking  Face Generation;"Generating talking person portraits with arbitrary speech audio is a crucial
problem in the field of digital human and metaverse. A modern talking face
generation method is expected to achieve the goals of generalized audio-lip
synchronization, good video quality, and high system efficiency. Recently,
neural radiance field (NeRF) has become a popular rendering technique in this
field since it could achieve high-fidelity and 3D-consistent talking face
generation with a few-minute-long training video. However, there still exist
several challenges for NeRF-based methods: 1) as for the lip synchronization,
it is hard to generate a long facial motion sequence of high temporal
consistency and audio-lip accuracy; 2) as for the video quality, due to the
limited data used to train the renderer, it is vulnerable to out-of-domain
input condition and produce bad rendering results occasionally; 3) as for the
system efficiency, the slow training and inference speed of the vanilla NeRF
severely obstruct its usage in real-world applications. In this paper, we
propose GeneFace++ to handle these challenges by 1) utilizing the pitch contour
as an auxiliary feature and introducing a temporal loss in the facial motion
prediction process; 2) proposing a landmark locally linear embedding method to
regulate the outliers in the predicted motion sequence to avoid robustness
issues; 3) designing a computationally efficient NeRF-based motion-to-video
renderer to achieves fast training and real-time inference. With these
settings, GeneFace++ becomes the first NeRF-based method that achieves stable
and real-time talking face generation with generalized audio-lip
synchronization. Extensive experiments show that our method outperforms
state-of-the-art baselines in terms of subjective and objective evaluation.
Video samples are available at https://genefaceplusplus.github.io .";Zhenhui Ye<author:sep>Jinzheng He<author:sep>Ziyue Jiang<author:sep>Rongjie Huang<author:sep>Jiawei Huang<author:sep>Jinglin Liu<author:sep>Yi Ren<author:sep>Xiang Yin<author:sep>Zejun Ma<author:sep>Zhou Zhao;http://arxiv.org/pdf/2305.00787v1;cs.CV;18 Pages, 7 figures;nerf
2305.00375v1;http://arxiv.org/abs/2305.00375v1;2023-04-30;Neural Radiance Fields (NeRFs): A Review and Some Recent Developments;"Neural Radiance Field (NeRF) is a framework that represents a 3D scene in the
weights of a fully connected neural network, known as the Multi-Layer
Perception(MLP). The method was introduced for the task of novel view synthesis
and is able to achieve state-of-the-art photorealistic image renderings from a
given continuous viewpoint. NeRFs have become a popular field of research as
recent developments have been made that expand the performance and capabilities
of the base framework. Recent developments include methods that require less
images to train the model for view synthesis as well as methods that are able
to generate views from unconstrained and dynamic scene representations.";Mohamed Debbagh;http://arxiv.org/pdf/2305.00375v1;cs.CV;volume rendering, view synthesis, scene representation, deep learning;nerf
2305.00393v3;http://arxiv.org/abs/2305.00393v3;2023-04-30;Unsupervised Object-Centric Voxelization for Dynamic Scene Understanding;"Understanding the compositional dynamics of multiple objects in unsupervised
visual environments is challenging, and existing object-centric representation
learning methods often ignore 3D consistency in scene decomposition. We propose
DynaVol, an inverse graphics approach that learns object-centric volumetric
representations in a neural rendering framework. DynaVol maintains time-varying
3D voxel grids that explicitly represent the probability of each spatial
location belonging to different objects, and decouple temporal dynamics and
spatial information by learning a canonical-space deformation field. To
optimize the volumetric features, we embed them into a fully differentiable
neural network, binding them to object-centric global features and then driving
a compositional NeRF for scene reconstruction. DynaVol outperforms existing
methods in novel view synthesis and unsupervised scene decomposition and allows
for the editing of dynamic scenes, such as adding, deleting, replacing objects,
and modifying their trajectories.";Siyu Gao<author:sep>Yanpeng Zhao<author:sep>Yunbo Wang<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2305.00393v3;cs.CV;;nerf
2305.00041v1;http://arxiv.org/abs/2305.00041v1;2023-04-28;ViP-NeRF: Visibility Prior for Sparse Input Neural Radiance Fields;"Neural radiance fields (NeRF) have achieved impressive performances in view
synthesis by encoding neural representations of a scene. However, NeRFs require
hundreds of images per scene to synthesize photo-realistic novel views.
Training them on sparse input views leads to overfitting and incorrect scene
depth estimation resulting in artifacts in the rendered novel views. Sparse
input NeRFs were recently regularized by providing dense depth estimated from
pre-trained networks as supervision, to achieve improved performance over
sparse depth constraints. However, we find that such depth priors may be
inaccurate due to generalization issues. Instead, we hypothesize that the
visibility of pixels in different input views can be more reliably estimated to
provide dense supervision. In this regard, we compute a visibility prior
through the use of plane sweep volumes, which does not require any
pre-training. By regularizing the NeRF training with the visibility prior, we
successfully train the NeRF with few input views. We reformulate the NeRF to
also directly output the visibility of a 3D point from a given viewpoint to
reduce the training time with the visibility constraint. On multiple datasets,
our model outperforms the competing sparse input NeRF models including those
that use learned priors. The source code for our model can be found on our
project page:
https://nagabhushansn95.github.io/publications/2023/ViP-NeRF.html.";Nagabhushan Somraj<author:sep>Rajiv Soundararajan;http://arxiv.org/pdf/2305.00041v1;cs.CV;SIGGRAPH 2023;nerf
2304.14811v2;http://arxiv.org/abs/2304.14811v2;2023-04-28;NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance  Fields;"Labeling LiDAR point clouds for training autonomous driving is extremely
expensive and difficult. LiDAR simulation aims at generating realistic LiDAR
data with labels for training and verifying self-driving algorithms more
efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for
novel view synthesis using implicit reconstruction of 3D scenes. Inspired by
this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages
real-world information to generate realistic LIDAR point clouds. Different from
existing LiDAR simulators, we use real images and point cloud data collected by
self-driving cars to learn the 3D scene representation, point cloud generation
and label rendering. We verify the effectiveness of our NeRF-LiDAR by training
different 3D segmentation models on the generated LiDAR point clouds. It
reveals that the trained models are able to achieve similar accuracy when
compared with the same model trained on the real LiDAR data. Besides, the
generated data is capable of boosting the accuracy through pre-training which
helps reduce the requirements of the real labeled data.";Junge Zhang<author:sep>Feihu Zhang<author:sep>Shaochen Kuang<author:sep>Li Zhang;http://arxiv.org/pdf/2304.14811v2;cs.CV;;nerf
2304.14005v2;http://arxiv.org/abs/2304.14005v2;2023-04-27;ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with  Unsupervised Implicit Pose Embedding;"Although 3D-aware GANs based on neural radiance fields have achieved
competitive performance, their applicability is still limited to objects or
scenes with the ground-truths or prediction models for clearly defined
canonical camera poses. To extend the scope of applicable datasets, we propose
a novel 3D-aware GAN optimization technique through contrastive learning with
implicit pose embeddings. To this end, we first revise the discriminator design
and remove dependency on ground-truth camera poses. Then, to capture complex
and challenging 3D scene structures more effectively, we make the discriminator
estimate a high-dimensional implicit pose embedding from a given image and
perform contrastive learning on the pose embedding. The proposed approach can
be employed for the dataset, where the canonical camera pose is ill-defined
because it does not look up or estimate camera poses. Experimental results show
that our algorithm outperforms existing methods by large margins on the
datasets with multiple object categories and inconsistent canonical camera
poses.";Mijeong Kim<author:sep>Hyunjoon Lee<author:sep>Bohyung Han;http://arxiv.org/pdf/2304.14005v2;cs.CV;"20 pages. For the project page, see
  https://cv.snu.ac.kr/research/ContraNeRF/";nerf
2304.14473v1;http://arxiv.org/abs/2304.14473v1;2023-04-27;Learning a Diffusion Prior for NeRFs;"Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D
representation for objects and scenes derived from 2D data. Generating NeRFs,
however, remains difficult in many scenarios. For instance, training a NeRF
with only a small number of views as supervision remains challenging since it
is an under-constrained problem. In such settings, it calls for some inductive
prior to filter out bad local minima. One way to introduce such inductive
priors is to learn a generative model for NeRFs modeling a certain class of
scenes. In this paper, we propose to use a diffusion model to generate NeRFs
encoded on a regularized grid. We show that our model can sample realistic
NeRFs, while at the same time allowing conditional generations, given a certain
observation as guidance.";Guandao Yang<author:sep>Abhijit Kundu<author:sep>Leonidas J. Guibas<author:sep>Jonathan T. Barron<author:sep>Ben Poole;http://arxiv.org/pdf/2304.14473v1;cs.CV;;nerf
2304.14070v1;http://arxiv.org/abs/2304.14070v1;2023-04-27;Compositional 3D Human-Object Neural Animation;"Human-object interactions (HOIs) are crucial for human-centric scene
understanding applications such as human-centric visual generation, AR/VR, and
robotics. Since existing methods mainly explore capturing HOIs, rendering HOI
remains less investigated. In this paper, we address this challenge in HOI
animation from a compositional perspective, i.e., animating novel HOIs
including novel interaction, novel human and/or novel object driven by a novel
pose sequence. Specifically, we adopt neural human-object deformation to model
and render HOI dynamics based on implicit neural representations. To enable the
interaction pose transferring among different persons and objects, we then
devise a new compositional conditional neural radiance field (or CC-NeRF),
which decomposes the interdependence between human and object using latent
codes to enable compositionally animation control of novel HOIs. Experiments
show that the proposed method can generalize well to various novel HOI
animation settings. Our project page is https://zhihou7.github.io/CHONA/";Zhi Hou<author:sep>Baosheng Yu<author:sep>Dacheng Tao;http://arxiv.org/pdf/2304.14070v1;cs.CV;14 pages, 6 figures;nerf
2304.14401v1;http://arxiv.org/abs/2304.14401v1;2023-04-27;ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs;"While NeRF-based human representations have shown impressive novel view
synthesis results, most methods still rely on a large number of images / views
for training. In this work, we propose a novel animatable NeRF called
ActorsNeRF. It is first pre-trained on diverse human subjects, and then adapted
with few-shot monocular video frames for a new actor with unseen poses.
Building on previous generalizable NeRFs with parameter sharing using a ConvNet
encoder, ActorsNeRF further adopts two human priors to capture the large human
appearance, shape, and pose variations. Specifically, in the encoded feature
space, we will first align different human subjects in a category-level
canonical space, and then align the same human from different frames in an
instance-level canonical space for rendering. We quantitatively and
qualitatively demonstrate that ActorsNeRF significantly outperforms the
existing state-of-the-art on few-shot generalization to new people and poses on
multiple datasets. Project Page: https://jitengmu.github.io/ActorsNeRF/";Jiteng Mu<author:sep>Shen Sang<author:sep>Nuno Vasconcelos<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2304.14401v1;cs.CV;Project Page : https://jitengmu.github.io/ActorsNeRF/;nerf
2304.14301v2;http://arxiv.org/abs/2304.14301v2;2023-04-27;Combining HoloLens with Instant-NeRFs: Advanced Real-Time 3D Mobile  Mapping;"This work represents a large step into modern ways of fast 3D reconstruction
based on RGB camera images. Utilizing a Microsoft HoloLens 2 as a multisensor
platform that includes an RGB camera and an inertial measurement unit for
SLAM-based camera-pose determination, we train a Neural Radiance Field (NeRF)
as a neural scene representation in real-time with the acquired data from the
HoloLens. The HoloLens is connected via Wifi to a high-performance PC that is
responsible for the training and 3D reconstruction. After the data stream ends,
the training is stopped and the 3D reconstruction is initiated, which extracts
a point cloud of the scene. With our specialized inference algorithm, five
million scene points can be extracted within 1 second. In addition, the point
cloud also includes radiometry per point. Our method of 3D reconstruction
outperforms grid point sampling with NeRFs by multiple orders of magnitude and
can be regarded as a complete real-time 3D reconstruction method in a mobile
mapping setup.";Dennis Haitz<author:sep>Boris Jutzi<author:sep>Markus Ulrich<author:sep>Miriam Jaeger<author:sep>Patrick Huebner;http://arxiv.org/pdf/2304.14301v2;cs.CV;8 pages, 6 figures;nerf
2304.13518v1;http://arxiv.org/abs/2304.13518v1;2023-04-26;Super-NeRF: View-consistent Detail Generation for NeRF super-resolution;"The neural radiance field (NeRF) achieved remarkable success in modeling 3D
scenes and synthesizing high-fidelity novel views. However, existing NeRF-based
methods focus more on the make full use of the image resolution to generate
novel views, but less considering the generation of details under the limited
input resolution. In analogy to the extensive usage of image super-resolution,
NeRF super-resolution is an effective way to generate the high-resolution
implicit representation of 3D scenes and holds great potential applications. Up
to now, such an important topic is still under-explored. In this paper, we
propose a NeRF super-resolution method, named Super-NeRF, to generate
high-resolution NeRF from only low-resolution inputs. Given multi-view
low-resolution images, Super-NeRF constructs a consistency-controlling
super-resolution module to generate view-consistent high-resolution details for
NeRF. Specifically, an optimizable latent code is introduced for each
low-resolution input image to control the 2D super-resolution images to
converge to the view-consistent output. The latent codes of each low-resolution
image are optimized synergistically with the target Super-NeRF representation
to fully utilize the view consistency constraint inherent in NeRF construction.
We verify the effectiveness of Super-NeRF on synthetic, real-world, and
AI-generated NeRF datasets. Super-NeRF achieves state-of-the-art NeRF
super-resolution performance on high-resolution detail generation and
cross-view consistency.";Yuqi Han<author:sep>Tao Yu<author:sep>Xiaohang Yu<author:sep>Yuwang Wang<author:sep>Qionghai Dai;http://arxiv.org/pdf/2304.13518v1;cs.CV;;nerf
2304.13386v2;http://arxiv.org/abs/2304.13386v2;2023-04-26;VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs;"Neural Radiance Fields (NeRF) has shown great success in novel view synthesis
due to its state-of-the-art quality and flexibility. However, NeRF requires
dense input views (tens to hundreds) and a long training time (hours to days)
for a single scene to generate high-fidelity images. Although using the voxel
grids to represent the radiance field can significantly accelerate the
optimization process, we observe that for sparse inputs, the voxel grids are
more prone to overfitting to the training views and will have holes and
floaters, which leads to artifacts. In this paper, we propose VGOS, an approach
for fast (3-5 minutes) radiance field reconstruction from sparse inputs (3-10
views) to address these issues. To improve the performance of voxel-based
radiance field in sparse input scenarios, we propose two methods: (a) We
introduce an incremental voxel training strategy, which prevents overfitting by
suppressing the optimization of peripheral voxels in the early stage of
reconstruction. (b) We use several regularization techniques to smooth the
voxels, which avoids degenerate solutions. Experiments demonstrate that VGOS
achieves state-of-the-art performance for sparse inputs with super-fast
convergence. Code will be available at https://github.com/SJoJoK/VGOS.";Jiakai Sun<author:sep>Zhanjie Zhang<author:sep>Jiafu Chen<author:sep>Guangyuan Li<author:sep>Boyan Ji<author:sep>Lei Zhao<author:sep>Wei Xing<author:sep>Huaizhong Lin;http://arxiv.org/pdf/2304.13386v2;cs.CV;IJCAI 2023 Accepted (Main Track);nerf
2304.12587v4;http://arxiv.org/abs/2304.12587v4;2023-04-25;MF-NeRF: Memory Efficient NeRF with Mixed-Feature Hash Table;"Neural radiance field (NeRF) has shown remarkable performance in generating
photo-realistic novel views. Among recent NeRF related research, the approaches
that involve the utilization of explicit structures like grids to manage
features achieve exceptionally fast training by reducing the complexity of
multilayer perceptron (MLP) networks. However, storing features in dense grids
demands a substantial amount of memory space, resulting in a notable memory
bottleneck within computer system. Consequently, it leads to a significant
increase in training times without prior hyper-parameter tuning. To address
this issue, in this work, we are the first to propose MF-NeRF, a
memory-efficient NeRF framework that employs a Mixed-Feature hash table to
improve memory efficiency and reduce training time while maintaining
reconstruction quality. Specifically, we first design a mixed-feature hash
encoding to adaptively mix part of multi-level feature grids and map it to a
single hash table. Following that, in order to obtain the correct index of a
grid point, we further develop an index transformation method that transforms
indices of an arbitrary level grid to those of a canonical grid. Extensive
experiments benchmarking with state-of-the-art Instant-NGP, TensoRF, and DVGO,
indicate our MF-NeRF could achieve the fastest training time on the same GPU
hardware with similar or even higher reconstruction quality.";Yongjae Lee<author:sep>Li Yang<author:sep>Deliang Fan;http://arxiv.org/pdf/2304.12587v4;cs.CV;;nerf
2304.12746v1;http://arxiv.org/abs/2304.12746v1;2023-04-25;Local Implicit Ray Function for Generalizable Radiance Field  Representation;"We propose LIRF (Local Implicit Ray Function), a generalizable neural
rendering approach for novel view rendering. Current generalizable neural
radiance fields (NeRF) methods sample a scene with a single ray per pixel and
may therefore render blurred or aliased views when the input views and rendered
views capture scene content with different resolutions. To solve this problem,
we propose LIRF to aggregate the information from conical frustums to construct
a ray. Given 3D positions within conical frustums, LIRF takes 3D coordinates
and the features of conical frustums as inputs and predicts a local volumetric
radiance field. Since the coordinates are continuous, LIRF renders high-quality
novel views at a continuously-valued scale via volume rendering. Besides, we
predict the visible weights for each input view via transformer-based feature
matching to improve the performance in occluded areas. Experimental results on
real-world scenes validate that our method outperforms state-of-the-art methods
on novel view rendering of unseen scenes at arbitrary scales.";Xin Huang<author:sep>Qi Zhang<author:sep>Ying Feng<author:sep>Xiaoyu Li<author:sep>Xuan Wang<author:sep>Qing Wang;http://arxiv.org/pdf/2304.12746v1;cs.CV;Accepted to CVPR 2023. Project page: https://xhuangcv.github.io/lirf/;nerf
2304.12439v1;http://arxiv.org/abs/2304.12439v1;2023-04-24;TextMesh: Generation of Realistic 3D Meshes From Text Prompts;"The ability to generate highly realistic 2D images from mere text prompts has
recently made huge progress in terms of speed and quality, thanks to the advent
of image diffusion models. Naturally, the question arises if this can be also
achieved in the generation of 3D content from such text prompts. To this end, a
new line of methods recently emerged trying to harness diffusion models,
trained on 2D images, for supervision of 3D model generation using view
dependent prompts. While achieving impressive results, these methods, however,
have two major drawbacks. First, rather than commonly used 3D meshes, they
instead generate neural radiance fields (NeRFs), making them impractical for
most real applications. Second, these approaches tend to produce over-saturated
models, giving the output a cartoonish looking effect. Therefore, in this work
we propose a novel method for generation of highly realistic-looking 3D meshes.
To this end, we extend NeRF to employ an SDF backbone, leading to improved 3D
mesh extraction. In addition, we propose a novel way to finetune the mesh
texture, removing the effect of high saturation and improving the details of
the output 3D mesh.";Christina Tsalicoglou<author:sep>Fabian Manhardt<author:sep>Alessio Tonioni<author:sep>Michael Niemeyer<author:sep>Federico Tombari;http://arxiv.org/pdf/2304.12439v1;cs.CV;Project Website: https://fabi92.github.io/textmesh/;nerf
2304.12281v1;http://arxiv.org/abs/2304.12281v1;2023-04-24;HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single  Video;"We introduce HOSNeRF, a novel 360{\deg} free-viewpoint rendering method that
reconstructs neural radiance fields for dynamic human-object-scene from a
single monocular in-the-wild video. Our method enables pausing the video at any
frame and rendering all scene details (dynamic humans, objects, and
backgrounds) from arbitrary viewpoints. The first challenge in this task is the
complex object motions in human-object interactions, which we tackle by
introducing the new object bones into the conventional human skeleton hierarchy
to effectively estimate large object deformations in our dynamic human-object
model. The second challenge is that humans interact with different objects at
different times, for which we introduce two new learnable object state
embeddings that can be used as conditions for learning our human-object
representation and scene representation, respectively. Extensive experiments
show that HOSNeRF significantly outperforms SOTA approaches on two challenging
datasets by a large margin of 40% ~ 50% in terms of LPIPS. The code, data, and
compelling examples of 360{\deg} free-viewpoint renderings from single videos
will be released in https://showlab.github.io/HOSNeRF.";Jia-Wei Liu<author:sep>Yan-Pei Cao<author:sep>Tianyuan Yang<author:sep>Eric Zhongcong Xu<author:sep>Jussi Keppo<author:sep>Ying Shan<author:sep>Xiaohu Qie<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2304.12281v1;cs.CV;Project page: https://showlab.github.io/HOSNeRF;nerf
2304.12308v4;http://arxiv.org/abs/2304.12308v4;2023-04-24;Segment Anything in 3D with NeRFs;"Recently, the Segment Anything Model (SAM) emerged as a powerful vision
foundation model which is capable to segment anything in 2D images. This paper
aims to generalize SAM to segment 3D objects. Rather than replicating the data
acquisition and annotation procedure which is costly in 3D, we design an
efficient solution, leveraging the Neural Radiance Field (NeRF) as a cheap and
off-the-shelf prior that connects multi-view 2D images to the 3D space. We
refer to the proposed solution as SA3D, for Segment Anything in 3D. It is only
required to provide a manual segmentation prompt (e.g., rough points) for the
target object in a single view, which is used to generate its 2D mask in this
view with SAM. Next, SA3D alternately performs mask inverse rendering and
cross-view self-prompting across various views to iteratively complete the 3D
mask of the target object constructed with voxel grids. The former projects the
2D mask obtained by SAM in the current view onto 3D mask with guidance of the
density distribution learned by the NeRF; The latter extracts reliable prompts
automatically as the input to SAM from the NeRF-rendered 2D mask in another
view. We show in experiments that SA3D adapts to various scenes and achieves 3D
segmentation within minutes. Our research reveals a potential methodology to
lift the ability of a 2D vision foundation model to 3D, as long as the 2D model
can steadily address promptable segmentation across multiple views. Our code is
available at https://github.com/Jumpat/SegmentAnythingin3D.";Jiazhong Cen<author:sep>Zanwei Zhou<author:sep>Jiemin Fang<author:sep>Chen Yang<author:sep>Wei Shen<author:sep>Lingxi Xie<author:sep>Dongsheng Jiang<author:sep>Xiaopeng Zhang<author:sep>Qi Tian;http://arxiv.org/pdf/2304.12308v4;cs.CV;NeurIPS 2023. Project page: https://jumpat.github.io/SA3D/;nerf
2304.12467v2;http://arxiv.org/abs/2304.12467v2;2023-04-24;Instant-3D: Instant Neural Radiance Field Training Towards On-Device  AR/VR 3D Reconstruction;"Neural Radiance Field (NeRF) based 3D reconstruction is highly desirable for
immersive Augmented and Virtual Reality (AR/VR) applications, but achieving
instant (i.e., < 5 seconds) on-device NeRF training remains a challenge. In
this work, we first identify the inefficiency bottleneck: the need to
interpolate NeRF embeddings up to 200,000 times from a 3D embedding grid during
each training iteration. To alleviate this, we propose Instant-3D, an
algorithm-hardware co-design acceleration framework that achieves instant
on-device NeRF training. Our algorithm decomposes the embedding grid
representation in terms of color and density, enabling computational redundancy
to be squeezed out by adopting different (1) grid sizes and (2) update
frequencies for the color and density branches. Our hardware accelerator
further reduces the dominant memory accesses for embedding grid interpolation
by (1) mapping multiple nearby points' memory read requests into one during the
feed-forward process, (2) merging embedding grid updates from the same sliding
time window during back-propagation, and (3) fusing different computation cores
to support the different grid sizes needed by the color and density branches of
Instant-3D algorithm. Extensive experiments validate the effectiveness of
Instant-3D, achieving a large training time reduction of 41x - 248x while
maintaining the same reconstruction quality. Excitingly, Instant-3D has enabled
instant 3D reconstruction for AR/VR, requiring a reconstruction time of only
1.6 seconds per scene and meeting the AR/VR power consumption constraint of 1.9
W.";Sixu Li<author:sep>Chaojian Li<author:sep>Wenbo Zhu<author:sep> Boyang<author:sep> Yu<author:sep> Yang<author:sep> Zhao<author:sep>Cheng Wan<author:sep>Haoran You<author:sep>Huihong Shi<author:sep> Yingyan<author:sep> Lin;http://arxiv.org/pdf/2304.12467v2;cs.AR;Accepted by ISCA'23;nerf
2304.11842v3;http://arxiv.org/abs/2304.11842v3;2023-04-24;Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via  Algorithm-Hardware Co-Design;"Novel view synthesis is an essential functionality for enabling immersive
experiences in various Augmented- and Virtual-Reality (AR/VR) applications, for
which generalizable Neural Radiance Fields (NeRFs) have gained increasing
popularity thanks to their cross-scene generalization capability. Despite their
promise, the real-device deployment of generalizable NeRFs is bottlenecked by
their prohibitive complexity due to the required massive memory accesses to
acquire scene features, causing their ray marching process to be
memory-bounded. To this end, we propose Gen-NeRF, an algorithm-hardware
co-design framework dedicated to generalizable NeRF acceleration, which for the
first time enables real-time generalizable NeRFs. On the algorithm side,
Gen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact
that different regions of a 3D scene contribute differently to the rendered
pixel, to enable sparse yet effective sampling. On the hardware side, Gen-NeRF
highlights an accelerator micro-architecture to maximize the data reuse
opportunities among different rays by making use of their epipolar geometric
relationship. Furthermore, our Gen-NeRF accelerator features a customized
dataflow to enhance data locality during point-to-hardware mapping and an
optimized scene feature storage strategy to minimize memory bank conflicts.
Extensive experiments validate the effectiveness of our proposed Gen-NeRF
framework in enabling real-time and generalizable novel view synthesis.";Yonggan Fu<author:sep>Zhifan Ye<author:sep>Jiayi Yuan<author:sep>Shunyao Zhang<author:sep>Sixu Li<author:sep>Haoran You<author:sep>Yingyan Lin;http://arxiv.org/pdf/2304.11842v3;cs.CV;Accepted by ISCA 2023;nerf
2304.12294v1;http://arxiv.org/abs/2304.12294v1;2023-04-24;Explicit Correspondence Matching for Generalizable Neural Radiance  Fields;"We present a new generalizable NeRF method that is able to directly
generalize to new unseen scenarios and perform novel view synthesis with as few
as two source views. The key to our approach lies in the explicitly modeled
correspondence matching information, so as to provide the geometry prior to the
prediction of NeRF color and density for volume rendering. The explicit
correspondence matching is quantified with the cosine similarity between image
features sampled at the 2D projections of a 3D point on different views, which
is able to provide reliable cues about the surface geometry. Unlike previous
methods where image features are extracted independently for each view, we
consider modeling the cross-view interactions via Transformer cross-attention,
which greatly improves the feature matching quality. Our method achieves
state-of-the-art results on different evaluation settings, with the experiments
showing a strong correlation between our learned cosine feature similarity and
volume density, demonstrating the effectiveness and superiority of our proposed
method. Code is at https://github.com/donydchen/matchnerf";Yuedong Chen<author:sep>Haofei Xu<author:sep>Qianyi Wu<author:sep>Chuanxia Zheng<author:sep>Tat-Jen Cham<author:sep>Jianfei Cai;http://arxiv.org/pdf/2304.12294v1;cs.CV;"Code and pre-trained models: https://github.com/donydchen/matchnerf
  Project Page: https://donydchen.github.io/matchnerf/";nerf
2304.11342v1;http://arxiv.org/abs/2304.11342v1;2023-04-22;NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent  Semantic Navigation;"3D representation disentanglement aims to identify, decompose, and manipulate
the underlying explanatory factors of 3D data, which helps AI fundamentally
understand our 3D world. This task is currently under-explored and poses great
challenges: (i) the 3D representations are complex and in general contains much
more information than 2D image; (ii) many 3D representations are not well
suited for gradient-based optimization, let alone disentanglement. To address
these challenges, we use NeRF as a differentiable 3D representation, and
introduce a self-supervised Navigation to identify interpretable semantic
directions in the latent space. To our best knowledge, this novel method,
dubbed NaviNeRF, is the first work to achieve fine-grained 3D disentanglement
without any priors or supervisions. Specifically, NaviNeRF is built upon the
generative NeRF pipeline, and equipped with an Outer Navigation Branch and an
Inner Refinement Branch. They are complementary -- the outer navigation is to
identify global-view semantic directions, and the inner refinement dedicates to
fine-grained attributes. A synergistic loss is further devised to coordinate
two branches. Extensive experiments demonstrate that NaviNeRF has a superior
fine-grained 3D disentanglement ability than the previous 3D-aware models. Its
performance is also comparable to editing-oriented models relying on semantic
or geometry priors.";Baao Xie<author:sep>Bohan Li<author:sep>Zequn Zhang<author:sep>Junting Dong<author:sep>Xin Jin<author:sep>Jingyu Yang<author:sep>Wenjun Zeng;http://arxiv.org/pdf/2304.11342v1;cs.CV;;nerf
2304.11470v1;http://arxiv.org/abs/2304.11470v1;2023-04-22;3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive  Physics under Challenging Scenes;"Given a visual scene, humans have strong intuitions about how a scene can
evolve over time under given actions. The intuition, often termed visual
intuitive physics, is a critical ability that allows us to make effective plans
to manipulate the scene to achieve desired outcomes without relying on
extensive trial and error. In this paper, we present a framework capable of
learning 3D-grounded visual intuitive physics models from videos of complex
scenes with fluids. Our method is composed of a conditional Neural Radiance
Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction
backend, using which we can impose strong relational and structural inductive
bias to capture the structure of the underlying environment. Unlike existing
intuitive point-based dynamics works that rely on the supervision of dense
point trajectory from simulators, we relax the requirements and only assume
access to multi-view RGB images and (imperfect) instance masks acquired using
color prior. This enables the proposed model to handle scenarios where accurate
point estimation and tracking are hard or impossible. We generate datasets
including three challenging scenarios involving fluid, granular materials, and
rigid objects in the simulation. The datasets do not include any dense particle
information so most previous 3D-based intuitive physics pipelines can barely
deal with that. We show our model can make long-horizon future predictions by
learning from raw images and significantly outperforms models that do not
employ an explicit 3D representation space. We also show that once trained, our
model can achieve strong generalization in complex scenarios under extrapolate
settings.";Haotian Xue<author:sep>Antonio Torralba<author:sep>Joshua B. Tenenbaum<author:sep>Daniel LK Yamins<author:sep>Yunzhu Li<author:sep>Hsiao-Yu Tung;http://arxiv.org/pdf/2304.11470v1;cs.CV;;nerf
2304.11448v1;http://arxiv.org/abs/2304.11448v1;2023-04-22;Dehazing-NeRF: Neural Radiance Fields from Hazy Images;"Neural Radiance Field (NeRF) has received much attention in recent years due
to the impressively high quality in 3D scene reconstruction and novel view
synthesis. However, image degradation caused by the scattering of atmospheric
light and object light by particles in the atmosphere can significantly
decrease the reconstruction quality when shooting scenes in hazy conditions. To
address this issue, we propose Dehazing-NeRF, a method that can recover clear
NeRF from hazy image inputs. Our method simulates the physical imaging process
of hazy images using an atmospheric scattering model, and jointly learns the
atmospheric scattering model and a clean NeRF model for both image dehazing and
novel view synthesis. Different from previous approaches, Dehazing-NeRF is an
unsupervised method with only hazy images as the input, and also does not rely
on hand-designed dehazing priors. By jointly combining the depth estimated from
the NeRF 3D scene with the atmospheric scattering model, our proposed model
breaks through the ill-posed problem of single-image dehazing while maintaining
geometric consistency. Besides, to alleviate the degradation of image quality
caused by information loss, soft margin consistency regularization, as well as
atmospheric consistency and contrast discriminative loss, are addressed during
the model training process. Extensive experiments demonstrate that our method
outperforms the simple combination of single-image dehazing and NeRF on both
image dehazing and novel view image synthesis.";Tian Li<author:sep>LU Li<author:sep>Wei Wang<author:sep>Zhangchi Feng;http://arxiv.org/pdf/2304.11448v1;cs.CV;;nerf
2304.11241v2;http://arxiv.org/abs/2304.11241v2;2023-04-21;AutoNeRF: Training Implicit Scene Representations with Autonomous Agents;"Implicit representations such as Neural Radiance Fields (NeRF) have been
shown to be very effective at novel view synthesis. However, these models
typically require manual and careful human data collection for training. In
this paper, we present AutoNeRF, a method to collect data required to train
NeRFs using autonomous embodied agents. Our method allows an agent to explore
an unseen environment efficiently and use the experience to build an implicit
map representation autonomously. We compare the impact of different exploration
strategies including handcrafted frontier-based exploration, end-to-end and
modular approaches composed of trained high-level planners and classical
low-level path followers. We train these models with different reward functions
tailored to this problem and evaluate the quality of the learned
representations on four different downstream tasks: classical viewpoint
rendering, map reconstruction, planning, and pose refinement. Empirical results
show that NeRFs can be trained on actively collected data using just a single
episode of experience in an unseen environment, and can be used for several
downstream robotic tasks, and that modular trained exploration models
outperform other classical and end-to-end baselines. Finally, we show that
AutoNeRF can reconstruct large-scale scenes, and is thus a useful tool to
perform scene-specific adaptation as the produced 3D environment models can be
loaded into a simulator to fine-tune a policy of interest.";Pierre Marza<author:sep>Laetitia Matignon<author:sep>Olivier Simonin<author:sep>Dhruv Batra<author:sep>Christian Wolf<author:sep>Devendra Singh Chaplot;http://arxiv.org/pdf/2304.11241v2;cs.CV;;nerf
2304.10780v1;http://arxiv.org/abs/2304.10780v1;2023-04-21;Omni-Line-of-Sight Imaging for Holistic Shape Reconstruction;"We introduce Omni-LOS, a neural computational imaging method for conducting
holistic shape reconstruction (HSR) of complex objects utilizing a
Single-Photon Avalanche Diode (SPAD)-based time-of-flight sensor. As
illustrated in Fig. 1, our method enables new capabilities to reconstruct
near-$360^\circ$ surrounding geometry of an object from a single scan spot. In
such a scenario, traditional line-of-sight (LOS) imaging methods only see the
front part of the object and typically fail to recover the occluded back
regions. Inspired by recent advances of non-line-of-sight (NLOS) imaging
techniques which have demonstrated great power to reconstruct occluded objects,
Omni-LOS marries LOS and NLOS together, leveraging their complementary
advantages to jointly recover the holistic shape of the object from a single
scan position. The core of our method is to put the object nearby diffuse walls
and augment the LOS scan in the front view with the NLOS scans from the
surrounding walls, which serve as virtual ``mirrors'' to trap lights toward the
object. Instead of separately recovering the LOS and NLOS signals, we adopt an
implicit neural network to represent the object, analogous to NeRF and NeTF.
While transients are measured along straight rays in LOS but over the spherical
wavefronts in NLOS, we derive differentiable ray propagation models to
simultaneously model both types of transient measurements so that the NLOS
reconstruction also takes into account the direct LOS measurements and vice
versa. We further develop a proof-of-concept Omni-LOS hardware prototype for
real-world validation. Comprehensive experiments on various wall settings
demonstrate that Omni-LOS successfully resolves shape ambiguities caused by
occlusions, achieves high-fidelity 3D scan quality, and manages to recover
objects of various scales and complexity.";Binbin Huang<author:sep>Xingyue Peng<author:sep>Siyuan Shen<author:sep>Suan Xia<author:sep>Ruiqian Li<author:sep>Yanhua Yu<author:sep>Yuehan Wang<author:sep>Shenghua Gao<author:sep>Wenzheng Chen<author:sep>Shiying Li<author:sep>Jingyi Yu;http://arxiv.org/pdf/2304.10780v1;cs.CV;;nerf
2304.10406v2;http://arxiv.org/abs/2304.10406v2;2023-04-20;LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields;"We introduce a new task, novel view synthesis for LiDAR sensors. While
traditional model-based LiDAR simulators with style-transfer neural networks
can be applied to render novel views, they fall short of producing accurate and
realistic LiDAR patterns because the renderers rely on explicit 3D
reconstruction and exploit game engines, that ignore important attributes of
LiDAR points. We address this challenge by formulating, to the best of our
knowledge, the first differentiable end-to-end LiDAR rendering framework,
LiDAR-NeRF, leveraging a neural radiance field (NeRF) to facilitate the joint
learning of geometry and the attributes of 3D points. However, simply employing
NeRF cannot achieve satisfactory results, as it only focuses on learning
individual pixels while ignoring local information, especially at low texture
areas, resulting in poor geometry. To this end, we have taken steps to address
this issue by introducing a structural regularization method to preserve local
structural details. To evaluate the effectiveness of our approach, we establish
an object-centric multi-view LiDAR dataset, dubbed NeRF-MVL. It contains
observations of objects from 9 categories seen from 360-degree viewpoints
captured with multiple LiDAR sensors. Our extensive experiments on the
scene-level KITTI-360 dataset, and on our object-level NeRF-MVL show that our
LiDAR-NeRF surpasses the model-based algorithms significantly.";Tang Tao<author:sep>Longfei Gao<author:sep>Guangrun Wang<author:sep>Yixing Lao<author:sep>Peng Chen<author:sep>Hengshuang Zhao<author:sep>Dayang Hao<author:sep>Xiaodan Liang<author:sep>Mathieu Salzmann<author:sep>Kaicheng Yu;http://arxiv.org/pdf/2304.10406v2;cs.CV;"This paper introduces a new task of novel LiDAR view synthesis, and
  proposes a differentiable framework called LiDAR-NeRF with a structural
  regularization, as well as an object-centric multi-view LiDAR dataset called
  NeRF-MVL";nerf
2304.10075v2;http://arxiv.org/abs/2304.10075v2;2023-04-20;Multiscale Representation for Real-Time Anti-Aliasing Neural Rendering;"The rendering scheme in neural radiance field (NeRF) is effective in
rendering a pixel by casting a ray into the scene. However, NeRF yields blurred
rendering results when the training images are captured at non-uniform scales,
and produces aliasing artifacts if the test images are taken in distant views.
To address this issue, Mip-NeRF proposes a multiscale representation as a
conical frustum to encode scale information. Nevertheless, this approach is
only suitable for offline rendering since it relies on integrated positional
encoding (IPE) to query a multilayer perceptron (MLP). To overcome this
limitation, we propose mip voxel grids (Mip-VoG), an explicit multiscale
representation with a deferred architecture for real-time anti-aliasing
rendering. Our approach includes a density Mip-VoG for scene geometry and a
feature Mip-VoG with a small MLP for view-dependent color. Mip-VoG encodes
scene scale using the level of detail (LOD) derived from ray differentials and
uses quadrilinear interpolation to map a queried 3D location to its features
and density from two neighboring downsampled voxel grids. To our knowledge, our
approach is the first to offer multiscale training and real-time anti-aliasing
rendering simultaneously. We conducted experiments on multiscale datasets, and
the results show that our approach outperforms state-of-the-art real-time
rendering baselines.";Dongting Hu<author:sep>Zhenkai Zhang<author:sep>Tingbo Hou<author:sep>Tongliang Liu<author:sep>Huan Fu<author:sep>Mingming Gong;http://arxiv.org/pdf/2304.10075v2;cs.CV;;nerf
2304.10050v2;http://arxiv.org/abs/2304.10050v2;2023-04-20;Neural Radiance Fields: Past, Present, and Future;"The various aspects like modeling and interpreting 3D environments and
surroundings have enticed humans to progress their research in 3D Computer
Vision, Computer Graphics, and Machine Learning. An attempt made by Mildenhall
et al in their paper about NeRFs (Neural Radiance Fields) led to a boom in
Computer Graphics, Robotics, Computer Vision, and the possible scope of
High-Resolution Low Storage Augmented Reality and Virtual Reality-based 3D
models have gained traction from res with more than 1000 preprints related to
NeRFs published. This paper serves as a bridge for people starting to study
these fields by building on the basics of Mathematics, Geometry, Computer
Vision, and Computer Graphics to the difficulties encountered in Implicit
Representations at the intersection of all these disciplines. This survey
provides the history of rendering, Implicit Learning, and NeRFs, the
progression of research on NeRFs, and the potential applications and
implications of NeRFs in today's world. In doing so, this survey categorizes
all the NeRF-related research in terms of the datasets used, objective
functions, applications solved, and evaluation criteria for these applications.";Ansh Mittal;http://arxiv.org/pdf/2304.10050v2;cs.CV;413 pages, 9 figures, 277 citations;nerf
2304.10250v1;http://arxiv.org/abs/2304.10250v1;2023-04-20;Revisiting Implicit Neural Representations in Low-Level Vision;"Implicit Neural Representation (INR) has been emerging in computer vision in
recent years. It has been shown to be effective in parameterising continuous
signals such as dense 3D models from discrete image data, e.g. the neural
radius field (NeRF). However, INR is under-explored in 2D image processing
tasks. Considering the basic definition and the structure of INR, we are
interested in its effectiveness in low-level vision problems such as image
restoration. In this work, we revisit INR and investigate its application in
low-level image restoration tasks including image denoising, super-resolution,
inpainting, and deblurring. Extensive experimental evaluations suggest the
superior performance of INR in several low-level vision tasks with limited
resources, outperforming its counterparts by over 2dB. Code and models are
available at https://github.com/WenTXuL/LINR";Wentian Xu<author:sep>Jianbo Jiao;http://arxiv.org/pdf/2304.10250v1;cs.CV;"Published at the ICLR 2023 Neural Fields workshop. Project Webpage:
  https://wentxul.github.io/LINR-projectpage";nerf
2304.10448v1;http://arxiv.org/abs/2304.10448v1;2023-04-20;ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of  Real World Objects;"In this paper, we focus on the problem of rendering novel views from a Neural
Radiance Field (NeRF) under unobserved light conditions. To this end, we
introduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real world
objects under one-light-at-time (OLAT) conditions, annotated with accurate
ground-truth camera and light poses. Our acquisition pipeline leverages two
robotic arms holding, respectively, a camera and an omni-directional point-wise
light source. We release a total of 20 scenes depicting a variety of objects
with complex geometry and challenging materials. Each scene includes 2000
images, acquired from 50 different points of views under 40 different OLAT
conditions. By leveraging the dataset, we perform an ablation study on the
relighting capability of variants of the vanilla NeRF architecture and identify
a lightweight architecture that can render novel views of an object under novel
light conditions, which we use to establish a non-trivial baseline for the
dataset. Dataset and benchmark are available at
https://eyecan-ai.github.io/rene.";Marco Toschi<author:sep>Riccardo De Matteo<author:sep>Riccardo Spezialetti<author:sep>Daniele De Gregorio<author:sep>Luigi Di Stefano<author:sep>Samuele Salti;http://arxiv.org/pdf/2304.10448v1;cs.CV;Accepted at CVPR 2023 as a highlight;nerf
2304.10532v3;http://arxiv.org/abs/2304.10532v3;2023-04-20;Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs;"Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such
as floaters or flawed geometry when rendered outside the camera trajectory.
Existing evaluation protocols often do not capture these effects, since they
usually only assess image quality at every 8th frame of the training capture.
To push forward progress in novel-view synthesis, we propose a new dataset and
evaluation procedure, where two camera trajectories are recorded of the scene:
one used for training, and the other for evaluation. In this more challenging
in-the-wild setting, we find that existing hand-crafted regularizers do not
remove floaters nor improve scene geometry. Thus, we propose a 3D
diffusion-based method that leverages local 3D priors and a novel density-based
score distillation sampling loss to discourage artifacts during NeRF
optimization. We show that this data-driven prior removes floaters and improves
scene geometry for casual captures.";Frederik Warburg<author:sep>Ethan Weber<author:sep>Matthew Tancik<author:sep>Aleksander Holynski<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2304.10532v3;cs.CV;ICCV 2023, project page: https://ethanweber.me/nerfbusters;nerf
2304.10537v1;http://arxiv.org/abs/2304.10537v1;2023-04-20;Learning Neural Duplex Radiance Fields for Real-Time View Synthesis;"Neural radiance fields (NeRFs) enable novel view synthesis with unprecedented
visual quality. However, to render photorealistic images, NeRFs require
hundreds of deep multilayer perceptron (MLP) evaluations - for each pixel. This
is prohibitively expensive and makes real-time rendering infeasible, even on
powerful modern GPUs. In this paper, we propose a novel approach to distill and
bake NeRFs into highly efficient mesh-based neural representations that are
fully compatible with the massively parallel graphics rendering pipeline. We
represent scenes as neural radiance features encoded on a two-layer duplex
mesh, which effectively overcomes the inherent inaccuracies in 3D surface
reconstruction by learning the aggregated radiance information from a reliable
interval of ray-surface intersections. To exploit local geometric relationships
of nearby pixels, we leverage screen-space convolutions instead of the MLPs
used in NeRFs to achieve high-quality appearance. Finally, the performance of
the whole framework is further boosted by a novel multi-view distillation
optimization strategy. We demonstrate the effectiveness and superiority of our
approach via extensive experiments on a range of standard datasets.";Ziyu Wan<author:sep>Christian Richardt<author:sep>Aljaž Božič<author:sep>Chao Li<author:sep>Vijay Rengarajan<author:sep>Seonghyeon Nam<author:sep>Xiaoyu Xiang<author:sep>Tuotuo Li<author:sep>Bo Zhu<author:sep>Rakesh Ranjan<author:sep>Jing Liao;http://arxiv.org/pdf/2304.10537v1;cs.CV;CVPR 2023. Project page: http://raywzy.com/NDRF;nerf
2304.10664v1;http://arxiv.org/abs/2304.10664v1;2023-04-20;A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses  from HoloLens Trajectories and Structure from Motion;"Neural Radiance Fields (NeRFs) are trained using a set of camera poses and
associated images as input to estimate density and color values for each
position. The position-dependent density learning is of particular interest for
photogrammetry, enabling 3D reconstruction by querying and filtering the NeRF
coordinate system based on the object density. While traditional methods like
Structure from Motion are commonly used for camera pose calculation in
pre-processing for NeRFs, the HoloLens offers an interesting interface for
extracting the required input data directly. We present a workflow for
high-resolution 3D reconstructions almost directly from HoloLens data using
NeRFs. Thereby, different investigations are considered: Internal camera poses
from the HoloLens trajectory via a server application, and external camera
poses from Structure from Motion, both with an enhanced variant applied through
pose refinement. Results show that the internal camera poses lead to NeRF
convergence with a PSNR of 25\,dB with a simple rotation around the x-axis and
enable a 3D reconstruction. Pose refinement enables comparable quality compared
to external camera poses, resulting in improved training process with a PSNR of
27\,dB and a better 3D reconstruction. Overall, NeRF reconstructions outperform
the conventional photogrammetric dense reconstruction using Multi-View Stereo
in terms of completeness and level of detail.";Miriam Jäger<author:sep>Patrick Hübner<author:sep>Dennis Haitz<author:sep>Boris Jutzi;http://arxiv.org/pdf/2304.10664v1;cs.CV;"7 pages, 5 figures. Will be published in the ISPRS The International
  Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences";nerf
2304.10261v1;http://arxiv.org/abs/2304.10261v1;2023-04-19;Anything-3D: Towards Single-view Anything Reconstruction in the Wild;"3D reconstruction from a single-RGB image in unconstrained real-world
scenarios presents numerous challenges due to the inherent diversity and
complexity of objects and environments. In this paper, we introduce
Anything-3D, a methodical framework that ingeniously combines a series of
visual-language models and the Segment-Anything object segmentation model to
elevate objects to 3D, yielding a reliable and versatile system for single-view
conditioned 3D reconstruction task. Our approach employs a BLIP model to
generate textural descriptions, utilizes the Segment-Anything model for the
effective extraction of objects of interest, and leverages a text-to-image
diffusion model to lift object into a neural radiance field. Demonstrating its
ability to produce accurate and detailed 3D reconstructions for a wide array of
objects, \emph{Anything-3D\footnotemark[2]} shows promise in addressing the
limitations of existing methodologies. Through comprehensive experiments and
evaluations on various datasets, we showcase the merits of our approach,
underscoring its potential to contribute meaningfully to the field of 3D
reconstruction. Demos and code will be available at
\href{https://github.com/Anything-of-anything/Anything-3D}{https://github.com/Anything-of-anything/Anything-3D}.";Qiuhong Shen<author:sep>Xingyi Yang<author:sep>Xinchao Wang;http://arxiv.org/pdf/2304.10261v1;cs.CV;;
2304.09987v3;http://arxiv.org/abs/2304.09987v3;2023-04-19;Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra;"Neural Radiance Fields (NeRFs) are a very recent and very popular approach
for the problems of novel view synthesis and 3D reconstruction. A popular scene
representation used by NeRFs is to combine a uniform, voxel-based subdivision
of the scene with an MLP. Based on the observation that a (sparse) point cloud
of the scene is often available, this paper proposes to use an adaptive
representation based on tetrahedra obtained by Delaunay triangulation instead
of uniform subdivision or point-based representations. We show that such a
representation enables efficient training and leads to state-of-the-art
results. Our approach elegantly combines concepts from 3D geometry processing,
triangle-based rendering, and modern neural radiance fields. Compared to
voxel-based representations, ours provides more detail around parts of the
scene likely to be close to the surface. Compared to point-based
representations, our approach achieves better performance. The source code is
publicly available at: https://jkulhanek.com/tetra-nerf.";Jonas Kulhanek<author:sep>Torsten Sattler;http://arxiv.org/pdf/2304.09987v3;cs.CV;ICCV 2023, Web: https://jkulhanek.com/tetra-nerf;nerf
2304.09677v2;http://arxiv.org/abs/2304.09677v2;2023-04-19;Reference-guided Controllable Inpainting of Neural Radiance Fields;"The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led
to a desire for NeRF editing tools. Here, we focus on inpainting regions in a
view-consistent and controllable manner. In addition to the typical NeRF inputs
and masks delineating the unwanted region in each view, we require only a
single inpainted view of the scene, i.e., a reference view. We use monocular
depth estimators to back-project the inpainted view to the correct 3D
positions. Then, via a novel rendering technique, a bilateral solver can
construct view-dependent effects in non-reference views, making the inpainted
region appear consistent from any view. For non-reference disoccluded regions,
which cannot be supervised by the single reference view, we devise a method
based on image inpainters to guide both the geometry and appearance. Our
approach shows superior performance to NeRF inpainting baselines, with the
additional advantage that a user can control the generated scene via a single
inpainted image. Project page: https://ashmrz.github.io/reference-guided-3d";Ashkan Mirzaei<author:sep>Tristan Aumentado-Armstrong<author:sep>Marcus A. Brubaker<author:sep>Jonathan Kelly<author:sep>Alex Levinshtein<author:sep>Konstantinos G. Derpanis<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2304.09677v2;cs.CV;Project Page: https://ashmrz.github.io/reference-guided-3d;nerf
2304.08971v1;http://arxiv.org/abs/2304.08971v1;2023-04-18;SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic  Reconstruction of Indoor Scenes;"Online reconstructing and rendering of large-scale indoor scenes is a
long-standing challenge. SLAM-based methods can reconstruct 3D scene geometry
progressively in real time but can not render photorealistic results. While
NeRF-based methods produce promising novel view synthesis results, their long
offline optimization time and lack of geometric constraints pose challenges to
efficiently handling online input. Inspired by the complementary advantages of
classical 3D reconstruction and NeRF, we thus investigate marrying explicit
geometric representation with NeRF rendering to achieve efficient online
reconstruction and high-quality rendering. We introduce SurfelNeRF, a variant
of neural radiance field which employs a flexible and scalable neural surfel
representation to store geometric attributes and extracted appearance features
from input images. We further extend the conventional surfel-based fusion
scheme to progressively integrate incoming input frames into the reconstructed
global neural scene representation. In addition, we propose a highly-efficient
differentiable rasterization scheme for rendering neural surfel radiance
fields, which helps SurfelNeRF achieve $10\times$ speedups in training and
inference time, respectively. Experimental results show that our method
achieves the state-of-the-art 23.82 PSNR and 29.58 PSNR on ScanNet in
feedforward inference and per-scene optimization settings, respectively.";Yiming Gao<author:sep>Yan-Pei Cao<author:sep>Ying Shan;http://arxiv.org/pdf/2304.08971v1;cs.CV;To appear in CVPR 2023;nerf
2304.08757v1;http://arxiv.org/abs/2304.08757v1;2023-04-18;NeAI: A Pre-convoluted Representation for Plug-and-Play Neural Ambient  Illumination;"Recent advances in implicit neural representation have demonstrated the
ability to recover detailed geometry and material from multi-view images.
However, the use of simplified lighting models such as environment maps to
represent non-distant illumination, or using a network to fit indirect light
modeling without a solid basis, can lead to an undesirable decomposition
between lighting and material. To address this, we propose a fully
differentiable framework named neural ambient illumination (NeAI) that uses
Neural Radiance Fields (NeRF) as a lighting model to handle complex lighting in
a physically based way. Together with integral lobe encoding for
roughness-adaptive specular lobe and leveraging the pre-convoluted background
for accurate decomposition, the proposed method represents a significant step
towards integrating physically based rendering into the NeRF representation.
The experiments demonstrate the superior performance of novel-view rendering
compared to previous works, and the capability to re-render objects under
arbitrary NeRF-style environments opens up exciting possibilities for bridging
the gap between virtual and real-world scenes. The project and supplementary
materials are available at https://yiyuzhuang.github.io/NeAI/.";Yiyu Zhuang<author:sep>Qi Zhang<author:sep>Xuan Wang<author:sep>Hao Zhu<author:sep>Ying Feng<author:sep>Xiaoyu Li<author:sep>Ying Shan<author:sep>Xun Cao;http://arxiv.org/pdf/2304.08757v1;cs.CV;"Project page: <a class=""link-external link-https""
  href=""https://yiyuzhuang.github.io/NeAI/"" rel=""external noopener
  nofollow"">https://yiyuzhuang.github.io/NeAI/</a>";nerf
2304.07979v1;http://arxiv.org/abs/2304.07979v1;2023-04-17;NeRF-Loc: Visual Localization with Conditional Neural Radiance Field;"We propose a novel visual re-localization method based on direct matching
between the implicit 3D descriptors and the 2D image with transformer. A
conditional neural radiance field(NeRF) is chosen as the 3D scene
representation in our pipeline, which supports continuous 3D descriptors
generation and neural rendering. By unifying the feature matching and the scene
coordinate regression to the same framework, our model learns both
generalizable knowledge and scene prior respectively during two training
stages. Furthermore, to improve the localization robustness when domain gap
exists between training and testing phases, we propose an appearance adaptation
layer to explicitly align styles between the 3D model and the query image.
Experiments show that our method achieves higher localization accuracy than
other learning-based approaches on multiple benchmarks. Code is available at
\url{https://github.com/JenningsL/nerf-loc}.";Jianlin Liu<author:sep>Qiang Nie<author:sep>Yong Liu<author:sep>Chengjie Wang;http://arxiv.org/pdf/2304.07979v1;cs.CV;accepted by ICRA 2023;nerf
2304.08279v2;http://arxiv.org/abs/2304.08279v2;2023-04-17;MoDA: Modeling Deformable 3D Objects from Casual Videos;"In this paper, we focus on the challenges of modeling deformable 3D objects
from casual videos. With the popularity of neural radiance fields (NeRF), many
works extend it to dynamic scenes with a canonical NeRF and a deformation model
that achieves 3D point transformation between the observation space and the
canonical space. Recent works rely on linear blend skinning (LBS) to achieve
the canonical-observation transformation. However, the linearly weighted
combination of rigid transformation matrices is not guaranteed to be rigid. As
a matter of fact, unexpected scale and shear factors often appear. In practice,
using LBS as the deformation model can always lead to skin-collapsing artifacts
for bending or twisting motions. To solve this problem, we propose neural dual
quaternion blend skinning (NeuDBS) to achieve 3D point deformation, which can
perform rigid transformation without skin-collapsing artifacts. In the endeavor
to register 2D pixels across different frames, we establish a correspondence
between canonical feature embeddings that encodes 3D points within the
canonical space, and 2D image features by solving an optimal transport problem.
Besides, we introduce a texture filtering approach for texture rendering that
effectively minimizes the impact of noisy colors outside target deformable
objects. Extensive experiments on real and synthetic datasets show that our
approach can reconstruct 3D models for humans and animals with better
qualitative and quantitative performance than state-of-the-art methods.";Chaoyue Song<author:sep>Tianyi Chen<author:sep>Yiwen Chen<author:sep>Jiacheng Wei<author:sep>Chuan Sheng Foo<author:sep>Fayao Liu<author:sep>Guosheng Lin;http://arxiv.org/pdf/2304.08279v2;cs.CV;;nerf
2304.07743v1;http://arxiv.org/abs/2304.07743v1;2023-04-16;SeaThru-NeRF: Neural Radiance Fields in Scattering Media;"Research on neural radiance fields (NeRFs) for novel view generation is
exploding with new models and extensions. However, a question that remains
unanswered is what happens in underwater or foggy scenes where the medium
strongly influences the appearance of objects. Thus far, NeRF and its variants
have ignored these cases. However, since the NeRF framework is based on
volumetric rendering, it has inherent capability to account for the medium's
effects, once modeled appropriately. We develop a new rendering model for NeRFs
in scattering media, which is based on the SeaThru image formation model, and
suggest a suitable architecture for learning both scene information and medium
parameters. We demonstrate the strength of our method using simulated and
real-world scenes, correctly rendering novel photorealistic views underwater.
Even more excitingly, we can render clear views of these scenes, removing the
medium between the camera and the scene and reconstructing the appearance and
depth of far objects, which are severely occluded by the medium. Our code and
unique datasets are available on the project's website.";Deborah Levy<author:sep>Amit Peleg<author:sep>Naama Pearl<author:sep>Dan Rosenbaum<author:sep>Derya Akkaynak<author:sep>Simon Korman<author:sep>Tali Treibitz;http://arxiv.org/pdf/2304.07743v1;cs.CV;;nerf
2304.07915v1;http://arxiv.org/abs/2304.07915v1;2023-04-16;CAT-NeRF: Constancy-Aware Tx$^2$Former for Dynamic Body Modeling;"This paper addresses the problem of human rendering in the video with
temporal appearance constancy. Reconstructing dynamic body shapes with
volumetric neural rendering methods, such as NeRF, requires finding the
correspondence of the points in the canonical and observation space, which
demands understanding human body shape and motion. Some methods use rigid
transformation, such as SE(3), which cannot precisely model each frame's unique
motion and muscle movements. Others generate the transformation for each frame
with a trainable network, such as neural blend weight field or translation
vector field, which does not consider the appearance constancy of general body
shape. In this paper, we propose CAT-NeRF for self-awareness of appearance
constancy with Tx$^2$Former, a novel way to combine two Transformer layers, to
separate appearance constancy and uniqueness. Appearance constancy models the
general shape across the video, and uniqueness models the unique patterns for
each frame. We further introduce a novel Covariance Loss to limit the
correlation between each pair of appearance uniquenesses to ensure the
frame-unique pattern is maximally captured in appearance uniqueness. We assess
our method on H36M and ZJU-MoCap and show state-of-the-art performance.";Haidong Zhu<author:sep>Zhaoheng Zheng<author:sep>Wanrong Zheng<author:sep>Ram Nevatia;http://arxiv.org/pdf/2304.07915v1;cs.CV;;nerf
2304.07918v1;http://arxiv.org/abs/2304.07918v1;2023-04-16;Likelihood-Based Generative Radiance Field with Latent Space  Energy-Based Model for 3D-Aware Disentangled Image Representation;"We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image
generative model that incorporates 3D representation via Neural Radiance Fields
(NeRF) and 2D imaging process via differentiable volume rendering. The model
represents an image as a rendering process from 3D object to 2D image and is
conditioned on some latent variables that account for object characteristics
and are assumed to follow informative trainable energy-based prior models. We
propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i)
maximum likelihood estimation with Markov chain Monte Carlo-based inference and
(ii) variational inference with the reparameterization trick. We study our
models in the scenarios with both known and unknown camera poses. Experiments
on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D
object structures from 2D images, generate 2D images with novel views and
objects, learn from incomplete 2D images, and learn from 2D images with known
or unknown camera poses.";Yaxuan Zhu<author:sep>Jianwen Xie<author:sep>Ping Li;http://arxiv.org/pdf/2304.07918v1;cs.CV;;nerf
2304.06969v1;http://arxiv.org/abs/2304.06969v1;2023-04-14;UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose  rendering, Geometry and Texture Editing;"Neural radiance field (NeRF) has become a popular 3D representation method
for human avatar reconstruction due to its high-quality rendering capabilities,
e.g., regarding novel views and poses. However, previous methods for editing
the geometry and appearance of the avatar only allow for global editing through
body shape parameters and 2D texture maps. In this paper, we propose a new
approach named \textbf{U}nified \textbf{V}olumetric \textbf{A}vatar
(\textbf{UVA}) that enables local and independent editing of both geometry and
texture, while retaining the ability to render novel views and poses. UVA
transforms each observation point to a canonical space using a skinning motion
field and represents geometry and texture in separate neural fields. Each field
is composed of a set of structured latent codes that are attached to anchor
nodes on a deformable mesh in canonical space and diffused into the entire
space via interpolation, allowing for local editing. To address spatial
ambiguity in code interpolation, we use a local signed height indicator. We
also replace the view-dependent radiance color with a pose-dependent shading
factor to better represent surface illumination in different poses. Experiments
on multiple human avatars demonstrate that our UVA achieves competitive results
in novel view synthesis and novel pose rendering while enabling local and
independent editing of geometry and appearance. The source code will be
released.";Jinlong Fan<author:sep>Jing Zhang<author:sep>Dacheng Tao;http://arxiv.org/pdf/2304.06969v1;cs.CV;;nerf
2304.06287v2;http://arxiv.org/abs/2304.06287v2;2023-04-13;NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry  Scaffolds;"We present NeRFVS, a novel neural radiance fields (NeRF) based method to
enable free navigation in a room. NeRF achieves impressive performance in
rendering images for novel views similar to the input views while suffering for
novel views that are significantly different from the training views. To
address this issue, we utilize the holistic priors, including pseudo depth maps
and view coverage information, from neural reconstruction to guide the learning
of implicit neural representations of 3D indoor scenes. Concretely, an
off-the-shelf neural reconstruction method is leveraged to generate a geometry
scaffold. Then, two loss functions based on the holistic priors are proposed to
improve the learning of NeRF: 1) A robust depth loss that can tolerate the
error of the pseudo depth map to guide the geometry learning of NeRF; 2) A
variance loss to regularize the variance of implicit neural representations to
reduce the geometry and color ambiguity in the learning procedure. These two
loss functions are modulated during NeRF optimization according to the view
coverage information to reduce the negative influence brought by the view
coverage imbalance. Extensive results demonstrate that our NeRFVS outperforms
state-of-the-art view synthesis methods quantitatively and qualitatively on
indoor scenes, achieving high-fidelity free navigation results.";Chen Yang<author:sep>Peihao Li<author:sep>Zanwei Zhou<author:sep>Shanxin Yuan<author:sep>Bingbing Liu<author:sep>Xiaokang Yang<author:sep>Weichao Qiu<author:sep>Wei Shen;http://arxiv.org/pdf/2304.06287v2;cs.CV;10 pages, 7 figures;nerf
2304.06714v4;http://arxiv.org/abs/2304.06714v4;2023-04-13;Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and  Reconstruction;"3D-aware image synthesis encompasses a variety of tasks, such as scene
generation and novel view synthesis from images. Despite numerous task-specific
methods, developing a comprehensive model remains challenging. In this paper,
we present SSDNeRF, a unified approach that employs an expressive diffusion
model to learn a generalizable prior of neural radiance fields (NeRF) from
multi-view images of diverse objects. Previous studies have used two-stage
approaches that rely on pretrained NeRFs as real data to train diffusion
models. In contrast, we propose a new single-stage training paradigm with an
end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent
diffusion model, enabling simultaneous 3D reconstruction and prior learning,
even from sparsely available views. At test time, we can directly sample the
diffusion prior for unconditional generation, or combine it with arbitrary
observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates
robust results comparable to or better than leading task-specific methods in
unconditional generation and single/sparse-view 3D reconstruction.";Hansheng Chen<author:sep>Jiatao Gu<author:sep>Anpei Chen<author:sep>Wei Tian<author:sep>Zhuowen Tu<author:sep>Lingjie Liu<author:sep>Hao Su;http://arxiv.org/pdf/2304.06714v4;cs.CV;"ICCV 2023 final version. Project page:
  https://lakonik.github.io/ssdnerf";nerf
2304.06706v3;http://arxiv.org/abs/2304.06706v3;2023-04-13;Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields;"Neural Radiance Field training can be accelerated through the use of
grid-based representations in NeRF's learned mapping from spatial coordinates
to colors and volumetric density. However, these grid-based approaches lack an
explicit understanding of scale and therefore often introduce aliasing, usually
in the form of jaggies or missing scene content. Anti-aliasing has previously
been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone
rather than points along a ray, but this approach is not natively compatible
with current grid-based techniques. We show how ideas from rendering and signal
processing can be used to construct a technique that combines mip-NeRF 360 and
grid-based models such as Instant NGP to yield error rates that are 8% - 77%
lower than either prior technique, and that trains 24x faster than mip-NeRF
360.";Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Dor Verbin<author:sep>Pratul P. Srinivasan<author:sep>Peter Hedman;http://arxiv.org/pdf/2304.06706v3;cs.CV;Project page: https://jonbarron.info/zipnerf/;nerf
2304.05735v2;http://arxiv.org/abs/2304.05735v2;2023-04-12;RO-MAP: Real-Time Multi-Object Mapping with Neural Radiance Fields;"Accurate perception of objects in the environment is important for improving
the scene understanding capability of SLAM systems. In robotic and augmented
reality applications, object maps with semantic and metric information show
attractive advantages. In this paper, we present RO-MAP, a novel multi-object
mapping pipeline that does not rely on 3D priors. Given only monocular input,
we use neural radiance fields to represent objects and couple them with a
lightweight object SLAM based on multi-view geometry, to simultaneously
localize objects and implicitly learn their dense geometry. We create separate
implicit models for each detected object and train them dynamically and in
parallel as new observations are added. Experiments on synthetic and real-world
datasets demonstrate that our method can generate semantic object map with
shape reconstruction, and be competitive with offline methods while achieving
real-time performance (25Hz). The code and dataset will be available at:
https://github.com/XiaoHan-Git/RO-MAP";Xiao Han<author:sep>Houxuan Liu<author:sep>Yunchao Ding<author:sep>Lu Yang;http://arxiv.org/pdf/2304.05735v2;cs.RO;"The code and dataset are available at:
  https://github.com/XiaoHan-Git/RO-MAP";
2304.05620v1;http://arxiv.org/abs/2304.05620v1;2023-04-12;NutritionVerse-Thin: An Optimized Strategy for Enabling Improved  Rendering of 3D Thin Food Models;"With the growth in capabilities of generative models, there has been growing
interest in using photo-realistic renders of common 3D food items to improve
downstream tasks such as food printing, nutrition prediction, or management of
food wastage. Despite 3D modelling capabilities being more accessible than ever
due to the success of NeRF based view-synthesis, such rendering methods still
struggle to correctly capture thin food objects, often generating meshes with
significant holes. In this study, we present an optimized strategy for enabling
improved rendering of thin 3D food models, and demonstrate qualitative
improvements in rendering quality. Our method generates the 3D model mesh via a
proposed thin-object-optimized differentiable reconstruction method and tailors
the strategy at both the data collection and training stages to better handle
thin objects. While simple, we find that this technique can be employed for
quick and highly consistent capturing of thin 3D objects.";Chi-en Amy Tai<author:sep>Jason Li<author:sep>Sriram Kumar<author:sep>Saeejith Nair<author:sep>Yuhao Chen<author:sep>Pengcheng Xi<author:sep>Alexander Wong;http://arxiv.org/pdf/2304.05620v1;cs.CV;;nerf
2304.05218v1;http://arxiv.org/abs/2304.05218v1;2023-04-11;Improving Neural Radiance Fields with Depth-aware Optimization for Novel  View Synthesis;"With dense inputs, Neural Radiance Fields (NeRF) is able to render
photo-realistic novel views under static conditions. Although the synthesis
quality is excellent, existing NeRF-based methods fail to obtain moderate
three-dimensional (3D) structures. The novel view synthesis quality drops
dramatically given sparse input due to the implicitly reconstructed inaccurate
3D-scene structure. We propose SfMNeRF, a method to better synthesize novel
views as well as reconstruct the 3D-scene geometry. SfMNeRF leverages the
knowledge from the self-supervised depth estimation methods to constrain the
3D-scene geometry during view synthesis training. Specifically, SfMNeRF employs
the epipolar, photometric consistency, depth smoothness, and
position-of-matches constraints to explicitly reconstruct the 3D-scene
structure. Through these explicit constraints and the implicit constraint from
NeRF, our method improves the view synthesis as well as the 3D-scene geometry
performance of NeRF at the same time. In addition, SfMNeRF synthesizes novel
sub-pixels in which the ground truth is obtained by image interpolation. This
strategy enables SfMNeRF to include more samples to improve generalization
performance. Experiments on two public datasets demonstrate that SfMNeRF
surpasses state-of-the-art approaches. Code is available at
https://github.com/XTU-PR-LAB/SfMNeRF";Shu Chen<author:sep>Junyao Li<author:sep>Yang Zhang<author:sep>Beiji Zou;http://arxiv.org/pdf/2304.05218v1;cs.CV;;nerf
2304.04962v1;http://arxiv.org/abs/2304.04962v1;2023-04-11;MRVM-NeRF: Mask-Based Pretraining for Neural Radiance Fields;"Most Neural Radiance Fields (NeRFs) have poor generalization ability,
limiting their application when representing multiple scenes by a single model.
To ameliorate this problem, existing methods simply condition NeRF models on
image features, lacking the global understanding and modeling of the entire 3D
scene. Inspired by the significant success of mask-based modeling in other
research fields, we propose a masked ray and view modeling method for
generalizable NeRF (MRVM-NeRF), the first attempt to incorporate mask-based
pretraining into 3D implicit representations. Specifically, considering that
the core of NeRFs lies in modeling 3D representations along the rays and across
the views, we randomly mask a proportion of sampled points along the ray at
fine stage by discarding partial information obtained from multi-viewpoints,
targeting at predicting the corresponding features produced in the coarse
branch. In this way, the learned prior knowledge of 3D scenes during
pretraining helps the model generalize better to novel scenarios after
finetuning. Extensive experiments demonstrate the superiority of our proposed
MRVM-NeRF under various synthetic and real-world settings, both qualitatively
and quantitatively. Our empirical studies reveal the effectiveness of our
proposed innovative MRVM which is specifically designed for NeRF models.";Ganlin Yang<author:sep>Guoqiang Wei<author:sep>Zhizheng Zhang<author:sep>Yan Lu<author:sep>Dong Liu;http://arxiv.org/pdf/2304.04962v1;cs.CV;;nerf
2304.05097v1;http://arxiv.org/abs/2304.05097v1;2023-04-11;One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural  Radiance Field;"Talking head generation aims to generate faces that maintain the identity
information of the source image and imitate the motion of the driving image.
Most pioneering methods rely primarily on 2D representations and thus will
inevitably suffer from face distortion when large head rotations are
encountered. Recent works instead employ explicit 3D structural representations
or implicit neural rendering to improve performance under large pose changes.
Nevertheless, the fidelity of identity and expression is not so desirable,
especially for novel-view synthesis. In this paper, we propose HiDe-NeRF, which
achieves high-fidelity and free-view talking-head synthesis. Drawing on the
recently proposed Deformable Neural Radiance Fields, HiDe-NeRF represents the
3D dynamic scene into a canonical appearance field and an implicit deformation
field, where the former comprises the canonical source face and the latter
models the driving pose and expression. In particular, we improve fidelity from
two aspects: (i) to enhance identity expressiveness, we design a generalized
appearance module that leverages multi-scale volume features to preserve face
shape and details; (ii) to improve expression preciseness, we propose a
lightweight deformation module that explicitly decouples the pose and
expression to enable precise expression modeling. Extensive experiments
demonstrate that our proposed approach can generate better results than
previous works. Project page: https://www.waytron.net/hidenerf/";Weichuang Li<author:sep>Longhao Zhang<author:sep>Dong Wang<author:sep>Bin Zhao<author:sep>Zhigang Wang<author:sep>Mulin Chen<author:sep>Bang Zhang<author:sep>Zhongjian Wang<author:sep>Liefeng Bo<author:sep>Xuelong Li;http://arxiv.org/pdf/2304.05097v1;cs.CV;Accepted by CVPR 2023;nerf
2304.04897v1;http://arxiv.org/abs/2304.04897v1;2023-04-10;Neural Image-based Avatars: Generalizable Radiance Fields for Human  Avatar Modeling;"We present a method that enables synthesizing novel views and novel poses of
arbitrary human performers from sparse multi-view images. A key ingredient of
our method is a hybrid appearance blending module that combines the advantages
of the implicit body NeRF representation and image-based rendering. Existing
generalizable human NeRF methods that are conditioned on the body model have
shown robustness against the geometric variation of arbitrary human performers.
Yet they often exhibit blurry results when generalized onto unseen identities.
Meanwhile, image-based rendering shows high-quality results when sufficient
observations are available, whereas it suffers artifacts in sparse-view
settings. We propose Neural Image-based Avatars (NIA) that exploits the best of
those two methods: to maintain robustness under new articulations and
self-occlusions while directly leveraging the available (sparse) source view
colors to preserve appearance details of new subject identities. Our hybrid
design outperforms recent methods on both in-domain identity generalization as
well as challenging cross-dataset generalization settings. Also, in terms of
the pose generalization, our method outperforms even the per-subject optimized
animatable NeRF methods. The video results are available at
https://youngjoongunc.github.io/nia";Youngjoong Kwon<author:sep>Dahun Kim<author:sep>Duygu Ceylan<author:sep>Henry Fuchs;http://arxiv.org/pdf/2304.04897v1;cs.CV;;nerf
2304.04452v2;http://arxiv.org/abs/2304.04452v2;2023-04-10;Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos;"The success of the Neural Radiance Fields (NeRFs) for modeling and free-view
rendering static objects has inspired numerous attempts on dynamic scenes.
Current techniques that utilize neural rendering for facilitating free-view
videos (FVVs) are restricted to either offline rendering or are capable of
processing only brief sequences with minimal motion. In this paper, we present
a novel technique, Residual Radiance Field or ReRF, as a highly compact neural
representation to achieve real-time FVV rendering on long-duration dynamic
scenes. ReRF explicitly models the residual information between adjacent
timestamps in the spatial-temporal feature space, with a global
coordinate-based tiny MLP as the feature decoder. Specifically, ReRF employs a
compact motion grid along with a residual feature grid to exploit inter-frame
feature similarities. We show such a strategy can handle large motions without
sacrificing quality. We further present a sequential training scheme to
maintain the smoothness and the sparsity of the motion/residual grids. Based on
ReRF, we design a special FVV codec that achieves three orders of magnitudes
compression rate and provides a companion ReRF player to support online
streaming of long-duration FVVs of dynamic scenes. Extensive experiments
demonstrate the effectiveness of ReRF for compactly representing dynamic
radiance fields, enabling an unprecedented free-viewpoint viewing experience in
speed and quality.";Liao Wang<author:sep>Qiang Hu<author:sep>Qihan He<author:sep>Ziyu Wang<author:sep>Jingyi Yu<author:sep>Tinne Tuytelaars<author:sep>Lan Xu<author:sep>Minye Wu;http://arxiv.org/pdf/2304.04452v2;cs.CV;"Accepted by CVPR 2023. Project page, see
  https://aoliao12138.github.io/ReRF/";nerf
2304.04395v3;http://arxiv.org/abs/2304.04395v3;2023-04-10;Instance Neural Radiance Field;"This paper presents one of the first learning-based NeRF 3D instance
segmentation pipelines, dubbed as Instance Neural Radiance Field, or Instance
NeRF. Taking a NeRF pretrained from multi-view RGB images as input, Instance
NeRF can learn 3D instance segmentation of a given scene, represented as an
instance field component of the NeRF model. To this end, we adopt a 3D
proposal-based mask prediction network on the sampled volumetric features from
NeRF, which generates discrete 3D instance masks. The coarse 3D mask prediction
is then projected to image space to match 2D segmentation masks from different
views generated by existing panoptic segmentation models, which are used to
supervise the training of the instance field. Notably, beyond generating
consistent 2D segmentation maps from novel views, Instance NeRF can query
instance information at any 3D point, which greatly enhances NeRF object
segmentation and manipulation. Our method is also one of the first to achieve
such results in pure inference. Experimented on synthetic and real-world NeRF
datasets with complex indoor scenes, Instance NeRF surpasses previous NeRF
segmentation works and competitive 2D segmentation methods in segmentation
performance on unseen views. Watch the demo video at
https://youtu.be/wW9Bme73coI. Code and data are available at
https://github.com/lyclyc52/Instance_NeRF.";Yichen Liu<author:sep>Benran Hu<author:sep>Junkai Huang<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2304.04395v3;cs.CV;International Conference on Computer Vision (ICCV) 2023;nerf
2304.04446v1;http://arxiv.org/abs/2304.04446v1;2023-04-10;Inferring Fluid Dynamics via Inverse Rendering;"Humans have a strong intuitive understanding of physical processes such as
fluid falling by just a glimpse of such a scene picture, i.e., quickly derived
from our immersive visual experiences in memory. This work achieves such a
photo-to-fluid-dynamics reconstruction functionality learned from unannotated
videos, without any supervision of ground-truth fluid dynamics. In a nutshell,
a differentiable Euler simulator modeled with a ConvNet-based pressure
projection solver, is integrated with a volumetric renderer, supporting
end-to-end/coherent differentiable dynamic simulation and rendering. By
endowing each sampled point with a fluid volume value, we derive a NeRF-like
differentiable renderer dedicated from fluid data; and thanks to this
volume-augmented representation, fluid dynamics could be inversely inferred
from the error signal between the rendered result and ground-truth video frame
(i.e., inverse rendering). Experiments on our generated Fluid Fall datasets and
DPI Dam Break dataset are conducted to demonstrate both effectiveness and
generalization ability of our method.";Jinxian Liu<author:sep>Ye Chen<author:sep>Bingbing Ni<author:sep>Jiyao Mao<author:sep>Zhenbo Yu;http://arxiv.org/pdf/2304.04446v1;cs.CV;;nerf
2304.04133v4;http://arxiv.org/abs/2304.04133v4;2023-04-09;NeRF applied to satellite imagery for surface reconstruction;"We present Surf-NeRF, a modified implementation of the recently introduced
Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize
novel views from a sparse set of satellite images of a scene, while accounting
for the variation in lighting present in the pictures. The trained model can
also be used to accurately estimate the surface elevation of the scene, which
is often a desirable quantity for satellite observation applications. S-NeRF
improves on the standard Neural Radiance Field (NeRF) method by considering the
radiance as a function of the albedo and the irradiance. Both these quantities
are output by fully connected neural network branches of the model, and the
latter is considered as a function of the direct light from the sun and the
diffuse color from the sky. The implementations were run on a dataset of
satellite images, augmented using a zoom-and-crop technique. A hyperparameter
study for NeRF was carried out, leading to intriguing observations on the
model's convergence. Finally, both NeRF and S-NeRF were run until 100k epochs
in order to fully fit the data and produce their best possible predictions. The
code related to this article can be found at
https://github.com/fsemerar/surfnerf.";Federico Semeraro<author:sep>Yi Zhang<author:sep>Wenying Wu<author:sep>Patrick Carroll;http://arxiv.org/pdf/2304.04133v4;cs.CV;;nerf
2304.04012v1;http://arxiv.org/abs/2304.04012v1;2023-04-08;PVD-AL: Progressive Volume Distillation with Active Learning for  Efficient Conversion Between Different NeRF Architectures;"Neural Radiance Fields (NeRF) have been widely adopted as practical and
versatile representations for 3D scenes, facilitating various downstream tasks.
However, different architectures, including plain Multi-Layer Perceptron (MLP),
Tensors, low-rank Tensors, Hashtables, and their compositions, have their
trade-offs. For instance, Hashtables-based representations allow for faster
rendering but lack clear geometric meaning, making spatial-relation-aware
editing challenging. To address this limitation and maximize the potential of
each architecture, we propose Progressive Volume Distillation with Active
Learning (PVD-AL), a systematic distillation method that enables any-to-any
conversions between different architectures. PVD-AL decomposes each structure
into two parts and progressively performs distillation from shallower to deeper
volume representation, leveraging effective information retrieved from the
rendering process. Additionally, a Three-Levels of active learning technique
provides continuous feedback during the distillation process, resulting in
high-performance results. Empirical evidence is presented to validate our
method on multiple benchmark datasets. For example, PVD-AL can distill an
MLP-based model from a Hashtables-based model at a 10~20X faster speed and
0.8dB~2dB higher PSNR than training the NeRF model from scratch. Moreover,
PVD-AL permits the fusion of diverse features among distinct structures,
enabling models with multiple editing properties and providing a more efficient
model to meet real-time requirements. Project website:http://sk-fun.fun/PVD-AL.";Shuangkang Fang<author:sep>Yufeng Wang<author:sep>Yi Yang<author:sep>Weixin Xu<author:sep>Heng Wang<author:sep>Wenrui Ding<author:sep>Shuchang Zhou;http://arxiv.org/pdf/2304.04012v1;cs.CV;"Project website: http://sk-fun.fun/PVD-AL. arXiv admin note:
  substantial text overlap with arXiv:2211.15977";nerf
2304.03526v1;http://arxiv.org/abs/2304.03526v1;2023-04-07;Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative  Radiance Field;"This work explores the use of 3D generative models to synthesize training
data for 3D vision tasks. The key requirements of the generative models are
that the generated data should be photorealistic to match the real-world
scenarios, and the corresponding 3D attributes should be aligned with given
sampling labels. However, we find that the recent NeRF-based 3D GANs hardly
meet the above requirements due to their designed generation pipeline and the
lack of explicit 3D supervision. In this work, we propose Lift3D, an inverted
2D-to-3D generation framework to achieve the data generation objectives. Lift3D
has several merits compared to prior methods: (1) Unlike previous 3D GANs that
the output resolution is fixed after training, Lift3D can generalize to any
camera intrinsic with higher resolution and photorealistic output. (2) By
lifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3D
information of generated objects, thus offering accurate 3D annotations for
downstream tasks. We evaluate the effectiveness of our framework by augmenting
autonomous driving datasets. Experimental results demonstrate that our data
generation framework can effectively improve the performance of 3D object
detectors. Project page: https://len-li.github.io/lift3d-web.";Leheng Li<author:sep>Qing Lian<author:sep>Luozhou Wang<author:sep>Ningning Ma<author:sep>Ying-Cong Chen;http://arxiv.org/pdf/2304.03526v1;cs.CV;CVPR 2023;nerf
2304.04559v1;http://arxiv.org/abs/2304.04559v1;2023-04-07;Event-based Camera Tracker by $\nabla$t NeRF;"When a camera travels across a 3D world, only a fraction of pixel value
changes; an event-based camera observes the change as sparse events. How can we
utilize sparse events for efficient recovery of the camera pose? We show that
we can recover the camera pose by minimizing the error between sparse events
and the temporal gradient of the scene represented as a neural radiance field
(NeRF). To enable the computation of the temporal gradient of the scene, we
augment NeRF's camera pose as a time function. When the input pose to the NeRF
coincides with the actual pose, the output of the temporal gradient of NeRF
equals the observed intensity changes on the event's points. Using this
principle, we propose an event-based camera pose tracking framework called
TeGRA which realizes the pose update by using the sparse event's observation.
To the best of our knowledge, this is the first camera pose estimation
algorithm using the scene's implicit representation and the sparse intensity
change from events.";Mana Masuda<author:sep>Yusuke Sekikawa<author:sep>Hideo Saito;http://arxiv.org/pdf/2304.04559v1;cs.CV;;nerf
2304.03384v2;http://arxiv.org/abs/2304.03384v2;2023-04-06;Beyond NeRF Underwater: Learning Neural Reflectance Fields for True  Color Correction of Marine Imagery;"Underwater imagery often exhibits distorted coloration as a result of
light-water interactions, which complicates the study of benthic environments
in marine biology and geography. In this research, we propose an algorithm to
restore the true color (albedo) in underwater imagery by jointly learning the
effects of the medium and neural scene representations. Our approach models
water effects as a combination of light attenuation with distance and
backscattered light. The proposed neural scene representation is based on a
neural reflectance field model, which learns albedos, normals, and volume
densities of the underwater environment. We introduce a logistic regression
model to separate water from the scene and apply distinct light physics during
training. Our method avoids the need to estimate complex backscatter effects in
water by employing several approximations, enhancing sampling efficiency and
numerical stability during training. The proposed technique integrates
underwater light effects into a volume rendering framework with end-to-end
differentiability. Experimental results on both synthetic and real-world data
demonstrate that our method effectively restores true color from underwater
imagery, outperforming existing approaches in terms of color consistency.";Tianyi Zhang<author:sep>Matthew Johnson-Roberson;http://arxiv.org/pdf/2304.03384v2;cs.CV;Robotics and Automation Letters (RA-L) VOL. 8, NO. 10, OCTOBER 2023;nerf
2304.03280v1;http://arxiv.org/abs/2304.03280v1;2023-04-06;LANe: Lighting-Aware Neural Fields for Compositional Scene Synthesis;"Neural fields have recently enjoyed great success in representing and
rendering 3D scenes. However, most state-of-the-art implicit representations
model static or dynamic scenes as a whole, with minor variations. Existing work
on learning disentangled world and object neural fields do not consider the
problem of composing objects into different world neural fields in a
lighting-aware manner. We present Lighting-Aware Neural Field (LANe) for the
compositional synthesis of driving scenes in a physically consistent manner.
Specifically, we learn a scene representation that disentangles the static
background and transient elements into a world-NeRF and class-specific
object-NeRFs to allow compositional synthesis of multiple objects in the scene.
Furthermore, we explicitly designed both the world and object models to handle
lighting variation, which allows us to compose objects into scenes with
spatially varying lighting. This is achieved by constructing a light field of
the scene and using it in conjunction with a learned shader to modulate the
appearance of the object NeRFs. We demonstrate the performance of our model on
a synthetic dataset of diverse lighting conditions rendered with the CARLA
simulator, as well as a novel real-world dataset of cars collected at different
times of the day. Our approach shows that it outperforms state-of-the-art
compositional scene synthesis on the challenging dataset setup, via composing
object-NeRFs learned from one scene into an entirely different scene whilst
still respecting the lighting variations in the novel scene. For more results,
please visit our project website https://lane-composition.github.io/.";Akshay Krishnan<author:sep>Amit Raj<author:sep>Xianling Zhang<author:sep>Alexandra Carlson<author:sep>Nathan Tseng<author:sep>Sandhya Sridhar<author:sep>Nikita Jaipuria<author:sep>James Hays;http://arxiv.org/pdf/2304.03280v1;cs.CV;Project website: https://lane-composition.github.io;nerf
2304.02827v1;http://arxiv.org/abs/2304.02827v1;2023-04-06;DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model;"The increasing demand for high-quality 3D content creation has motivated the
development of automated methods for creating 3D object models from a single
image and/or from a text prompt. However, the reconstructed 3D objects using
state-of-the-art image-to-3D methods still exhibit low correspondence to the
given image and low multi-view consistency. Recent state-of-the-art text-to-3D
methods are also limited, yielding 3D samples with low diversity per prompt
with long synthesis time. To address these challenges, we propose DITTO-NeRF, a
novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a
single image. Our DITTO-NeRF consists of constructing high-quality partial 3D
object for limited in-boundary (IB) angles using the given or text-generated 2D
image from the frontal view and then iteratively reconstructing the remaining
3D NeRF using inpainting latent diffusion model. We propose progressive 3D
object reconstruction schemes in terms of scales (low to high resolution),
angles (IB angles initially to outer-boundary (OB) later), and masks (object to
background boundary) in our DITTO-NeRF so that high-quality information on IB
can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods
in terms of fidelity and diversity qualitatively and quantitatively with much
faster training times than prior arts on image/text-to-3D such as DreamFusion,
and NeuralLift-360.";Hoigi Seo<author:sep>Hayeon Kim<author:sep>Gwanghyun Kim<author:sep>Se Young Chun;http://arxiv.org/pdf/2304.02827v1;cs.CV;Project page: https://janeyeon.github.io/ditto-nerf/;nerf
2304.03266v1;http://arxiv.org/abs/2304.03266v1;2023-04-06;Neural Fields meet Explicit Geometric Representation for Inverse  Rendering of Urban Scenes;"Reconstruction and intrinsic decomposition of scenes from captured imagery
would enable many applications such as relighting and virtual object insertion.
Recent NeRF based methods achieve impressive fidelity of 3D reconstruction, but
bake the lighting and shadows into the radiance field, while mesh-based methods
that facilitate intrinsic decomposition through differentiable rendering have
not yet scaled to the complexity and scale of outdoor scenes. We present a
novel inverse rendering framework for large urban scenes capable of jointly
reconstructing the scene geometry, spatially-varying materials, and HDR
lighting from a set of posed RGB images with optional depth. Specifically, we
use a neural field to account for the primary rays, and use an explicit mesh
(reconstructed from the underlying neural field) for modeling secondary rays
that produce higher-order lighting effects such as cast shadows. By faithfully
disentangling complex geometry and materials from lighting effects, our method
enables photorealistic relighting with specular and shadow effects on several
outdoor datasets. Moreover, it supports physics-based scene manipulations such
as virtual object insertion with ray-traced shadow casting.";Zian Wang<author:sep>Tianchang Shen<author:sep>Jun Gao<author:sep>Shengyu Huang<author:sep>Jacob Munkberg<author:sep>Jon Hasselgren<author:sep>Zan Gojcic<author:sep>Wenzheng Chen<author:sep>Sanja Fidler;http://arxiv.org/pdf/2304.03266v1;cs.CV;CVPR 2023. Project page: https://nv-tlabs.github.io/fegr/;nerf
2304.02736v1;http://arxiv.org/abs/2304.02736v1;2023-04-05;Image Stabilization for Hololens Camera in Remote Collaboration;"With the advent of new technologies, Augmented Reality (AR) has become an
effective tool in remote collaboration. Narrow field-of-view (FoV) and motion
blur can offer an unpleasant experience with limited cognition for remote
viewers of AR headsets. In this article, we propose a two-stage pipeline to
tackle this issue and ensure a stable viewing experience with a larger FoV. The
solution involves an offline 3D reconstruction of the indoor environment,
followed by enhanced rendering using only the live poses of AR device. We
experiment with and evaluate the two different 3D reconstruction methods, RGB-D
geometric approach and Neural Radiance Fields (NeRF), based on their data
requirements, reconstruction quality, rendering, and training times. The
generated sequences from these methods had smoother transitions and provided a
better perspective of the environment. The geometry-based enhanced FoV method
had better renderings as it lacked blurry outputs making it better than the
other attempted approaches. Structural Similarity Index (SSIM) and Peak Signal
to Noise Ratio (PSNR) metrics were used to quantitatively show that the
rendering quality using the geometry-based enhanced FoV method is better. Link
to the code repository -
https://github.com/MixedRealityETHZ/ImageStabilization.";Gowtham Senthil<author:sep>Siva Vignesh Krishnan<author:sep>Annamalai Lakshmanan<author:sep>Florence Kissling;http://arxiv.org/pdf/2304.02736v1;cs.CV;;nerf
2304.01436v1;http://arxiv.org/abs/2304.01436v1;2023-04-04;Learning Personalized High Quality Volumetric Head Avatars from  Monocular RGB Videos;"We propose a method to learn a high-quality implicit 3D head avatar from a
monocular RGB video captured in the wild. The learnt avatar is driven by a
parametric face model to achieve user-controlled facial expressions and head
poses. Our hybrid pipeline combines the geometry prior and dynamic tracking of
a 3DMM with a neural radiance field to achieve fine-grained control and
photorealism. To reduce over-smoothing and improve out-of-model expressions
synthesis, we propose to predict local features anchored on the 3DMM geometry.
These learnt features are driven by 3DMM deformation and interpolated in 3D
space to yield the volumetric radiance at a designated query point. We further
show that using a Convolutional Neural Network in the UV space is critical in
incorporating spatial context and producing representative local features.
Extensive experiments show that we are able to reconstruct high-quality
avatars, with more accurate expression-dependent details, good generalization
to out-of-training expressions, and quantitatively superior renderings compared
to other state-of-the-art approaches.";Ziqian Bai<author:sep>Feitong Tan<author:sep>Zeng Huang<author:sep>Kripasindhu Sarkar<author:sep>Danhang Tang<author:sep>Di Qiu<author:sep>Abhimitra Meka<author:sep>Ruofei Du<author:sep>Mingsong Dou<author:sep>Sergio Orts-Escolano<author:sep>Rohit Pandey<author:sep>Ping Tan<author:sep>Thabo Beeler<author:sep>Sean Fanello<author:sep>Yinda Zhang;http://arxiv.org/pdf/2304.01436v1;cs.CV;"In CVPR2023. Project page:
  https://augmentedperception.github.io/monoavatar/";
2304.02061v3;http://arxiv.org/abs/2304.02061v3;2023-04-04;Generating Continual Human Motion in Diverse 3D Scenes;"We introduce a method to synthesize animator guided human motion across 3D
scenes. Given a set of sparse (3 or 4) joint locations (such as the location of
a person's hand and two feet) and a seed motion sequence in a 3D scene, our
method generates a plausible motion sequence starting from the seed motion
while satisfying the constraints imposed by the provided keypoints. We
decompose the continual motion synthesis problem into walking along paths and
transitioning in and out of the actions specified by the keypoints, which
enables long generation of motions that satisfy scene constraints without
explicitly incorporating scene information. Our method is trained only using
scene agnostic mocap data. As a result, our approach is deployable across 3D
scenes with various geometries. For achieving plausible continual motion
synthesis without drift, our key contribution is to generate motion in a
goal-centric canonical coordinate frame where the next immediate target is
situated at the origin. Our model can generate long sequences of diverse
actions such as grabbing, sitting and leaning chained together in arbitrary
order, demonstrated on scenes of varying geometry: HPS, Replica, Matterport,
ScanNet and scenes represented using NeRFs. Several experiments demonstrate
that our method outperforms existing methods that navigate paths in 3D scenes.";Aymen Mir<author:sep>Xavier Puig<author:sep>Angjoo Kanazawa<author:sep>Gerard Pons-Moll;http://arxiv.org/pdf/2304.02061v3;cs.CV;;nerf
2304.02001v1;http://arxiv.org/abs/2304.02001v1;2023-04-04;MonoHuman: Animatable Human Neural Field from Monocular Video;"Animating virtual avatars with free-view control is crucial for various
applications like virtual reality and digital entertainment. Previous studies
have attempted to utilize the representation power of the neural radiance field
(NeRF) to reconstruct the human body from monocular videos. Recent works
propose to graft a deformation network into the NeRF to further model the
dynamics of the human neural field for animating vivid human motions. However,
such pipelines either rely on pose-dependent representations or fall short of
motion coherency due to frame-independent optimization, making it difficult to
generalize to unseen pose sequences realistically. In this paper, we propose a
novel framework MonoHuman, which robustly renders view-consistent and
high-fidelity avatars under arbitrary novel poses. Our key insight is to model
the deformation field with bi-directional constraints and explicitly leverage
the off-the-peg keyframe information to reason the feature correlations for
coherent results. Specifically, we first propose a Shared Bidirectional
Deformation module, which creates a pose-independent generalizable deformation
field by disentangling backward and forward deformation correspondences into
shared skeletal motion weight and separate non-rigid motions. Then, we devise a
Forward Correspondence Search module, which queries the correspondence feature
of keyframes to guide the rendering network. The rendered results are thus
multi-view consistent with high fidelity, even under challenging novel pose
settings. Extensive experiments demonstrate the superiority of our proposed
MonoHuman over state-of-the-art methods.";Zhengming Yu<author:sep>Wei Cheng<author:sep>Xian Liu<author:sep>Wayne Wu<author:sep>Kwan-Yee Lin;http://arxiv.org/pdf/2304.02001v1;cs.CV;"15 pages, 14 figures. Accepted to CVPR 2023. Project page:
  https://yzmblog.github.io/projects/MonoHuman/";nerf
2304.00837v1;http://arxiv.org/abs/2304.00837v1;2023-04-03;Disorder-invariant Implicit Neural Representation;"Implicit neural representation (INR) characterizes the attributes of a signal
as a function of corresponding coordinates which emerges as a sharp weapon for
solving inverse problems. However, the expressive power of INR is limited by
the spectral bias in the network training. In this paper, we find that such a
frequency-related problem could be greatly solved by re-arranging the
coordinates of the input signal, for which we propose the disorder-invariant
implicit neural representation (DINER) by augmenting a hash-table to a
traditional INR backbone. Given discrete signals sharing the same histogram of
attributes and different arrangement orders, the hash-table could project the
coordinates into the same distribution for which the mapped signal can be
better modeled using the subsequent INR network, leading to significantly
alleviated spectral bias. Furthermore, the expressive power of the DINER is
determined by the width of the hash-table. Different width corresponds to
different geometrical elements in the attribute space, \textit{e.g.}, 1D curve,
2D curved-plane and 3D curved-volume when the width is set as $1$, $2$ and $3$,
respectively. More covered areas of the geometrical elements result in stronger
expressive power. Experiments not only reveal the generalization of the DINER
for different INR backbones (MLP vs. SIREN) and various tasks (image/video
representation, phase retrieval, refractive index recovery, and neural radiance
field optimization) but also show the superiority over the state-of-the-art
algorithms both in quality and speed. \textit{Project page:}
\url{https://ezio77.github.io/DINER-website/}";Hao Zhu<author:sep>Shaowen Xie<author:sep>Zhen Liu<author:sep>Fengyi Liu<author:sep>Qi Zhang<author:sep>You Zhou<author:sep>Yi Lin<author:sep>Zhan Ma<author:sep>Xun Cao;http://arxiv.org/pdf/2304.00837v1;cs.CV;"Journal extension of the CVPR'23 highlight paper ""DINER:
  Disorder-invariant Implicit Neural Representation"". In the extension, we
  model the expressive power of the DINER using parametric functions in the
  attribute space. As a result, better results are achieved than the conference
  version. arXiv admin note: substantial text overlap with arXiv:2211.07871";
2304.00916v3;http://arxiv.org/abs/2304.00916v3;2023-04-03;DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via  Diffusion Models;"We present DreamAvatar, a text-and-shape guided framework for generating
high-quality 3D human avatars with controllable poses. While encouraging
results have been reported by recent methods on text-guided 3D common object
generation, generating high-quality human avatars remains an open challenge due
to the complexity of the human body's shape, pose, and appearance. We propose
DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for
predicting density and color for 3D points and pretrained text-to-image
diffusion models for providing 2D self-supervision. Specifically, we leverage
the SMPL model to provide shape and pose guidance for the generation. We
introduce a dual-observation-space design that involves the joint optimization
of a canonical space and a posed space that are related by a learnable
deformation field. This facilitates the generation of more complete textures
and geometry faithful to the target pose. We also jointly optimize the losses
computed from the full body and from the zoomed-in 3D head to alleviate the
common multi-face ''Janus'' problem and improve facial details in the generated
avatars. Extensive evaluations demonstrate that DreamAvatar significantly
outperforms existing methods, establishing a new state-of-the-art for
text-and-shape guided 3D human avatar generation.";Yukang Cao<author:sep>Yan-Pei Cao<author:sep>Kai Han<author:sep>Ying Shan<author:sep>Kwan-Yee K. Wong;http://arxiv.org/pdf/2304.00916v3;cs.CV;Project page: https://yukangcao.github.io/DreamAvatar/;nerf
2304.00341v1;http://arxiv.org/abs/2304.00341v1;2023-04-01;JacobiNeRF: NeRF Shaping with Mutual Information Gradients;"We propose a method that trains a neural radiance field (NeRF) to encode not
only the appearance of the scene but also semantic correlations between scene
points, regions, or entities -- aiming to capture their mutual co-variation
patterns. In contrast to the traditional first-order photometric reconstruction
objective, our method explicitly regularizes the learning dynamics to align the
Jacobians of highly-correlated entities, which proves to maximize the mutual
information between them under random scene perturbations. By paying attention
to this second-order information, we can shape a NeRF to express semantically
meaningful synergies when the network weights are changed by a delta along the
gradient of a single entity, region, or even a point. To demonstrate the merit
of this mutual information modeling, we leverage the coordinated behavior of
scene entities that emerges from our shaping to perform label propagation for
semantic and instance segmentation. Our experiments show that a JacobiNeRF is
more efficient in propagating annotations among 2D pixels and 3D points
compared to NeRFs without mutual information shaping, especially in extremely
sparse label regimes -- thus reducing annotation burden. The same machinery can
further be used for entity selection or scene modifications.";Xiaomeng Xu<author:sep>Yanchao Yang<author:sep>Kaichun Mo<author:sep>Boxiao Pan<author:sep>Li Yi<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2304.00341v1;cs.CV;;nerf
2303.17968v1;http://arxiv.org/abs/2303.17968v1;2023-03-31;VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence  Normalization;"We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) for
better geometry under non-Lambertian surface and dynamic lighting conditions
that cause significant variation in the radiance of a point when viewed from
different angles. Instead of explicitly modeling the underlying factors that
result in the view-dependent phenomenon, which could be complex yet not
inclusive, we develop a simple and effective technique that normalizes the
view-dependence by distilling invariant information already encoded in the
learned NeRFs. We then jointly train NeRFs for view synthesis with
view-dependence normalization to attain quality geometry. Our experiments show
that even though shape-radiance ambiguity is inevitable, the proposed
normalization can minimize its effect on geometry, which essentially aligns the
optimal capacity needed for explaining view-dependent variations. Our method
applies to various baselines and significantly improves geometry without
changing the volume rendering pipeline, even if the data is captured under a
moving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF.";Bingfan Zhu<author:sep>Yanchao Yang<author:sep>Xulong Wang<author:sep>Youyi Zheng<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2303.17968v1;cs.CV;;nerf
2303.17094v1;http://arxiv.org/abs/2303.17094v1;2023-03-30;Enhanced Stable View Synthesis;"We introduce an approach to enhance the novel view synthesis from images
taken from a freely moving camera. The introduced approach focuses on outdoor
scenes where recovering accurate geometric scaffold and camera pose is
challenging, leading to inferior results using the state-of-the-art stable view
synthesis (SVS) method. SVS and related methods fail for outdoor scenes
primarily due to (i) over-relying on the multiview stereo (MVS) for geometric
scaffold recovery and (ii) assuming COLMAP computed camera poses as the best
possible estimates, despite it being well-studied that MVS 3D reconstruction
accuracy is limited to scene disparity and camera-pose accuracy is sensitive to
key-point correspondence selection. This work proposes a principled way to
enhance novel view synthesis solutions drawing inspiration from the basics of
multiple view geometry. By leveraging the complementary behavior of MVS and
monocular depth, we arrive at a better scene depth per view for nearby and far
points, respectively. Moreover, our approach jointly refines camera poses with
image-based rendering via multiple rotation averaging graph optimization. The
recovered scene depth and the camera-pose help better view-dependent on-surface
feature aggregation of the entire scene. Extensive evaluation of our approach
on the popular benchmark dataset, such as Tanks and Temples, shows substantial
improvement in view synthesis results compared to the prior art. For instance,
our method shows 1.5 dB of PSNR improvement on the Tank and Temples. Similar
statistics are observed when tested on other benchmark datasets such as FVS,
Mip-NeRF 360, and DTU.";Nishant Jain<author:sep>Suryansh Kumar<author:sep>Luc Van Gool;http://arxiv.org/pdf/2303.17094v1;cs.CV;"Accepted to IEEE/CVF CVPR 2023. Draft info: 13 pages, 6 Figures, 7
  Tables";nerf
2303.17147v1;http://arxiv.org/abs/2303.17147v1;2023-03-30;NeILF++: Inter-Reflectable Light Fields for Geometry and Material  Estimation;"We present a novel differentiable rendering framework for joint geometry,
material, and lighting estimation from multi-view images. In contrast to
previous methods which assume a simplified environment map or co-located
flashlights, in this work, we formulate the lighting of a static scene as one
neural incident light field (NeILF) and one outgoing neural radiance field
(NeRF). The key insight of the proposed method is the union of the incident and
outgoing light fields through physically-based rendering and inter-reflections
between surfaces, making it possible to disentangle the scene geometry,
material, and lighting from image observations in a physically-based manner.
The proposed incident light and inter-reflection framework can be easily
applied to other NeRF systems. We show that our method can not only decompose
the outgoing radiance into incident lights and surface materials, but also
serve as a surface refinement module that further improves the reconstruction
detail of the neural surface. We demonstrate on several datasets that the
proposed method is able to achieve state-of-the-art results in terms of
geometry reconstruction quality, material estimation accuracy, and the fidelity
of novel view rendering.";Jingyang Zhang<author:sep>Yao Yao<author:sep>Shiwei Li<author:sep>Jingbo Liu<author:sep>Tian Fang<author:sep>David McKinnon<author:sep>Yanghai Tsin<author:sep>Long Quan;http://arxiv.org/pdf/2303.17147v1;cs.CV;Project page: \url{https://yoyo000.github.io/NeILF_pp};nerf
2303.17603v1;http://arxiv.org/abs/2303.17603v1;2023-03-30;NeRF-Supervised Deep Stereo;"We introduce a novel framework for training deep stereo networks effortlessly
and without any ground-truth. By leveraging state-of-the-art neural rendering
solutions, we generate stereo training data from image sequences collected with
a single handheld camera. On top of them, a NeRF-supervised training procedure
is carried out, from which we exploit rendered stereo triplets to compensate
for occlusions and depth maps as proxy labels. This results in stereo networks
capable of predicting sharp and detailed disparity maps. Experimental results
show that models trained under this regime yield a 30-40% improvement over
existing self-supervised methods on the challenging Middlebury dataset, filling
the gap to supervised models and, most times, outperforming them at zero-shot
generalization.";Fabio Tosi<author:sep>Alessio Tonioni<author:sep>Daniele De Gregorio<author:sep>Matteo Poggi;http://arxiv.org/pdf/2303.17603v1;cs.CV;"CVPR 2023. Project page: https://nerfstereo.github.io/ Code:
  https://github.com/fabiotosi92/NeRF-Supervised-Deep-Stereo";nerf
2303.17368v2;http://arxiv.org/abs/2303.17368v2;2023-03-30;SynBody: Synthetic Dataset with Layered Human Models for 3D Human  Perception and Modeling;"Synthetic data has emerged as a promising source for 3D human research as it
offers low-cost access to large-scale human datasets. To advance the diversity
and annotation quality of human models, we introduce a new synthetic dataset,
SynBody, with three appealing features: 1) a clothed parametric human model
that can generate a diverse range of subjects; 2) the layered human
representation that naturally offers high-quality 3D annotations to support
multiple tasks; 3) a scalable system for producing realistic data to facilitate
real-world tasks. The dataset comprises 1.2M images with corresponding accurate
3D annotations, covering 10,000 human body models, 1,187 actions, and various
viewpoints. The dataset includes two subsets for human pose and shape
estimation as well as human neural rendering. Extensive experiments on SynBody
indicate that it substantially enhances both SMPL and SMPL-X estimation.
Furthermore, the incorporation of layered annotations offers a valuable
training resource for investigating the Human Neural Radiance Fields (NeRF).";Zhitao Yang<author:sep>Zhongang Cai<author:sep>Haiyi Mei<author:sep>Shuai Liu<author:sep>Zhaoxi Chen<author:sep>Weiye Xiao<author:sep>Yukun Wei<author:sep>Zhongfei Qing<author:sep>Chen Wei<author:sep>Bo Dai<author:sep>Wayne Wu<author:sep>Chen Qian<author:sep>Dahua Lin<author:sep>Ziwei Liu<author:sep>Lei Yang;http://arxiv.org/pdf/2303.17368v2;cs.CV;Accepted by ICCV 2023. Project webpage: https://synbody.github.io/;nerf
2303.16485v1;http://arxiv.org/abs/2303.16485v1;2023-03-29;TriVol: Point Cloud Rendering via Triple Volumes;"Existing learning-based methods for point cloud rendering adopt various 3D
representations and feature querying mechanisms to alleviate the sparsity
problem of point clouds. However, artifacts still appear in rendered images,
due to the challenges in extracting continuous and discriminative 3D features
from point clouds. In this paper, we present a dense while lightweight 3D
representation, named TriVol, that can be combined with NeRF to render
photo-realistic images from point clouds. Our TriVol consists of triple slim
volumes, each of which is encoded from the point cloud. TriVol has two
advantages. First, it fuses respective fields at different scales and thus
extracts local and non-local features for discriminative representation.
Second, since the volume size is greatly reduced, our 3D decoder can be
efficiently inferred, allowing us to increase the resolution of the 3D space to
render more point details. Extensive experiments on different benchmarks with
varying kinds of scenes/objects demonstrate our framework's effectiveness
compared with current approaches. Moreover, our framework has excellent
generalization ability to render a category of scenes/objects without
fine-tuning.";Tao Hu<author:sep>Xiaogang Xu<author:sep>Ruihang Chu<author:sep>Jiaya Jia;http://arxiv.org/pdf/2303.16485v1;cs.CV;;nerf
2303.16884v1;http://arxiv.org/abs/2303.16884v1;2023-03-29;Instant Neural Radiance Fields Stylization;"We present Instant Neural Radiance Fields Stylization, a novel approach for
multi-view image stylization for the 3D scene. Our approach models a neural
radiance field based on neural graphics primitives, which use a hash
table-based position encoder for position embedding. We split the position
encoder into two parts, the content and style sub-branches, and train the
network for normal novel view image synthesis with the content and style
targets. In the inference stage, we execute AdaIN to the output features of the
position encoder, with content and style voxel grid features as reference. With
the adjusted features, the stylization of novel view images could be obtained.
Our method extends the style target from style images to image sets of scenes
and does not require additional network training for stylization. Given a set
of images of 3D scenes and a style target(a style image or another set of 3D
scenes), our method can generate stylized novel views with a consistent
appearance at various view angles in less than 10 minutes on modern GPU
hardware. Extensive experimental results demonstrate the validity and
superiority of our method.";Shaoxu Li<author:sep>Ye Pan;http://arxiv.org/pdf/2303.16884v1;cs.CV;;
2303.16482v1;http://arxiv.org/abs/2303.16482v1;2023-03-29;Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance  Fields;"Synthesizing photo-realistic images from a point cloud is challenging because
of the sparsity of point cloud representation. Recent Neural Radiance Fields
and extensions are proposed to synthesize realistic images from 2D input. In
this paper, we present Point2Pix as a novel point renderer to link the 3D
sparse point clouds with 2D dense image pixels. Taking advantage of the point
cloud 3D prior and NeRF rendering pipeline, our method can synthesize
high-quality images from colored point clouds, generally for novel indoor
scenes. To improve the efficiency of ray sampling, we propose point-guided
sampling, which focuses on valid samples. Also, we present Point Encoding to
build Multi-scale Radiance Fields that provide discriminative 3D point
features. Finally, we propose Fusion Encoding to efficiently synthesize
high-quality images. Extensive experiments on the ScanNet and ArkitScenes
datasets demonstrate the effectiveness and generalization.";Tao Hu<author:sep>Xiaogang Xu<author:sep>Shu Liu<author:sep>Jiaya Jia;http://arxiv.org/pdf/2303.16482v1;cs.CV;;nerf
2303.15951v1;http://arxiv.org/abs/2303.15951v1;2023-03-28;F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera  Trajectories;"This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF)
for novel view synthesis, which enables arbitrary input camera trajectories and
only costs a few minutes for training. Existing fast grid-based NeRF training
frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed
for bounded scenes and rely on space warping to handle unbounded scenes.
Existing two widely-used space-warping methods are only designed for the
forward-facing trajectory or the 360-degree object-centric trajectory but
cannot process arbitrary trajectories. In this paper, we delve deep into the
mechanism of space warping to handle unbounded scenes. Based on our analysis,
we further propose a novel space-warping method called perspective warping,
which allows us to handle arbitrary trajectories in the grid-based NeRF
framework. Extensive experiments demonstrate that F2-NeRF is able to use the
same perspective warping to render high-quality images on two standard datasets
and a new free trajectory dataset collected by us. Project page:
https://totoro97.github.io/projects/f2-nerf.";Peng Wang<author:sep>Yuan Liu<author:sep>Zhaoxi Chen<author:sep>Lingjie Liu<author:sep>Ziwei Liu<author:sep>Taku Komura<author:sep>Christian Theobalt<author:sep>Wenping Wang;http://arxiv.org/pdf/2303.15951v1;cs.CV;CVPR 2023. Project page: https://totoro97.github.io/projects/f2-nerf;nerf
2303.16001v2;http://arxiv.org/abs/2303.16001v2;2023-03-28;Adaptive Voronoi NeRFs;"Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a set
of registered images. Increasing sizes of a scene demands more complex
functions, typically represented by neural networks, to capture all details.
Training and inference then involves querying the neural network millions of
times per image, which becomes impractically slow. Since such complex functions
can be replaced by multiple simpler functions to improve speed, we show that a
hierarchy of Voronoi diagrams is a suitable choice to partition the scene. By
equipping each Voronoi cell with its own NeRF, our approach is able to quickly
learn a scene representation. We propose an intuitive partitioning of the space
that increases quality gains during training by distributing information evenly
among the networks and avoids artifacts through a top-down adaptive refinement.
Our framework is agnostic to the underlying NeRF method and easy to implement,
which allows it to be applied to various NeRF variants for improved learning
and rendering speeds.";Tim Elsner<author:sep>Victor Czech<author:sep>Julia Berger<author:sep>Zain Selman<author:sep>Isaak Lim<author:sep>Leif Kobbelt;http://arxiv.org/pdf/2303.16001v2;cs.CV;;nerf
2303.16242v3;http://arxiv.org/abs/2303.16242v3;2023-03-28;CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image  Arbitrary-Scale Super Resolution;"Medical image arbitrary-scale super-resolution (MIASSR) has recently gained
widespread attention, aiming to super sample medical volumes at arbitrary
scales via a single model. However, existing MIASSR methods face two major
limitations: (i) reliance on high-resolution (HR) volumes and (ii) limited
generalization ability, which restricts their application in various scenarios.
To overcome these limitations, we propose Cube-based Neural Radiance Field
(CuNeRF), a zero-shot MIASSR framework that can yield medical images at
arbitrary scales and viewpoints in a continuous domain. Unlike existing MIASSR
methods that fit the mapping between low-resolution (LR) and HR volumes, CuNeRF
focuses on building a coordinate-intensity continuous representation from LR
volumes without the need for HR references. This is achieved by the proposed
differentiable modules: including cube-based sampling, isotropic volume
rendering, and cube-based hierarchical rendering. Through extensive experiments
on magnetic resource imaging (MRI) and computed tomography (CT) modalities, we
demonstrate that CuNeRF outperforms state-of-the-art MIASSR methods. CuNeRF
yields better visual verisimilitude and reduces aliasing artifacts at various
upsampling factors. Moreover, our CuNeRF does not need any LR-HR training
pairs, which is more flexible and easier to be used than others. Our code will
be publicly available soon.";Zixuan Chen<author:sep>Jian-Huang Lai<author:sep>Lingxiao Yang<author:sep>Xiaohua Xie;http://arxiv.org/pdf/2303.16242v3;eess.IV;"This paper is accepted by the International Conference on Computer
  Vision (ICCV) 2023";nerf
2303.16333v1;http://arxiv.org/abs/2303.16333v1;2023-03-28;Flow supervision for Deformable NeRF;"In this paper we present a new method for deformable NeRF that can directly
use optical flow as supervision. We overcome the major challenge with respect
to the computationally inefficiency of enforcing the flow constraints to the
backward deformation field, used by deformable NeRFs. Specifically, we show
that inverting the backward deformation function is actually not needed for
computing scene flows between frames. This insight dramatically simplifies the
problem, as one is no longer constrained to deformation functions that can be
analytically inverted. Instead, thanks to the weak assumptions required by our
derivation based on the inverse function theorem, our approach can be extended
to a broad class of commonly used backward deformation field. We present
results on monocular novel view synthesis with rapid object motion, and
demonstrate significant improvements over baselines without flow supervision.";Chaoyang Wang<author:sep>Lachlan Ewen MacDonald<author:sep>Laszlo A. Jeni<author:sep>Simon Lucey;http://arxiv.org/pdf/2303.16333v1;cs.CV;;nerf
2303.16196v2;http://arxiv.org/abs/2303.16196v2;2023-03-28;SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis;"Neural Radiance Field (NeRF) significantly degrades when only a limited
number of views are available. To complement the lack of 3D information,
depth-based models, such as DSNeRF and MonoSDF, explicitly assume the
availability of accurate depth maps of multiple views. They linearly scale the
accurate depth maps as supervision to guide the predicted depth of few-shot
NeRFs. However, accurate depth maps are difficult and expensive to capture due
to wide-range depth distances in the wild.
  In this work, we present a new Sparse-view NeRF (SparseNeRF) framework that
exploits depth priors from real-world inaccurate observations. The inaccurate
depth observations are either from pre-trained depth models or coarse depth
maps of consumer-level depth sensors. Since coarse depth maps are not strictly
scaled to the ground-truth depth maps, we propose a simple yet effective
constraint, a local depth ranking method, on NeRFs such that the expected depth
ranking of the NeRF is consistent with that of the coarse depth maps in local
patches. To preserve the spatial continuity of the estimated depth of NeRF, we
further propose a spatial continuity constraint to encourage the consistency of
the expected depth continuity of NeRF with coarse depth maps. Surprisingly,
with simple depth ranking constraints, SparseNeRF outperforms all
state-of-the-art few-shot NeRF methods (including depth-based models) on
standard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBD
that contains real-world depth maps from Azure Kinect, ZED 2, and iPhone 13
Pro. Extensive experiments on NVS-RGBD dataset also validate the superiority
and generalizability of SparseNeRF. Code and dataset are available at
https://sparsenerf.github.io/.";Guangcong Wang<author:sep>Zhaoxi Chen<author:sep>Chen Change Loy<author:sep>Ziwei Liu;http://arxiv.org/pdf/2303.16196v2;cs.CV;Accepted by ICCV 2023, Project page: https://sparsenerf.github.io/;nerf
2303.16184v1;http://arxiv.org/abs/2303.16184v1;2023-03-28;VMesh: Hybrid Volume-Mesh Representation for Efficient View Synthesis;"With the emergence of neural radiance fields (NeRFs), view synthesis quality
has reached an unprecedented level. Compared to traditional mesh-based assets,
this volumetric representation is more powerful in expressing scene geometry
but inevitably suffers from high rendering costs and can hardly be involved in
further processes like editing, posing significant difficulties in combination
with the existing graphics pipeline. In this paper, we present a hybrid
volume-mesh representation, VMesh, which depicts an object with a textured mesh
along with an auxiliary sparse volume. VMesh retains the advantages of
mesh-based assets, such as efficient rendering, compact storage, and easy
editing, while also incorporating the ability to represent subtle geometric
structures provided by the volumetric counterpart. VMesh can be obtained from
multi-view images of an object and renders at 2K 60FPS on common consumer
devices with high fidelity, unleashing new opportunities for real-time
immersive applications.";Yuan-Chen Guo<author:sep>Yan-Pei Cao<author:sep>Chen Wang<author:sep>Yu He<author:sep>Ying Shan<author:sep>Xiaohu Qie<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2303.16184v1;cs.CV;Project page: https://bennyguo.github.io/vmesh/;nerf
2303.15368v1;http://arxiv.org/abs/2303.15368v1;2023-03-27;NeUDF: Learning Unsigned Distance Fields from Multi-view Images for  Reconstructing Non-watertight Models;"Volume rendering-based 3D reconstruction from multi-view images has gained
popularity in recent years, largely due to the success of neural radiance
fields (NeRF). A number of methods have been developed that build upon NeRF and
use neural volume rendering to learn signed distance fields (SDFs) for
reconstructing 3D models. However, SDF-based methods cannot represent
non-watertight models and, therefore, cannot capture open boundaries. This
paper proposes a new algorithm for learning an accurate unsigned distance field
(UDF) from multi-view images, which is specifically designed for reconstructing
non-watertight, textureless models. The proposed method, called NeUDF,
addresses the limitations of existing UDF-based methods by introducing a simple
and approximately unbiased and occlusion-aware density function. In addition, a
smooth and differentiable UDF representation is presented to make the learning
process easier and more efficient. Experiments on both texture-rich and
textureless models demonstrate the robustness and effectiveness of the proposed
approach, making it a promising solution for reconstructing challenging 3D
models from multi-view images.";Fei Hou<author:sep>Jukai Deng<author:sep>Xuhui Chen<author:sep>Wencheng Wang<author:sep>Ying He;http://arxiv.org/pdf/2303.15368v1;cs.CV;;nerf
2303.15387v1;http://arxiv.org/abs/2303.15387v1;2023-03-27;Generalizable Neural Voxels for Fast Human Radiance Fields;"Rendering moving human bodies at free viewpoints only from a monocular video
is quite a challenging problem. The information is too sparse to model
complicated human body structures and motions from both view and pose
dimensions. Neural radiance fields (NeRF) have shown great power in novel view
synthesis and have been applied to human body rendering. However, most current
NeRF-based methods bear huge costs for both training and rendering, which
impedes the wide applications in real-life scenarios. In this paper, we propose
a rendering framework that can learn moving human body structures extremely
quickly from a monocular video. The framework is built by integrating both
neural fields and neural voxels. Especially, a set of generalizable neural
voxels are constructed. With pretrained on various human bodies, these general
voxels represent a basic skeleton and can provide strong geometric priors. For
the fine-tuning process, individual voxels are constructed for learning
differential textures, complementary to general voxels. Thus learning a novel
body can be further accelerated, taking only a few minutes. Our method shows
significantly higher training efficiency compared with previous methods, while
maintaining similar rendering quality. The project page is at
https://taoranyi.com/gneuvox .";Taoran Yi<author:sep>Jiemin Fang<author:sep>Xinggang Wang<author:sep>Wenyu Liu;http://arxiv.org/pdf/2303.15387v1;cs.CV;Project page: http://taoranyi.com/gneuvox;nerf
2303.15427v1;http://arxiv.org/abs/2303.15427v1;2023-03-27;JAWS: Just A Wild Shot for Cinematic Transfer in Neural Radiance Fields;"This paper presents JAWS, an optimization-driven approach that achieves the
robust transfer of visual cinematic features from a reference in-the-wild video
clip to a newly generated clip. To this end, we rely on an
implicit-neural-representation (INR) in a way to compute a clip that shares the
same cinematic features as the reference clip. We propose a general formulation
of a camera optimization problem in an INR that computes extrinsic and
intrinsic camera parameters as well as timing. By leveraging the
differentiability of neural representations, we can back-propagate our designed
cinematic losses measured on proxy estimators through a NeRF network to the
proposed cinematic parameters directly. We also introduce specific enhancements
such as guidance maps to improve the overall quality and efficiency. Results
display the capacity of our system to replicate well known camera sequences
from movies, adapting the framing, camera parameters and timing of the
generated video clip to maximize the similarity with the reference clip.";Xi Wang<author:sep>Robin Courant<author:sep>Jinglei Shi<author:sep>Eric Marchand<author:sep>Marc Christie;http://arxiv.org/pdf/2303.15427v1;cs.CV;"CVPR 2023. Project page with videos and code:
  http://www.lix.polytechnique.fr/vista/projects/2023_cvpr_wang";nerf
2303.15012v1;http://arxiv.org/abs/2303.15012v1;2023-03-27;3D-Aware Multi-Class Image-to-Image Translation with NeRFs;"Recent advances in 3D-aware generative models (3D-aware GANs) combined with
Neural Radiance Fields (NeRF) have achieved impressive results. However no
prior works investigate 3D-aware GANs for 3D consistent multi-class
image-to-image (3D-aware I2I) translation. Naively using 2D-I2I translation
methods suffers from unrealistic shape/identity change. To perform 3D-aware
multi-class I2I translation, we decouple this learning process into a
multi-class 3D-aware GAN step and a 3D-aware I2I translation step. In the first
step, we propose two novel techniques: a new conditional architecture and an
effective training strategy. In the second step, based on the well-trained
multi-class 3D-aware GAN architecture, that preserves view-consistency, we
construct a 3D-aware I2I translation system. To further reduce the
view-consistency problems, we propose several new techniques, including a
U-net-like adaptor network design, a hierarchical representation constrain and
a relative regularization loss. In extensive experiments on two datasets,
quantitative and qualitative results demonstrate that we successfully perform
3D-aware I2I translation with multi-view consistency.";Senmao Li<author:sep>Joost van de Weijer<author:sep>Yaxing Wang<author:sep>Fahad Shahbaz Khan<author:sep>Meiqin Liu<author:sep>Jian Yang;http://arxiv.org/pdf/2303.15012v1;cs.CV;Accepted by CVPR2023;nerf
2303.14707v1;http://arxiv.org/abs/2303.14707v1;2023-03-26;Clean-NeRF: Reformulating NeRF to account for View-Dependent  Observations;"While Neural Radiance Fields (NeRFs) had achieved unprecedented novel view
synthesis results, they have been struggling in dealing with large-scale
cluttered scenes with sparse input views and highly view-dependent appearances.
Specifically, existing NeRF-based models tend to produce blurry rendering with
the volumetric reconstruction often inaccurate, where a lot of reconstruction
errors are observed in the form of foggy ""floaters"" hovering within the entire
volume of an opaque 3D scene. Such inaccuracies impede NeRF's potential for
accurate 3D NeRF registration, object detection, segmentation, etc., which
possibly accounts for only limited significant research effort so far to
directly address these important 3D fundamental computer vision problems to
date. This paper analyzes the NeRF's struggles in such settings and proposes
Clean-NeRF for accurate 3D reconstruction and novel view rendering in complex
scenes. Our key insights consist of enforcing effective appearance and geometry
constraints, which are absent in the conventional NeRF reconstruction, by 1)
automatically detecting and modeling view-dependent appearances in the training
views to prevent them from interfering with density estimation, which is
complete with 2) a geometric correction procedure performed on each traced ray
during inference. Clean-NeRF can be implemented as a plug-in that can
immediately benefit existing NeRF-based methods without additional input. Codes
will be released.";Xinhang Liu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2303.14707v1;cs.CV;;nerf
2303.14435v1;http://arxiv.org/abs/2303.14435v1;2023-03-25;NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects;"Dynamic Neural Radiance Field (NeRF) is a powerful algorithm capable of
rendering photo-realistic novel view images from a monocular RGB video of a
dynamic scene. Although it warps moving points across frames from the
observation spaces to a common canonical space for rendering, dynamic NeRF does
not model the change of the reflected color during the warping. As a result,
this approach often fails drastically on challenging specular objects in
motion. We address this limitation by reformulating the neural radiance field
function to be conditioned on surface position and orientation in the
observation space. This allows the specular surface at different poses to keep
the different reflected colors when mapped to the common canonical space.
Additionally, we add the mask of moving objects to guide the deformation field.
As the specular surface changes color during motion, the mask mitigates the
problem of failure to find temporal correspondences with only RGB supervision.
We evaluate our model based on the novel view synthesis quality with a
self-collected dataset of different moving specular objects in realistic
environments. The experimental results demonstrate that our method
significantly improves the reconstruction quality of moving specular objects
from monocular RGB videos compared to the existing NeRF models. Our code and
data are available at the project website https://github.com/JokerYan/NeRF-DS.";Zhiwen Yan<author:sep>Chen Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2303.14435v1;cs.CV;CVPR 2023;nerf
2303.14478v1;http://arxiv.org/abs/2303.14478v1;2023-03-25;DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields;"Recent works such as BARF and GARF can bundle adjust camera poses with neural
radiance fields (NeRF) which is based on coordinate-MLPs. Despite the
impressive results, these methods cannot be applied to Generalizable NeRFs
(GeNeRFs) which require image feature extractions that are often based on more
complicated 3D CNN or transformer architectures. In this work, we first analyze
the difficulties of jointly optimizing camera poses with GeNeRFs, and then
further propose our DBARF to tackle these issues. Our DBARF which bundle
adjusts camera poses by taking a cost feature map as an implicit cost function
can be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARF
and its follow-up works, which can only be applied to per-scene optimized NeRFs
and need accurate initial camera poses with the exception of forward-facing
scenes, our method can generalize across scenes and does not require any good
initialization. Experiments show the effectiveness and generalization ability
of our DBARF when evaluated on real-world datasets. Our code is available at
\url{https://aibluefisher.github.io/dbarf}.";Yu Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2303.14478v1;cs.CV;;nerf
2303.14536v1;http://arxiv.org/abs/2303.14536v1;2023-03-25;SUDS: Scalable Urban Dynamic Scenes;"We extend neural radiance fields (NeRFs) to dynamic large-scale urban scenes.
Prior work tends to reconstruct single video clips of short durations (up to 10
seconds). Two reasons are that such methods (a) tend to scale linearly with the
number of moving objects and input videos because a separate model is built for
each and (b) tend to require supervision via 3D bounding boxes and panoptic
labels, obtained manually or via category-specific models. As a step towards
truly open-world reconstructions of dynamic cities, we introduce two key
innovations: (a) we factorize the scene into three separate hash table data
structures to efficiently encode static, dynamic, and far-field radiance
fields, and (b) we make use of unlabeled target signals consisting of RGB
images, sparse LiDAR, off-the-shelf self-supervised 2D descriptors, and most
importantly, 2D optical flow.
  Operationalizing such inputs via photometric, geometric, and feature-metric
reconstruction losses enables SUDS to decompose dynamic scenes into the static
background, individual objects, and their motions. When combined with our
multi-branch table representation, such reconstructions can be scaled to tens
of thousands of objects across 1.2 million frames from 1700 videos spanning
geospatial footprints of hundreds of kilometers, (to our knowledge) the largest
dynamic NeRF built to date.
  We present qualitative initial results on a variety of tasks enabled by our
representations, including novel-view synthesis of dynamic urban scenes,
unsupervised 3D instance segmentation, and unsupervised 3D cuboid detection. To
compare to prior work, we also evaluate on KITTI and Virtual KITTI 2,
surpassing state-of-the-art methods that rely on ground truth 3D bounding box
annotations while being 10x quicker to train.";Haithem Turki<author:sep>Jason Y. Zhang<author:sep>Francesco Ferroni<author:sep>Deva Ramanan;http://arxiv.org/pdf/2303.14536v1;cs.CV;CVPR 2023 Project page: https://haithemturki.com/suds/;nerf
2303.13843v3;http://arxiv.org/abs/2303.13843v3;2023-03-24;CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D  Scene Layout;"Recent advances have shown promise in merging neural radiance fields (NeRFs)
with pre-trained diffusion models for text-to-3D object generation. However,
one enduring challenge is their inadequate capability to accurately parse and
regenerate consistent multi-object environments. Specifically, these models
encounter difficulties in accurately representing quantity and style prompted
by multi-object texts, often resulting in a collapse of the rendering fidelity
that fails to match the semantic intricacies. Moreover, amalgamating these
elements into a coherent 3D scene is a substantial challenge, stemming from
generic distribution inherent in diffusion models. To tackle the issue of
'guidance collapse' and enhance consistency, we propose a novel framework,
dubbed CompoNeRF, by integrating an editable 3D scene layout with object
specific and scene-wide guidance mechanisms. It initiates by interpreting a
complex text into an editable 3D layout populated with multiple NeRFs, each
paired with a corresponding subtext prompt for precise object depiction. Next,
a tailored composition module seamlessly blends these NeRFs, promoting
consistency, while the dual-level text guidance reduces ambiguity and boosts
accuracy. Noticeably, the unique modularity of CompoNeRF permits NeRF
decomposition. This enables flexible scene editing and recomposition into new
scenes based on the edited layout or text prompts. Utilizing the open source
Stable Diffusion model, CompoNeRF not only generates scenes with high fidelity
but also paves the way for innovative multi-object composition using editable
3D layouts. Remarkably, our framework achieves up to a 54\% improvement in
performance, as measured by the multi-view CLIP score metric. Code is available
at https://github.com/hbai98/Componerf.";Haotian Bai<author:sep>Yuanhuiyi Lyu<author:sep>Lutao Jiang<author:sep>Sijia Li<author:sep>Haonan Lu<author:sep>Xiaodong Lin<author:sep>Lin Wang;http://arxiv.org/pdf/2303.13843v3;cs.CV;;nerf
2303.13825v1;http://arxiv.org/abs/2303.13825v1;2023-03-24;HandNeRF: Neural Radiance Fields for Animatable Interacting Hands;"We propose a novel framework to reconstruct accurate appearance and geometry
with neural radiance fields (NeRF) for interacting hands, enabling the
rendering of photo-realistic images and videos for gesture animation from
arbitrary views. Given multi-view images of a single hand or interacting hands,
an off-the-shelf skeleton estimator is first employed to parameterize the hand
poses. Then we design a pose-driven deformation field to establish
correspondence from those different poses to a shared canonical space, where a
pose-disentangled NeRF for one hand is optimized. Such unified modeling
efficiently complements the geometry and texture cues in rarely-observed areas
for both hands. Meanwhile, we further leverage the pose priors to generate
pseudo depth maps as guidance for occlusion-aware density learning. Moreover, a
neural feature distillation method is proposed to achieve cross-domain
alignment for color optimization. We conduct extensive experiments to verify
the merits of our proposed HandNeRF and report a series of state-of-the-art
results both qualitatively and quantitatively on the large-scale InterHand2.6M
dataset.";Zhiyang Guo<author:sep>Wengang Zhou<author:sep>Min Wang<author:sep>Li Li<author:sep>Houqiang Li;http://arxiv.org/pdf/2303.13825v1;cs.CV;CVPR 2023;nerf
2303.14001v1;http://arxiv.org/abs/2303.14001v1;2023-03-24;Grid-guided Neural Radiance Fields for Large Urban Scenes;"Purely MLP-based neural radiance fields (NeRF-based methods) often suffer
from underfitting with blurred renderings on large-scale scenes due to limited
model capacity. Recent approaches propose to geographically divide the scene
and adopt multiple sub-NeRFs to model each region individually, leading to
linear scale-up in training costs and the number of sub-NeRFs as the scene
expands. An alternative solution is to use a feature grid representation, which
is computationally efficient and can naturally scale to a large scene with
increased grid resolutions. However, the feature grid tends to be less
constrained and often reaches suboptimal solutions, producing noisy artifacts
in renderings, especially in regions with complex geometry and texture. In this
work, we present a new framework that realizes high-fidelity rendering on large
urban scenes while being computationally efficient. We propose to use a compact
multiresolution ground feature plane representation to coarsely capture the
scene, and complement it with positional encoding inputs through another NeRF
branch for rendering in a joint learning fashion. We show that such an
integration can utilize the advantages of two alternative solutions: a
light-weighted NeRF is sufficient, under the guidance of the feature grid
representation, to render photorealistic novel views with fine details; and the
jointly optimized ground feature planes, can meanwhile gain further
refinements, forming a more accurate and compact feature space and output much
more natural rendering results.";Linning Xu<author:sep>Yuanbo Xiangli<author:sep>Sida Peng<author:sep>Xingang Pan<author:sep>Nanxuan Zhao<author:sep>Christian Theobalt<author:sep>Bo Dai<author:sep>Dahua Lin;http://arxiv.org/pdf/2303.14001v1;cs.CV;CVPR2023, Project page at https://city-super.github.io/gridnerf/;nerf
2303.13777v1;http://arxiv.org/abs/2303.13777v1;2023-03-24;GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from  Multi-view Images;"In this work, we focus on synthesizing high-fidelity novel view images for
arbitrary human performers, given a set of sparse multi-view images. It is a
challenging task due to the large variation among articulated body poses and
heavy self-occlusions. To alleviate this, we introduce an effective
generalizable framework Generalizable Model-based Neural Radiance Fields
(GM-NeRF) to synthesize free-viewpoint images. Specifically, we propose a
geometry-guided attention mechanism to register the appearance code from
multi-view 2D images to a geometry proxy which can alleviate the misalignment
between inaccurate geometry prior and pixel space. On top of that, we further
conduct neural rendering and partial gradient backpropagation for efficient
perceptual supervision and improvement of the perceptual quality of synthesis.
To evaluate our method, we conduct experiments on synthesized datasets
THuman2.0 and Multi-garment, and real-world datasets Genebody and ZJUMocap. The
results demonstrate that our approach outperforms state-of-the-art methods in
terms of novel view synthesis and geometric reconstruction.";Jianchuan Chen<author:sep>Wentao Yi<author:sep>Liqian Ma<author:sep>Xu Jia<author:sep>Huchuan Lu;http://arxiv.org/pdf/2303.13777v1;cs.CV;Accepted at CVPR 2023;nerf
2303.13817v1;http://arxiv.org/abs/2303.13817v1;2023-03-24;ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for  Neural Radiance Field;"Neural Radiance Field (NeRF) is a popular method in representing 3D scenes by
optimising a continuous volumetric scene function. Its large success which lies
in applying volumetric rendering (VR) is also its Achilles' heel in producing
view-dependent effects. As a consequence, glossy and transparent surfaces often
appear murky. A remedy to reduce these artefacts is to constrain this VR
equation by excluding volumes with back-facing normal. While this approach has
some success in rendering glossy surfaces, translucent objects are still poorly
represented. In this paper, we present an alternative to the physics-based VR
approach by introducing a self-attention-based framework on volumes along a
ray. In addition, inspired by modern game engines which utilise Light Probes to
store local lighting passing through the scene, we incorporate Learnable
Embeddings to capture view dependent effects within the scene. Our method,
which we call ABLE-NeRF, significantly reduces `blurry' glossy surfaces in
rendering and produces realistic translucent surfaces which lack in prior art.
In the Blender dataset, ABLE-NeRF achieves SOTA results and surpasses Ref-NeRF
in all 3 image quality metrics PSNR, SSIM, LPIPS.";Zhe Jun Tang<author:sep>Tat-Jen Cham<author:sep>Haiyu Zhao;http://arxiv.org/pdf/2303.13817v1;cs.CV;"IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)
  2023";nerf
2303.13743v1;http://arxiv.org/abs/2303.13743v1;2023-03-24;TEGLO: High Fidelity Canonical Texture Mapping from Single-View Images;"Recent work in Neural Fields (NFs) learn 3D representations from
class-specific single view image collections. However, they are unable to
reconstruct the input data preserving high-frequency details. Further, these
methods do not disentangle appearance from geometry and hence are not suitable
for tasks such as texture transfer and editing. In this work, we propose TEGLO
(Textured EG3D-GLO) for learning 3D representations from single view
in-the-wild image collections for a given class of objects. We accomplish this
by training a conditional Neural Radiance Field (NeRF) without any explicit 3D
supervision. We equip our method with editing capabilities by creating a dense
correspondence mapping to a 2D canonical space. We demonstrate that such
mapping enables texture transfer and texture editing without requiring meshes
with shared topology. Our key insight is that by mapping the input image pixels
onto the texture space we can achieve near perfect reconstruction (>= 74 dB
PSNR at 1024^2 resolution). Our formulation allows for high quality 3D
consistent novel view synthesis with high-frequency details at megapixel image
resolution.";Vishal Vinod<author:sep>Tanmay Shah<author:sep>Dmitry Lagun;http://arxiv.org/pdf/2303.13743v1;cs.CV;;nerf
2303.15206v3;http://arxiv.org/abs/2303.15206v3;2023-03-24;Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods  for Front-Facing Views;"Neural view synthesis (NVS) is one of the most successful techniques for
synthesizing free viewpoint videos, capable of achieving high fidelity from
only a sparse set of captured images. This success has led to many variants of
the techniques, each evaluated on a set of test views typically using image
quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research
on how NVS methods perform with respect to perceived video quality. We present
the first study on perceptual evaluation of NVS and NeRF variants. For this
study, we collected two datasets of scenes captured in a controlled lab
environment as well as in-the-wild. In contrast to existing datasets, these
scenes come with reference video sequences, allowing us to test for temporal
artifacts and subtle distortions that are easily overlooked when viewing only
static images. We measured the quality of videos synthesized by several NVS
methods in a well-controlled perceptual quality assessment experiment as well
as with many existing state-of-the-art image/video quality metrics. We present
a detailed analysis of the results and recommendations for dataset and metric
selection for NVS evaluation.";Hanxue Liang<author:sep>Tianhao Wu<author:sep>Param Hanji<author:sep>Francesco Banterle<author:sep>Hongyun Gao<author:sep>Rafal Mantiuk<author:sep>Cengiz Oztireli;http://arxiv.org/pdf/2303.15206v3;cs.CV;;nerf
2303.14184v2;http://arxiv.org/abs/2303.14184v2;2023-03-24;Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion  Prior;"In this work, we investigate the problem of creating high-fidelity 3D content
from only a single image. This is inherently challenging: it essentially
involves estimating the underlying 3D geometry while simultaneously
hallucinating unseen textures. To address this challenge, we leverage prior
knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision
for 3D creation. Our approach, Make-It-3D, employs a two-stage optimization
pipeline: the first stage optimizes a neural radiance field by incorporating
constraints from the reference image at the frontal view and diffusion prior at
novel views; the second stage transforms the coarse model into textured point
clouds and further elevates the realism with diffusion prior while leveraging
the high-quality textures from the reference image. Extensive experiments
demonstrate that our method outperforms prior works by a large margin,
resulting in faithful reconstructions and impressive visual quality. Our method
presents the first attempt to achieve high-quality 3D creation from a single
image for general objects and enables various applications such as text-to-3D
creation and texture editing.";Junshu Tang<author:sep>Tengfei Wang<author:sep>Bo Zhang<author:sep>Ting Zhang<author:sep>Ran Yi<author:sep>Lizhuang Ma<author:sep>Dong Chen;http://arxiv.org/pdf/2303.14184v2;cs.CV;17 pages, 18 figures, Project page: https://make-it-3d.github.io/;
2303.13277v2;http://arxiv.org/abs/2303.13277v2;2023-03-23;SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing  Field;"Despite the great success in 2D editing using user-friendly tools, such as
Photoshop, semantic strokes, or even text prompts, similar capabilities in 3D
areas are still limited, either relying on 3D modeling skills or allowing
editing within only a few categories. In this paper, we present a novel
semantic-driven NeRF editing approach, which enables users to edit a neural
radiance field with a single image, and faithfully delivers edited novel views
with high fidelity and multi-view consistency. To achieve this goal, we propose
a prior-guided editing field to encode fine-grained geometric and texture
editing in 3D space, and develop a series of techniques to aid the editing
process, including cyclic constraints with a proxy mesh to facilitate geometric
supervision, a color compositing mechanism to stabilize semantic-driven texture
editing, and a feature-cluster-based regularization to preserve the irrelevant
content unchanged. Extensive experiments and editing examples on both
real-world and synthetic data demonstrate that our method achieves
photo-realistic 3D editing using only a single edited image, pushing the bound
of semantic-driven editing in 3D real-world scenes. Our project webpage:
https://zju3dv.github.io/sine/.";Chong Bao<author:sep>Yinda Zhang<author:sep>Bangbang Yang<author:sep>Tianxing Fan<author:sep>Zesong Yang<author:sep>Hujun Bao<author:sep>Guofeng Zhang<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2303.13277v2;cs.CV;Accepted to CVPR 2023. Project Page: https://zju3dv.github.io/sine/;nerf
2303.13014v1;http://arxiv.org/abs/2303.13014v1;2023-03-23;Semantic Ray: Learning a Generalizable Semantic Field with  Cross-Reprojection Attention;"In this paper, we aim to learn a semantic radiance field from multiple scenes
that is accurate, efficient and generalizable. While most existing NeRFs target
at the tasks of neural scene rendering, image synthesis and multi-view
reconstruction, there are a few attempts such as Semantic-NeRF that explore to
learn high-level semantic understanding with the NeRF structure. However,
Semantic-NeRF simultaneously learns color and semantic label from a single ray
with multiple heads, where the single ray fails to provide rich semantic
information. As a result, Semantic NeRF relies on positional encoding and needs
to train one specific model for each scene. To address this, we propose
Semantic Ray (S-Ray) to fully exploit semantic information along the ray
direction from its multi-view reprojections. As directly performing dense
attention over multi-view reprojected rays would suffer from heavy
computational cost, we design a Cross-Reprojection Attention module with
consecutive intra-view radial and cross-view sparse attentions, which
decomposes contextual information along reprojected rays and cross multiple
views and then collects dense connections by stacking the modules. Experiments
show that our S-Ray is able to learn from multiple scenes, and it presents
strong generalization ability to adapt to unseen scenes.";Fangfu Liu<author:sep>Chubin Zhang<author:sep>Yu Zheng<author:sep>Yueqi Duan;http://arxiv.org/pdf/2303.13014v1;cs.CV;Accepted by CVPR 2023. Project page: https://liuff19.github.io/S-Ray/;nerf
2303.13450v1;http://arxiv.org/abs/2303.13450v1;2023-03-23;Set-the-Scene: Global-Local Training for Generating Controllable NeRF  Scenes;"Recent breakthroughs in text-guided image generation have led to remarkable
progress in the field of 3D synthesis from text. By optimizing neural radiance
fields (NeRF) directly from text, recent methods are able to produce remarkable
results. Yet, these methods are limited in their control of each object's
placement or appearance, as they represent the scene as a whole. This can be a
major issue in scenarios that require refining or manipulating objects in the
scene. To remedy this deficit, we propose a novel GlobalLocal training
framework for synthesizing a 3D scene using object proxies. A proxy represents
the object's placement in the generated scene and optionally defines its coarse
geometry. The key to our approach is to represent each object as an independent
NeRF. We alternate between optimizing each NeRF on its own and as part of the
full scene. Thus, a complete representation of each object can be learned,
while also creating a harmonious scene with style and lighting match. We show
that using proxies allows a wide variety of editing options, such as adjusting
the placement of each independent object, removing objects from a scene, or
refining an object. Our results show that Set-the-Scene offers a powerful
solution for scene synthesis and manipulation, filling a crucial gap in
controllable text-to-3D synthesis.";Dana Cohen-Bar<author:sep>Elad Richardson<author:sep>Gal Metzer<author:sep>Raja Giryes<author:sep>Daniel Cohen-Or;http://arxiv.org/pdf/2303.13450v1;cs.CV;project page at https://danacohen95.github.io/Set-the-Scene/;nerf
2303.13232v1;http://arxiv.org/abs/2303.13232v1;2023-03-23;Transforming Radiance Field with Lipschitz Network for Photorealistic 3D  Scene Stylization;"Recent advances in 3D scene representation and novel view synthesis have
witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not
trivial to exploit NeRF for the photorealistic 3D scene stylization task, which
aims to generate visually consistent and photorealistic stylized scenes from
novel views. Simply coupling NeRF with photorealistic style transfer (PST) will
result in cross-view inconsistency and degradation of stylized view syntheses.
Through a thorough analysis, we demonstrate that this non-trivial task can be
simplified in a new light: When transforming the appearance representation of a
pre-trained NeRF with Lipschitz mapping, the consistency and photorealism
across source views will be seamlessly encoded into the syntheses. That
motivates us to build a concise and flexible learning framework namely LipRF,
which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the
3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct
the 3D scene, and then emulates the style on each view by 2D PST as the prior
to learn a Lipschitz network to stylize the pre-trained appearance. In view of
that Lipschitz condition highly impacts the expressivity of the neural network,
we devise an adaptive regularization to balance the reconstruction and
stylization. A gradual gradient aggregation strategy is further introduced to
optimize LipRF in a cost-efficient manner. We conduct extensive experiments to
show the high quality and robust performance of LipRF on both photorealistic 3D
stylization and object appearance editing.";Zicheng Zhang<author:sep>Yinglu Liu<author:sep>Congying Han<author:sep>Yingwei Pan<author:sep>Tiande Guo<author:sep>Ting Yao;http://arxiv.org/pdf/2303.13232v1;cs.CV;CVPR 2023, Highlight;nerf
2303.13508v2;http://arxiv.org/abs/2303.13508v2;2023-03-23;DreamBooth3D: Subject-Driven Text-to-3D Generation;"We present DreamBooth3D, an approach to personalize text-to-3D generative
models from as few as 3-6 casually captured images of a subject. Our approach
combines recent advances in personalizing text-to-image models (DreamBooth)
with text-to-3D generation (DreamFusion). We find that naively combining these
methods fails to yield satisfactory subject-specific 3D assets due to
personalized text-to-image models overfitting to the input viewpoints of the
subject. We overcome this through a 3-stage optimization strategy where we
jointly leverage the 3D consistency of neural radiance fields together with the
personalization capability of text-to-image models. Our method can produce
high-quality, subject-specific 3D assets with text-driven modifications such as
novel poses, colors and attributes that are not seen in any of the input images
of the subject.";Amit Raj<author:sep>Srinivas Kaza<author:sep>Ben Poole<author:sep>Michael Niemeyer<author:sep>Nataniel Ruiz<author:sep>Ben Mildenhall<author:sep>Shiran Zada<author:sep>Kfir Aberman<author:sep>Michael Rubinstein<author:sep>Jonathan Barron<author:sep>Yuanzhen Li<author:sep>Varun Jampani;http://arxiv.org/pdf/2303.13508v2;cs.CV;"Project page at https://dreambooth3d.github.io/ Video Summary at
  https://youtu.be/kKVDrbfvOoA";
2303.13497v2;http://arxiv.org/abs/2303.13497v2;2023-03-23;TriPlaneNet: An Encoder for EG3D Inversion;"Recent progress in NeRF-based GANs has introduced a number of approaches for
high-resolution and high-fidelity generative modeling of human heads with a
possibility for novel view rendering. At the same time, one must solve an
inverse problem to be able to re-render or modify an existing image or video.
Despite the success of universal optimization-based methods for 2D GAN
inversion, those applied to 3D GANs may fail to extrapolate the result onto the
novel view, whereas optimization-based 3D GAN inversion methods are
time-consuming and can require at least several minutes per image. Fast
encoder-based techniques, such as those developed for StyleGAN, may also be
less appealing due to the lack of identity preservation. Our work introduces a
fast technique that bridges the gap between the two approaches by directly
utilizing the tri-plane representation presented for the EG3D generative model.
In particular, we build upon a feed-forward convolutional encoder for the
latent code and extend it with a fully-convolutional predictor of tri-plane
numerical offsets. The renderings are similar in quality to the ones produced
by optimization-based techniques and outperform the ones by encoder-based
methods. As we empirically prove, this is a consequence of directly operating
in the tri-plane space, not in the GAN parameter space, while making use of an
encoder-based trainable approach. Finally, we demonstrate significantly more
correct embedding of a face image in 3D than for all the baselines, further
strengthened by a probably symmetric prior enabled during training.";Ananta R. Bhattarai<author:sep>Matthias Nießner<author:sep>Artem Sevastopolsky;http://arxiv.org/pdf/2303.13497v2;cs.CV;Project page: https://anantarb.github.io/triplanenet;nerf
2303.13472v2;http://arxiv.org/abs/2303.13472v2;2023-03-23;Plotting Behind the Scenes: Towards Learnable Game Engines;"Neural video game simulators emerged as powerful tools to generate and edit
videos. Their idea is to represent games as the evolution of an environment's
state driven by the actions of its agents. While such a paradigm enables users
to play a game action-by-action, its rigidity precludes more semantic forms of
control. To overcome this limitation, we augment game models with prompts
specified as a set of natural language actions and desired states. The result-a
Promptable Game Model (PGM)-makes it possible for a user to play the game by
prompting it with high- and low-level action sequences. Most captivatingly, our
PGM unlocks the director's mode, where the game is played by specifying goals
for the agents in the form of a prompt. This requires learning ""game AI"",
encapsulated by our animation model, to navigate the scene using high-level
constraints, play against an adversary, and devise a strategy to win a point.
To render the resulting state, we use a compositional NeRF representation
encapsulated in our synthesis model. To foster future research, we present
newly collected, annotated and calibrated Tennis and Minecraft datasets. Our
method significantly outperforms existing neural video game simulators in terms
of rendering quality and unlocks applications beyond the capabilities of the
current state of the art. Our framework, data, and models are available at
https://snap-research.github.io/promptable-game-models/.";Willi Menapace<author:sep>Aliaksandr Siarohin<author:sep>Stéphane Lathuilière<author:sep>Panos Achlioptas<author:sep>Vladislav Golyanik<author:sep>Sergey Tulyakov<author:sep>Elisa Ricci;http://arxiv.org/pdf/2303.13472v2;cs.CV;"ACM Transactions on Graphics \c{opyright} Copyright is held by the
  owner/author(s) 2023. This is the author's version of the work. It is posted
  here for your personal use. Not for redistribution. The definitive Version of
  Record was published in ACM Transactions on Graphics,
  http://dx.doi.org/10.1145/3635705";nerf
2303.13582v1;http://arxiv.org/abs/2303.13582v1;2023-03-23;SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates;"Neural radiance fields (NeRFs) have enabled high fidelity 3D reconstruction
from multiple 2D input views. However, a well-known drawback of NeRFs is the
less-than-ideal performance under a small number of views, due to insufficient
constraints enforced by volumetric rendering. To address this issue, we
introduce SCADE, a novel technique that improves NeRF reconstruction quality on
sparse, unconstrained input views for in-the-wild indoor scenes. To constrain
NeRF reconstruction, we leverage geometric priors in the form of per-view depth
estimates produced with state-of-the-art monocular depth estimation models,
which can generalize across scenes. A key challenge is that monocular depth
estimation is an ill-posed problem, with inherent ambiguities. To handle this
issue, we propose a new method that learns to predict, for each view, a
continuous, multimodal distribution of depth estimates using conditional
Implicit Maximum Likelihood Estimation (cIMLE). In order to disambiguate
exploiting multiple views, we introduce an original space carving loss that
guides the NeRF representation to fuse multiple hypothesized depth maps from
each view and distill from them a common geometry that is consistent with all
views. Experiments show that our approach enables higher fidelity novel view
synthesis from sparse views. Our project page can be found at
https://scade-spacecarving-nerfs.github.io .";Mikaela Angelina Uy<author:sep>Ricardo Martin-Brualla<author:sep>Leonidas Guibas<author:sep>Ke Li;http://arxiv.org/pdf/2303.13582v1;cs.CV;CVPR 2023;nerf
2303.12786v1;http://arxiv.org/abs/2303.12786v1;2023-03-22;FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation  Models;"Recent works on generalizable NeRFs have shown promising results on novel
view synthesis from single or few images. However, such models have rarely been
applied on other downstream tasks beyond synthesis such as semantic
understanding and parsing. In this paper, we propose a novel framework named
FeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision
foundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D
pre-trained foundation models to 3D space via neural rendering, and then
extract deep features for 3D query points from NeRF MLPs. Consequently, it
allows to map 2D images to continuous 3D semantic feature volumes, which can be
used for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D
semantic keypoint transfer and 2D/3D object part segmentation. Our extensive
experiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D
semantic feature extractor. Our project page is available at
https://jianglongye.com/featurenerf/ .";Jianglong Ye<author:sep>Naiyan Wang<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2303.12786v1;cs.CV;Project page: https://jianglongye.com/featurenerf/;nerf
2303.12280v2;http://arxiv.org/abs/2303.12280v2;2023-03-22;NLOS-NeuS: Non-line-of-sight Neural Implicit Surface;"Non-line-of-sight (NLOS) imaging is conducted to infer invisible scenes from
indirect light on visible objects. The neural transient field (NeTF) was
proposed for representing scenes as neural radiance fields in NLOS scenes. We
propose NLOS neural implicit surface (NLOS-NeuS), which extends the NeTF to
neural implicit surfaces with a signed distance function (SDF) for
reconstructing three-dimensional surfaces in NLOS scenes. We introduce two
constraints as loss functions for correctly learning an SDF to avoid non-zero
level-set surfaces. We also introduce a lower bound constraint of an SDF based
on the geometry of the first-returning photons. The experimental results
indicate that these constraints are essential for learning a correct SDF in
NLOS scenes. Compared with previous methods with discretized representation,
NLOS-NeuS with the neural continuous representation enables us to reconstruct
smooth surfaces while preserving fine details in NLOS scenes. To the best of
our knowledge, this is the first study on neural implicit surfaces with volume
rendering in NLOS scenes.";Yuki Fujimura<author:sep>Takahiro Kushida<author:sep>Takuya Funatomi<author:sep>Yasuhiro Mukaigawa;http://arxiv.org/pdf/2303.12280v2;cs.CV;ICCV 2023;
2303.12791v2;http://arxiv.org/abs/2303.12791v2;2023-03-22;SHERF: Generalizable Human NeRF from a Single Image;"Existing Human NeRF methods for reconstructing 3D humans typically rely on
multiple 2D images from multi-view cameras or monocular videos captured from
fixed camera views. However, in real-world scenarios, human images are often
captured from random camera angles, presenting challenges for high-quality 3D
human reconstruction. In this paper, we propose SHERF, the first generalizable
Human NeRF model for recovering animatable 3D humans from a single input image.
SHERF extracts and encodes 3D human representations in canonical space,
enabling rendering and animation from free views and poses. To achieve
high-fidelity novel view and pose synthesis, the encoded 3D human
representations should capture both global appearance and local fine-grained
textures. To this end, we propose a bank of 3D-aware hierarchical features,
including global, point-level, and pixel-aligned features, to facilitate
informative encoding. Global features enhance the information extracted from
the single input image and complement the information missing from the partial
2D observation. Point-level features provide strong clues of 3D human
structure, while pixel-aligned features preserve more fine-grained details. To
effectively integrate the 3D-aware hierarchical feature bank, we design a
feature fusion transformer. Extensive experiments on THuman, RenderPeople,
ZJU_MoCap, and HuMMan datasets demonstrate that SHERF achieves state-of-the-art
performance, with better generalizability for novel view and pose synthesis.";Shoukang Hu<author:sep>Fangzhou Hong<author:sep>Liang Pan<author:sep>Haiyi Mei<author:sep>Lei Yang<author:sep>Ziwei Liu;http://arxiv.org/pdf/2303.12791v2;cs.CV;"Accepted by ICCV2023. Project webpage:
  https://skhu101.github.io/SHERF/";nerf
2303.12408v2;http://arxiv.org/abs/2303.12408v2;2023-03-22;Balanced Spherical Grid for Egocentric View Synthesis;"We present EgoNeRF, a practical solution to reconstruct large-scale
real-world environments for VR assets. Given a few seconds of casually captured
360 video, EgoNeRF can efficiently build neural radiance fields which enable
high-quality rendering from novel viewpoints. Motivated by the recent
acceleration of NeRF using feature grids, we adopt spherical coordinate instead
of conventional Cartesian coordinate. Cartesian feature grid is inefficient to
represent large-scale unbounded scenes because it has a spatially uniform
resolution, regardless of distance from viewers. The spherical parameterization
better aligns with the rays of egocentric images, and yet enables factorization
for performance enhancement. However, the na\""ive spherical grid suffers from
irregularities at two poles, and also cannot represent unbounded scenes. To
avoid singularities near poles, we combine two balanced grids, which results in
a quasi-uniform angular grid. We also partition the radial grid exponentially
and place an environment map at infinity to represent unbounded scenes.
Furthermore, with our resampling technique for grid-based methods, we can
increase the number of valid samples to train NeRF volume. We extensively
evaluate our method in our newly introduced synthetic and real-world egocentric
360 video datasets, and it consistently achieves state-of-the-art performance.";Changwoon Choi<author:sep>Sang Min Kim<author:sep>Young Min Kim;http://arxiv.org/pdf/2303.12408v2;cs.CV;Accepted to CVPR 2023;nerf
2303.12865v3;http://arxiv.org/abs/2303.12865v3;2023-03-22;NeRF-GAN Distillation for Efficient 3D-Aware Generation with  Convolutions;"Pose-conditioned convolutional generative models struggle with high-quality
3D-consistent image generation from single-view datasets, due to their lack of
sufficient 3D priors. Recently, the integration of Neural Radiance Fields
(NeRFs) and generative models, such as Generative Adversarial Networks (GANs),
has transformed 3D-aware generation from single-view images. NeRF-GANs exploit
the strong inductive bias of neural 3D representations and volumetric rendering
at the cost of higher computational complexity. This study aims at revisiting
pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by
distilling 3D knowledge from pretrained NeRF-GANs. We propose a simple and
effective method, based on re-using the well-disentangled latent space of a
pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly
generate 3D-consistent images corresponding to the underlying 3D
representations. Experiments on several datasets demonstrate that the proposed
method obtains results comparable with volumetric rendering in terms of quality
and 3D consistency while benefiting from the computational advantage of
convolutional networks. The code will be available at:
https://github.com/mshahbazi72/NeRF-GAN-Distillation";Mohamad Shahbazi<author:sep>Evangelos Ntavelis<author:sep>Alessio Tonioni<author:sep>Edo Collins<author:sep>Danda Pani Paudel<author:sep>Martin Danelljan<author:sep>Luc Van Gool;http://arxiv.org/pdf/2303.12865v3;cs.CV;;nerf
2303.12789v2;http://arxiv.org/abs/2303.12789v2;2023-03-22;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions;"We propose a method for editing NeRF scenes with text-instructions. Given a
NeRF of a scene and the collection of images used to reconstruct it, our method
uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit
the input images while optimizing the underlying scene, resulting in an
optimized 3D scene that respects the edit instruction. We demonstrate that our
proposed method is able to edit large-scale, real-world scenes, and is able to
accomplish more realistic, targeted edits than prior work.";Ayaan Haque<author:sep>Matthew Tancik<author:sep>Alexei A. Efros<author:sep>Aleksander Holynski<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2303.12789v2;cs.CV;"Project website: https://instruct-nerf2nerf.github.io; v1. Revisions
  to related work and discussion";nerf
2303.11537v2;http://arxiv.org/abs/2303.11537v2;2023-03-21;Interactive Geometry Editing of Neural Radiance Fields;"In this paper, we propose a method that enables interactive geometry editing
for neural radiance fields manipulation. We use two proxy cages(inner cage and
outer cage) to edit a scene. The inner cage defines the operation target, and
the outer cage defines the adjustment space. Various operations apply to the
two cages. After cage selection, operations on the inner cage lead to the
desired transformation of the inner cage and adjustment of the outer cage.
Users can edit the scene with translation, rotation, scaling, or combinations.
The operations on the corners and edges of the cage are also supported. Our
method does not need any explicit 3D geometry representations. The interactive
geometry editing applies directly to the implicit neural radiance fields.
Extensive experimental results demonstrate the effectiveness of our approach.";Shaoxu Li<author:sep>Ye Pan;http://arxiv.org/pdf/2303.11537v2;cs.CV;;
2303.11728v3;http://arxiv.org/abs/2303.11728v3;2023-03-21;Few-shot Neural Radiance Fields Under Unconstrained Illumination;"In this paper, we introduce a new challenge for synthesizing novel view
images in practical environments with limited input multi-view images and
varying lighting conditions. Neural radiance fields (NeRF), one of the
pioneering works for this task, demand an extensive set of multi-view images
taken under constrained illumination, which is often unattainable in real-world
settings. While some previous works have managed to synthesize novel views
given images with different illumination, their performance still relies on a
substantial number of input multi-view images. To address this problem, we
suggest ExtremeNeRF, which utilizes multi-view albedo consistency, supported by
geometric alignment. Specifically, we extract intrinsic image components that
should be illumination-invariant across different views, enabling direct
appearance comparison between the input and novel view under unconstrained
illumination. We offer thorough experimental results for task evaluation,
employing the newly created NeRF Extreme benchmark-the first in-the-wild
benchmark for novel view synthesis under multiple viewing directions and
varying illuminations.";SeokYeong Lee<author:sep>JunYong Choi<author:sep>Seungryong Kim<author:sep>Ig-Jae Kim<author:sep>Junghyun Cho;http://arxiv.org/pdf/2303.11728v3;cs.CV;Project Page: https://seokyeong94.github.io/ExtremeNeRF/;nerf
2303.11938v2;http://arxiv.org/abs/2303.11938v2;2023-03-21;3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion;"We tackle the task of text-to-3D creation with pre-trained latent-based NeRFs
(NeRFs that generate 3D objects given input latent code). Recent works such as
DreamFusion and Magic3D have shown great success in generating 3D content using
NeRFs and text prompts, but the current approach of optimizing a NeRF for every
text prompt is 1) extremely time-consuming and 2) often leads to low-resolution
outputs. To address these challenges, we propose a novel method named
3D-CLFusion which leverages the pre-trained latent-based NeRFs and performs
fast 3D content creation in less than a minute. In particular, we introduce a
latent diffusion prior network for learning the w latent from the input CLIP
text/image embeddings. This pipeline allows us to produce the w latent without
further optimization during inference and the pre-trained NeRF is able to
perform multi-view high-resolution 3D synthesis based on the latent. We note
that the novelty of our model lies in that we introduce contrastive learning
during training the diffusion prior which enables the generation of the valid
view-invariant latent code. We demonstrate through experiments the
effectiveness of our proposed view-invariant diffusion process for fast
text-to-3D creation, e.g., 100 times faster than DreamFusion. We note that our
model is able to serve as the role of a plug-and-play tool for text-to-3D with
pre-trained NeRFs.";Yu-Jhe Li<author:sep>Tao Xu<author:sep>Ji Hou<author:sep>Bichen Wu<author:sep>Xiaoliang Dai<author:sep>Albert Pumarola<author:sep>Peizhao Zhang<author:sep>Peter Vajda<author:sep>Kris Kitani;http://arxiv.org/pdf/2303.11938v2;cs.CV;15 pages;nerf
2303.12234v1;http://arxiv.org/abs/2303.12234v1;2023-03-21;Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields;"Neural radiance fields (NeRF) appeared recently as a powerful tool to
generate realistic views of objects and confined areas. Still, they face
serious challenges with open scenes, where the camera has unrestricted movement
and content can appear at any distance. In such scenarios, current
NeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow
training times, and might display irregularities, because of the challenging
task of reconstructing an extensive scene from a limited number of images. We
propose a new framework to boost the performance of NeRF-based architectures
yielding significantly superior outcomes compared to the prior work. Our
solution overcomes several obstacles that plagued earlier versions of NeRF,
including handling multiple video inputs, selecting keyframes, and extracting
poses from real-world frames that are ambiguous and symmetrical. Furthermore,
we applied our framework, dubbed as ""Pre-NeRF 360"", to enable the use of the
Nutrition5k dataset in NeRF and introduce an updated version of this dataset,
known as the N5k360 dataset.";Ahmad AlMughrabi<author:sep>Umair Haroon<author:sep>Ricardo Marques<author:sep>Petia Radeva;http://arxiv.org/pdf/2303.12234v1;cs.CV;;nerf
2303.11364v1;http://arxiv.org/abs/2303.11364v1;2023-03-20;DehazeNeRF: Multiple Image Haze Removal and 3D Shape Reconstruction  using Neural Radiance Fields;"Neural radiance fields (NeRFs) have demonstrated state-of-the-art performance
for 3D computer vision tasks, including novel view synthesis and 3D shape
reconstruction. However, these methods fail in adverse weather conditions. To
address this challenge, we introduce DehazeNeRF as a framework that robustly
operates in hazy conditions. DehazeNeRF extends the volume rendering equation
by adding physically realistic terms that model atmospheric scattering. By
parameterizing these terms using suitable networks that match the physical
properties, we introduce effective inductive biases, which, together with the
proposed regularizations, allow DehazeNeRF to demonstrate successful multi-view
haze removal, novel view synthesis, and 3D shape reconstruction where existing
approaches fail.";Wei-Ting Chen<author:sep>Wang Yifan<author:sep>Sy-Yen Kuo<author:sep>Gordon Wetzstein;http://arxiv.org/pdf/2303.11364v1;cs.CV;"including supplemental material; project page:
  https://www.computationalimaging.org/publications/dehazenerf";nerf
2303.11052v3;http://arxiv.org/abs/2303.11052v3;2023-03-20;ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real  Novel View Synthesis via Contrastive Learning;"Although many recent works have investigated generalizable NeRF-based novel
view synthesis for unseen scenes, they seldom consider the synthetic-to-real
generalization, which is desired in many practical applications. In this work,
we first investigate the effects of synthetic data in synthetic-to-real novel
view synthesis and surprisingly observe that models trained with synthetic data
tend to produce sharper but less accurate volume densities. For pixels where
the volume densities are correct, fine-grained details will be obtained.
Otherwise, severe artifacts will be produced. To maintain the advantages of
using synthetic data while avoiding its negative effects, we propose to
introduce geometry-aware contrastive learning to learn multi-view consistent
features with geometric constraints. Meanwhile, we adopt cross-view attention
to further enhance the geometry perception of features by querying features
across input views. Experiments demonstrate that under the synthetic-to-real
setting, our method can render images with higher quality and better
fine-grained details, outperforming existing generalizable novel view synthesis
methods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our
method also achieves state-of-the-art results.";Hao Yang<author:sep>Lanqing Hong<author:sep>Aoxue Li<author:sep>Tianyang Hu<author:sep>Zhenguo Li<author:sep>Gim Hee Lee<author:sep>Liwei Wang;http://arxiv.org/pdf/2303.11052v3;cs.CV;;nerf
2303.10709v1;http://arxiv.org/abs/2303.10709v1;2023-03-19;NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental  LiDAR Odometry and Mapping;"Simultaneously odometry and mapping using LiDAR data is an important task for
mobile systems to achieve full autonomy in large-scale environments. However,
most existing LiDAR-based methods prioritize tracking quality over
reconstruction quality. Although the recently developed neural radiance fields
(NeRF) have shown promising advances in implicit reconstruction for indoor
environments, the problem of simultaneous odometry and mapping for large-scale
scenarios using incremental LiDAR data remains unexplored. To bridge this gap,
in this paper, we propose a novel NeRF-based LiDAR odometry and mapping
approach, NeRF-LOAM, consisting of three modules neural odometry, neural
mapping, and mesh reconstruction. All these modules utilize our proposed neural
signed distance function, which separates LiDAR points into ground and
non-ground points to reduce Z-axis drift, optimizes odometry and voxel
embeddings concurrently, and in the end generates dense smooth mesh maps of the
environment. Moreover, this joint optimization allows our NeRF-LOAM to be
pre-trained free and exhibit strong generalization abilities when applied to
different environments. Extensive evaluations on three publicly available
datasets demonstrate that our approach achieves state-of-the-art odometry and
mapping performance, as well as a strong generalization in large-scale
environments utilizing LiDAR data. Furthermore, we perform multiple ablation
studies to validate the effectiveness of our network design. The implementation
of our approach will be made available at
https://github.com/JunyuanDeng/NeRF-LOAM.";Junyuan Deng<author:sep>Xieyuanli Chen<author:sep>Songpengcheng Xia<author:sep>Zhen Sun<author:sep>Guoqing Liu<author:sep>Wenxian Yu<author:sep>Ling Pei;http://arxiv.org/pdf/2303.10709v1;cs.CV;;nerf
2303.10598v3;http://arxiv.org/abs/2303.10598v3;2023-03-19;StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields;"3D style transfer aims to render stylized novel views of a 3D scene with
multi-view consistency. However, most existing work suffers from a three-way
dilemma over accurate geometry reconstruction, high-quality stylization, and
being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance
Fields), an innovative 3D style transfer technique that resolves the three-way
dilemma by performing style transformation within the feature space of a
radiance field. StyleRF employs an explicit grid of high-level features to
represent 3D scenes, with which high-fidelity geometry can be reliably restored
via volume rendering. In addition, it transforms the grid features according to
the reference style which directly leads to high-quality zero-shot style
transfer. StyleRF consists of two innovative designs. The first is
sampling-invariant content transformation that makes the transformation
invariant to the holistic statistics of the sampled 3D points and accordingly
ensures multi-view consistency. The second is deferred style transformation of
2D feature maps which is equivalent to the transformation of 3D points but
greatly reduces memory footprint without degrading multi-view consistency.
Extensive experiments show that StyleRF achieves superior 3D stylization
quality with precise geometry reconstruction and it can generalize to various
new styles in a zero-shot manner.";Kunhao Liu<author:sep>Fangneng Zhan<author:sep>Yiwen Chen<author:sep>Jiahui Zhang<author:sep>Yingchen Yu<author:sep>Abdulmotaleb El Saddik<author:sep>Shijian Lu<author:sep>Eric Xing;http://arxiv.org/pdf/2303.10598v3;cs.CV;"Accepted to CVPR 2023. Project website:
  https://kunhao-liu.github.io/StyleRF/";
2303.10735v4;http://arxiv.org/abs/2303.10735v4;2023-03-19;SKED: Sketch-guided Text-based 3D Editing;"Text-to-image diffusion models are gradually introduced into computer
graphics, recently enabling the development of Text-to-3D pipelines in an open
domain. However, for interactive editing purposes, local manipulations of
content through a simplistic textual interface can be arduous. Incorporating
user guided sketches with Text-to-image pipelines offers users more intuitive
control. Still, as state-of-the-art Text-to-3D pipelines rely on optimizing
Neural Radiance Fields (NeRF) through gradients from arbitrary rendering views,
conditioning on sketches is not straightforward. In this paper, we present
SKED, a technique for editing 3D shapes represented by NeRFs. Our technique
utilizes as few as two guiding sketches from different views to alter an
existing neural field. The edited region respects the prompt semantics through
a pre-trained diffusion model. To ensure the generated output adheres to the
provided sketches, we propose novel loss functions to generate the desired
edits while preserving the density and radiance of the base instance. We
demonstrate the effectiveness of our proposed method through several
qualitative and quantitative experiments. https://sked-paper.github.io/";Aryan Mikaeili<author:sep>Or Perel<author:sep>Mehdi Safaee<author:sep>Daniel Cohen-Or<author:sep>Ali Mahdavi-Amiri;http://arxiv.org/pdf/2303.10735v4;cs.CV;;nerf
2303.10340v1;http://arxiv.org/abs/2303.10340v1;2023-03-18;3D Data Augmentation for Driving Scenes on Camera;"Driving scenes are extremely diverse and complicated that it is impossible to
collect all cases with human effort alone. While data augmentation is an
effective technique to enrich the training data, existing methods for camera
data in autonomous driving applications are confined to the 2D image plane,
which may not optimally increase data diversity in 3D real-world scenarios. To
this end, we propose a 3D data augmentation approach termed Drive-3DAug, aiming
at augmenting the driving scenes on camera in the 3D space. We first utilize
Neural Radiance Field (NeRF) to reconstruct the 3D models of background and
foreground objects. Then, augmented driving scenes can be obtained by placing
the 3D objects with adapted location and orientation at the pre-defined valid
region of backgrounds. As such, the training database could be effectively
scaled up. However, the 3D object modeling is constrained to the image quality
and the limited viewpoints. To overcome these problems, we modify the original
NeRF by introducing a geometric rectified loss and a symmetric-aware training
strategy. We evaluate our method for the camera-only monocular 3D detection
task on the Waymo and nuScences datasets. The proposed data augmentation
approach contributes to a gain of 1.7% and 1.4% in terms of detection accuracy,
on Waymo and nuScences respectively. Furthermore, the constructed 3D models
serve as digital driving assets and could be recycled for different detectors
or other 3D perception tasks.";Wenwen Tong<author:sep>Jiangwei Xie<author:sep>Tianyu Li<author:sep>Hanming Deng<author:sep>Xiangwei Geng<author:sep>Ruoyi Zhou<author:sep>Dingchen Yang<author:sep>Bo Dai<author:sep>Lewei Lu<author:sep>Hongyang Li;http://arxiv.org/pdf/2303.10340v1;cs.CV;;nerf
2303.09952v2;http://arxiv.org/abs/2303.09952v2;2023-03-17;Single-view Neural Radiance Fields with Depth Teacher;"Neural Radiance Fields (NeRF) have been proposed for photorealistic novel
view rendering. However, it requires many different views of one scene for
training. Moreover, it has poor generalizations to new scenes and requires
retraining or fine-tuning on each scene. In this paper, we develop a new NeRF
model for novel view synthesis using only a single image as input. We propose
to combine the (coarse) planar rendering and the (fine) volume rendering to
achieve higher rendering quality and better generalizations. We also design a
depth teacher net that predicts dense pseudo depth maps to supervise the joint
rendering mechanism and boost the learning of consistent 3D geometry. We
evaluate our method on three challenging datasets. It outperforms
state-of-the-art single-view NeRFs by achieving 5$\sim$20\% improvements in
PSNR and reducing 20$\sim$50\% of the errors in the depth rendering. It also
shows excellent generalization abilities to unseen data without the need to
fine-tune on each new scene.";Yurui Chen<author:sep>Chun Gu<author:sep>Feihu Zhang<author:sep>Li Zhang;http://arxiv.org/pdf/2303.09952v2;cs.CV;;nerf
2303.10083v1;http://arxiv.org/abs/2303.10083v1;2023-03-17;$α$Surf: Implicit Surface Reconstruction for Semi-Transparent and  Thin Objects with Decoupled Geometry and Opacity;"Implicit surface representations such as the signed distance function (SDF)
have emerged as a promising approach for image-based surface reconstruction.
However, existing optimization methods assume solid surfaces and are therefore
unable to properly reconstruct semi-transparent surfaces and thin structures,
which also exhibit low opacity due to the blending effect with the background.
While neural radiance field (NeRF) based methods can model semi-transparency
and achieve photo-realistic quality in synthesized novel views, their
volumetric geometry representation tightly couples geometry and opacity, and
therefore cannot be easily converted into surfaces without introducing
artifacts. We present $\alpha$Surf, a novel surface representation with
decoupled geometry and opacity for the reconstruction of semi-transparent and
thin surfaces where the colors mix. Ray-surface intersections on our
representation can be found in closed-form via analytical solutions of cubic
polynomials, avoiding Monte-Carlo sampling and is fully differentiable by
construction. Our qualitative and quantitative evaluations show that our
approach can accurately reconstruct surfaces with semi-transparent and thin
parts with fewer artifacts, achieving better reconstruction quality than
state-of-the-art SDF and NeRF methods. Website: https://alphasurf.netlify.app/";Tianhao Wu<author:sep>Hanxue Liang<author:sep>Fangcheng Zhong<author:sep>Gernot Riegler<author:sep>Shimon Vainer<author:sep>Cengiz Oztireli;http://arxiv.org/pdf/2303.10083v1;cs.CV;;nerf
2303.09553v1;http://arxiv.org/abs/2303.09553v1;2023-03-16;LERF: Language Embedded Radiance Fields;"Humans describe the physical world using natural language to refer to
specific 3D locations based on a vast range of properties: visual appearance,
semantics, abstract associations, or actionable affordances. In this work we
propose Language Embedded Radiance Fields (LERFs), a method for grounding
language embeddings from off-the-shelf models like CLIP into NeRF, which enable
these types of open-ended language queries in 3D. LERF learns a dense,
multi-scale language field inside NeRF by volume rendering CLIP embeddings
along training rays, supervising these embeddings across training views to
provide multi-view consistency and smooth the underlying language field. After
optimization, LERF can extract 3D relevancy maps for a broad range of language
prompts interactively in real-time, which has potential use cases in robotics,
understanding vision-language models, and interacting with 3D scenes. LERF
enables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings
without relying on region proposals or masks, supporting long-tail
open-vocabulary queries hierarchically across the volume. The project website
can be found at https://lerf.io .";Justin Kerr<author:sep>Chung Min Kim<author:sep>Ken Goldberg<author:sep>Angjoo Kanazawa<author:sep>Matthew Tancik;http://arxiv.org/pdf/2303.09553v1;cs.CV;Project website can be found at https://lerf.io;nerf
2303.09431v1;http://arxiv.org/abs/2303.09431v1;2023-03-16;NeRFMeshing: Distilling Neural Radiance Fields into  Geometrically-Accurate 3D Meshes;"With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis
has recently made a big leap forward. At the core, NeRF proposes that each 3D
point can emit radiance, allowing to conduct view synthesis using
differentiable volumetric rendering. While neural radiance fields can
accurately represent 3D scenes for computing the image rendering, 3D meshes are
still the main scene representation supported by most computer graphics and
simulation pipelines, enabling tasks such as real time rendering and
physics-based simulations. Obtaining 3D meshes from neural radiance fields
still remains an open challenge since NeRFs are optimized for view synthesis,
not enforcing an accurate underlying geometry on the radiance field. We thus
propose a novel compact and flexible architecture that enables easy 3D surface
reconstruction from any NeRF-driven approach. Upon having trained the radiance
field, we distill the volumetric 3D representation into a Signed Surface
Approximation Network, allowing easy extraction of the 3D mesh and appearance.
Our final 3D mesh is physically accurate and can be rendered in real time on an
array of devices.";Marie-Julie Rakotosaona<author:sep>Fabian Manhardt<author:sep>Diego Martin Arroyo<author:sep>Michael Niemeyer<author:sep>Abhijit Kundu<author:sep>Federico Tombari;http://arxiv.org/pdf/2303.09431v1;cs.CV;;nerf
2303.09412v4;http://arxiv.org/abs/2303.09412v4;2023-03-16;NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing  Diverse Intrinsic and Extrinsic Camera Parameters;"Novel view synthesis using neural radiance fields (NeRF) is the
state-of-the-art technique for generating high-quality images from novel
viewpoints. Existing methods require a priori knowledge about extrinsic and
intrinsic camera parameters. This limits their applicability to synthetic
scenes, or real-world scenarios with the necessity of a preprocessing step.
Current research on the joint optimization of camera parameters and NeRF
focuses on refining noisy extrinsic camera parameters and often relies on the
preprocessing of intrinsic camera parameters. Further approaches are limited to
cover only one single camera intrinsic. To address these limitations, we
propose a novel end-to-end trainable approach called NeRFtrinsic Four. We
utilize Gaussian Fourier features to estimate extrinsic camera parameters and
dynamically predict varying intrinsic camera parameters through the supervision
of the projection error. Our approach outperforms existing joint optimization
methods on LLFF and BLEFF. In addition to these existing datasets, we introduce
a new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsic
Four is a step forward in joint optimization NeRF-based view synthesis and
enables more realistic and flexible rendering in real-world scenarios with
varying camera parameters.";Hannah Schieber<author:sep>Fabian Deuser<author:sep>Bernhard Egger<author:sep>Norbert Oswald<author:sep>Daniel Roth;http://arxiv.org/pdf/2303.09412v4;cs.CV;;nerf
2303.09153v1;http://arxiv.org/abs/2303.09153v1;2023-03-16;Reliable Image Dehazing by NeRF;"We present an image dehazing algorithm with high quality, wide application,
and no data training or prior needed. We analyze the defects of the original
dehazing model, and propose a new and reliable dehazing reconstruction and
dehazing model based on the combination of optical scattering model and
computer graphics lighting rendering model. Based on the new haze model and the
images obtained by the cameras, we can reconstruct the three-dimensional space,
accurately calculate the objects and haze in the space, and use the
transparency relationship of haze to perform accurate haze removal. To obtain a
3D simulation dataset we used the Unreal 5 computer graphics rendering engine.
In order to obtain real shot data in different scenes, we used fog generators,
array cameras, mobile phones, underwater cameras and drones to obtain haze
data. We use formula derivation, simulation data set and real shot data set
result experimental results to prove the feasibility of the new method.
Compared with various other methods, we are far ahead in terms of calculation
indicators (4 dB higher quality average scene), color remains more natural, and
the algorithm is more robust in different scenarios and best in the subjective
perception.";Zheyan Jin<author:sep>Shiqi Chen<author:sep>Huajun Feng<author:sep>Zhihai Xu<author:sep>Qi Li<author:sep>Yueting Chen;http://arxiv.org/pdf/2303.09153v1;cs.CV;12pages, 8figures;nerf
2303.09554v3;http://arxiv.org/abs/2303.09554v3;2023-03-16;PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D  Supervision;"Impressive progress in generative models and implicit representations gave
rise to methods that can generate 3D shapes of high quality. However, being
able to locally control and edit shapes is another essential property that can
unlock several content creation applications. Local control can be achieved
with part-aware models, but existing methods require 3D supervision and cannot
produce textures. In this work, we devise PartNeRF, a novel part-aware
generative model for editable 3D shape synthesis that does not require any
explicit 3D supervision. Our model generates objects as a set of locally
defined NeRFs, augmented with an affine transformation. This enables several
editing operations such as applying transformations on parts, mixing parts from
different objects etc. To ensure distinct, manipulable parts we enforce a hard
assignment of rays to parts that makes sure that the color of each ray is only
determined by a single NeRF. As a result, altering one part does not affect the
appearance of the others. Evaluations on various ShapeNet categories
demonstrate the ability of our model to generate editable 3D objects of
improved fidelity, compared to previous part-based generative approaches that
require 3D supervision or models relying on NeRFs.";Konstantinos Tertikas<author:sep>Despoina Paschalidou<author:sep>Boxiao Pan<author:sep>Jeong Joon Park<author:sep>Mikaela Angelina Uy<author:sep>Ioannis Emiris<author:sep>Yannis Avrithis<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2303.09554v3;cs.CV;"To appear in CVPR 2023, Project Page:
  https://ktertikas.github.io/part_nerf";nerf
2303.08695v1;http://arxiv.org/abs/2303.08695v1;2023-03-15;RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or  missing camera parameters;"Novel view synthesis (NVS) is a challenging task in computer vision that
involves synthesizing new views of a scene from a limited set of input images.
Neural Radiance Fields (NeRF) have emerged as a powerful approach to address
this problem, but they require accurate knowledge of camera \textit{intrinsic}
and \textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM)
and multi-view stereo (MVS) approaches have been used to extract camera
parameters, but these methods can be unreliable and may fail in certain cases.
In this paper, we propose a novel technique that leverages unposed images from
dynamic datasets, such as the NVIDIA dynamic scenes dataset, to learn camera
parameters directly from data. Our approach is highly extensible and can be
integrated into existing NeRF architectures with minimal modifications. We
demonstrate the effectiveness of our method on a variety of static and dynamic
scenes and show that it outperforms traditional SfM and MVS approaches. The
code for our method is publicly available at
\href{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}.
Our approach offers a promising new direction for improving the accuracy and
robustness of NVS using NeRF, and we anticipate that it will be a valuable tool
for a wide range of applications in computer vision and graphics.";Shuja Khalid<author:sep>Frank Rudzicz;http://arxiv.org/pdf/2303.08695v1;cs.CV;;nerf
2303.08808v1;http://arxiv.org/abs/2303.08808v1;2023-03-15;Mesh Strikes Back: Fast and Efficient Human Reconstruction from RGB  videos;"Human reconstruction and synthesis from monocular RGB videos is a challenging
problem due to clothing, occlusion, texture discontinuities and sharpness, and
framespecific pose changes. Many methods employ deferred rendering, NeRFs and
implicit methods to represent clothed humans, on the premise that mesh-based
representations cannot capture complex clothing and textures from RGB,
silhouettes, and keypoints alone. We provide a counter viewpoint to this
fundamental premise by optimizing a SMPL+D mesh and an efficient,
multi-resolution texture representation using only RGB images, binary
silhouettes and sparse 2D keypoints. Experimental results demonstrate that our
approach is more capable of capturing geometric details compared to visual
hull, mesh-based methods. We show competitive novel view synthesis and
improvements in novel pose synthesis compared to NeRF-based methods, which
introduce noticeable, unwanted artifacts. By restricting the solution space to
the SMPL+D model combined with differentiable rendering, we obtain dramatic
speedups in compute, training times (up to 24x) and inference times (up to
192x). Our method therefore can be used as is or as a fast initialization to
NeRF-based methods.";Rohit Jena<author:sep>Pratik Chaudhari<author:sep>James Gee<author:sep>Ganesh Iyer<author:sep>Siddharth Choudhary<author:sep>Brandon M. Smith;http://arxiv.org/pdf/2303.08808v1;cs.CV;;nerf
2303.08717v1;http://arxiv.org/abs/2303.08717v1;2023-03-15;Re-ReND: Real-time Rendering of NeRFs across Devices;"This paper proposes a novel approach for rendering a pre-trained Neural
Radiance Field (NeRF) in real-time on resource-constrained devices. We
introduce Re-ReND, a method enabling Real-time Rendering of NeRFs across
Devices. Re-ReND is designed to achieve real-time performance by converting the
NeRF into a representation that can be efficiently processed by standard
graphics pipelines. The proposed method distills the NeRF by extracting the
learned density into a mesh, while the learned color information is factorized
into a set of matrices that represent the scene's light field. Factorization
implies the field is queried via inexpensive MLP-free matrix multiplications,
while using a light field allows rendering a pixel by querying the field a
single time-as opposed to hundreds of queries when employing a radiance field.
Since the proposed representation can be implemented using a fragment shader,
it can be directly integrated with standard rasterization frameworks. Our
flexible implementation can render a NeRF in real-time with low memory
requirements and on a wide range of resource-constrained devices, including
mobiles and AR/VR headsets. Notably, we find that Re-ReND can achieve over a
2.6-fold increase in rendering speed versus the state-of-the-art without
perceptible losses in quality.";Sara Rojas<author:sep>Jesus Zarzar<author:sep>Juan Camilo Perez<author:sep>Artsiom Sanakoyeu<author:sep>Ali Thabet<author:sep>Albert Pumarola<author:sep>Bernard Ghanem;http://arxiv.org/pdf/2303.08717v1;cs.CV;;nerf
2303.08370v1;http://arxiv.org/abs/2303.08370v1;2023-03-15;Harnessing Low-Frequency Neural Fields for Few-Shot View Synthesis;"Neural Radiance Fields (NeRF) have led to breakthroughs in the novel view
synthesis problem. Positional Encoding (P.E.) is a critical factor that brings
the impressive performance of NeRF, where low-dimensional coordinates are
mapped to high-dimensional space to better recover scene details. However,
blindly increasing the frequency of P.E. leads to overfitting when the
reconstruction problem is highly underconstrained, \eg, few-shot images for
training. We harness low-frequency neural fields to regularize high-frequency
neural fields from overfitting to better address the problem of few-shot view
synthesis. We propose reconstructing with a low-frequency only field and then
finishing details with a high-frequency equipped field. Unlike most existing
solutions that regularize the output space (\ie, rendered images), our
regularization is conducted in the input space (\ie, signal frequency). We
further propose a simple-yet-effective strategy for tuning the frequency to
avoid overfitting few-shot inputs: enforcing consistency among the frequency
domain of rendered 2D images. Thanks to the input space regularizing scheme,
our method readily applies to inputs beyond spatial locations, such as the time
dimension in dynamic scenes. Comparisons with state-of-the-art on both
synthetic and natural datasets validate the effectiveness of our proposed
solution for few-shot view synthesis. Code is available at
\href{https://github.com/lsongx/halo}{https://github.com/lsongx/halo}.";Liangchen Song<author:sep>Zhong Li<author:sep>Xuan Gong<author:sep>Lele Chen<author:sep>Zhang Chen<author:sep>Yi Xu<author:sep>Junsong Yuan;http://arxiv.org/pdf/2303.08370v1;cs.CV;;nerf
2303.07937v3;http://arxiv.org/abs/2303.07937v3;2023-03-14;Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D  Generation;"Text-to-3D generation has shown rapid progress in recent days with the advent
of score distillation, a methodology of using pretrained text-to-2D diffusion
models to optimize neural radiance field (NeRF) in the zero-shot setting.
However, the lack of 3D awareness in the 2D diffusion models destabilizes score
distillation-based methods from reconstructing a plausible 3D scene. To address
this issue, we propose 3DFuse, a novel framework that incorporates 3D awareness
into pretrained 2D diffusion models, enhancing the robustness and 3D
consistency of score distillation-based methods. We realize this by first
constructing a coarse 3D structure of a given text prompt and then utilizing
projected, view-specific depth map as a condition for the diffusion model.
Additionally, we introduce a training strategy that enables the 2D diffusion
model learns to handle the errors and sparsity within the coarse 3D structure
for robust generation, as well as a method for ensuring semantic consistency
throughout all viewpoints of the scene. Our framework surpasses the limitations
of prior arts, and has significant implications for 3D consistent generation of
2D diffusion models.";Junyoung Seo<author:sep>Wooseok Jang<author:sep>Min-Seop Kwak<author:sep>Jaehoon Ko<author:sep>Hyeonsu Kim<author:sep>Junho Kim<author:sep>Jin-Hwa Kim<author:sep>Jiyoung Lee<author:sep>Seungryong Kim;http://arxiv.org/pdf/2303.07937v3;cs.CV;Project page https://ku-cvlab.github.io/3DFuse/;nerf
2303.08096v2;http://arxiv.org/abs/2303.08096v2;2023-03-14;MELON: NeRF with Unposed Images in SO(3);"Neural radiance fields enable novel-view synthesis and scene reconstruction
with photorealistic quality from a few images, but require known and accurate
camera poses. Conventional pose estimation algorithms fail on smooth or
self-similar scenes, while methods performing inverse rendering from unposed
views require a rough initialization of the camera orientations. The main
difficulty of pose estimation lies in real-life objects being almost invariant
under certain transformations, making the photometric distance between rendered
views non-convex with respect to the camera parameters. Using an equivalence
relation that matches the distribution of local minima in camera space, we
reduce this space to its quotient set, in which pose estimation becomes a more
convex problem. Using a neural-network to regularize pose estimation, we
demonstrate that our method - MELON - can reconstruct a neural radiance field
from unposed images with state-of-the-art accuracy while requiring ten times
fewer views than adversarial approaches.";Axel Levy<author:sep>Mark Matthews<author:sep>Matan Sela<author:sep>Gordon Wetzstein<author:sep>Dmitry Lagun;http://arxiv.org/pdf/2303.08096v2;cs.CV;;nerf
2303.07596v2;http://arxiv.org/abs/2303.07596v2;2023-03-14;Frequency-Modulated Point Cloud Rendering with Easy Editing;"We develop an effective point cloud rendering pipeline for novel view
synthesis, which enables high fidelity local detail reconstruction, real-time
rendering and user-friendly editing. In the heart of our pipeline is an
adaptive frequency modulation module called Adaptive Frequency Net (AFNet),
which utilizes a hypernetwork to learn the local texture frequency encoding
that is consecutively injected into adaptive frequency activation layers to
modulate the implicit radiance signal. This mechanism improves the frequency
expressive ability of the network with richer frequency basis support, only at
a small computational budget. To further boost performance, a preprocessing
module is also proposed for point cloud geometry optimization via point opacity
estimation. In contrast to implicit rendering, our pipeline supports
high-fidelity interactive editing based on point cloud manipulation. Extensive
experimental results on NeRF-Synthetic, ScanNet, DTU and Tanks and Temples
datasets demonstrate the superior performances achieved by our method in terms
of PSNR, SSIM and LPIPS, in comparison to the state-of-the-art.";Yi Zhang<author:sep>Xiaoyang Huang<author:sep>Bingbing Ni<author:sep>Teng Li<author:sep>Wenjun Zhang;http://arxiv.org/pdf/2303.07596v2;cs.CV;Accepted by CVPR 2023;nerf
2303.07634v2;http://arxiv.org/abs/2303.07634v2;2023-03-14;I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via  Raytracing in Neural SDFs;"In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene
reconstruction and editing using differentiable Monte Carlo raytracing on
neural signed distance fields (SDFs). Our holistic neural SDF-based framework
jointly recovers the underlying shapes, incident radiance and materials from
multi-view images. We introduce a novel bubble loss for fine-grained small
objects and error-guided adaptive sampling scheme to largely improve the
reconstruction quality on large-scale indoor scenes. Further, we propose to
decompose the neural radiance field into spatially-varying material of the
scene as a neural field through surface-based, differentiable Monte Carlo
raytracing and emitter semantic segmentations, which enables physically based
and photorealistic scene relighting and editing applications. Through a number
of qualitative and quantitative experiments, we demonstrate the superior
quality of our method on indoor scene reconstruction, novel view synthesis, and
scene editing compared to state-of-the-art baselines.";Jingsen Zhu<author:sep>Yuchi Huo<author:sep>Qi Ye<author:sep>Fujun Luan<author:sep>Jifan Li<author:sep>Dianbing Xi<author:sep>Lisha Wang<author:sep>Rui Tang<author:sep>Wei Hua<author:sep>Hujun Bao<author:sep>Rui Wang;http://arxiv.org/pdf/2303.07634v2;cs.CV;"Accepted by CVPR 2023, project page:
  https://jingsenzhu.github.io/i2-sdf";
2303.07653v2;http://arxiv.org/abs/2303.07653v2;2023-03-14;NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from  Multi-view Images;"We study the problem of reconstructing 3D feature curves of an object from a
set of calibrated multi-view images. To do so, we learn a neural implicit field
representing the density distribution of 3D edges which we refer to as Neural
Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based
rendering loss where a 2D edge map is rendered at a given view and is compared
to the ground-truth edge map extracted from the image of that view. The
rendering-based differentiable optimization of NEF fully exploits 2D edge
detection, without needing a supervision of 3D edges, a 3D geometric operator
or cross-view edge correspondence. Several technical designs are devised to
ensure learning a range-limited and view-independent NEF for robust edge
extraction. The final parametric 3D curves are extracted from NEF with an
iterative optimization method. On our benchmark with synthetic data, we
demonstrate that NEF outperforms existing state-of-the-art methods on all
metrics. Project page: https://yunfan1202.github.io/NEF/.";Yunfan Ye<author:sep>Renjiao Yi<author:sep>Zhirui Gao<author:sep>Chenyang Zhu<author:sep>Zhiping Cai<author:sep>Kai Xu;http://arxiv.org/pdf/2303.07653v2;cs.CV;CVPR 2023;nerf
2303.06919v2;http://arxiv.org/abs/2303.06919v2;2023-03-13;NeRFLiX: High-Quality Neural View Synthesis by Learning a  Degradation-Driven Inter-viewpoint MiXer;"Neural radiance fields (NeRF) show great success in novel view synthesis.
However, in real-world scenes, recovering high-quality details from the source
images is still challenging for the existing NeRF-based approaches, due to the
potential imperfect calibration information and scene representation
inaccuracy. Even with high-quality training frames, the synthetic novel views
produced by NeRF models still suffer from notable rendering artifacts, such as
noise, blur, etc. Towards to improve the synthesis quality of NeRF-based
approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by
learning a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for existing deep neural networks. Moreover, beyond the degradation
removal, we propose an inter-viewpoint aggregation framework that is able to
fuse highly related high-quality training images, pushing the performance of
cutting-edge NeRF models to entirely new levels and producing highly
photo-realistic synthetic views.";Kun Zhou<author:sep>Wenbo Li<author:sep>Yi Wang<author:sep>Tao Hu<author:sep>Nianjuan Jiang<author:sep>Xiaoguang Han<author:sep>Jiangbo Lu;http://arxiv.org/pdf/2303.06919v2;cs.CV;"Accepted to CVPR 2023; Project Page: see
  https://redrock303.github.io/nerflix/";nerf
2303.07418v1;http://arxiv.org/abs/2303.07418v1;2023-03-13;FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency  Regularization;"Novel view synthesis with sparse inputs is a challenging problem for neural
radiance fields (NeRF). Recent efforts alleviate this challenge by introducing
external supervision, such as pre-trained models and extra depth signals, and
by non-trivial patch-based rendering. In this paper, we present Frequency
regularized NeRF (FreeNeRF), a surprisingly simple baseline that outperforms
previous methods with minimal modifications to the plain NeRF. We analyze the
key challenges in few-shot neural rendering and find that frequency plays an
important role in NeRF's training. Based on the analysis, we propose two
regularization terms. One is to regularize the frequency range of NeRF's
inputs, while the other is to penalize the near-camera density fields. Both
techniques are ``free lunches'' at no additional computational cost. We
demonstrate that even with one line of code change, the original NeRF can
achieve similar performance as other complicated methods in the few-shot
setting. FreeNeRF achieves state-of-the-art performance across diverse
datasets, including Blender, DTU, and LLFF. We hope this simple baseline will
motivate a rethinking of the fundamental role of frequency in NeRF's training
under the low-data regime and beyond.";Jiawei Yang<author:sep>Marco Pavone<author:sep>Yue Wang;http://arxiv.org/pdf/2303.07418v1;cs.CV;"Project page: https://jiawei-yang.github.io/FreeNeRF/, Code at:
  https://github.com/Jiawei-Yang/FreeNeRF";nerf
2303.06335v3;http://arxiv.org/abs/2303.06335v3;2023-03-11;Just Flip: Flipped Observation Generation and Optimization for Neural  Radiance Fields to Cover Unobserved View;"With the advent of Neural Radiance Field (NeRF), representing 3D scenes
through multiple observations has shown remarkable improvements in performance.
Since this cutting-edge technique is able to obtain high-resolution renderings
by interpolating dense 3D environments, various approaches have been proposed
to apply NeRF for the spatial understanding of robot perception. However,
previous works are challenging to represent unobserved scenes or views on the
unexplored robot trajectory, as these works do not take into account 3D
reconstruction without observation information. To overcome this problem, we
propose a method to generate flipped observation in order to cover unexisting
observation for unexplored robot trajectory. To achieve this, we propose a data
augmentation method for 3D reconstruction using NeRF by flipping observed
images, and estimating flipped camera 6DOF poses. Our technique exploits the
property of objects being geometrically symmetric, making it simple but fast
and powerful, thereby making it suitable for robotic applications where
real-time performance is important. We demonstrate that our method
significantly improves three representative perceptual quality measures on the
NeRF synthetic dataset.";Minjae Lee<author:sep>Kyeongsu Kang<author:sep>Hyeonwoo Yu;http://arxiv.org/pdf/2303.06335v3;cs.RO;;nerf
2303.06226v2;http://arxiv.org/abs/2303.06226v2;2023-03-10;NeRFlame: FLAME-based conditioning of NeRF for 3D face rendering;"Traditional 3D face models are based on mesh representations with texture.
One of the most important models is FLAME (Faces Learned with an Articulated
Model and Expressions), which produces meshes of human faces that are fully
controllable. Unfortunately, such models have problems with capturing geometric
and appearance details. In contrast to mesh representation, the neural radiance
field (NeRF) produces extremely sharp renders. However, implicit methods are
hard to animate and do not generalize well to unseen expressions. It is not
trivial to effectively control NeRF models to obtain face manipulation.
  The present paper proposes a novel approach, named NeRFlame, which combines
the strengths of both NeRF and FLAME methods. Our method enables high-quality
rendering capabilities of NeRF while also offering complete control over the
visual appearance, similar to FLAME. In contrast to traditional NeRF-based
structures that use neural networks for RGB color and volume density modeling,
our approach utilizes the FLAME mesh as a distinct density volume.
Consequently, color values exist only in the vicinity of the FLAME mesh. This
FLAME framework is seamlessly incorporated into the NeRF architecture for
predicting RGB colors, enabling our model to explicitly represent volume
density and implicitly capture RGB colors.";Wojciech Zając<author:sep>Joanna Waczyńska<author:sep>Piotr Borycki<author:sep>Jacek Tabor<author:sep>Maciej Zięba<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2303.06226v2;cs.CV;;nerf
2303.05735v6;http://arxiv.org/abs/2303.05735v6;2023-03-10;Hardware Acceleration of Neural Graphics;"Rendering and inverse-rendering algorithms that drive conventional computer
graphics have recently been superseded by neural representations (NR). NRs have
recently been used to learn the geometric and the material properties of the
scenes and use the information to synthesize photorealistic imagery, thereby
promising a replacement for traditional rendering algorithms with scalable
quality and predictable performance. In this work we ask the question: Does
neural graphics (NG) need hardware support? We studied representative NG
applications showing that, if we want to render 4k res. at 60FPS there is a gap
of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,
there is an even larger gap of 2-4 OOM between the desired performance and the
required system power. We identify that the input encoding and the MLP kernels
are the performance bottlenecks, consuming 72%,60% and 59% of application time
for multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,
respectively. We propose a NG processing cluster, a scalable and flexible
hardware architecture that directly accelerates the input encoding and MLP
kernels through dedicated engines and supports a wide range of NG applications.
We also accelerate the rest of the kernels by fusing them together in Vulkan,
which leads to 9.94X kernel-level performance improvement compared to un-fused
implementation of the pre-processing and the post-processing kernels. Our
results show that, NGPC gives up to 58X end-to-end application-level
performance improvement, for multi res. hashgrid encoding on average across the
four NG applications, the performance benefits are 12X,20X,33X and 39X for the
scaling factor of 8,16,32 and 64, respectively. Our results show that with
multi res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPS
for NeRF and 8k res. at 120FPS for all our other NG applications.";Muhammad Husnain Mubarik<author:sep>Ramakrishna Kanungo<author:sep>Tobias Zirr<author:sep>Rakesh Kumar;http://arxiv.org/pdf/2303.05735v6;cs.AR;;nerf
2303.05807v2;http://arxiv.org/abs/2303.05807v2;2023-03-10;Aleth-NeRF: Low-light Condition View Synthesis with Concealing Fields;"Common capture low-light scenes are challenging for most computer vision
techniques, including Neural Radiance Fields (NeRF). Vanilla NeRF is
viewer-centred simplifies the rendering process only as light emission from 3D
locations in the viewing direction, thus failing to model the low-illumination
induced darkness. Inspired by the emission theory of ancient Greeks that visual
perception is accomplished by rays casting from eyes, we make slight
modifications on vanilla NeRF to train on multiple views of low-light scenes,
we can thus render out the well-lit scene in an unsupervised manner. We
introduce a surrogate concept, Concealing Fields, that reduces the transport of
light during the volume rendering stage. Specifically, our proposed method,
Aleth-NeRF, directly learns from the dark image to understand volumetric object
representation and concealing field under priors. By simply eliminating
Concealing Fields, we can render a single or multi-view well-lit image(s) and
gain superior performance over other 2D low-light enhancement methods.
Additionally, we collect the first paired LOw-light and normal-light Multi-view
(LOM) datasets for future research. This version is invalid, please refer to
our new AAAI version: arXiv:2312.09093";Ziteng Cui<author:sep>Lin Gu<author:sep>Xiao Sun<author:sep>Xianzheng Ma<author:sep>Yu Qiao<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2303.05807v2;cs.CV;"website page: https://cuiziteng.github.io/Aleth_NeRF_web/, refer to
  new version: arXiv:2312.09093";nerf
2303.05775v1;http://arxiv.org/abs/2303.05775v1;2023-03-10;Self-NeRF: A Self-Training Pipeline for Few-Shot Neural Radiance Fields;"Recently, Neural Radiance Fields (NeRF) have emerged as a potent method for
synthesizing novel views from a dense set of images. Despite its impressive
performance, NeRF is plagued by its necessity for numerous calibrated views and
its accuracy diminishes significantly in a few-shot setting. To address this
challenge, we propose Self-NeRF, a self-evolved NeRF that iteratively refines
the radiance fields with very few number of input views, without incorporating
additional priors. Basically, we train our model under the supervision of
reference and unseen views simultaneously in an iterative procedure. In each
iteration, we label unseen views with the predicted colors or warped pixels
generated by the model from the preceding iteration. However, these expanded
pseudo-views are afflicted by imprecision in color and warping artifacts, which
degrades the performance of NeRF. To alleviate this issue, we construct an
uncertainty-aware NeRF with specialized embeddings. Some techniques such as
cone entropy regularization are further utilized to leverage the pseudo-views
in the most efficient manner. Through experiments under various settings, we
verified that our Self-NeRF is robust to input with uncertainty and surpasses
existing methods when trained on limited training data.";Jiayang Bai<author:sep>Letian Huang<author:sep>Wen Gong<author:sep>Jie Guo<author:sep>Yanwen Guo;http://arxiv.org/pdf/2303.05775v1;cs.CV;11 pages, 11 figures;nerf
2303.06138v4;http://arxiv.org/abs/2303.06138v4;2023-03-10;Learning Object-Centric Neural Scattering Functions for Free-Viewpoint  Relighting and Scene Composition;"Photorealistic object appearance modeling from 2D images is a constant topic
in vision and graphics. While neural implicit methods (such as Neural Radiance
Fields) have shown high-fidelity view synthesis results, they cannot relight
the captured objects. More recent neural inverse rendering approaches have
enabled object relighting, but they represent surface properties as simple
BRDFs, and therefore cannot handle translucent objects. We propose
Object-Centric Neural Scattering Functions (OSFs) for learning to reconstruct
object appearance from only images. OSFs not only support free-viewpoint object
relighting, but also can model both opaque and translucent objects. While
accurately modeling subsurface light transport for translucent objects can be
highly complex and even intractable for neural methods, OSFs learn to
approximate the radiance transfer from a distant light to an outgoing direction
at any spatial location. This approximation avoids explicitly modeling complex
subsurface scattering, making learning a neural implicit model tractable.
Experiments on real and synthetic data show that OSFs accurately reconstruct
appearances for both opaque and translucent objects, allowing faithful
free-viewpoint relighting as well as scene composition.";Hong-Xing Yu<author:sep>Michelle Guo<author:sep>Alireza Fathi<author:sep>Yen-Yu Chang<author:sep>Eric Ryan Chan<author:sep>Ruohan Gao<author:sep>Thomas Funkhouser<author:sep>Jiajun Wu;http://arxiv.org/pdf/2303.06138v4;cs.CV;"Journal extension of arXiv:2012.08503 (TMLR 2023). The first two
  authors contributed equally to this work. Project page:
  https://kovenyu.com/osf/";
2303.05835v1;http://arxiv.org/abs/2303.05835v1;2023-03-10;You Only Train Once: Multi-Identity Free-Viewpoint Neural Human  Rendering from Monocular Videos;"We introduce You Only Train Once (YOTO), a dynamic human generation
framework, which performs free-viewpoint rendering of different human
identities with distinct motions, via only one-time training from monocular
videos. Most prior works for the task require individualized optimization for
each input video that contains a distinct human identity, leading to a
significant amount of time and resources for the deployment, thereby impeding
the scalability and the overall application potential of the system. In this
paper, we tackle this problem by proposing a set of learnable identity codes to
expand the capability of the framework for multi-identity free-viewpoint
rendering, and an effective pose-conditioned code query mechanism to finely
model the pose-dependent non-rigid motions. YOTO optimizes neural radiance
fields (NeRF) by utilizing designed identity codes to condition the model for
learning various canonical T-pose appearances in a single shared volumetric
representation. Besides, our joint learning of multiple identities within a
unified model incidentally enables flexible motion transfer in high-quality
photo-realistic renderings for all learned appearances. This capability expands
its potential use in important applications, including Virtual Reality. We
present extensive experimental results on ZJU-MoCap and PeopleSnapshot to
clearly demonstrate the effectiveness of our proposed model. YOTO shows
state-of-the-art performance on all evaluation metrics while showing
significant benefits in training and inference efficiency as well as rendering
quality. The code and model will be made publicly available soon.";Jaehyeok Kim<author:sep>Dongyoon Wee<author:sep>Dan Xu;http://arxiv.org/pdf/2303.05835v1;cs.CV;;nerf
2303.05703v2;http://arxiv.org/abs/2303.05703v2;2023-03-10;MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field;"We present MovingParts, a NeRF-based method for dynamic scene reconstruction
and part discovery. We consider motion as an important cue for identifying
parts, that all particles on the same part share the common motion pattern.
From the perspective of fluid simulation, existing deformation-based methods
for dynamic NeRF can be seen as parameterizing the scene motion under the
Eulerian view, i.e., focusing on specific locations in space through which the
fluid flows as time passes. However, it is intractable to extract the motion of
constituting objects or parts using the Eulerian view representation. In this
work, we introduce the dual Lagrangian view and enforce representations under
the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian
view, we parameterize the scene motion by tracking the trajectory of particles
on objects. The Lagrangian view makes it convenient to discover parts by
factorizing the scene motion as a composition of part-level rigid motions.
Experimentally, our method can achieve fast and high-quality dynamic scene
reconstruction from even a single moving camera, and the induced part-based
representation allows direct applications of part tracking, animation, 3D scene
editing, etc.";Kaizhi Yang<author:sep>Xiaoshuai Zhang<author:sep>Zhiao Huang<author:sep>Xuejin Chen<author:sep>Zexiang Xu<author:sep>Hao Su;http://arxiv.org/pdf/2303.05703v2;cs.CV;Project Page: https://silenkzyoung.github.io/MovingParts-WebPage/;nerf
2303.05512v1;http://arxiv.org/abs/2303.05512v1;2023-03-09;PAC-NeRF: Physics Augmented Continuum Neural Radiance Fields for  Geometry-Agnostic System Identification;"Existing approaches to system identification (estimating the physical
parameters of an object) from videos assume known object geometries. This
precludes their applicability in a vast majority of scenes where object
geometries are complex or unknown. In this work, we aim to identify parameters
characterizing a physical system from a set of multi-view videos without any
assumption on object geometry or topology. To this end, we propose ""Physics
Augmented Continuum Neural Radiance Fields"" (PAC-NeRF), to estimate both the
unknown geometry and physical parameters of highly dynamic objects from
multi-view videos. We design PAC-NeRF to only ever produce physically plausible
states by enforcing the neural radiance field to follow the conservation laws
of continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian
representation of the neural radiance field, i.e., we use the Eulerian grid
representation for NeRF density and color fields, while advecting the neural
radiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian
representation seamlessly blends efficient neural rendering with the material
point method (MPM) for robust differentiable physics simulation. We validate
the effectiveness of our proposed framework on geometry and physical parameter
estimation over a vast range of materials, including elastic bodies,
plasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate
significant performance gain on most tasks.";Xuan Li<author:sep>Yi-Ling Qiao<author:sep>Peter Yichen Chen<author:sep>Krishna Murthy Jatavallabhula<author:sep>Ming Lin<author:sep>Chenfanfu Jiang<author:sep>Chuang Gan;http://arxiv.org/pdf/2303.05512v1;cs.CV;"ICLR 2023 Spotlight. Project page:
  https://sites.google.com/view/PAC-NeRF";nerf
2303.04508v2;http://arxiv.org/abs/2303.04508v2;2023-03-08;InFusionSurf: Refining Neural RGB-D Surface Reconstruction Using  Per-Frame Intrinsic Refinement and TSDF Fusion Prior Learning;"We introduce InFusionSurf, a novel approach to enhance the fidelity of neural
radiance field (NeRF) frameworks for 3D surface reconstruction using RGB-D
video frames. Building upon previous methods that have employed feature
encoding to improve optimization speed, we further improve the reconstruction
quality with minimal impact on optimization time by refining depth information.
Our per-frame intrinsic refinement scheme addresses frame-specific blurs caused
by camera motion in each depth frame. Furthermore, InFusionSurf utilizes a
classical real-time 3D surface reconstruction method, the truncated signed
distance field (TSDF) Fusion, as prior knowledge to pretrain the feature grid
to support reconstruction details while accelerating the training. The
quantitative and qualitative experiments comparing the performances of
InFusionSurf against prior work indicate that our method is capable of
accurately reconstructing a scene without sacrificing optimization speed. We
also demonstrate the effectiveness of our per-frame intrinsic refinement and
TSDF Fusion prior learning techniques via an ablation study.";Seunghwan Lee<author:sep>Gwanmo Park<author:sep>Hyewon Son<author:sep>Jiwon Ryu<author:sep>Han Joo Chae;http://arxiv.org/pdf/2303.04508v2;cs.CV;;nerf
2303.04869v2;http://arxiv.org/abs/2303.04869v2;2023-03-08;CROSSFIRE: Camera Relocalization On Self-Supervised Features from an  Implicit Representation;"Beyond novel view synthesis, Neural Radiance Fields are useful for
applications that interact with the real world. In this paper, we use them as
an implicit map of a given scene and propose a camera relocalization algorithm
tailored for this representation. The proposed method enables to compute in
real-time the precise position of a device using a single RGB camera, during
its navigation. In contrast with previous work, we do not rely on pose
regression or photometric alignment but rather use dense local features
obtained through volumetric rendering which are specialized on the scene with a
self-supervised objective. As a result, our algorithm is more accurate than
competitors, able to operate in dynamic outdoor environments with changing
lightning conditions and can be readily integrated in any volumetric neural
renderer.";Arthur Moreau<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Dzmitry Tsishkou<author:sep>Bogdan Stanciulescu<author:sep>Arnaud de La Fortelle;http://arxiv.org/pdf/2303.04869v2;cs.CV;Accepted to ICCV 2023;
2303.04322v2;http://arxiv.org/abs/2303.04322v2;2023-03-08;DroNeRF: Real-time Multi-agent Drone Pose Optimization for Computing  Neural Radiance Fields;"We present a novel optimization algorithm called DroNeRF for the autonomous
positioning of monocular camera drones around an object for real-time 3D
reconstruction using only a few images. Neural Radiance Fields or NeRF, is a
novel view synthesis technique used to generate new views of an object or scene
from a set of input images. Using drones in conjunction with NeRF provides a
unique and dynamic way to generate novel views of a scene, especially with
limited scene capabilities of restricted movements. Our approach focuses on
calculating optimized pose for individual drones while solely depending on the
object geometry without using any external localization system. The unique
camera positioning during the data-capturing phase significantly impacts the
quality of the 3D model. To evaluate the quality of our generated novel views,
we compute different perceptual metrics like the Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure(SSIM). Our work demonstrates the
benefit of using an optimal placement of various drones with limited mobility
to generate perceptually better results.";Dipam Patel<author:sep>Phu Pham<author:sep>Aniket Bera;http://arxiv.org/pdf/2303.04322v2;cs.RO;"To appear in 2023 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2023)";nerf
2303.04086v1;http://arxiv.org/abs/2303.04086v1;2023-03-07;NEPHELE: A Neural Platform for Highly Realistic Cloud Radiance Rendering;"We have recently seen tremendous progress in neural rendering (NR) advances,
i.e., NeRF, for photo-real free-view synthesis. Yet, as a local technique based
on a single computer/GPU, even the best-engineered Instant-NGP or i-NGP cannot
reach real-time performance when rendering at a high resolution, and often
requires huge local computing resources. In this paper, we resort to cloud
rendering and present NEPHELE, a neural platform for highly realistic cloud
radiance rendering. In stark contrast with existing NR approaches, our NEPHELE
allows for more powerful rendering capabilities by combining multiple remote
GPUs and facilitates collaboration by allowing multiple people to view the same
NeRF scene simultaneously. We introduce i-NOLF to employ opacity light fields
for ultra-fast neural radiance rendering in a one-query-per-ray manner. We
further resemble the Lumigraph with geometry proxies for fast ray querying and
subsequently employ a small MLP to model the local opacity lumishperes for
high-quality rendering. We also adopt Perfect Spatial Hashing in i-NOLF to
enhance cache coherence. As a result, our i-NOLF achieves an order of magnitude
performance gain in terms of efficiency than i-NGP, especially for the
multi-user multi-viewpoint setting under cloud rendering scenarios. We further
tailor a task scheduler accompanied by our i-NOLF representation and
demonstrate the advance of our methodological design through a comprehensive
cloud platform, consisting of a series of cooperated modules, i.e., render
farms, task assigner, frame composer, and detailed streaming strategies. Using
such a cloud platform compatible with neural rendering, we further showcase the
capabilities of our cloud radiance rendering through a series of applications,
ranging from cloud VR/AR rendering.";Haimin Luo<author:sep>Siyuan Zhang<author:sep>Fuqiang Zhao<author:sep>Haotian Jing<author:sep>Penghao Wang<author:sep>Zhenxiao Yu<author:sep>Dongxue Yan<author:sep>Junran Ding<author:sep>Boyuan Zhang<author:sep>Qiang Hu<author:sep>Shu Yin<author:sep>Lan Xu<author:sep>JIngyi Yu;http://arxiv.org/pdf/2303.04086v1;cs.GR;;nerf
2303.03808v2;http://arxiv.org/abs/2303.03808v2;2023-03-07;Multiscale Tensor Decomposition and Rendering Equation Encoding for View  Synthesis;"Rendering novel views from captured multi-view images has made considerable
progress since the emergence of the neural radiance field. This paper aims to
further advance the quality of view synthesis by proposing a novel approach
dubbed the neural radiance feature field (NRFF). We first propose a multiscale
tensor decomposition scheme to organize learnable features so as to represent
scenes from coarse to fine scales. We demonstrate many benefits of the proposed
multiscale representation, including more accurate scene shape and appearance
reconstruction, and faster convergence compared with the single-scale
representation. Instead of encoding view directions to model view-dependent
effects, we further propose to encode the rendering equation in the feature
space by employing the anisotropic spherical Gaussian mixture predicted from
the proposed multiscale representation. The proposed NRFF improves
state-of-the-art rendering results by over 1 dB in PSNR on both the NeRF and
NSVF synthetic datasets. A significant improvement has also been observed on
the real-world Tanks & Temples dataset. Code can be found at
https://github.com/imkanghan/nrff.";Kang Han<author:sep>Wei Xiang;http://arxiv.org/pdf/2303.03808v2;cs.CV;;nerf
2303.03056v3;http://arxiv.org/abs/2303.03056v3;2023-03-06;MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal  calibration;"With the recent advances in autonomous driving and the decreasing cost of
LiDARs, the use of multimodal sensor systems is on the rise. However, in order
to make use of the information provided by a variety of complimentary sensors,
it is necessary to accurately calibrate them. We take advantage of recent
advances in computer graphics and implicit volumetric scene representation to
tackle the problem of multi-sensor spatial and temporal calibration. Thanks to
a new formulation of the Neural Radiance Field (NeRF) optimization, we are able
to jointly optimize calibration parameters along with scene representation
based on radiometric and geometric measurements. Our method enables accurate
and robust calibration from data captured in uncontrolled and unstructured
urban environments, making our solution more scalable than existing calibration
solutions. We demonstrate the accuracy and robustness of our method in urban
scenes typically encountered in autonomous driving scenarios.";Quentin Herau<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Luis Roldão<author:sep>Dzmitry Tsishkou<author:sep>Cyrille Migniot<author:sep>Pascal Vasseur<author:sep>Cédric Demonceaux;http://arxiv.org/pdf/2303.03056v3;cs.CV;Accepted at IROS2023 Project site: https://qherau.github.io/MOISST/;nerf
2303.03003v2;http://arxiv.org/abs/2303.03003v2;2023-03-06;Efficient Large-scale Scene Representation with a Hybrid of  High-resolution Grid and Plane Features;"Existing neural radiance fields (NeRF) methods for large-scale scene modeling
require days of training using multiple GPUs, hindering their applications in
scenarios with limited computing resources. Despite fast optimization NeRF
variants have been proposed based on the explicit dense or hash grid features,
their effectivenesses are mainly demonstrated in object-scale scene
representation. In this paper, we point out that the low feature resolution in
explicit representation is the bottleneck for large-scale unbounded scene
representation. To address this problem, we introduce a new and efficient
hybrid feature representation for NeRF that fuses the 3D hash-grids and
high-resolution 2D dense plane features. Compared with the dense-grid
representation, the resolution of a dense 2D plane can be scaled up more
efficiently. Based on this hybrid representation, we propose a fast
optimization NeRF variant, called GP-NeRF, that achieves better rendering
results while maintaining a compact model size. Extensive experiments on
multiple large-scale unbounded scene datasets show that our model can converge
in 1.5 hours using a single GPU while achieving results comparable to or even
better than the existing method that requires about one day's training with 8
GPUs.";Yuqi Zhang<author:sep>Guanying Chen<author:sep>Shuguang Cui;http://arxiv.org/pdf/2303.03003v2;cs.CV;;nerf
2303.03361v2;http://arxiv.org/abs/2303.03361v2;2023-03-06;Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene  Representation from 2D Supervision;"We address efficient and structure-aware 3D scene representation from images.
Nerflets are our key contribution -- a set of local neural radiance fields that
together represent a scene. Each nerflet maintains its own spatial position,
orientation, and extent, within which it contributes to panoptic, density, and
radiance reconstructions. By leveraging only photometric and inferred panoptic
image supervision, we can directly and jointly optimize the parameters of a set
of nerflets so as to form a decomposed representation of the scene, where each
object instance is represented by a group of nerflets. During experiments with
indoor and outdoor environments, we find that nerflets: (1) fit and approximate
the scene more efficiently than traditional global NeRFs, (2) allow the
extraction of panoptic and photometric renderings from arbitrary views, and (3)
enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive
editing.";Xiaoshuai Zhang<author:sep>Abhijit Kundu<author:sep>Thomas Funkhouser<author:sep>Leonidas Guibas<author:sep>Hao Su<author:sep>Kyle Genova;http://arxiv.org/pdf/2303.03361v2;cs.CV;accepted by CVPR 2023;nerf
2303.03966v1;http://arxiv.org/abs/2303.03966v1;2023-03-05;Semantic-aware Occlusion Filtering Neural Radiance Fields in the Wild;"We present a learning framework for reconstructing neural scene
representations from a small number of unconstrained tourist photos. Since each
image contains transient occluders, decomposing the static and transient
components is necessary to construct radiance fields with such in-the-wild
photographs where existing methods require a lot of training data. We introduce
SF-NeRF, aiming to disentangle those two components with only a few images
given, which exploits semantic information without any supervision. The
proposed method contains an occlusion filtering module that predicts the
transient color and its opacity for each pixel, which enables the NeRF model to
solely learn the static scene representation. This filtering module learns the
transient phenomena guided by pixel-wise semantic features obtained by a
trainable image encoder that can be trained across multiple scenes to learn the
prior of transient objects. Furthermore, we present two techniques to prevent
ambiguous decomposition and noisy results of the filtering module. We
demonstrate that our method outperforms state-of-the-art novel view synthesis
methods on Phototourism dataset in a few-shot setting.";Jaewon Lee<author:sep>Injae Kim<author:sep>Hwan Heo<author:sep>Hyunwoo J. Kim;http://arxiv.org/pdf/2303.03966v1;cs.CV;11 pages, 5 figures;nerf
2303.01736v1;http://arxiv.org/abs/2303.01736v1;2023-03-03;Multi-Plane Neural Radiance Fields for Novel View Synthesis;"Novel view synthesis is a long-standing problem that revolves around
rendering frames of scenes from novel camera viewpoints. Volumetric approaches
provide a solution for modeling occlusions through the explicit 3D
representation of the camera frustum. Multi-plane Images (MPI) are volumetric
methods that represent the scene using front-parallel planes at distinct depths
but suffer from depth discretization leading to a 2.D scene representation.
Another line of approach relies on implicit 3D scene representations. Neural
Radiance Fields (NeRF) utilize neural networks for encapsulating the continuous
3D scene structure within the network weights achieving photorealistic
synthesis results, however, methods are constrained to per-scene optimization
settings which are inefficient in practice. Multi-plane Neural Radiance Fields
(MINE) open the door for combining implicit and explicit scene representations.
It enables continuous 3D scene representations, especially in the depth
dimension, while utilizing the input image features to avoid per-scene
optimization. The main drawback of the current literature work in this domain
is being constrained to single-view input, limiting the synthesis ability to
narrow viewpoint ranges. In this work, we thoroughly examine the performance,
generalization, and efficiency of single-view multi-plane neural radiance
fields. In addition, we propose a new multiplane NeRF architecture that accepts
multiple views to improve the synthesis results and expand the viewing range.
Features from the input source frames are effectively fused through a proposed
attention-aware fusion module to highlight important information from different
viewpoints. Experiments show the effectiveness of attention-based fusion and
the promising outcomes of our proposed method when compared to multi-view NeRF
and MPI techniques.";Youssef Abdelkareem<author:sep>Shady Shehata<author:sep>Fakhri Karray;http://arxiv.org/pdf/2303.01736v1;cs.CV;ICDIPV 2023;nerf
2303.02091v2;http://arxiv.org/abs/2303.02091v2;2023-03-03;Delicate Textured Mesh Recovery from NeRF via Adaptive Surface  Refinement;"Neural Radiance Fields (NeRF) have constituted a remarkable breakthrough in
image-based 3D reconstruction. However, their implicit volumetric
representations differ significantly from the widely-adopted polygonal meshes
and lack support from common 3D software and hardware, making their rendering
and manipulation inefficient. To overcome this limitation, we present a novel
framework that generates textured surface meshes from images. Our approach
begins by efficiently initializing the geometry and view-dependency decomposed
appearance with a NeRF. Subsequently, a coarse mesh is extracted, and an
iterative surface refining algorithm is developed to adaptively adjust both
vertex positions and face density based on re-projected rendering errors. We
jointly refine the appearance with geometry and bake it into texture images for
real-time rendering. Extensive experiments demonstrate that our method achieves
superior mesh quality and competitive rendering quality.";Jiaxiang Tang<author:sep>Hang Zhou<author:sep>Xiaokang Chen<author:sep>Tianshu Hu<author:sep>Errui Ding<author:sep>Jingdong Wang<author:sep>Gang Zeng;http://arxiv.org/pdf/2303.02091v2;cs.CV;ICCV 2023 camera-ready, Project Page: https://me.kiui.moe/nerf2mesh;nerf
2303.00749v1;http://arxiv.org/abs/2303.00749v1;2023-03-01;S-NeRF: Neural Radiance Fields for Street Views;"Neural Radiance Fields (NeRFs) aim to synthesize novel views of objects and
scenes, given the object-centric camera views with large overlaps. However, we
conjugate that this paradigm does not fit the nature of the street views that
are collected by many self-driving cars from the large-scale unbounded scenes.
Also, the onboard cameras perceive scenes without much overlapping. Thus,
existing NeRFs often produce blurs, 'floaters' and other artifacts on
street-view synthesis. In this paper, we propose a new street-view NeRF
(S-NeRF) that considers novel view synthesis of both the large-scale background
scenes and the foreground moving vehicles jointly. Specifically, we improve the
scene parameterization function and the camera poses for learning better neural
representations from street views. We also use the the noisy and sparse LiDAR
points to boost the training and learn a robust geometry and reprojection based
confidence to address the depth outliers. Moreover, we extend our S-NeRF for
reconstructing moving vehicles that is impracticable for conventional NeRFs.
Thorough experiments on the large-scale driving datasets (e.g., nuScenes and
Waymo) demonstrate that our method beats the state-of-the-art rivals by
reducing 7% to 40% of the mean-squared error in the street-view synthesis and a
45% PSNR gain for the moving vehicles rendering.";Ziyang Xie<author:sep>Junge Zhang<author:sep>Wenye Li<author:sep>Feihu Zhang<author:sep>Li Zhang;http://arxiv.org/pdf/2303.00749v1;cs.CV;ICLR 2023;nerf
2303.00304v4;http://arxiv.org/abs/2303.00304v4;2023-03-01;Renderable Neural Radiance Map for Visual Navigation;"We propose a novel type of map for visual navigation, a renderable neural
radiance map (RNR-Map), which is designed to contain the overall visual
information of a 3D environment. The RNR-Map has a grid form and consists of
latent codes at each pixel. These latent codes are embedded from image
observations, and can be converted to the neural radiance field which enables
image rendering given a camera pose. The recorded latent codes implicitly
contain visual information about the environment, which makes the RNR-Map
visually descriptive. This visual information in RNR-Map can be a useful
guideline for visual localization and navigation. We develop localization and
navigation frameworks that can effectively utilize the RNR-Map. We evaluate the
proposed frameworks on camera tracking, visual localization, and image-goal
navigation. Experimental results show that the RNR-Map-based localization
framework can find the target location based on a single query image with fast
speed and competitive accuracy compared to other baselines. Also, this
localization framework is robust to environmental changes, and even finds the
most visually similar places when a query image from a different environment is
given. The proposed navigation framework outperforms the existing image-goal
navigation methods in difficult scenarios, under odometry and actuation noises.
The navigation framework shows 65.7% success rate in curved scenarios of the
NRNS dataset, which is an improvement of 18.6% over the current
state-of-the-art. Project page: https://rllab-snu.github.io/projects/RNR-Map/";Obin Kwon<author:sep>Jeongho Park<author:sep>Songhwai Oh;http://arxiv.org/pdf/2303.00304v4;cs.CV;"Preprint version. CVPR 2023 accepted, highlight paper. Project page:
  https://rllab-snu.github.io/projects/RNR-Map/";
2303.00050v1;http://arxiv.org/abs/2303.00050v1;2023-02-28;Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface;"Reconstructing general dynamic scenes is important for many computer vision
and graphics applications. Recent works represent the dynamic scene with neural
radiance fields for photorealistic view synthesis, while their surface geometry
is under-constrained and noisy. Other works introduce surface constraints to
the implicit neural representation to disentangle the ambiguity of geometry and
appearance field for static scene reconstruction. To bridge the gap between
rendering dynamic scenes and recovering static surface geometry, we propose a
template-free method to reconstruct surface geometry and appearance using
neural implicit representations from multi-view videos. We leverage
topology-aware deformation and the signed distance field to learn complex
dynamic surfaces via differentiable volume rendering without scene-specific
prior knowledge like template models. Furthermore, we propose a novel
mask-based ray selection strategy to significantly boost the optimization on
challenging time-varying regions. Experiments on different multi-view video
datasets demonstrate that our method achieves high-fidelity surface
reconstruction as well as photorealistic novel view synthesis.";Decai Chen<author:sep>Haofei Lu<author:sep>Ingo Feldmann<author:sep>Oliver Schreer<author:sep>Peter Eisert;http://arxiv.org/pdf/2303.00050v1;cs.CV;5 pages, accepted by ICASSP 2023;
2302.14683v2;http://arxiv.org/abs/2302.14683v2;2023-02-28;IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF;"Recently, many works have been proposed to utilize the neural radiance field
for novel view synthesis of human performers. However, most of these methods
require hours of training, making them difficult for practical use. To address
this challenging problem, we propose IntrinsicNGP, which can train from scratch
and achieve high-fidelity results in few minutes with videos of a human
performer. To achieve this target, we introduce a continuous and optimizable
intrinsic coordinate rather than the original explicit Euclidean coordinate in
the hash encoding module of instant-NGP. With this novel intrinsic coordinate,
IntrinsicNGP can aggregate inter-frame information for dynamic objects with the
help of proxy geometry shapes. Moreover, the results trained with the given
rough geometry shapes can be further refined with an optimizable offset field
based on the intrinsic coordinate.Extensive experimental results on several
datasets demonstrate the effectiveness and efficiency of IntrinsicNGP. We also
illustrate our approach's ability to edit the shape of reconstructed subjects.";Bo Peng<author:sep>Jun Hu<author:sep>Jingtao Zhou<author:sep>Xuan Gao<author:sep>Juyong Zhang;http://arxiv.org/pdf/2302.14683v2;cs.CV;"Project page:https://ustc3dv.github.io/IntrinsicNGP/. arXiv admin
  note: substantial text overlap with arXiv:2210.01651";nerf
2302.13543v3;http://arxiv.org/abs/2302.13543v3;2023-02-27;BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling;"Reasoning the 3D structure of a non-rigid dynamic scene from a single moving
camera is an under-constrained problem. Inspired by the remarkable progress of
neural radiance fields (NeRFs) in photo-realistic novel view synthesis of
static scenes, extensions have been proposed for dynamic settings. These
methods heavily rely on neural priors in order to regularize the problem. In
this work, we take a step back and reinvestigate how current implementations
may entail deleterious effects, including limited expressiveness, entanglement
of light and density fields, and sub-optimal motion localization. As a remedy,
we advocate for a bridge between classic non-rigid-structure-from-motion
(\nrsfm) and NeRF, enabling the well-studied priors of the former to constrain
the latter. To this end, we propose a framework that factorizes time and space
by formulating a scene as a composition of bandlimited, high-dimensional
signals. We demonstrate compelling results across complex dynamic scenes that
involve changes in lighting, texture and long-range dynamics.";Sameera Ramasinghe<author:sep>Violetta Shevchenko<author:sep>Gil Avraham<author:sep>Anton Van Den Hengel;http://arxiv.org/pdf/2302.13543v3;cs.CV;;nerf
2302.13397v1;http://arxiv.org/abs/2302.13397v1;2023-02-26;Efficient physics-informed neural networks using hash encoding;"Physics-informed neural networks (PINNs) have attracted a lot of attention in
scientific computing as their functional representation of partial differential
equation (PDE) solutions offers flexibility and accuracy features. However,
their training cost has limited their practical use as a real alternative to
classic numerical methods. Thus, we propose to incorporate multi-resolution
hash encoding into PINNs to improve the training efficiency, as such encoding
offers a locally-aware (at multi resolution) coordinate inputs to the neural
network. Borrowed from the neural representation field community (NeRF), we
investigate the robustness of calculating the derivatives of such hash encoded
neural networks with respect to the input coordinates, which is often needed by
the PINN loss terms. We propose to replace the automatic differentiation with
finite-difference calculations of the derivatives to address the discontinuous
nature of such derivatives. We also share the appropriate ranges for the hash
encoding hyperparameters to obtain robust derivatives. We test the proposed
method on three problems, including Burgers equation, Helmholtz equation, and
Navier-Stokes equation. The proposed method admits about a 10-fold improvement
in efficiency over the vanilla PINN implementation.";Xinquan Huang<author:sep>Tariq Alkhalifah;http://arxiv.org/pdf/2302.13397v1;cs.LG;;nerf
2302.12931v2;http://arxiv.org/abs/2302.12931v2;2023-02-24;CATNIPS: Collision Avoidance Through Neural Implicit Probabilistic  Scenes;"We introduce a transformation of a Neural Radiance Field (NeRF) to an
equivalent Poisson Point Process (PPP). This PPP transformation allows for
rigorous quantification of uncertainty in NeRFs, in particular, for computing
collision probabilities for a robot navigating through a NeRF environment. The
PPP is a generalization of a probabilistic occupancy grid to the continuous
volume and is fundamental to the volumetric ray-tracing model underlying
radiance fields. Building upon this PPP representation, we present a
chance-constrained trajectory optimization method for safe robot navigation in
NeRFs. Our method relies on a voxel representation called the Probabilistic
Unsafe Robot Region (PURR) that spatially fuses the chance constraint with the
NeRF model to facilitate fast trajectory optimization. We then combine a
graph-based search with a spline-based trajectory optimization to yield robot
trajectories through the NeRF that are guaranteed to satisfy a user-specific
collision probability. We validate our chance constrained planning method
through simulations and hardware experiments, showing superior performance
compared to prior works on trajectory planning in NeRF environments.";Timothy Chen<author:sep>Preston Culbertson<author:sep>Mac Schwager;http://arxiv.org/pdf/2302.12931v2;cs.RO;Under Review in IEEE Transactions on Robotics;nerf
2302.12249v1;http://arxiv.org/abs/2302.12249v1;2023-02-23;MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in  Unbounded Scenes;"Neural radiance fields enable state-of-the-art photorealistic view synthesis.
However, existing radiance field representations are either too
compute-intensive for real-time rendering or require too much memory to scale
to large scenes. We present a Memory-Efficient Radiance Field (MERF)
representation that achieves real-time rendering of large-scale scenes in a
browser. MERF reduces the memory consumption of prior sparse volumetric
radiance fields using a combination of a sparse feature grid and
high-resolution 2D feature planes. To support large-scale unbounded scenes, we
introduce a novel contraction function that maps scene coordinates into a
bounded volume while still allowing for efficient ray-box intersection. We
design a lossless procedure for baking the parameterization used during
training into a model that achieves real-time rendering while still preserving
the photorealistic view synthesis quality of a volumetric radiance field.";Christian Reiser<author:sep>Richard Szeliski<author:sep>Dor Verbin<author:sep>Pratul P. Srinivasan<author:sep>Ben Mildenhall<author:sep>Andreas Geiger<author:sep>Jonathan T. Barron<author:sep>Peter Hedman;http://arxiv.org/pdf/2302.12249v1;cs.CV;Video and interactive web demo available at https://merf42.github.io;
2302.12231v3;http://arxiv.org/abs/2302.12231v3;2023-02-23;DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising  Diffusion Models;"Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive
results on novel view synthesis tasks. NeRFs learn a scene's color and density
fields by minimizing the photometric discrepancy between training views and
differentiable renderings of the scene. Once trained from a sufficient set of
views, NeRFs can generate novel views from arbitrary camera positions. However,
the scene geometry and color fields are severely under-constrained, which can
lead to artifacts, especially when trained with few input views.
  To alleviate this problem we learn a prior over scene geometry and color,
using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of
the synthetic Hypersim dataset and can be used to predict the gradient of the
logarithm of a joint probability distribution of color and depth patches. We
show that, these gradients of logarithms of RGBD patch priors serve to
regularize geometry and color of a scene. During NeRF training, random RGBD
patches are rendered and the estimated gradient of the log-likelihood is
backpropagated to the color and density fields. Evaluations on LLFF, the most
relevant dataset, show that our learned prior achieves improved quality in the
reconstructed geometry and improved generalization to novel views. Evaluations
on DTU show improved reconstruction quality among NeRF methods.";Jamie Wynn<author:sep>Daniyar Turmukhambetov;http://arxiv.org/pdf/2302.12231v3;cs.CV;CVPR 2023. Updated LPIPS scores in Table 1;nerf
2302.12237v2;http://arxiv.org/abs/2302.12237v2;2023-02-23;Learning Neural Volumetric Representations of Dynamic Humans in Minutes;"This paper addresses the challenge of quickly reconstructing free-viewpoint
videos of dynamic humans from sparse multi-view videos. Some recent works
represent the dynamic human as a canonical neural radiance field (NeRF) and a
motion field, which are learned from videos through differentiable rendering.
But the per-scene optimization generally requires hours. Other generalizable
NeRF models leverage learned prior from datasets and reduce the optimization
time by only finetuning on new scenes at the cost of visual fidelity. In this
paper, we propose a novel method for learning neural volumetric videos of
dynamic humans from sparse view videos in minutes with competitive visual
quality. Specifically, we define a novel part-based voxelized human
representation to better distribute the representational power of the network
to different human parts. Furthermore, we propose a novel 2D motion
parameterization scheme to increase the convergence rate of deformation field
learning. Experiments demonstrate that our model can be learned 100 times
faster than prior per-scene optimization methods while being competitive in the
rendering quality. Training our model on a $512 \times 512$ video with 100
frames typically takes about 5 minutes on a single RTX 3090 GPU. The code will
be released on our project page: https://zju3dv.github.io/instant_nvr";Chen Geng<author:sep>Sida Peng<author:sep>Zhen Xu<author:sep>Hujun Bao<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2302.12237v2;cs.CV;Project page: https://zju3dv.github.io/instant_nvr;nerf
2302.10663v2;http://arxiv.org/abs/2302.10663v2;2023-02-21;RealFusion: 360° Reconstruction of Any Object from a Single Image;"We consider the problem of reconstructing a full 360{\deg} photographic model
of an object from a single image of it. We do so by fitting a neural radiance
field to the image, but find this problem to be severely ill-posed. We thus
take an off-the-self conditional image generator based on diffusion and
engineer a prompt that encourages it to ""dream up"" novel views of the object.
Using an approach inspired by DreamFields and DreamFusion, we fuse the given
input view, the conditional prior, and other regularizers in a final,
consistent reconstruction. We demonstrate state-of-the-art reconstruction
results on benchmark images when compared to prior methods for monocular 3D
reconstruction of objects. Qualitatively, our reconstructions provide a
faithful match of the input view and a plausible extrapolation of its
appearance and 3D shape, including to the side of the object not visible in the
image.";Luke Melas-Kyriazi<author:sep>Christian Rupprecht<author:sep>Iro Laina<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2302.10663v2;cs.CV;Project page: https://lukemelas.github.io/realfusion;
2302.10518v3;http://arxiv.org/abs/2302.10518v3;2023-02-21;USR: Unsupervised Separated 3D Garment and Human Reconstruction via  Geometry and Semantic Consistency;"Dressed people reconstruction from images is a popular task with promising
applications in the creative media and game industry. However, most existing
methods reconstruct the human body and garments as a whole with the supervision
of 3D models, which hinders the downstream interaction tasks and requires
hard-to-obtain data. To address these issues, we propose an unsupervised
separated 3D garments and human reconstruction model (USR), which reconstructs
the human body and authentic textured clothes in layers without 3D models. More
specifically, our method proposes a generalized surface-aware neural radiance
field to learn the mapping between sparse multi-view images and geometries of
the dressed people. Based on the full geometry, we introduce a Semantic and
Confidence Guided Separation strategy (SCGS) to detect, segment, and
reconstruct the clothes layer, leveraging the consistency between 2D semantic
and 3D geometry. Moreover, we propose a Geometry Fine-tune Module to smooth
edges. Extensive experiments on our dataset show that comparing with
state-of-the-art methods, USR achieves improvements on both geometry and
appearance reconstruction while supporting generalizing to unseen people in
real time. Besides, we also introduce SMPL-D model to show the benefit of the
separated modeling of clothes and the human body that allows swapping clothes
and virtual try-on.";Yue Shi<author:sep>Yuxuan Xiong<author:sep>Jingyi Chai<author:sep>Bingbing Ni<author:sep>Wenjun Zhang;http://arxiv.org/pdf/2302.10518v3;cs.CV;;
2302.10970v2;http://arxiv.org/abs/2302.10970v2;2023-02-21;Differentiable Rendering with Reparameterized Volume Sampling;"In view synthesis, a neural radiance field approximates underlying density
and radiance fields based on a sparse set of scene pictures. To generate a
pixel of a novel view, it marches a ray through the pixel and computes a
weighted sum of radiance emitted from a dense set of ray points. This rendering
algorithm is fully differentiable and facilitates gradient-based optimization
of the fields. However, in practice, only a tiny opaque portion of the ray
contributes most of the radiance to the sum. We propose a simple end-to-end
differentiable sampling algorithm based on inverse transform sampling. It
generates samples according to the probability distribution induced by the
density field and picks non-transparent points on the ray. We utilize the
algorithm in two ways. First, we propose a novel rendering approach based on
Monte Carlo estimates. This approach allows for evaluating and optimizing a
neural radiance field with just a few radiance field calls per ray. Second, we
use the sampling algorithm to modify the hierarchical scheme proposed in the
original NeRF work. We show that our modification improves reconstruction
quality of hierarchical models, at the same time simplifying the training
procedure by removing the need for auxiliary proposal network losses.";Nikita Morozov<author:sep>Denis Rakitin<author:sep>Oleg Desheulin<author:sep>Dmitry Vetrov<author:sep>Kirill Struminsky;http://arxiv.org/pdf/2302.10970v2;cs.CV;Preprint;nerf
2302.10109v1;http://arxiv.org/abs/2302.10109v1;2023-02-20;NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from  3D-aware Diffusion;"Novel view synthesis from a single image requires inferring occluded regions
of objects and scenes whilst simultaneously maintaining semantic and physical
consistency with the input. Existing approaches condition neural radiance
fields (NeRF) on local image features, projecting points to the input image
plane, and aggregating 2D features to perform volume rendering. However, under
severe occlusion, this projection fails to resolve uncertainty, resulting in
blurry renderings that lack details. In this work, we propose NerfDiff, which
addresses this issue by distilling the knowledge of a 3D-aware conditional
diffusion model (CDM) into NeRF through synthesizing and refining a set of
virtual views at test time. We further propose a novel NeRF-guided distillation
algorithm that simultaneously generates 3D consistent virtual views from the
CDM samples, and finetunes the NeRF based on the improved virtual views. Our
approach significantly outperforms existing NeRF-based and geometry-free
approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.";Jiatao Gu<author:sep>Alex Trevithick<author:sep>Kai-En Lin<author:sep>Josh Susskind<author:sep>Christian Theobalt<author:sep>Lingjie Liu<author:sep>Ravi Ramamoorthi;http://arxiv.org/pdf/2302.10109v1;cs.CV;Project page: https://jiataogu.me/nerfdiff/;nerf
2302.09486v1;http://arxiv.org/abs/2302.09486v1;2023-02-19;LC-NeRF: Local Controllable Face Generation in Neural Randiance Field;"3D face generation has achieved high visual quality and 3D consistency thanks
to the development of neural radiance fields (NeRF). Recently, to generate and
edit 3D faces with NeRF representation, some methods are proposed and achieve
good results in decoupling geometry and texture. The latent codes of these
generative models affect the whole face, and hence modifications to these codes
cause the entire face to change. However, users usually edit a local region
when editing faces and do not want other regions to be affected. Since changes
to the latent code affect global generation results, these methods do not allow
for fine-grained control of local facial regions. To improve local
controllability in NeRF-based face editing, we propose LC-NeRF, which is
composed of a Local Region Generators Module and a Spatial-Aware Fusion Module,
allowing for local geometry and texture control of local facial regions.
Qualitative and quantitative evaluations show that our method provides better
local editing than state-of-the-art face editing methods. Our method also
performs well in downstream tasks, such as text-driven facial image editing.";Wenyang Zhou<author:sep>Lu Yuan<author:sep>Shuyu Chen<author:sep>Lin Gao<author:sep>Shimin Hu;http://arxiv.org/pdf/2302.09486v1;cs.CV;;nerf
2302.09311v2;http://arxiv.org/abs/2302.09311v2;2023-02-18;Temporal Interpolation Is All You Need for Dynamic Neural Radiance  Fields;"Temporal interpolation often plays a crucial role to learn meaningful
representations in dynamic scenes. In this paper, we propose a novel method to
train spatiotemporal neural radiance fields of dynamic scenes based on temporal
interpolation of feature vectors. Two feature interpolation methods are
suggested depending on underlying representations, neural networks or grids. In
the neural representation, we extract features from space-time inputs via
multiple neural network modules and interpolate them based on time frames. The
proposed multi-level feature interpolation network effectively captures
features of both short-term and long-term time ranges. In the grid
representation, space-time features are learned via four-dimensional hash
grids, which remarkably reduces training time. The grid representation shows
more than 100 times faster training speed than the previous neural-net-based
methods while maintaining the rendering quality. Concatenating static and
dynamic features and adding a simple smoothness term further improve the
performance of our proposed models. Despite the simplicity of the model
architectures, our method achieved state-of-the-art performance both in
rendering quality for the neural representation and in training speed for the
grid representation.";Sungheon Park<author:sep>Minjung Son<author:sep>Seokhwan Jang<author:sep>Young Chun Ahn<author:sep>Ji-Yeon Kim<author:sep>Nahyup Kang;http://arxiv.org/pdf/2302.09311v2;cs.CV;"CVPR 2023. Project page:
  https://sungheonpark.github.io/tempinterpnerf";
2302.08788v2;http://arxiv.org/abs/2302.08788v2;2023-02-17;MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis  from Sparse Inputs;"Neural Radiance Field (NeRF) has broken new ground in the novel view
synthesis due to its simple concept and state-of-the-art quality. However, it
suffers from severe performance degradation unless trained with a dense set of
images with different camera poses, which hinders its practical applications.
Although previous methods addressing this problem achieved promising results,
they relied heavily on the additional training resources, which goes against
the philosophy of sparse-input novel-view synthesis pursuing the training
efficiency. In this work, we propose MixNeRF, an effective training strategy
for novel view synthesis from sparse inputs by modeling a ray with a mixture
density model. Our MixNeRF estimates the joint distribution of RGB colors along
the ray samples by modeling it with mixture of distributions. We also propose a
new task of ray depth estimation as a useful training objective, which is
highly correlated with 3D scene geometry. Moreover, we remodel the colors with
regenerated blending weights based on the estimated ray depth and further
improves the robustness for colors and viewpoints. Our MixNeRF outperforms
other state-of-the-art methods in various standard benchmarks with superior
efficiency of training and inference.";Seunghyeon Seo<author:sep>Donghoon Han<author:sep>Yeonjin Chang<author:sep>Nojun Kwak;http://arxiv.org/pdf/2302.08788v2;cs.CV;CVPR 2023. Project Page: https://shawn615.github.io/mixnerf/;nerf
2302.08509v2;http://arxiv.org/abs/2302.08509v2;2023-02-16;3D-aware Conditional Image Synthesis;"We propose pix2pix3D, a 3D-aware conditional generative model for
controllable photorealistic image synthesis. Given a 2D label map, such as a
segmentation or edge map, our model learns to synthesize a corresponding image
from different viewpoints. To enable explicit 3D user control, we extend
conditional generative models with neural radiance fields. Given
widely-available monocular images and label map pairs, our model learns to
assign a label to every 3D point in addition to color and density, which
enables it to render the image and pixel-aligned label map simultaneously.
Finally, we build an interactive system that allows users to edit the label map
from any viewpoint and generate outputs accordingly.";Kangle Deng<author:sep>Gengshan Yang<author:sep>Deva Ramanan<author:sep>Jun-Yan Zhu;http://arxiv.org/pdf/2302.08509v2;cs.CV;Project Page: https://www.cs.cmu.edu/~pix2pix3D/;
2302.07672v3;http://arxiv.org/abs/2302.07672v3;2023-02-15;LiveHand: Real-time and Photorealistic Neural Hand Rendering;"The human hand is the main medium through which we interact with our
surroundings, making its digitization an important problem. While there are
several works modeling the geometry of hands, little attention has been paid to
capturing photo-realistic appearance. Moreover, for applications in extended
reality and gaming, real-time rendering is critical. We present the first
neural-implicit approach to photo-realistically render hands in real-time. This
is a challenging problem as hands are textured and undergo strong articulations
with pose-dependent effects. However, we show that this aim is achievable
through our carefully designed method. This includes training on a
low-resolution rendering of a neural radiance field, together with a
3D-consistent super-resolution module and mesh-guided sampling and space
canonicalization. We demonstrate a novel application of perceptual loss on the
image space, which is critical for learning details accurately. We also show a
live demo where we photo-realistically render the human hand in real-time for
the first time, while also modeling pose- and view-dependent appearance
effects. We ablate all our design choices and show that they optimize for
rendering speed and quality. Video results and our code can be accessed from
https://vcai.mpi-inf.mpg.de/projects/LiveHand/";Akshay Mundra<author:sep>Mallikarjun B R<author:sep>Jiayi Wang<author:sep>Marc Habermann<author:sep>Christian Theobalt<author:sep>Mohamed Elgharib;http://arxiv.org/pdf/2302.07672v3;cs.GR;"Project page: https://vcai.mpi-inf.mpg.de/projects/LiveHand/ |
  Accepted at ICCV '23 | 11 pages, 7 figures";
2302.06833v1;http://arxiv.org/abs/2302.06833v1;2023-02-14;VQ3D: Learning a 3D-Aware Generative Model on ImageNet;"Recent work has shown the possibility of training generative models of 3D
content from 2D image collections on small datasets corresponding to a single
object class, such as human faces, animal faces, or cars. However, these models
struggle on larger, more complex datasets. To model diverse and unconstrained
image collections such as ImageNet, we present VQ3D, which introduces a
NeRF-based decoder into a two-stage vector-quantized autoencoder. Our Stage 1
allows for the reconstruction of an input image and the ability to change the
camera position around the image, and our Stage 2 allows for the generation of
new 3D scenes. VQ3D is capable of generating and reconstructing 3D-aware images
from the 1000-class ImageNet dataset of 1.2 million training images. We achieve
an ImageNet generation FID score of 16.8, compared to 69.8 for the next best
baseline method.";Kyle Sargent<author:sep>Jing Yu Koh<author:sep>Han Zhang<author:sep>Huiwen Chang<author:sep>Charles Herrmann<author:sep>Pratul Srinivasan<author:sep>Jiajun Wu<author:sep>Deqing Sun;http://arxiv.org/pdf/2302.06833v1;cs.CV;"15 pages. For visual results, please visit the project webpage at
  http://kylesargent.github.io/vq3d";nerf
2302.06608v3;http://arxiv.org/abs/2302.06608v3;2023-02-13;3D-aware Blending with Generative NeRFs;"Image blending aims to combine multiple images seamlessly. It remains
challenging for existing 2D-based methods, especially when input images are
misaligned due to differences in 3D camera poses and object shapes. To tackle
these issues, we propose a 3D-aware blending method using generative Neural
Radiance Fields (NeRF), including two key components: 3D-aware alignment and
3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of
the reference image with respect to generative NeRFs and then perform 3D local
alignment for each part. To further leverage 3D information of the generative
NeRF, we propose 3D-aware blending that directly blends images on the NeRF's
latent representation space, rather than raw pixel space. Collectively, our
method outperforms existing 2D baselines, as validated by extensive
quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.";Hyunsu Kim<author:sep>Gayoung Lee<author:sep>Yunjey Choi<author:sep>Jin-Hwa Kim<author:sep>Jun-Yan Zhu;http://arxiv.org/pdf/2302.06608v3;cs.CV;ICCV 2023, Project page: https://blandocs.github.io/blendnerf;nerf
2302.05573v1;http://arxiv.org/abs/2302.05573v1;2023-02-11;3D Colored Shape Reconstruction from a Single RGB Image through  Diffusion;"We propose a novel 3d colored shape reconstruction method from a single RGB
image through diffusion model. Diffusion models have shown great development
potentials for high-quality 3D shape generation. However, most existing work
based on diffusion models only focus on geometric shape generation, they cannot
either accomplish 3D reconstruction from a single image, or produce 3D
geometric shape with color information. In this work, we propose to reconstruct
a 3D colored shape from a single RGB image through a novel conditional
diffusion model. The reverse process of the proposed diffusion model is
consisted of three modules, shape prediction module, color prediction module
and NeRF-like rendering module. In shape prediction module, the reference RGB
image is first encoded into a high-level shape feature and then the shape
feature is utilized as a condition to predict the reverse geometric noise in
diffusion model. Then the color of each 3D point updated in shape prediction
module is predicted by color prediction module. Finally, a NeRF-like rendering
module is designed to render the colored point cloud predicted by the former
two modules to 2D image space to guide the training conditioned only on a
reference image. As far as the authors know, the proposed method is the first
diffusion model for 3D colored shape reconstruction from a single RGB image.
Experimental results demonstrate that the proposed method achieves competitive
performance on colored 3D shape reconstruction, and the ablation study
validates the positive role of the color prediction module in improving the
reconstruction quality of 3D geometric point cloud.";Bo Li<author:sep>Xiaolin Wei<author:sep>Fengwei Chen<author:sep>Bin Liu;http://arxiv.org/pdf/2302.05573v1;cs.CV;9 pages, 8 figures;nerf
2302.04871v3;http://arxiv.org/abs/2302.04871v3;2023-02-09;In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for  Face Editing;"3D-aware GANs offer new capabilities for view synthesis while preserving the
editing functionalities of their 2D counterparts. GAN inversion is a crucial
step that seeks the latent code to reconstruct input images or videos,
subsequently enabling diverse editing tasks through manipulation of this latent
code. However, a model pre-trained on a particular dataset (e.g., FFHQ) often
has difficulty reconstructing images with out-of-distribution (OOD) objects
such as faces with heavy make-up or occluding objects. We address this issue by
explicitly modeling OOD objects from the input in 3D-aware GANs. Our core idea
is to represent the image using two individual neural radiance fields: one for
the in-distribution content and the other for the out-of-distribution object.
The final reconstruction is achieved by optimizing the composition of these two
radiance fields with carefully designed regularization. We demonstrate that our
explicit decomposition alleviates the inherent trade-off between reconstruction
fidelity and editability. We evaluate reconstruction accuracy and editability
of our method on challenging real face images and videos and showcase favorable
results against other baselines.";Yiran Xu<author:sep>Zhixin Shu<author:sep>Cameron Smith<author:sep>Seoung Wug Oh<author:sep>Jia-Bin Huang;http://arxiv.org/pdf/2302.04871v3;cs.CV;Project page: https://in-n-out-3d.github.io/;
2302.04264v4;http://arxiv.org/abs/2302.04264v4;2023-02-08;Nerfstudio: A Modular Framework for Neural Radiance Field Development;"Neural Radiance Fields (NeRF) are a rapidly growing area of research with
wide-ranging applications in computer vision, graphics, robotics, and more. In
order to streamline the development and deployment of NeRF research, we propose
a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play
components for implementing NeRF-based methods, which make it easy for
researchers and practitioners to incorporate NeRF into their projects.
Additionally, the modular design enables support for extensive real-time
visualization tools, streamlined pipelines for importing captured in-the-wild
data, and tools for exporting to video, point cloud and mesh representations.
The modularity of Nerfstudio enables the development of Nerfacto, our method
that combines components from recent papers to achieve a balance between speed
and quality, while also remaining flexible to future modifications. To promote
community-driven development, all associated code and data are made publicly
available with open-source licensing at https://nerf.studio.";Matthew Tancik<author:sep>Ethan Weber<author:sep>Evonne Ng<author:sep>Ruilong Li<author:sep>Brent Yi<author:sep>Justin Kerr<author:sep>Terrance Wang<author:sep>Alexander Kristoffersen<author:sep>Jake Austin<author:sep>Kamyar Salahi<author:sep>Abhik Ahuja<author:sep>David McAllister<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2302.04264v4;cs.CV;Project page at https://nerf.studio;nerf
2302.02088v3;http://arxiv.org/abs/2302.02088v3;2023-02-04;AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene  Synthesis;"Can machines recording an audio-visual scene produce realistic, matching
audio-visual experiences at novel positions and novel view directions? We
answer it by studying a new task -- real-world audio-visual scene synthesis --
and a first-of-its-kind NeRF-based approach for multimodal learning.
Concretely, given a video recording of an audio-visual scene, the task is to
synthesize new videos with spatial audios along arbitrary novel camera
trajectories in that scene. We propose an acoustic-aware audio generation
module that integrates prior knowledge of audio propagation into NeRF, in which
we implicitly associate audio generation with the 3D geometry and material
properties of a visual environment. Furthermore, we present a coordinate
transformation module that expresses a view direction relative to the sound
source, enabling the model to learn sound source-centric acoustic fields. To
facilitate the study of this new task, we collect a high-quality Real-World
Audio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our method
on this real-world dataset and the simulation-based SoundSpaces dataset.";Susan Liang<author:sep>Chao Huang<author:sep>Yapeng Tian<author:sep>Anurag Kumar<author:sep>Chenliang Xu;http://arxiv.org/pdf/2302.02088v3;cs.CV;NeurIPS 2023;nerf
2302.01571v1;http://arxiv.org/abs/2302.01571v1;2023-02-03;Robust Camera Pose Refinement for Multi-Resolution Hash Encoding;"Multi-resolution hash encoding has recently been proposed to reduce the
computational cost of neural renderings, such as NeRF. This method requires
accurate camera poses for the neural renderings of given scenes. However,
contrary to previous methods jointly optimizing camera poses and 3D scenes, the
naive gradient-based camera pose refinement method using multi-resolution hash
encoding severely deteriorates performance. We propose a joint optimization
algorithm to calibrate the camera pose and learn a geometric representation
using efficient multi-resolution hash encoding. Showing that the oscillating
gradient flows of hash encoding interfere with the registration of camera
poses, our method addresses the issue by utilizing smooth interpolation
weighting to stabilize the gradient oscillation for the ray samplings across
hash grids. Moreover, the curriculum training procedure helps to learn the
level-wise hash encoding, further increasing the pose refinement. Experiments
on the novel-view synthesis datasets validate that our learning frameworks
achieve state-of-the-art performance and rapid convergence of neural rendering,
even when initial camera poses are unknown.";Hwan Heo<author:sep>Taekyung Kim<author:sep>Jiyoung Lee<author:sep>Jaewon Lee<author:sep>Soohyun Kim<author:sep>Hyunwoo J. Kim<author:sep>Jin-Hwa Kim;http://arxiv.org/pdf/2302.01571v1;cs.CV;;nerf
2302.01579v2;http://arxiv.org/abs/2302.01579v2;2023-02-03;Semantic 3D-aware Portrait Synthesis and Manipulation Based on  Compositional Neural Radiance Field;"Recently 3D-aware GAN methods with neural radiance field have developed
rapidly. However, current methods model the whole image as an overall neural
radiance field, which limits the partial semantic editability of synthetic
results. Since NeRF renders an image pixel by pixel, it is possible to split
NeRF in the spatial dimension. We propose a Compositional Neural Radiance Field
(CNeRF) for semantic 3D-aware portrait synthesis and manipulation. CNeRF
divides the image by semantic regions and learns an independent neural radiance
field for each region, and finally fuses them and renders the complete image.
Thus we can manipulate the synthesized semantic regions independently, while
fixing the other parts unchanged. Furthermore, CNeRF is also designed to
decouple shape and texture within each semantic region. Compared to
state-of-the-art 3D-aware GAN methods, our approach enables fine-grained
semantic region manipulation, while maintaining high-quality 3D-consistent
synthesis. The ablation studies show the effectiveness of the structure and
loss function used by our method. In addition real image inversion and cartoon
portrait 3D editing experiments demonstrate the application potential of our
method.";Tianxiang Ma<author:sep>Bingchuan Li<author:sep>Qian He<author:sep>Jing Dong<author:sep>Tieniu Tan;http://arxiv.org/pdf/2302.01579v2;cs.CV;Accepted by AAAI2023 Oral;nerf
2302.01532v1;http://arxiv.org/abs/2302.01532v1;2023-02-03;INV: Towards Streaming Incremental Neural Videos;"Recent works in spatiotemporal radiance fields can produce photorealistic
free-viewpoint videos. However, they are inherently unsuitable for interactive
streaming scenarios (e.g. video conferencing, telepresence) because have an
inevitable lag even if the training is instantaneous. This is because these
approaches consume videos and thus have to buffer chunks of frames (often
seconds) before processing. In this work, we take a step towards interactive
streaming via a frame-by-frame approach naturally free of lag. Conventional
wisdom believes that per-frame NeRFs are impractical due to prohibitive
training costs and storage. We break this belief by introducing Incremental
Neural Videos (INV), a per-frame NeRF that is efficiently trained and
streamable. We designed INV based on two insights: (1) Our main finding is that
MLPs naturally partition themselves into Structure and Color Layers, which
store structural and color/texture information respectively. (2) We leverage
this property to retain and improve upon knowledge from previous frames, thus
amortizing training across frames and reducing redundant learning. As a result,
with negligible changes to NeRF, INV can achieve good qualities (>28.6db) in
8min/frame. It can also outperform prior SOTA in 19% less training time.
Additionally, our Temporal Weight Compression reduces the per-frame size to
0.3MB/frame (6.6% of NeRF). More importantly, INV is free from buffer lag and
is naturally fit for streaming. While this work does not achieve real-time
training, it shows that incremental approaches like INV present new
possibilities in interactive 3D streaming. Moreover, our discovery of natural
information partition leads to a better understanding and manipulation of MLPs.
Code and dataset will be released soon.";Shengze Wang<author:sep>Alexey Supikov<author:sep>Joshua Ratcliff<author:sep>Henry Fuchs<author:sep>Ronald Azuma;http://arxiv.org/pdf/2302.01532v1;cs.CV;;nerf
2302.00833v1;http://arxiv.org/abs/2302.00833v1;2023-02-02;RobustNeRF: Ignoring Distractors with Robust Losses;"Neural radiance fields (NeRF) excel at synthesizing new views given
multi-view, calibrated images of a static scene. When scenes include
distractors, which are not persistent during image capture (moving objects,
lighting variations, shadows), artifacts appear as view-dependent effects or
'floaters'. To cope with distractors, we advocate a form of robust estimation
for NeRF training, modeling distractors in training data as outliers of an
optimization problem. Our method successfully removes outliers from a scene and
improves upon our baselines, on synthetic and real-world scenes. Our technique
is simple to incorporate in modern NeRF frameworks, with few hyper-parameters.
It does not assume a priori knowledge of the types of distractors, and is
instead focused on the optimization problem rather than pre-processing or
modeling transient objects. More results on our page
https://robustnerf.github.io/public.";Sara Sabour<author:sep>Suhani Vora<author:sep>Daniel Duckworth<author:sep>Ivan Krasin<author:sep>David J. Fleet<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2302.00833v1;cs.CV;;nerf
2302.01226v3;http://arxiv.org/abs/2302.01226v3;2023-02-02;Factor Fields: A Unified Framework for Neural Fields and Beyond;"We present Factor Fields, a novel framework for modeling and representing
signals. Factor Fields decomposes a signal into a product of factors, each
represented by a classical or neural field representation which operates on
transformed input coordinates. This decomposition results in a unified
framework that accommodates several recent signal representations including
NeRF, Plenoxels, EG3D, Instant-NGP, and TensoRF. Additionally, our framework
allows for the creation of powerful new signal representations, such as the
""Dictionary Field"" (DiF) which is a second contribution of this paper. Our
experiments show that DiF leads to improvements in approximation quality,
compactness, and training time when compared to previous fast reconstruction
methods. Experimentally, our representation achieves better image approximation
quality on 2D image regression tasks, higher geometric quality when
reconstructing 3D signed distance fields, and higher compactness for radiance
field reconstruction tasks. Furthermore, DiF enables generalization to unseen
images/3D scenes by sharing bases across signals during training which greatly
benefits use cases such as image regression from sparse observations and
few-shot radiance field reconstruction.";Anpei Chen<author:sep>Zexiang Xu<author:sep>Xinyue Wei<author:sep>Siyu Tang<author:sep>Hao Su<author:sep>Andreas Geiger;http://arxiv.org/pdf/2302.01226v3;cs.CV;"13 pages, 7 figures; Project Page:
  https://apchenstu.github.io/FactorFields/";nerf
2301.13430v1;http://arxiv.org/abs/2301.13430v1;2023-01-31;GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face  Synthesis;"Generating photo-realistic video portrait with arbitrary speech audio is a
crucial problem in film-making and virtual reality. Recently, several works
explore the usage of neural radiance field in this task to improve 3D realness
and image fidelity. However, the generalizability of previous NeRF-based
methods to out-of-domain audio is limited by the small scale of training data.
In this work, we propose GeneFace, a generalized and high-fidelity NeRF-based
talking face generation method, which can generate natural results
corresponding to various out-of-domain audio. Specifically, we learn a
variaitional motion generator on a large lip-reading corpus, and introduce a
domain adaptative post-net to calibrate the result. Moreover, we learn a
NeRF-based renderer conditioned on the predicted facial motion. A head-aware
torso-NeRF is proposed to eliminate the head-torso separation problem.
Extensive experiments show that our method achieves more generalized and
high-fidelity talking face generation compared to previous methods.";Zhenhui Ye<author:sep>Ziyue Jiang<author:sep>Yi Ren<author:sep>Jinglin Liu<author:sep>JinZheng He<author:sep>Zhou Zhao;http://arxiv.org/pdf/2301.13430v1;cs.CV;Accepted by ICLR2023. Project page: https://geneface.github.io/;nerf
2301.12780v2;http://arxiv.org/abs/2301.12780v2;2023-01-30;Equivariant Architectures for Learning in Deep Weight Spaces;"Designing machine learning architectures for processing neural networks in
their raw weight matrix form is a newly introduced research direction.
Unfortunately, the unique symmetry structure of deep weight spaces makes this
design very challenging. If successful, such architectures would be capable of
performing a wide range of intriguing tasks, from adapting a pre-trained
network to a new domain to editing objects represented as functions (INRs or
NeRFs). As a first step towards this goal, we present here a novel network
architecture for learning in deep weight spaces. It takes as input a
concatenation of weights and biases of a pre-trained MLP and processes it using
a composition of layers that are equivariant to the natural permutation
symmetry of the MLP's weights: Changing the order of neurons in intermediate
layers of the MLP does not affect the function it represents. We provide a full
characterization of all affine equivariant and invariant layers for these
symmetries and show how these layers can be implemented using three basic
operations: pooling, broadcasting, and fully connected layers applied to the
input in an appropriate manner. We demonstrate the effectiveness of our
architecture and its advantages over natural baselines in a variety of learning
tasks.";Aviv Navon<author:sep>Aviv Shamsian<author:sep>Idan Achituve<author:sep>Ethan Fetaya<author:sep>Gal Chechik<author:sep>Haggai Maron;http://arxiv.org/pdf/2301.12780v2;cs.LG;ICML 2023;nerf
2301.11520v3;http://arxiv.org/abs/2301.11520v3;2023-01-27;SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning;"As previous representations for reinforcement learning cannot effectively
incorporate a human-intuitive understanding of the 3D environment, they usually
suffer from sub-optimal performances. In this paper, we present Semantic-aware
Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly
optimizes semantic-aware neural radiance fields (NeRF) with a convolutional
encoder to learn 3D-aware neural implicit representation from multi-view
images. We introduce 3D semantic and distilled feature fields in parallel to
the RGB radiance fields in NeRF to learn semantic and object-centric
representation for reinforcement learning. SNeRL outperforms not only previous
pixel-based representations but also recent 3D-aware representations both in
model-free and model-based reinforcement learning.";Dongseok Shim<author:sep>Seungjae Lee<author:sep>H. Jin Kim;http://arxiv.org/pdf/2301.11520v3;cs.LG;"ICML 2023. First two authors contributed equally. Order was
  determined by coin flip";nerf
2301.11522v1;http://arxiv.org/abs/2301.11522v1;2023-01-27;A Comparison of Tiny-nerf versus Spatial Representations for 3d  Reconstruction;"Neural rendering has emerged as a powerful paradigm for synthesizing images,
offering many benefits over classical rendering by using neural networks to
reconstruct surfaces, represent shapes, and synthesize novel views, either for
objects or scenes. In this neural rendering, the environment is encoded into a
neural network. We believe that these new representations can be used to codify
the scene for a mobile robot. Therefore, in this work, we perform a comparison
between a trending neural rendering, called tiny-NeRF, and other volume
representations that are commonly used as maps in robotics, such as voxel maps,
point clouds, and triangular meshes. The target is to know the advantages and
disadvantages of neural representations in the robotics context. The comparison
is made in terms of spatial complexity and processing time to obtain a model.
Experiments show that tiny-NeRF requires three times less memory space compared
to other representations. In terms of processing time, tiny-NeRF takes about
six times more to compute the model.";Saulo Abraham Gante<author:sep>Juan Irving Vasquez<author:sep>Marco Antonio Valencia<author:sep>Mauricio Olguín Carbajal;http://arxiv.org/pdf/2301.11522v1;cs.AI;;nerf
2301.11631v1;http://arxiv.org/abs/2301.11631v1;2023-01-27;HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN;"Recently, generative models for 3D objects are gaining much popularity in VR
and augmented reality applications. Training such models using standard 3D
representations, like voxels or point clouds, is challenging and requires
complex tools for proper color rendering. In order to overcome this limitation,
Neural Radiance Fields (NeRFs) offer a state-of-the-art quality in synthesizing
novel views of complex 3D scenes from a small subset of 2D images.
  In the paper, we propose a generative model called HyperNeRFGAN, which uses
hypernetworks paradigm to produce 3D objects represented by NeRF. Our GAN
architecture leverages a hypernetwork paradigm to transfer gaussian noise into
weights of NeRF model. The model is further used to render 2D novel views, and
a classical 2D discriminator is utilized for training the entire GAN-based
structure. Our architecture produces 2D images, but we use 3D-aware NeRF
representation, which forces the model to produce correct 3D objects. The
advantage of the model over existing approaches is that it produces a dedicated
NeRF representation for the object without sharing some global parameters of
the rendering component. We show the superiority of our approach compared to
reference baselines on three challenging datasets from various domains.";Adam Kania<author:sep>Artur Kasymov<author:sep>Maciej Zięba<author:sep>Przemysław Spurek;http://arxiv.org/pdf/2301.11631v1;cs.CV;;nerf
2301.10941v3;http://arxiv.org/abs/2301.10941v3;2023-01-26;GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency;"We present a novel framework to regularize Neural Radiance Field (NeRF) in a
few-shot setting with a geometry-aware consistency regularization. The proposed
approach leverages a rendered depth map at unobserved viewpoint to warp sparse
input images to the unobserved viewpoint and impose them as pseudo ground
truths to facilitate learning of NeRF. By encouraging such geometry-aware
consistency at a feature-level instead of using pixel-level reconstruction
loss, we regularize the NeRF at semantic and structural levels while allowing
for modeling view dependent radiance to account for color variations across
viewpoints. We also propose an effective method to filter out erroneous warped
solutions, along with training strategies to stabilize training during
optimization. We show that our model achieves competitive results compared to
state-of-the-art few-shot NeRF models. Project page is available at
https://ku-cvlab.github.io/GeCoNeRF/.";Min-seop Kwak<author:sep>Jiuhn Song<author:sep>Seungryong Kim;http://arxiv.org/pdf/2301.10941v3;cs.CV;ICML 2023;nerf
2301.11280v1;http://arxiv.org/abs/2301.11280v1;2023-01-26;Text-To-4D Dynamic Scene Generation;"We present MAV3D (Make-A-Video3D), a method for generating three-dimensional
dynamic scenes from text descriptions. Our approach uses a 4D dynamic Neural
Radiance Field (NeRF), which is optimized for scene appearance, density, and
motion consistency by querying a Text-to-Video (T2V) diffusion-based model. The
dynamic video output generated from the provided text can be viewed from any
camera location and angle, and can be composited into any 3D environment. MAV3D
does not require any 3D or 4D data and the T2V model is trained only on
Text-Image pairs and unlabeled videos. We demonstrate the effectiveness of our
approach using comprehensive quantitative and qualitative experiments and show
an improvement over previously established internal baselines. To the best of
our knowledge, our method is the first to generate 3D dynamic scenes given a
text description.";Uriel Singer<author:sep>Shelly Sheynin<author:sep>Adam Polyak<author:sep>Oron Ashual<author:sep>Iurii Makarov<author:sep>Filippos Kokkinos<author:sep>Naman Goyal<author:sep>Andrea Vedaldi<author:sep>Devi Parikh<author:sep>Justin Johnson<author:sep>Yaniv Taigman;http://arxiv.org/pdf/2301.11280v1;cs.CV;;nerf
2301.10520v2;http://arxiv.org/abs/2301.10520v2;2023-01-25;Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging;"We present a physics-enhanced implicit neural representation (INR) for
ultrasound (US) imaging that learns tissue properties from overlapping US
sweeps. Our proposed method leverages a ray-tracing-based neural rendering for
novel view US synthesis. Recent publications demonstrated that INR models could
encode a representation of a three-dimensional scene from a set of
two-dimensional US frames. However, these models fail to consider the
view-dependent changes in appearance and geometry intrinsic to US imaging. In
our work, we discuss direction-dependent changes in the scene and show that a
physics-inspired rendering improves the fidelity of US image synthesis. In
particular, we demonstrate experimentally that our proposed method generates
geometrically accurate B-mode images for regions with ambiguous representation
owing to view-dependent differences of the US images. We conduct our
experiments using simulated B-mode US sweeps of the liver and acquired US
sweeps of a spine phantom tracked with a robotic arm. The experiments
corroborate that our method generates US frames that enable consistent volume
compounding from previously unseen views. To the best of our knowledge, the
presented work is the first to address view-dependent US image synthesis using
INR.";Magdalena Wysocki<author:sep>Mohammad Farid Azampour<author:sep>Christine Eilers<author:sep>Benjamin Busam<author:sep>Mehrdad Salehi<author:sep>Nassir Navab;http://arxiv.org/pdf/2301.10520v2;eess.IV;"accepted for oral presentation at MIDL 2023
  (https://openreview.net/forum?id=x4McMBwVyi)";nerf
2301.09632v2;http://arxiv.org/abs/2301.09632v2;2023-01-23;HexPlane: A Fast Representation for Dynamic Scenes;"Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D
vision. Prior approaches build on NeRF and rely on implicit representations.
This is slow since it requires many MLP evaluations, constraining real-world
applications. We show that dynamic 3D scenes can be explicitly represented by
six planes of learned features, leading to an elegant solution we call
HexPlane. A HexPlane computes features for points in spacetime by fusing
vectors extracted from each plane, which is highly efficient. Pairing a
HexPlane with a tiny MLP to regress output colors and training via volume
rendering gives impressive results for novel view synthesis on dynamic scenes,
matching the image quality of prior work but reducing training time by more
than $100\times$. Extensive ablations confirm our HexPlane design and show that
it is robust to different feature fusion mechanisms, coordinate systems, and
decoding mechanisms. HexPlane is a simple and effective solution for
representing 4D volumes, and we hope they can broadly contribute to modeling
spacetime for dynamic 3D scenes.";Ang Cao<author:sep>Justin Johnson;http://arxiv.org/pdf/2301.09632v2;cs.CV;"CVPR 2023, Camera Ready Project page:
  https://caoang327.github.io/HexPlane";nerf
2301.09060v3;http://arxiv.org/abs/2301.09060v3;2023-01-22;3D Reconstruction of Non-cooperative Resident Space Objects using  Instant NGP-accelerated NeRF and D-NeRF;"The proliferation of non-cooperative resident space objects (RSOs) in orbit
has spurred the demand for active space debris removal, on-orbit servicing
(OOS), classification, and functionality identification of these RSOs. Recent
advances in computer vision have enabled high-definition 3D modeling of objects
based on a set of 2D images captured from different viewing angles. This work
adapts Instant NeRF and D-NeRF, variations of the neural radiance field (NeRF)
algorithm to the problem of mapping RSOs in orbit for the purposes of
functionality identification and assisting with OOS. The algorithms are
evaluated for 3D reconstruction quality and hardware requirements using
datasets of images of a spacecraft mock-up taken under two different lighting
and motion conditions at the Orbital Robotic Interaction, On-Orbit Servicing
and Navigation (ORION) Laboratory at Florida Institute of Technology. Instant
NeRF is shown to learn high-fidelity 3D models with a computational cost that
could feasibly be trained on on-board computers.";Basilio Caruso<author:sep>Trupti Mahendrakar<author:sep>Van Minh Nguyen<author:sep>Ryan T. White<author:sep>Todd Steffen;http://arxiv.org/pdf/2301.09060v3;cs.CV;"Presented at AAS/AIAA Spaceflight Mechanics Conference 2023, 14
  pages, 10 figures, 2 tables";nerf
2301.07958v3;http://arxiv.org/abs/2301.07958v3;2023-01-19;RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color  Editing of 3D Scenes;"Radiance fields have gradually become a main representation of media.
Although its appearance editing has been studied, how to achieve
view-consistent recoloring in an efficient manner is still under explored. We
present RecolorNeRF, a novel user-friendly color editing approach for the
neural radiance fields. Our key idea is to decompose the scene into a set of
pure-colored layers, forming a palette. By this means, color manipulation can
be conducted by altering the color components of the palette directly. To
support efficient palette-based editing, the color of each layer needs to be as
representative as possible. In the end, the problem is formulated as an
optimization problem, where the layers and their blending weights are jointly
optimized with the NeRF itself. Extensive experiments show that our
jointly-optimized layer decomposition can be used against multiple backbones
and produce photo-realistic recolored novel-view renderings. We demonstrate
that RecolorNeRF outperforms baseline methods both quantitatively and
qualitatively for color editing even in complex real-world scenes.";Bingchen Gong<author:sep>Yuehao Wang<author:sep>Xiaoguang Han<author:sep>Qi Dou;http://arxiv.org/pdf/2301.07958v3;cs.CV;"To appear in ACM Multimedia 2023. Project website is accessible at
  https://sites.google.com/view/recolornerf";nerf
2301.07668v3;http://arxiv.org/abs/2301.07668v3;2023-01-18;Behind the Scenes: Density Fields for Single View Reconstruction;"Inferring a meaningful geometric scene representation from a single image is
a fundamental problem in computer vision. Approaches based on traditional depth
map prediction can only reason about areas that are visible in the image.
Currently, neural radiance fields (NeRFs) can capture true 3D including color,
but are too complex to be generated from a single image. As an alternative, we
propose to predict implicit density fields. A density field maps every location
in the frustum of the input image to volumetric density. By directly sampling
color from the available views instead of storing color in the density field,
our scene representation becomes significantly less complex compared to NeRFs,
and a neural network can predict it in a single forward pass. The prediction
network is trained through self-supervision from only video data. Our
formulation allows volume rendering to perform both depth prediction and novel
view synthesis. Through experiments, we show that our method is able to predict
meaningful geometry for regions that are occluded in the input image.
Additionally, we demonstrate the potential of our approach on three datasets
for depth prediction and novel-view synthesis.";Felix Wimbauer<author:sep>Nan Yang<author:sep>Christian Rupprecht<author:sep>Daniel Cremers;http://arxiv.org/pdf/2301.07668v3;cs.CV;Project Page: https://fwmb.github.io/bts/;nerf
2301.08556v1;http://arxiv.org/abs/2301.08556v1;2023-01-18;NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via  Novel-View Synthesis;"Expert demonstrations are a rich source of supervision for training visual
robotic manipulation policies, but imitation learning methods often require
either a large number of demonstrations or expensive online expert supervision
to learn reactive closed-loop behaviors. In this work, we introduce SPARTN
(Synthetic Perturbations for Augmenting Robot Trajectories via NeRF): a
fully-offline data augmentation scheme for improving robot policies that use
eye-in-hand cameras. Our approach leverages neural radiance fields (NeRFs) to
synthetically inject corrective noise into visual demonstrations, using NeRFs
to generate perturbed viewpoints while simultaneously calculating the
corrective actions. This requires no additional expert supervision or
environment interaction, and distills the geometric information in NeRFs into a
real-time reactive RGB-only policy. In a simulated 6-DoF visual grasping
benchmark, SPARTN improves success rates by 2.8$\times$ over imitation learning
without the corrective augmentations and even outperforms some methods that use
online supervision. It additionally closes the gap between RGB-only and RGB-D
success rates, eliminating the previous need for depth sensors. In real-world
6-DoF robotic grasping experiments from limited human demonstrations, our
method improves absolute success rates by $22.5\%$ on average, including
objects that are traditionally challenging for depth-based methods. See video
results at \url{https://bland.website/spartn}.";Allan Zhou<author:sep>Moo Jin Kim<author:sep>Lirui Wang<author:sep>Pete Florence<author:sep>Chelsea Finn;http://arxiv.org/pdf/2301.08556v1;cs.LG;;nerf
2301.06782v1;http://arxiv.org/abs/2301.06782v1;2023-01-17;A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View  Synthesis and Implicit Scene Reconstruction;"Neural Radiance Fields (NeRF) has achieved impressive results in single
object scene reconstruction and novel view synthesis, which have been
demonstrated on many single modality and single object focused indoor scene
datasets like DTU, BMVS, and NeRF Synthetic.However, the study of NeRF on
large-scale outdoor scene reconstruction is still limited, as there is no
unified outdoor scene dataset for large-scale NeRF evaluation due to expensive
data acquisition and calibration costs. In this paper, we propose a large-scale
outdoor multi-modal dataset, OMMO dataset, containing complex land objects and
scenes with calibrated images, point clouds and prompt annotations. Meanwhile,
a new benchmark for several outdoor NeRF-based tasks is established, such as
novel view synthesis, surface reconstruction, and multi-modal NeRF. To create
the dataset, we capture and collect a large number of real fly-view videos and
select high-quality and high-resolution clips from them. Then we design a
quality review module to refine images, remove low-quality frames and
fail-to-calibrate scenes through a learning-based automatic evaluation plus
manual review. Finally, a number of volunteers are employed to add the text
descriptions for each scene and key-frame to meet the potential multi-modal
requirements in the future. Compared with existing NeRF datasets, our dataset
contains abundant real-world urban and natural scenes with various scales,
camera trajectories, and lighting conditions. Experiments show that our dataset
can benchmark most state-of-the-art NeRF methods on different tasks. We will
release the dataset and model weights very soon.";Chongshan Lu<author:sep>Fukun Yin<author:sep>Xin Chen<author:sep>Tao Chen<author:sep>Gang YU<author:sep>Jiayuan Fan;http://arxiv.org/pdf/2301.06782v1;cs.CV;;nerf
2301.05747v1;http://arxiv.org/abs/2301.05747v1;2023-01-13;Laser: Latent Set Representations for 3D Generative Modeling;"NeRF provides unparalleled fidelity of novel view synthesis: rendering a 3D
scene from an arbitrary viewpoint. NeRF requires training on a large number of
views that fully cover a scene, which limits its applicability. While these
issues can be addressed by learning a prior over scenes in various forms,
previous approaches have been either applied to overly simple scenes or
struggling to render unobserved parts. We introduce Laser-NV: a generative
model which achieves high modelling capacity, and which is based on a
set-valued latent representation modelled by normalizing flows. Similarly to
previous amortized approaches, Laser-NV learns structure from multiple scenes
and is capable of fast, feed-forward inference from few views. To encourage
higher rendering fidelity and consistency with observed views, Laser-NV further
incorporates a geometry-informed attention mechanism over the observed views.
Laser-NV further produces diverse and plausible completions of occluded parts
of a scene while remaining consistent with observations. Laser-NV shows
state-of-the-art novel-view synthesis quality when evaluated on ShapeNet and on
a novel simulated City dataset, which features high uncertainty in the
unobserved regions of the scene.";Pol Moreno<author:sep>Adam R. Kosiorek<author:sep>Heiko Strathmann<author:sep>Daniel Zoran<author:sep>Rosalia G. Schneider<author:sep>Björn Winckler<author:sep>Larisa Markeeva<author:sep>Théophane Weber<author:sep>Danilo J. Rezende;http://arxiv.org/pdf/2301.05747v1;cs.CV;See https://laser-nv-paper.github.io/ for video results;nerf
2301.04075v1;http://arxiv.org/abs/2301.04075v1;2023-01-10;Benchmarking Robustness in Neural Radiance Fields;"Neural Radiance Field (NeRF) has demonstrated excellent quality in novel view
synthesis, thanks to its ability to model 3D object geometries in a concise
formulation. However, current approaches to NeRF-based models rely on clean
images with accurate camera calibration, which can be difficult to obtain in
the real world, where data is often subject to corruption and distortion. In
this work, we provide the first comprehensive analysis of the robustness of
NeRF-based novel view synthesis algorithms in the presence of different types
of corruptions.
  We find that NeRF-based models are significantly degraded in the presence of
corruption, and are more sensitive to a different set of corruptions than image
recognition models. Furthermore, we analyze the robustness of the feature
encoder in generalizable methods, which synthesize images using neural features
extracted via convolutional neural networks or transformers, and find that it
only contributes marginally to robustness. Finally, we reveal that standard
data augmentation techniques, which can significantly improve the robustness of
recognition models, do not help the robustness of NeRF-based models. We hope
that our findings will attract more researchers to study the robustness of
NeRF-based approaches and help to improve their performance in the real world.";Chen Wang<author:sep>Angtian Wang<author:sep>Junbo Li<author:sep>Alan Yuille<author:sep>Cihang Xie;http://arxiv.org/pdf/2301.04075v1;cs.CV;;nerf
2301.04101v2;http://arxiv.org/abs/2301.04101v2;2023-01-10;Neural Radiance Field Codebooks;"Compositional representations of the world are a promising step towards
enabling high-level scene understanding and efficient transfer to downstream
tasks. Learning such representations for complex scenes and tasks remains an
open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks
(NRC), a scalable method for learning object-centric representations through
novel view reconstruction. NRC learns to reconstruct scenes from novel views
using a dictionary of object codes which are decoded through a volumetric
renderer. This enables the discovery of reoccurring visual and geometric
patterns across scenes which are transferable to downstream tasks. We show that
NRC representations transfer well to object navigation in THOR, outperforming
2D and 3D representation learning methods by 3.1% success rate. We demonstrate
that our approach is able to perform unsupervised segmentation for more complex
synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29%
relative improvement). Finally, we show that NRC improves on the task of depth
ordering by 5.5% accuracy in THOR.";Matthew Wallingford<author:sep>Aditya Kusupati<author:sep>Alex Fang<author:sep>Vivek Ramanujan<author:sep>Aniruddha Kembhavi<author:sep>Roozbeh Mottaghi<author:sep>Ali Farhadi;http://arxiv.org/pdf/2301.04101v2;cs.CV;19 pages, 8 figures, 9 tables;
2301.03102v4;http://arxiv.org/abs/2301.03102v4;2023-01-08;Towards Open World NeRF-Based SLAM;"Neural Radiance Fields (NeRFs) offer versatility and robustness in map
representations for Simultaneous Localization and Mapping (SLAM) tasks. This
paper extends NICE-SLAM, a recent state-of-the-art NeRF-based SLAM algorithm
capable of producing high quality NeRF maps. However, depending on the hardware
used, the required number of iterations to produce these maps often makes
NICE-SLAM run at less than real time. Additionally, the estimated trajectories
fail to be competitive with classical SLAM approaches. Finally, NICE-SLAM
requires a grid covering the considered environment to be defined prior to
runtime, making it difficult to extend into previously unseen scenes. This
paper seeks to make NICE-SLAM more open-world-capable by improving the
robustness and tracking accuracy, and generalizing the map representation to
handle unconstrained environments. This is done by improving measurement
uncertainty handling, incorporating motion information, and modelling the map
as having an explicit foreground and background. It is shown that these changes
are able to improve tracking accuracy by 85% to 97% depending on the available
resources, while also improving mapping in environments with visual information
extending outside of the predefined grid.";Daniil Lisus<author:sep>Connor Holmes<author:sep>Steven Waslander;http://arxiv.org/pdf/2301.03102v4;cs.RO;"Presented at Conference on Robots and Vision (CRV) 2023. 8 pages, 2
  figures, 2 tables";nerf
2301.02975v2;http://arxiv.org/abs/2301.02975v2;2023-01-08;Traditional Readability Formulas Compared for English;"Traditional English readability formulas, or equations, were largely
developed in the 20th century. Nonetheless, many researchers still rely on them
for various NLP applications. This phenomenon is presumably due to the
convenience and straightforwardness of readability formulas. In this work, we
contribute to the NLP community by 1. introducing New English Readability
Formula (NERF), 2. recalibrating the coefficients of old readability formulas
(Flesch-Kincaid Grade Level, Fog Index, SMOG Index, Coleman-Liau Index, and
Automated Readability Index), 3. evaluating the readability formulas, for use
in text simplification studies and medical texts, and 4. developing a
Python-based program for the wide application to various NLP projects.";Bruce W. Lee<author:sep>Jason Hyung-Jong Lee;http://arxiv.org/pdf/2301.02975v2;cs.CL;Submitted to EMNLP 2022;nerf
2301.05187v1;http://arxiv.org/abs/2301.05187v1;2023-01-05;WIRE: Wavelet Implicit Neural Representations;"Implicit neural representations (INRs) have recently advanced numerous
vision-related areas. INR performance depends strongly on the choice of the
nonlinear activation function employed in its multilayer perceptron (MLP)
network. A wide range of nonlinearities have been explored, but, unfortunately,
current INRs designed to have high accuracy also suffer from poor robustness
(to signal noise, parameter variation, etc.). Inspired by harmonic analysis, we
develop a new, highly accurate and robust INR that does not exhibit this
tradeoff. Wavelet Implicit neural REpresentation (WIRE) uses a continuous
complex Gabor wavelet activation function that is well-known to be optimally
concentrated in space-frequency and to have excellent biases for representing
images. A wide range of experiments (image denoising, image inpainting,
super-resolution, computed tomography reconstruction, image overfitting, and
novel view synthesis with neural radiance fields) demonstrate that WIRE defines
the new state of the art in INR accuracy, training time, and robustness.";Vishwanath Saragadam<author:sep>Daniel LeJeune<author:sep>Jasper Tan<author:sep>Guha Balakrishnan<author:sep>Ashok Veeraraghavan<author:sep>Richard G. Baraniuk;http://arxiv.org/pdf/2301.05187v1;cs.CV;;
2301.00950v3;http://arxiv.org/abs/2301.00950v3;2023-01-03;Class-Continuous Conditional Generative Neural Radiance Field;"The 3D-aware image synthesis focuses on conserving spatial consistency
besides generating high-resolution images with fine details. Recently, Neural
Radiance Field (NeRF) has been introduced for synthesizing novel views with low
computational cost and superior performance. While several works investigate a
generative NeRF and show remarkable achievement, they cannot handle conditional
and continuous feature manipulation in the generation procedure. In this work,
we introduce a novel model, called Class-Continuous Conditional Generative NeRF
($\text{C}^{3}$G-NeRF), which can synthesize conditionally manipulated
photorealistic 3D-consistent images by projecting conditional features to the
generator and the discriminator. The proposed $\text{C}^{3}$G-NeRF is evaluated
with three image datasets, AFHQ, CelebA, and Cars. As a result, our model shows
strong 3D-consistency with fine details and smooth interpolation in conditional
feature manipulation. For instance, $\text{C}^{3}$G-NeRF exhibits a Fr\'echet
Inception Distance (FID) of 7.64 in 3D-aware face image synthesis with a
$\text{128}^{2}$ resolution. Additionally, we provide FIDs of generated
3D-aware images of each class of the datasets as it is possible to synthesize
class-conditional images with $\text{C}^{3}$G-NeRF.";Jiwook Kim<author:sep>Minhyeok Lee;http://arxiv.org/pdf/2301.00950v3;cs.CV;BMVC 2023 (Accepted);nerf
2301.00411v2;http://arxiv.org/abs/2301.00411v2;2023-01-01;Detachable Novel Views Synthesis of Dynamic Scenes Using  Distribution-Driven Neural Radiance Fields;"Representing and synthesizing novel views in real-world dynamic scenes from
casual monocular videos is a long-standing problem. Existing solutions
typically approach dynamic scenes by applying geometry techniques or utilizing
temporal information between several adjacent frames without considering the
underlying background distribution in the entire scene or the transmittance
over the ray dimension, limiting their performance on static and occlusion
areas. Our approach $\textbf{D}$istribution-$\textbf{D}$riven neural radiance
fields offers high-quality view synthesis and a 3D solution to
$\textbf{D}$etach the background from the entire $\textbf{D}$ynamic scene,
which is called $\text{D}^4$NeRF. Specifically, it employs a neural
representation to capture the scene distribution in the static background and a
6D-input NeRF to represent dynamic objects, respectively. Each ray sample is
given an additional occlusion weight to indicate the transmittance lying in the
static and dynamic components. We evaluate $\text{D}^4$NeRF on public dynamic
scenes and our urban driving scenes acquired from an autonomous-driving
dataset. Extensive experiments demonstrate that our approach outperforms
previous methods in rendering texture details and motion areas while also
producing a clean static background. Our code will be released at
https://github.com/Luciferbobo/D4NeRF.";Boyu Zhang<author:sep>Wenbo Xu<author:sep>Zheng Zhu<author:sep>Guan Huang;http://arxiv.org/pdf/2301.00411v2;cs.CV;;nerf
2212.14710v1;http://arxiv.org/abs/2212.14710v1;2022-12-30;NeRF-Gaze: A Head-Eye Redirection Parametric Model for Gaze Estimation;"Gaze estimation is the fundamental basis for many visual tasks. Yet, the high
cost of acquiring gaze datasets with 3D annotations hinders the optimization
and application of gaze estimation models. In this work, we propose a novel
Head-Eye redirection parametric model based on Neural Radiance Field, which
allows dense gaze data generation with view consistency and accurate gaze
direction. Moreover, our head-eye redirection parametric model can decouple the
face and eyes for separate neural rendering, so it can achieve the purpose of
separately controlling the attributes of the face, identity, illumination, and
eye gaze direction. Thus diverse 3D-aware gaze datasets could be obtained by
manipulating the latent code belonging to different face attributions in an
unsupervised manner. Extensive experiments on several benchmarks demonstrate
the effectiveness of our method in domain generalization and domain adaptation
for gaze estimation tasks.";Pengwei Yin<author:sep>Jiawu Dai<author:sep>Jingjing Wang<author:sep>Di Xie<author:sep>Shiliang Pu;http://arxiv.org/pdf/2212.14710v1;cs.CV;10 pages, 8 figures, submitted to CVPR 2023;nerf
2212.14704v2;http://arxiv.org/abs/2212.14704v2;2022-12-28;Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and  Text-to-Image Diffusion Models;"Recent CLIP-guided 3D optimization methods, such as DreamFields and
PureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D
synthesis. However, due to scratch training and random initialization without
prior knowledge, these methods often fail to generate accurate and faithful 3D
structures that conform to the input text. In this paper, we make the first
attempt to introduce explicit 3D shape priors into the CLIP-guided 3D
optimization process. Specifically, we first generate a high-quality 3D shape
from the input text in the text-to-shape stage as a 3D shape prior. We then use
it as the initialization of a neural radiance field and optimize it with the
full prompt. To address the challenging text-to-shape generation task, we
present a simple yet effective approach that directly bridges the text and
image modalities with a powerful text-to-image diffusion model. To narrow the
style domain gap between the images synthesized by the text-to-image diffusion
model and shape renderings used to train the image-to-shape generator, we
further propose to jointly optimize a learnable text prompt and fine-tune the
text-to-image diffusion model for rendering-style image generation. Our method,
Dream3D, is capable of generating imaginative 3D content with superior visual
quality and shape accuracy compared to state-of-the-art methods.";Jiale Xu<author:sep>Xintao Wang<author:sep>Weihao Cheng<author:sep>Yan-Pei Cao<author:sep>Ying Shan<author:sep>Xiaohu Qie<author:sep>Shenghua Gao;http://arxiv.org/pdf/2212.14704v2;cs.CV;"Accepted by CVPR 2023. Project page:
  https://bluestyle97.github.io/dream3d/";nerf
2212.13056v3;http://arxiv.org/abs/2212.13056v3;2022-12-26;MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular  Videos;"In this paper, we target at the problem of learning a generalizable dynamic
radiance field from monocular videos. Different from most existing NeRF methods
that are based on multiple views, monocular videos only contain one view at
each timestamp, thereby suffering from ambiguity along the view direction in
estimating point features and scene flows. Previous studies such as DynNeRF
disambiguate point features by positional encoding, which is not transferable
and severely limits the generalization ability. As a result, these methods have
to train one independent model for each scene and suffer from heavy
computational costs when applying to increasing monocular videos in real-world
applications. To address this, We propose MonoNeRF to simultaneously learn
point features and scene flows with point trajectory and feature correspondence
constraints across frames. More specifically, we learn an implicit velocity
field to estimate point trajectory from temporal features with Neural ODE,
which is followed by a flow-based feature aggregation module to obtain spatial
features along the point trajectory. We jointly optimize temporal and spatial
features in an end-to-end manner. Experiments show that our MonoNeRF is able to
learn from multiple scenes and support new applications such as scene editing,
unseen frame synthesis, and fast novel scene adaptation. Codes are available at
https://github.com/tianfr/MonoNeRF.";Fengrui Tian<author:sep>Shaoyi Du<author:sep>Yueqi Duan;http://arxiv.org/pdf/2212.13056v3;cs.CV;Accepted by ICCV 2023;nerf
2212.12871v1;http://arxiv.org/abs/2212.12871v1;2022-12-25;PaletteNeRF: Palette-based Color Editing for NeRFs;"Neural Radiance Field (NeRF) is a powerful tool to faithfully generate novel
views for scenes with only sparse captured images. Despite its strong
capability for representing 3D scenes and their appearance, its editing ability
is very limited. In this paper, we propose a simple but effective extension of
vanilla NeRF, named PaletteNeRF, to enable efficient color editing on
NeRF-represented scenes. Motivated by recent palette-based image decomposition
works, we approximate each pixel color as a sum of palette colors modulated by
additive weights. Instead of predicting pixel colors as in vanilla NeRFs, our
method predicts additive weights. The underlying NeRF backbone could also be
replaced with more recent NeRF models such as KiloNeRF to achieve real-time
editing. Experimental results demonstrate that our method achieves efficient,
view-consistent, and artifact-free color editing on a wide range of
NeRF-represented scenes.";Qiling Wu<author:sep>Jianchao Tan<author:sep>Kun Xu;http://arxiv.org/pdf/2212.12871v1;cs.CV;12 pages, 10 figures;nerf
2212.11966v1;http://arxiv.org/abs/2212.11966v1;2022-12-22;Removing Objects From Neural Radiance Fields;"Neural Radiance Fields (NeRFs) are emerging as a ubiquitous scene
representation that allows for novel view synthesis. Increasingly, NeRFs will
be shareable with other people. Before sharing a NeRF, though, it might be
desirable to remove personal information or unsightly objects. Such removal is
not easily achieved with the current NeRF editing frameworks. We propose a
framework to remove objects from a NeRF representation created from an RGB-D
sequence. Our NeRF inpainting method leverages recent work in 2D image
inpainting and is guided by a user-provided mask. Our algorithm is underpinned
by a confidence based view selection procedure. It chooses which of the
individual 2D inpainted images to use in the creation of the NeRF, so that the
resulting inpainted NeRF is 3D consistent. We show that our method for NeRF
editing is effective for synthesizing plausible inpaintings in a multi-view
coherent manner. We validate our approach using a new and still-challenging
dataset for the task of NeRF inpainting.";Silvan Weder<author:sep>Guillermo Garcia-Hernando<author:sep>Aron Monszpart<author:sep>Marc Pollefeys<author:sep>Gabriel Brostow<author:sep>Michael Firman<author:sep>Sara Vicente;http://arxiv.org/pdf/2212.11966v1;cs.CV;;nerf
2212.10699v2;http://arxiv.org/abs/2212.10699v2;2022-12-21;PaletteNeRF: Palette-based Appearance Editing of Neural Radiance Fields;"Recent advances in neural radiance fields have enabled the high-fidelity 3D
reconstruction of complex scenes for novel view synthesis. However, it remains
underexplored how the appearance of such representations can be efficiently
edited while maintaining photorealism.
  In this work, we present PaletteNeRF, a novel method for photorealistic
appearance editing of neural radiance fields (NeRF) based on 3D color
decomposition. Our method decomposes the appearance of each 3D point into a
linear combination of palette-based bases (i.e., 3D segmentations defined by a
group of NeRF-type functions) that are shared across the scene. While our
palette-based bases are view-independent, we also predict a view-dependent
function to capture the color residual (e.g., specular shading). During
training, we jointly optimize the basis functions and the color palettes, and
we also introduce novel regularizers to encourage the spatial coherence of the
decomposition.
  Our method allows users to efficiently edit the appearance of the 3D scene by
modifying the color palettes. We also extend our framework with compressed
semantic features for semantic-aware appearance editing. We demonstrate that
our technique is superior to baseline methods both quantitatively and
qualitatively for appearance editing of complex real-world scenes.";Zhengfei Kuang<author:sep>Fujun Luan<author:sep>Sai Bi<author:sep>Zhixin Shu<author:sep>Gordon Wetzstein<author:sep>Kalyan Sunkavalli;http://arxiv.org/pdf/2212.10699v2;cs.CV;;nerf
2212.10950v2;http://arxiv.org/abs/2212.10950v2;2022-12-21;Incremental Neural Implicit Representation with Uncertainty-Filtered  Knowledge Distillation;"Recent neural implicit representations (NIRs) have achieved great success in
the tasks of 3D reconstruction and novel view synthesis. However, they suffer
from the catastrophic forgetting problem when continuously learning from
streaming data without revisiting the previously seen data. This limitation
prohibits the application of existing NIRs to scenarios where images come in
sequentially. In view of this, we explore the task of incremental learning for
NIRs in this work. We design a student-teacher framework to mitigate the
catastrophic forgetting problem. Specifically, we iterate the process of using
the student as the teacher at the end of each time step and let the teacher
guide the training of the student in the next step. As a result, the student
network is able to learn new information from the streaming data and retain old
knowledge from the teacher network simultaneously. Although intuitive, naively
applying the student-teacher pipeline does not work well in our task. Not all
information from the teacher network is helpful since it is only trained with
the old data. To alleviate this problem, we further introduce a random inquirer
and an uncertainty-based filter to filter useful information. Our proposed
method is general and thus can be adapted to different implicit representations
such as neural radiance field (NeRF) and neural SDF. Extensive experimental
results for both 3D reconstruction and novel view synthesis demonstrate the
effectiveness of our approach compared to different baselines.";Mengqi Guo<author:sep>Chen Li<author:sep>Hanlin Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2212.10950v2;cs.CV;;nerf
2212.09330v1;http://arxiv.org/abs/2212.09330v1;2022-12-19;StyleTRF: Stylizing Tensorial Radiance Fields;"Stylized view generation of scenes captured casually using a camera has
received much attention recently. The geometry and appearance of the scene are
typically captured as neural point sets or neural radiance fields in the
previous work. An image stylization method is used to stylize the captured
appearance by training its network jointly or iteratively with the structure
capture network. The state-of-the-art SNeRF method trains the NeRF and
stylization network in an alternating manner. These methods have high training
time and require joint optimization. In this work, we present StyleTRF, a
compact, quick-to-optimize strategy for stylized view generation using TensoRF.
The appearance part is fine-tuned using sparse stylized priors of a few views
rendered using the TensoRF representation for a few iterations. Our method thus
effectively decouples style-adaption from view capture and is much faster than
the previous methods. We show state-of-the-art results on several scenes used
for this purpose.";Rahul Goel<author:sep>Sirikonda Dhawal<author:sep>Saurabh Saini<author:sep>P. J. Narayanan;http://arxiv.org/pdf/2212.09330v1;cs.CV;Accepted at ICVGIP-2022;nerf
2212.09735v2;http://arxiv.org/abs/2212.09735v2;2022-12-19;Correspondence Distillation from NeRF-based GAN;"The neural radiance field (NeRF) has shown promising results in preserving
the fine details of objects and scenes. However, unlike mesh-based
representations, it remains an open problem to build dense correspondences
across different NeRFs of the same category, which is essential in many
downstream tasks. The main difficulties of this problem lie in the implicit
nature of NeRF and the lack of ground-truth correspondence annotations. In this
paper, we show it is possible to bypass these challenges by leveraging the rich
semantics and structural priors encapsulated in a pre-trained NeRF-based GAN.
Specifically, we exploit such priors from three aspects, namely 1) a dual
deformation field that takes latent codes as global structural indicators, 2) a
learning objective that regards generator features as geometric-aware local
descriptors, and 3) a source of infinite object-specific NeRF samples. Our
experiments demonstrate that such priors lead to 3D dense correspondence that
is accurate, smooth, and robust. We also show that established dense
correspondence across NeRFs can effectively enable many NeRF-based downstream
applications such as texture transfer.";Yushi Lan<author:sep>Chen Change Loy<author:sep>Bo Dai;http://arxiv.org/pdf/2212.09735v2;cs.CV;Project page: https://nirvanalan.github.io/projects/DDF/index.html;nerf
2212.09100v3;http://arxiv.org/abs/2212.09100v3;2022-12-18;SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input  Images;"Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel
view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels
for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage
machine learning and adoption of SRFs as a 3D representation, we present SPARF,
a large-scale ShapeNet-based synthetic dataset for novel view synthesis
consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at
high resolution (400 X 400 pixels). The dataset is orders of magnitude larger
than existing synthetic datasets for novel view synthesis and includes more
than one million 3D-optimized radiance fields with multiple voxel resolutions.
Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate
sparse voxel radiance fields from only few views. This is done by using the
densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs
partial SRFs from few/one images and a specialized SRF loss to learn to
generate high-quality sparse voxel radiance fields that can be rendered from
novel views. Our approach achieves state-of-the-art results in the task of
unconstrained novel view synthesis based on few views on ShapeNet as compared
to recent baselines. The SPARF dataset is made public with the code and models
on the project website https://abdullahamdi.com/sparf/ .";Abdullah Hamdi<author:sep>Bernard Ghanem<author:sep>Matthias Nießner;http://arxiv.org/pdf/2212.09100v3;cs.CV;published at ICCV 2023 workshop proceedings;nerf
2212.09069v2;http://arxiv.org/abs/2212.09069v2;2022-12-18;Masked Wavelet Representation for Compact Neural Radiance Fields;"Neural radiance fields (NeRF) have demonstrated the potential of
coordinate-based neural representation (neural fields or implicit neural
representation) in neural rendering. However, using a multi-layer perceptron
(MLP) to represent a 3D scene or object requires enormous computational
resources and time. There have been recent studies on how to reduce these
computational inefficiencies by using additional data structures, such as grids
or trees. Despite the promising performance, the explicit data structure
necessitates a substantial amount of memory. In this work, we present a method
to reduce the size without compromising the advantages of having additional
data structures. In detail, we propose using the wavelet transform on
grid-based neural fields. Grid-based neural fields are for fast convergence,
and the wavelet transform, whose efficiency has been demonstrated in
high-performance standard codecs, is to improve the parameter efficiency of
grids. Furthermore, in order to achieve a higher sparsity of grid coefficients
while maintaining reconstruction quality, we present a novel trainable masking
approach. Experimental results demonstrate that non-spatial grid coefficients,
such as wavelet coefficients, are capable of attaining a higher level of
sparsity than spatial grid coefficients, resulting in a more compact
representation. With our proposed mask and compression pipeline, we achieved
state-of-the-art performance within a memory budget of 2 MB. Our code is
available at https://github.com/daniel03c1/masked_wavelet_nerf.";Daniel Rho<author:sep>Byeonghyeon Lee<author:sep>Seungtae Nam<author:sep>Joo Chan Lee<author:sep>Jong Hwan Ko<author:sep>Eunbyung Park;http://arxiv.org/pdf/2212.09069v2;cs.CV;Accepted to CVPR 2023;nerf
2212.08328v2;http://arxiv.org/abs/2212.08328v2;2022-12-16;MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance  Fields;"Hinged on the representation power of neural networks, neural radiance fields
(NeRF) have recently emerged as one of the promising and widely applicable
methods for 3D object and scene representation. However, NeRF faces challenges
in practical applications, such as large-scale scenes and edge devices with a
limited amount of memory, where data needs to be processed sequentially. Under
such incremental learning scenarios, neural networks are known to suffer
catastrophic forgetting: easily forgetting previously seen data after training
with new data. We observe that previous incremental learning algorithms are
limited by either low performance or memory scalability issues. As such, we
develop a Memory-Efficient Incremental Learning algorithm for NeRF (MEIL-NeRF).
MEIL-NeRF takes inspiration from NeRF itself in that a neural network can serve
as a memory that provides the pixel RGB values, given rays as queries. Upon the
motivation, our framework learns which rays to query NeRF to extract previous
pixel values. The extracted pixel values are then used to train NeRF in a
self-distillation manner to prevent catastrophic forgetting. As a result,
MEIL-NeRF demonstrates constant memory consumption and competitive performance.";Jaeyoung Chung<author:sep>Kanggeon Lee<author:sep>Sungyong Baik<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2212.08328v2;cs.CV;"18 pages. For the project page, see
  https://robot0321.github.io/meil-nerf/index.html";nerf
2212.08057v2;http://arxiv.org/abs/2212.08057v2;2022-12-15;Real-Time Neural Light Field on Mobile Devices;"Recent efforts in Neural Rendering Fields (NeRF) have shown impressive
results on novel view synthesis by utilizing implicit neural representation to
represent 3D scenes. Due to the process of volumetric rendering, the inference
speed for NeRF is extremely slow, limiting the application scenarios of
utilizing NeRF on resource-constrained hardware, such as mobile devices. Many
works have been conducted to reduce the latency of running NeRF models.
However, most of them still require high-end GPU for acceleration or extra
storage memory, which is all unavailable on mobile devices. Another emerging
direction utilizes the neural light field (NeLF) for speedup, as only one
forward pass is performed on a ray to predict the pixel color. Nevertheless, to
reach a similar rendering quality as NeRF, the network in NeLF is designed with
intensive computation, which is not mobile-friendly. In this work, we propose
an efficient network that runs in real-time on mobile devices for neural
rendering. We follow the setting of NeLF to train our network. Unlike existing
works, we introduce a novel network architecture that runs efficiently on
mobile devices with low latency and small size, i.e., saving $15\times \sim
24\times$ storage compared with MobileNeRF. Our model achieves high-resolution
generation while maintaining real-time inference for both synthetic and
real-world scenes on mobile devices, e.g., $18.04$ms (iPhone 13) for rendering
one $1008\times756$ image of real 3D scenes. Additionally, we achieve similar
image quality as NeRF and better quality than MobileNeRF (PSNR $26.15$ vs.
$25.91$ on the real-world forward-facing dataset).";Junli Cao<author:sep>Huan Wang<author:sep>Pavlo Chemerys<author:sep>Vladislav Shakhrai<author:sep>Ju Hu<author:sep>Yun Fu<author:sep>Denys Makoviichuk<author:sep>Sergey Tulyakov<author:sep>Jian Ren;http://arxiv.org/pdf/2212.08057v2;cs.CV;"CVPR 2023. Project page: https://snap-research.github.io/MobileR2L/
  Code: https://github.com/snap-research/MobileR2L/";nerf
2212.08476v1;http://arxiv.org/abs/2212.08476v1;2022-12-15;SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory;"Neural Radiance Fields (NeRF) have demonstrated superior novel view synthesis
performance but are slow at rendering. To speed up the volume rendering
process, many acceleration methods have been proposed at the cost of large
memory consumption. To push the frontier of the efficiency-memory trade-off, we
explore a new perspective to accelerate NeRF rendering, leveraging a key fact
that the viewpoint change is usually smooth and continuous in interactive
viewpoint control. This allows us to leverage the information of preceding
viewpoints to reduce the number of rendered pixels as well as the number of
sampled points along the ray of the remaining pixels. In our pipeline, a
low-resolution feature map is rendered first by volume rendering, then a
lightweight 2D neural renderer is applied to generate the output image at
target resolution leveraging the features of preceding and current frames. We
show that the proposed method can achieve competitive rendering quality while
reducing the rendering time with little memory overhead, enabling 30FPS at
1080P image resolution with a low memory footprint.";Sicheng Li<author:sep>Hao Li<author:sep>Yue Wang<author:sep>Yiyi Liao<author:sep>Lu Yu;http://arxiv.org/pdf/2212.08476v1;cs.CV;;nerf
2212.08070v1;http://arxiv.org/abs/2212.08070v1;2022-12-15;NeRF-Art: Text-Driven Neural Radiance Fields Stylization;"As a powerful representation of 3D scenes, the neural radiance field (NeRF)
enables high-quality novel view synthesis from multi-view images. Stylizing
NeRF, however, remains challenging, especially on simulating a text-guided
style with both the appearance and the geometry altered simultaneously. In this
paper, we present NeRF-Art, a text-guided NeRF stylization approach that
manipulates the style of a pre-trained NeRF model with a simple text prompt.
Unlike previous approaches that either lack sufficient geometry deformations
and texture details or require meshes to guide the stylization, our method can
shift a 3D scene to the target style characterized by desired geometry and
appearance variations without any mesh guidance. This is achieved by
introducing a novel global-local contrastive learning strategy, combined with
the directional constraint to simultaneously control both the trajectory and
the strength of the target style. Moreover, we adopt a weight regularization
method to effectively suppress cloudy artifacts and geometry noises which arise
easily when the density field is transformed during geometry stylization.
Through extensive experiments on various styles, we demonstrate that our method
is effective and robust regarding both single-view stylization quality and
cross-view consistency. The code and more results can be found in our project
page: https://cassiepython.github.io/nerfart/.";Can Wang<author:sep>Ruixiang Jiang<author:sep>Menglei Chai<author:sep>Mingming He<author:sep>Dongdong Chen<author:sep>Jing Liao;http://arxiv.org/pdf/2212.08070v1;cs.CV;Project page: https://cassiepython.github.io/nerfart/;nerf
2212.08067v2;http://arxiv.org/abs/2212.08067v2;2022-12-15;VolRecon: Volume Rendering of Signed Ray Distance Functions for  Generalizable Multi-View Reconstruction;"The success of the Neural Radiance Fields (NeRF) in novel view synthesis has
inspired researchers to propose neural implicit scene reconstruction. However,
most existing neural implicit reconstruction methods optimize per-scene
parameters and therefore lack generalizability to new scenes. We introduce
VolRecon, a novel generalizable implicit reconstruction method with Signed Ray
Distance Function (SRDF). To reconstruct the scene with fine details and little
noise, VolRecon combines projection features aggregated from multi-view
features, and volume features interpolated from a coarse global feature volume.
Using a ray transformer, we compute SRDF values of sampled points on a ray and
then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by
about 30% in sparse view reconstruction and achieves comparable accuracy as
MVSNet in full view reconstruction. Furthermore, our approach exhibits good
generalization performance on the large-scale ETH3D benchmark.";Yufan Ren<author:sep>Fangjinhua Wang<author:sep>Tong Zhang<author:sep>Marc Pollefeys<author:sep>Sabine Süsstrunk;http://arxiv.org/pdf/2212.08067v2;cs.CV;;nerf
2212.07388v3;http://arxiv.org/abs/2212.07388v3;2022-12-14;NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior;"Training a Neural Radiance Field (NeRF) without pre-computed camera poses is
challenging. Recent advances in this direction demonstrate the possibility of
jointly optimising a NeRF and camera poses in forward-facing scenes. However,
these methods still face difficulties during dramatic camera movement. We
tackle this challenging problem by incorporating undistorted monocular depth
priors. These priors are generated by correcting scale and shift parameters
during training, with which we are then able to constrain the relative poses
between consecutive frames. This constraint is achieved using our proposed
novel loss functions. Experiments on real-world indoor and outdoor scenes show
that our method can handle challenging camera trajectories and outperforms
existing methods in terms of novel view rendering quality and pose estimation
accuracy. Our project page is https://nope-nerf.active.vision.";Wenjing Bian<author:sep>Zirui Wang<author:sep>Kejie Li<author:sep>Jia-Wang Bian<author:sep>Victor Adrian Prisacariu;http://arxiv.org/pdf/2212.07388v3;cs.CV;;nerf
2212.06135v1;http://arxiv.org/abs/2212.06135v1;2022-12-12;Rodin: A Generative Model for Sculpting 3D Digital Avatars Using  Diffusion;"This paper presents a 3D generative model that uses diffusion models to
automatically generate 3D digital avatars represented as neural radiance
fields. A significant challenge in generating such avatars is that the memory
and processing costs in 3D are prohibitive for producing the rich details
required for high-quality avatars. To tackle this problem we propose the
roll-out diffusion network (Rodin), which represents a neural radiance field as
multiple 2D feature maps and rolls out these maps into a single 2D feature
plane within which we perform 3D-aware diffusion. The Rodin model brings the
much-needed computational efficiency while preserving the integrity of
diffusion in 3D by using 3D-aware convolution that attends to projected
features in the 2D feature plane according to their original relationship in
3D. We also use latent conditioning to orchestrate the feature generation for
global coherence, leading to high-fidelity avatars and enabling their semantic
editing based on text prompts. Finally, we use hierarchical synthesis to
further enhance details. The 3D avatars generated by our model compare
favorably with those produced by existing generative techniques. We can
generate highly detailed avatars with realistic hairstyles and facial hair like
beards. We also demonstrate 3D avatar generation from image or text as well as
text-guided editability.";Tengfei Wang<author:sep>Bo Zhang<author:sep>Ting Zhang<author:sep>Shuyang Gu<author:sep>Jianmin Bao<author:sep>Tadas Baltrusaitis<author:sep>Jingjing Shen<author:sep>Dong Chen<author:sep>Fang Wen<author:sep>Qifeng Chen<author:sep>Baining Guo;http://arxiv.org/pdf/2212.06135v1;cs.CV;Project Webpage: https://3d-avatar-diffusion.microsoft.com/;
2212.04701v2;http://arxiv.org/abs/2212.04701v2;2022-12-09;4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions;"In this paper, we present a novel and effective framework, named 4K-NeRF, to
pursue high fidelity view synthesis on the challenging scenarios of ultra high
resolutions, building on the methodology of neural radiance fields (NeRF). The
rendering procedure of NeRF-based methods typically relies on a pixel-wise
manner in which rays (or pixels) are treated independently on both training and
inference phases, limiting its representational ability on describing subtle
details, especially when lifting to a extremely high resolution. We address the
issue by exploring ray correlation to enhance high-frequency details recovery.
Particularly, we use the 3D-aware encoder to model geometric information
effectively in a lower resolution space and recover fine details through the
3D-aware decoder, conditioned on ray features and depths estimated by the
encoder. Joint training with patch-based sampling further facilitates our
method incorporating the supervision from perception oriented regularization
beyond pixel-wise loss. Benefiting from the use of geometry-aware local
context, our method can significantly boost rendering quality on high-frequency
details compared with modern NeRF methods, and achieve the state-of-the-art
visual quality on 4K ultra-high-resolution scenarios. Code Available at
\url{https://github.com/frozoul/4K-NeRF}";Zhongshu Wang<author:sep>Lingzhi Li<author:sep>Zhen Shen<author:sep>Li Shen<author:sep>Liefeng Bo;http://arxiv.org/pdf/2212.04701v2;cs.CV;;nerf
2212.04492v2;http://arxiv.org/abs/2212.04492v2;2022-12-08;Few-View Object Reconstruction with Unknown Categories and Camera Poses;"While object reconstruction has made great strides in recent years, current
methods typically require densely captured images and/or known camera poses,
and generalize poorly to novel object categories. To step toward object
reconstruction in the wild, this work explores reconstructing general
real-world objects from a few images without known camera poses or object
categories. The crux of our work is solving two fundamental 3D vision problems
-- shape reconstruction and pose estimation -- in a unified approach. Our
approach captures the synergies of these two problems: reliable camera pose
estimation gives rise to accurate shape reconstruction, and the accurate
reconstruction, in turn, induces robust correspondence between different views
and facilitates pose estimation. Our method FORGE predicts 3D features from
each view and leverages them in conjunction with the input images to establish
cross-view correspondence for estimating relative camera poses. The 3D features
are then transformed by the estimated poses into a shared space and are fused
into a neural radiance field. The reconstruction results are rendered by volume
rendering techniques, enabling us to train the model without 3D shape
ground-truth. Our experiments show that FORGE reliably reconstructs objects
from five views. Our pose estimation method outperforms existing ones by a
large margin. The reconstruction results under predicted poses are comparable
to the ones using ground-truth poses. The performance on novel testing
categories matches the results on categories seen during training. Project
page: https://ut-austin-rpl.github.io/FORGE/";Hanwen Jiang<author:sep>Zhenyu Jiang<author:sep>Kristen Grauman<author:sep>Yuke Zhu;http://arxiv.org/pdf/2212.04492v2;cs.CV;;
2212.04823v2;http://arxiv.org/abs/2212.04823v2;2022-12-08;GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields;"We propose GazeNeRF, a 3D-aware method for the task of gaze redirection.
Existing gaze redirection methods operate on 2D images and struggle to generate
3D consistent results. Instead, we build on the intuition that the face region
and eyeballs are separate 3D structures that move in a coordinated yet
independent fashion. Our method leverages recent advancements in conditional
image-based neural radiance fields and proposes a two-stream architecture that
predicts volumetric features for the face and eye regions separately. Rigidly
transforming the eye features via a 3D rotation matrix provides fine-grained
control over the desired gaze angle. The final, redirected image is then
attained via differentiable volume compositing. Our experiments show that this
architecture outperforms naively conditioned NeRF baselines as well as previous
state-of-the-art 2D gaze redirection methods in terms of redirection accuracy
and identity preservation.";Alessandro Ruzzi<author:sep>Xiangwei Shi<author:sep>Xi Wang<author:sep>Gengyan Li<author:sep>Shalini De Mello<author:sep>Hyung Jin Chang<author:sep>Xucong Zhang<author:sep>Otmar Hilliges;http://arxiv.org/pdf/2212.04823v2;cs.CV;"Accepted at CVPR 2023. Github page:
  https://github.com/AlessandroRuzzi/GazeNeRF";nerf
2212.03848v2;http://arxiv.org/abs/2212.03848v2;2022-12-07;NeRFEditor: Differentiable Style Decomposition for Full 3D Scene Editing;"We present NeRFEditor, an efficient learning framework for 3D scene editing,
which takes a video captured over 360{\deg} as input and outputs a
high-quality, identity-preserving stylized 3D scene. Our method supports
diverse types of editing such as guided by reference images, text prompts, and
user interactions. We achieve this by encouraging a pre-trained StyleGAN model
and a NeRF model to learn from each other mutually. Specifically, we use a NeRF
model to generate numerous image-angle pairs to train an adjustor, which can
adjust the StyleGAN latent code to generate high-fidelity stylized images for
any given angle. To extrapolate editing to GAN out-of-domain views, we devise
another module that is trained in a self-supervised learning manner. This
module maps novel-view images to the hidden space of StyleGAN that allows
StyleGAN to generate stylized images on novel views. These two modules together
produce guided images in 360{\deg}views to finetune a NeRF to make stylization
effects, where a stable fine-tuning strategy is proposed to achieve this.
Experiments show that NeRFEditor outperforms prior work on benchmark and
real-world scenes with better editability, fidelity, and identity preservation.";Chunyi Sun<author:sep>Yanbin Liu<author:sep>Junlin Han<author:sep>Stephen Gould;http://arxiv.org/pdf/2212.03848v2;cs.CV;Project page: https://chuny1.github.io/NeRFEditor/nerfeditor.html;nerf
2212.04247v2;http://arxiv.org/abs/2212.04247v2;2022-12-07;EditableNeRF: Editing Topologically Varying Neural Radiance Fields by  Key Points;"Neural radiance fields (NeRF) achieve highly photo-realistic novel-view
synthesis, but it's a challenging problem to edit the scenes modeled by
NeRF-based methods, especially for dynamic scenes. We propose editable neural
radiance fields that enable end-users to easily edit dynamic scenes and even
support topological changes. Input with an image sequence from a single camera,
our network is trained fully automatically and models topologically varying
dynamics using our picked-out surface key points. Then end-users can edit the
scene by easily dragging the key points to desired new positions. To achieve
this, we propose a scene analysis method to detect and initialize key points by
considering the dynamics in the scene, and a weighted key points strategy to
model topologically varying dynamics by joint key points and weights
optimization. Our method supports intuitive multi-dimensional (up to 3D)
editing and can generate novel scenes that are unseen in the input sequence.
Experiments demonstrate that our method achieves high-quality editing on
various dynamic scenes and outperforms the state-of-the-art. Our code and
captured data are available at https://chengwei-zheng.github.io/EditableNeRF/.";Chengwei Zheng<author:sep>Wenbin Lin<author:sep>Feng Xu;http://arxiv.org/pdf/2212.04247v2;cs.CV;Accepted by CVPR 2023;nerf
2212.03635v1;http://arxiv.org/abs/2212.03635v1;2022-12-07;Non-uniform Sampling Strategies for NeRF on 360{\textdegree} images;"In recent years, the performance of novel view synthesis using perspective
images has dramatically improved with the advent of neural radiance fields
(NeRF). This study proposes two novel techniques that effectively build NeRF
for 360{\textdegree} omnidirectional images. Due to the characteristics of a
360{\textdegree} image of ERP format that has spatial distortion in their high
latitude regions and a 360{\textdegree} wide viewing angle, NeRF's general ray
sampling strategy is ineffective. Hence, the view synthesis accuracy of NeRF is
limited and learning is not efficient. We propose two non-uniform ray sampling
schemes for NeRF to suit 360{\textdegree} images - distortion-aware ray
sampling and content-aware ray sampling. We created an evaluation dataset
Synth360 using Replica and SceneCity models of indoor and outdoor scenes,
respectively. In experiments, we show that our proposal successfully builds
360{\textdegree} image NeRF in terms of both accuracy and efficiency. The
proposal is widely applicable to advanced variants of NeRF. DietNeRF, AugNeRF,
and NeRF++ combined with the proposed techniques further improve the
performance. Moreover, we show that our proposed method enhances the quality of
real-world scenes in 360{\textdegree} images. Synth360:
https://drive.google.com/drive/folders/1suL9B7DO2no21ggiIHkH3JF3OecasQLb.";Takashi Otonari<author:sep>Satoshi Ikehata<author:sep>Kiyoharu Aizawa;http://arxiv.org/pdf/2212.03635v1;cs.CV;Accepted at the 33rd British Machine Vision Conference (BMVC) 2022;nerf
2212.03406v1;http://arxiv.org/abs/2212.03406v1;2022-12-07;SSDNeRF: Semantic Soft Decomposition of Neural Radiance Fields;"Neural Radiance Fields (NeRFs) encode the radiance in a scene parameterized
by the scene's plenoptic function. This is achieved by using an MLP together
with a mapping to a higher-dimensional space, and has been proven to capture
scenes with a great level of detail. Naturally, the same parameterization can
be used to encode additional properties of the scene, beyond just its radiance.
A particularly interesting property in this regard is the semantic
decomposition of the scene. We introduce a novel technique for semantic soft
decomposition of neural radiance fields (named SSDNeRF) which jointly encodes
semantic signals in combination with radiance signals of a scene. Our approach
provides a soft decomposition of the scene into semantic parts, enabling us to
correctly encode multiple semantic classes blending along the same direction --
an impossible feat for existing methods. Not only does this lead to a detailed,
3D semantic representation of the scene, but we also show that the regularizing
effects of the MLP used for encoding help to improve the semantic
representation. We show state-of-the-art segmentation and reconstruction
results on a dataset of common objects and demonstrate how the proposed
approach can be applied for high quality temporally consistent video editing
and re-compositing on a dataset of casually captured selfie videos.";Siddhant Ranade<author:sep>Christoph Lassner<author:sep>Kai Li<author:sep>Christian Haene<author:sep>Shen-Chi Chen<author:sep>Jean-Charles Bazin<author:sep>Sofien Bouaziz;http://arxiv.org/pdf/2212.03406v1;cs.CV;"Project page:
  https://www.siddhantranade.com/research/2022/12/06/SSDNeRF-Semantic-Soft-Decomposition-of-Neural-Radiance-Fields.html";nerf
2212.03267v1;http://arxiv.org/abs/2212.03267v1;2022-12-06;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as  General Image Priors;"2D-to-3D reconstruction is an ill-posed problem, yet humans are good at
solving this problem due to their prior knowledge of the 3D world developed
over years. Driven by this observation, we propose NeRDi, a single-view NeRF
synthesis framework with general image priors from 2D diffusion models.
Formulating single-view reconstruction as an image-conditioned 3D generation
problem, we optimize the NeRF representations by minimizing a diffusion loss on
its arbitrary view renderings with a pretrained image diffusion model under the
input-view constraint. We leverage off-the-shelf vision-language models and
introduce a two-section language guidance as conditioning inputs to the
diffusion model. This is essentially helpful for improving multiview content
coherence as it narrows down the general image prior conditioned on the
semantic and visual features of the single-view input image. Additionally, we
introduce a geometric loss based on estimated depth maps to regularize the
underlying 3D geometry of the NeRF. Experimental results on the DTU MVS dataset
show that our method can synthesize novel views with higher quality even
compared to existing methods trained on this dataset. We also demonstrate our
generalizability in zero-shot NeRF synthesis for in-the-wild images.";"Congyue Deng<author:sep>Chiyu ""Max'' Jiang<author:sep>Charles R. Qi<author:sep>Xinchen Yan<author:sep>Yin Zhou<author:sep>Leonidas Guibas<author:sep>Dragomir Anguelov";http://arxiv.org/pdf/2212.03267v1;cs.CV;;nerf
2212.02493v3;http://arxiv.org/abs/2212.02493v3;2022-12-05;Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural  Fields;"Coordinate-based implicit neural networks, or neural fields, have emerged as
useful representations of shape and appearance in 3D computer vision. Despite
advances, however, it remains challenging to build neural fields for categories
of objects without datasets like ShapeNet that provide ""canonicalized"" object
instances that are consistently aligned for their 3D position and orientation
(pose). We present Canonical Field Network (CaFi-Net), a self-supervised method
to canonicalize the 3D pose of instances from an object category represented as
neural fields, specifically neural radiance fields (NeRFs). CaFi-Net directly
learns from continuous and noisy radiance fields using a Siamese network
architecture that is designed to extract equivariant field features for
category-level canonicalization. During inference, our method takes pre-trained
neural radiance fields of novel object instances at arbitrary 3D pose and
estimates a canonical field with consistent 3D pose across the entire category.
Extensive experiments on a new dataset of 1300 NeRF models across 13 object
categories show that our method matches or exceeds the performance of 3D point
cloud-based methods.";Rohith Agaram<author:sep>Shaurya Dewan<author:sep>Rahul Sajnani<author:sep>Adrien Poulenard<author:sep>Madhava Krishna<author:sep>Srinath Sridhar;http://arxiv.org/pdf/2212.02493v3;cs.CV;;nerf
2212.01959v1;http://arxiv.org/abs/2212.01959v1;2022-12-05;INGeo: Accelerating Instant Neural Scene Reconstruction with Noisy  Geometry Priors;"We present a method that accelerates reconstruction of 3D scenes and objects,
aiming to enable instant reconstruction on edge devices such as mobile phones
and AR/VR headsets. While recent works have accelerated scene reconstruction
training to minute/second-level on high-end GPUs, there is still a large gap to
the goal of instant training on edge devices which is yet highly desired in
many emerging applications such as immersive AR/VR. To this end, this work aims
to further accelerate training by leveraging geometry priors of the target
scene. Our method proposes strategies to alleviate the noise of the imperfect
geometry priors to accelerate the training speed on top of the highly optimized
Instant-NGP. On the NeRF Synthetic dataset, our work uses half of the training
iterations to reach an average test PSNR of >30.";Chaojian Li<author:sep>Bichen Wu<author:sep>Albert Pumarola<author:sep>Peizhao Zhang<author:sep>Yingyan Lin<author:sep>Peter Vajda;http://arxiv.org/pdf/2212.01959v1;cs.CV;Accepted by Computer Vision for Metaverse Workshop @ ECCV'22;nerf
2212.02375v2;http://arxiv.org/abs/2212.02375v2;2022-12-05;D-TensoRF: Tensorial Radiance Fields for Dynamic Scenes;"Neural radiance field (NeRF) attracts attention as a promising approach to
reconstructing the 3D scene. As NeRF emerges, subsequent studies have been
conducted to model dynamic scenes, which include motions or topological
changes. However, most of them use an additional deformation network, slowing
down the training and rendering speed. Tensorial radiance field (TensoRF)
recently shows its potential for fast, high-quality reconstruction of static
scenes with compact model size. In this paper, we present D-TensoRF, a
tensorial radiance field for dynamic scenes, enabling novel view synthesis at a
specific time. We consider the radiance field of a dynamic scene as a 5D
tensor. The 5D tensor represents a 4D grid in which each axis corresponds to X,
Y, Z, and time and has 1D multi-channel features per element. Similar to
TensoRF, we decompose the grid either into rank-one vector components (CP
decomposition) or low-rank matrix components (newly proposed MM decomposition).
We also use smoothing regularization to reflect the relationship between
features at different times (temporal dependency). We conduct extensive
evaluations to analyze our models. We show that D-TensoRF with CP decomposition
and MM decomposition both have short training times and significantly low
memory footprints with quantitatively and qualitatively competitive rendering
results in comparison to the state-of-the-art methods in 3D dynamic scene
modeling.";Hankyu Jang<author:sep>Daeyoung Kim;http://arxiv.org/pdf/2212.02375v2;cs.CV;21 pages, 11 figures;nerf
2212.02469v4;http://arxiv.org/abs/2212.02469v4;2022-12-05;One-shot Implicit Animatable Avatars with Model-based Priors;"Existing neural rendering methods for creating human avatars typically either
require dense input signals such as video or multi-view images, or leverage a
learned prior from large-scale specific 3D human datasets such that
reconstruction can be performed with sparse-view inputs. Most of these methods
fail to achieve realistic reconstruction when only a single image is available.
To enable the data-efficient creation of realistic animatable 3D humans, we
propose ELICIT, a novel method for learning human-specific neural radiance
fields from a single image. Inspired by the fact that humans can effortlessly
estimate the body geometry and imagine full-body clothing from a single image,
we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior.
Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned
vertex-based template model (i.e., SMPL) and implements the visual clothing
semantic prior with the CLIP-based pretrained models. Both priors are used to
jointly guide the optimization for creating plausible content in the invisible
areas. Taking advantage of the CLIP models, ELICIT can use text descriptions to
generate text-conditioned unseen regions. In order to further improve visual
details, we propose a segmentation-based sampling strategy that locally refines
different parts of the avatar. Comprehensive evaluations on multiple popular
benchmarks, including ZJU-MoCAP, Human3.6M, and DeepFashion, show that ELICIT
has outperformed strong baseline methods of avatar creation when only a single
image is available. The code is public for research purposes at
https://huangyangyi.github.io/ELICIT/.";Yangyi Huang<author:sep>Hongwei Yi<author:sep>Weiyang Liu<author:sep>Haofan Wang<author:sep>Boxi Wu<author:sep>Wenxiao Wang<author:sep>Binbin Lin<author:sep>Debing Zhang<author:sep>Deng Cai;http://arxiv.org/pdf/2212.02469v4;cs.CV;"To appear at ICCV 2023. Project website:
  https://huangyangyi.github.io/ELICIT/";
2212.02280v2;http://arxiv.org/abs/2212.02280v2;2022-12-05;GARF:Geometry-Aware Generalized Neural Radiance Field;"Neural Radiance Field (NeRF) has revolutionized free viewpoint rendering
tasks and achieved impressive results. However, the efficiency and accuracy
problems hinder its wide applications. To address these issues, we propose
Geometry-Aware Generalized Neural Radiance Field (GARF) with a geometry-aware
dynamic sampling (GADS) strategy to perform real-time novel view rendering and
unsupervised depth estimation on unseen scenes without per-scene optimization.
Distinct from most existing generalized NeRFs, our framework infers the unseen
scenes on both pixel-scale and geometry-scale with only a few input images.
More specifically, our method learns common attributes of novel-view synthesis
by an encoder-decoder structure and a point-level learnable multi-view feature
fusion module which helps avoid occlusion. To preserve scene characteristics in
the generalized model, we introduce an unsupervised depth estimation module to
derive the coarse geometry, narrow down the ray sampling interval to proximity
space of the estimated surface and sample in expectation maximum position,
constituting Geometry-Aware Dynamic Sampling strategy (GADS). Moreover, we
introduce a Multi-level Semantic Consistency loss (MSC) to assist more
informative representation learning. Extensive experiments on indoor and
outdoor datasets show that comparing with state-of-the-art generalized NeRF
methods, GARF reduces samples by more than 25\%, while improving rendering
quality and 3D geometry estimation.";Yue Shi<author:sep>Dingyi Rong<author:sep>Bingbing Ni<author:sep>Chang Chen<author:sep>Wenjun Zhang;http://arxiv.org/pdf/2212.02280v2;cs.CV;;nerf
2212.02501v4;http://arxiv.org/abs/2212.02501v4;2022-12-05;SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance  Fields;"3D reconstruction from a single 2D image was extensively covered in the
literature but relies on depth supervision at training time, which limits its
applicability. To relax the dependence to depth we propose SceneRF, a
self-supervised monocular scene reconstruction method using only posed image
sequences for training. Fueled by the recent progress in neural radiance fields
(NeRF) we optimize a radiance field though with explicit depth optimization and
a novel probabilistic sampling strategy to efficiently handle large scenes. At
inference, a single input image suffices to hallucinate novel depth views which
are fused together to obtain 3D scene reconstruction. Thorough experiments
demonstrate that we outperform all baselines for novel depth views synthesis
and scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI.
Code is available at https://astra-vision.github.io/SceneRF .";Anh-Quan Cao<author:sep>Raoul de Charette;http://arxiv.org/pdf/2212.02501v4;cs.CV;ICCV 2023. Project page: https://astra-vision.github.io/SceneRF;nerf
2212.01735v4;http://arxiv.org/abs/2212.01735v4;2022-12-04;Neural Fourier Filter Bank;"We present a novel method to provide efficient and highly detailed
reconstructions. Inspired by wavelets, we learn a neural field that decompose
the signal both spatially and frequency-wise. We follow the recent grid-based
paradigm for spatial decomposition, but unlike existing work, encourage
specific frequencies to be stored in each grid via Fourier features encodings.
We then apply a multi-layer perceptron with sine activations, taking these
Fourier encoded features in at appropriate layers so that higher-frequency
components are accumulated on top of lower-frequency components sequentially,
which we sum up to form the final output. We demonstrate that our method
outperforms the state of the art regarding model compactness and convergence
speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural
radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.";Zhijie Wu<author:sep>Yuhe Jin<author:sep>Kwang Moo Yi;http://arxiv.org/pdf/2212.01735v4;cs.CV;;
2212.01672v1;http://arxiv.org/abs/2212.01672v1;2022-12-03;MaRF: Representing Mars as Neural Radiance Fields;"The aim of this work is to introduce MaRF, a novel framework able to
synthesize the Martian environment using several collections of images from
rover cameras. The idea is to generate a 3D scene of Mars' surface to address
key challenges in planetary surface exploration such as: planetary geology,
simulated navigation and shape analysis. Although there exist different methods
to enable a 3D reconstruction of Mars' surface, they rely on classical computer
graphics techniques that incur high amounts of computational resources during
the reconstruction process, and have limitations with generalizing
reconstructions to unseen scenes and adapting to new images coming from rover
cameras. The proposed framework solves the aforementioned limitations by
exploiting Neural Radiance Fields (NeRFs), a method that synthesize complex
scenes by optimizing a continuous volumetric scene function using a sparse set
of images. To speed up the learning process, we replaced the sparse set of
rover images with their neural graphics primitives (NGPs), a set of vectors of
fixed length that are learned to preserve the information of the original
images in a significantly smaller size. In the experimental section, we
demonstrate the environments created from actual Mars datasets captured by
Curiosity rover, Perseverance rover and Ingenuity helicopter, all of which are
available on the Planetary Data System (PDS).";Lorenzo Giusti<author:sep>Josue Garcia<author:sep>Steven Cozine<author:sep>Darrick Suen<author:sep>Christina Nguyen<author:sep>Ryan Alimo;http://arxiv.org/pdf/2212.01672v1;cs.CV;ECCV 2022 (oral);nerf
2212.01602v1;http://arxiv.org/abs/2212.01602v1;2022-12-03;StegaNeRF: Embedding Invisible Information within Neural Radiance Fields;"Recent advances in neural rendering imply a future of widespread visual data
distributions through sharing NeRF model weights. However, while common visual
data (images and videos) have standard approaches to embed ownership or
copyright information explicitly or subtly, the problem remains unexplored for
the emerging NeRF format. We present StegaNeRF, a method for steganographic
information embedding in NeRF renderings. We design an optimization framework
allowing accurate hidden information extractions from images rendered by NeRF,
while preserving its original visual quality. We perform experimental
evaluations of our method under several potential deployment scenarios, and we
further discuss the insights discovered through our analysis. StegaNeRF
signifies an initial exploration into the novel problem of instilling
customizable, imperceptible, and recoverable information to NeRF renderings,
with minimal impact to rendered images. Project page:
https://xggnet.github.io/StegaNeRF/.";Chenxin Li<author:sep>Brandon Y. Feng<author:sep>Zhiwen Fan<author:sep>Panwang Pan<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2212.01602v1;cs.CV;Project page: https://xggnet.github.io/StegaNeRF/;nerf
2212.01331v4;http://arxiv.org/abs/2212.01331v4;2022-12-02;Surface Normal Clustering for Implicit Representation of Manhattan  Scenes;"Novel view synthesis and 3D modeling using implicit neural field
representation are shown to be very effective for calibrated multi-view
cameras. Such representations are known to benefit from additional geometric
and semantic supervision. Most existing methods that exploit additional
supervision require dense pixel-wise labels or localized scene priors. These
methods cannot benefit from high-level vague scene priors provided in terms of
scenes' descriptions. In this work, we aim to leverage the geometric prior of
Manhattan scenes to improve the implicit neural radiance field representations.
More precisely, we assume that only the knowledge of the indoor scene (under
investigation) being Manhattan is known -- with no additional information
whatsoever -- with an unknown Manhattan coordinate frame. Such high-level prior
is used to self-supervise the surface normals derived explicitly in the
implicit neural fields. Our modeling allows us to cluster the derived normals
and exploit their orthogonality constraints for self-supervision. Our
exhaustive experiments on datasets of diverse indoor scenes demonstrate the
significant benefit of the proposed method over the established baselines. The
source code is available at
https://github.com/nikola3794/normal-clustering-nerf.";Nikola Popovic<author:sep>Danda Pani Paudel<author:sep>Luc Van Gool;http://arxiv.org/pdf/2212.01331v4;cs.CV;Paper accepted to ICCV23;nerf
2212.01368v2;http://arxiv.org/abs/2212.01368v2;2022-12-02;Fast Non-Rigid Radiance Fields from Monocularized Data;"The reconstruction and novel view synthesis of dynamic scenes recently gained
increased attention. As reconstruction from large-scale multi-view data
involves immense memory and computational requirements, recent benchmark
datasets provide collections of single monocular views per timestamp sampled
from multiple (virtual) cameras. We refer to this form of inputs as
""monocularized"" data. Existing work shows impressive results for synthetic
setups and forward-facing real-world data, but is often limited in the training
speed and angular range for generating novel views. This paper addresses these
limitations and proposes a new method for full 360{\deg} inward-facing novel
view synthesis of non-rigidly deforming scenes. At the core of our method are:
1) An efficient deformation module that decouples the processing of spatial and
temporal information for accelerated training and inference; and 2) A static
module representing the canonical scene as a fast hash-encoded neural radiance
field. In addition to existing synthetic monocularized data, we systematically
analyze the performance on real-world inward-facing scenes using a newly
recorded challenging dataset sampled from a synchronized large-scale multi-view
rig. In both cases, our method is significantly faster than previous methods,
converging in less than 7 minutes and achieving real-time framerates at 1K
resolution, while obtaining a higher visual accuracy for generated novel views.
Our source code and data is available at our project page
https://graphics.tu-bs.de/publications/kappel2022fast.";Moritz Kappel<author:sep>Vladislav Golyanik<author:sep>Susana Castillo<author:sep>Christian Theobalt<author:sep>Marcus Magnor;http://arxiv.org/pdf/2212.01368v2;cs.CV;"18 pages, 14 figures; project page:
  https://graphics.tu-bs.de/publications/kappel2022fast";
2212.01120v1;http://arxiv.org/abs/2212.01120v1;2022-12-02;RT-NeRF: Real-Time On-Device Neural Radiance Fields Towards Immersive  AR/VR Rendering;"Neural Radiance Field (NeRF) based rendering has attracted growing attention
thanks to its state-of-the-art (SOTA) rendering quality and wide applications
in Augmented and Virtual Reality (AR/VR). However, immersive real-time (> 30
FPS) NeRF based rendering enabled interactions are still limited due to the low
achievable throughput on AR/VR devices. To this end, we first profile SOTA
efficient NeRF algorithms on commercial devices and identify two primary causes
of the aforementioned inefficiency: (1) the uniform point sampling and (2) the
dense accesses and computations of the required embeddings in NeRF.
Furthermore, we propose RT-NeRF, which to the best of our knowledge is the
first algorithm-hardware co-design acceleration of NeRF. Specifically, on the
algorithm level, RT-NeRF integrates an efficient rendering pipeline for largely
alleviating the inefficiency due to the commonly adopted uniform point sampling
method in NeRF by directly computing the geometry of pre-existing points.
Additionally, RT-NeRF leverages a coarse-grained view-dependent computing
ordering scheme for eliminating the (unnecessary) processing of invisible
points. On the hardware level, our proposed RT-NeRF accelerator (1) adopts a
hybrid encoding scheme to adaptively switch between a bitmap- or
coordinate-based sparsity encoding format for NeRF's sparse embeddings, aiming
to maximize the storage savings and thus reduce the required DRAM accesses
while supporting efficient NeRF decoding; and (2) integrates both a
dual-purpose bi-direction adder & search tree and a high-density sparse search
unit to coordinate the two aforementioned encoding formats. Extensive
experiments on eight datasets consistently validate the effectiveness of
RT-NeRF, achieving a large throughput improvement (e.g., 9.7x - 3,201x) while
maintaining the rendering quality as compared with SOTA efficient NeRF
solutions.";Chaojian Li<author:sep>Sixu Li<author:sep>Yang Zhao<author:sep>Wenbo Zhu<author:sep>Yingyan Lin;http://arxiv.org/pdf/2212.01120v1;cs.AR;Accepted to ICCAD 2022;nerf
2212.01103v2;http://arxiv.org/abs/2212.01103v2;2022-12-02;3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation;"Text-guided 3D object generation aims to generate 3D objects described by
user-defined captions, which paves a flexible way to visualize what we
imagined. Although some works have been devoted to solving this challenging
task, these works either utilize some explicit 3D representations (e.g., mesh),
which lack texture and require post-processing for rendering photo-realistic
views; or require individual time-consuming optimization for every single case.
Here, we make the first attempt to achieve generic text-guided cross-category
3D object generation via a new 3D-TOGO model, which integrates a text-to-views
generation module and a views-to-3D generation module. The text-to-views
generation module is designed to generate different views of the target 3D
object given an input caption. prior-guidance, caption-guidance and view
contrastive learning are proposed for achieving better view-consistency and
caption similarity. Meanwhile, a pixelNeRF model is adopted for the views-to-3D
generation module to obtain the implicit 3D neural representation from the
previously-generated views. Our 3D-TOGO model generates 3D objects in the form
of the neural radiance field with good texture and requires no time-cost
optimization for every single caption. Besides, 3D-TOGO can control the
category, color and shape of generated 3D objects with the input caption.
Extensive experiments on the largest 3D object dataset (i.e., ABO) are
conducted to verify that 3D-TOGO can better generate high-quality 3D objects
according to the input captions across 98 different categories, in terms of
PSNR, SSIM, LPIPS and CLIP-score, compared with text-NeRF and Dreamfields.";Zutao Jiang<author:sep>Guansong Lu<author:sep>Xiaodan Liang<author:sep>Jihua Zhu<author:sep>Wei Zhang<author:sep>Xiaojun Chang<author:sep>Hang Xu;http://arxiv.org/pdf/2212.01103v2;cs.CV;;nerf
2212.00914v1;http://arxiv.org/abs/2212.00914v1;2022-12-02;QFF: Quantized Fourier Features for Neural Field Representations;"Multilayer perceptrons (MLPs) learn high frequencies slowly. Recent
approaches encode features in spatial bins to improve speed of learning
details, but at the cost of larger model size and loss of continuity. Instead,
we propose to encode features in bins of Fourier features that are commonly
used for positional encoding. We call these Quantized Fourier Features (QFF).
As a naturally multiresolution and periodic representation, our experiments
show that using QFF can result in smaller model size, faster training, and
better quality outputs for several applications, including Neural Image
Representations (NIR), Neural Radiance Field (NeRF) and Signed Distance
Function (SDF) modeling. QFF are easy to code, fast to compute, and serve as a
simple drop-in addition to many neural field representations.";Jae Yong Lee<author:sep>Yuqun Wu<author:sep>Chuhang Zou<author:sep>Shenlong Wang<author:sep>Derek Hoiem;http://arxiv.org/pdf/2212.00914v1;cs.CV;;nerf
2212.00436v1;http://arxiv.org/abs/2212.00436v1;2022-12-01;ViewNeRF: Unsupervised Viewpoint Estimation Using Category-Level Neural  Radiance Fields;"We introduce ViewNeRF, a Neural Radiance Field-based viewpoint estimation
method that learns to predict category-level viewpoints directly from images
during training. While NeRF is usually trained with ground-truth camera poses,
multiple extensions have been proposed to reduce the need for this expensive
supervision. Nonetheless, most of these methods still struggle in complex
settings with large camera movements, and are restricted to single scenes, i.e.
they cannot be trained on a collection of scenes depicting the same object
category. To address these issues, our method uses an analysis by synthesis
approach, combining a conditional NeRF with a viewpoint predictor and a scene
encoder in order to produce self-supervised reconstructions for whole object
categories. Rather than focusing on high fidelity reconstruction, we target
efficient and accurate viewpoint prediction in complex scenarios, e.g.
360{\deg} rotation on real data. Our model shows competitive results on
synthetic and real datasets, both for single scenes and multi-instance
collections.";Octave Mariotti<author:sep>Oisin Mac Aodha<author:sep>Hakan Bilen;http://arxiv.org/pdf/2212.00436v1;cs.CV;;nerf
2212.00190v2;http://arxiv.org/abs/2212.00190v2;2022-12-01;Mixed Neural Voxels for Fast Multi-view Video Synthesis;"Synthesizing high-fidelity videos from real-world multi-view input is
challenging because of the complexities of real-world environments and highly
dynamic motions. Previous works based on neural radiance fields have
demonstrated high-quality reconstructions of dynamic scenes. However, training
such models on real-world scenes is time-consuming, usually taking days or
weeks. In this paper, we present a novel method named MixVoxels to better
represent the dynamic scenes with fast training speed and competitive rendering
qualities. The proposed MixVoxels represents the 4D dynamic scenes as a mixture
of static and dynamic voxels and processes them with different networks. In
this way, the computation of the required modalities for static voxels can be
processed by a lightweight model, which essentially reduces the amount of
computation, especially for many daily dynamic scenes dominated by the static
background. To separate the two kinds of voxels, we propose a novel variation
field to estimate the temporal variance of each voxel. For the dynamic voxels,
we design an inner-product time query method to efficiently query multiple time
steps, which is essential to recover the high-dynamic motions. As a result,
with 15 minutes of training for dynamic scenes with inputs of 300-frame videos,
MixVoxels achieves better PSNR than previous methods. Codes and trained models
are available at https://github.com/fengres/mixvoxels";Feng Wang<author:sep>Sinan Tan<author:sep>Xinghang Li<author:sep>Zeyue Tian<author:sep>Yafei Song<author:sep>Huaping Liu;http://arxiv.org/pdf/2212.00190v2;cs.CV;ICCV 2023 (Oral);
2211.17235v1;http://arxiv.org/abs/2211.17235v1;2022-11-30;NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real  Image Animation;"Nerf-based Generative models have shown impressive capacity in generating
high-quality images with consistent 3D geometry. Despite successful synthesis
of fake identity images randomly sampled from latent space, adopting these
models for generating face images of real subjects is still a challenging task
due to its so-called inversion issue. In this paper, we propose a universal
method to surgically fine-tune these NeRF-GAN models in order to achieve
high-fidelity animation of real subjects only by a single image. Given the
optimized latent code for an out-of-domain real image, we employ 2D loss
functions on the rendered image to reduce the identity gap. Furthermore, our
method leverages explicit and implicit 3D regularizations using the in-domain
neighborhood samples around the optimized latent code to remove geometrical and
visual artifacts. Our experiments confirm the effectiveness of our method in
realistic, high-fidelity, and 3D consistent animation of real faces on multiple
NeRF-GAN models across different datasets.";Yu Yin<author:sep>Kamran Ghasedi<author:sep>HsiangTao Wu<author:sep>Jiaolong Yang<author:sep>Xin Tong<author:sep>Yun Fu;http://arxiv.org/pdf/2211.17235v1;cs.CV;;nerf
2211.16630v2;http://arxiv.org/abs/2211.16630v2;2022-11-29;DINER: Depth-aware Image-based NEural Radiance fields;"We present Depth-aware Image-based NEural Radiance fields (DINER). Given a
sparse set of RGB input views, we predict depth and feature maps to guide the
reconstruction of a volumetric scene representation that allows us to render 3D
objects under novel views. Specifically, we propose novel techniques to
incorporate depth information into feature fusion and efficient scene sampling.
In comparison to the previous state of the art, DINER achieves higher synthesis
quality and can process input views with greater disparity. This allows us to
capture scenes more completely without changing capturing hardware requirements
and ultimately enables larger viewpoint changes during novel view synthesis. We
evaluate our method by synthesizing novel views, both for human heads and for
general objects, and observe significantly improved qualitative results and
increased perceptual metrics compared to the previous state of the art. The
code is publicly available for research purposes.";Malte Prinzler<author:sep>Otmar Hilliges<author:sep>Justus Thies;http://arxiv.org/pdf/2211.16630v2;cs.CV;"Website: https://malteprinzler.github.io/projects/diner/diner.html ;
  Video: https://www.youtube.com/watch?v=iI_fpjY5k8Y&t=1s";
2211.16386v1;http://arxiv.org/abs/2211.16386v1;2022-11-29;Compressing Volumetric Radiance Fields to 1 MB;"Approximating radiance fields with volumetric grids is one of promising
directions for improving NeRF, represented by methods like Plenoxels and DVGO,
which achieve super-fast training convergence and real-time rendering. However,
these methods typically require a tremendous storage overhead, costing up to
hundreds of megabytes of disk space and runtime memory for a single scene. We
address this issue in this paper by introducing a simple yet effective
framework, called vector quantized radiance fields (VQRF), for compressing
these volume-grid-based radiance fields. We first present a robust and adaptive
metric for estimating redundancy in grid models and performing voxel pruning by
better exploring intermediate outputs of volumetric rendering. A trainable
vector quantization is further proposed to improve the compactness of grid
models. In combination with an efficient joint tuning strategy and
post-processing, our method can achieve a compression ratio of 100$\times$ by
reducing the overall model size to 1 MB with negligible loss on visual quality.
Extensive experiments demonstrate that the proposed framework is capable of
achieving unrivaled performance and well generalization across multiple methods
with distinct volumetric structures, facilitating the wide use of volumetric
radiance fields methods in real-world applications. Code Available at
\url{https://github.com/AlgoHunt/VQRF}";Lingzhi Li<author:sep>Zhen Shen<author:sep>Zhongshu Wang<author:sep>Li Shen<author:sep>Liefeng Bo;http://arxiv.org/pdf/2211.16386v1;cs.CV;;nerf
2211.16431v2;http://arxiv.org/abs/2211.16431v2;2022-11-29;NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with  360° Views;"Virtual reality and augmented reality (XR) bring increasing demand for 3D
content. However, creating high-quality 3D content requires tedious work that a
human expert must do. In this work, we study the challenging task of lifting a
single image to a 3D object and, for the first time, demonstrate the ability to
generate a plausible 3D object with 360{\deg} views that correspond well with
the given reference image. By conditioning on the reference image, our model
can fulfill the everlasting curiosity for synthesizing novel views of objects
from images. Our technique sheds light on a promising direction of easing the
workflows for 3D artists and XR designers. We propose a novel framework, dubbed
NeuralLift-360, that utilizes a depth-aware neural radiance representation
(NeRF) and learns to craft the scene guided by denoising diffusion models. By
introducing a ranking loss, our NeuralLift-360 can be guided with rough depth
estimation in the wild. We also adopt a CLIP-guided sampling strategy for the
diffusion prior to provide coherent guidance. Extensive experiments demonstrate
that our NeuralLift-360 significantly outperforms existing state-of-the-art
baselines. Project page: https://vita-group.github.io/NeuralLift-360/";Dejia Xu<author:sep>Yifan Jiang<author:sep>Peihao Wang<author:sep>Zhiwen Fan<author:sep>Yi Wang<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2211.16431v2;cs.CV;Project page: https://vita-group.github.io/NeuralLift-360/;nerf
2211.15977v3;http://arxiv.org/abs/2211.15977v3;2022-11-29;One is All: Bridging the Gap Between Neural Radiance Fields  Architectures with Progressive Volume Distillation;"Neural Radiance Fields (NeRF) methods have proved effective as compact,
high-quality and versatile representations for 3D scenes, and enable downstream
tasks such as editing, retrieval, navigation, etc. Various neural architectures
are vying for the core structure of NeRF, including the plain Multi-Layer
Perceptron (MLP), sparse tensors, low-rank tensors, hashtables and their
compositions. Each of these representations has its particular set of
trade-offs. For example, the hashtable-based representations admit faster
training and rendering but their lack of clear geometric meaning hampers
downstream tasks like spatial-relation-aware editing. In this paper, we propose
Progressive Volume Distillation (PVD), a systematic distillation method that
allows any-to-any conversions between different architectures, including MLP,
sparse or low-rank tensors, hashtables and their compositions. PVD consequently
empowers downstream applications to optimally adapt the neural representations
for the task at hand in a post hoc fashion. The conversions are fast, as
distillation is progressively performed on different levels of volume
representations, from shallower to deeper. We also employ special treatment of
density to deal with its specific numerical instability problem. Empirical
evidence is presented to validate our method on the NeRF-Synthetic, LLFF and
TanksAndTemples datasets. For example, with PVD, an MLP-based NeRF model can be
distilled from a hashtable-based Instant-NGP model at a 10X~20X faster speed
than being trained the original NeRF from scratch, while achieving a superior
level of synthesis quality. Code is available at
https://github.com/megvii-research/AAAI2023-PVD.";Shuangkang Fang<author:sep>Weixin Xu<author:sep>Heng Wang<author:sep>Yi Yang<author:sep>Yufeng Wang<author:sep>Shuchang Zhou;http://arxiv.org/pdf/2211.15977v3;cs.CV;Accepted by AAAI2023. Project Page: https://sk-fun.fun/PVD;nerf
2211.16193v2;http://arxiv.org/abs/2211.16193v2;2022-11-28;In-Hand 3D Object Scanning from an RGB Sequence;"We propose a method for in-hand 3D scanning of an unknown object with a
monocular camera. Our method relies on a neural implicit surface representation
that captures both the geometry and the appearance of the object, however, by
contrast with most NeRF-based methods, we do not assume that the camera-object
relative poses are known. Instead, we simultaneously optimize both the object
shape and the pose trajectory. As direct optimization over all shape and pose
parameters is prone to fail without coarse-level initialization, we propose an
incremental approach that starts by splitting the sequence into carefully
selected overlapping segments within which the optimization is likely to
succeed. We reconstruct the object shape and track its poses independently
within each segment, then merge all the segments before performing a global
optimization. We show that our method is able to reconstruct the shape and
color of both textured and challenging texture-less objects, outperforms
classical methods that rely only on appearance features, and that its
performance is close to recent methods that assume known camera poses.";Shreyas Hampali<author:sep>Tomas Hodan<author:sep>Luan Tran<author:sep>Lingni Ma<author:sep>Cem Keskin<author:sep>Vincent Lepetit;http://arxiv.org/pdf/2211.16193v2;cs.CV;CVPR 2023;nerf
2211.15064v2;http://arxiv.org/abs/2211.15064v2;2022-11-28;High-fidelity Facial Avatar Reconstruction from Monocular Video with  Generative Priors;"High-fidelity facial avatar reconstruction from a monocular video is a
significant research problem in computer graphics and computer vision.
Recently, Neural Radiance Field (NeRF) has shown impressive novel view
rendering results and has been considered for facial avatar reconstruction.
However, the complex facial dynamics and missing 3D information in monocular
videos raise significant challenges for faithful facial reconstruction. In this
work, we propose a new method for NeRF-based facial avatar reconstruction that
utilizes 3D-aware generative prior. Different from existing works that depend
on a conditional deformation field for dynamic modeling, we propose to learn a
personalized generative prior, which is formulated as a local and low
dimensional subspace in the latent space of 3D-GAN. We propose an efficient
method to construct the personalized generative prior based on a small set of
facial images of a given individual. After learning, it allows for
photo-realistic rendering with novel views and the face reenactment can be
realized by performing navigation in the latent space. Our proposed method is
applicable for different driven signals, including RGB images, 3DMM
coefficients, and audios. Compared with existing works, we obtain superior
novel view synthesis results and faithfully face reenactment performance.";Yunpeng Bai<author:sep>Yanbo Fan<author:sep>Xuan Wang<author:sep>Yong Zhang<author:sep>Jingxiang Sun<author:sep>Chun Yuan<author:sep>Ying Shan;http://arxiv.org/pdf/2211.15064v2;cs.CV;8 pages, 7 figures;nerf
2211.14879v1;http://arxiv.org/abs/2211.14879v1;2022-11-27;SuNeRF: Validation of a 3D Global Reconstruction of the Solar Corona  Using Simulated EUV Images;"Extreme Ultraviolet (EUV) light emitted by the Sun impacts satellite
operations and communications and affects the habitability of planets.
Currently, EUV-observing instruments are constrained to viewing the Sun from
its equator (i.e., ecliptic), limiting our ability to forecast EUV emission for
other viewpoints (e.g. solar poles), and to generalize our knowledge of the
Sun-Earth system to other host stars. In this work, we adapt Neural Radiance
Fields (NeRFs) to the physical properties of the Sun and demonstrate that
non-ecliptic viewpoints could be reconstructed from observations limited to the
solar ecliptic. To validate our approach, we train on simulations of solar EUV
emission that provide a ground truth for all viewpoints. Our model accurately
reconstructs the simulated 3D structure of the Sun, achieving a peak
signal-to-noise ratio of 43.3 dB and a mean absolute relative error of 0.3\%
for non-ecliptic viewpoints. Our method provides a consistent 3D reconstruction
of the Sun from a limited number of viewpoints, thus highlighting the potential
to create a virtual instrument for satellite observations of the Sun. Its
extension to real observations will provide the missing link to compare the Sun
to other stars and to improve space-weather forecasting.";Kyriaki-Margarita Bintsi<author:sep>Robert Jarolim<author:sep>Benoit Tremblay<author:sep>Miraflor Santos<author:sep>Anna Jungbluth<author:sep>James Paul Mason<author:sep>Sairam Sundaresan<author:sep>Angelos Vourlidas<author:sep>Cooper Downs<author:sep>Ronald M. Caplan<author:sep>Andrés Muñoz Jaramillo;http://arxiv.org/pdf/2211.14879v1;astro-ph.SR;"Accepted at Machine Learning and the Physical Sciences workshop,
  NeurIPS 2022";nerf
2211.14799v1;http://arxiv.org/abs/2211.14799v1;2022-11-27;Sampling Neural Radiance Fields for Refractive Objects;"Recently, differentiable volume rendering in neural radiance fields (NeRF)
has gained a lot of popularity, and its variants have attained many impressive
results. However, existing methods usually assume the scene is a homogeneous
volume so that a ray is cast along the straight path. In this work, the scene
is instead a heterogeneous volume with a piecewise-constant refractive index,
where the path will be curved if it intersects the different refractive
indices. For novel view synthesis of refractive objects, our NeRF-based
framework aims to optimize the radiance fields of bounded volume and boundary
from multi-view posed images with refractive object silhouettes. To tackle this
challenging problem, the refractive index of a scene is reconstructed from
silhouettes. Given the refractive index, we extend the stratified and
hierarchical sampling techniques in NeRF to allow drawing samples along a
curved path tracked by the Eikonal equation. The results indicate that our
framework outperforms the state-of-the-art method both quantitatively and
qualitatively, demonstrating better performance on the perceptual similarity
metric and an apparent improvement in the rendering quality on several
synthetic and real scenes.";Jen-I Pan<author:sep>Jheng-Wei Su<author:sep>Kai-Wen Hsiao<author:sep>Ting-Yu Yen<author:sep>Hung-Kuo Chu;http://arxiv.org/pdf/2211.14799v1;cs.CV;"SIGGRAPH Asia 2022 Technical Communications. 4 pages, 4 figures, 1
  table. Project: https://alexkeroro86.github.io/SampleNeRFRO/ Code:
  https://github.com/alexkeroro86/SampleNeRFRO";nerf
2211.14823v2;http://arxiv.org/abs/2211.14823v2;2022-11-27;3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer  Avenue;"This paper studies how to flexibly integrate reconstructed 3D models into
practical 3D modeling pipelines such as 3D scene creation and rendering. Due to
the technical difficulty, one can only obtain rough 3D models (R3DMs) for most
real objects using existing 3D reconstruction techniques. As a result,
physically-based rendering (PBR) would render low-quality images or videos for
scenes that are constructed by R3DMs. One promising solution would be
representing real-world objects as Neural Fields such as NeRFs, which are able
to generate photo-realistic renderings of an object under desired viewpoints.
However, a drawback is that the synthesized views through Neural Fields
Rendering (NFR) cannot reflect the simulated lighting details on R3DMs in PBR
pipelines, especially when object interactions in the 3D scene creation cause
local shadows. To solve this dilemma, we propose a lighting transfer network
(LighTNet) to bridge NFR and PBR, such that they can benefit from each other.
LighTNet reasons about a simplified image composition model, remedies the
uneven surface issue caused by R3DMs, and is empowered by several
perceptual-motivated constraints and a new Lab angle loss which enhances the
contrast between lighting strength and colors. Comparisons demonstrate that
LighTNet is superior in synthesizing impressive lighting, and is promising in
pushing NFR further in practical 3D modeling workflows. Project page:
https://3d-front-future.github.io/LighTNet .";Yujie Li<author:sep>Bowen Cai<author:sep>Yuqin Liang<author:sep>Rongfei Jia<author:sep>Binqiang Zhao<author:sep>Mingming Gong<author:sep>Huan Fu;http://arxiv.org/pdf/2211.14823v2;cs.CV;;nerf
2211.16211v3;http://arxiv.org/abs/2211.16211v3;2022-11-26;ResNeRF: Geometry-Guided Residual Neural Radiance Field for Indoor Scene  Novel View Synthesis;"We represent the ResNeRF, a novel geometry-guided two-stage framework for
indoor scene novel view synthesis. Be aware of that a good geometry would
greatly boost the performance of novel view synthesis, and to avoid the
geometry ambiguity issue, we propose to characterize the density distribution
of the scene based on a base density estimated from scene geometry and a
residual density parameterized by the geometry. In the first stage, we focus on
geometry reconstruction based on SDF representation, which would lead to a good
geometry surface of the scene and also a sharp density. In the second stage,
the residual density is learned based on the SDF learned in the first stage for
encoding more details about the appearance. In this way, our method can better
learn the density distribution with the geometry prior for high-fidelity novel
view synthesis while preserving the 3D structures. Experiments on large-scale
indoor scenes with many less-observed and textureless areas show that with the
good 3D surface, our method achieves state-of-the-art performance for novel
view synthesis.";Yuting Xiao<author:sep>Yiqun Zhao<author:sep>Yanyu Xu<author:sep>Shenghua Gao;http://arxiv.org/pdf/2211.16211v3;cs.CV;This is an incomplete paper;nerf
2211.14086v2;http://arxiv.org/abs/2211.14086v2;2022-11-25;ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision;"By supervising camera rays between a scene and multi-view image planes, NeRF
reconstructs a neural scene representation for the task of novel view
synthesis. On the other hand, shadow rays between the light source and the
scene have yet to be considered. Therefore, we propose a novel shadow ray
supervision scheme that optimizes both the samples along the ray and the ray
location. By supervising shadow rays, we successfully reconstruct a neural SDF
of the scene from single-view images under multiple lighting conditions. Given
single-view binary shadows, we train a neural network to reconstruct a complete
scene not limited by the camera's line of sight. By further modeling the
correlation between the image colors and the shadow rays, our technique can
also be effectively extended to RGB inputs. We compare our method with previous
works on challenging tasks of shape reconstruction from single-view binary
shadow or RGB images and observe significant improvements. The code and data
are available at https://github.com/gerwang/ShadowNeuS.";Jingwang Ling<author:sep>Zhibo Wang<author:sep>Feng Xu;http://arxiv.org/pdf/2211.14086v2;cs.CV;CVPR 2023. Project page: https://gerwang.github.io/shadowneus/;nerf
2211.13969v2;http://arxiv.org/abs/2211.13969v2;2022-11-25;Unsupervised Continual Semantic Adaptation through Neural Rendering;"An increasing amount of applications rely on data-driven models that are
deployed for perception tasks across a sequence of scenes. Due to the mismatch
between training and deployment data, adapting the model on the new scenes is
often crucial to obtain good performance. In this work, we study continual
multi-scene adaptation for the task of semantic segmentation, assuming that no
ground-truth labels are available during deployment and that performance on the
previous scenes should be maintained. We propose training a Semantic-NeRF
network for each scene by fusing the predictions of a segmentation model and
then using the view-consistent rendered semantic labels as pseudo-labels to
adapt the model. Through joint training with the segmentation model, the
Semantic-NeRF model effectively enables 2D-3D knowledge transfer. Furthermore,
due to its compact size, it can be stored in a long-term memory and
subsequently used to render data from arbitrary viewpoints to reduce
forgetting. We evaluate our approach on ScanNet, where we outperform both a
voxel-based baseline and a state-of-the-art unsupervised domain adaptation
method.";Zhizheng Liu<author:sep>Francesco Milano<author:sep>Jonas Frey<author:sep>Roland Siegwart<author:sep>Hermann Blum<author:sep>Cesar Cadena;http://arxiv.org/pdf/2211.13969v2;cs.CV;"Accepted by the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2023. Zhizheng Liu and Francesco Milano share first
  authorship. Hermann Blum and Cesar Cadena share senior authorship. 18 pages,
  8 figures, 9 tables";nerf
2211.14108v3;http://arxiv.org/abs/2211.14108v3;2022-11-25;3DDesigner: Towards Photorealistic 3D Object Generation and Editing with  Text-guided Diffusion Models;"Text-guided diffusion models have shown superior performance in image/video
generation and editing. While few explorations have been performed in 3D
scenarios. In this paper, we discuss three fundamental and interesting problems
on this topic. First, we equip text-guided diffusion models to achieve
3D-consistent generation. Specifically, we integrate a NeRF-like neural field
to generate low-resolution coarse results for a given camera view. Such results
can provide 3D priors as condition information for the following diffusion
process. During denoising diffusion, we further enhance the 3D consistency by
modeling cross-view correspondences with a novel two-stream (corresponding to
two different views) asynchronous diffusion process. Second, we study 3D local
editing and propose a two-step solution that can generate 360-degree
manipulated results by editing an object from a single view. Step 1, we propose
to perform 2D local editing by blending the predicted noises. Step 2, we
conduct a noise-to-text inversion process that maps 2D blended noises into the
view-independent text embedding space. Once the corresponding text embedding is
obtained, 360-degree images can be generated. Last but not least, we extend our
model to perform one-shot novel view synthesis by fine-tuning on a single
image, firstly showing the potential of leveraging text guidance for novel view
synthesis. Extensive experiments and various applications show the prowess of
our 3DDesigner. The project page is available at
https://3ddesigner-diffusion.github.io/.";Gang Li<author:sep>Heliang Zheng<author:sep>Chaoyue Wang<author:sep>Chang Li<author:sep>Changwen Zheng<author:sep>Dacheng Tao;http://arxiv.org/pdf/2211.14108v3;cs.CV;Submitted to IJCV;nerf
2211.13887v1;http://arxiv.org/abs/2211.13887v1;2022-11-25;TPA-Net: Generate A Dataset for Text to Physics-based Animation;"Recent breakthroughs in Vision-Language (V&L) joint research have achieved
remarkable results in various text-driven tasks. High-quality Text-to-video
(T2V), a task that has been long considered mission-impossible, was proven
feasible with reasonably good results in latest works. However, the resulting
videos often have undesired artifacts largely because the system is purely
data-driven and agnostic to the physical laws. To tackle this issue and further
push T2V towards high-level physical realism, we present an autonomous data
generation technique and a dataset, which intend to narrow the gap with a large
number of multi-modal, 3D Text-to-Video/Simulation (T2V/S) data. In the
dataset, we provide high-resolution 3D physical simulations for both solids and
fluids, along with textual descriptions of the physical phenomena. We take
advantage of state-of-the-art physical simulation methods (i) Incremental
Potential Contact (IPC) and (ii) Material Point Method (MPM) to simulate
diverse scenarios, including elastic deformations, material fractures,
collisions, turbulence, etc. Additionally, high-quality, multi-view rendering
videos are supplied for the benefit of T2V, Neural Radiance Fields (NeRF), and
other communities. This work is the first step towards fully automated
Text-to-Video/Simulation (T2V/S). Live examples and subsequent work are at
https://sites.google.com/view/tpa-net.";Yuxing Qiu<author:sep>Feng Gao<author:sep>Minchen Li<author:sep>Govind Thattai<author:sep>Yin Yang<author:sep>Chenfanfu Jiang;http://arxiv.org/pdf/2211.13887v1;cs.AI;;nerf
2211.13994v1;http://arxiv.org/abs/2211.13994v1;2022-11-25;Dynamic Neural Portraits;"We present Dynamic Neural Portraits, a novel approach to the problem of
full-head reenactment. Our method generates photo-realistic video portraits by
explicitly controlling head pose, facial expressions and eye gaze. Our proposed
architecture is different from existing methods that rely on GAN-based
image-to-image translation networks for transforming renderings of 3D faces
into photo-realistic images. Instead, we build our system upon a 2D
coordinate-based MLP with controllable dynamics. Our intuition to adopt a
2D-based representation, as opposed to recent 3D NeRF-like systems, stems from
the fact that video portraits are captured by monocular stationary cameras,
therefore, only a single viewpoint of the scene is available. Primarily, we
condition our generative model on expression blendshapes, nonetheless, we show
that our system can be successfully driven by audio features as well. Our
experiments demonstrate that the proposed method is 270 times faster than
recent NeRF-based reenactment methods, with our networks achieving speeds of 24
fps for resolutions up to 1024 x 1024, while outperforming prior works in terms
of visual quality.";Michail Christos Doukas<author:sep>Stylianos Ploumpis<author:sep>Stefanos Zafeiriou;http://arxiv.org/pdf/2211.13994v1;cs.CV;"In IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV) 2023";nerf
2211.13762v2;http://arxiv.org/abs/2211.13762v2;2022-11-24;ScanNeRF: a Scalable Benchmark for Neural Radiance Fields;"In this paper, we propose the first-ever real benchmark thought for
evaluating Neural Radiance Fields (NeRFs) and, in general, Neural Rendering
(NR) frameworks. We design and implement an effective pipeline for scanning
real objects in quantity and effortlessly. Our scan station is built with less
than 500$ hardware budget and can collect roughly 4000 images of a scanned
object in just 5 minutes. Such a platform is used to build ScanNeRF, a dataset
characterized by several train/val/test splits aimed at benchmarking the
performance of modern NeRF methods under different conditions. Accordingly, we
evaluate three cutting-edge NeRF variants on it to highlight their strengths
and weaknesses. The dataset is available on our project page, together with an
online benchmark to foster the development of better and better NeRFs.";Luca De Luigi<author:sep>Damiano Bolognini<author:sep>Federico Domeniconi<author:sep>Daniele De Gregorio<author:sep>Matteo Poggi<author:sep>Luigi Di Stefano;http://arxiv.org/pdf/2211.13762v2;cs.CV;"WACV 2023. The first three authors contributed equally. Project page:
  https://eyecan-ai.github.io/scannerf/";nerf
2211.13494v1;http://arxiv.org/abs/2211.13494v1;2022-11-24;Immersive Neural Graphics Primitives;"Neural radiance field (NeRF), in particular its extension by instant neural
graphics primitives, is a novel rendering method for view synthesis that uses
real-world images to build photo-realistic immersive virtual scenes. Despite
its potential, research on the combination of NeRF and virtual reality (VR)
remains sparse. Currently, there is no integration into typical VR systems
available, and the performance and suitability of NeRF implementations for VR
have not been evaluated, for instance, for different scene complexities or
screen resolutions. In this paper, we present and evaluate a NeRF-based
framework that is capable of rendering scenes in immersive VR allowing users to
freely move their heads to explore complex real-world scenes. We evaluate our
framework by benchmarking three different NeRF scenes concerning their
rendering performance at different scene complexities and resolutions.
Utilizing super-resolution, our approach can yield a frame rate of 30 frames
per second with a resolution of 1280x720 pixels per eye. We discuss potential
applications of our framework and provide an open source implementation online.";Ke Li<author:sep>Tim Rolff<author:sep>Susanne Schmidt<author:sep>Reinhard Bacher<author:sep>Simone Frintrop<author:sep>Wim Leemans<author:sep>Frank Steinicke;http://arxiv.org/pdf/2211.13494v1;cs.CV;Submitted to IEEE VR, currently under review;nerf
2211.13226v3;http://arxiv.org/abs/2211.13226v3;2022-11-23;ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field;"Physical simulations produce excellent predictions of weather effects. Neural
radiance fields produce SOTA scene models. We describe a novel NeRF-editing
procedure that can fuse physical simulations with NeRF models of scenes,
producing realistic movies of physical phenomena in those scenes. Our
application -- Climate NeRF -- allows people to visualize what climate change
outcomes will do to them. ClimateNeRF allows us to render realistic weather
effects, including smog, snow, and flood. Results can be controlled with
physically meaningful variables like water level. Qualitative and quantitative
studies show that our simulated results are significantly more realistic than
those from SOTA 2D image editing and SOTA 3D NeRF stylization.";Yuan Li<author:sep>Zhi-Hao Lin<author:sep>David Forsyth<author:sep>Jia-Bin Huang<author:sep>Shenlong Wang;http://arxiv.org/pdf/2211.13226v3;cs.CV;project page: https://climatenerf.github.io/;nerf
2211.12853v2;http://arxiv.org/abs/2211.12853v2;2022-11-23;BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields;"Neural Radiance Fields (NeRF) have received considerable attention recently,
due to its impressive capability in photo-realistic 3D reconstruction and novel
view synthesis, given a set of posed camera images. Earlier work usually
assumes the input images are of good quality. However, image degradation (e.g.
image motion blur in low-light conditions) can easily happen in real-world
scenarios, which would further affect the rendering quality of NeRF. In this
paper, we present a novel bundle adjusted deblur Neural Radiance Fields
(BAD-NeRF), which can be robust to severe motion blurred images and inaccurate
camera poses. Our approach models the physical image formation process of a
motion blurred image, and jointly learns the parameters of NeRF and recovers
the camera motion trajectories during exposure time. In experiments, we show
that by directly modeling the real physical image formation process, BAD-NeRF
achieves superior performance over prior works on both synthetic and real
datasets. Code and data are available at https://github.com/WU-CVGL/BAD-NeRF.";Peng Wang<author:sep>Lingzhe Zhao<author:sep>Ruijie Ma<author:sep>Peidong Liu;http://arxiv.org/pdf/2211.12853v2;cs.CV;"Accepted to CVPR 2023, Project page:
  https://wangpeng000.github.io/BAD-NeRF/";nerf
2211.12656v1;http://arxiv.org/abs/2211.12656v1;2022-11-23;ActiveRMAP: Radiance Field for Active Mapping And Planning;"A high-quality 3D reconstruction of a scene from a collection of 2D images
can be achieved through offline/online mapping methods. In this paper, we
explore active mapping from the perspective of implicit representations, which
have recently produced compelling results in a variety of applications. One of
the most popular implicit representations - Neural Radiance Field (NeRF), first
demonstrated photorealistic rendering results using multi-layer perceptrons,
with promising offline 3D reconstruction as a by-product of the radiance field.
More recently, researchers also applied this implicit representation for online
reconstruction and localization (i.e. implicit SLAM systems). However, the
study on using implicit representation for active vision tasks is still very
limited. In this paper, we are particularly interested in applying the neural
radiance field for active mapping and planning problems, which are closely
coupled tasks in an active system. We, for the first time, present an RGB-only
active vision framework using radiance field representation for active 3D
reconstruction and planning in an online manner. Specifically, we formulate
this joint task as an iterative dual-stage optimization problem, where we
alternatively optimize for the radiance field representation and path planning.
Experimental results suggest that the proposed method achieves competitive
results compared to other offline methods and outperforms active reconstruction
methods using NeRFs.";Huangying Zhan<author:sep>Jiyang Zheng<author:sep>Yi Xu<author:sep>Ian Reid<author:sep>Hamid Rezatofighi;http://arxiv.org/pdf/2211.12656v1;cs.CV;Under review;nerf
2211.12758v1;http://arxiv.org/abs/2211.12758v1;2022-11-23;PANeRF: Pseudo-view Augmentation for Improved Neural Radiance Fields  Based on Few-shot Inputs;"The method of neural radiance fields (NeRF) has been developed in recent
years, and this technology has promising applications for synthesizing novel
views of complex scenes. However, NeRF requires dense input views, typically
numbering in the hundreds, for generating high-quality images. With a decrease
in the number of input views, the rendering quality of NeRF for unseen
viewpoints tends to degenerate drastically. To overcome this challenge, we
propose pseudo-view augmentation of NeRF, a scheme that expands a sufficient
amount of data by considering the geometry of few-shot inputs. We first
initialized the NeRF network by leveraging the expanded pseudo-views, which
efficiently minimizes uncertainty when rendering unseen views. Subsequently, we
fine-tuned the network by utilizing sparse-view inputs containing precise
geometry and color information. Through experiments under various settings, we
verified that our model faithfully synthesizes novel-view images of superior
quality and outperforms existing methods for multi-view datasets.";Young Chun Ahn<author:sep>Seokhwan Jang<author:sep>Sungheon Park<author:sep>Ji-Yeon Kim<author:sep>Nahyup Kang;http://arxiv.org/pdf/2211.12758v1;cs.CV;;nerf
2211.13206v3;http://arxiv.org/abs/2211.13206v3;2022-11-23;AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural  Voxels;"With NeRF widely used for facial reenactment, recent methods can recover
photo-realistic 3D head avatar from just a monocular video. Unfortunately, the
training process of the NeRF-based methods is quite time-consuming, as MLP used
in the NeRF-based methods is inefficient and requires too many iterations to
converge. To overcome this problem, we propose AvatarMAV, a fast 3D head avatar
reconstruction method using Motion-Aware Neural Voxels. AvatarMAV is the first
to model both the canonical appearance and the decoupled expression motion by
neural voxels for head avatar. In particular, the motion-aware neural voxels is
generated from the weighted concatenation of multiple 4D tensors. The 4D
tensors semantically correspond one-to-one with 3DMM expression basis and share
the same weights as 3DMM expression coefficients. Benefiting from our novel
representation, the proposed AvatarMAV can recover photo-realistic head avatars
in just 5 minutes (implemented with pure PyTorch), which is significantly
faster than the state-of-the-art facial reenactment methods. Project page:
https://www.liuyebin.com/avatarmav.";Yuelang Xu<author:sep>Lizhen Wang<author:sep>Xiaochen Zhao<author:sep>Hongwen Zhang<author:sep>Yebin Liu;http://arxiv.org/pdf/2211.13206v3;cs.CV;Accepted by SIGGRAPH 2023;nerf
2211.13251v2;http://arxiv.org/abs/2211.13251v2;2022-11-23;CGOF++: Controllable 3D Face Synthesis with Conditional Generative  Occupancy Fields;"Capitalizing on the recent advances in image generation models, existing
controllable face image synthesis methods are able to generate high-fidelity
images with some levels of controllability, e.g., controlling the shapes,
expressions, textures, and poses of the generated face images. However,
previous methods focus on controllable 2D image generative models, which are
prone to producing inconsistent face images under large expression and pose
changes. In this paper, we propose a new NeRF-based conditional 3D face
synthesis framework, which enables 3D controllability over the generated face
images by imposing explicit 3D conditions from 3D face priors. At its core is a
conditional Generative Occupancy Field (cGOF++) that effectively enforces the
shape of the generated face to conform to a given 3D Morphable Model (3DMM)
mesh, built on top of EG3D [1], a recent tri-plane-based generative model. To
achieve accurate control over fine-grained 3D face shapes of the synthesized
images, we additionally incorporate a 3D landmark loss as well as a volume
warping loss into our synthesis framework. Experiments validate the
effectiveness of the proposed method, which is able to generate high-fidelity
face images and shows more precise 3D controllability than state-of-the-art
2D-based controllable face synthesis methods.";Keqiang Sun<author:sep>Shangzhe Wu<author:sep>Ning Zhang<author:sep>Zhaoyang Huang<author:sep>Quan Wang<author:sep>Hongsheng Li;http://arxiv.org/pdf/2211.13251v2;cs.CV;"Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). This article is an extension of the NeurIPS'22 paper
  arXiv:2206.08361";nerf
2211.12368v1;http://arxiv.org/abs/2211.12368v1;2022-11-22;Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial  Decomposition;"While dynamic Neural Radiance Fields (NeRF) have shown success in
high-fidelity 3D modeling of talking portraits, the slow training and inference
speed severely obstruct their potential usage. In this paper, we propose an
efficient NeRF-based framework that enables real-time synthesizing of talking
portraits and faster convergence by leveraging the recent success of grid-based
NeRF. Our key insight is to decompose the inherently high-dimensional talking
portrait representation into three low-dimensional feature grids. Specifically,
a Decomposed Audio-spatial Encoding Module models the dynamic head with a 3D
spatial grid and a 2D audio grid. The torso is handled with another 2D grid in
a lightweight Pseudo-3D Deformable Module. Both modules focus on efficiency
under the premise of good rendering quality. Extensive experiments demonstrate
that our method can generate realistic and audio-lips synchronized talking
portrait videos, while also being highly efficient compared to previous
methods.";Jiaxiang Tang<author:sep>Kaisiyuan Wang<author:sep>Hang Zhou<author:sep>Xiaokang Chen<author:sep>Dongliang He<author:sep>Tianshu Hu<author:sep>Jingtuo Liu<author:sep>Gang Zeng<author:sep>Jingdong Wang;http://arxiv.org/pdf/2211.12368v1;cs.CV;Project page: https://me.kiui.moe/radnerf/;nerf
2211.12285v2;http://arxiv.org/abs/2211.12285v2;2022-11-22;Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for  Neural Radiance Fields;"Neural Radiance Fields (NeRF) have attracted significant attention due to
their ability to synthesize novel scene views with great accuracy. However,
inherent to their underlying formulation, the sampling of points along a ray
with zero width may result in ambiguous representations that lead to further
rendering artifacts such as aliasing in the final scene. To address this issue,
the recent variant mip-NeRF proposes an Integrated Positional Encoding (IPE)
based on a conical view frustum. Although this is expressed with an integral
formulation, mip-NeRF instead approximates this integral as the expected value
of a multivariate Gaussian distribution. This approximation is reliable for
short frustums but degrades with highly elongated regions, which arises when
dealing with distant scene objects under a larger depth of field. In this
paper, we explore the use of an exact approach for calculating the IPE by using
a pyramid-based integral formulation instead of an approximated conical-based
one. We denote this formulation as Exact-NeRF and contribute the first approach
to offer a precise analytical solution to the IPE within the NeRF domain. Our
exploratory work illustrates that such an exact formulation Exact-NeRF matches
the accuracy of mip-NeRF and furthermore provides a natural extension to more
challenging scenarios without further modification, such as in the case of
unbounded scenes. Our contribution aims to both address the hitherto unexplored
issues of frustum approximation in earlier NeRF work and additionally provide
insight into the potential future consideration of analytical solutions in
future NeRF extensions.";Brian K. S. Isaac-Medina<author:sep>Chris G. Willcocks<author:sep>Toby P. Breckon;http://arxiv.org/pdf/2211.12285v2;cs.CV;15 pages,10 figures;nerf
2211.12254v2;http://arxiv.org/abs/2211.12254v2;2022-11-22;SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural  Radiance Fields;"Neural Radiance Fields (NeRFs) have emerged as a popular approach for novel
view synthesis. While NeRFs are quickly being adapted for a wider set of
applications, intuitively editing NeRF scenes is still an open challenge. One
important editing task is the removal of unwanted objects from a 3D scene, such
that the replaced region is visually plausible and consistent with its context.
We refer to this task as 3D inpainting. In 3D, solutions must be both
consistent across multiple views and geometrically valid. In this paper, we
propose a novel 3D inpainting method that addresses these challenges. Given a
small set of posed images and sparse annotations in a single input image, our
framework first rapidly obtains a 3D segmentation mask for a target object.
Using the mask, a perceptual optimizationbased approach is then introduced that
leverages learned 2D image inpainters, distilling their information into 3D
space, while ensuring view consistency. We also address the lack of a diverse
benchmark for evaluating 3D scene inpainting methods by introducing a dataset
comprised of challenging real-world scenes. In particular, our dataset contains
views of the same scene with and without a target object, enabling more
principled benchmarking of the 3D inpainting task. We first demonstrate the
superiority of our approach on multiview segmentation, comparing to NeRFbased
methods and 2D segmentation approaches. We then evaluate on the task of 3D
inpainting, establishing state-ofthe-art performance against other NeRF
manipulation algorithms, as well as a strong 2D image inpainter baseline.
Project Page: https://spinnerf3d.github.io";Ashkan Mirzaei<author:sep>Tristan Aumentado-Armstrong<author:sep>Konstantinos G. Derpanis<author:sep>Jonathan Kelly<author:sep>Marcus A. Brubaker<author:sep>Igor Gilitschenski<author:sep>Alex Levinshtein;http://arxiv.org/pdf/2211.12254v2;cs.CV;Project Page: https://spinnerf3d.github.io;nerf
2211.12436v2;http://arxiv.org/abs/2211.12436v2;2022-11-22;Dynamic Depth-Supervised NeRF for Multi-View RGB-D Operating Room Images;"The operating room (OR) is an environment of interest for the development of
sensing systems, enabling the detection of people, objects, and their semantic
relations. Due to frequent occlusions in the OR, these systems often rely on
input from multiple cameras. While increasing the number of cameras generally
increases algorithm performance, there are hard limitations to the number and
locations of cameras in the OR. Neural Radiance Fields (NeRF) can be used to
render synthetic views from arbitrary camera positions, virtually enlarging the
number of cameras in the dataset. In this work, we explore the use of NeRF for
view synthesis of dynamic scenes in the OR, and we show that regularisation
with depth supervision from RGB-D sensor data results in higher image quality.
We optimise a dynamic depth-supervised NeRF with up to six synchronised cameras
that capture the surgical field in five distinct phases before and during a
knee replacement surgery. We qualitatively inspect views rendered by a virtual
camera that moves 180 degrees around the surgical field at differing time
values. Quantitatively, we evaluate view synthesis from an unseen camera
position in terms of PSNR, SSIM and LPIPS for the colour channels and in MAE
and error percentage for the estimated depth. We find that NeRFs can be used to
generate geometrically consistent views, also from interpolated camera
positions and at interpolated time intervals. Views are generated from an
unseen camera pose with an average PSNR of 18.2 and a depth estimation error of
2.0%. Our results show the potential of a dynamic NeRF for view synthesis in
the OR and stress the relevance of depth supervision in a clinical setting.";Beerend G. A. Gerats<author:sep>Jelmer M. Wolterink<author:sep>Ivo A. M. J. Broeders;http://arxiv.org/pdf/2211.12436v2;cs.CV;Accepted to the Workshop on Ambient Intelligence for HealthCare 2023;nerf
2211.12046v4;http://arxiv.org/abs/2211.12046v4;2022-11-22;DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors;"Neural Radiance Field (NeRF) has exhibited outstanding three-dimensional (3D)
reconstruction quality via the novel view synthesis from multi-view images and
paired calibrated camera parameters. However, previous NeRF-based systems have
been demonstrated under strictly controlled settings, with little attention
paid to less ideal scenarios, including with the presence of noise such as
exposure, illumination changes, and blur. In particular, though blur frequently
occurs in real situations, NeRF that can handle blurred images has received
little attention. The few studies that have investigated NeRF for blurred
images have not considered geometric and appearance consistency in 3D space,
which is one of the most important factors in 3D reconstruction. This leads to
inconsistency and the degradation of the perceptual quality of the constructed
scene. Hence, this paper proposes a DP-NeRF, a novel clean NeRF framework for
blurred images, which is constrained with two physical priors. These priors are
derived from the actual blurring process during image acquisition by the
camera. DP-NeRF proposes rigid blurring kernel to impose 3D consistency
utilizing the physical priors and adaptive weight proposal to refine the color
composition error in consideration of the relationship between depth and blur.
We present extensive experimental results for synthetic and real scenes with
two types of blur: camera motion blur and defocus blur. The results demonstrate
that DP-NeRF successfully improves the perceptual quality of the constructed
NeRF ensuring 3D geometric and appearance consistency. We further demonstrate
the effectiveness of our model with comprehensive ablation analysis.";Dogyoon Lee<author:sep>Minhyeok Lee<author:sep>Chajin Shin<author:sep>Sangyoun Lee;http://arxiv.org/pdf/2211.12046v4;cs.CV;"Accepted at CVPR 2023, Code: https://github.com/dogyoonlee/DP-NeRF,
  Project page: https://dogyoonlee.github.io/dpnerf/";nerf
2211.12499v2;http://arxiv.org/abs/2211.12499v2;2022-11-22;Instant Volumetric Head Avatars;"We present Instant Volumetric Head Avatars (INSTA), a novel approach for
reconstructing photo-realistic digital avatars instantaneously. INSTA models a
dynamic neural radiance field based on neural graphics primitives embedded
around a parametric face model. Our pipeline is trained on a single monocular
RGB portrait video that observes the subject under different expressions and
views. While state-of-the-art methods take up to several days to train an
avatar, our method can reconstruct a digital avatar in less than 10 minutes on
modern GPU hardware, which is orders of magnitude faster than previous
solutions. In addition, it allows for the interactive rendering of novel poses
and expressions. By leveraging the geometry prior of the underlying parametric
face model, we demonstrate that INSTA extrapolates to unseen poses. In
quantitative and qualitative studies on various subjects, INSTA outperforms
state-of-the-art methods regarding rendering quality and training time.";Wojciech Zielonka<author:sep>Timo Bolkart<author:sep>Justus Thies;http://arxiv.org/pdf/2211.12499v2;cs.CV;"Website: https://zielon.github.io/insta/ Video:
  https://youtu.be/HOgaeWTih7Q Accepted to CVPR2023";
2211.12544v1;http://arxiv.org/abs/2211.12544v1;2022-11-22;Zero NeRF: Registration with Zero Overlap;"We present Zero-NeRF, a projective surface registration method that, to the
best of our knowledge, offers the first general solution capable of alignment
between scene representations with minimal or zero visual correspondence. To do
this, we enforce consistency between visible surfaces of partial and complete
reconstructions, which allows us to constrain occluded geometry. We use a NeRF
as our surface representation and the NeRF rendering pipeline to perform this
alignment. To demonstrate the efficacy of our method, we register real-world
scenes from opposite sides with infinitesimal overlaps that cannot be
accurately registered using prior methods, and we compare these results against
widely used registration methods.";Casey Peat<author:sep>Oliver Batchelor<author:sep>Richard Green<author:sep>James Atlas;http://arxiv.org/pdf/2211.12544v1;cs.CV;;nerf
2211.12038v1;http://arxiv.org/abs/2211.12038v1;2022-11-22;ONeRF: Unsupervised 3D Object Segmentation from Multiple Views;"We present ONeRF, a method that automatically segments and reconstructs
object instances in 3D from multi-view RGB images without any additional manual
annotations. The segmented 3D objects are represented using separate Neural
Radiance Fields (NeRFs) which allow for various 3D scene editing and novel view
rendering. At the core of our method is an unsupervised approach using the
iterative Expectation-Maximization algorithm, which effectively aggregates 2D
visual features and the corresponding 3D cues from multi-views for joint 3D
object segmentation and reconstruction. Unlike existing approaches that can
only handle simple objects, our method produces segmented full 3D NeRFs of
individual objects with complex shapes, topologies and appearance. The
segmented ONeRfs enable a range of 3D scene editing, such as object
transformation, insertion and deletion.";Shengnan Liang<author:sep>Yichen Liu<author:sep>Shangzhe Wu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2211.12038v1;cs.CV;;nerf
2211.11674v2;http://arxiv.org/abs/2211.11674v2;2022-11-21;Shape, Pose, and Appearance from a Single Image via Bootstrapped  Radiance Field Inversion;"Neural Radiance Fields (NeRF) coupled with GANs represent a promising
direction in the area of 3D reconstruction from a single view, owing to their
ability to efficiently model arbitrary topologies. Recent work in this area,
however, has mostly focused on synthetic datasets where exact ground-truth
poses are known, and has overlooked pose estimation, which is important for
certain downstream applications such as augmented reality (AR) and robotics. We
introduce a principled end-to-end reconstruction framework for natural images,
where accurate ground-truth poses are not available. Our approach recovers an
SDF-parameterized 3D shape, pose, and appearance from a single image of an
object, without exploiting multiple views during training. More specifically,
we leverage an unconditional 3D-aware generator, to which we apply a hybrid
inversion scheme where a model produces a first guess of the solution which is
then refined via optimization. Our framework can de-render an image in as few
as 10 steps, enabling its use in practical scenarios. We demonstrate
state-of-the-art results on a variety of real and synthetic benchmarks.";Dario Pavllo<author:sep>David Joseph Tan<author:sep>Marie-Julie Rakotosaona<author:sep>Federico Tombari;http://arxiv.org/pdf/2211.11674v2;cs.CV;"CVPR 2023. Code and models are available at
  https://github.com/google-research/nerf-from-image";nerf
2211.11202v3;http://arxiv.org/abs/2211.11202v3;2022-11-21;FLNeRF: 3D Facial Landmarks Estimation in Neural Radiance Fields;"This paper presents the first significant work on directly predicting 3D face
landmarks on neural radiance fields (NeRFs). Our 3D coarse-to-fine Face
Landmarks NeRF (FLNeRF) model efficiently samples from a given face NeRF with
individual facial features for accurate landmarks detection. Expression
augmentation is applied to facial features in a fine scale to simulate large
emotions range including exaggerated facial expressions (e.g., cheek blowing,
wide opening mouth, eye blinking) for training FLNeRF. Qualitative and
quantitative comparison with related state-of-the-art 3D facial landmark
estimation methods demonstrate the efficacy of FLNeRF, which contributes to
downstream tasks such as high-quality face editing and swapping with direct
control using our NeRF landmarks. Code and data will be available. Github link:
https://github.com/ZHANG1023/FLNeRF.";Hao Zhang<author:sep>Tianyuan Dai<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2211.11202v3;cs.CV;"Hao Zhang and Tianyuan Dai contributed equally. Project website:
  https://github.com/ZHANG1023/FLNeRF";nerf
2211.11836v1;http://arxiv.org/abs/2211.11836v1;2022-11-21;Towards Live 3D Reconstruction from Wearable Video: An Evaluation of  V-SLAM, NeRF, and Videogrammetry Techniques;"Mixed reality (MR) is a key technology which promises to change the future of
warfare. An MR hybrid of physical outdoor environments and virtual military
training will enable engagements with long distance enemies, both real and
simulated. To enable this technology, a large-scale 3D model of a physical
environment must be maintained based on live sensor observations. 3D
reconstruction algorithms should utilize the low cost and pervasiveness of
video camera sensors, from both overhead and soldier-level perspectives.
Mapping speed and 3D quality can be balanced to enable live MR training in
dynamic environments. Given these requirements, we survey several 3D
reconstruction algorithms for large-scale mapping for military applications
given only live video. We measure 3D reconstruction performance from common
structure from motion, visual-SLAM, and photogrammetry techniques. This
includes the open source algorithms COLMAP, ORB-SLAM3, and NeRF using
Instant-NGP. We utilize the autonomous driving academic benchmark KITTI, which
includes both dashboard camera video and lidar produced 3D ground truth. With
the KITTI data, our primary contribution is a quantitative evaluation of 3D
reconstruction computational speed when considering live video.";David Ramirez<author:sep>Suren Jayasuriya<author:sep>Andreas Spanias;http://arxiv.org/pdf/2211.11836v1;eess.IV;"Accepted to 2022 Interservice/Industry Training, Simulation, and
  Education Conference (I/ITSEC), 13 pages";nerf
2211.11646v3;http://arxiv.org/abs/2211.11646v3;2022-11-21;NeRF-RPN: A general framework for object detection in NeRFs;"This paper presents the first significant object detection framework,
NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,
NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting
a novel voxel representation that incorporates multi-scale 3D neural volumetric
features, we demonstrate it is possible to regress the 3D bounding boxes of
objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN
is a general framework and can be applied to detect objects without class
labels. We experimented NeRF-RPN with various backbone architectures, RPN head
designs and loss functions. All of them can be trained in an end-to-end manner
to estimate high quality 3D bounding boxes. To facilitate future research in
object detection for NeRF, we built a new benchmark dataset which consists of
both synthetic and real-world data with careful labeling and clean up. Code and
dataset are available at https://github.com/lyclyc52/NeRF_RPN.";Benran Hu<author:sep>Junkai Huang<author:sep>Yichen Liu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2211.11646v3;cs.CV;Accepted by CVPR 2023;nerf
2211.11505v3;http://arxiv.org/abs/2211.11505v3;2022-11-21;Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields;"Neural Radiance Fields (NeRF) have achieved photorealistic novel views
synthesis; however, the requirement of accurate camera poses limits its
application. Despite analysis-by-synthesis extensions for jointly learning
neural 3D representations and registering camera frames exist, they are
susceptible to suboptimal solutions if poorly initialized. We propose L2G-NeRF,
a Local-to-Global registration method for bundle-adjusting Neural Radiance
Fields: first, a pixel-wise flexible alignment, followed by a frame-wise
constrained parametric alignment. Pixel-wise local alignment is learned in an
unsupervised way via a deep network which optimizes photometric reconstruction
errors. Frame-wise global alignment is performed using differentiable parameter
estimation solvers on the pixel-wise correspondences to find a global
transformation. Experiments on synthetic and real-world data show that our
method outperforms the current state-of-the-art in terms of high-fidelity
reconstruction and resolving large camera pose misalignment. Our module is an
easy-to-use plugin that can be applied to NeRF variants and other neural field
applications. The Code and supplementary materials are available at
https://rover-xingyu.github.io/L2G-NeRF/.";Yue Chen<author:sep>Xingyu Chen<author:sep>Xuan Wang<author:sep>Qi Zhang<author:sep>Yu Guo<author:sep>Ying Shan<author:sep>Fei Wang;http://arxiv.org/pdf/2211.11505v3;cs.CV;Accepted to CVPR 2023;nerf
2211.11704v2;http://arxiv.org/abs/2211.11704v2;2022-11-21;ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of  Signed Distance Fields;"We present ESLAM, an efficient implicit neural representation method for
Simultaneous Localization and Mapping (SLAM). ESLAM reads RGB-D frames with
unknown camera poses in a sequential manner and incrementally reconstructs the
scene representation while estimating the current camera position in the scene.
We incorporate the latest advances in Neural Radiance Fields (NeRF) into a SLAM
system, resulting in an efficient and accurate dense visual SLAM method. Our
scene representation consists of multi-scale axis-aligned perpendicular feature
planes and shallow decoders that, for each point in the continuous space,
decode the interpolated features into Truncated Signed Distance Field (TSDF)
and RGB values. Our extensive experiments on three standard datasets, Replica,
ScanNet, and TUM RGB-D show that ESLAM improves the accuracy of 3D
reconstruction and camera localization of state-of-the-art dense visual SLAM
methods by more than 50%, while it runs up to 10 times faster and does not
require any pre-training.";Mohammad Mahdi Johari<author:sep>Camilla Carta<author:sep>François Fleuret;http://arxiv.org/pdf/2211.11704v2;cs.CV;CVPR 2023 Highlight. Project page: https://www.idiap.ch/paper/eslam/;nerf
2211.11215v2;http://arxiv.org/abs/2211.11215v2;2022-11-21;SegNeRF: 3D Part Segmentation with Neural Radiance Fields;"Recent advances in Neural Radiance Fields (NeRF) boast impressive
performances for generative tasks such as novel view synthesis and 3D
reconstruction. Methods based on neural radiance fields are able to represent
the 3D world implicitly by relying exclusively on posed images. Yet, they have
seldom been explored in the realm of discriminative tasks such as 3D part
segmentation. In this work, we attempt to bridge that gap by proposing SegNeRF:
a neural field representation that integrates a semantic field along with the
usual radiance field. SegNeRF inherits from previous works the ability to
perform novel view synthesis and 3D reconstruction, and enables 3D part
segmentation from a few images. Our extensive experiments on PartNet show that
SegNeRF is capable of simultaneously predicting geometry, appearance, and
semantic information from posed images, even for unseen objects. The predicted
semantic fields allow SegNeRF to achieve an average mIoU of $\textbf{30.30%}$
for 2D novel view segmentation, and $\textbf{37.46%}$ for 3D part segmentation,
boasting competitive performance against point-based methods by using only a
few posed images. Additionally, SegNeRF is able to generate an explicit 3D
model from a single image of an object taken in the wild, with its
corresponding part segmentation.";Jesus Zarzar<author:sep>Sara Rojas<author:sep>Silvio Giancola<author:sep>Bernard Ghanem;http://arxiv.org/pdf/2211.11215v2;cs.CV;Fixed abstract typo;nerf
2211.11738v3;http://arxiv.org/abs/2211.11738v3;2022-11-21;SPARF: Neural Radiance Fields from Sparse and Noisy Poses;"Neural Radiance Field (NeRF) has recently emerged as a powerful
representation to synthesize photorealistic novel views. While showing
impressive performance, it relies on the availability of dense input views with
highly accurate camera poses, thus limiting its application in real-world
scenarios. In this work, we introduce Sparse Pose Adjusting Radiance Field
(SPARF), to address the challenge of novel-view synthesis given only few
wide-baseline input images (as low as 3) with noisy camera poses. Our approach
exploits multi-view geometry constraints in order to jointly learn the NeRF and
refine the camera poses. By relying on pixel matches extracted between the
input views, our multi-view correspondence objective enforces the optimized
scene and camera poses to converge to a global and geometrically accurate
solution. Our depth consistency loss further encourages the reconstructed scene
to be consistent from any viewpoint. Our approach sets a new state of the art
in the sparse-view regime on multiple challenging datasets.";Prune Truong<author:sep>Marie-Julie Rakotosaona<author:sep>Fabian Manhardt<author:sep>Federico Tombari;http://arxiv.org/pdf/2211.11738v3;cs.CV;"Code is released at https://github.com/google-research/sparf.
  Published at CVPR 2023 as a Highlight";nerf
2211.11082v3;http://arxiv.org/abs/2211.11082v3;2022-11-20;DynIBaR: Neural Dynamic Image-Based Rendering;"We address the problem of synthesizing novel views from a monocular video
depicting a complex dynamic scene. State-of-the-art methods based on temporally
varying Neural Radiance Fields (aka dynamic NeRFs) have shown impressive
results on this task. However, for long videos with complex object motions and
uncontrolled camera trajectories, these methods can produce blurry or
inaccurate renderings, hampering their use in real-world applications. Instead
of encoding the entire dynamic scene within the weights of MLPs, we present a
new approach that addresses these limitations by adopting a volumetric
image-based rendering framework that synthesizes new viewpoints by aggregating
features from nearby views in a scene-motion-aware manner. Our system retains
the advantages of prior methods in its ability to model complex scenes and
view-dependent effects, but also enables synthesizing photo-realistic novel
views from long videos featuring complex scene dynamics with unconstrained
camera trajectories. We demonstrate significant improvements over
state-of-the-art methods on dynamic scene datasets, and also apply our approach
to in-the-wild videos with challenging camera and object motion, where prior
methods fail to produce high-quality renderings. Our project webpage is at
dynibar.github.io.";Zhengqi Li<author:sep>Qianqian Wang<author:sep>Forrester Cole<author:sep>Richard Tucker<author:sep>Noah Snavely;http://arxiv.org/pdf/2211.11082v3;cs.CV;Award Candidate, CVPR 2023 Project page: dynibar.github.io;nerf
2211.10440v2;http://arxiv.org/abs/2211.10440v2;2022-11-18;Magic3D: High-Resolution Text-to-3D Content Creation;"DreamFusion has recently demonstrated the utility of a pre-trained
text-to-image diffusion model to optimize Neural Radiance Fields (NeRF),
achieving remarkable text-to-3D synthesis results. However, the method has two
inherent limitations: (a) extremely slow optimization of NeRF and (b)
low-resolution image space supervision on NeRF, leading to low-quality 3D
models with a long processing time. In this paper, we address these limitations
by utilizing a two-stage optimization framework. First, we obtain a coarse
model using a low-resolution diffusion prior and accelerate with a sparse 3D
hash grid structure. Using the coarse representation as the initialization, we
further optimize a textured 3D mesh model with an efficient differentiable
renderer interacting with a high-resolution latent diffusion model. Our method,
dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is
2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also
achieving higher resolution. User studies show 61.7% raters to prefer our
approach over DreamFusion. Together with the image-conditioned generation
capabilities, we provide users with new ways to control 3D synthesis, opening
up new avenues to various creative applications.";Chen-Hsuan Lin<author:sep>Jun Gao<author:sep>Luming Tang<author:sep>Towaki Takikawa<author:sep>Xiaohui Zeng<author:sep>Xun Huang<author:sep>Karsten Kreis<author:sep>Sanja Fidler<author:sep>Ming-Yu Liu<author:sep>Tsung-Yi Lin;http://arxiv.org/pdf/2211.10440v2;cs.CV;"Accepted to CVPR 2023 as highlight. Project website:
  https://research.nvidia.com/labs/dir/magic3d";nerf
2211.10444v1;http://arxiv.org/abs/2211.10444v1;2022-11-18;Neural Fields for Fast and Scalable Interpolation of Geophysical Ocean  Variables;"Optimal Interpolation (OI) is a widely used, highly trusted algorithm for
interpolation and reconstruction problems in geosciences. With the influx of
more satellite missions, we have access to more and more observations and it is
becoming more pertinent to take advantage of these observations in applications
such as forecasting and reanalysis. With the increase in the volume of
available data, scalability remains an issue for standard OI and it prevents
many practitioners from effectively and efficiently taking advantage of these
large sums of data to learn the model hyperparameters. In this work, we
leverage recent advances in Neural Fields (NerFs) as an alternative to the OI
framework where we show how they can be easily applied to standard
reconstruction problems in physical oceanography. We illustrate the relevance
of NerFs for gap-filling of sparse measurements of sea surface height (SSH) via
satellite altimetry and demonstrate how NerFs are scalable with comparable
results to the standard OI. We find that NerFs are a practical set of methods
that can be readily applied to geoscience interpolation problems and we
anticipate a wider adoption in the future.";J. Emmanuel Johnson<author:sep>Redouane Lguensat<author:sep>Ronan Fablet<author:sep>Emmanuel Cosme<author:sep>Julien Le Sommer;http://arxiv.org/pdf/2211.10444v1;physics.ao-ph;Machine Learning and the Physical Sciences workshop, NeurIPS 2022;nerf
2211.09682v1;http://arxiv.org/abs/2211.09682v1;2022-11-17;AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware  Training;"Neural Radiance Fields (NeRFs) are a powerful representation for modeling a
3D scene as a continuous function. Though NeRF is able to render complex 3D
scenes with view-dependent effects, few efforts have been devoted to exploring
its limits in a high-resolution setting. Specifically, existing NeRF-based
methods face several limitations when reconstructing high-resolution real
scenes, including a very large number of parameters, misaligned input data, and
overly smooth details. In this work, we conduct the first pilot study on
training NeRF with high-resolution data and propose the corresponding
solutions: 1) marrying the multilayer perceptron (MLP) with convolutional
layers which can encode more neighborhood information while reducing the total
number of parameters; 2) a novel training strategy to address misalignment
caused by moving objects or small camera calibration errors; and 3) a
high-frequency aware loss. Our approach is nearly free without introducing
obvious training/testing costs, while experiments on different datasets
demonstrate that it can recover more high-frequency details compared with the
current state-of-the-art NeRF models. Project page:
\url{https://yifanjiang.net/alignerf.}";Yifan Jiang<author:sep>Peter Hedman<author:sep>Ben Mildenhall<author:sep>Dejia Xu<author:sep>Jonathan T. Barron<author:sep>Zhangyang Wang<author:sep>Tianfan Xue;http://arxiv.org/pdf/2211.09682v1;cs.CV;;nerf
2211.08610v1;http://arxiv.org/abs/2211.08610v1;2022-11-16;CoNFies: Controllable Neural Face Avatars;"Neural Radiance Fields (NeRF) are compelling techniques for modeling dynamic
3D scenes from 2D image collections. These volumetric representations would be
well suited for synthesizing novel facial expressions but for two problems.
First, deformable NeRFs are object agnostic and model holistic movement of the
scene: they can replay how the motion changes over time, but they cannot alter
it in an interpretable way. Second, controllable volumetric representations
typically require either time-consuming manual annotations or 3D supervision to
provide semantic meaning to the scene. We propose a controllable neural
representation for face self-portraits (CoNFies), that solves both of these
problems within a common framework, and it can rely on automated processing. We
use automated facial action recognition (AFAR) to characterize facial
expressions as a combination of action units (AU) and their intensities. AUs
provide both the semantic locations and control labels for the system. CoNFies
outperformed competing methods for novel view and expression synthesis in terms
of visual and anatomic fidelity of expressions.";Heng Yu<author:sep>Koichiro Niinuma<author:sep>Laszlo A. Jeni;http://arxiv.org/pdf/2211.08610v1;cs.CV;accepted by FG2023;nerf
2211.07968v1;http://arxiv.org/abs/2211.07968v1;2022-11-15;NeRFFaceEditing: Disentangled Face Editing in Neural Radiance Fields;"Recent methods for synthesizing 3D-aware face images have achieved rapid
development thanks to neural radiance fields, allowing for high quality and
fast inference speed. However, existing solutions for editing facial geometry
and appearance independently usually require retraining and are not optimized
for the recent work of generation, thus tending to lag behind the generation
process. To address these issues, we introduce NeRFFaceEditing, which enables
editing and decoupling geometry and appearance in the pretrained
tri-plane-based neural radiance field while retaining its high quality and fast
inference speed. Our key idea for disentanglement is to use the statistics of
the tri-plane to represent the high-level appearance of its corresponding
facial volume. Moreover, we leverage a generated 3D-continuous semantic mask as
an intermediary for geometry editing. We devise a geometry decoder (whose
output is unchanged when the appearance changes) and an appearance decoder. The
geometry decoder aligns the original facial volume with the semantic mask
volume. We also enhance the disentanglement by explicitly regularizing rendered
images with the same appearance but different geometry to be similar in terms
of color distribution for each facial component separately. Our method allows
users to edit via semantic masks with decoupled control of geometry and
appearance. Both qualitative and quantitative evaluations show the superior
geometry and appearance control abilities of our method compared to existing
and alternative solutions.";Kaiwen Jiang<author:sep>Shu-Yu Chen<author:sep>Feng-Lin Liu<author:sep>Hongbo Fu<author:sep>Lin Gao;http://arxiv.org/pdf/2211.07968v1;cs.GR;;nerf
2211.07600v1;http://arxiv.org/abs/2211.07600v1;2022-11-14;Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures;"Text-guided image generation has progressed rapidly in recent years,
inspiring major breakthroughs in text-guided shape generation. Recently, it has
been shown that using score distillation, one can successfully text-guide a
NeRF model to generate a 3D object. We adapt the score distillation to the
publicly available, and computationally efficient, Latent Diffusion Models,
which apply the entire diffusion process in a compact latent space of a
pretrained autoencoder. As NeRFs operate in image space, a naive solution for
guiding them with latent score distillation would require encoding to the
latent space at each guidance step. Instead, we propose to bring the NeRF to
the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we
show that while Text-to-3D models can generate impressive results, they are
inherently unconstrained and may lack the ability to guide or enforce a
specific 3D structure. To assist and direct the 3D generation, we propose to
guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines
the coarse structure of the desired object. Then, we present means to integrate
such a constraint directly into a Latent-NeRF. This unique combination of text
and shape guidance allows for increased control over the generation process. We
also show that latent score distillation can be successfully applied directly
on 3D meshes. This allows for generating high-quality textures on a given
geometry. Our experiments validate the power of our different forms of guidance
and the efficiency of using latent rendering. Implementation is available at
https://github.com/eladrich/latent-nerf";Gal Metzer<author:sep>Elad Richardson<author:sep>Or Patashnik<author:sep>Raja Giryes<author:sep>Daniel Cohen-Or;http://arxiv.org/pdf/2211.07600v1;cs.CV;;nerf
2211.06583v1;http://arxiv.org/abs/2211.06583v1;2022-11-12;3D-Aware Encoding for Style-based Neural Radiance Fields;"We tackle the task of NeRF inversion for style-based neural radiance fields,
(e.g., StyleNeRF). In the task, we aim to learn an inversion function to
project an input image to the latent space of a NeRF generator and then
synthesize novel views of the original image based on the latent code. Compared
with GAN inversion for 2D generative models, NeRF inversion not only needs to
1) preserve the identity of the input image, but also 2) ensure 3D consistency
in generated novel views. This requires the latent code obtained from the
single-view image to be invariant across multiple views. To address this new
challenge, we propose a two-stage encoder for style-based NeRF inversion. In
the first stage, we introduce a base encoder that converts the input image to a
latent code. To ensure the latent code is view-invariant and is able to
synthesize 3D consistent novel view images, we utilize identity contrastive
learning to train the base encoder. Second, to better preserve the identity of
the input image, we introduce a refining encoder to refine the latent code and
add finer details to the output image. Importantly note that the novelty of
this model lies in the design of its first-stage encoder which produces the
closest latent code lying on the latent manifold and thus the refinement in the
second stage would be close to the NeRF manifold. Through extensive
experiments, we demonstrate that our proposed two-stage encoder qualitatively
and quantitatively exhibits superiority over the existing encoders for
inversion in both image reconstruction and novel-view rendering.";Yu-Jhe Li<author:sep>Tao Xu<author:sep>Bichen Wu<author:sep>Ningyuan Zheng<author:sep>Xiaoliang Dai<author:sep>Albert Pumarola<author:sep>Peizhao Zhang<author:sep>Peter Vajda<author:sep>Kris Kitani;http://arxiv.org/pdf/2211.06583v1;cs.CV;21 pages (under review);nerf
2211.04041v4;http://arxiv.org/abs/2211.04041v4;2022-11-08;ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance  Fields;"While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline
methods with an emphasis on visual fidelity, our paper addresses the online use
case that prioritises real-time adaptability. We present ParticleNeRF, a new
approach that dynamically adapts to changes in the scene geometry by learning
an up-to-date representation online, every 200ms. ParticleNeRF achieves this
using a novel particle-based parametric encoding. We couple features to
particles in space and backpropagate the photometric reconstruction loss into
the particles' position gradients, which are then interpreted as velocity
vectors. Governed by a lightweight physics system to handle collisions, this
lets the features move freely with the changing scene geometry. We demonstrate
ParticleNeRF on various dynamic scenes containing translating, rotating,
articulated, and deformable objects. ParticleNeRF is the first online dynamic
NeRF and achieves fast adaptability with better visual fidelity than
brute-force online InstantNGP and other baseline approaches on dynamic scenes
with online constraints. Videos of our system can be found at our project
website https://sites.google.com/view/particlenerf.";Jad Abou-Chakra<author:sep>Feras Dayoub<author:sep>Niko Sünderhauf;http://arxiv.org/pdf/2211.04041v4;cs.CV;;nerf
2211.03889v1;http://arxiv.org/abs/2211.03889v1;2022-11-07;Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable  Categories;"Obtaining photorealistic reconstructions of objects from sparse views is
inherently ambiguous and can only be achieved by learning suitable
reconstruction priors. Earlier works on sparse rigid object reconstruction
successfully learned such priors from large datasets such as CO3D. In this
paper, we extend this approach to dynamic objects. We use cats and dogs as a
representative example and introduce Common Pets in 3D (CoP3D), a collection of
crowd-sourced videos showing around 4,200 distinct pets. CoP3D is one of the
first large-scale datasets for benchmarking non-rigid 3D reconstruction ""in the
wild"". We also propose Tracker-NeRF, a method for learning 4D reconstruction
from our dataset. At test time, given a small number of video frames of an
unseen object, Tracker-NeRF predicts the trajectories of its 3D points and
generates new views, interpolating viewpoint and time. Results on CoP3D reveal
significantly better non-rigid new-view synthesis performance than existing
baselines.";Samarth Sinha<author:sep>Roman Shapovalov<author:sep>Jeremy Reizenstein<author:sep>Ignacio Rocco<author:sep>Natalia Neverova<author:sep>Andrea Vedaldi<author:sep>David Novotny;http://arxiv.org/pdf/2211.03889v1;cs.CV;;nerf
2211.03017v2;http://arxiv.org/abs/2211.03017v2;2022-11-06;Learning-based Inverse Rendering of Complex Indoor Scenes with  Differentiable Monte Carlo Raytracing;"Indoor scenes typically exhibit complex, spatially-varying appearance from
global illumination, making inverse rendering a challenging ill-posed problem.
This work presents an end-to-end, learning-based inverse rendering framework
incorporating differentiable Monte Carlo raytracing with importance sampling.
The framework takes a single image as input to jointly recover the underlying
geometry, spatially-varying lighting, and photorealistic materials.
Specifically, we introduce a physically-based differentiable rendering layer
with screen-space ray tracing, resulting in more realistic specular reflections
that match the input photo. In addition, we create a large-scale,
photorealistic indoor scene dataset with significantly richer details like
complex furniture and dedicated decorations. Further, we design a novel
out-of-view lighting network with uncertainty-aware refinement leveraging
hypernetwork-based neural radiance fields to predict lighting outside the view
of the input photo. Through extensive evaluations on common benchmark datasets,
we demonstrate superior inverse rendering quality of our method compared to
state-of-the-art baselines, enabling various applications such as complex
object insertion and material editing with high fidelity. Code and data will be
made available at \url{https://jingsenzhu.github.io/invrend}.";Jingsen Zhu<author:sep>Fujun Luan<author:sep>Yuchi Huo<author:sep>Zihao Lin<author:sep>Zhihua Zhong<author:sep>Dianbing Xi<author:sep>Jiaxiang Zheng<author:sep>Rui Tang<author:sep>Hujun Bao<author:sep>Rui Wang;http://arxiv.org/pdf/2211.03017v2;cs.CV;;
2211.01600v1;http://arxiv.org/abs/2211.01600v1;2022-11-03;nerf2nerf: Pairwise Registration of Neural Radiance Fields;"We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io";Lily Goli<author:sep>Daniel Rebain<author:sep>Sara Sabour<author:sep>Animesh Garg<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2211.01600v1;cs.CV;;nerf
2211.00597v1;http://arxiv.org/abs/2211.00597v1;2022-10-29;Mixed Reality Interface for Digital Twin of Plant Factory;"An easier and intuitive interface architecture is necessary for digital twin
of plant factory. I suggest an immersive and interactive mixed reality
interface for digital twin models of smart farming, for remote work rather than
simulation of components. The environment is constructed with UI display and a
streaming background scene, which is a real time scene taken from camera device
located in the plant factory, processed with deformable neural radiance fields.
User can monitor and control the remote plant factory facilities with HMD or 2D
display based mixed reality environment. This paper also introduces detailed
concept and describes the system architecture to implement suggested mixed
reality interface.";Byunghyun Ban;http://arxiv.org/pdf/2211.00597v1;cs.HC;5 pages, 7 figures;
2210.15947v2;http://arxiv.org/abs/2210.15947v2;2022-10-28;NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed  Neural Radiance Fields;"Visually exploring in a real-world 4D spatiotemporal space freely in VR has
been a long-term quest. The task is especially appealing when only a few or
even single RGB cameras are used for capturing the dynamic scene. To this end,
we present an efficient framework capable of fast reconstruction, compact
modeling, and streamable rendering. First, we propose to decompose the 4D
spatiotemporal space according to temporal characteristics. Points in the 4D
space are associated with probabilities of belonging to three categories:
static, deforming, and new areas. Each area is represented and regularized by a
separate neural field. Second, we propose a hybrid representations based
feature streaming scheme for efficiently modeling the neural fields. Our
approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single
hand-held cameras and multi-camera arrays, achieving comparable or superior
rendering performance in terms of quality and speed comparable to recent
state-of-the-art methods, achieving reconstruction in 10 seconds per frame and
interactive rendering.";Liangchen Song<author:sep>Anpei Chen<author:sep>Zhong Li<author:sep>Zhang Chen<author:sep>Lele Chen<author:sep>Junsong Yuan<author:sep>Yi Xu<author:sep>Andreas Geiger;http://arxiv.org/pdf/2210.15947v2;cs.CV;Project page: https://lsongx.github.io/projects/nerfplayer.html;nerf
2210.15107v2;http://arxiv.org/abs/2210.15107v2;2022-10-27;Boosting Point Clouds Rendering via Radiance Mapping;"Recent years we have witnessed rapid development in NeRF-based image
rendering due to its high quality. However, point clouds rendering is somehow
less explored. Compared to NeRF-based rendering which suffers from dense
spatial sampling, point clouds rendering is naturally less computation
intensive, which enables its deployment in mobile computing device. In this
work, we focus on boosting the image quality of point clouds rendering with a
compact model design. We first analyze the adaption of the volume rendering
formulation on point clouds. Based on the analysis, we simplify the NeRF
representation to a spatial mapping function which only requires single
evaluation per pixel. Further, motivated by ray marching, we rectify the the
noisy raw point clouds to the estimated intersection between rays and surfaces
as queried coordinates, which could avoid \textit{spatial frequency collapse}
and neighbor point disturbance. Composed of rasterization, spatial mapping and
the refinement stages, our method achieves the state-of-the-art performance on
point clouds rendering, outperforming prior works by notable margins, with a
smaller model size. We obtain a PSNR of 31.74 on NeRF-Synthetic, 25.88 on
ScanNet and 30.81 on DTU. Code and data are publicly available at
https://github.com/seanywang0408/RadianceMapping.";Xiaoyang Huang<author:sep>Yi Zhang<author:sep>Bingbing Ni<author:sep>Teng Li<author:sep>Kai Chen<author:sep>Wenjun Zhang;http://arxiv.org/pdf/2210.15107v2;cs.CV;"Accepted by Thirty-Seventh AAAI Conference on Artificial Intelligence
  (AAAI 2023)";nerf
2210.17415v1;http://arxiv.org/abs/2210.17415v1;2022-10-27;ProbNeRF: Uncertainty-Aware Inference of 3D Shapes from 2D Images;"The problem of inferring object shape from a single 2D image is
underconstrained. Prior knowledge about what objects are plausible can help,
but even given such prior knowledge there may still be uncertainty about the
shapes of occluded parts of objects. Recently, conditional neural radiance
field (NeRF) models have been developed that can learn to infer good point
estimates of 3D models from single 2D images. The problem of inferring
uncertainty estimates for these models has received less attention. In this
work, we propose probabilistic NeRF (ProbNeRF), a model and inference strategy
for learning probabilistic generative models of 3D objects' shapes and
appearances, and for doing posterior inference to recover those properties from
2D images. ProbNeRF is trained as a variational autoencoder, but at test time
we use Hamiltonian Monte Carlo (HMC) for inference. Given one or a few 2D
images of an object (which may be partially occluded), ProbNeRF is able not
only to accurately model the parts it sees, but also to propose realistic and
diverse hypotheses about the parts it does not see. We show that key to the
success of ProbNeRF are (i) a deterministic rendering scheme, (ii) an
annealed-HMC strategy, (iii) a hypernetwork-based decoder architecture, and
(iv) doing inference over a full set of NeRF weights, rather than just a
low-dimensional code.";Matthew D. Hoffman<author:sep>Tuan Anh Le<author:sep>Pavel Sountsov<author:sep>Christopher Suter<author:sep>Ben Lee<author:sep>Vikash K. Mansinghka<author:sep>Rif A. Saurous;http://arxiv.org/pdf/2210.17415v1;cs.CV;"18 pages, 18 figures, 1 table; submitted to the 26th International
  Conference on Artificial Intelligence and Statistics (AISTATS 2023)";nerf
2210.13041v1;http://arxiv.org/abs/2210.13041v1;2022-10-24;Learning Neural Radiance Fields from Multi-View Geometry;"We present a framework, called MVG-NeRF, that combines classical Multi-View
Geometry algorithms and Neural Radiance Fields (NeRF) for image-based 3D
reconstruction. NeRF has revolutionized the field of implicit 3D
representations, mainly due to a differentiable volumetric rendering
formulation that enables high-quality and geometry-aware novel view synthesis.
However, the underlying geometry of the scene is not explicitly constrained
during training, thus leading to noisy and incorrect results when extracting a
mesh with marching cubes. To this end, we propose to leverage pixelwise depths
and normals from a classical 3D reconstruction pipeline as geometric priors to
guide NeRF optimization. Such priors are used as pseudo-ground truth during
training in order to improve the quality of the estimated underlying surface.
Moreover, each pixel is weighted by a confidence value based on the
forward-backward reprojection error for additional robustness. Experimental
results on real-world data demonstrate the effectiveness of this approach in
obtaining clean 3D meshes from images, while maintaining competitive
performances in novel view synthesis.";Marco Orsingher<author:sep>Paolo Zani<author:sep>Paolo Medici<author:sep>Massimo Bertozzi;http://arxiv.org/pdf/2210.13041v1;cs.CV;"ECCV 2022 Workshop on ""Learning to Generate 3D Shapes and Scenes""";nerf
2210.13641v1;http://arxiv.org/abs/2210.13641v1;2022-10-24;NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields;"We propose a novel geometric and photometric 3D mapping pipeline for accurate
and real-time scene reconstruction from monocular images. To achieve this, we
leverage recent advances in dense monocular SLAM and real-time hierarchical
volumetric neural radiance fields. Our insight is that dense monocular SLAM
provides the right information to fit a neural radiance field of the scene in
real-time, by providing accurate pose estimates and depth-maps with associated
uncertainty. With our proposed uncertainty-based depth loss, we achieve not
only good photometric accuracy, but also great geometric accuracy. In fact, our
proposed pipeline achieves better geometric and photometric accuracy than
competing approaches (up to 179% better PSNR and 86% better L1 depth), while
working in real-time and using only monocular images.";Antoni Rosinol<author:sep>John J. Leonard<author:sep>Luca Carlone;http://arxiv.org/pdf/2210.13641v1;cs.CV;10 pages, 6 figures;nerf
2210.12782v1;http://arxiv.org/abs/2210.12782v1;2022-10-23;Compressing Explicit Voxel Grid Representations: fast NeRFs become also  small;"NeRFs have revolutionized the world of per-scene radiance field
reconstruction because of their intrinsic compactness. One of the main
limitations of NeRFs is their slow rendering speed, both at training and
inference time. Recent research focuses on the optimization of an explicit
voxel grid (EVG) that represents the scene, which can be paired with neural
networks to learn radiance fields. This approach significantly enhances the
speed both at train and inference time, but at the cost of large memory
occupation. In this work we propose Re:NeRF, an approach that specifically
targets EVG-NeRFs compressibility, aiming to reduce memory storage of NeRF
models while maintaining comparable performance. We benchmark our approach with
three different EVG-NeRF architectures on four popular benchmarks, showing
Re:NeRF's broad usability and effectiveness.";Chenxi Lola Deng<author:sep>Enzo Tartaglione;http://arxiv.org/pdf/2210.12782v1;cs.CV;;nerf
2210.12731v2;http://arxiv.org/abs/2210.12731v2;2022-10-23;Joint Rigid Motion Correction and Sparse-View CT via Self-Calibrating  Neural Field;"Neural Radiance Field (NeRF) has widely received attention in Sparse-View
Computed Tomography (SVCT) reconstruction tasks as a self-supervised deep
learning framework. NeRF-based SVCT methods represent the desired CT image as a
continuous function of spatial coordinates and train a Multi-Layer Perceptron
(MLP) to learn the function by minimizing loss on the SV sinogram. Benefiting
from the continuous representation provided by NeRF, the high-quality CT image
can be reconstructed. However, existing NeRF-based SVCT methods strictly
suppose there is completely no relative motion during the CT acquisition
because they require \textit{accurate} projection poses to model the X-rays
that scan the SV sinogram. Therefore, these methods suffer from severe
performance drops for real SVCT imaging with motion. In this work, we propose a
self-calibrating neural field to recover the artifacts-free image from the
rigid motion-corrupted SV sinogram without using any external data.
Specifically, we parametrize the inaccurate projection poses caused by rigid
motion as trainable variables and then jointly optimize these pose variables
and the MLP. We conduct numerical experiments on a public CT image dataset. The
results indicate our model significantly outperforms two representative
NeRF-based methods for SVCT reconstruction tasks with four different levels of
rigid motion.";Qing Wu<author:sep>Xin Li<author:sep>Hongjiang Wei<author:sep>Jingyi Yu<author:sep>Yuyao Zhang;http://arxiv.org/pdf/2210.12731v2;eess.IV;5 pages;nerf
2210.12268v1;http://arxiv.org/abs/2210.12268v1;2022-10-21;An Exploration of Neural Radiance Field Scene Reconstruction: Synthetic,  Real-world and Dynamic Scenes;"This project presents an exploration into 3D scene reconstruction of
synthetic and real-world scenes using Neural Radiance Field (NeRF) approaches.
We primarily take advantage of the reduction in training and rendering time of
neural graphic primitives multi-resolution hash encoding, to reconstruct static
video game scenes and real-world scenes, comparing and observing reconstruction
detail and limitations. Additionally, we explore dynamic scene reconstruction
using Neural Radiance Fields for Dynamic Scenes(D-NeRF). Finally, we extend the
implementation of D-NeRF, originally constrained to handle synthetic scenes to
also handle real-world dynamic scenes.";Benedict Quartey<author:sep>Tuluhan Akbulut<author:sep>Wasiwasi Mgonzo<author:sep>Zheng Xin Yong;http://arxiv.org/pdf/2210.12268v1;cs.CV;;nerf
2210.12126v3;http://arxiv.org/abs/2210.12126v3;2022-10-21;One-Shot Neural Fields for 3D Object Understanding;"We present a unified and compact scene representation for robotics, where
each object in the scene is depicted by a latent code capturing geometry and
appearance. This representation can be decoded for various tasks such as novel
view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or
voxel maps), collision checking, and stable grasp prediction. We build our
representation from a single RGB input image at test time by leveraging recent
advances in Neural Radiance Fields (NeRF) that learn category-level priors on
large multiview datasets, then fine-tune on novel objects from one or few
views. We expand the NeRF model for additional grasp outputs and explore ways
to leverage this representation for robotics. At test-time, we build the
representation from a single RGB input image observing the scene from only one
viewpoint. We find that the recovered representation allows rendering from
novel views, including of occluded object parts, and also for predicting
successful stable grasps. Grasp poses can be directly decoded from our latent
representation with an implicit grasp decoder. We experimented in both
simulation and real world and demonstrated the capability for robust robotic
grasping using such compact representation. Website:
https://nerfgrasp.github.io";Valts Blukis<author:sep>Taeyeop Lee<author:sep>Jonathan Tremblay<author:sep>Bowen Wen<author:sep>In So Kweon<author:sep>Kuk-Jin Yoon<author:sep>Dieter Fox<author:sep>Stan Birchfield;http://arxiv.org/pdf/2210.12126v3;cs.RO;"IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshop (CVPRW) on XRNeRF: Advances in NeRF for the Metaverse 2023";nerf
2210.11668v2;http://arxiv.org/abs/2210.11668v2;2022-10-21;RGB-Only Reconstruction of Tabletop Scenes for Collision-Free  Manipulator Control;"We present a system for collision-free control of a robot manipulator that
uses only RGB views of the world. Perceptual input of a tabletop scene is
provided by multiple images of an RGB camera (without depth) that is either
handheld or mounted on the robot end effector. A NeRF-like process is used to
reconstruct the 3D geometry of the scene, from which the Euclidean full signed
distance function (ESDF) is computed. A model predictive control algorithm is
then used to control the manipulator to reach a desired pose while avoiding
obstacles in the ESDF. We show results on a real dataset collected and
annotated in our lab.";Zhenggang Tang<author:sep>Balakumar Sundaralingam<author:sep>Jonathan Tremblay<author:sep>Bowen Wen<author:sep>Ye Yuan<author:sep>Stephen Tyree<author:sep>Charles Loop<author:sep>Alexander Schwing<author:sep>Stan Birchfield;http://arxiv.org/pdf/2210.11668v2;cs.RO;ICRA 2023. Project page at https://ngp-mpc.github.io/;nerf
2210.12003v2;http://arxiv.org/abs/2210.12003v2;2022-10-21;HDHumans: A Hybrid Approach for High-fidelity Digital Humans;"Photo-real digital human avatars are of enormous importance in graphics, as
they enable immersive communication over the globe, improve gaming and
entertainment experiences, and can be particularly beneficial for AR and VR
settings. However, current avatar generation approaches either fall short in
high-fidelity novel view synthesis, generalization to novel motions,
reproduction of loose clothing, or they cannot render characters at the high
resolution offered by modern displays. To this end, we propose HDHumans, which
is the first method for HD human character synthesis that jointly produces an
accurate and temporally coherent 3D deforming surface and highly
photo-realistic images of arbitrary novel views and of motions not seen at
training time. At the technical core, our method tightly integrates a classical
deforming character template with neural radiance fields (NeRF). Our method is
carefully designed to achieve a synergy between classical surface deformation
and NeRF. First, the template guides the NeRF, which allows synthesizing novel
views of a highly dynamic and articulated character and even enables the
synthesis of novel motions. Second, we also leverage the dense pointclouds
resulting from NeRF to further improve the deforming surface via 3D-to-3D
supervision. We outperform the state of the art quantitatively and
qualitatively in terms of synthesis quality and resolution, as well as the
quality of 3D surface reconstruction.";Marc Habermann<author:sep>Lingjie Liu<author:sep>Weipeng Xu<author:sep>Gerard Pons-Moll<author:sep>Michael Zollhoefer<author:sep>Christian Theobalt;http://arxiv.org/pdf/2210.12003v2;cs.CV;;nerf
2210.11170v2;http://arxiv.org/abs/2210.11170v2;2022-10-20;Coordinates Are NOT Lonely -- Codebook Prior Helps Implicit Neural 3D  Representations;"Implicit neural 3D representation has achieved impressive results in surface
or scene reconstruction and novel view synthesis, which typically uses the
coordinate-based multi-layer perceptrons (MLPs) to learn a continuous scene
representation. However, existing approaches, such as Neural Radiance Field
(NeRF) and its variants, usually require dense input views (i.e. 50-150) to
obtain decent results. To relive the over-dependence on massive calibrated
images and enrich the coordinate-based feature representation, we explore
injecting the prior information into the coordinate-based network and introduce
a novel coordinate-based model, CoCo-INR, for implicit neural 3D
representation. The cores of our method are two attention modules: codebook
attention and coordinate attention. The former extracts the useful prototypes
containing rich geometry and appearance information from the prior codebook,
and the latter propagates such prior information into each coordinate and
enriches its feature representation for a scene or object surface. With the
help of the prior information, our method can render 3D views with more
photo-realistic appearance and geometries than the current methods using fewer
calibrated images available. Experiments on various scene reconstruction
datasets, including DTU and BlendedMVS, and the full 3D head reconstruction
dataset, H3DS, demonstrate the robustness under fewer input views and fine
detail-preserving capability of our proposed method.";Fukun Yin<author:sep>Wen Liu<author:sep>Zilong Huang<author:sep>Pei Cheng<author:sep>Tao Chen<author:sep>Gang YU;http://arxiv.org/pdf/2210.11170v2;cs.CV;NeurIPS 2022;nerf
2210.10108v2;http://arxiv.org/abs/2210.10108v2;2022-10-18;Parallel Inversion of Neural Radiance Fields for Robust Pose Estimation;"We present a parallelized optimization method based on fast Neural Radiance
Fields (NeRF) for estimating 6-DoF pose of a camera with respect to an object
or scene. Given a single observed RGB image of the target, we can predict the
translation and rotation of the camera by minimizing the residual between
pixels rendered from a fast NeRF model and pixels in the observed image. We
integrate a momentum-based camera extrinsic optimization procedure into Instant
Neural Graphics Primitives, a recent exceptionally fast NeRF implementation. By
introducing parallel Monte Carlo sampling into the pose estimation task, our
method overcomes local minima and improves efficiency in a more extensive
search space. We also show the importance of adopting a more robust pixel-based
loss function to reduce error. Experiments demonstrate that our method can
achieve improved generalization and robustness on both synthetic and real-world
benchmarks.";Yunzhi Lin<author:sep>Thomas Müller<author:sep>Jonathan Tremblay<author:sep>Bowen Wen<author:sep>Stephen Tyree<author:sep>Alex Evans<author:sep>Patricio A. Vela<author:sep>Stan Birchfield;http://arxiv.org/pdf/2210.10108v2;cs.CV;ICRA 2023. Project page at https://pnerfp.github.io/;nerf
2210.10036v1;http://arxiv.org/abs/2210.10036v1;2022-10-18;ARAH: Animatable Volume Rendering of Articulated Human SDFs;"Combining human body models with differentiable rendering has recently
enabled animatable avatars of clothed humans from sparse sets of multi-view RGB
videos. While state-of-the-art approaches achieve realistic appearance with
neural radiance fields (NeRF), the inferred geometry often lacks detail due to
missing geometric constraints. Further, animating avatars in
out-of-distribution poses is not yet possible because the mapping from
observation space to canonical space does not generalize faithfully to unseen
poses. In this work, we address these shortcomings and propose a model to
create animatable clothed human avatars with detailed geometry that generalize
well to out-of-distribution poses. To achieve detailed geometry, we combine an
articulated implicit surface representation with volume rendering. For
generalization, we propose a novel joint root-finding algorithm for
simultaneous ray-surface intersection search and correspondence search. Our
algorithm enables efficient point sampling and accurate point canonicalization
while generalizing well to unseen poses. We demonstrate that our proposed
pipeline can generate clothed avatars with high-quality pose-dependent geometry
and appearance from a sparse set of multi-view RGB videos. Our method achieves
state-of-the-art performance on geometry and appearance reconstruction while
creating animatable avatars that generalize well to out-of-distribution poses
beyond the small number of training poses.";Shaofei Wang<author:sep>Katja Schwarz<author:sep>Andreas Geiger<author:sep>Siyu Tang;http://arxiv.org/pdf/2210.10036v1;cs.CV;"Accepted to ECCV 2022. Project page:
  https://neuralbodies.github.io/arah/";nerf
2210.09420v3;http://arxiv.org/abs/2210.09420v3;2022-10-17;Differentiable Physics Simulation of Dynamics-Augmented Neural Objects;"We present a differentiable pipeline for simulating the motion of objects
that represent their geometry as a continuous density field parameterized as a
deep network. This includes Neural Radiance Fields (NeRFs), and other related
models. From the density field, we estimate the dynamical properties of the
object, including its mass, center of mass, and inertia matrix. We then
introduce a differentiable contact model based on the density field for
computing normal and friction forces resulting from collisions. This allows a
robot to autonomously build object models that are visually and
\emph{dynamically} accurate from still images and videos of objects in motion.
The resulting Dynamics-Augmented Neural Objects (DANOs) are simulated with an
existing differentiable simulation engine, Dojo, interacting with other
standard simulation objects, such as spheres, planes, and robots specified as
URDFs. A robot can use this simulation to optimize grasps and manipulation
trajectories of neural objects, or to improve the neural object models through
gradient-based real-to-simulation transfer. We demonstrate the pipeline to
learn the coefficient of friction of a bar of soap from a real video of the
soap sliding on a table. We also learn the coefficient of friction and mass of
a Stanford bunny through interactions with a Panda robot arm from synthetic
data, and we optimize trajectories in simulation for the Panda arm to push the
bunny to a goal location.";Simon Le Cleac'h<author:sep>Hong-Xing Yu<author:sep>Michelle Guo<author:sep>Taylor A. Howell<author:sep>Ruohan Gao<author:sep>Jiajun Wu<author:sep>Zachary Manchester<author:sep>Mac Schwager;http://arxiv.org/pdf/2210.09420v3;cs.RO;;nerf
2210.08202v2;http://arxiv.org/abs/2210.08202v2;2022-10-15;IBL-NeRF: Image-Based Lighting Formulation of Neural Radiance Fields;"We propose IBL-NeRF, which decomposes the neural radiance fields (NeRF) of
large-scale indoor scenes into intrinsic components. Recent approaches further
decompose the baked radiance of the implicit volume into intrinsic components
such that one can partially approximate the rendering equation. However, they
are limited to representing isolated objects with a shared environment
lighting, and suffer from computational burden to aggregate rays with Monte
Carlo integration. In contrast, our prefiltered radiance field extends the
original NeRF formulation to capture the spatial variation of lighting within
the scene volume, in addition to surface properties. Specifically, the scenes
of diverse materials are decomposed into intrinsic components for rendering,
namely, albedo, roughness, surface normal, irradiance, and prefiltered
radiance. All of the components are inferred as neural images from MLP, which
can model large-scale general scenes. Especially the prefiltered radiance
effectively models the volumetric light field, and captures spatial variation
beyond a single environment light. The prefiltering aggregates rays in a set of
predefined neighborhood sizes such that we can replace the costly Monte Carlo
integration of global illumination with a simple query from a neural image. By
adopting NeRF, our approach inherits superior visual quality and multi-view
consistency for synthesized images as well as the intrinsic components. We
demonstrate the performance on scenes with complex object layouts and light
configurations, which could not be processed in any of the previous works.";Changwoon Choi<author:sep>Juhyeon Kim<author:sep>Young Min Kim;http://arxiv.org/pdf/2210.08202v2;cs.CV;Computer Graphics Forum (Pacific Graphics 2023);nerf
2210.08398v3;http://arxiv.org/abs/2210.08398v3;2022-10-15;SPIDR: SDF-based Neural Point Fields for Illumination and Deformation;"Neural radiance fields (NeRFs) have recently emerged as a promising approach
for 3D reconstruction and novel view synthesis. However, NeRF-based methods
encode shape, reflectance, and illumination implicitly and this makes it
challenging for users to manipulate these properties in the rendered images
explicitly. Existing approaches only enable limited editing of the scene and
deformation of the geometry. Furthermore, no existing work enables accurate
scene illumination after object deformation. In this work, we introduce SPIDR,
a new hybrid neural SDF representation. SPIDR combines point cloud and neural
implicit representations to enable the reconstruction of higher quality object
surfaces for geometry deformation and lighting estimation. meshes and surfaces
for object deformation and lighting estimation. To more accurately capture
environment illumination for scene relighting, we propose a novel neural
implicit model to learn environment light. To enable more accurate illumination
updates after deformation, we use the shadow mapping technique to approximate
the light visibility updates caused by geometry editing. We demonstrate the
effectiveness of SPIDR in enabling high quality geometry editing with more
accurate updates to the illumination of the scene.";Ruofan Liang<author:sep>Jiahao Zhang<author:sep>Haoda Li<author:sep>Chen Yang<author:sep>Yushi Guan<author:sep>Nandita Vijaykumar;http://arxiv.org/pdf/2210.08398v3;cs.CV;Project page: https://nexuslrf.github.io/SPIDR_webpage/;nerf
2210.07301v2;http://arxiv.org/abs/2210.07301v2;2022-10-13;3D GAN Inversion with Pose Optimization;"With the recent advances in NeRF-based 3D aware GANs quality, projecting an
image into the latent space of these 3D-aware GANs has a natural advantage over
2D GAN inversion: not only does it allow multi-view consistent editing of the
projected image, but it also enables 3D reconstruction and novel view synthesis
when given only a single image. However, the explicit viewpoint control acts as
a main hindrance in the 3D GAN inversion process, as both camera pose and
latent code have to be optimized simultaneously to reconstruct the given image.
Most works that explore the latent space of the 3D-aware GANs rely on
ground-truth camera viewpoint or deformable 3D model, thus limiting their
applicability. In this work, we introduce a generalizable 3D GAN inversion
method that infers camera viewpoint and latent code simultaneously to enable
multi-view consistent semantic image editing. The key to our approach is to
leverage pre-trained estimators for better initialization and utilize the
pixel-wise depth calculated from NeRF parameters to better reconstruct the
given image. We conduct extensive experiments on image reconstruction and
editing both quantitatively and qualitatively, and further compare our results
with 2D GAN-based editing to demonstrate the advantages of utilizing the latent
space of 3D GANs. Additional results and visualizations are available at
https://3dgan-inversion.github.io .";Jaehoon Ko<author:sep>Kyusun Cho<author:sep>Daewon Choi<author:sep>Kwangrok Ryoo<author:sep>Seungryong Kim;http://arxiv.org/pdf/2210.07301v2;cs.CV;Project Page: https://3dgan-inversion.github.io;nerf
2210.07181v2;http://arxiv.org/abs/2210.07181v2;2022-10-13;MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without  Camera Pose;"We propose a generalizable neural radiance fields - MonoNeRF, that can be
trained on large-scale monocular videos of moving in static scenes without any
ground-truth annotations of depth and camera poses. MonoNeRF follows an
Autoencoder-based architecture, where the encoder estimates the monocular depth
and the camera pose, and the decoder constructs a Multiplane NeRF
representation based on the depth encoder feature, and renders the input frames
with the estimated camera. The learning is supervised by the reconstruction
error. Once the model is learned, it can be applied to multiple applications
including depth estimation, camera pose estimation, and single-image novel view
synthesis. More qualitative results are available at:
https://oasisyang.github.io/mononerf .";Yang Fu<author:sep>Ishan Misra<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2210.07181v2;cs.CV;"ICML 2023 camera ready version. Project page:
  https://oasisyang.github.io/mononerf";nerf
2210.06108v1;http://arxiv.org/abs/2210.06108v1;2022-10-12;Reconstructing Personalized Semantic Facial NeRF Models From Monocular  Video;"We present a novel semantic model for human head defined with neural radiance
field. The 3D-consistent head model consist of a set of disentangled and
interpretable bases, and can be driven by low-dimensional expression
coefficients. Thanks to the powerful representation ability of neural radiance
field, the constructed model can represent complex facial attributes including
hair, wearings, which can not be represented by traditional mesh blendshape. To
construct the personalized semantic facial model, we propose to define the
bases as several multi-level voxel fields. With a short monocular RGB video as
input, our method can construct the subject's semantic facial NeRF model with
only ten to twenty minutes, and can render a photo-realistic human head image
in tens of miliseconds with a given expression coefficient and view direction.
With this novel representation, we apply it to many tasks like facial
retargeting and expression editing. Experimental results demonstrate its strong
representation ability and training/inference speed. Demo videos and released
code are provided in our project page:
https://ustc3dv.github.io/NeRFBlendShape/";Xuan Gao<author:sep>Chenglai Zhong<author:sep>Jun Xiang<author:sep>Yang Hong<author:sep>Yudong Guo<author:sep>Juyong Zhang;http://arxiv.org/pdf/2210.06108v1;cs.GR;"Accepted by SIGGRAPH Asia 2022 (Journal Track). Project page:
  https://ustc3dv.github.io/NeRFBlendShape/";nerf
2210.06575v3;http://arxiv.org/abs/2210.06575v3;2022-10-12;GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and  Specular Objects Using Generalizable NeRF;"In this work, we tackle 6-DoF grasp detection for transparent and specular
objects, which is an important yet challenging problem in vision-based robotic
systems, due to the failure of depth cameras in sensing their geometry. We, for
the first time, propose a multiview RGB-based 6-DoF grasp detection network,
GraspNeRF, that leverages the generalizable neural radiance field (NeRF) to
achieve material-agnostic object grasping in clutter. Compared to the existing
NeRF-based 3-DoF grasp detection methods that rely on densely captured input
images and time-consuming per-scene optimization, our system can perform
zero-shot NeRF construction with sparse RGB inputs and reliably detect 6-DoF
grasps, both in real-time. The proposed framework jointly learns generalizable
NeRF and grasp detection in an end-to-end manner, optimizing the scene
representation construction for the grasping. For training data, we generate a
large-scale photorealistic domain-randomized synthetic dataset of grasping in
cluttered tabletop scenes that enables direct transfer to the real world. Our
extensive experiments in synthetic and real-world environments demonstrate that
our method significantly outperforms all the baselines in all the experiments
while remaining in real-time. Project page can be found at
https://pku-epic.github.io/GraspNeRF";Qiyu Dai<author:sep>Yan Zhu<author:sep>Yiran Geng<author:sep>Ciyu Ruan<author:sep>Jiazhao Zhang<author:sep>He Wang;http://arxiv.org/pdf/2210.06575v3;cs.RO;IEEE International Conference on Robotics and Automation (ICRA), 2023;nerf
2210.05135v1;http://arxiv.org/abs/2210.05135v1;2022-10-11;X-NeRF: Explicit Neural Radiance Field for Multi-Scene 360$^{\circ} $  Insufficient RGB-D Views;"Neural Radiance Fields (NeRFs), despite their outstanding performance on
novel view synthesis, often need dense input views. Many papers train one model
for each scene respectively and few of them explore incorporating multi-modal
data into this problem. In this paper, we focus on a rarely discussed but
important setting: can we train one model that can represent multiple scenes,
with 360$^\circ $ insufficient views and RGB-D images? We refer insufficient
views to few extremely sparse and almost non-overlapping views. To deal with
it, X-NeRF, a fully explicit approach which learns a general scene completion
process instead of a coordinate-based mapping, is proposed. Given a few
insufficient RGB-D input views, X-NeRF first transforms them to a sparse point
cloud tensor and then applies a 3D sparse generative Convolutional Neural
Network (CNN) to complete it to an explicit radiance field whose volumetric
rendering can be conducted fast without running networks during inference. To
avoid overfitting, besides common rendering loss, we apply perceptual loss as
well as view augmentation through random rotation on point clouds. The proposed
methodology significantly out-performs previous implicit methods in our
setting, indicating the great potential of proposed problem and approach. Codes
and data are available at https://github.com/HaoyiZhu/XNeRF.";Haoyi Zhu<author:sep>Hao-Shu Fang<author:sep>Cewu Lu;http://arxiv.org/pdf/2210.05135v1;cs.CV;;nerf
2210.04888v1;http://arxiv.org/abs/2210.04888v1;2022-10-10;EVA3D: Compositional 3D Human Generation from 2D Image Collections;"Inverse graphics aims to recover 3D models from 2D observations. Utilizing
differentiable rendering, recent 3D-aware generative models have shown
impressive results of rigid object generation using 2D images. However, it
remains challenging to generate articulated objects, like human bodies, due to
their complexity and diversity in poses and appearances. In this work, we
propose, EVA3D, an unconditional 3D human generative model learned from 2D
image collections only. EVA3D can sample 3D humans with detailed geometry and
render high-quality images (up to 512x256) without bells and whistles (e.g.
super resolution). At the core of EVA3D is a compositional human NeRF
representation, which divides the human body into local parts. Each part is
represented by an individual volume. This compositional representation enables
1) inherent human priors, 2) adaptive allocation of network parameters, 3)
efficient training and rendering. Moreover, to accommodate for the
characteristics of sparse 2D human image collections (e.g. imbalanced pose
distribution), we propose a pose-guided sampling strategy for better GAN
learning. Extensive experiments validate that EVA3D achieves state-of-the-art
3D human generation performance regarding both geometry and texture quality.
Notably, EVA3D demonstrates great potential and scalability to
""inverse-graphics"" diverse human bodies with a clean framework.";Fangzhou Hong<author:sep>Zhaoxi Chen<author:sep>Yushi Lan<author:sep>Liang Pan<author:sep>Ziwei Liu;http://arxiv.org/pdf/2210.04888v1;cs.CV;Project Page at https://hongfz16.github.io/projects/EVA3D.html;nerf
2210.04932v1;http://arxiv.org/abs/2210.04932v1;2022-10-10;NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills  using Neural Radiance Fields;"We present a system for applying sim2real approaches to ""in the wild"" scenes
with realistic visuals, and to policies which rely on active perception using
RGB cameras. Given a short video of a static scene collected using a generic
phone, we learn the scene's contact geometry and a function for novel view
synthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering
of the static scene by overlaying the rendering of other dynamic objects (e.g.
the robot's own body, a ball). A simulation is then created using the rendering
engine in a physics simulator which computes contact dynamics from the static
scene geometry (estimated from the NeRF volume density) and the dynamic
objects' geometry and physical properties (assumed known). We demonstrate that
we can use this simulation to learn vision-based whole body navigation and ball
pushing policies for a 20 degrees of freedom humanoid robot with an actuated
head-mounted RGB camera, and we successfully transfer these policies to a real
robot. Project video is available at
https://sites.google.com/view/nerf2real/home";Arunkumar Byravan<author:sep>Jan Humplik<author:sep>Leonard Hasenclever<author:sep>Arthur Brussee<author:sep>Francesco Nori<author:sep>Tuomas Haarnoja<author:sep>Ben Moran<author:sep>Steven Bohez<author:sep>Fereshteh Sadeghi<author:sep>Bojan Vujatovic<author:sep>Nicolas Heess;http://arxiv.org/pdf/2210.04932v1;cs.RO;;nerf
2210.04553v1;http://arxiv.org/abs/2210.04553v1;2022-10-10;SiNeRF: Sinusoidal Neural Radiance Fields for Joint Pose Estimation and  Scene Reconstruction;"NeRFmm is the Neural Radiance Fields (NeRF) that deal with Joint Optimization
tasks, i.e., reconstructing real-world scenes and registering camera parameters
simultaneously. Despite NeRFmm producing precise scene synthesis and pose
estimations, it still struggles to outperform the full-annotated baseline on
challenging scenes. In this work, we identify that there exists a systematic
sub-optimality in joint optimization and further identify multiple potential
sources for it. To diminish the impacts of potential sources, we propose
Sinusoidal Neural Radiance Fields (SiNeRF) that leverage sinusoidal activations
for radiance mapping and a novel Mixed Region Sampling (MRS) for selecting ray
batch efficiently. Quantitative and qualitative results show that compared to
NeRFmm, SiNeRF achieves comprehensive significant improvements in image
synthesis quality and pose estimation accuracy. Codes are available at
https://github.com/yitongx/sinerf.";Yitong Xia<author:sep>Hao Tang<author:sep>Radu Timofte<author:sep>Luc Van Gool;http://arxiv.org/pdf/2210.04553v1;cs.CV;Accepted yet not published by BMVC2022;nerf
2210.04847v3;http://arxiv.org/abs/2210.04847v3;2022-10-10;NerfAcc: A General NeRF Acceleration Toolbox;"We propose NerfAcc, a toolbox for efficient volumetric rendering of radiance
fields. We build on the techniques proposed in Instant-NGP, and extend these
techniques to not only support bounded static scenes, but also for dynamic
scenes and unbounded scenes. NerfAcc comes with a user-friendly Python API, and
is ready for plug-and-play acceleration of most NeRFs. Various examples are
provided to show how to use this toolbox. Code can be found here:
https://github.com/KAIR-BAIR/nerfacc. Note this write-up matches with NerfAcc
v0.3.5. For the latest features in NerfAcc, please check out our more recent
write-up at arXiv:2305.04966";Ruilong Li<author:sep>Matthew Tancik<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2210.04847v3;cs.CV;"Webpage: https://www.nerfacc.com/; Updated Write-up: arXiv:2305.04966";nerf
2210.04127v1;http://arxiv.org/abs/2210.04127v1;2022-10-09;Towards Efficient Neural Scene Graphs by Learning Consistency Fields;"Neural Radiance Fields (NeRF) achieves photo-realistic image rendering from
novel views, and the Neural Scene Graphs (NSG) \cite{ost2021neural} extends it
to dynamic scenes (video) with multiple objects. Nevertheless, computationally
heavy ray marching for every image frame becomes a huge burden. In this paper,
taking advantage of significant redundancy across adjacent frames in videos, we
propose a feature-reusing framework. From the first try of naively reusing the
NSG features, however, we learn that it is crucial to disentangle
object-intrinsic properties consistent across frames from transient ones. Our
proposed method, \textit{Consistency-Field-based NSG (CF-NSG)}, reformulates
neural radiance fields to additionally consider \textit{consistency fields}.
With disentangled representations, CF-NSG takes full advantage of the
feature-reusing scheme and performs an extended degree of scene manipulation in
a more controllable manner. We empirically verify that CF-NSG greatly improves
the inference efficiency by using 85\% less queries than NSG without notable
degradation in rendering quality. Code will be available at:
https://github.com/ldynx/CF-NSG";Yeji Song<author:sep>Chaerin Kong<author:sep>Seoyoung Lee<author:sep>Nojun Kwak<author:sep>Joonseok Lee;http://arxiv.org/pdf/2210.04127v1;cs.CV;BMVC 2022, 22 pages;nerf
2210.04217v1;http://arxiv.org/abs/2210.04217v1;2022-10-09;Estimating Neural Reflectance Field from Radiance Field using Tree  Structures;"We present a new method for estimating the Neural Reflectance Field (NReF) of
an object from a set of posed multi-view images under unknown lighting. NReF
represents 3D geometry and appearance of objects in a disentangled manner, and
are hard to be estimated from images only. Our method solves this problem by
exploiting the Neural Radiance Field (NeRF) as a proxy representation, from
which we perform further decomposition. A high-quality NeRF decomposition
relies on good geometry information extraction as well as good prior terms to
properly resolve ambiguities between different components. To extract
high-quality geometry information from radiance fields, we re-design a new
ray-casting based method for surface point extraction. To efficiently compute
and apply prior terms, we convert different prior terms into different type of
filter operations on the surface extracted from radiance field. We then employ
two type of auxiliary data structures, namely Gaussian KD-tree and octree, to
support fast querying of surface points and efficient computation of surface
filters during training. Based on this, we design a multi-stage decomposition
optimization pipeline for estimating neural reflectance field from neural
radiance fields. Extensive experiments show our method outperforms other
state-of-the-art methods on different data, and enable high-quality free-view
relighting as well as material editing tasks.";Xiu Li<author:sep>Xiao Li<author:sep>Yan Lu;http://arxiv.org/pdf/2210.04217v1;cs.CV;;nerf
2210.04214v2;http://arxiv.org/abs/2210.04214v2;2022-10-09;VM-NeRF: Tackling Sparsity in NeRF with View Morphing;"NeRF aims to learn a continuous neural scene representation by using a finite
set of input images taken from various viewpoints. A well-known limitation of
NeRF methods is their reliance on data: the fewer the viewpoints, the higher
the likelihood of overfitting. This paper addresses this issue by introducing a
novel method to generate geometrically consistent image transitions between
viewpoints using View Morphing. Our VM-NeRF approach requires no prior
knowledge about the scene structure, as View Morphing is based on the
fundamental principles of projective geometry. VM-NeRF tightly integrates this
geometric view generation process during the training procedure of standard
NeRF approaches. Notably, our method significantly improves novel view
synthesis, particularly when only a few views are available. Experimental
evaluation reveals consistent improvement over current methods that handle
sparse viewpoints in NeRF models. We report an increase in PSNR of up to 1.8dB
and 1.0dB when training uses eight and four views, respectively. Source code:
\url{https://github.com/mbortolon97/VM-NeRF}";Matteo Bortolon<author:sep>Alessio Del Bue<author:sep>Fabio Poiesi;http://arxiv.org/pdf/2210.04214v2;cs.CV;ICIAP 2023;nerf
2210.04233v1;http://arxiv.org/abs/2210.04233v1;2022-10-09;Robustifying the Multi-Scale Representation of Neural Radiance Fields;"Neural Radiance Fields (NeRF) recently emerged as a new paradigm for object
representation from multi-view (MV) images. Yet, it cannot handle multi-scale
(MS) images and camera pose estimation errors, which generally is the case with
multi-view images captured from a day-to-day commodity camera. Although
recently proposed Mip-NeRF could handle multi-scale imaging problems with NeRF,
it cannot handle camera pose estimation error. On the other hand, the newly
proposed BARF can solve the camera pose problem with NeRF but fails if the
images are multi-scale in nature. This paper presents a robust multi-scale
neural radiance fields representation approach to simultaneously overcome both
real-world imaging issues. Our method handles multi-scale imaging effects and
camera-pose estimation problems with NeRF-inspired approaches by leveraging the
fundamentals of scene rigidity. To reduce unpleasant aliasing artifacts due to
multi-scale images in the ray space, we leverage Mip-NeRF multi-scale
representation. For joint estimation of robust camera pose, we propose
graph-neural network-based multiple motion averaging in the neural volume
rendering framework. We demonstrate, with examples, that for an accurate neural
representation of an object from day-to-day acquired multi-view images, it is
crucial to have precise camera-pose estimates. Without considering robustness
measures in the camera pose estimation, modeling for multi-scale aliasing
artifacts via conical frustum can be counterproductive. We present extensive
experiments on the benchmark datasets to demonstrate that our approach provides
better results than the recent NeRF-inspired approaches for such realistic
settings.";Nishant Jain<author:sep>Suryansh Kumar<author:sep>Luc Van Gool;http://arxiv.org/pdf/2210.04233v1;cs.CV;"Accepted for publication at British Machine Vision Conference (BMVC)
  2022. Draft info: 13 pages, 3 Figures, and 4 Tables";nerf
2210.03895v1;http://arxiv.org/abs/2210.03895v1;2022-10-08;ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial  Viewpoints;"Recent studies have demonstrated that visual recognition models lack
robustness to distribution shift. However, current work mainly considers model
robustness to 2D image transformations, leaving viewpoint changes in the 3D
world less explored. In general, viewpoint changes are prevalent in various
real-world applications (e.g., autonomous driving), making it imperative to
evaluate viewpoint robustness. In this paper, we propose a novel method called
ViewFool to find adversarial viewpoints that mislead visual recognition models.
By encoding real-world objects as neural radiance fields (NeRF), ViewFool
characterizes a distribution of diverse adversarial viewpoints under an
entropic regularizer, which helps to handle the fluctuations of the real camera
pose and mitigate the reality gap between the real objects and their neural
representations. Experiments validate that the common image classifiers are
extremely vulnerable to the generated adversarial viewpoints, which also
exhibit high cross-model transferability. Based on ViewFool, we introduce
ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint
robustness of image classifiers. Evaluation results on 40 classifiers with
diverse architectures, objective functions, and data augmentations reveal a
significant drop in model performance when tested on ImageNet-V, which provides
a possibility to leverage ViewFool as an effective data augmentation strategy
to improve viewpoint robustness.";Yinpeng Dong<author:sep>Shouwei Ruan<author:sep>Hang Su<author:sep>Caixin Kang<author:sep>Xingxing Wei<author:sep>Jun Zhu;http://arxiv.org/pdf/2210.03895v1;cs.CV;NeurIPS 2022;nerf
2210.01651v1;http://arxiv.org/abs/2210.01651v1;2022-10-04;SelfNeRF: Fast Training NeRF for Human from Monocular Self-rotating  Video;"In this paper, we propose SelfNeRF, an efficient neural radiance field based
novel view synthesis method for human performance. Given monocular
self-rotating videos of human performers, SelfNeRF can train from scratch and
achieve high-fidelity results in about twenty minutes. Some recent works have
utilized the neural radiance field for dynamic human reconstruction. However,
most of these methods need multi-view inputs and require hours of training,
making it still difficult for practical use. To address this challenging
problem, we introduce a surface-relative representation based on
multi-resolution hash encoding that can greatly improve the training speed and
aggregate inter-frame information. Extensive experimental results on several
different datasets demonstrate the effectiveness and efficiency of SelfNeRF to
challenging monocular videos.";Bo Peng<author:sep>Jun Hu<author:sep>Jingtao Zhou<author:sep>Juyong Zhang;http://arxiv.org/pdf/2210.01651v1;cs.CV;Project page: https://ustc3dv.github.io/SelfNeRF;nerf
2210.01868v1;http://arxiv.org/abs/2210.01868v1;2022-10-04;Capturing and Animation of Body and Clothing from Monocular Video;"While recent work has shown progress on extracting clothed 3D human avatars
from a single image, video, or a set of 3D scans, several limitations remain.
Most methods use a holistic representation to jointly model the body and
clothing, which means that the clothing and body cannot be separated for
applications like virtual try-on. Other methods separately model the body and
clothing, but they require training from a large set of 3D clothed human meshes
obtained from 3D/4D scanners or physics simulations. Our insight is that the
body and clothing have different modeling requirements. While the body is well
represented by a mesh-based parametric 3D model, implicit representations and
neural radiance fields are better suited to capturing the large variety in
shape and appearance present in clothing. Building on this insight, we propose
SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a
mesh-based body with a neural radiance field. Integrating the mesh into the
volumetric rendering in combination with a differentiable rasterizer enables us
to optimize SCARF directly from monocular videos, without any 3D supervision.
The hybrid modeling enables SCARF to (i) animate the clothed body avatar by
changing body poses (including hand articulation and facial expressions), (ii)
synthesize novel views of the avatar, and (iii) transfer clothing between
avatars in virtual try-on applications. We demonstrate that SCARF reconstructs
clothing with higher visual quality than existing methods, that the clothing
deforms with changing body pose and body shape, and that clothing can be
successfully transferred between avatars of different subjects. The code and
models are available at https://github.com/YadiraF/SCARF.";Yao Feng<author:sep>Jinlong Yang<author:sep>Marc Pollefeys<author:sep>Michael J. Black<author:sep>Timo Bolkart;http://arxiv.org/pdf/2210.01868v1;cs.CV;7 pages main paper, 2 pages supp. mat;
2210.01166v1;http://arxiv.org/abs/2210.01166v1;2022-10-03;NARF22: Neural Articulated Radiance Fields for Configuration-Aware  Rendering;"Articulated objects pose a unique challenge for robotic perception and
manipulation. Their increased number of degrees-of-freedom makes tasks such as
localization computationally difficult, while also making the process of
real-world dataset collection unscalable. With the aim of addressing these
scalability issues, we propose Neural Articulated Radiance Fields (NARF22), a
pipeline which uses a fully-differentiable, configuration-parameterized Neural
Radiance Field (NeRF) as a means of providing high quality renderings of
articulated objects. NARF22 requires no explicit knowledge of the object
structure at inference time. We propose a two-stage parts-based training
mechanism which allows the object rendering models to generalize well across
the configuration space even if the underlying training data has as few as one
configuration represented. We demonstrate the efficacy of NARF22 by training
configurable renderers on a real-world articulated tool dataset collected via a
Fetch mobile manipulation robot. We show the applicability of the model to
gradient-based inference methods through a configuration estimation and 6
degree-of-freedom pose refinement task. The project webpage is available at:
https://progress.eecs.umich.edu/projects/narf/.";Stanley Lewis<author:sep>Jana Pavlasek<author:sep>Odest Chadwicke Jenkins;http://arxiv.org/pdf/2210.01166v1;cs.RO;"Accepted to the 2022 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS). Contact: Stanley Lewis, stanlew@umich.edu";nerf
2210.00647v3;http://arxiv.org/abs/2210.00647v3;2022-10-02;IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable  Novel View Synthesis;"Existing inverse rendering combined with neural rendering methods can only
perform editable novel view synthesis on object-specific scenes, while we
present intrinsic neural radiance fields, dubbed IntrinsicNeRF, which introduce
intrinsic decomposition into the NeRF-based neural rendering method and can
extend its application to room-scale scenes. Since intrinsic decomposition is a
fundamentally under-constrained inverse problem, we propose a novel
distance-aware point sampling and adaptive reflectance iterative clustering
optimization method, which enables IntrinsicNeRF with traditional intrinsic
decomposition constraints to be trained in an unsupervised manner, resulting in
multi-view consistent intrinsic decomposition results. To cope with the problem
that different adjacent instances of similar reflectance in a scene are
incorrectly clustered together, we further propose a hierarchical clustering
method with coarse-to-fine optimization to obtain a fast hierarchical indexing
representation. It supports compelling real-time augmented applications such as
recoloring and illumination variation. Extensive experiments and editing
samples on both object-specific/room-scale scenes and synthetic/real-word data
demonstrate that we can obtain consistent intrinsic decomposition results and
high-fidelity novel view synthesis even for challenging sequences.";Weicai Ye<author:sep>Shuo Chen<author:sep>Chong Bao<author:sep>Hujun Bao<author:sep>Marc Pollefeys<author:sep>Zhaopeng Cui<author:sep>Guofeng Zhang;http://arxiv.org/pdf/2210.00647v3;cs.CV;"Accepted to ICCV2023, Project webpage:
  https://zju3dv.github.io/intrinsic_nerf/, code:
  https://github.com/zju3dv/IntrinsicNeRF";nerf
2210.00489v2;http://arxiv.org/abs/2210.00489v2;2022-10-02;Unsupervised Multi-View Object Segmentation Using Radiance Field  Propagation;"We present radiance field propagation (RFP), a novel approach to segmenting
objects in 3D during reconstruction given only unlabeled multi-view images of a
scene. RFP is derived from emerging neural radiance field-based techniques,
which jointly encodes semantics with appearance and geometry. The core of our
method is a novel propagation strategy for individual objects' radiance fields
with a bidirectional photometric loss, enabling an unsupervised partitioning of
a scene into salient or meaningful regions corresponding to different object
instances. To better handle complex scenes with multiple objects and
occlusions, we further propose an iterative expectation-maximization algorithm
to refine object masks. RFP is one of the first unsupervised approach for
tackling 3D real scene object segmentation for neural radiance field (NeRF)
without any supervision, annotations, or other cues such as 3D bounding boxes
and prior knowledge of object class. Experiments demonstrate that RFP achieves
feasible segmentation results that are more accurate than previous unsupervised
image/scene segmentation approaches, and are comparable to existing supervised
NeRF-based methods. The segmented object representations enable individual 3D
object editing operations.";Xinhang Liu<author:sep>Jiaben Chen<author:sep>Huai Yu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2210.00489v2;cs.CV;23 pages, 14 figures, NeurIPS 2022;nerf
2210.01548v1;http://arxiv.org/abs/2210.01548v1;2022-10-02;Neural Implicit Surface Reconstruction from Noisy Camera Observations;"Representing 3D objects and scenes with neural radiance fields has become
very popular over the last years. Recently, surface-based representations have
been proposed, that allow to reconstruct 3D objects from simple photographs.
However, most current techniques require an accurate camera calibration, i.e.
camera parameters corresponding to each image, which is often a difficult task
to do in real-life situations. To this end, we propose a method for learning 3D
surfaces from noisy camera parameters. We show that we can learn camera
parameters together with learning the surface representation, and demonstrate
good quality 3D surface reconstruction even with noisy camera observations.";Sarthak Gupta<author:sep>Patrik Huber;http://arxiv.org/pdf/2210.01548v1;cs.CV;4 pages - 2 for paper, 2 for supplementary;
2210.00379v5;http://arxiv.org/abs/2210.00379v5;2022-10-01;NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review;"Neural Radiance Field (NeRF) has recently become a significant development in
the field of Computer Vision, allowing for implicit, neural network-based scene
representation and novel view synthesis. NeRF models have found diverse
applications in robotics, urban mapping, autonomous navigation, virtual
reality/augmented reality, and more. Due to the growing popularity of NeRF and
its expanding research area, we present a comprehensive survey of NeRF papers
from the past two years. Our survey is organized into architecture and
application-based taxonomies and provides an introduction to the theory of NeRF
and its training via differentiable volume rendering. We also present a
benchmark comparison of the performance and speed of key NeRF models. By
creating this survey, we hope to introduce new researchers to NeRF, provide a
helpful reference for influential works in this field, as well as motivate
future research directions with our discussion section.";Kyle Gao<author:sep>Yina Gao<author:sep>Hongjie He<author:sep>Dening Lu<author:sep>Linlin Xu<author:sep>Jonathan Li;http://arxiv.org/pdf/2210.00379v5;cs.CV;Fixed some typos from previous version;nerf
2210.00183v1;http://arxiv.org/abs/2210.00183v1;2022-10-01;Structure-Aware NeRF without Posed Camera via Epipolar Constraint;"The neural radiance field (NeRF) for realistic novel view synthesis requires
camera poses to be pre-acquired by a structure-from-motion (SfM) approach. This
two-stage strategy is not convenient to use and degrades the performance
because the error in the pose extraction can propagate to the view synthesis.
We integrate the pose extraction and view synthesis into a single end-to-end
procedure so they can benefit from each other. For training NeRF models, only
RGB images are given, without pre-known camera poses. The camera poses are
obtained by the epipolar constraint in which the identical feature in different
views has the same world coordinates transformed from the local camera
coordinates according to the extracted poses. The epipolar constraint is
jointly optimized with pixel color constraint. The poses are represented by a
CNN-based deep network, whose input is the related frames. This joint
optimization enables NeRF to be aware of the scene's structure that has an
improved generalization performance. Extensive experiments on a variety of
scenes demonstrate the effectiveness of the proposed approach. Code is
available at https://github.com/XTU-PR-LAB/SaNerf.";Shu Chen<author:sep>Yang Zhang<author:sep>Yaxin Xu<author:sep>Beiji Zou;http://arxiv.org/pdf/2210.00183v1;cs.CV;;nerf
2209.15637v1;http://arxiv.org/abs/2209.15637v1;2022-09-30;Improving 3D-aware Image Synthesis with A Geometry-aware Discriminator;"3D-aware image synthesis aims at learning a generative model that can render
photo-realistic 2D images while capturing decent underlying 3D shapes. A
popular solution is to adopt the generative adversarial network (GAN) and
replace the generator with a 3D renderer, where volume rendering with neural
radiance field (NeRF) is commonly used. Despite the advancement of synthesis
quality, existing methods fail to obtain moderate 3D shapes. We argue that,
considering the two-player game in the formulation of GANs, only making the
generator 3D-aware is not enough. In other words, displacing the generative
mechanism only offers the capability, but not the guarantee, of producing
3D-aware images, because the supervision of the generator primarily comes from
the discriminator. To address this issue, we propose GeoD through learning a
geometry-aware discriminator to improve 3D-aware GANs. Concretely, besides
differentiating real and fake samples from the 2D image space, the
discriminator is additionally asked to derive the geometry information from the
inputs, which is then applied as the guidance of the generator. Such a simple
yet effective design facilitates learning substantially more accurate 3D
shapes. Extensive experiments on various generator architectures and training
datasets verify the superiority of GeoD over state-of-the-art alternatives.
Moreover, our approach is registered as a general framework such that a more
capable discriminator (i.e., with a third task of novel view synthesis beyond
domain classification and geometry extraction) can further assist the generator
with a better multi-view consistency.";Zifan Shi<author:sep>Yinghao Xu<author:sep>Yujun Shen<author:sep>Deli Zhao<author:sep>Qifeng Chen<author:sep>Dit-Yan Yeung;http://arxiv.org/pdf/2209.15637v1;cs.CV;"Accepted by NeurIPS 2022. Project page:
  https://vivianszf.github.io/geod";nerf
2209.15529v1;http://arxiv.org/abs/2209.15529v1;2022-09-30;TT-NF: Tensor Train Neural Fields;"Learning neural fields has been an active topic in deep learning research,
focusing, among other issues, on finding more compact and easy-to-fit
representations. In this paper, we introduce a novel low-rank representation
termed Tensor Train Neural Fields (TT-NF) for learning neural fields on dense
regular grids and efficient methods for sampling from them. Our representation
is a TT parameterization of the neural field, trained with backpropagation to
minimize a non-convex objective. We analyze the effect of low-rank compression
on the downstream task quality metrics in two settings. First, we demonstrate
the efficiency of our method in a sandbox task of tensor denoising, which
admits comparison with SVD-based schemes designed to minimize reconstruction
error. Furthermore, we apply the proposed approach to Neural Radiance Fields,
where the low-rank structure of the field corresponding to the best quality can
be discovered only through learning.";Anton Obukhov<author:sep>Mikhail Usvyatsov<author:sep>Christos Sakaridis<author:sep>Konrad Schindler<author:sep>Luc Van Gool;http://arxiv.org/pdf/2209.15529v1;cs.LG;Preprint, under review;
2209.15172v1;http://arxiv.org/abs/2209.15172v1;2022-09-30;Understanding Pure CLIP Guidance for Voxel Grid NeRF Models;"We explore the task of text to 3D object generation using CLIP. Specifically,
we use CLIP for guidance without access to any datasets, a setting we refer to
as pure CLIP guidance. While prior work has adopted this setting, there is no
systematic study of mechanics for preventing adversarial generations within
CLIP. We illustrate how different image-based augmentations prevent the
adversarial generation problem, and how the generated results are impacted. We
test different CLIP model architectures and show that ensembling different
models for guidance can prevent adversarial generations within bigger models
and generate sharper results. Furthermore, we implement an implicit voxel grid
model to show how neural networks provide an additional layer of
regularization, resulting in better geometrical structure and coherency of
generated objects. Compared to prior work, we achieve more coherent results
with higher memory efficiency and faster training speeds.";Han-Hung Lee<author:sep>Angel X. Chang;http://arxiv.org/pdf/2209.15172v1;cs.CV;;nerf
2209.14819v2;http://arxiv.org/abs/2209.14819v2;2022-09-29;SymmNeRF: Learning to Explore Symmetry Prior for Single-View View  Synthesis;"We study the problem of novel view synthesis of objects from a single image.
Existing methods have demonstrated the potential in single-view view synthesis.
However, they still fail to recover the fine appearance details, especially in
self-occluded areas. This is because a single view only provides limited
information. We observe that manmade objects usually exhibit symmetric
appearances, which introduce additional prior knowledge. Motivated by this, we
investigate the potential performance gains of explicitly embedding symmetry
into the scene representation. In this paper, we propose SymmNeRF, a neural
radiance field (NeRF) based framework that combines local and global
conditioning under the introduction of symmetry priors. In particular, SymmNeRF
takes the pixel-aligned image features and the corresponding symmetric features
as extra inputs to the NeRF, whose parameters are generated by a hypernetwork.
As the parameters are conditioned on the image-encoded latent codes, SymmNeRF
is thus scene-independent and can generalize to new scenes. Experiments on
synthetic and real-world datasets show that SymmNeRF synthesizes novel views
with more details regardless of the pose transformation, and demonstrates good
generalization when applied to unseen objects. Code is available at:
https://github.com/xingyi-li/SymmNeRF.";Xingyi Li<author:sep>Chaoyi Hong<author:sep>Yiran Wang<author:sep>Zhiguo Cao<author:sep>Ke Xian<author:sep>Guosheng Lin;http://arxiv.org/pdf/2209.14819v2;cs.CV;Accepted by ACCV 2022;nerf
2209.14988v1;http://arxiv.org/abs/2209.14988v1;2022-09-29;DreamFusion: Text-to-3D using 2D Diffusion;"Recent breakthroughs in text-to-image synthesis have been driven by diffusion
models trained on billions of image-text pairs. Adapting this approach to 3D
synthesis would require large-scale datasets of labeled 3D data and efficient
architectures for denoising 3D data, neither of which currently exist. In this
work, we circumvent these limitations by using a pretrained 2D text-to-image
diffusion model to perform text-to-3D synthesis. We introduce a loss based on
probability density distillation that enables the use of a 2D diffusion model
as a prior for optimization of a parametric image generator. Using this loss in
a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a
Neural Radiance Field, or NeRF) via gradient descent such that its 2D
renderings from random angles achieve a low loss. The resulting 3D model of the
given text can be viewed from any angle, relit by arbitrary illumination, or
composited into any 3D environment. Our approach requires no 3D training data
and no modifications to the image diffusion model, demonstrating the
effectiveness of pretrained image diffusion models as priors.";Ben Poole<author:sep>Ajay Jain<author:sep>Jonathan T. Barron<author:sep>Ben Mildenhall;http://arxiv.org/pdf/2209.14988v1;cs.CV;see project page at https://dreamfusion3d.github.io/;nerf
2209.14265v2;http://arxiv.org/abs/2209.14265v2;2022-09-28;360FusionNeRF: Panoramic Neural Radiance Fields with Joint Guidance;"We present a method to synthesize novel views from a single $360^\circ$
panorama image based on the neural radiance field (NeRF). Prior studies in a
similar setting rely on the neighborhood interpolation capability of
multi-layer perceptions to complete missing regions caused by occlusion, which
leads to artifacts in their predictions. We propose 360FusionNeRF, a
semi-supervised learning framework where we introduce geometric supervision and
semantic consistency to guide the progressive training process. Firstly, the
input image is re-projected to $360^\circ$ images, and auxiliary depth maps are
extracted at other camera positions. The depth supervision, in addition to the
NeRF color guidance, improves the geometry of the synthesized views.
Additionally, we introduce a semantic consistency loss that encourages
realistic renderings of novel views. We extract these semantic features using a
pre-trained visual encoder such as CLIP, a Vision Transformer trained on
hundreds of millions of diverse 2D photographs mined from the web with natural
language supervision. Experiments indicate that our proposed method can produce
plausible completions of unobserved regions while preserving the features of
the scene. When trained across various scenes, 360FusionNeRF consistently
achieves the state-of-the-art performance when transferring to synthetic
Structured3D dataset (PSNR~5%, SSIM~3% LPIPS~13%), real-world Matterport3D
dataset (PSNR~3%, SSIM~3% LPIPS~9%) and Replica360 dataset (PSNR~8%, SSIM~2%
LPIPS~18%).";Shreyas Kulkarni<author:sep>Peng Yin<author:sep>Sebastian Scherer;http://arxiv.org/pdf/2209.14265v2;cs.CV;"8 pages, Fig 3, Submitted to IEEE RAL. arXiv admin note: text overlap
  with arXiv:2106.10859, arXiv:2104.00677, arXiv:2203.09957, arXiv:2204.00928
  by other authors";nerf
2209.13433v1;http://arxiv.org/abs/2209.13433v1;2022-09-27;OmniNeRF: Hybriding Omnidirectional Distance and Radiance fields for  Neural Surface Reconstruction;"3D reconstruction from images has wide applications in Virtual Reality and
Automatic Driving, where the precision requirement is very high.
Ground-breaking research in the neural radiance field (NeRF) by utilizing
Multi-Layer Perceptions has dramatically improved the representation quality of
3D objects. Some later studies improved NeRF by building truncated signed
distance fields (TSDFs) but still suffer from the problem of blurred surfaces
in 3D reconstruction. In this work, this surface ambiguity is addressed by
proposing a novel way of 3D shape representation, OmniNeRF. It is based on
training a hybrid implicit field of Omni-directional Distance Field (ODF) and
neural radiance field, replacing the apparent density in NeRF with
omnidirectional information. Moreover, we introduce additional supervision on
the depth map to further improve reconstruction quality. The proposed method
has been proven to effectively deal with NeRF defects at the edges of the
surface reconstruction, providing higher quality 3D scene reconstruction
results.";Jiaming Shen<author:sep>Bolin Song<author:sep>Zirui Wu<author:sep>Yi Xu;http://arxiv.org/pdf/2209.13433v1;cs.CV;Accepted by CMSDA 2022;nerf
2209.13274v2;http://arxiv.org/abs/2209.13274v2;2022-09-27;Orbeez-SLAM: A Real-time Monocular Visual SLAM with ORB Features and  NeRF-realized Mapping;"A spatial AI that can perform complex tasks through visual signals and
cooperate with humans is highly anticipated. To achieve this, we need a visual
SLAM that easily adapts to new scenes without pre-training and generates dense
maps for downstream tasks in real-time. None of the previous learning-based and
non-learning-based visual SLAMs satisfy all needs due to the intrinsic
limitations of their components. In this work, we develop a visual SLAM named
Orbeez-SLAM, which successfully collaborates with implicit neural
representation and visual odometry to achieve our goals. Moreover, Orbeez-SLAM
can work with the monocular camera since it only needs RGB inputs, making it
widely applicable to the real world. Results show that our SLAM is up to 800x
faster than the strong baseline with superior rendering outcomes. Code link:
https://github.com/MarvinChung/Orbeez-SLAM.";Chi-Ming Chung<author:sep>Yang-Che Tseng<author:sep>Ya-Ching Hsu<author:sep>Xiang-Qian Shi<author:sep>Yun-Hung Hua<author:sep>Jia-Fong Yeh<author:sep>Wen-Chin Chen<author:sep>Yi-Ting Chen<author:sep>Winston H. Hsu;http://arxiv.org/pdf/2209.13274v2;cs.RO;;nerf
2209.13091v2;http://arxiv.org/abs/2209.13091v2;2022-09-27;WaterNeRF: Neural Radiance Fields for Underwater Scenes;"Underwater imaging is a critical task performed by marine robots for a wide
range of applications including aquaculture, marine infrastructure inspection,
and environmental monitoring. However, water column effects, such as
attenuation and backscattering, drastically change the color and quality of
imagery captured underwater. Due to varying water conditions and
range-dependency of these effects, restoring underwater imagery is a
challenging problem. This impacts downstream perception tasks including depth
estimation and 3D reconstruction. In this paper, we advance state-of-the-art in
neural radiance fields (NeRFs) to enable physics-informed dense depth
estimation and color correction. Our proposed method, WaterNeRF, estimates
parameters of a physics-based model for underwater image formation, leading to
a hybrid data-driven and model-based solution. After determining the scene
structure and radiance field, we can produce novel views of degraded as well as
corrected underwater images, along with dense depth of the scene. We evaluate
the proposed method qualitatively and quantitatively on a real underwater
dataset.";Advaith Venkatramanan Sethuraman<author:sep>Manikandasriram Srinivasan Ramanagopal<author:sep>Katherine A. Skinner;http://arxiv.org/pdf/2209.13091v2;cs.RO;;nerf
2209.12744v1;http://arxiv.org/abs/2209.12744v1;2022-09-26;Baking in the Feature: Accelerating Volumetric Segmentation by Rendering  Feature Maps;"Methods have recently been proposed that densely segment 3D volumes into
classes using only color images and expert supervision in the form of sparse
semantically annotated pixels. While impressive, these methods still require a
relatively large amount of supervision and segmenting an object can take
several minutes in practice. Such systems typically only optimize their
representation on the particular scene they are fitting, without leveraging any
prior information from previously seen images. In this paper, we propose to use
features extracted with models trained on large existing datasets to improve
segmentation performance. We bake this feature representation into a Neural
Radiance Field (NeRF) by volumetrically rendering feature maps and supervising
on features extracted from each input image. We show that by baking this
representation into the NeRF, we make the subsequent classification task much
easier. Our experiments show that our method achieves higher segmentation
accuracy with fewer semantic annotations than existing methods over a wide
range of scenes.";Kenneth Blomqvist<author:sep>Lionel Ott<author:sep>Jen Jen Chung<author:sep>Roland Siegwart;http://arxiv.org/pdf/2209.12744v1;cs.CV;;nerf
2209.12266v3;http://arxiv.org/abs/2209.12266v3;2022-09-25;Enforcing safety for vision-based controllers via Control Barrier  Functions and Neural Radiance Fields;"To navigate complex environments, robots must increasingly use
high-dimensional visual feedback (e.g. images) for control. However, relying on
high-dimensional image data to make control decisions raises important
questions; particularly, how might we prove the safety of a visual-feedback
controller? Control barrier functions (CBFs) are powerful tools for certifying
the safety of feedback controllers in the state-feedback setting, but CBFs have
traditionally been poorly-suited to visual feedback control due to the need to
predict future observations in order to evaluate the barrier function. In this
work, we solve this issue by leveraging recent advances in neural radiance
fields (NeRFs), which learn implicit representations of 3D scenes and can
render images from previously-unseen camera perspectives, to provide
single-step visual foresight for a CBF-based controller. This novel combination
is able to filter out unsafe actions and intervene to preserve safety. We
demonstrate the effect of our controller in real-time simulation experiments
where it successfully prevents the robot from taking dangerous actions.";Mukun Tong<author:sep>Charles Dawson<author:sep>Chuchu Fan;http://arxiv.org/pdf/2209.12266v3;cs.RO;Accepted to ICRA 2023;nerf
2209.12068v2;http://arxiv.org/abs/2209.12068v2;2022-09-24;NeRF-Loc: Transformer-Based Object Localization Within Neural Radiance  Fields;"Neural Radiance Fields (NeRFs) have become a widely-applied scene
representation technique in recent years, showing advantages for robot
navigation and manipulation tasks. To further advance the utility of NeRFs for
robotics, we propose a transformer-based framework, NeRF-Loc, to extract 3D
bounding boxes of objects in NeRF scenes. NeRF-Loc takes a pre-trained NeRF
model and camera view as input and produces labeled, oriented 3D bounding boxes
of objects as output. Using current NeRF training tools, a robot can train a
NeRF environment model in real-time and, using our algorithm, identify 3D
bounding boxes of objects of interest within the NeRF for downstream navigation
or manipulation tasks. Concretely, we design a pair of paralleled transformer
encoder branches, namely the coarse stream and the fine stream, to encode both
the context and details of target objects. The encoded features are then fused
together with attention layers to alleviate ambiguities for accurate object
localization. We have compared our method with conventional RGB(-D) based
methods that take rendered RGB images and depths from NeRFs as inputs. Our
method is better than the baselines.";Jiankai Sun<author:sep>Yan Xu<author:sep>Mingyu Ding<author:sep>Hongwei Yi<author:sep>Chen Wang<author:sep>Jingdong Wang<author:sep>Liangjun Zhang<author:sep>Mac Schwager;http://arxiv.org/pdf/2209.12068v2;cs.CV;;nerf
2209.08776v6;http://arxiv.org/abs/2209.08776v6;2022-09-19;NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes;"Neural volumetric representations have shown the potential that Multi-layer
Perceptrons (MLPs) can be optimized with multi-view calibrated images to
represent scene geometry and appearance, without explicit 3D supervision.
Object segmentation can enrich many downstream applications based on the
learned radiance field. However, introducing hand-crafted segmentation to
define regions of interest in a complex real-world scene is non-trivial and
expensive as it acquires per view annotation. This paper carries out the
exploration of self-supervised learning for object segmentation using NeRF for
complex real-world scenes. Our framework, called NeRF with Self-supervised
Object Segmentation NeRF-SOS, couples object segmentation and neural radiance
field to segment objects in any view within a scene. By proposing a novel
collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS
encourages NeRF models to distill compact geometry-aware segmentation clusters
from their density fields and the self-supervised pre-trained 2D visual
features. The self-supervised object segmentation framework can be applied to
various NeRF models that both lead to photo-realistic rendering results and
convincing segmentation maps for both indoor and outdoor scenarios. Extensive
results on the LLFF, Tank & Temple, and BlendedMVS datasets validate the
effectiveness of NeRF-SOS. It consistently surpasses other 2D-based
self-supervised baselines and predicts finer semantics masks than existing
supervised counterparts. Please refer to the video on our project page for more
details:https://zhiwenfan.github.io/NeRF-SOS.";Zhiwen Fan<author:sep>Peihao Wang<author:sep>Yifan Jiang<author:sep>Xinyu Gong<author:sep>Dejia Xu<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2209.08776v6;cs.CV;;nerf
2209.09050v1;http://arxiv.org/abs/2209.09050v1;2022-09-19;Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields;"We present Loc-NeRF, a real-time vision-based robot localization approach
that combines Monte Carlo localization and Neural Radiance Fields (NeRF). Our
system uses a pre-trained NeRF model as the map of an environment and can
localize itself in real-time using an RGB camera as the only exteroceptive
sensor onboard the robot. While neural radiance fields have seen significant
applications for visual rendering in computer vision and graphics, they have
found limited use in robotics. Existing approaches for NeRF-based localization
require both a good initial pose guess and significant computation, making them
impractical for real-time robotics applications. By using Monte Carlo
localization as a workhorse to estimate poses using a NeRF map model, Loc-NeRF
is able to perform localization faster than the state of the art and without
relying on an initial pose estimate. In addition to testing on synthetic data,
we also run our system using real data collected by a Clearpath Jackal UGV and
demonstrate for the first time the ability to perform real-time global
localization with neural radiance fields. We make our code publicly available
at https://github.com/MIT-SPARK/Loc-NeRF.";Dominic Maggio<author:sep>Marcus Abate<author:sep>Jingnan Shi<author:sep>Courtney Mario<author:sep>Luca Carlone;http://arxiv.org/pdf/2209.09050v1;cs.RO;;nerf
2209.08718v1;http://arxiv.org/abs/2209.08718v1;2022-09-19;Density-aware NeRF Ensembles: Quantifying Predictive Uncertainty in  Neural Radiance Fields;"We show that ensembling effectively quantifies model uncertainty in Neural
Radiance Fields (NeRFs) if a density-aware epistemic uncertainty term is
considered. The naive ensembles investigated in prior work simply average
rendered RGB images to quantify the model uncertainty caused by conflicting
explanations of the observed scene. In contrast, we additionally consider the
termination probabilities along individual rays to identify epistemic model
uncertainty due to a lack of knowledge about the parts of a scene unobserved
during training. We achieve new state-of-the-art performance across established
uncertainty quantification benchmarks for NeRFs, outperforming methods that
require complex changes to the NeRF architecture and training regime. We
furthermore demonstrate that NeRF uncertainty can be utilised for next-best
view selection and model refinement.";Niko Sünderhauf<author:sep>Jad Abou-Chakra<author:sep>Dimity Miller;http://arxiv.org/pdf/2209.08718v1;cs.CV;;nerf
2209.08498v2;http://arxiv.org/abs/2209.08498v2;2022-09-18;LATITUDE: Robotic Global Localization with Truncated Dynamic Low-pass  Filter in City-scale NeRF;"Neural Radiance Fields (NeRFs) have made great success in representing
complex 3D scenes with high-resolution details and efficient memory.
Nevertheless, current NeRF-based pose estimators have no initial pose
prediction and are prone to local optima during optimization. In this paper, we
present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter,
which introduces a two-stage localization mechanism in city-scale NeRF. In
place recognition stage, we train a regressor through images generated from
trained NeRFs, which provides an initial value for global localization. In pose
optimization stage, we minimize the residual between the observed image and
rendered image by directly optimizing the pose on tangent plane. To avoid
convergence to local optimum, we introduce a Truncated Dynamic Low-pass Filter
(TDLF) for coarse-to-fine pose registration. We evaluate our method on both
synthetic and real-world data and show its potential applications for
high-precision navigation in large-scale city scenes. Codes and data will be
publicly available at https://github.com/jike5/LATITUDE.";Zhenxin Zhu<author:sep>Yuantao Chen<author:sep>Zirui Wu<author:sep>Chao Hou<author:sep>Yongliang Shi<author:sep>Chuxuan Li<author:sep>Pengfei Li<author:sep>Hao Zhao<author:sep>Guyue Zhou;http://arxiv.org/pdf/2209.08498v2;cs.CV;7 pages, 6 figures, ICRA 2023;nerf
2209.08546v1;http://arxiv.org/abs/2209.08546v1;2022-09-18;ActiveNeRF: Learning where to See with Uncertainty Estimation;"Recently, Neural Radiance Fields (NeRF) has shown promising performances on
reconstructing 3D scenes and synthesizing novel views from a sparse set of 2D
images. Albeit effective, the performance of NeRF is highly influenced by the
quality of training samples. With limited posed images from the scene, NeRF
fails to generalize well to novel views and may collapse to trivial solutions
in unobserved regions. This makes NeRF impractical under resource-constrained
scenarios. In this paper, we present a novel learning framework, ActiveNeRF,
aiming to model a 3D scene with a constrained input budget. Specifically, we
first incorporate uncertainty estimation into a NeRF model, which ensures
robustness under few observations and provides an interpretation of how NeRF
understands the scene. On this basis, we propose to supplement the existing
training set with newly captured samples based on an active learning scheme. By
evaluating the reduction of uncertainty given new inputs, we select the samples
that bring the most information gain. In this way, the quality of novel view
synthesis can be improved with minimal additional resources. Extensive
experiments validate the performance of our model on both realistic and
synthetic scenes, especially with scarcer training data. Code will be released
at \url{https://github.com/LeapLabTHU/ActiveNeRF}.";Xuran Pan<author:sep>Zihang Lai<author:sep>Shiji Song<author:sep>Gao Huang;http://arxiv.org/pdf/2209.08546v1;cs.CV;Accepted by ECCV2022;nerf
2209.08409v1;http://arxiv.org/abs/2209.08409v1;2022-09-17;Uncertainty Guided Policy for Active Robotic 3D Reconstruction using  Neural Radiance Fields;"In this paper, we tackle the problem of active robotic 3D reconstruction of
an object. In particular, we study how a mobile robot with an arm-held camera
can select a favorable number of views to recover an object's 3D shape
efficiently. Contrary to the existing solution to this problem, we leverage the
popular neural radiance fields-based object representation, which has recently
shown impressive results for various computer vision tasks. However, it is not
straightforward to directly reason about an object's explicit 3D geometric
details using such a representation, making the next-best-view selection
problem for dense 3D reconstruction challenging. This paper introduces a
ray-based volumetric uncertainty estimator, which computes the entropy of the
weight distribution of the color samples along each ray of the object's
implicit neural representation. We show that it is possible to infer the
uncertainty of the underlying 3D geometry given a novel view with the proposed
estimator. We then present a next-best-view selection policy guided by the
ray-based volumetric uncertainty in neural radiance fields-based
representations. Encouraging experimental results on synthetic and real-world
data suggest that the approach presented in this paper can enable a new
research direction of using an implicit 3D object representation for the
next-best-view problem in robot vision applications, distinguishing our
approach from the existing approaches that rely on explicit 3D geometric
modeling.";Soomin Lee<author:sep>Le Chen<author:sep>Jiahao Wang<author:sep>Alexander Liniger<author:sep>Suryansh Kumar<author:sep>Fisher Yu;http://arxiv.org/pdf/2209.08409v1;cs.CV;"8 pages, 9 figure; Accepted for publication at IEEE Robotics and
  Automation Letters (RA-L) 2022";
2209.07919v1;http://arxiv.org/abs/2209.07919v1;2022-09-16;iDF-SLAM: End-to-End RGB-D SLAM with Neural Implicit Mapping and Deep  Feature Tracking;"We propose a novel end-to-end RGB-D SLAM, iDF-SLAM, which adopts a
feature-based deep neural tracker as the front-end and a NeRF-style neural
implicit mapper as the back-end. The neural implicit mapper is trained
on-the-fly, while though the neural tracker is pretrained on the ScanNet
dataset, it is also finetuned along with the training of the neural implicit
mapper. Under such a design, our iDF-SLAM is capable of learning to use
scene-specific features for camera tracking, thus enabling lifelong learning of
the SLAM system. Both the training for the tracker and the mapper are
self-supervised without introducing ground truth poses. We test the performance
of our iDF-SLAM on the Replica and ScanNet datasets and compare the results to
the two recent NeRF-based neural SLAM systems. The proposed iDF-SLAM
demonstrates state-of-the-art results in terms of scene reconstruction and
competitive performance in camera tracking.";Yuhang Ming<author:sep>Weicai Ye<author:sep>Andrew Calway;http://arxiv.org/pdf/2209.07919v1;cs.RO;7 pages, 6 figures, 3 tables;nerf
2209.07366v1;http://arxiv.org/abs/2209.07366v1;2022-09-15;3DMM-RF: Convolutional Radiance Fields for 3D Face Modeling;"Facial 3D Morphable Models are a main computer vision subject with countless
applications and have been highly optimized in the last two decades. The
tremendous improvements of deep generative networks have created various
possibilities for improving such models and have attracted wide interest.
Moreover, the recent advances in neural radiance fields, are revolutionising
novel-view synthesis of known scenes. In this work, we present a facial 3D
Morphable Model, which exploits both of the above, and can accurately model a
subject's identity, pose and expression and render it in arbitrary
illumination. This is achieved by utilizing a powerful deep style-based
generator to overcome two main weaknesses of neural radiance fields, their
rigidity and rendering speed. We introduce a style-based generative network
that synthesizes in one pass all and only the required rendering samples of a
neural radiance field. We create a vast labelled synthetic dataset of facial
renders, and train the network on these data, so that it can accurately model
and generalize on facial identity, pose and appearance. Finally, we show that
this model can accurately be fit to ""in-the-wild"" facial images of arbitrary
pose and illumination, extract the facial characteristics, and be used to
re-render the face in controllable conditions.";Stathis Galanakis<author:sep>Baris Gecer<author:sep>Alexandros Lattas<author:sep>Stefanos Zafeiriou;http://arxiv.org/pdf/2209.07366v1;cs.CV;;
2209.05277v1;http://arxiv.org/abs/2209.05277v1;2022-09-12;StructNeRF: Neural Radiance Fields for Indoor Scenes with Structural  Hints;"Neural Radiance Fields (NeRF) achieve photo-realistic view synthesis with
densely captured input images. However, the geometry of NeRF is extremely
under-constrained given sparse views, resulting in significant degradation of
novel view synthesis quality. Inspired by self-supervised depth estimation
methods, we propose StructNeRF, a solution to novel view synthesis for indoor
scenes with sparse inputs. StructNeRF leverages the structural hints naturally
embedded in multi-view inputs to handle the unconstrained geometry issue in
NeRF. Specifically, it tackles the texture and non-texture regions
respectively: a patch-based multi-view consistent photometric loss is proposed
to constrain the geometry of textured regions; for non-textured ones, we
explicitly restrict them to be 3D consistent planes. Through the dense
self-supervised depth constraints, our method improves both the geometry and
the view synthesis performance of NeRF without any additional training on
external data. Extensive experiments on several real-world datasets demonstrate
that StructNeRF surpasses state-of-the-art methods for indoor scenes with
sparse inputs both quantitatively and qualitatively.";Zheng Chen<author:sep>Chen Wang<author:sep>Yuan-Chen Guo<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2209.05277v1;cs.CV;;nerf
2209.04183v1;http://arxiv.org/abs/2209.04183v1;2022-09-09;Generative Deformable Radiance Fields for Disentangled Image Synthesis  of Topology-Varying Objects;"3D-aware generative models have demonstrated their superb performance to
generate 3D neural radiance fields (NeRF) from a collection of monocular 2D
images even for topology-varying object categories. However, these methods
still lack the capability to separately control the shape and appearance of the
objects in the generated radiance fields. In this paper, we propose a
generative model for synthesizing radiance fields of topology-varying objects
with disentangled shape and appearance variations. Our method generates
deformable radiance fields, which builds the dense correspondence between the
density fields of the objects and encodes their appearances in a shared
template field. Our disentanglement is achieved in an unsupervised manner
without introducing extra labels to previous 3D-aware GAN training. We also
develop an effective image inversion scheme for reconstructing the radiance
field of an object in a real monocular image and manipulating its shape and
appearance. Experiments show that our method can successfully learn the
generative model from unstructured monocular images and well disentangle the
shape and appearance for objects (e.g., chairs) with large topological
variance. The model trained on synthetic data can faithfully reconstruct the
real object in a given single image and achieve high-quality texture and shape
editing results.";Ziyu Wang<author:sep>Yu Deng<author:sep>Jiaolong Yang<author:sep>Jingyi Yu<author:sep>Xin Tong;http://arxiv.org/pdf/2209.04183v1;cs.CV;"Accepted at Pacific Graphics 2022 & COMPUTER GRAPHICS Forum, Project
  Page: https://ziyuwang98.github.io/GDRF/";nerf
2209.04061v1;http://arxiv.org/abs/2209.04061v1;2022-09-08;im2nerf: Image to Neural Radiance Field in the Wild;"We propose im2nerf, a learning framework that predicts a continuous neural
object representation given a single input image in the wild, supervised by
only segmentation output from off-the-shelf recognition methods. The standard
approach to constructing neural radiance fields takes advantage of multi-view
consistency and requires many calibrated views of a scene, a requirement that
cannot be satisfied when learning on large-scale image data in the wild. We
take a step towards addressing this shortcoming by introducing a model that
encodes the input image into a disentangled object representation that contains
a code for object shape, a code for object appearance, and an estimated camera
pose from which the object image is captured. Our model conditions a NeRF on
the predicted object representation and uses volume rendering to generate
images from novel views. We train the model end-to-end on a large collection of
input images. As the model is only provided with single-view images, the
problem is highly under-constrained. Therefore, in addition to using a
reconstruction loss on the synthesized input view, we use an auxiliary
adversarial loss on the novel rendered views. Furthermore, we leverage object
symmetry and cycle camera pose consistency. We conduct extensive quantitative
and qualitative experiments on the ShapeNet dataset as well as qualitative
experiments on Open Images dataset. We show that in all cases, im2nerf achieves
the state-of-the-art performance for novel view synthesis from a single-view
unposed image in the wild.";Lu Mi<author:sep>Abhijit Kundu<author:sep>David Ross<author:sep>Frank Dellaert<author:sep>Noah Snavely<author:sep>Alireza Fathi;http://arxiv.org/pdf/2209.04061v1;cs.CV;12 pages, 8 figures, 4 tables;nerf
2209.03910v1;http://arxiv.org/abs/2209.03910v1;2022-09-08;PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and  Feature-metric Alignment;"We present PixTrack, a vision based object pose tracking framework using
novel view synthesis and deep feature-metric alignment. Our evaluations
demonstrate that our method produces highly accurate, robust, and jitter-free
6DoF pose estimates of objects in RGB images without the need of any data
annotation or trajectory smoothing. Our method is also computationally
efficient making it easy to have multi-object tracking with no alteration to
our method and just using CPU multiprocessing.";Prajwal Chidananda<author:sep>Saurabh Nair<author:sep>Douglas Lee<author:sep>Adrian Kaehler;http://arxiv.org/pdf/2209.03910v1;cs.CV;;nerf
2209.03494v1;http://arxiv.org/abs/2209.03494v1;2022-09-07;Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D  Image Representations;"We present Neural Feature Fusion Fields (N3F), a method that improves dense
2D image feature extractors when the latter are applied to the analysis of
multiple images reconstructible as a 3D scene. Given an image feature
extractor, for example pre-trained using self-supervision, N3F uses it as a
teacher to learn a student network defined in 3D space. The 3D student network
is similar to a neural radiance field that distills said features and can be
trained with the usual differentiable rendering machinery. As a consequence,
N3F is readily applicable to most neural rendering formulations, including
vanilla NeRF and its extensions to complex dynamic scenes. We show that our
method not only enables semantic understanding in the context of scene-specific
neural fields without the use of manual labels, but also consistently improves
over the self-supervised 2D baselines. This is demonstrated by considering
various tasks, such as 2D object retrieval, 3D segmentation, and scene editing,
in diverse sequences, including long egocentric videos in the EPIC-KITCHENS
benchmark.";Vadim Tschernezki<author:sep>Iro Laina<author:sep>Diane Larlus<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2209.03494v1;cs.CV;3DV2022, Oral. Project page: https://www.robots.ox.ac.uk/~vadim/n3f/;nerf
2209.01194v4;http://arxiv.org/abs/2209.01194v4;2022-09-02;CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural  Representations;"Recent advances in neural radiance fields (NeRFs) achieve state-of-the-art
novel view synthesis and facilitate dense estimation of scene properties.
However, NeRFs often fail for large, unbounded scenes that are captured under
very sparse views with the scene content concentrated far away from the camera,
as is typical for field robotics applications. In particular, NeRF-style
algorithms perform poorly: (1) when there are insufficient views with little
pose diversity, (2) when scenes contain saturation and shadows, and (3) when
finely sampling large unbounded scenes with fine structures becomes
computationally intensive.
  This paper proposes CLONeR, which significantly improves upon NeRF by
allowing it to model large outdoor driving scenes that are observed from sparse
input sensor views. This is achieved by decoupling occupancy and color learning
within the NeRF framework into separate Multi-Layer Perceptrons (MLPs) trained
using LiDAR and camera data, respectively. In addition, this paper proposes a
novel method to build differentiable 3D Occupancy Grid Maps (OGM) alongside the
NeRF model, and leverage this occupancy grid for improved sampling of points
along a ray for volumetric rendering in metric space.
  Through extensive quantitative and qualitative experiments on scenes from the
KITTI dataset, this paper demonstrates that the proposed method outperforms
state-of-the-art NeRF models on both novel view synthesis and dense depth
prediction tasks when trained on sparse input data.";Alexandra Carlson<author:sep>Manikandasriram Srinivasan Ramanagopal<author:sep>Nathan Tseng<author:sep>Matthew Johnson-Roberson<author:sep>Ram Vasudevan<author:sep>Katherine A. Skinner;http://arxiv.org/pdf/2209.01194v4;cs.CV;first two authors equally contributed;nerf
2209.01019v1;http://arxiv.org/abs/2209.01019v1;2022-09-01;On Quantizing Implicit Neural Representations;"The role of quantization within implicit/coordinate neural networks is still
not fully understood. We note that using a canonical fixed quantization scheme
during training produces poor performance at low-rates due to the network
weight distributions changing over the course of training. In this work, we
show that a non-uniform quantization of neural weights can lead to significant
improvements. Specifically, we demonstrate that a clustered quantization
enables improved reconstruction. Finally, by characterising a trade-off between
quantization and network capacity, we demonstrate that it is possible (while
memory inefficient) to reconstruct signals using binary neural networks. We
demonstrate our findings experimentally on 2D image reconstruction and 3D
radiance fields; and show that simple quantization methods and architecture
search can achieve compression of NeRF to less than 16kb with minimal loss in
performance (323x smaller than the original NeRF).";Cameron Gordon<author:sep>Shin-Fang Chng<author:sep>Lachlan MacDonald<author:sep>Simon Lucey;http://arxiv.org/pdf/2209.01019v1;cs.CV;10 pages, 10 figures;nerf
2209.00648v1;http://arxiv.org/abs/2209.00648v1;2022-09-01;Cross-Spectral Neural Radiance Fields;"We propose X-NeRF, a novel method to learn a Cross-Spectral scene
representation given images captured from cameras with different light spectrum
sensitivity, based on the Neural Radiance Fields formulation. X-NeRF optimizes
camera poses across spectra during training and exploits Normalized
Cross-Device Coordinates (NXDC) to render images of different modalities from
arbitrary viewpoints, which are aligned and at the same resolution. Experiments
on 16 forward-facing scenes, featuring color, multi-spectral and infrared
images, confirm the effectiveness of X-NeRF at modeling Cross-Spectral scene
representations.";Matteo Poggi<author:sep>Pierluigi Zama Ramirez<author:sep>Fabio Tosi<author:sep>Samuele Salti<author:sep>Stefano Mattoccia<author:sep>Luigi Di Stefano;http://arxiv.org/pdf/2209.00648v1;cs.CV;3DV 2022. Project page: https://cvlab-unibo.github.io/xnerf-web/;nerf
2208.14851v1;http://arxiv.org/abs/2208.14851v1;2022-08-31;Dual-Space NeRF: Learning Animatable Avatars and Scene Lighting in  Separate Spaces;"Modeling the human body in a canonical space is a common practice for
capturing and animation. But when involving the neural radiance field (NeRF),
learning a static NeRF in the canonical space is not enough because the
lighting of the body changes when the person moves even though the scene
lighting is constant. Previous methods alleviate the inconsistency of lighting
by learning a per-frame embedding, but this operation does not generalize to
unseen poses. Given that the lighting condition is static in the world space
while the human body is consistent in the canonical space, we propose a
dual-space NeRF that models the scene lighting and the human body with two MLPs
in two separate spaces. To bridge these two spaces, previous methods mostly
rely on the linear blend skinning (LBS) algorithm. However, the blending
weights for LBS of a dynamic neural field are intractable and thus are usually
memorized with another MLP, which does not generalize to novel poses. Although
it is possible to borrow the blending weights of a parametric mesh such as
SMPL, the interpolation operation introduces more artifacts. In this paper, we
propose to use the barycentric mapping, which can directly generalize to unseen
poses and surprisingly achieves superior results than LBS with neural blending
weights. Quantitative and qualitative results on the Human3.6M and the
ZJU-MoCap datasets show the effectiveness of our method.";Yihao Zhi<author:sep>Shenhan Qian<author:sep>Xinhao Yan<author:sep>Shenghua Gao;http://arxiv.org/pdf/2208.14851v1;cs.CV;Accepted by 3DV 2022;nerf
2208.14433v1;http://arxiv.org/abs/2208.14433v1;2022-08-30;A Portable Multiscopic Camera for Novel View and Time Synthesis in  Dynamic Scenes;"We present a portable multiscopic camera system with a dedicated model for
novel view and time synthesis in dynamic scenes. Our goal is to render
high-quality images for a dynamic scene from any viewpoint at any time using
our portable multiscopic camera. To achieve such novel view and time synthesis,
we develop a physical multiscopic camera equipped with five cameras to train a
neural radiance field (NeRF) in both time and spatial domains for dynamic
scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal
coordinate, and 2D viewing direction) to view-dependent and time-varying
emitted radiance and volume density. Volume rendering is applied to render a
photo-realistic image at a specified camera pose and time. To improve the
robustness of our physical camera, we propose a camera parameter optimization
module and a temporal frame interpolation module to promote information
propagation across time. We conduct experiments on both real-world and
synthetic datasets to evaluate our system, and the results show that our
approach outperforms alternative solutions qualitatively and quantitatively.
Our code and dataset are available at https://yuenfuilau.github.io.";Tianjia Zhang<author:sep>Yuen-Fui Lau<author:sep>Qifeng Chen;http://arxiv.org/pdf/2208.14433v1;cs.CV;To be presented at IROS2022;nerf
2209.02417v1;http://arxiv.org/abs/2209.02417v1;2022-08-29;Volume Rendering Digest (for NeRF);"Neural Radiance Fields employ simple volume rendering as a way to overcome
the challenges of differentiating through ray-triangle intersections by
leveraging a probabilistic notion of visibility. This is achieved by assuming
the scene is composed by a cloud of light-emitting particles whose density
changes in space. This technical report summarizes the derivations for
differentiable volume rendering. It is a condensed version of previous reports,
but rewritten in the context of NeRF, and adopting its commonly used notation.";Andrea Tagliasacchi<author:sep>Ben Mildenhall;http://arxiv.org/pdf/2209.02417v1;cs.CV;Overleaf: https://www.overleaf.com/read/fkhpkzxhnyws;nerf
2208.12550v2;http://arxiv.org/abs/2208.12550v2;2022-08-26;Training and Tuning Generative Neural Radiance Fields for  Attribute-Conditional 3D-Aware Face Generation;"Generative Neural Radiance Fields (GNeRF) based 3D-aware GANs have
demonstrated remarkable capabilities in generating high-quality images while
maintaining strong 3D consistency. Notably, significant advancements have been
made in the domain of face generation. However, most existing models prioritize
view consistency over disentanglement, resulting in limited semantic/attribute
control during generation. To address this limitation, we propose a conditional
GNeRF model incorporating specific attribute labels as input to enhance the
controllability and disentanglement abilities of 3D-aware generative models.
Our approach builds upon a pre-trained 3D-aware face model, and we introduce a
Training as Init and Optimizing for Tuning (TRIOT) method to train a
conditional normalized flow module to enable the facial attribute editing, then
optimize the latent vector to improve attribute-editing precision further. Our
extensive experiments demonstrate that our model produces high-quality edits
with superior view consistency while preserving non-target regions. Code is
available at https://github.com/zhangqianhui/TT-GNeRF.";Jichao Zhang<author:sep>Aliaksandr Siarohin<author:sep>Yahui Liu<author:sep>Hao Tang<author:sep>Nicu Sebe<author:sep>Wei Wang;http://arxiv.org/pdf/2208.12550v2;cs.CV;13 pages;nerf
2208.11537v1;http://arxiv.org/abs/2208.11537v1;2022-08-24;PeRFception: Perception using Radiance Fields;"The recent progress in implicit 3D representation, i.e., Neural Radiance
Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible
in a differentiable manner. This new representation can effectively convey the
information of hundreds of high-resolution images in one compact format and
allows photorealistic synthesis of novel views. In this work, using the variant
of NeRF called Plenoxels, we create the first large-scale implicit
representation datasets for perception tasks, called the PeRFception, which
consists of two parts that incorporate both object-centric and scene-centric
scans for classification and segmentation. It shows a significant memory
compression rate (96.4\%) from the original dataset, while containing both 2D
and 3D information in a unified form. We construct the classification and
segmentation models that directly take as input this implicit format and also
propose a novel augmentation technique to avoid overfitting on backgrounds of
images. The code and data are publicly available in
https://postech-cvlab.github.io/PeRFception .";Yoonwoo Jeong<author:sep>Seungjoo Shin<author:sep>Junha Lee<author:sep>Christopher Choy<author:sep>Animashree Anandkumar<author:sep>Minsu Cho<author:sep>Jaesik Park;http://arxiv.org/pdf/2208.11537v1;cs.CV;Project Page: https://postech-cvlab.github.io/PeRFception/;nerf
2208.11300v2;http://arxiv.org/abs/2208.11300v2;2022-08-24;E-NeRF: Neural Radiance Fields from a Moving Event Camera;"Estimating neural radiance fields (NeRFs) from ""ideal"" images has been
extensively studied in the computer vision community. Most approaches assume
optimal illumination and slow camera motion. These assumptions are often
violated in robotic applications, where images may contain motion blur, and the
scene may not have suitable illumination. This can cause significant problems
for downstream tasks such as navigation, inspection, or visualization of the
scene. To alleviate these problems, we present E-NeRF, the first method which
estimates a volumetric scene representation in the form of a NeRF from a
fast-moving event camera. Our method can recover NeRFs during very fast motion
and in high-dynamic-range conditions where frame-based approaches fail. We show
that rendering high-quality frames is possible by only providing an event
stream as input. Furthermore, by combining events and frames, we can estimate
NeRFs of higher quality than state-of-the-art approaches under severe motion
blur. We also show that combining events and frames can overcome failure cases
of NeRF estimation in scenarios where only a few input views are available
without requiring additional regularization.";Simon Klenk<author:sep>Lukas Koestler<author:sep>Davide Scaramuzza<author:sep>Daniel Cremers;http://arxiv.org/pdf/2208.11300v2;cs.CV;revised RAL version + added suppl. material;nerf
2208.08728v1;http://arxiv.org/abs/2208.08728v1;2022-08-18;Neural Capture of Animatable 3D Human from Monocular Video;"We present a novel paradigm of building an animatable 3D human representation
from a monocular video input, such that it can be rendered in any unseen poses
and views. Our method is based on a dynamic Neural Radiance Field (NeRF) rigged
by a mesh-based parametric 3D human model serving as a geometry proxy. Previous
methods usually rely on multi-view videos or accurate 3D geometry information
as additional inputs; besides, most methods suffer from degraded quality when
generalized to unseen poses. We identify that the key to generalization is a
good input embedding for querying dynamic NeRF: A good input embedding should
define an injective mapping in the full volumetric space, guided by surface
mesh deformation under pose variation. Based on this observation, we propose to
embed the input query with its relationship to local surface regions spanned by
a set of geodesic nearest neighbors on mesh vertices. By including both
position and relative distance information, our embedding defines a
distance-preserved deformation mapping and generalizes well to unseen poses. To
reduce the dependency on additional inputs, we first initialize per-frame 3D
meshes using off-the-shelf tools and then propose a pipeline to jointly
optimize NeRF and refine the initial mesh. Extensive experiments show our
method can synthesize plausible human rendering results under unseen poses and
views.";Gusi Te<author:sep>Xiu Li<author:sep>Xiao Li<author:sep>Jinglu Wang<author:sep>Wei Hu<author:sep>Yan Lu;http://arxiv.org/pdf/2208.08728v1;cs.CV;ECCV 2022;nerf
2208.07903v2;http://arxiv.org/abs/2208.07903v2;2022-08-16;Casual Indoor HDR Radiance Capture from Omnidirectional Images;"We present PanoHDR-NeRF, a neural representation of the full HDR radiance
field of an indoor scene, and a pipeline to capture it casually, without
elaborate setups or complex capture protocols. First, a user captures a low
dynamic range (LDR) omnidirectional video of the scene by freely waving an
off-the-shelf camera around the scene. Then, an LDR2HDR network uplifts the
captured LDR frames to HDR, which are used to train a tailored NeRF++ model.
The resulting PanoHDR-NeRF can render full HDR images from any location of the
scene. Through experiments on a novel test dataset of real scenes with the
ground truth HDR radiance captured at locations not seen during training, we
show that PanoHDR-NeRF predicts plausible HDR radiance from any scene point. We
also show that the predicted radiance can synthesize correct lighting effects,
enabling the augmentation of indoor scenes with synthetic objects that are lit
correctly. Datasets and code are available at
https://lvsn.github.io/PanoHDR-NeRF/.";Pulkit Gera<author:sep>Mohammad Reza Karimi Dastjerdi<author:sep>Charles Renaud<author:sep>P. J. Narayanan<author:sep>Jean-François Lalonde;http://arxiv.org/pdf/2208.07903v2;cs.CV;BMVC 2022;nerf
2208.07059v2;http://arxiv.org/abs/2208.07059v2;2022-08-15;UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance  Fields for 3D Scene;"3D scenes photorealistic stylization aims to generate photorealistic images
from arbitrary novel views according to a given style image while ensuring
consistency when rendering from different viewpoints. Some existing stylization
methods with neural radiance fields can effectively predict stylized scenes by
combining the features of the style image with multi-view images to train 3D
scenes. However, these methods generate novel view images that contain
objectionable artifacts. Besides, they cannot achieve universal photorealistic
stylization for a 3D scene. Therefore, a styling image must retrain a 3D scene
representation network based on a neural radiation field. We propose a novel 3D
scene photorealistic style transfer framework to address these issues. It can
realize photorealistic 3D scene style transfer with a 2D style image. We first
pre-trained a 2D photorealistic style transfer network, which can meet the
photorealistic style transfer between any given content image and style image.
Then, we use voxel features to optimize a 3D scene and get the geometric
representation of the scene. Finally, we jointly optimize a hyper network to
realize the scene photorealistic style transfer of arbitrary style images. In
the transfer stage, we use a pre-trained 2D photorealistic network to constrain
the photorealistic style of different views and different style images in the
3D scene. The experimental results show that our method not only realizes the
3D photorealistic style transfer of arbitrary style images but also outperforms
the existing methods in terms of visual quality and consistency. Project
page:https://semchan.github.io/UPST_NeRF.";Yaosen Chen<author:sep>Qi Yuan<author:sep>Zhiqiang Li<author:sep>Yuegen Liu<author:sep>Wei Wang<author:sep>Chaoping Xie<author:sep>Xuming Wen<author:sep>Qien Yu;http://arxiv.org/pdf/2208.07059v2;cs.CV;arXiv admin note: text overlap with arXiv:2205.12183 by other authors;nerf
2208.07227v2;http://arxiv.org/abs/2208.07227v2;2022-08-15;DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images;"In this paper, we study the problem of 3D scene geometry decomposition and
manipulation from 2D views. By leveraging the recent implicit neural
representation techniques, particularly the appealing neural radiance fields,
we introduce an object field component to learn unique codes for all individual
objects in 3D space only from 2D supervision. The key to this component is a
series of carefully designed loss functions to enable every 3D point,
especially in non-occupied space, to be effectively optimized even without 3D
labels. In addition, we introduce an inverse query algorithm to freely
manipulate any specified 3D object shape in the learned scene representation.
Notably, our manipulation algorithm can explicitly tackle key issues such as
object collisions and visual occlusions. Our method, called DM-NeRF, is among
the first to simultaneously reconstruct, decompose, manipulate and render
complex 3D scenes in a single pipeline. Extensive experiments on three datasets
clearly show that our method can accurately decompose all 3D objects from 2D
views, allowing any interested object to be freely manipulated in 3D space such
as translation, rotation, size adjustment, and deformation.";Bing Wang<author:sep>Lu Chen<author:sep>Bo Yang;http://arxiv.org/pdf/2208.07227v2;cs.CV;"ICLR 2023. Our data and code are available at:
  https://github.com/vLAR-group/DM-NeRF";nerf
2208.06335v1;http://arxiv.org/abs/2208.06335v1;2022-08-12;OmniVoxel: A Fast and Precise Reconstruction Method of Omnidirectional  Neural Radiance Field;"This paper proposes a method to reconstruct the neural radiance field with
equirectangular omnidirectional images. Implicit neural scene representation
with a radiance field can reconstruct the 3D shape of a scene continuously
within a limited spatial area. However, training a fully implicit
representation on commercial PC hardware requires a lot of time and computing
resources (15 $\sim$ 20 hours per scene). Therefore, we propose a method to
accelerate this process significantly (20 $\sim$ 40 minutes per scene). Instead
of using a fully implicit representation of rays for radiance field
reconstruction, we adopt feature voxels that contain density and color features
in tensors. Considering omnidirectional equirectangular input and the camera
layout, we use spherical voxelization for representation instead of cubic
representation. Our voxelization method could balance the reconstruction
quality of the inner scene and outer scene. In addition, we adopt the
axis-aligned positional encoding method on the color features to increase the
total image quality. Our method achieves satisfying empirical performance on
synthetic datasets with random camera poses. Moreover, we test our method with
real scenes which contain complex geometries and also achieve state-of-the-art
performance. Our code and complete dataset will be released at the same time as
the paper publication.";Qiaoge Li<author:sep>Itsuki Ueda<author:sep>Chun Xie<author:sep>Hidehiko Shishido<author:sep>Itaru Kitahara;http://arxiv.org/pdf/2208.06335v1;cs.CV;will be appeared in GCCE 2022;
2208.05751v2;http://arxiv.org/abs/2208.05751v2;2022-08-11;FDNeRF: Few-shot Dynamic Neural Radiance Fields for Face Reconstruction  and Expression Editing;"We propose a Few-shot Dynamic Neural Radiance Field (FDNeRF), the first
NeRF-based method capable of reconstruction and expression editing of 3D faces
based on a small number of dynamic images. Unlike existing dynamic NeRFs that
require dense images as input and can only be modeled for a single identity,
our method enables face reconstruction across different persons with few-shot
inputs. Compared to state-of-the-art few-shot NeRFs designed for modeling
static scenes, the proposed FDNeRF accepts view-inconsistent dynamic inputs and
supports arbitrary facial expression editing, i.e., producing faces with novel
expressions beyond the input ones. To handle the inconsistencies between
dynamic inputs, we introduce a well-designed conditional feature warping (CFW)
module to perform expression conditioned warping in 2D feature space, which is
also identity adaptive and 3D constrained. As a result, features of different
expressions are transformed into the target ones. We then construct a radiance
field based on these view-consistent features and use volumetric rendering to
synthesize novel views of the modeled faces. Extensive experiments with
quantitative and qualitative evaluation demonstrate that our method outperforms
existing dynamic and few-shot NeRFs on both 3D face reconstruction and
expression editing tasks. Code is available at
https://github.com/FDNeRF/FDNeRF.";Jingbo Zhang<author:sep>Xiaoyu Li<author:sep>Ziyu Wan<author:sep>Can Wang<author:sep>Jing Liao;http://arxiv.org/pdf/2208.05751v2;cs.CV;"Accepted at SIGGRAPH Asia 2022. Project page:
  https://fdnerf.github.io";nerf
2208.05963v2;http://arxiv.org/abs/2208.05963v2;2022-08-11;RelPose: Predicting Probabilistic Relative Rotation for Single Objects  in the Wild;"We describe a data-driven method for inferring the camera viewpoints given
multiple images of an arbitrary object. This task is a core component of
classic geometric pipelines such as SfM and SLAM, and also serves as a vital
pre-processing requirement for contemporary neural approaches (e.g. NeRF) to
object reconstruction and view synthesis. In contrast to existing
correspondence-driven methods that do not perform well given sparse views, we
propose a top-down prediction based approach for estimating camera viewpoints.
Our key technical insight is the use of an energy-based formulation for
representing distributions over relative camera rotations, thus allowing us to
explicitly represent multiple camera modes arising from object symmetries or
views. Leveraging these relative predictions, we jointly estimate a consistent
set of camera rotations from multiple images. We show that our approach
outperforms state-of-the-art SfM and SLAM methods given sparse images on both
seen and unseen categories. Further, our probabilistic approach significantly
outperforms directly regressing relative poses, suggesting that modeling
multimodality is important for coherent joint reconstruction. We demonstrate
that our system can be a stepping stone toward in-the-wild reconstruction from
multi-view datasets. The project page with code and videos can be found at
https://jasonyzhang.com/relpose.";Jason Y. Zhang<author:sep>Deva Ramanan<author:sep>Shubham Tulsiani;http://arxiv.org/pdf/2208.05963v2;cs.CV;In ECCV 2022. V2: updated references;nerf
2208.04717v2;http://arxiv.org/abs/2208.04717v2;2022-08-09;Cascaded and Generalizable Neural Radiance Fields for Fast View  Synthesis;"We present CG-NeRF, a cascade and generalizable neural radiance fields method
for view synthesis. Recent generalizing view synthesis methods can render
high-quality novel views using a set of nearby input views. However, the
rendering speed is still slow due to the nature of uniformly-point sampling of
neural radiance fields. Existing scene-specific methods can train and render
novel views efficiently but can not generalize to unseen data. Our approach
addresses the problems of fast and generalizing view synthesis by proposing two
novel modules: a coarse radiance fields predictor and a convolutional-based
neural renderer. This architecture infers consistent scene geometry based on
the implicit neural fields and renders new views efficiently using a single
GPU. We first train CG-NeRF on multiple 3D scenes of the DTU dataset, and the
network can produce high-quality and accurate novel views on unseen real and
synthetic data using only photometric losses. Moreover, our method can leverage
a denser set of reference images of a single scene to produce accurate novel
views without relying on additional explicit representations and still
maintains the high-speed rendering of the pre-trained model. Experimental
results show that CG-NeRF outperforms state-of-the-art generalizable neural
rendering methods on various synthetic and real datasets.";Phong Nguyen-Ha<author:sep>Lam Huynh<author:sep>Esa Rahtu<author:sep>Jiri Matas<author:sep>Janne Heikkila;http://arxiv.org/pdf/2208.04717v2;cs.CV;"Accepted at IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)";nerf
2208.02705v2;http://arxiv.org/abs/2208.02705v2;2022-08-04;360Roam: Real-Time Indoor Roaming Using Geometry-Aware 360$^\circ$  Radiance Fields;"Virtual tour among sparse 360$^\circ$ images is widely used while hindering
smooth and immersive roaming experiences. The emergence of Neural Radiance
Field (NeRF) has showcased significant progress in synthesizing novel views,
unlocking the potential for immersive scene exploration. Nevertheless, previous
NeRF works primarily focused on object-centric scenarios, resulting in
noticeable performance degradation when applied to outward-facing and
large-scale scenes due to limitations in scene parameterization. To achieve
seamless and real-time indoor roaming, we propose a novel approach using
geometry-aware radiance fields with adaptively assigned local radiance fields.
Initially, we employ multiple 360$^\circ$ images of an indoor scene to
progressively reconstruct explicit geometry in the form of a probabilistic
occupancy map, derived from a global omnidirectional radiance field.
Subsequently, we assign local radiance fields through an adaptive
divide-and-conquer strategy based on the recovered geometry. By incorporating
geometry-aware sampling and decomposition of the global radiance field, our
system effectively utilizes positional encoding and compact neural networks to
enhance rendering quality and speed. Additionally, the extracted floorplan of
the scene aids in providing visual guidance, contributing to a realistic
roaming experience. To demonstrate the effectiveness of our system, we curated
a diverse dataset of 360$^\circ$ images encompassing various real-life scenes,
on which we conducted extensive experiments. Quantitative and qualitative
comparisons against baseline approaches illustrated the superior performance of
our system in large-scale indoor scene roaming.";Huajian Huang<author:sep>Yingshu Chen<author:sep>Tianjia Zhang<author:sep>Sai-Kit Yeung;http://arxiv.org/pdf/2208.02705v2;cs.CV;;nerf
2208.01421v2;http://arxiv.org/abs/2208.01421v2;2022-08-02;T4DT: Tensorizing Time for Learning Temporal 3D Visual Data;"Unlike 2D raster images, there is no single dominant representation for 3D
visual data processing. Different formats like point clouds, meshes, or
implicit functions each have their strengths and weaknesses. Still, grid
representations such as signed distance functions have attractive properties
also in 3D. In particular, they offer constant-time random access and are
eminently suitable for modern machine learning. Unfortunately, the storage size
of a grid grows exponentially with its dimension. Hence they often exceed
memory limits even at moderate resolution. This work proposes using low-rank
tensor formats, including the Tucker, tensor train, and quantics tensor train
decompositions, to compress time-varying 3D data. Our method iteratively
computes, voxelizes, and compresses each frame's truncated signed distance
function and applies tensor rank truncation to condense all frames into a
single, compressed tensor that represents the entire 4D scene. We show that
low-rank tensor compression is extremely compact to store and query
time-varying signed distance functions. It significantly reduces the memory
footprint of 4D scenes while remarkably preserving their geometric quality.
Unlike existing, iterative learning-based approaches like DeepSDF and NeRF, our
method uses a closed-form algorithm with theoretical guarantees.";Mikhail Usvyatsov<author:sep>Rafael Ballester-Rippoll<author:sep>Lina Bashaeva<author:sep>Konrad Schindler<author:sep>Gonzalo Ferrer<author:sep>Ivan Oseledets;http://arxiv.org/pdf/2208.01421v2;cs.CV;;nerf
2208.00945v1;http://arxiv.org/abs/2208.00945v1;2022-08-01;DoF-NeRF: Depth-of-Field Meets Neural Radiance Fields;"Neural Radiance Field (NeRF) and its variants have exhibited great success on
representing 3D scenes and synthesizing photo-realistic novel views. However,
they are generally based on the pinhole camera model and assume all-in-focus
inputs. This limits their applicability as images captured from the real world
often have finite depth-of-field (DoF). To mitigate this issue, we introduce
DoF-NeRF, a novel neural rendering approach that can deal with shallow DoF
inputs and can simulate DoF effect. In particular, it extends NeRF to simulate
the aperture of lens following the principles of geometric optics. Such a
physical guarantee allows DoF-NeRF to operate views with different focus
configurations. Benefiting from explicit aperture modeling, DoF-NeRF also
enables direct manipulation of DoF effect by adjusting virtual aperture and
focus parameters. It is plug-and-play and can be inserted into NeRF-based
frameworks. Experiments on synthetic and real-world datasets show that,
DoF-NeRF not only performs comparably with NeRF in the all-in-focus setting,
but also can synthesize all-in-focus novel views conditioned on shallow DoF
inputs. An interesting application of DoF-NeRF to DoF rendering is also
demonstrated. The source code will be made available at
https://github.com/zijinwuzijin/DoF-NeRF.";Zijin Wu<author:sep>Xingyi Li<author:sep>Juewen Peng<author:sep>Hao Lu<author:sep>Zhiguo Cao<author:sep>Weicai Zhong;http://arxiv.org/pdf/2208.00945v1;cs.CV;Accepted by ACMMM 2022;nerf
2208.00164v3;http://arxiv.org/abs/2208.00164v3;2022-07-30;Distilled Low Rank Neural Radiance Field with Quantization for Light  Field Compression;"We propose in this paper a Quantized Distilled Low-Rank Neural Radiance Field
(QDLR-NeRF) representation for the task of light field compression. While
existing compression methods encode the set of light field sub-aperture images,
our proposed method learns an implicit scene representation in the form of a
Neural Radiance Field (NeRF), which also enables view synthesis. To reduce its
size, the model is first learned under a Low-Rank (LR) constraint using a
Tensor Train (TT) decomposition within an Alternating Direction Method of
Multipliers (ADMM) optimization framework. To further reduce the model's size,
the components of the tensor train decomposition need to be quantized. However,
simultaneously considering the optimization of the NeRF model with both the
low-rank constraint and rate-constrained weight quantization is challenging. To
address this difficulty, we introduce a network distillation operation that
separates the low-rank approximation and the weight quantization during network
training. The information from the initial LR-constrained NeRF (LR-NeRF) is
distilled into a model of much smaller dimension (DLR-NeRF) based on the TT
decomposition of the LR-NeRF. We then learn an optimized global codebook to
quantize all TT components, producing the final QDLR-NeRF. Experimental results
show that our proposed method yields better compression efficiency compared to
state-of-the-art methods, and it additionally has the advantage of allowing the
synthesis of any light field view with high quality.";Jinglei Shi<author:sep>Christine Guillemot;http://arxiv.org/pdf/2208.00164v3;cs.CV;;nerf
2208.00277v5;http://arxiv.org/abs/2208.00277v5;2022-07-30;MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient  Neural Field Rendering on Mobile Architectures;"Neural Radiance Fields (NeRFs) have demonstrated amazing ability to
synthesize images of 3D scenes from novel views. However, they rely upon
specialized volumetric rendering algorithms based on ray marching that are
mismatched to the capabilities of widely deployed graphics hardware. This paper
introduces a new NeRF representation based on textured polygons that can
synthesize novel images efficiently with standard rendering pipelines. The NeRF
is represented as a set of polygons with textures representing binary opacities
and feature vectors. Traditional rendering of the polygons with a z-buffer
yields an image with features at every pixel, which are interpreted by a small,
view-dependent MLP running in a fragment shader to produce a final pixel color.
This approach enables NeRFs to be rendered with the traditional polygon
rasterization pipeline, which provides massive pixel-level parallelism,
achieving interactive frame rates on a wide range of compute platforms,
including mobile phones.";Zhiqin Chen<author:sep>Thomas Funkhouser<author:sep>Peter Hedman<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2208.00277v5;cs.CV;"CVPR 2023. Project page: https://mobile-nerf.github.io, code:
  https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf";nerf
2207.14455v1;http://arxiv.org/abs/2207.14455v1;2022-07-29;Neural Density-Distance Fields;"The success of neural fields for 3D vision tasks is now indisputable.
Following this trend, several methods aiming for visual localization (e.g.,
SLAM) have been proposed to estimate distance or density fields using neural
fields. However, it is difficult to achieve high localization performance by
only density fields-based methods such as Neural Radiance Field (NeRF) since
they do not provide density gradient in most empty regions. On the other hand,
distance field-based methods such as Neural Implicit Surface (NeuS) have
limitations in objects' surface shapes. This paper proposes Neural
Density-Distance Field (NeDDF), a novel 3D representation that reciprocally
constrains the distance and density fields. We extend distance field
formulation to shapes with no explicit boundary surface, such as fur or smoke,
which enable explicit conversion from distance field to density field.
Consistent distance and density fields realized by explicit conversion enable
both robustness to initial values and high-quality registration. Furthermore,
the consistency between fields allows fast convergence from sparse point
clouds. Experiments show that NeDDF can achieve high localization performance
while providing comparable results to NeRF on novel view synthesis. The code is
available at https://github.com/ueda0319/neddf.";Itsuki Ueda<author:sep>Yoshihiro Fukuhara<author:sep>Hirokatsu Kataoka<author:sep>Hiroaki Aizawa<author:sep>Hidehiko Shishido<author:sep>Itaru Kitahara;http://arxiv.org/pdf/2207.14455v1;cs.CV;ECCV 2022 (poster). project page: https://ueda0319.github.io/neddf/;nerf
2207.14741v3;http://arxiv.org/abs/2207.14741v3;2022-07-29;End-to-end View Synthesis via NeRF Attention;"In this paper, we present a simple seq2seq formulation for view synthesis
where we take a set of ray points as input and output colors corresponding to
the rays. Directly applying a standard transformer on this seq2seq formulation
has two limitations. First, the standard attention cannot successfully fit the
volumetric rendering procedure, and therefore high-frequency components are
missing in the synthesized views. Second, applying global attention to all rays
and pixels is extremely inefficient. Inspired by the neural radiance field
(NeRF), we propose the NeRF attention (NeRFA) to address the above problems. On
the one hand, NeRFA considers the volumetric rendering equation as a soft
feature modulation procedure. In this way, the feature modulation enhances the
transformers with the NeRF-like inductive bias. On the other hand, NeRFA
performs multi-stage attention to reduce the computational overhead.
Furthermore, the NeRFA model adopts the ray and pixel transformers to learn the
interactions between rays and pixels. NeRFA demonstrates superior performance
over NeRF and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D.
Besides, NeRFA establishes a new state-of-the-art under two settings: the
single-scene view synthesis and the category-centric novel view synthesis.";Zelin Zhao<author:sep>Jiaya Jia;http://arxiv.org/pdf/2207.14741v3;cs.CV;Fixed reference formatting issues;nerf
2207.13298v3;http://arxiv.org/abs/2207.13298v3;2022-07-27;Is Attention All That NeRF Needs?;"We present Generalizable NeRF Transformer (GNT), a transformer-based
architecture that reconstructs Neural Radiance Fields (NeRFs) and learns to
renders novel views on the fly from source views. While prior works on NeRFs
optimize a scene representation by inverting a handcrafted rendering equation,
GNT achieves neural representation and rendering that generalizes across scenes
using transformers at two stages. (1) The view transformer leverages multi-view
geometry as an inductive bias for attention-based scene representation, and
predicts coordinate-aligned features by aggregating information from epipolar
lines on the neighboring views. (2) The ray transformer renders novel views
using attention to decode the features from the view transformer along the
sampled points during ray marching. Our experiments demonstrate that when
optimized on a single scene, GNT can successfully reconstruct NeRF without an
explicit rendering formula due to the learned ray renderer. When trained on
multiple scenes, GNT consistently achieves state-of-the-art performance when
transferring to unseen scenes and outperform all other methods by ~10% on
average. Our analysis of the learned attention maps to infer depth and
occlusion indicate that attention enables learning a physically-grounded
rendering. Our results show the promise of transformers as a universal modeling
tool for graphics. Please refer to our project page for video results:
https://vita-group.github.io/GNT/.";Mukund Varma T<author:sep>Peihao Wang<author:sep>Xuxi Chen<author:sep>Tianlong Chen<author:sep>Subhashini Venugopalan<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2207.13298v3;cs.CV;International Conference on Learning Representations (ICLR), 2023;nerf
2207.12298v1;http://arxiv.org/abs/2207.12298v1;2022-07-25;Deforming Radiance Fields with Cages;"Recent advances in radiance fields enable photorealistic rendering of static
or dynamic 3D scenes, but still do not support explicit deformation that is
used for scene manipulation or animation. In this paper, we propose a method
that enables a new type of deformation of the radiance field: free-form
radiance field deformation. We use a triangular mesh that encloses the
foreground object called cage as an interface, and by manipulating the cage
vertices, our approach enables the free-form deformation of the radiance field.
The core of our approach is cage-based deformation which is commonly used in
mesh deformation. We propose a novel formulation to extend it to the radiance
field, which maps the position and the view direction of the sampling points
from the deformed space to the canonical space, thus enabling the rendering of
the deformed scene. The deformation results of the synthetic datasets and the
real-world datasets demonstrate the effectiveness of our approach.";Tianhan Xu<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2207.12298v1;cs.CV;ECCV 2022. Project page: https://xth430.github.io/deforming-nerf/;
2207.11757v1;http://arxiv.org/abs/2207.11757v1;2022-07-24;Learning Generalizable Light Field Networks from Few Images;"We explore a new strategy for few-shot novel view synthesis based on a neural
light field representation. Given a target camera pose, an implicit neural
network maps each ray to its target pixel's color directly. The network is
conditioned on local ray features generated by coarse volumetric rendering from
an explicit 3D feature volume. This volume is built from the input images using
a 3D ConvNet. Our method achieves competitive performances on synthetic and
real MVS data with respect to state-of-the-art neural radiance field based
competition, while offering a 100 times faster rendering.";Qian Li<author:sep>Franck Multon<author:sep>Adnane Boukhayma;http://arxiv.org/pdf/2207.11757v1;cs.CV;;
2207.11770v1;http://arxiv.org/abs/2207.11770v1;2022-07-24;Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head  Synthesis;"Talking head synthesis is an emerging technology with wide applications in
film dubbing, virtual avatars and online education. Recent NeRF-based methods
generate more natural talking videos, as they better capture the 3D structural
information of faces. However, a specific model needs to be trained for each
identity with a large dataset. In this paper, we propose Dynamic Facial
Radiance Fields (DFRF) for few-shot talking head synthesis, which can rapidly
generalize to an unseen identity with few training data. Different from the
existing NeRF-based methods which directly encode the 3D geometry and
appearance of a specific person into the network, our DFRF conditions face
radiance field on 2D appearance images to learn the face prior. Thus the facial
radiance field can be flexibly adjusted to the new identity with few reference
images. Additionally, for better modeling of the facial deformations, we
propose a differentiable face warping module conditioned on audio signals to
deform all reference images to the query space. Extensive experiments show that
with only tens of seconds of training clip available, our proposed DFRF can
synthesize natural and high-quality audio-driven talking head videos for novel
identities with only 40k iterations. We highly recommend readers view our
supplementary video for intuitive comparisons. Code is available in
https://sstzal.github.io/DFRF/.";Shuai Shen<author:sep>Wanhua Li<author:sep>Zheng Zhu<author:sep>Yueqi Duan<author:sep>Jie Zhou<author:sep>Jiwen Lu;http://arxiv.org/pdf/2207.11770v1;cs.CV;Accepted by ECCV 2022. Project page: https://sstzal.github.io/DFRF/;nerf
2207.11406v2;http://arxiv.org/abs/2207.11406v2;2022-07-23;PS-NeRF: Neural Inverse Rendering for Multi-view Photometric Stereo;"Traditional multi-view photometric stereo (MVPS) methods are often composed
of multiple disjoint stages, resulting in noticeable accumulated errors. In
this paper, we present a neural inverse rendering method for MVPS based on
implicit representation. Given multi-view images of a non-Lambertian object
illuminated by multiple unknown directional lights, our method jointly
estimates the geometry, materials, and lights. Our method first employs
multi-light images to estimate per-view surface normal maps, which are used to
regularize the normals derived from the neural radiance field. It then jointly
optimizes the surface normals, spatially-varying BRDFs, and lights based on a
shadow-aware differentiable rendering layer. After optimization, the
reconstructed object can be used for novel-view rendering, relighting, and
material editing. Experiments on both synthetic and real datasets demonstrate
that our method achieves far more accurate shape reconstruction than existing
MVPS and neural rendering methods. Our code and model can be found at
https://ywq.github.io/psnerf.";Wenqi Yang<author:sep>Guanying Chen<author:sep>Chaofeng Chen<author:sep>Zhenfang Chen<author:sep>Kwan-Yee K. Wong;http://arxiv.org/pdf/2207.11406v2;cs.CV;ECCV 2022, Project page: https://ywq.github.io/psnerf;nerf
2207.11368v1;http://arxiv.org/abs/2207.11368v1;2022-07-22;Neural-Sim: Learning to Generate Training Data with NeRF;"Training computer vision models usually requires collecting and labeling vast
amounts of imagery under a diverse set of scene configurations and properties.
This process is incredibly time-consuming, and it is challenging to ensure that
the captured data distribution maps well to the target domain of an application
scenario. Recently, synthetic data has emerged as a way to address both of
these issues. However, existing approaches either require human experts to
manually tune each scene property or use automatic methods that provide little
to no control; this requires rendering large amounts of random data variations,
which is slow and is often suboptimal for the target domain. We present the
first fully differentiable synthetic data pipeline that uses Neural Radiance
Fields (NeRFs) in a closed-loop with a target application's loss function. Our
approach generates data on-demand, with no human labor, to maximize accuracy
for a target task. We illustrate the effectiveness of our method on synthetic
and real-world object detection tasks. We also introduce a new
""YCB-in-the-Wild"" dataset and benchmark that provides a test scenario for
object detection with varied poses in real-world environments.";Yunhao Ge<author:sep>Harkirat Behl<author:sep>Jiashu Xu<author:sep>Suriya Gunasekar<author:sep>Neel Joshi<author:sep>Yale Song<author:sep>Xin Wang<author:sep>Laurent Itti<author:sep>Vibhav Vineet;http://arxiv.org/pdf/2207.11368v1;cs.CV;ECCV 2022;nerf
2207.10312v2;http://arxiv.org/abs/2207.10312v2;2022-07-21;AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance  Fields;"Novel view synthesis has recently been revolutionized by learning neural
radiance fields directly from sparse observations. However, rendering images
with this new paradigm is slow due to the fact that an accurate quadrature of
the volume rendering equation requires a large number of samples for each ray.
Previous work has mainly focused on speeding up the network evaluations that
are associated with each sample point, e.g., via caching of radiance values
into explicit spatial data structures, but this comes at the expense of model
compactness. In this paper, we propose a novel dual-network architecture that
takes an orthogonal direction by learning how to best reduce the number of
required sample points. To this end, we split our network into a sampling and
shading network that are jointly trained. Our training scheme employs fixed
sample positions along each ray, and incrementally introduces sparsity
throughout training to achieve high quality even at low sample counts. After
fine-tuning with the target number of samples, the resulting compact neural
representation can be rendered in real-time. Our experiments demonstrate that
our approach outperforms concurrent compact neural representations in terms of
quality and frame rate and performs on par with highly efficient hybrid
representations. Code and supplementary material is available at
https://thomasneff.github.io/adanerf.";Andreas Kurz<author:sep>Thomas Neff<author:sep>Zhaoyang Lv<author:sep>Michael Zollhöfer<author:sep>Markus Steinberger;http://arxiv.org/pdf/2207.10312v2;cs.CV;ECCV 2022. Project page: https://thomasneff.github.io/adanerf;nerf
2207.10257v2;http://arxiv.org/abs/2207.10257v2;2022-07-21;Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for  Editable Portrait Image Synthesis;"Over the years, 2D GANs have achieved great successes in photorealistic
portrait generation. However, they lack 3D understanding in the generation
process, thus they suffer from multi-view inconsistency problem. To alleviate
the issue, many 3D-aware GANs have been proposed and shown notable results, but
3D GANs struggle with editing semantic attributes. The controllability and
interpretability of 3D GANs have not been much explored. In this work, we
propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware
GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of
discovering semantic attributes during training and controlling them in an
unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN
to obtain a high-fidelity 3D-controllable generator. Unlike existing
latent-based methods allowing implicit pose control, the proposed
3D-controllable StyleGAN enables explicit pose control over portrait
generation. This distillation allows direct compatibility between 3D control
and many StyleGAN-based techniques (e.g., inversion and stylization), and also
brings an advantage in terms of computational resources. Our codes are
available at https://github.com/jgkwak95/SURF-GAN.";Jeong-gi Kwak<author:sep>Yuanming Li<author:sep>Dongsik Yoon<author:sep>Donghyeon Kim<author:sep>David Han<author:sep>Hanseok Ko;http://arxiv.org/pdf/2207.10257v2;cs.CV;ECCV 2022, project page: https://jgkwak95.github.io/surfgan/;nerf
2207.10662v2;http://arxiv.org/abs/2207.10662v2;2022-07-21;Generalizable Patch-Based Neural Rendering;"Neural rendering has received tremendous attention since the advent of Neural
Radiance Fields (NeRF), and has pushed the state-of-the-art on novel-view
synthesis considerably. The recent focus has been on models that overfit to a
single scene, and the few attempts to learn models that can synthesize novel
views of unseen scenes mostly consist of combining deep convolutional features
with a NeRF-like model. We propose a different paradigm, where no deep features
and no NeRF-like volume rendering are needed. Our method is capable of
predicting the color of a target ray in a novel scene directly, just from a
collection of patches sampled from the scene. We first leverage epipolar
geometry to extract patches along the epipolar lines of each reference view.
Each patch is linearly projected into a 1D feature vector and a sequence of
transformers process the collection. For positional encoding, we parameterize
rays as in a light field representation, with the crucial difference that the
coordinates are canonicalized with respect to the target ray, which makes our
method independent of the reference frame and improves generalization. We show
that our approach outperforms the state-of-the-art on novel view synthesis of
unseen scenes even when being trained with considerably less data than prior
work.";Mohammed Suhail<author:sep>Carlos Esteves<author:sep>Leonid Sigal<author:sep>Ameesh Makadia;http://arxiv.org/pdf/2207.10662v2;cs.CV;"Project Page with code and results at
  https://mohammedsuhail.net/gen_patch_neural_rendering/";nerf
2207.09193v1;http://arxiv.org/abs/2207.09193v1;2022-07-19;NDF: Neural Deformable Fields for Dynamic Human Modelling;"We propose Neural Deformable Fields (NDF), a new representation for dynamic
human digitization from a multi-view video. Recent works proposed to represent
a dynamic human body with shared canonical neural radiance fields which links
to the observation space with deformation fields estimations. However, the
learned canonical representation is static and the current design of the
deformation fields is not able to represent large movements or detailed
geometry changes. In this paper, we propose to learn a neural deformable field
wrapped around a fitted parametric body model to represent the dynamic human.
The NDF is spatially aligned by the underlying reference surface. A neural
network is then learned to map pose to the dynamics of NDF. The proposed NDF
representation can synthesize the digitized performer with novel views and
novel poses with a detailed and reasonable dynamic appearance. Experiments show
that our method significantly outperforms recent human synthesis methods.";Ruiqi Zhang<author:sep>Jie Chen;http://arxiv.org/pdf/2207.09193v1;cs.CV;16 pages, 7 figures. Accepted by ECCV 2022;
2207.06793v1;http://arxiv.org/abs/2207.06793v1;2022-07-14;Neural apparent BRDF fields for multiview photometric stereo;"We propose to tackle the multiview photometric stereo problem using an
extension of Neural Radiance Fields (NeRFs), conditioned on light source
direction. The geometric part of our neural representation predicts surface
normal direction, allowing us to reason about local surface reflectance. The
appearance part of our neural representation is decomposed into a neural
bidirectional reflectance function (BRDF), learnt as part of the fitting
process, and a shadow prediction network (conditioned on light source
direction) allowing us to model the apparent BRDF. This balance of learnt
components with inductive biases based on physical image formation models
allows us to extrapolate far from the light source and viewer directions
observed during training. We demonstrate our approach on a multiview
photometric stereo benchmark and show that competitive performance can be
obtained with the neural density representation of a NeRF.";Meghna Asthana<author:sep>William A. P. Smith<author:sep>Patrik Huber;http://arxiv.org/pdf/2207.06793v1;cs.CV;9 pages, 6 figures, 1 table;nerf
2207.05736v2;http://arxiv.org/abs/2207.05736v2;2022-07-12;Vision Transformer for NeRF-Based View Synthesis from a Single Input  Image;"Although neural radiance fields (NeRF) have shown impressive advances for
novel view synthesis, most methods typically require multiple input images of
the same scene with accurate camera poses. In this work, we seek to
substantially reduce the inputs to a single unposed image. Existing approaches
condition on local image features to reconstruct a 3D object, but often render
blurry predictions at viewpoints that are far away from the source view. To
address this issue, we propose to leverage both the global and local features
to form an expressive 3D representation. The global features are learned from a
vision transformer, while the local features are extracted from a 2D
convolutional network. To synthesize a novel view, we train a multilayer
perceptron (MLP) network conditioned on the learned 3D representation to
perform volume rendering. This novel 3D representation allows the network to
reconstruct unseen regions without enforcing constraints like symmetry or
canonical coordinate systems. Our method can render novel views from only a
single input image and generalize across multiple object categories using a
single model. Quantitative and qualitative evaluations demonstrate that the
proposed method achieves state-of-the-art performance and renders richer
details than existing approaches.";Kai-En Lin<author:sep>Lin Yen-Chen<author:sep>Wei-Sheng Lai<author:sep>Tsung-Yi Lin<author:sep>Yi-Chang Shih<author:sep>Ravi Ramamoorthi;http://arxiv.org/pdf/2207.05736v2;cs.CV;"WACV 2023 Project website:
  https://cseweb.ucsd.edu/~viscomp/projects/VisionNeRF/";nerf
2207.05009v1;http://arxiv.org/abs/2207.05009v1;2022-07-11;A Learned Radiance-Field Representation for Complex Luminaires;"We propose an efficient method for rendering complex luminaires using a
high-quality octree-based representation of the luminaire emission. Complex
luminaires are a particularly challenging problem in rendering, due to their
caustic light paths inside the luminaire. We reduce the geometric complexity of
luminaires by using a simple proxy geometry and encode the visually-complex
emitted light field by using a neural radiance field. We tackle the multiple
challenges of using NeRFs for representing luminaires, including their high
dynamic range, high-frequency content and null-emission areas, by proposing a
specialized loss function. For rendering, we distill our luminaires' NeRF into
a Plenoctree, which we can be easily integrated into traditional rendering
systems. Our approach allows for speed-ups of up to 2 orders of magnitude in
scenes containing complex luminaires introducing minimal error.";Jorge Condor<author:sep>Adrián Jarabo;http://arxiv.org/pdf/2207.05009v1;cs.GR;"10 pages, 7 figures. Eurographics Proceedings (EGSR 2022,
  Symposium-only track) (https://diglib.eg.org/handle/10.2312/sr20221155)";nerf
2207.04465v1;http://arxiv.org/abs/2207.04465v1;2022-07-10;Progressively-connected Light Field Network for Efficient View Synthesis;"This paper presents a Progressively-connected Light Field network (ProLiF),
for the novel view synthesis of complex forward-facing scenes. ProLiF encodes a
4D light field, which allows rendering a large batch of rays in one training
step for image- or patch-level losses. Directly learning a neural light field
from images has difficulty in rendering multi-view consistent images due to its
unawareness of the underlying 3D geometry. To address this problem, we propose
a progressive training scheme and regularization losses to infer the underlying
geometry during training, both of which enforce the multi-view consistency and
thus greatly improves the rendering quality. Experiments demonstrate that our
method is able to achieve significantly better rendering quality than the
vanilla neural light fields and comparable results to NeRF-like rendering
methods on the challenging LLFF dataset and Shiny Object dataset. Moreover, we
demonstrate better compatibility with LPIPS loss to achieve robustness to
varying light conditions and CLIP loss to control the rendering style of the
scene. Project page: https://totoro97.github.io/projects/prolif.";Peng Wang<author:sep>Yuan Liu<author:sep>Guying Lin<author:sep>Jiatao Gu<author:sep>Lingjie Liu<author:sep>Taku Komura<author:sep>Wenping Wang;http://arxiv.org/pdf/2207.04465v1;cs.CV;Project page: https://totoro97.github.io/projects/prolif;nerf
2207.02621v2;http://arxiv.org/abs/2207.02621v2;2022-07-06;VMRF: View Matching Neural Radiance Fields;"Neural Radiance Fields (NeRF) have demonstrated very impressive performance
in novel view synthesis via implicitly modelling 3D representations from
multi-view 2D images. However, most existing studies train NeRF models with
either reasonable camera pose initialization or manually-crafted camera pose
distributions which are often unavailable or hard to acquire in various
real-world data. We design VMRF, an innovative view matching NeRF that enables
effective NeRF training without requiring prior knowledge in camera poses or
camera pose distributions. VMRF introduces a view matching scheme, which
exploits unbalanced optimal transport to produce a feature transport plan for
mapping a rendered image with randomly initialized camera pose to the
corresponding real image. With the feature transport plan as the guidance, a
novel pose calibration technique is designed which rectifies the initially
randomized camera poses by predicting relative pose transformations between the
pair of rendered and real images. Extensive experiments over a number of
synthetic and real datasets show that the proposed VMRF outperforms the
state-of-the-art qualitatively and quantitatively by large margins.";Jiahui Zhang<author:sep>Fangneng Zhan<author:sep>Rongliang Wu<author:sep>Yingchen Yu<author:sep>Wenqing Zhang<author:sep>Bai Song<author:sep>Xiaoqin Zhang<author:sep>Shijian Lu;http://arxiv.org/pdf/2207.02621v2;cs.CV;This paper has been accepted to ACM MM 2022;nerf
2207.02363v1;http://arxiv.org/abs/2207.02363v1;2022-07-05;SNeRF: Stylized Neural Implicit Representations for 3D Scenes;"This paper presents a stylized novel view synthesis method. Applying
state-of-the-art stylization methods to novel views frame by frame often causes
jittering artifacts due to the lack of cross-view consistency. Therefore, this
paper investigates 3D scene stylization that provides a strong inductive bias
for consistent novel view synthesis. Specifically, we adopt the emerging neural
radiance fields (NeRF) as our choice of 3D scene representation for their
capability to render high-quality novel views for a variety of scenes. However,
as rendering a novel view from a NeRF requires a large number of samples,
training a stylized NeRF requires a large amount of GPU memory that goes beyond
an off-the-shelf GPU capacity. We introduce a new training method to address
this problem by alternating the NeRF and stylization optimization steps. Such a
method enables us to make full use of our hardware memory capacity to both
generate images at higher resolution and adopt more expressive image style
transfer methods. Our experiments show that our method produces stylized NeRFs
for a wide range of content, including indoor, outdoor and dynamic scenes, and
synthesizes high-quality novel views with cross-view consistency.";Thu Nguyen-Phuoc<author:sep>Feng Liu<author:sep>Lei Xiao;http://arxiv.org/pdf/2207.02363v1;cs.CV;"SIGGRAPH 2022 (Journal track). Project page:
  https://research.facebook.com/publications/snerf-stylized-neural-implicit-representations-for-3d-scenes/";nerf
2207.01583v3;http://arxiv.org/abs/2207.01583v3;2022-07-04;LaTeRF: Label and Text Driven Object Radiance Fields;"Obtaining 3D object representations is important for creating photo-realistic
simulations and for collecting AR and VR assets. Neural fields have shown their
effectiveness in learning a continuous volumetric representation of a scene
from 2D images, but acquiring object representations from these models with
weak supervision remains an open challenge. In this paper we introduce LaTeRF,
a method for extracting an object of interest from a scene given 2D images of
the entire scene, known camera poses, a natural language description of the
object, and a set of point-labels of object and non-object points in the input
images. To faithfully extract the object from the scene, LaTeRF extends the
NeRF formulation with an additional `objectness' probability at each 3D point.
Additionally, we leverage the rich latent space of a pre-trained CLIP model
combined with our differentiable object renderer, to inpaint the occluded parts
of the object. We demonstrate high-fidelity object extraction on both synthetic
and real-world datasets and justify our design choices through an extensive
ablation study.";Ashkan Mirzaei<author:sep>Yash Kant<author:sep>Jonathan Kelly<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2207.01583v3;cs.CV;;nerf
2207.01164v1;http://arxiv.org/abs/2207.01164v1;2022-07-04;Aug-NeRF: Training Stronger Neural Radiance Fields with Triple-Level  Physically-Grounded Augmentations;"Neural Radiance Field (NeRF) regresses a neural parameterized scene by
differentially rendering multi-view images with ground-truth supervision.
However, when interpolating novel views, NeRF often yields inconsistent and
visually non-smooth geometric results, which we consider as a generalization
gap between seen and unseen views. Recent advances in convolutional neural
networks have demonstrated the promise of advanced robust data augmentations,
either random or learned, in enhancing both in-distribution and
out-of-distribution generalization. Inspired by that, we propose Augmented NeRF
(Aug-NeRF), which for the first time brings the power of robust data
augmentations into regularizing the NeRF training. Particularly, our proposal
learns to seamlessly blend worst-case perturbations into three distinct levels
of the NeRF pipeline with physical grounds, including (1) the input
coordinates, to simulate imprecise camera parameters at image capture; (2)
intermediate features, to smoothen the intrinsic feature manifold; and (3)
pre-rendering output, to account for the potential degradation factors in the
multi-view image supervision. Extensive results demonstrate that Aug-NeRF
effectively boosts NeRF performance in both novel view synthesis (up to 1.5dB
PSNR gain) and underlying geometry reconstruction. Furthermore, thanks to the
implicit smooth prior injected by the triple-level augmentations, Aug-NeRF can
even recover scenes from heavily corrupted images, a highly challenging setting
untackled before. Our codes are available in
https://github.com/VITA-Group/Aug-NeRF.";Tianlong Chen<author:sep>Peihao Wang<author:sep>Zhiwen Fan<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2207.01164v1;cs.CV;"IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2022";nerf
2206.15255v1;http://arxiv.org/abs/2206.15255v1;2022-06-30;Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in  Robotic Surgery;"Reconstruction of the soft tissues in robotic surgery from endoscopic stereo
videos is important for many applications such as intra-operative navigation
and image-guided robotic surgery automation. Previous works on this task mainly
rely on SLAM-based approaches, which struggle to handle complex surgical
scenes. Inspired by recent progress in neural rendering, we present a novel
framework for deformable tissue reconstruction from binocular captures in
robotic surgery under the single-viewpoint setting. Our framework adopts
dynamic neural radiance fields to represent deformable surgical scenes in MLPs
and optimize shapes and deformations in a learning-based manner. In addition to
non-rigid deformations, tool occlusion and poor 3D clues from a single
viewpoint are also particular challenges in soft tissue reconstruction. To
overcome these difficulties, we present a series of strategies of tool
mask-guided ray casting, stereo depth-cueing ray marching and stereo
depth-supervised optimization. With experiments on DaVinci robotic surgery
videos, our method significantly outperforms the current state-of-the-art
reconstruction method for handling various complex non-rigid deformations. To
our best knowledge, this is the first work leveraging neural rendering for
surgical scene 3D reconstruction with remarkable potential demonstrated. Code
is available at: https://github.com/med-air/EndoNeRF.";Yuehao Wang<author:sep>Yonghao Long<author:sep>Siu Hin Fan<author:sep>Qi Dou;http://arxiv.org/pdf/2206.15255v1;cs.CV;11 pages, 4 figures, conference;nerf
2206.14938v2;http://arxiv.org/abs/2206.14938v2;2022-06-29;Regularization of NeRFs using differential geometry;"Neural radiance fields, or NeRF, represent a breakthrough in the field of
novel view synthesis and 3D modeling of complex scenes from multi-view image
collections. Numerous recent works have shown the importance of making NeRF
models more robust, by means of regularization, in order to train with possibly
inconsistent and/or very sparse data. In this work, we explore how differential
geometry can provide elegant regularization tools for robustly training
NeRF-like models, which are modified so as to represent continuous and
infinitely differentiable functions. In particular, we present a generic
framework for regularizing different types of NeRFs observations to improve the
performance in challenging conditions. We also show how the same formalism can
also be used to natively encourage the regularity of surfaces by means of
Gaussian or mean curvatures.";Thibaud Ehret<author:sep>Roger Marí<author:sep>Gabriele Facciolo;http://arxiv.org/pdf/2206.14938v2;cs.CV;;nerf
2206.12455v2;http://arxiv.org/abs/2206.12455v2;2022-06-24;Ev-NeRF: Event Based Neural Radiance Field;"We present Ev-NeRF, a Neural Radiance Field derived from event data. While
event cameras can measure subtle brightness changes in high frame rates, the
measurements in low lighting or extreme motion suffer from significant domain
discrepancy with complex noise. As a result, the performance of event-based
vision tasks does not transfer to challenging environments, where the event
cameras are expected to thrive over normal cameras. We find that the multi-view
consistency of NeRF provides a powerful self-supervision signal for eliminating
the spurious measurements and extracting the consistent underlying structure
despite highly noisy input. Instead of posed images of the original NeRF, the
input to Ev-NeRF is the event measurements accompanied by the movements of the
sensors. Using the loss function that reflects the measurement model of the
sensor, Ev-NeRF creates an integrated neural volume that summarizes the
unstructured and sparse data points captured for about 2-4 seconds. The
generated neural volume can also produce intensity images from novel views with
reasonable depth estimates, which can serve as a high-quality input to various
vision-based tasks. Our results show that Ev-NeRF achieves competitive
performance for intensity image reconstruction under extreme noise conditions
and high-dynamic-range imaging.";Inwoo Hwang<author:sep>Junho Kim<author:sep>Young Min Kim;http://arxiv.org/pdf/2206.12455v2;cs.CV;Accepted to WACV 2023;nerf
2206.11896v3;http://arxiv.org/abs/2206.11896v3;2022-06-23;EventNeRF: Neural Radiance Fields from a Single Colour Event Camera;"Asynchronously operating event cameras find many applications due to their
high dynamic range, vanishingly low motion blur, low latency and low data
bandwidth. The field saw remarkable progress during the last few years, and
existing event-based 3D reconstruction approaches recover sparse point clouds
of the scene. However, such sparsity is a limiting factor in many cases,
especially in computer vision and graphics, that has not been addressed
satisfactorily so far. Accordingly, this paper proposes the first approach for
3D-consistent, dense and photorealistic novel view synthesis using just a
single colour event stream as input. At its core is a neural radiance field
trained entirely in a self-supervised manner from events while preserving the
original resolution of the colour event channels. Next, our ray sampling
strategy is tailored to events and allows for data-efficient training. At test,
our method produces results in the RGB space at unprecedented quality. We
evaluate our method qualitatively and numerically on several challenging
synthetic and real scenes and show that it produces significantly denser and
more visually appealing renderings than the existing methods. We also
demonstrate robustness in challenging scenarios with fast motion and under low
lighting conditions. We release the newly recorded dataset and our source code
to facilitate the research field, see https://4dqv.mpi-inf.mpg.de/EventNeRF.";Viktor Rudnev<author:sep>Mohamed Elgharib<author:sep>Christian Theobalt<author:sep>Vladislav Golyanik;http://arxiv.org/pdf/2206.11896v3;cs.CV;"19 pages, 21 figures, 3 tables; CVPR 2023";nerf
2206.11952v1;http://arxiv.org/abs/2206.11952v1;2022-06-23;UNeRF: Time and Memory Conscious U-Shaped Network for Training Neural  Radiance Fields;"Neural Radiance Fields (NeRFs) increase reconstruction detail for novel view
synthesis and scene reconstruction, with applications ranging from large static
scenes to dynamic human motion. However, the increased resolution and
model-free nature of such neural fields come at the cost of high training times
and excessive memory requirements. Recent advances improve the inference time
by using complementary data structures yet these methods are ill-suited for
dynamic scenes and often increase memory consumption. Little has been done to
reduce the resources required at training time. We propose a method to exploit
the redundancy of NeRF's sample-based computations by partially sharing
evaluations across neighboring sample points. Our UNeRF architecture is
inspired by the UNet, where spatial resolution is reduced in the middle of the
network and information is shared between adjacent samples. Although this
change violates the strict and conscious separation of view-dependent
appearance and view-independent density estimation in the NeRF method, we show
that it improves novel view synthesis. We also introduce an alternative
subsampling strategy which shares computation while minimizing any violation of
view invariance. UNeRF is a plug-in module for the original NeRF network. Our
major contributions include reduction of the memory footprint, improved
accuracy, and reduced amortized processing time both during training and
inference. With only weak assumptions on locality, we achieve improved resource
utilization on a variety of neural radiance fields tasks. We demonstrate
applications to the novel view synthesis of static scenes as well as dynamic
human shape and motion.";Abiramy Kuganesan<author:sep>Shih-yang Su<author:sep>James J. Little<author:sep>Helge Rhodin;http://arxiv.org/pdf/2206.11952v1;cs.CV;;nerf
2206.10885v2;http://arxiv.org/abs/2206.10885v2;2022-06-22;KiloNeuS: A Versatile Neural Implicit Surface Representation for  Real-Time Rendering;"NeRF-based techniques fit wide and deep multi-layer perceptrons (MLPs) to a
continuous radiance field that can be rendered from any unseen viewpoint.
However, the lack of surface and normals definition and high rendering times
limit their usage in typical computer graphics applications. Such limitations
have recently been overcome separately, but solving them together remains an
open problem. We present KiloNeuS, a neural representation reconstructing an
implicit surface represented as a signed distance function (SDF) from
multi-view images and enabling real-time rendering by partitioning the space
into thousands of tiny MLPs fast to inference. As we learn the implicit surface
locally using independent models, resulting in a globally coherent geometry is
non-trivial and needs to be addressed during training. We evaluate rendering
performance on a GPU-accelerated ray-caster with in-shader neural network
inference, resulting in an average of 46 FPS at high resolution, proving a
satisfying tradeoff between storage costs and rendering quality. In fact, our
evaluation for rendering quality and surface recovery shows that KiloNeuS
outperforms its single-MLP counterpart. Finally, to exhibit the versatility of
KiloNeuS, we integrate it into an interactive path-tracer taking full advantage
of its surface normals. We consider our work a crucial first step toward
real-time rendering of implicit neural representations under global
illumination.";Stefano Esposito<author:sep>Daniele Baieri<author:sep>Stefan Zellmann<author:sep>André Hinkenjann<author:sep>Emanuele Rodolà;http://arxiv.org/pdf/2206.10885v2;cs.CV;9 pages, 8 figures;nerf
2206.08355v3;http://arxiv.org/abs/2206.08355v3;2022-06-16;FWD: Real-time Novel View Synthesis with Forward Warping and Depth;"Novel view synthesis (NVS) is a challenging task requiring systems to
generate photorealistic images of scenes from new viewpoints, where both
quality and speed are important for applications. Previous image-based
rendering (IBR) methods are fast, but have poor quality when input views are
sparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give
impressive results but are not real-time. In our paper, we propose a
generalizable NVS method with sparse inputs, called FWD, which gives
high-quality synthesis in real-time. With explicit depth and differentiable
rendering, it achieves competitive results to the SOTA methods with 130-1000x
speedup and better perceptual quality. If available, we can seamlessly
integrate sensor depth during either training or inference to improve image
quality while retaining real-time speed. With the growing prevalence of depths
sensors, we hope that methods making use of depth will become increasingly
useful.";Ang Cao<author:sep>Chris Rockwell<author:sep>Justin Johnson;http://arxiv.org/pdf/2206.08355v3;cs.CV;CVPR 2022. Project website https://caoang327.github.io/FWD/;nerf
2206.08361v2;http://arxiv.org/abs/2206.08361v2;2022-06-16;Controllable 3D Face Synthesis with Conditional Generative Occupancy  Fields;"Capitalizing on the recent advances in image generation models, existing
controllable face image synthesis methods are able to generate high-fidelity
images with some levels of controllability, e.g., controlling the shapes,
expressions, textures, and poses of the generated face images. However, these
methods focus on 2D image generative models, which are prone to producing
inconsistent face images under large expression and pose changes. In this
paper, we propose a new NeRF-based conditional 3D face synthesis framework,
which enables 3D controllability over the generated face images by imposing
explicit 3D conditions from 3D face priors. At its core is a conditional
Generative Occupancy Field (cGOF) that effectively enforces the shape of the
generated face to commit to a given 3D Morphable Model (3DMM) mesh. To achieve
accurate control over fine-grained 3D face shapes of the synthesized image, we
additionally incorporate a 3D landmark loss as well as a volume warping loss
into our synthesis algorithm. Experiments validate the effectiveness of the
proposed method, which is able to generate high-fidelity face images and shows
more precise 3D controllability than state-of-the-art 2D-based controllable
face synthesis methods. Find code and demo at
https://keqiangsun.github.io/projects/cgof.";Keqiang Sun<author:sep>Shangzhe Wu<author:sep>Zhaoyang Huang<author:sep>Ning Zhang<author:sep>Quan Wang<author:sep>HongSheng Li;http://arxiv.org/pdf/2206.08361v2;cs.CV;;nerf
2206.07698v2;http://arxiv.org/abs/2206.07698v2;2022-06-15;Neural Deformable Voxel Grid for Fast Optimization of Dynamic View  Synthesis;"Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel
view synthesis (NVS) for its superior performance. In this paper, we propose to
synthesize dynamic scenes. Extending the methods for static scenes to dynamic
scenes is not straightforward as both the scene geometry and appearance change
over time, especially under monocular setup. Also, the existing dynamic NeRF
methods generally require a lengthy per-scene training procedure, where
multi-layer perceptrons (MLP) are fitted to model both motions and radiance. In
this paper, built on top of the recent advances in voxel-grid optimization, we
propose a fast deformable radiance field method to handle dynamic scenes. Our
method consists of two modules. The first module adopts a deformation grid to
store 3D dynamic features, and a light-weight MLP for decoding the deformation
that maps a 3D point in the observation space to the canonical space using the
interpolated features. The second module contains a density and a color grid to
model the geometry and density of the scene. The occlusion is explicitly
modeled to further improve the rendering quality. Experimental results show
that our method achieves comparable performance to D-NeRF using only 20 minutes
for training, which is more than 70x faster than D-NeRF, clearly demonstrating
the efficiency of our proposed method.";Xiang Guo<author:sep>Guanying Chen<author:sep>Yuchao Dai<author:sep>Xiaoqing Ye<author:sep>Jiadai Sun<author:sep>Xiao Tan<author:sep>Errui Ding;http://arxiv.org/pdf/2206.07698v2;cs.CV;"Technical Report: 29 pages; project page:
  https://npucvr.github.io/NDVG";nerf
2206.06577v1;http://arxiv.org/abs/2206.06577v1;2022-06-14;Physics Informed Neural Fields for Smoke Reconstruction with Sparse Data;"High-fidelity reconstruction of fluids from sparse multiview RGB videos
remains a formidable challenge due to the complexity of the underlying physics
as well as complex occlusion and lighting in captures. Existing solutions
either assume knowledge of obstacles and lighting, or only focus on simple
fluid scenes without obstacles or complex lighting, and thus are unsuitable for
real-world scenes with unknown lighting or arbitrary obstacles. We present the
first method to reconstruct dynamic fluid by leveraging the governing physics
(ie, Navier -Stokes equations) in an end-to-end optimization from sparse videos
without taking lighting conditions, geometry information, or boundary
conditions as input. We provide a continuous spatio-temporal scene
representation using neural networks as the ansatz of density and velocity
solution functions for fluids as well as the radiance field for static objects.
With a hybrid architecture that separates static and dynamic contents, fluid
interactions with static obstacles are reconstructed for the first time without
additional geometry input or human labeling. By augmenting time-varying neural
radiance fields with physics-informed deep learning, our method benefits from
the supervision of images and physical priors. To achieve robust optimization
from sparse views, we introduced a layer-by-layer growing strategy to
progressively increase the network capacity. Using progressively growing models
with a new regularization term, we manage to disentangle density-color
ambiguity in radiance fields without overfitting. A pretrained
density-to-velocity fluid model is leveraged in addition as the data prior to
avoid suboptimal velocity which underestimates vorticity but trivially fulfills
physical equations. Our method exhibits high-quality results with relaxed
constraints and strong flexibility on a representative set of synthetic and
real flow captures.";Mengyu Chu<author:sep>Lingjie Liu<author:sep>Quan Zheng<author:sep>Erik Franz<author:sep>Hans-Peter Seidel<author:sep>Christian Theobalt<author:sep>Rhaleb Zayer;http://arxiv.org/pdf/2206.06577v1;cs.GR;"accepted to ACM Transactions On Graphics (SIGGRAPH 2022), further
  info:\url{https://people.mpi-inf.mpg.de/~mchu/projects/PI-NeRF/}";
2206.06340v1;http://arxiv.org/abs/2206.06340v1;2022-06-13;SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data;"We present a method for the accurate 3D reconstruction of partly-symmetric
objects. We build on the strengths of recent advances in neural reconstruction
and rendering such as Neural Radiance Fields (NeRF). A major shortcoming of
such approaches is that they fail to reconstruct any part of the object which
is not clearly visible in the training image, which is often the case for
in-the-wild images and videos. When evidence is lacking, structural priors such
as symmetry can be used to complete the missing information. However,
exploiting such priors in neural rendering is highly non-trivial: while
geometry and non-reflective materials may be symmetric, shadows and reflections
from the ambient scene are not symmetric in general. To address this, we apply
a soft symmetry constraint to the 3D geometry and material properties, having
factored appearance into lighting, albedo colour and reflectivity. We evaluate
our method on the recently introduced CO3D dataset, focusing on the car
category due to the challenge of reconstructing highly-reflective materials. We
show that it can reconstruct unobserved regions with high fidelity and render
high-quality novel view images.";Eldar Insafutdinov<author:sep>Dylan Campbell<author:sep>João F. Henriques<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2206.06340v1;cs.CV;First two authors contributed equally;nerf
2206.06100v1;http://arxiv.org/abs/2206.06100v1;2022-06-13;AR-NeRF: Unsupervised Learning of Depth and Defocus Effects from Natural  Images with Aperture Rendering Neural Radiance Fields;"Fully unsupervised 3D representation learning has gained attention owing to
its advantages in data collection. A successful approach involves a
viewpoint-aware approach that learns an image distribution based on generative
models (e.g., generative adversarial networks (GANs)) while generating various
view images based on 3D-aware models (e.g., neural radiance fields (NeRFs)).
However, they require images with various views for training, and consequently,
their application to datasets with few or limited viewpoints remains a
challenge. As a complementary approach, an aperture rendering GAN (AR-GAN) that
employs a defocus cue was proposed. However, an AR-GAN is a CNN-based model and
represents a defocus independently from a viewpoint change despite its high
correlation, which is one of the reasons for its performance. As an alternative
to an AR-GAN, we propose an aperture rendering NeRF (AR-NeRF), which can
utilize viewpoint and defocus cues in a unified manner by representing both
factors in a common ray-tracing framework. Moreover, to learn defocus-aware and
defocus-independent representations in a disentangled manner, we propose
aperture randomized training, for which we learn to generate images while
randomizing the aperture size and latent codes independently. During our
experiments, we applied AR-NeRF to various natural image datasets, including
flower, bird, and face images, the results of which demonstrate the utility of
AR-NeRF for unsupervised learning of the depth and defocus effects.";Takuhiro Kaneko;http://arxiv.org/pdf/2206.06100v1;cs.CV;"Accepted to CVPR 2022. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-nerf/";nerf
2206.06481v1;http://arxiv.org/abs/2206.06481v1;2022-06-13;RigNeRF: Fully Controllable Neural 3D Portraits;"Volumetric neural rendering methods, such as neural radiance fields (NeRFs),
have enabled photo-realistic novel view synthesis. However, in their standard
form, NeRFs do not support the editing of objects, such as a human head, within
a scene. In this work, we propose RigNeRF, a system that goes beyond just novel
view synthesis and enables full control of head pose and facial expressions
learned from a single portrait video. We model changes in head pose and facial
expressions using a deformation field that is guided by a 3D morphable face
model (3DMM). The 3DMM effectively acts as a prior for RigNeRF that learns to
predict only residuals to the 3DMM deformations and allows us to render novel
(rigid) poses and (non-rigid) expressions that were not present in the input
sequence. Using only a smartphone-captured short video of a subject for
training, we demonstrate the effectiveness of our method on free view synthesis
of a portrait scene with explicit head pose and expression controls. The
project page can be found here:
http://shahrukhathar.github.io/2022/06/06/RigNeRF.html";ShahRukh Athar<author:sep>Zexiang Xu<author:sep>Kalyan Sunkavalli<author:sep>Eli Shechtman<author:sep>Zhixin Shu;http://arxiv.org/pdf/2206.06481v1;cs.CV;"The project page can be found here:
  http://shahrukhathar.github.io/2022/06/06/RigNeRF.html";nerf
2206.04901v1;http://arxiv.org/abs/2206.04901v1;2022-06-10;NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors;"Though Neural Radiance Field (NeRF) demonstrates compelling novel view
synthesis results, it is still unintuitive to edit a pre-trained NeRF because
the neural network's parameters and the scene geometry/appearance are often not
explicitly associated. In this paper, we introduce the first framework that
enables users to remove unwanted objects or retouch undesired regions in a 3D
scene represented by a pre-trained NeRF without any category-specific data and
training. The user first draws a free-form mask to specify a region containing
unwanted objects over a rendered view from the pre-trained NeRF. Our framework
first transfers the user-provided mask to other rendered views and estimates
guiding color and depth images within these transferred masked regions. Next,
we formulate an optimization problem that jointly inpaints the image content in
all masked regions across multiple views by updating the NeRF model's
parameters. We demonstrate our framework on diverse scenes and show it obtained
visual plausible and structurally consistent results across multiple views
using shorter time and less user manual efforts.";Hao-Kang Liu<author:sep>I-Chao Shen<author:sep>Bing-Yu Chen;http://arxiv.org/pdf/2206.04901v1;cs.CV;"Hao-Kang Liu and I-Chao Shen contributed equally to the paper.
  Project page: https://jdily.github.io/proj_site/nerfin_proj.html";nerf
2206.05375v1;http://arxiv.org/abs/2206.05375v1;2022-06-10;Generalizable Neural Radiance Fields for Novel View Synthesis with  Transformer;"We propose a Transformer-based NeRF (TransNeRF) to learn a generic neural
radiance field conditioned on observed-view images for the novel view synthesis
task. By contrast, existing MLP-based NeRFs are not able to directly receive
observed views with an arbitrary number and require an auxiliary pooling-based
operation to fuse source-view information, resulting in the missing of
complicated relationships between source views and the target rendering view.
Furthermore, current approaches process each 3D point individually and ignore
the local consistency of a radiance field scene representation. These
limitations potentially can reduce their performance in challenging real-world
applications where large differences between source views and a novel rendering
view may exist. To address these challenges, our TransNeRF utilizes the
attention mechanism to naturally decode deep associations of an arbitrary
number of source views into a coordinate-based scene representation. Local
consistency of shape and appearance are considered in the ray-cast space and
the surrounding-view space within a unified Transformer network. Experiments
demonstrate that our TransNeRF, trained on a wide variety of scenes, can
achieve better performance in comparison to state-of-the-art image-based neural
rendering methods in both scene-agnostic and per-scene finetuning scenarios
especially when there is a considerable gap between source views and a
rendering view.";Dan Wang<author:sep>Xinrui Cui<author:sep>Septimiu Salcudean<author:sep>Z. Jane Wang;http://arxiv.org/pdf/2206.05375v1;cs.CV;;nerf
2206.05085v4;http://arxiv.org/abs/2206.05085v4;2022-06-10;Improved Direct Voxel Grid Optimization for Radiance Fields  Reconstruction;"In this technical report, we improve the DVGO framework (called DVGOv2),
which is based on Pytorch and uses the simplest dense grid representation.
First, we re-implement part of the Pytorch operations with cuda, achieving 2-3x
speedup. The cuda extension is automatically compiled just in time. Second, we
extend DVGO to support Forward-facing and Unbounded Inward-facing capturing.
Third, we improve the space time complexity of the distortion loss proposed by
mip-NeRF 360 from O(N^2) to O(N). The distortion loss improves our quality and
training speed. Our efficient implementation could allow more future works to
benefit from the loss.";Cheng Sun<author:sep>Min Sun<author:sep>Hwann-Tzong Chen;http://arxiv.org/pdf/2206.05085v4;cs.GR;"Project page https://sunset1995.github.io/dvgo/ ; Code
  https://github.com/sunset1995/DirectVoxGO ; Results updated";nerf
2206.04669v1;http://arxiv.org/abs/2206.04669v1;2022-06-09;Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields;"Comprehensive 3D scene understanding, both geometrically and semantically, is
important for real-world applications such as robot perception. Most of the
existing work has focused on developing data-driven discriminative models for
scene understanding. This paper provides a new approach to scene understanding,
from a synthesis model perspective, by leveraging the recent progress on
implicit 3D representation and neural rendering. Building upon the great
success of Neural Radiance Fields (NeRFs), we introduce Scene-Property
Synthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic
RGB images from novel viewpoints, but also render various accurate scene
properties (e.g., appearance, geometry, and semantics). By doing so, we
facilitate addressing a variety of scene understanding tasks under a unified
framework, including semantic segmentation, surface normal estimation,
reshading, keypoint detection, and edge detection. Our SS-NeRF framework can be
a powerful tool for bridging generative learning and discriminative learning,
and thus be beneficial to the investigation of a wide range of interesting
problems, such as studying task relationships within a synthesis paradigm,
transferring knowledge to novel tasks, facilitating downstream discriminative
tasks as ways of data augmentation, and serving as auto-labeller for data
creation.";Mingtong Zhang<author:sep>Shuhong Zheng<author:sep>Zhipeng Bao<author:sep>Martial Hebert<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2206.04669v1;cs.CV;;nerf
2206.03591v3;http://arxiv.org/abs/2206.03591v3;2022-06-07;ObPose: Leveraging Pose for Object-Centric Scene Inference and  Generation in 3D;"We present ObPose, an unsupervised object-centric inference and generation
model which learns 3D-structured latent representations from RGB-D scenes.
Inspired by prior art in 2D representation learning, ObPose considers a
factorised latent space, separately encoding object location (where) and
appearance (what). ObPose further leverages an object's pose (i.e. location and
orientation), defined via a minimum volume principle, as a novel inductive bias
for learning the where component. To achieve this, we propose an efficient,
voxelised approximation approach to recover the object shape directly from a
neural radiance field (NeRF). As a consequence, ObPose models each scene as a
composition of NeRFs, richly representing individual objects. To evaluate the
quality of the learned representations, ObPose is evaluated quantitatively on
the YCB, MultiShapeNet, and CLEVR datatasets for unsupervised scene
segmentation, outperforming the current state-of-the-art in 3D scene inference
(ObSuRF) by a significant margin. Generative results provide qualitative
demonstration that the same ObPose model can both generate novel scenes and
flexibly edit the objects in them. These capacities again reflect the quality
of the learned latents and the benefits of disentangling the where and what
components of a scene. Key design choices made in the ObPose encoder are
validated with ablations.";Yizhe Wu<author:sep>Oiwi Parker Jones<author:sep>Ingmar Posner;http://arxiv.org/pdf/2206.03591v3;cs.CV;14 pages, 4 figures;nerf
2206.01634v1;http://arxiv.org/abs/2206.01634v1;2022-06-03;Reinforcement Learning with Neural Radiance Fields;"It is a long-standing problem to find effective representations for training
reinforcement learning (RL) agents. This paper demonstrates that learning state
representations with supervision from Neural Radiance Fields (NeRFs) can
improve the performance of RL compared to other learned representations or even
low-dimensional, hand-engineered state information. Specifically, we propose to
train an encoder that maps multiple image observations to a latent space
describing the objects in the scene. The decoder built from a
latent-conditioned NeRF serves as the supervision signal to learn the latent
space. An RL algorithm then operates on the learned latent space as its state
representation. We call this NeRF-RL. Our experiments indicate that NeRF as
supervision leads to a latent space better suited for the downstream RL tasks
involving robotic object manipulations like hanging mugs on hooks, pushing
objects, or opening doors. Video: https://dannydriess.github.io/nerf-rl";Danny Driess<author:sep>Ingmar Schubert<author:sep>Pete Florence<author:sep>Yunzhu Li<author:sep>Marc Toussaint;http://arxiv.org/pdf/2206.01634v1;cs.LG;;nerf
2206.00878v1;http://arxiv.org/abs/2206.00878v1;2022-06-02;EfficientNeRF: Efficient Neural Radiance Fields;"Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications.";Tao Hu<author:sep>Shu Liu<author:sep>Yilun Chen<author:sep>Tiancheng Shen<author:sep>Jiaya Jia;http://arxiv.org/pdf/2206.00878v1;cs.CV;;nerf
2206.01290v2;http://arxiv.org/abs/2206.01290v2;2022-06-02;Points2NeRF: Generating Neural Radiance Fields from 3D point cloud;"Contemporary registration devices for 3D visual information, such as LIDARs
and various depth cameras, capture data as 3D point clouds. In turn, such
clouds are challenging to be processed due to their size and complexity.
Existing methods address this problem by fitting a mesh to the point cloud and
rendering it instead. This approach, however, leads to the reduced fidelity of
the resulting visualization and misses color information of the objects crucial
in computer graphics applications. In this work, we propose to mitigate this
challenge by representing 3D objects as Neural Radiance Fields (NeRFs). We
leverage a hypernetwork paradigm and train the model to take a 3D point cloud
with the associated color values and return a NeRF network's weights that
reconstruct 3D objects from input 2D images. Our method provides efficient 3D
object representation and offers several advantages over the existing
approaches, including the ability to condition NeRFs and improved
generalization beyond objects seen in training. The latter we also confirmed in
the results of our empirical evaluation.";D. Zimny<author:sep>T. Trzciński<author:sep>P. Spurek;http://arxiv.org/pdf/2206.01290v2;cs.CV;arXiv admin note: text overlap with arXiv:2003.08934 by other authors;nerf
2205.15838v4;http://arxiv.org/abs/2205.15838v4;2022-05-31;D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from  a Monocular Video;"Given a monocular video, segmenting and decoupling dynamic objects while
recovering the static environment is a widely studied problem in machine
intelligence. Existing solutions usually approach this problem in the image
domain, limiting their performance and understanding of the environment. We
introduce Decoupled Dynamic Neural Radiance Field (D$^2$NeRF), a
self-supervised approach that takes a monocular video and learns a 3D scene
representation which decouples moving objects, including their shadows, from
the static background. Our method represents the moving objects and the static
background by two separate neural radiance fields with only one allowing for
temporal changes. A naive implementation of this approach leads to the dynamic
component taking over the static one as the representation of the former is
inherently more general and prone to overfitting. To this end, we propose a
novel loss to promote correct separation of phenomena. We further propose a
shadow field network to detect and decouple dynamically moving shadows. We
introduce a new dataset containing various dynamic objects and shadows and
demonstrate that our method can achieve better performance than
state-of-the-art approaches in decoupling dynamic and static 3D objects,
occlusion and shadow removal, and image segmentation for moving objects.";Tianhao Wu<author:sep>Fangcheng Zhong<author:sep>Andrea Tagliasacchi<author:sep>Forrester Cole<author:sep>Cengiz Oztireli;http://arxiv.org/pdf/2205.15838v4;cs.CV;;nerf
2205.15585v2;http://arxiv.org/abs/2205.15585v2;2022-05-31;Decomposing NeRF for Editing via Feature Field Distillation;"Emerging neural radiance fields (NeRF) are a promising scene representation
for computer graphics, enabling high-quality 3D reconstruction and novel view
synthesis from image observations. However, editing a scene represented by a
NeRF is challenging, as the underlying connectionist representations such as
MLPs or voxel grids are not object-centric or compositional. In particular, it
has been difficult to selectively edit specific regions or objects. In this
work, we tackle the problem of semantic scene decomposition of NeRFs to enable
query-based local editing of the represented 3D scenes. We propose to distill
the knowledge of off-the-shelf, self-supervised 2D image feature extractors
such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the
radiance field. Given a user-specified query of various modalities such as
text, an image patch, or a point-and-click selection, 3D feature fields
semantically decompose 3D space without the need for re-training and enable us
to semantically select and edit regions in the radiance field. Our experiments
validate that the distilled feature fields (DFFs) can transfer recent progress
in 2D vision and language foundation models to 3D scene representations,
enabling convincing 3D segmentation and selective editing of emerging neural
graphics representations.";Sosuke Kobayashi<author:sep>Eiichi Matsumoto<author:sep>Vincent Sitzmann;http://arxiv.org/pdf/2205.15585v2;cs.CV;"Accepted to NeurIPS 2022
  https://pfnet-research.github.io/distilled-feature-fields/";nerf
2205.15768v1;http://arxiv.org/abs/2205.15768v1;2022-05-31;SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary  Image collections;"Inverse rendering of an object under entirely unknown capture conditions is a
fundamental challenge in computer vision and graphics. Neural approaches such
as NeRF have achieved photorealistic results on novel view synthesis, but they
require known camera poses. Solving this problem with unknown camera poses is
highly challenging as it requires joint optimization over shape, radiance, and
pose. This problem is exacerbated when the input images are captured in the
wild with varying backgrounds and illuminations. Standard pose estimation
techniques fail in such image collections in the wild due to very few estimated
correspondences across images. Furthermore, NeRF cannot relight a scene under
any illumination, as it operates on radiance (the product of reflectance and
illumination). We propose a joint optimization framework to estimate the shape,
BRDF, and per-image camera pose and illumination. Our method works on
in-the-wild online image collections of an object and produces relightable 3D
assets for several use-cases such as AR/VR. To our knowledge, our method is the
first to tackle this severely unconstrained task with minimal user interaction.
Project page: https://markboss.me/publication/2022-samurai/ Video:
https://youtu.be/LlYuGDjXp-8";Mark Boss<author:sep>Andreas Engelhardt<author:sep>Abhishek Kar<author:sep>Yuanzhen Li<author:sep>Deqing Sun<author:sep>Jonathan T. Barron<author:sep>Hendrik P. A. Lensch<author:sep>Varun Jampani;http://arxiv.org/pdf/2205.15768v1;cs.CV;;nerf
2205.15723v2;http://arxiv.org/abs/2205.15723v2;2022-05-31;DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes;"Modeling dynamic scenes is important for many applications such as virtual
reality and telepresence. Despite achieving unprecedented fidelity for novel
view synthesis in dynamic scenes, existing methods based on Neural Radiance
Fields (NeRF) suffer from slow convergence (i.e., model training time measured
in days). In this paper, we present DeVRF, a novel representation to accelerate
learning dynamic radiance fields. The core of DeVRF is to model both the 3D
canonical space and 4D deformation field of a dynamic, non-rigid scene with
explicit and discrete voxel-based representations. However, it is quite
challenging to train such a representation which has a large number of model
parameters, often resulting in overfitting issues. To overcome this challenge,
we devise a novel static-to-dynamic learning paradigm together with a new data
capture setup that is convenient to deploy in practice. This paradigm unlocks
efficient learning of deformable radiance fields via utilizing the 3D
volumetric canonical space learnt from multi-view static images to ease the
learning of 4D voxel deformation field with only few-view dynamic sequences. To
further improve the efficiency of our DeVRF and its synthesized novel view's
quality, we conduct thorough explorations and identify a set of strategies. We
evaluate DeVRF on both synthetic and real-world dynamic scenes with different
types of deformation. Experiments demonstrate that DeVRF achieves two orders of
magnitude speedup (100x faster) with on-par high-fidelity results compared to
the previous state-of-the-art approaches. The code and dataset will be released
in https://github.com/showlab/DeVRF.";Jia-Wei Liu<author:sep>Yan-Pei Cao<author:sep>Weijia Mao<author:sep>Wenqiao Zhang<author:sep>David Junhao Zhang<author:sep>Jussi Keppo<author:sep>Ying Shan<author:sep>Xiaohu Qie<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2205.15723v2;cs.CV;Project page: https://jia-wei-liu.github.io/DeVRF/;nerf
2205.15595v1;http://arxiv.org/abs/2205.15595v1;2022-05-31;Novel View Synthesis for High-fidelity Headshot Scenes;"Rendering scenes with a high-quality human face from arbitrary viewpoints is
a practical and useful technique for many real-world applications. Recently,
Neural Radiance Fields (NeRF), a rendering technique that uses neural networks
to approximate classical ray tracing, have been considered as one of the
promising approaches for synthesizing novel views from a sparse set of images.
We find that NeRF can render new views while maintaining geometric consistency,
but it does not properly maintain skin details, such as moles and pores. These
details are important particularly for faces because when we look at an image
of a face, we are much more sensitive to details than when we look at other
objects. On the other hand, 3D Morpable Models (3DMMs) based on traditional
meshes and textures can perform well in terms of skin detail despite that it
has less precise geometry and cannot cover the head and the entire scene with
background. Based on these observations, we propose a method to use both NeRF
and 3DMM to synthesize a high-fidelity novel view of a scene with a face. Our
method learns a Generative Adversarial Network (GAN) to mix a NeRF-synthesized
image and a 3DMM-rendered image and produces a photorealistic scene with a face
preserving the skin details. Experiments with various real-world scenes
demonstrate the effectiveness of our approach. The code will be available on
https://github.com/showlab/headshot .";Satoshi Tsutsui<author:sep>Weijia Mao<author:sep>Sijing Lin<author:sep>Yunyi Zhu<author:sep>Murong Ma<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2205.15595v1;cs.CV;;nerf
2205.14929v1;http://arxiv.org/abs/2205.14929v1;2022-05-30;Neural Volumetric Object Selection;"We introduce an approach for selecting objects in neural volumetric 3D
representations, such as multi-plane images (MPI) and neural radiance fields
(NeRF). Our approach takes a set of foreground and background 2D user scribbles
in one view and automatically estimates a 3D segmentation of the desired
object, which can be rendered into novel views. To achieve this result, we
propose a novel voxel feature embedding that incorporates the neural volumetric
3D representation and multi-view image features from all input views. To
evaluate our approach, we introduce a new dataset of human-provided
segmentation masks for depicted objects in real-world multi-view scene
captures. We show that our approach out-performs strong baselines, including 2D
segmentation and 3D segmentation approaches adapted to our task.";Zhongzheng Ren<author:sep>Aseem Agarwala<author:sep>Bryan Russell<author:sep>Alexander G. Schwing<author:sep>Oliver Wang;http://arxiv.org/pdf/2205.14929v1;cs.CV;CVPR 2022 camera ready;nerf
2205.15285v2;http://arxiv.org/abs/2205.15285v2;2022-05-30;Fast Dynamic Radiance Fields with Time-Aware Neural Voxels;"Neural radiance fields (NeRF) have shown great success in modeling 3D scenes
and synthesizing novel-view images. However, most previous NeRF methods take
much time to optimize one single scene. Explicit data structures, e.g. voxel
features, show great potential to accelerate the training process. However,
voxel features face two big challenges to be applied to dynamic scenes, i.e.
modeling temporal information and capturing different scales of point motions.
We propose a radiance field framework by representing scenes with time-aware
voxel features, named as TiNeuVox. A tiny coordinate deformation network is
introduced to model coarse motion trajectories and temporal information is
further enhanced in the radiance network. A multi-distance interpolation method
is proposed and applied on voxel features to model both small and large
motions. Our framework significantly accelerates the optimization of dynamic
radiance fields while maintaining high rendering quality. Empirical evaluation
is performed on both synthetic and real scenes. Our TiNeuVox completes training
with only 8 minutes and 8-MB storage cost while showing similar or even better
rendering performance than previous dynamic NeRF methods.";Jiemin Fang<author:sep>Taoran Yi<author:sep>Xinggang Wang<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wenyu Liu<author:sep>Matthias Nießner<author:sep>Qi Tian;http://arxiv.org/pdf/2205.15285v2;cs.CV;SIGGRAPH Asia 2022. Project page: https://jaminfong.cn/tineuvox;nerf
2205.14870v2;http://arxiv.org/abs/2205.14870v2;2022-05-30;Compressible-composable NeRF via Rank-residual Decomposition;"Neural Radiance Field (NeRF) has emerged as a compelling method to represent
3D objects and scenes for photo-realistic rendering. However, its implicit
representation causes difficulty in manipulating the models like the explicit
mesh representation. Several recent advances in NeRF manipulation are usually
restricted by a shared renderer network, or suffer from large model size. To
circumvent the hurdle, in this paper, we present an explicit neural field
representation that enables efficient and convenient manipulation of models. To
achieve this goal, we learn a hybrid tensor rank decomposition of the scene
without neural networks. Motivated by the low-rank approximation property of
the SVD algorithm, we propose a rank-residual learning strategy to encourage
the preservation of primary information in lower ranks. The model size can then
be dynamically adjusted by rank truncation to control the levels of detail,
achieving near-optimal compression without extra optimization. Furthermore,
different models can be arbitrarily transformed and composed into one scene by
concatenating along the rank dimension. The growth of storage cost can also be
mitigated by compressing the unimportant objects in the composed scene. We
demonstrate that our method is able to achieve comparable rendering quality to
state-of-the-art methods, while enabling extra capability of compression and
composition. Code will be made available at https://github.com/ashawkey/CCNeRF.";Jiaxiang Tang<author:sep>Xiaokang Chen<author:sep>Jingbo Wang<author:sep>Gang Zeng;http://arxiv.org/pdf/2205.14870v2;cs.CV;NeurIPS 2022 camera-ready version;nerf
2205.14332v2;http://arxiv.org/abs/2205.14332v2;2022-05-28;V4D: Voxel for 4D Novel View Synthesis;"Neural radiance fields have made a remarkable breakthrough in the novel view
synthesis task at the 3D static scene. However, for the 4D circumstance (e.g.,
dynamic scene), the performance of the existing method is still limited by the
capacity of the neural network, typically in a multilayer perceptron network
(MLP). In this paper, we utilize 3D Voxel to model the 4D neural radiance
field, short as V4D, where the 3D voxel has two formats. The first one is to
regularly model the 3D space and then use the sampled local 3D feature with the
time index to model the density field and the texture field by a tiny MLP. The
second one is in look-up tables (LUTs) format that is for the pixel-level
refinement, where the pseudo-surface produced by the volume rendering is
utilized as the guidance information to learn a 2D pixel-level refinement
mapping. The proposed LUTs-based refinement module achieves the performance
gain with little computational cost and could serve as the plug-and-play module
in the novel view synthesis task. Moreover, we propose a more effective
conditional positional encoding toward the 4D data that achieves performance
gain with negligible computational burdens. Extensive experiments demonstrate
that the proposed method achieves state-of-the-art performance at a low
computational cost.";Wanshui Gan<author:sep>Hongbin Xu<author:sep>Yi Huang<author:sep>Shifeng Chen<author:sep>Naoto Yokoya;http://arxiv.org/pdf/2205.14332v2;cs.CV;;
2205.14330v4;http://arxiv.org/abs/2205.14330v4;2022-05-28;Differentiable Point-Based Radiance Fields for Efficient View Synthesis;"We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.";Qiang Zhang<author:sep>Seung-Hwan Baek<author:sep>Szymon Rusinkiewicz<author:sep>Felix Heide;http://arxiv.org/pdf/2205.14330v4;cs.CV;;nerf
2205.13524v3;http://arxiv.org/abs/2205.13524v3;2022-05-26;PREF: Phasorial Embedding Fields for Compact Neural Representations;"We present an efficient frequency-based neural representation termed PREF: a
shallow MLP augmented with a phasor volume that covers significant border
spectra than previous Fourier feature mapping or Positional Encoding. At the
core is our compact 3D phasor volume where frequencies distribute uniformly
along a 2D plane and dilate along a 1D axis. To this end, we develop a tailored
and efficient Fourier transform that combines both Fast Fourier transform and
local interpolation to accelerate na\""ive Fourier mapping. We also introduce a
Parsvel regularizer that stables frequency-based learning. In these ways, Our
PREF reduces the costly MLP in the frequency-based representation, thereby
significantly closing the efficiency gap between it and other hybrid
representations, and improving its interpretability. Comprehensive experiments
demonstrate that our PREF is able to capture high-frequency details while
remaining compact and robust, including 2D image generalization, 3D signed
distance function regression and 5D neural radiance field reconstruction.";Binbin Huang<author:sep>Xinhao Yan<author:sep>Anpei Chen<author:sep>Shenghua Gao<author:sep>Jingyi Yu;http://arxiv.org/pdf/2205.13524v3;cs.CV;;
2205.12183v2;http://arxiv.org/abs/2205.12183v2;2022-05-24;StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D  Mutual Learning;"3D scene stylization aims at generating stylized images of the scene from
arbitrary novel views following a given set of style examples, while ensuring
consistency when rendered from different views. Directly applying methods for
image or video stylization to 3D scenes cannot achieve such consistency. Thanks
to recently proposed neural radiance fields (NeRF), we are able to represent a
3D scene in a consistent way. Consistent 3D scene stylization can be
effectively achieved by stylizing the corresponding NeRF. However, there is a
significant domain gap between style examples which are 2D images and NeRF
which is an implicit volumetric representation. To address this problem, we
propose a novel mutual learning framework for 3D scene stylization that
combines a 2D image stylization network and NeRF to fuse the stylization
ability of 2D stylization network with the 3D consistency of NeRF. We first
pre-train a standard NeRF of the 3D scene to be stylized and replace its color
prediction module with a style network to obtain a stylized NeRF. It is
followed by distilling the prior knowledge of spatial consistency from NeRF to
the 2D stylization network through an introduced consistency loss. We also
introduce a mimic loss to supervise the mutual learning of the NeRF style
module and fine-tune the 2D stylization decoder. In order to further make our
model handle ambiguities of 2D stylization results, we introduce learnable
latent codes that obey the probability distributions conditioned on the style.
They are attached to training samples as conditional inputs to better learn the
style module in our novel stylized NeRF. Experimental results demonstrate that
our method is superior to existing approaches in both visual quality and
long-range consistency.";Yi-Hua Huang<author:sep>Yue He<author:sep>Yu-Jie Yuan<author:sep>Yu-Kun Lai<author:sep>Lin Gao;http://arxiv.org/pdf/2205.12183v2;cs.GR;Accepted by CVPR 2022;nerf
2205.09351v3;http://arxiv.org/abs/2205.09351v3;2022-05-19;Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields;"Neural scene representations, such as Neural Radiance Fields (NeRF), are
based on training a multilayer perceptron (MLP) using a set of color images
with known poses. An increasing number of devices now produce RGB-D(color +
depth) information, which has been shown to be very important for a wide range
of tasks. Therefore, the aim of this paper is to investigate what improvements
can be made to these promising implicit representations by incorporating depth
information with the color images. In particular, the recently proposed
Mip-NeRF approach, which uses conical frustums instead of rays for volume
rendering, allows one to account for the varying area of a pixel with distance
from the camera center. The proposed method additionally models depth
uncertainty. This allows to address major limitations of NeRF-based approaches
including improving the accuracy of geometry, reduced artifacts, faster
training time, and shortened prediction time. Experiments are performed on
well-known benchmark scenes, and comparisons show improved accuracy in scene
geometry and photometric reconstruction, while reducing the training time by 3
- 5 times.";Arnab Dey<author:sep>Yassine Ahmine<author:sep>Andrew I. Comport;http://arxiv.org/pdf/2205.09351v3;cs.CV;;nerf
2205.08978v2;http://arxiv.org/abs/2205.08978v2;2022-05-18;Fast Neural Network based Solving of Partial Differential Equations;"We present a novel method for using Neural Networks (NNs) for finding
solutions to a class of Partial Differential Equations (PDEs). Our method
builds on recent advances in Neural Radiance Field research (NeRFs) and allows
for a NN to converge to a PDE solution much faster than classic Physically
Informed Neural Network (PINNs) approaches.";Jaroslaw Rzepecki<author:sep>Daniel Bates<author:sep>Chris Doran;http://arxiv.org/pdf/2205.08978v2;cs.LG;;nerf
2205.07058v2;http://arxiv.org/abs/2205.07058v2;2022-05-14;RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis;"We present a large-scale synthetic dataset for novel view synthesis
consisting of ~300k images rendered from nearly 2000 complex scenes using
high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset
is orders of magnitude larger than existing synthetic datasets for novel view
synthesis, thus providing a large unified benchmark for both training and
evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of
our dataset exhibit challenging variations in camera views, lighting, shape,
materials, and textures. Because our dataset is too large for existing methods
to process, we propose Sparse Voxel Light Field (SVLF), an efficient
voxel-based light field approach for novel view synthesis that achieves
comparable performance to NeRF on synthetic data, while being an order of
magnitude faster to train and two orders of magnitude faster to render. SVLF
achieves this speed by relying on a sparse voxel octree, careful voxel sampling
(requiring only a handful of queries per ray), and reduced network structure;
as well as ground truth depth maps at training time. Our dataset is generated
by NViSII, a Python-based ray tracing renderer, which is designed to be simple
for non-experts to use and share, flexible and powerful through its use of
scripting, and able to create high-quality and physically-based rendered
images. Experiments with a subset of our dataset allow us to compare standard
methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for
category-level modeling, pointing toward the need for future improvements in
this area.";Jonathan Tremblay<author:sep>Moustafa Meshry<author:sep>Alex Evans<author:sep>Jan Kautz<author:sep>Alexander Keller<author:sep>Sameh Khamis<author:sep>Thomas Müller<author:sep>Charles Loop<author:sep>Nathan Morrical<author:sep>Koki Nagano<author:sep>Towaki Takikawa<author:sep>Stan Birchfield;http://arxiv.org/pdf/2205.07058v2;cs.CV;"ECCV 2022 Workshop on Learning to Generate 3D Shapes and Scenes.
  Project page at http://www.cs.umd.edu/~mmeshry/projects/rtmv";nerf
2205.05869v2;http://arxiv.org/abs/2205.05869v2;2022-05-12;View Synthesis with Sculpted Neural Points;"We address the task of view synthesis, generating novel views of a scene
given a set of images as input. In many recent works such as NeRF (Mildenhall
et al., 2020), the scene geometry is parameterized using neural implicit
representations (i.e., MLPs). Implicit neural representations have achieved
impressive visual quality but have drawbacks in computational efficiency. In
this work, we propose a new approach that performs view synthesis using point
clouds. It is the first point-based method that achieves better visual quality
than NeRF while being 100x faster in rendering speed. Our approach builds on
existing works on differentiable point-based rendering but introduces a novel
technique we call ""Sculpted Neural Points (SNP)"", which significantly improves
the robustness to errors and holes in the reconstructed point cloud. We further
propose to use view-dependent point features based on spherical harmonics to
capture non-Lambertian surfaces, and new designs in the point-based rendering
pipeline that further boost the performance. Finally, we show that our system
supports fine-grained scene editing. Code is available at
https://github.com/princeton-vl/SNP.";Yiming Zuo<author:sep>Jia Deng;http://arxiv.org/pdf/2205.05869v2;cs.CV;;nerf
2205.05922v1;http://arxiv.org/abs/2205.05922v1;2022-05-12;Ray Priors through Reprojection: Improving Neural Radiance Fields for  Novel View Extrapolation;"Neural Radiance Fields (NeRF) have emerged as a potent paradigm for
representing scenes and synthesizing photo-realistic images. A main limitation
of conventional NeRFs is that they often fail to produce high-quality
renderings under novel viewpoints that are significantly different from the
training viewpoints. In this paper, instead of exploiting few-shot image
synthesis, we study the novel view extrapolation setting that (1) the training
images can well describe an object, and (2) there is a notable discrepancy
between the training and test viewpoints' distributions. We present RapNeRF
(RAy Priors) as a solution. Our insight is that the inherent appearances of a
3D surface's arbitrary visible projections should be consistent. We thus
propose a random ray casting policy that allows training unseen views using
seen views. Furthermore, we show that a ray atlas pre-computed from the
observed rays' viewing directions could further enhance the rendering quality
for extrapolated views. A main limitation is that RapNeRF would remove the
strong view-dependent effects because it leverages the multi-view consistency
property.";Jian Zhang<author:sep>Yuanqing Zhang<author:sep>Huan Fu<author:sep>Xiaowei Zhou<author:sep>Bowen Cai<author:sep>Jinchi Huang<author:sep>Rongfei Jia<author:sep>Binqiang Zhao<author:sep>Xing Tang;http://arxiv.org/pdf/2205.05922v1;cs.CV;;nerf
2205.04978v1;http://arxiv.org/abs/2205.04978v1;2022-05-10;NeRF-Editing: Geometry Editing of Neural Radiance Fields;"Implicit neural rendering, especially Neural Radiance Field (NeRF), has shown
great potential in novel view synthesis of a scene. However, current NeRF-based
methods cannot enable users to perform user-controlled shape deformation in the
scene. While existing works have proposed some approaches to modify the
radiance field according to the user's constraints, the modification is limited
to color editing or object translation and rotation. In this paper, we propose
a method that allows users to perform controllable shape deformation on the
implicit representation of the scene, and synthesizes the novel view images of
the edited scene without re-training the network. Specifically, we establish a
correspondence between the extracted explicit mesh representation and the
implicit neural representation of the target scene. Users can first utilize
well-developed mesh-based deformation methods to deform the mesh representation
of the scene. Our method then utilizes user edits from the mesh representation
to bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining
the rendering results of the edited scene. Extensive experiments demonstrate
that our framework can achieve ideal editing results not only on synthetic
data, but also on real scenes captured by users.";Yu-Jie Yuan<author:sep>Yang-Tian Sun<author:sep>Yu-Kun Lai<author:sep>Yuewen Ma<author:sep>Rongfei Jia<author:sep>Lin Gao;http://arxiv.org/pdf/2205.04978v1;cs.GR;Accepted by CVPR 2022;nerf
2205.01389v1;http://arxiv.org/abs/2205.01389v1;2022-05-03;Sampling-free obstacle gradients and reactive planning in Neural  Radiance Fields (NeRF);"This work investigates the use of Neural implicit representations,
specifically Neural Radiance Fields (NeRF), for geometrical queries and motion
planning. We show that by adding the capacity to infer occupancy in a radius to
a pre-trained NeRF, we are effectively learning an approximation to a Euclidean
Signed Distance Field (ESDF). Using backward differentiation of the augmented
network, we obtain an obstacle gradient that is integrated into an obstacle
avoidance policy based on the Riemannian Motion Policies (RMP) framework. Thus,
our findings allow for very fast sampling-free obstacle avoidance planning in
the implicit representation.";Michael Pantic<author:sep>Cesar Cadena<author:sep>Roland Siegwart<author:sep>Lionel Ott;http://arxiv.org/pdf/2205.01389v1;cs.RO;"Accepted to the ""Motion Planning with Implicit Neural Representations
  of Geometry"" Workshop at ICRA 2022";nerf
2204.13696v1;http://arxiv.org/abs/2204.13696v1;2022-04-28;NeurMiPs: Neural Mixture of Planar Experts for View Synthesis;"We present Neural Mixtures of Planar Experts (NeurMiPs), a novel planar-based
scene representation for modeling geometry and appearance. NeurMiPs leverages a
collection of local planar experts in 3D space as the scene representation.
Each planar expert consists of the parameters of the local rectangular shape
representing geometry and a neural radiance field modeling the color and
opacity. We render novel views by calculating ray-plane intersections and
composite output colors and densities at intersected points to the image.
NeurMiPs blends the efficiency of explicit mesh rendering and flexibility of
the neural radiance field. Experiments demonstrate superior performance and
speed of our proposed method, compared to other 3D representations in novel
view synthesis.";Zhi-Hao Lin<author:sep>Wei-Chiu Ma<author:sep>Hao-Yu Hsu<author:sep>Yu-Chiang Frank Wang<author:sep>Shenlong Wang;http://arxiv.org/pdf/2204.13696v1;cs.CV;CVPR 2022. Project page: https://zhihao-lin.github.io/neurmips/;
2204.13426v1;http://arxiv.org/abs/2204.13426v1;2022-04-28;AE-NeRF: Auto-Encoding Neural Radiance Fields for 3D-Aware Object  Manipulation;"We propose a novel framework for 3D-aware object manipulation, called
Auto-Encoding Neural Radiance Fields (AE-NeRF). Our model, which is formulated
in an auto-encoder architecture, extracts disentangled 3D attributes such as 3D
shape, appearance, and camera pose from an image, and a high-quality image is
rendered from the attributes through disentangled generative Neural Radiance
Fields (NeRF). To improve the disentanglement ability, we present two losses,
global-local attribute consistency loss defined between input and output, and
swapped-attribute classification loss. Since training such auto-encoding
networks from scratch without ground-truth shape and appearance information is
non-trivial, we present a stage-wise training scheme, which dramatically helps
to boost the performance. We conduct experiments to demonstrate the
effectiveness of the proposed model over the latest methods and provide
extensive ablation studies.";Mira Kim<author:sep>Jaehoon Ko<author:sep>Kyusun Cho<author:sep>Junmyeong Choi<author:sep>Daewon Choi<author:sep>Seungryong Kim;http://arxiv.org/pdf/2204.13426v1;cs.CV;;nerf
2204.11798v1;http://arxiv.org/abs/2204.11798v1;2022-04-25;Generalizable Neural Performer: Learning Robust Radiance Fields for  Human Novel View Synthesis;"This work targets at using a general deep learning framework to synthesize
free-viewpoint images of arbitrary human performers, only requiring a sparse
number of camera views as inputs and skirting per-case fine-tuning. The large
variation of geometry and appearance, caused by articulated body poses, shapes
and clothing types, are the key bottlenecks of this task. To overcome these
challenges, we present a simple yet powerful framework, named Generalizable
Neural Performer (GNR), that learns a generalizable and robust neural body
representation over various geometry and appearance. Specifically, we compress
the light fields for novel view human rendering as conditional implicit neural
radiance fields from both geometry and appearance aspects. We first introduce
an Implicit Geometric Body Embedding strategy to enhance the robustness based
on both parametric 3D human body model and multi-view images hints. We further
propose a Screen-Space Occlusion-Aware Appearance Blending technique to
preserve the high-quality appearance, through interpolating source view
appearance to the radiance fields with a relax but approximate geometric
guidance.
  To evaluate our method, we present our ongoing effort of constructing a
dataset with remarkable complexity and diversity. The dataset GeneBody-1.0,
includes over 360M frames of 370 subjects under multi-view cameras capturing,
performing a large variety of pose actions, along with diverse body shapes,
clothing, accessories and hairdos. Experiments on GeneBody-1.0 and ZJU-Mocap
show better robustness of our methods than recent state-of-the-art
generalizable methods among all cross-dataset, unseen subjects and unseen poses
settings. We also demonstrate the competitiveness of our model compared with
cutting-edge case-specific ones. Dataset, code and model will be made publicly
available.";Wei Cheng<author:sep>Su Xu<author:sep>Jingtan Piao<author:sep>Chen Qian<author:sep>Wayne Wu<author:sep>Kwan-Yee Lin<author:sep>Hongsheng Li;http://arxiv.org/pdf/2204.11798v1;cs.CV;"Project Page: https://generalizable-neural-performer.github.io/
  Dataset: https://generalizable-neural-performer.github.io/genebody.html/";
2204.10850v1;http://arxiv.org/abs/2204.10850v1;2022-04-22;Control-NeRF: Editable Feature Volumes for Scene Rendering and  Manipulation;"We present a novel method for performing flexible, 3D-aware image content
manipulation while enabling high-quality novel view synthesis. While NeRF-based
approaches are effective for novel view synthesis, such models memorize the
radiance for every point in a scene within a neural network. Since these models
are scene-specific and lack a 3D scene representation, classical editing such
as shape manipulation, or combining scenes is not possible. Hence, editing and
combining NeRF-based scenes has not been demonstrated. With the aim of
obtaining interpretable and controllable scene representations, our model
couples learnt scene-specific feature volumes with a scene agnostic neural
rendering network. With this hybrid representation, we decouple neural
rendering from scene-specific geometry and appearance. We can generalize to
novel scenes by optimizing only the scene-specific 3D feature representation,
while keeping the parameters of the rendering network fixed. The rendering
function learnt during the initial training stage can thus be easily applied to
new scenes, making our approach more flexible. More importantly, since the
feature volumes are independent of the rendering model, we can manipulate and
combine scenes by editing their corresponding feature volumes. The edited
volume can then be plugged into the rendering model to synthesize high-quality
novel views. We demonstrate various scene manipulations, including mixing
scenes, deforming objects and inserting objects into scenes, while still
producing photo-realistic results.";Verica Lazova<author:sep>Vladimir Guzov<author:sep>Kyle Olszewski<author:sep>Sergey Tulyakov<author:sep>Gerard Pons-Moll;http://arxiv.org/pdf/2204.10850v1;cs.CV;;nerf
2204.10516v2;http://arxiv.org/abs/2204.10516v2;2022-04-22;Implicit Object Mapping With Noisy Data;"Modelling individual objects in a scene as Neural Radiance Fields (NeRFs)
provides an alternative geometric scene representation that may benefit
downstream robotics tasks such as scene understanding and object manipulation.
However, we identify three challenges to using real-world training data
collected by a robot to train a NeRF: (i) The camera trajectories are
constrained, and full visual coverage is not guaranteed - especially when
obstructions to the objects of interest are present; (ii) the poses associated
with the images are noisy due to odometry or localization noise; (iii) the
objects are not easily isolated from the background. This paper evaluates the
extent to which above factors degrade the quality of the learnt implicit object
representation. We introduce a pipeline that decomposes a scene into multiple
individual object-NeRFs, using noisy object instance masks and bounding boxes,
and evaluate the sensitivity of this pipeline with respect to noisy poses,
instance masks, and the number of training images. We uncover that the
sensitivity to noisy instance masks can be partially alleviated with depth
supervision and quantify the importance of including the camera extrinsics in
the NeRF optimisation process.";Jad Abou-Chakra<author:sep>Feras Dayoub<author:sep>Niko Sünderhauf;http://arxiv.org/pdf/2204.10516v2;cs.RO;;nerf
2204.09523v1;http://arxiv.org/abs/2204.09523v1;2022-04-20;SILVR: A Synthetic Immersive Large-Volume Plenoptic Dataset;"In six-degrees-of-freedom light-field (LF) experiences, the viewer's freedom
is limited by the extent to which the plenoptic function was sampled. Existing
LF datasets represent only small portions of the plenoptic function, such that
they either cover a small volume, or they have limited field of view.
Therefore, we propose a new LF image dataset ""SILVR"" that allows for
six-degrees-of-freedom navigation in much larger volumes while maintaining full
panoramic field of view. We rendered three different virtual scenes in various
configurations, where the number of views ranges from 642 to 2226. One of these
scenes (called Zen Garden) is a novel scene, and is made publicly available. We
chose to position the virtual cameras closely together in large cuboid and
spherical organisations ($2.2m^3$ to $48m^3$), equipped with 180{\deg} fish-eye
lenses. Every view is rendered to a color image and depth map of 2048px
$\times$ 2048px. Additionally, we present the software used to automate the
multi-view rendering process, as well as a lens-reprojection tool that converts
between images with panoramic or fish-eye projection to a standard rectilinear
(i.e., perspective) projection. Finally, we demonstrate how the proposed
dataset and software can be used to evaluate LF coding/rendering techniques(in
this case for training NeRFs with instant-ngp). As such, we provide the first
publicly-available LF dataset for large volumes of light with full panoramic
field of view";Martijn Courteaux<author:sep>Julie Artois<author:sep>Stijn De Pauw<author:sep>Peter Lambert<author:sep>Glenn Van Wallendael;http://arxiv.org/pdf/2204.09523v1;cs.GR;"In 13th ACM Multimedia Systems Conference (MMSys '22), June 14-17,
  2022, Athlone, Ireland. ACM, New York, NY, USA, 6 pages";nerf
2204.06837v1;http://arxiv.org/abs/2204.06837v1;2022-04-14;Modeling Indirect Illumination for Inverse Rendering;"Recent advances in implicit neural representations and differentiable
rendering make it possible to simultaneously recover the geometry and materials
of an object from multi-view RGB images captured under unknown static
illumination. Despite the promising results achieved, indirect illumination is
rarely modeled in previous methods, as it requires expensive recursive path
tracing which makes the inverse rendering computationally intractable. In this
paper, we propose a novel approach to efficiently recovering spatially-varying
indirect illumination. The key insight is that indirect illumination can be
conveniently derived from the neural radiance field learned from input images
instead of being estimated jointly with direct illumination and materials. By
properly modeling the indirect illumination and visibility of direct
illumination, interreflection- and shadow-free albedo can be recovered. The
experiments on both synthetic and real data demonstrate the superior
performance of our approach compared to previous work and its capability to
synthesize realistic renderings under novel viewpoints and illumination. Our
code and data are available at https://zju3dv.github.io/invrender/.";Yuanqing Zhang<author:sep>Jiaming Sun<author:sep>Xingyi He<author:sep>Huan Fu<author:sep>Rongfei Jia<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2204.06837v1;cs.CV;;
2204.05735v1;http://arxiv.org/abs/2204.05735v1;2022-04-12;GARF: Gaussian Activated Radiance Fields for High Fidelity  Reconstruction and Pose Estimation;"Despite Neural Radiance Fields (NeRF) showing compelling results in
photorealistic novel views synthesis of real-world scenes, most existing
approaches require accurate prior camera poses. Although approaches for jointly
recovering the radiance field and camera pose exist (BARF), they rely on a
cumbersome coarse-to-fine auxiliary positional embedding to ensure good
performance. We present Gaussian Activated neural Radiance Fields (GARF), a new
positional embedding-free neural radiance field architecture - employing
Gaussian activations - that outperforms the current state-of-the-art in terms
of high fidelity reconstruction and pose estimation.";Shin-Fang Chng<author:sep>Sameera Ramasinghe<author:sep>Jamie Sherrah<author:sep>Simon Lucey;http://arxiv.org/pdf/2204.05735v1;cs.CV;Project page: https://sfchng.github.io/garf/;nerf
2204.04668v2;http://arxiv.org/abs/2204.04668v2;2022-04-10;NAN: Noise-Aware NeRFs for Burst-Denoising;"Burst denoising is now more relevant than ever, as computational photography
helps overcome sensitivity issues inherent in mobile phones and small cameras.
A major challenge in burst-denoising is in coping with pixel misalignment,
which was so far handled with rather simplistic assumptions of simple motion,
or the ability to align in pre-processing. Such assumptions are not realistic
in the presence of large motion and high levels of noise. We show that Neural
Radiance Fields (NeRFs), originally suggested for physics-based novel-view
rendering, can serve as a powerful framework for burst denoising. NeRFs have an
inherent capability of handling noise as they integrate information from
multiple images, but they are limited in doing so, mainly since they build on
pixel-wise operations which are suitable to ideal imaging conditions. Our
approach, termed NAN, leverages inter-view and spatial information in NeRFs to
better deal with noise. It achieves state-of-the-art results in burst denoising
and is especially successful in coping with large movement and occlusions,
under very high levels of noise. With the rapid advances in accelerating NeRFs,
it could provide a powerful platform for denoising in challenging environments.";Naama Pearl<author:sep>Tali Treibitz<author:sep>Simon Korman;http://arxiv.org/pdf/2204.04668v2;cs.CV;to appear at CVPR 2022;nerf
2204.03715v1;http://arxiv.org/abs/2204.03715v1;2022-04-07;Gravitationally Lensed Black Hole Emission Tomography;"Measurements from the Event Horizon Telescope enabled the visualization of
light emission around a black hole for the first time. So far, these
measurements have been used to recover a 2D image under the assumption that the
emission field is static over the period of acquisition. In this work, we
propose BH-NeRF, a novel tomography approach that leverages gravitational
lensing to recover the continuous 3D emission field near a black hole. Compared
to other 3D reconstruction or tomography settings, this task poses two
significant challenges: first, rays near black holes follow curved paths
dictated by general relativity, and second, we only observe measurements from a
single viewpoint. Our method captures the unknown emission field using a
continuous volumetric function parameterized by a coordinate-based neural
network, and uses knowledge of Keplerian orbital dynamics to establish
correspondence between 3D points over time. Together, these enable BH-NeRF to
recover accurate 3D emission fields, even in challenging situations with sparse
measurements and uncertain orbital dynamics. This work takes the first steps in
showing how future measurements from the Event Horizon Telescope could be used
to recover evolving 3D emission around the supermassive black hole in our
Galactic center.";Aviad Levis<author:sep>Pratul P. Srinivasan<author:sep>Andrew A. Chael<author:sep>Ren Ng<author:sep>Katherine L. Bouman;http://arxiv.org/pdf/2204.03715v1;cs.CV;"To appear in the IEEE Proceedings of the Conference on Computer
  Vision and Pattern Recognition (CVPR), 2022. Supplemental material including
  accompanying pdf, code, and video highlight can be found in the project page:
  http://imaging.cms.caltech.edu/bhnerf/";nerf
2204.02585v3;http://arxiv.org/abs/2204.02585v3;2022-04-06;SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference;"Neural Radiance Fields (NeRF) has emerged as the state-of-the-art method for
novel view generation of complex scenes, but is very slow during inference.
Recently, there have been multiple works on speeding up NeRF inference, but the
state of the art methods for real-time NeRF inference rely on caching the
neural network output, which occupies several giga-bytes of disk space that
limits their real-world applicability. As caching the neural network of
original NeRF network is not feasible, Garbin et al. proposed ""FastNeRF"" which
factorizes the problem into 2 sub-networks - one which depends only on the 3D
coordinate of a sample point and one which depends only on the 2D camera
viewing direction. Although this factorization enables them to reduce the cache
size and perform inference at over 200 frames per second, the memory overhead
is still substantial. In this work, we propose SqueezeNeRF, which is more than
60 times memory-efficient than the sparse cache of FastNeRF and is still able
to render at more than 190 frames per second on a high spec GPU during
inference.";Krishna Wadhwani<author:sep>Tamaki Kojima;http://arxiv.org/pdf/2204.02585v3;cs.CV;"9 pages, 3 figures, 5 tables. Presented in the ""5th Efficient Deep
  Learning for Computer Vision"" CVPR 2022 Workshop""";nerf
2204.01943v3;http://arxiv.org/abs/2204.01943v3;2022-04-05;Unified Implicit Neural Stylization;"Representing visual signals by implicit representation (e.g., a coordinate
based deep network) has prevailed among many vision tasks. This work explores a
new intriguing direction: training a stylized implicit representation, using a
generalized approach that can apply to various 2D and 3D scenarios. We conduct
a pilot study on a variety of implicit functions, including 2D coordinate-based
representation, neural radiance field, and signed distance function. Our
solution is a Unified Implicit Neural Stylization framework, dubbed INS. In
contrary to vanilla implicit representation, INS decouples the ordinary
implicit function into a style implicit module and a content implicit module,
in order to separately encode the representations from the style image and
input scenes. An amalgamation module is then applied to aggregate these
information and synthesize the stylized output. To regularize the geometry in
3D scenes, we propose a novel self-distillation geometry consistency loss which
preserves the geometry fidelity of the stylized scenes. Comprehensive
experiments are conducted on multiple task settings, including novel view
synthesis of complex scenes, stylization for implicit surfaces, and fitting
images using MLPs. We further demonstrate that the learned representation is
continuous not only spatially but also style-wise, leading to effortlessly
interpolating between different styles and generating images with new mixed
styles. Please refer to the video on our project page for more view synthesis
results: https://zhiwenfan.github.io/INS.";Zhiwen Fan<author:sep>Yifan Jiang<author:sep>Peihao Wang<author:sep>Xinyu Gong<author:sep>Dejia Xu<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2204.01943v3;cs.CV;;
2204.01218v2;http://arxiv.org/abs/2204.01218v2;2022-04-04;Neural Rendering of Humans in Novel View and Pose from Monocular Video;"We introduce a new method that generates photo-realistic humans under novel
views and poses given a monocular video as input. Despite the significant
progress recently on this topic, with several methods exploring shared
canonical neural radiance fields in dynamic scene scenarios, learning a
user-controlled model for unseen poses remains a challenging task. To tackle
this problem, we introduce an effective method to a) integrate observations
across several frames and b) encode the appearance at each individual frame. We
accomplish this by utilizing both the human pose that models the body shape as
well as point clouds that partially cover the human as input. Our approach
simultaneously learns a shared set of latent codes anchored to the human pose
among several frames, and an appearance-dependent code anchored to incomplete
point clouds generated by each frame and its predicted depth. The former human
pose-based code models the shape of the performer whereas the latter point
cloud-based code predicts fine-level details and reasons about missing
structures at the unseen poses. To further recover non-visible regions in query
frames, we employ a temporal transformer to integrate features of points in
query frames and tracked body points from automatically-selected key frames.
Experiments on various sequences of dynamic humans from different datasets
including ZJU-MoCap show that our method significantly outperforms existing
approaches under unseen poses and novel views given monocular videos as input.";Tiantian Wang<author:sep>Nikolaos Sarafianos<author:sep>Ming-Hsuan Yang<author:sep>Tony Tung;http://arxiv.org/pdf/2204.01218v2;cs.CV;10 pages;
2204.00928v2;http://arxiv.org/abs/2204.00928v2;2022-04-02;SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single  Image;"Despite the rapid development of Neural Radiance Field (NeRF), the necessity
of dense covers largely prohibits its wider applications. While several recent
works have attempted to address this issue, they either operate with sparse
views (yet still, a few of them) or on simple objects/scenes. In this work, we
consider a more ambitious task: training neural radiance field, over
realistically complex visual scenes, by ""looking only once"", i.e., using only a
single view. To attain this goal, we present a Single View NeRF (SinNeRF)
framework consisting of thoughtfully designed semantic and geometry
regularizations. Specifically, SinNeRF constructs a semi-supervised learning
process, where we introduce and propagate geometry pseudo labels and semantic
pseudo labels to guide the progressive training process. Extensive experiments
are conducted on complex scene benchmarks, including NeRF synthetic dataset,
Local Light Field Fusion dataset, and DTU dataset. We show that even without
pre-training on multi-view datasets, SinNeRF can yield photo-realistic
novel-view synthesis results. Under the single image setting, SinNeRF
significantly outperforms the current state-of-the-art NeRF baselines in all
cases. Project page: https://vita-group.github.io/SinNeRF/";Dejia Xu<author:sep>Yifan Jiang<author:sep>Peihao Wang<author:sep>Zhiwen Fan<author:sep>Humphrey Shi<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2204.00928v2;cs.CV;Project page: https://vita-group.github.io/SinNeRF/;nerf
2203.17261v2;http://arxiv.org/abs/2203.17261v2;2022-03-31;R2L: Distilling Neural Radiance Field to Neural Light Field for  Efficient Novel View Synthesis;"Recent research explosion on Neural Radiance Field (NeRF) shows the
encouraging potential to represent complex scenes with neural networks. One
major drawback of NeRF is its prohibitive inference time: Rendering a single
pixel requires querying the NeRF network hundreds of times. To resolve it,
existing efforts mainly attempt to reduce the number of required sampled
points. However, the problem of iterative sampling still exists. On the other
hand, Neural Light Field (NeLF) presents a more straightforward representation
over NeRF in novel view synthesis -- the rendering of a pixel amounts to one
single forward pass without ray-marching. In this work, we present a deep
residual MLP network (88 layers) to effectively learn the light field. We show
the key to successfully learning such a deep NeLF network is to have sufficient
data, for which we transfer the knowledge from a pre-trained NeRF model via
data distillation. Extensive experiments on both synthetic and real-world
scenes show the merits of our method over other counterpart algorithms. On the
synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x
runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average
PSNR improvement) rendering quality than NeRF without any customized
parallelism requirement.";Huan Wang<author:sep>Jian Ren<author:sep>Zeng Huang<author:sep>Kyle Olszewski<author:sep>Menglei Chai<author:sep>Yun Fu<author:sep>Sergey Tulyakov;http://arxiv.org/pdf/2203.17261v2;cs.CV;Accepted by ECCV 2022. Code: https://github.com/snap-research/R2L;nerf
2203.16875v2;http://arxiv.org/abs/2203.16875v2;2022-03-31;MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images;"There has been rapid progress recently on 3D human rendering, including novel
view synthesis and pose animation, based on the advances of neural radiance
fields (NeRF). However, most existing methods focus on person-specific training
and their training typically requires multi-view videos. This paper deals with
a new challenging task -- rendering novel views and novel poses for a person
unseen in training, using only multiview images as input. For this task, we
propose a simple yet effective method to train a generalizable NeRF with
multiview images as conditional input. The key ingredient is a dedicated
representation combining a canonical NeRF and a volume deformation scheme.
Using a canonical space enables our method to learn shared properties of human
and easily generalize to different people. Volume deformation is used to
connect the canonical space with input and target images and query image
features for radiance and density prediction. We leverage the parametric 3D
human model fitted on the input images to derive the deformation, which works
quite well in practice when combined with our canonical NeRF. The experiments
on both real and synthetic data with the novel view synthesis and pose
animation tasks collectively demonstrate the efficacy of our method.";Xiangjun Gao<author:sep>Jiaolong Yang<author:sep>Jongyoo Kim<author:sep>Sida Peng<author:sep>Zicheng Liu<author:sep>Xin Tong;http://arxiv.org/pdf/2203.16875v2;cs.CV;;nerf
2203.16626v1;http://arxiv.org/abs/2203.16626v1;2022-03-30;DDNeRF: Depth Distribution Neural Radiance Fields;"In recent years, the field of implicit neural representation has progressed
significantly. Models such as neural radiance fields (NeRF), which uses
relatively small neural networks, can represent high-quality scenes and achieve
state-of-the-art results for novel view synthesis. Training these types of
networks, however, is still computationally very expensive. We present depth
distribution neural radiance field (DDNeRF), a new method that significantly
increases sampling efficiency along rays during training while achieving
superior results for a given sampling budget. DDNeRF achieves this by learning
a more accurate representation of the density distribution along rays. More
specifically, we train a coarse model to predict the internal distribution of
the transparency of an input volume in addition to the volume's total density.
This finer distribution then guides the sampling procedure of the fine model.
This method allows us to use fewer samples during training while reducing
computational resources.";David Dadon<author:sep>Ohad Fried<author:sep>Yacov Hel-Or;http://arxiv.org/pdf/2203.16626v1;cs.CV;;nerf
2203.15798v1;http://arxiv.org/abs/2203.15798v1;2022-03-29;DRaCoN -- Differentiable Rasterization Conditioned Neural Radiance  Fields for Articulated Avatars;"Acquisition and creation of digital human avatars is an important problem
with applications to virtual telepresence, gaming, and human modeling. Most
contemporary approaches for avatar generation can be viewed either as 3D-based
methods, which use multi-view data to learn a 3D representation with appearance
(such as a mesh, implicit surface, or volume), or 2D-based methods which learn
photo-realistic renderings of avatars but lack accurate 3D representations. In
this work, we present, DRaCoN, a framework for learning full-body volumetric
avatars which exploits the advantages of both the 2D and 3D neural rendering
techniques. It consists of a Differentiable Rasterization module, DiffRas, that
synthesizes a low-resolution version of the target image along with additional
latent features guided by a parametric body model. The output of DiffRas is
then used as conditioning to our conditional neural 3D representation module
(c-NeRF) which generates the final high-res image along with body geometry
using volumetric rendering. While DiffRas helps in obtaining photo-realistic
image quality, c-NeRF, which employs signed distance fields (SDF) for 3D
representations, helps to obtain fine 3D geometric details. Experiments on the
challenging ZJU-MoCap and Human3.6M datasets indicate that DRaCoN outperforms
state-of-the-art methods both in terms of error metrics and visual quality.";Amit Raj<author:sep>Umar Iqbal<author:sep>Koki Nagano<author:sep>Sameh Khamis<author:sep>Pavlo Molchanov<author:sep>James Hays<author:sep>Jan Kautz;http://arxiv.org/pdf/2203.15798v1;cs.CV;Project page at https://dracon-avatars.github.io/;nerf
2203.15946v2;http://arxiv.org/abs/2203.15946v2;2022-03-29;Towards Learning Neural Representations from Shadows;"We present a method that learns neural shadow fields which are neural scene
representations that are only learnt from the shadows present in the scene.
While traditional shape-from-shadow (SfS) algorithms reconstruct geometry from
shadows, they assume a fixed scanning setup and fail to generalize to complex
scenes. Neural rendering algorithms, on the other hand, rely on photometric
consistency between RGB images, but largely ignore physical cues such as
shadows, which have been shown to provide valuable information about the scene.
We observe that shadows are a powerful cue that can constrain neural scene
representations to learn SfS, and even outperform NeRF to reconstruct otherwise
hidden geometry. We propose a graphics-inspired differentiable approach to
render accurate shadows with volumetric rendering, predicting a shadow map that
can be compared to the ground truth shadow. Even with just binary shadow maps,
we show that neural rendering can localize the object and estimate coarse
geometry. Our approach reveals that sparse cues in images can be used to
estimate geometry using differentiable volumetric rendering. Moreover, our
framework is highly generalizable and can work alongside existing 3D
reconstruction techniques that otherwise only use photometric consistency.";Kushagra Tiwary<author:sep>Tzofi Klinghoffer<author:sep>Ramesh Raskar;http://arxiv.org/pdf/2203.15946v2;cs.CV;;nerf
2203.15224v2;http://arxiv.org/abs/2203.15224v2;2022-03-29;Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene  Segmentation;"Large-scale training data with high-quality annotations is critical for
training semantic and instance segmentation models. Unfortunately, pixel-wise
annotation is labor-intensive and costly, raising the demand for more efficient
labeling strategies. In this work, we present a novel 3D-to-2D label transfer
method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and
instance labels from easy-to-obtain coarse 3D bounding primitives. Our method
utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D
semantic cues transferred from existing datasets. We demonstrate that this
combination allows for improved geometry guided by semantic information,
enabling rendering of accurate semantic maps across multiple views.
Furthermore, this fusion process resolves label ambiguity of the coarse 3D
annotations and filters noise in the 2D predictions. By inferring in 3D space
and rendering to 2D labels, our 2D semantic and instance labels are multi-view
consistent by design. Experimental results show that Panoptic NeRF outperforms
existing label transfer methods in terms of accuracy and multi-view consistency
on challenging urban scenes of the KITTI-360 dataset.";Xiao Fu<author:sep>Shangzhan Zhang<author:sep>Tianrun Chen<author:sep>Yichong Lu<author:sep>Lanyun Zhu<author:sep>Xiaowei Zhou<author:sep>Andreas Geiger<author:sep>Yiyi Liao;http://arxiv.org/pdf/2203.15224v2;cs.CV;Project page: https://fuxiao0719.github.io/projects/panopticnerf/;nerf
2203.15587v2;http://arxiv.org/abs/2203.15587v2;2022-03-26;RGB-D Neural Radiance Fields: Local Sampling for Faster Training;"Learning a 3D representation of a scene has been a challenging problem for
decades in computer vision. Recent advances in implicit neural representation
from images using neural radiance fields(NeRF) have shown promising results.
Some of the limitations of previous NeRF based methods include longer training
time, and inaccurate underlying geometry. The proposed method takes advantage
of RGB-D data to reduce training time by leveraging depth sensing to improve
local sampling. This paper proposes a depth-guided local sampling strategy and
a smaller neural network architecture to achieve faster training time without
compromising quality.";Arnab Dey<author:sep>Andrew I. Comport;http://arxiv.org/pdf/2203.15587v2;cs.CV;;nerf
2203.13800v1;http://arxiv.org/abs/2203.13800v1;2022-03-25;Continuous Dynamic-NeRF: Spline-NeRF;"The problem of reconstructing continuous functions over time is important for
problems such as reconstructing moving scenes, and interpolating between time
steps. Previous approaches that use deep-learning rely on regularization to
ensure that reconstructions are approximately continuous, which works well on
short sequences. As sequence length grows, though, it becomes more difficult to
regularize, and it becomes less feasible to learn only through regularization.
We propose a new architecture for function reconstruction based on classical
Bezier splines, which ensures $C^0$ and $C^1$-continuity, where $C^0$
continuity is that $\forall c:\lim\limits_{x\to c} f(x)
  = f(c)$, or more intuitively that there are no breaks at any point in the
function. In order to demonstrate our architecture, we reconstruct dynamic
scenes using Neural Radiance Fields, but hope it is clear that our approach is
general and can be applied to a variety of problems. We recover a Bezier spline
$B(\beta, t\in[0,1])$, parametrized by the control points $\beta$. Using Bezier
splines ensures reconstructions have $C^0$ and $C^1$ continuity, allowing for
guaranteed interpolation over time. We reconstruct $\beta$ with a multi-layer
perceptron (MLP), blending machine learning with classical animation
techniques. All code is available at https://github.com/JulianKnodt/nerf_atlas,
and datasets are from prior work.";Julian Knodt;http://arxiv.org/pdf/2203.13800v1;cs.CV;;nerf
2203.12575v2;http://arxiv.org/abs/2203.12575v2;2022-03-23;NeuMan: Neural Human Radiance Field from a Single Video;"Photorealistic rendering and reposing of humans is important for enabling
augmented reality experiences. We propose a novel framework to reconstruct the
human and the scene that can be rendered with novel human poses and views from
just a single in-the-wild video. Given a video captured by a moving camera, we
train two NeRF models: a human NeRF model and a scene NeRF model. To train
these models, we rely on existing methods to estimate the rough geometry of the
human and the scene. Those rough geometry estimates allow us to create a
warping field from the observation space to the canonical pose-independent
space, where we train the human model in. Our method is able to learn subject
specific details, including cloth wrinkles and accessories, from just a 10
seconds video clip, and to provide high quality renderings of the human under
novel poses, from novel views, together with the background.";Wei Jiang<author:sep>Kwang Moo Yi<author:sep>Golnoosh Samei<author:sep>Oncel Tuzel<author:sep>Anurag Ranjan;http://arxiv.org/pdf/2203.12575v2;cs.CV;;nerf
2203.10821v2;http://arxiv.org/abs/2203.10821v2;2022-03-21;Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance  Fields;"Image translation and manipulation have gain increasing attention along with
the rapid development of deep generative models. Although existing approaches
have brought impressive results, they mainly operated in 2D space. In light of
recent advances in NeRF-based 3D-aware generative models, we introduce a new
task, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene
modelled by NeRF, conditioned on one single-view semantic mask as input. To
kick-off this novel task, we propose the Sem2NeRF framework. In particular,
Sem2NeRF addresses the highly challenging task by encoding the semantic mask
into the latent code that controls the 3D scene representation of a pre-trained
decoder. To further improve the accuracy of the mapping, we integrate a new
region-aware learning strategy into the design of both the encoder and the
decoder. We verify the efficacy of the proposed Sem2NeRF and demonstrate that
it outperforms several strong baselines on two benchmark datasets. Code and
video are available at https://donydchen.github.io/sem2nerf/";Yuedong Chen<author:sep>Qianyi Wu<author:sep>Chuanxia Zheng<author:sep>Tat-Jen Cham<author:sep>Jianfei Cai;http://arxiv.org/pdf/2203.10821v2;cs.CV;"ECCV2022, Code: https://github.com/donydchen/sem2nerf Project Page:
  https://donydchen.github.io/sem2nerf/";nerf
2203.11283v1;http://arxiv.org/abs/2203.11283v1;2022-03-21;NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction;"While NeRF has shown great success for neural reconstruction and rendering,
its limited MLP capacity and long per-scene optimization times make it
challenging to model large-scale indoor scenes. In contrast, classical 3D
reconstruction methods can handle large-scale scenes but do not produce
realistic renderings. We propose NeRFusion, a method that combines the
advantages of NeRF and TSDF-based fusion techniques to achieve efficient
large-scale reconstruction and photo-realistic rendering. We process the input
image sequence to predict per-frame local radiance fields via direct network
inference. These are then fused using a novel recurrent neural network that
incrementally reconstructs a global, sparse scene representation in real-time
at 22 fps. This global volume can be further fine-tuned to boost rendering
quality. We demonstrate that NeRFusion achieves state-of-the-art quality on
both large-scale indoor and small-scale object scenes, with substantially
faster reconstruction than NeRF and other recent methods.";Xiaoshuai Zhang<author:sep>Sai Bi<author:sep>Kalyan Sunkavalli<author:sep>Hao Su<author:sep>Zexiang Xu;http://arxiv.org/pdf/2203.11283v1;cs.CV;CVPR 2022;nerf
2203.09957v4;http://arxiv.org/abs/2203.09957v4;2022-03-18;Enhancement of Novel View Synthesis Using Omnidirectional Image  Completion;"In this study, we present a method for synthesizing novel views from a single
360-degree RGB-D image based on the neural radiance field (NeRF) . Prior
studies relied on the neighborhood interpolation capability of multi-layer
perceptrons to complete missing regions caused by occlusion and zooming, which
leads to artifacts. In the method proposed in this study, the input image is
reprojected to 360-degree RGB images at other camera positions, the missing
regions of the reprojected images are completed by a 2D image generative model,
and the completed images are utilized to train the NeRF. Because multiple
completed images contain inconsistencies in 3D, we introduce a method to learn
the NeRF model using a subset of completed images that cover the target scene
with less overlap of completed regions. The selection of such a subset of
images can be attributed to the maximum weight independent set problem, which
is solved through simulated annealing. Experiments demonstrated that the
proposed method can synthesize plausible novel views while preserving the
features of the scene for both artificial and real-world data.";Takayuki Hara<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2203.09957v4;cs.CV;20 pages, 19 figures;nerf
2203.10157v2;http://arxiv.org/abs/2203.10157v2;2022-03-18;ViewFormer: NeRF-free Neural Rendering from Few Images Using  Transformers;"Novel view synthesis is a long-standing problem. In this work, we consider a
variant of the problem where we are given only a few context views sparsely
covering a scene or an object. The goal is to predict novel viewpoints in the
scene, which requires learning priors. The current state of the art is based on
Neural Radiance Field (NeRF), and while achieving impressive results, the
methods suffer from long training times as they require evaluating millions of
3D point samples via a neural network for each image. We propose a 2D-only
method that maps multiple context views and a query pose to a new image in a
single pass of a neural network. Our model uses a two-stage architecture
consisting of a codebook and a transformer model. The codebook is used to embed
individual images into a smaller latent space, and the transformer solves the
view synthesis task in this more compact space. To train our model efficiently,
we introduce a novel branching attention mechanism that allows us to use the
same model not only for neural rendering but also for camera pose estimation.
Experimental results on real-world scenes show that our approach is competitive
compared to NeRF-based methods while not reasoning explicitly in 3D, and it is
faster to train.";Jonáš Kulhánek<author:sep>Erik Derner<author:sep>Torsten Sattler<author:sep>Robert Babuška;http://arxiv.org/pdf/2203.10157v2;cs.CV;ECCV 2022 poster;nerf
2203.10192v1;http://arxiv.org/abs/2203.10192v1;2022-03-18;Conditional-Flow NeRF: Accurate 3D Modelling with Reliable Uncertainty  Quantification;"A critical limitation of current methods based on Neural Radiance Fields
(NeRF) is that they are unable to quantify the uncertainty associated with the
learned appearance and geometry of the scene. This information is paramount in
real applications such as medical diagnosis or autonomous driving where, to
reduce potentially catastrophic failures, the confidence on the model outputs
must be included into the decision-making process. In this context, we
introduce Conditional-Flow NeRF (CF-NeRF), a novel probabilistic framework to
incorporate uncertainty quantification into NeRF-based approaches. For this
purpose, our method learns a distribution over all possible radiance fields
modelling which is used to quantify the uncertainty associated with the
modelled scene. In contrast to previous approaches enforcing strong constraints
over the radiance field distribution, CF-NeRF learns it in a flexible and fully
data-driven manner by coupling Latent Variable Modelling and Conditional
Normalizing Flows. This strategy allows to obtain reliable uncertainty
estimation while preserving model expressivity. Compared to previous
state-of-the-art methods proposed for uncertainty quantification in NeRF, our
experiments show that the proposed method achieves significantly lower
prediction errors and more reliable uncertainty values for synthetic novel view
and depth-map estimation.";Jianxiong Shen<author:sep>Antonio Agudo<author:sep>Francesc Moreno-Noguer<author:sep>Adria Ruiz;http://arxiv.org/pdf/2203.10192v1;cs.CV;;nerf
2203.09517v2;http://arxiv.org/abs/2203.09517v2;2022-03-17;TensoRF: Tensorial Radiance Fields;"We present TensoRF, a novel approach to model and reconstruct radiance
fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a
scene as a 4D tensor, which represents a 3D voxel grid with per-voxel
multi-channel features. Our central idea is to factorize the 4D scene tensor
into multiple compact low-rank tensor components. We demonstrate that applying
traditional CP decomposition -- that factorizes tensors into rank-one
components with compact vectors -- in our framework leads to improvements over
vanilla NeRF. To further boost performance, we introduce a novel vector-matrix
(VM) decomposition that relaxes the low-rank constraints for two modes of a
tensor and factorizes tensors into compact vector and matrix factors. Beyond
superior rendering quality, our models with CP and VM decompositions lead to a
significantly lower memory footprint in comparison to previous and concurrent
works that directly optimize per-voxel features. Experimentally, we demonstrate
that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with
better rendering quality and even a smaller model size (<4 MB) compared to
NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality
and outperforms previous state-of-the-art methods, while reducing the
reconstruction time (<10 min) and retaining a compact model size (<75 MB).";Anpei Chen<author:sep>Zexiang Xu<author:sep>Andreas Geiger<author:sep>Jingyi Yu<author:sep>Hao Su;http://arxiv.org/pdf/2203.09517v2;cs.CV;Project Page: https://apchenstu.github.io/TensoRF/;nerf
2203.08896v2;http://arxiv.org/abs/2203.08896v2;2022-03-16;Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient  Objects and Shadow Modeling Using RPC Cameras;"We introduce the Satellite Neural Radiance Field (Sat-NeRF), a new end-to-end
model for learning multi-view satellite photogrammetry in the wild. Sat-NeRF
combines some of the latest trends in neural rendering with native satellite
camera models, represented by rational polynomial coefficient (RPC) functions.
The proposed method renders new views and infers surface models of similar
quality to those obtained with traditional state-of-the-art stereo pipelines.
Multi-date images exhibit significant changes in appearance, mainly due to
varying shadows and transient objects (cars, vegetation). Robustness to these
challenges is achieved by a shadow-aware irradiance model and uncertainty
weighting to deal with transient phenomena that cannot be explained by the
position of the sun. We evaluate Sat-NeRF using WorldView-3 images from
different locations and stress the advantages of applying a bundle adjustment
to the satellite camera models prior to training. This boosts the network
performance and can optionally be used to extract additional cues for depth
supervision.";Roger Marí<author:sep>Gabriele Facciolo<author:sep>Thibaud Ehret;http://arxiv.org/pdf/2203.08896v2;cs.CV;Accepted at CVPR EarthVision Workshop 2022;nerf
2203.07931v2;http://arxiv.org/abs/2203.07931v2;2022-03-15;DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video  Generation;"Conversation is an essential component of virtual avatar activities in the
metaverse. With the development of natural language processing, textual and
vocal conversation generation has achieved a significant breakthrough. However,
face-to-face conversations account for the vast majority of daily
conversations, while most existing methods focused on single-person talking
head generation. In this work, we take a step further and consider generating
realistic face-to-face conversation videos. Conversation generation is more
challenging than single-person talking head generation, since it not only
requires generating photo-realistic individual talking heads but also demands
the listener to respond to the speaker. In this paper, we propose a novel
unified framework based on neural radiance field (NeRF) to address this task.
Specifically, we model both the speaker and listener with a NeRF framework,
with different conditions to control individual expressions. The speaker is
driven by the audio signal, while the response of the listener depends on both
visual and acoustic information. In this way, face-to-face conversation videos
are generated between human avatars, with all the interlocutors modeled within
the same network. Moreover, to facilitate future research on this task, we
collect a new human conversation dataset containing 34 clips of videos.
Quantitative and qualitative experiments evaluate our method in different
aspects, e.g., image quality, pose sequence trend, and naturalness of the
rendering videos. Experimental results demonstrate that the avatars in the
resulting videos are able to perform a realistic conversation, and maintain
individual styles. All the code, data, and models will be made publicly
available.";Yichao Yan<author:sep>Zanwei Zhou<author:sep>Zi Wang<author:sep>Jingnan Gao<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2203.07931v2;cs.CV;;nerf
2203.08133v4;http://arxiv.org/abs/2203.08133v4;2022-03-15;Animatable Implicit Neural Representations for Creating Realistic  Avatars from Videos;"This paper addresses the challenge of reconstructing an animatable human
model from a multi-view video. Some recent works have proposed to decompose a
non-rigidly deforming scene into a canonical neural radiance field and a set of
deformation fields that map observation-space points to the canonical space,
thereby enabling them to learn the dynamic scene from images. However, they
represent the deformation field as translational vector field or SE(3) field,
which makes the optimization highly under-constrained. Moreover, these
representations cannot be explicitly controlled by input motions. Instead, we
introduce a pose-driven deformation field based on the linear blend skinning
algorithm, which combines the blend weight field and the 3D human skeleton to
produce observation-to-canonical correspondences. Since 3D human skeletons are
more observable, they can regularize the learning of the deformation field.
Moreover, the pose-driven deformation field can be controlled by input skeletal
motions to generate new deformation fields to animate the canonical human
model. Experiments show that our approach significantly outperforms recent
human modeling methods. The code is available at
https://zju3dv.github.io/animatable_nerf/.";Sida Peng<author:sep>Zhen Xu<author:sep>Junting Dong<author:sep>Qianqian Wang<author:sep>Shangzhan Zhang<author:sep>Qing Shuai<author:sep>Hujun Bao<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2203.08133v4;cs.CV;"Project page: https://zju3dv.github.io/animatable_nerf/. arXiv admin
  note: substantial text overlap with arXiv:2105.02872";nerf
2203.06457v1;http://arxiv.org/abs/2203.06457v1;2022-03-12;3D-GIF: 3D-Controllable Object Generation via Implicit Factorized  Representations;"While NeRF-based 3D-aware image generation methods enable viewpoint control,
limitations still remain to be adopted to various 3D applications. Due to their
view-dependent and light-entangled volume representation, the 3D geometry
presents unrealistic quality and the color should be re-rendered for every
desired viewpoint. To broaden the 3D applicability from 3D-aware image
generation to 3D-controllable object generation, we propose the factorized
representations which are view-independent and light-disentangled, and training
schemes with randomly sampled light conditions. We demonstrate the superiority
of our method by visualizing factorized representations, re-lighted images, and
albedo-textured meshes. In addition, we show that our approach improves the
quality of the generated geometry via visualization and quantitative
comparison. To the best of our knowledge, this is the first work that extracts
albedo-textured meshes with unposed 2D images without any additional labels or
assumptions.";Minsoo Lee<author:sep>Chaeyeon Chung<author:sep>Hojun Cho<author:sep>Minjung Kim<author:sep>Sanghun Jung<author:sep>Jaegul Choo<author:sep>Minhyuk Sung;http://arxiv.org/pdf/2203.06457v1;cs.CV;;nerf
2203.05189v2;http://arxiv.org/abs/2203.05189v2;2022-03-10;NeRFocus: Neural Radiance Field for 3D Synthetic Defocus;"Neural radiance fields (NeRF) bring a new wave for 3D interactive
experiences. However, as an important part of the immersive experiences, the
defocus effects have not been fully explored within NeRF. Some recent
NeRF-based methods generate 3D defocus effects in a post-process fashion by
utilizing multiplane technology. Still, they are either time-consuming or
memory-consuming. This paper proposes a novel thin-lens-imaging-based NeRF
framework that can directly render various 3D defocus effects, dubbed NeRFocus.
Unlike the pinhole, the thin lens refracts rays of a scene point, so its
imaging on the sensor plane is scattered as a circle of confusion (CoC). A
direct solution sampling enough rays to approximate this process is
computationally expensive. Instead, we propose to inverse the thin lens imaging
to explicitly model the beam path for each point on the sensor plane and
generalize this paradigm to the beam path of each pixel, then use the
frustum-based volume rendering to render each pixel's beam path. We further
design an efficient probabilistic training (p-training) strategy to simplify
the training process vastly. Extensive experiments demonstrate that our
NeRFocus can achieve various 3D defocus effects with adjustable camera pose,
focus distance, and aperture size. Existing NeRF can be regarded as our special
case by setting aperture size as zero to render large depth-of-field images.
Despite such merits, NeRFocus does not sacrifice NeRF's original performance
(e.g., training and inference time, parameter consumption, rendering quality),
which implies its great potential for broader application and further
improvement. Code and video are available at
https://github.com/wyhuai/NeRFocus.";Yinhuai Wang<author:sep>Shuzhou Yang<author:sep>Yujie Hu<author:sep>Jian Zhang;http://arxiv.org/pdf/2203.05189v2;cs.CV;;nerf
2203.04802v2;http://arxiv.org/abs/2203.04802v2;2022-03-09;NeRF-Pose: A First-Reconstruct-Then-Regress Approach for  Weakly-supervised 6D Object Pose Estimation;"Pose estimation of 3D objects in monocular images is a fundamental and
long-standing problem in computer vision. Existing deep learning approaches for
6D pose estimation typically rely on the assumption of availability of 3D
object models and 6D pose annotations. However, precise annotation of 6D poses
in real data is intricate, time-consuming and not scalable, while synthetic
data scales well but lacks realism. To avoid these problems, we present a
weakly-supervised reconstruction-based pipeline, named NeRF-Pose, which needs
only 2D object segmentation and known relative camera poses during training.
Following the first-reconstruct-then-regress idea, we first reconstruct the
objects from multiple views in the form of an implicit neural representation.
Then, we train a pose regression network to predict pixel-wise 2D-3D
correspondences between images and the reconstructed model. At inference, the
approach only needs a single image as input. A NeRF-enabled PnP+RANSAC
algorithm is used to estimate stable and accurate pose from the predicted
correspondences. Experiments on LineMod and LineMod-Occlusion show that the
proposed method has state-of-the-art accuracy in comparison to the best 6D pose
estimation methods in spite of being trained only with weak labels. Besides, we
extend the Homebrewed DB dataset with more real training images to support the
weakly supervised task and achieve compelling results on this dataset. The
extended dataset and code will be released soon.";Fu Li<author:sep>Hao Yu<author:sep>Ivan Shugurov<author:sep>Benjamin Busam<author:sep>Shaowu Yang<author:sep>Slobodan Ilic;http://arxiv.org/pdf/2203.04802v2;cs.CV;;nerf
2203.04130v1;http://arxiv.org/abs/2203.04130v1;2022-03-08;NeReF: Neural Refractive Field for Fluid Surface Reconstruction and  Implicit Representation;"Existing neural reconstruction schemes such as Neural Radiance Field (NeRF)
are largely focused on modeling opaque objects. We present a novel neural
refractive field(NeReF) to recover wavefront of transparent fluids by
simultaneously estimating the surface position and normal of the fluid front.
Unlike prior arts that treat the reconstruction target as a single layer of the
surface, NeReF is specifically formulated to recover a volumetric normal field
with its corresponding density field. A query ray will be refracted by NeReF
according to its accumulated refractive point and normal, and we employ the
correspondences and uniqueness of refracted ray for NeReF optimization. We show
NeReF, as a global optimization scheme, can more robustly tackle refraction
distortions detrimental to traditional methods for correspondence matching.
Furthermore, the continuous NeReF representation of wavefront enables view
synthesis as well as normal integration. We validate our approach on both
synthetic and real data and show it is particularly suitable for sparse
multi-view acquisition. We hence build a small light field array and experiment
on various surface shapes to demonstrate high fidelity NeReF reconstruction.";Ziyu Wang<author:sep>Wei Yang<author:sep>Junming Cao<author:sep>Lan Xu<author:sep>Junqing Yu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2203.04130v1;cs.CV;;nerf
2203.03570v1;http://arxiv.org/abs/2203.03570v1;2022-03-07;Kubric: A scalable dataset generator;"Data is the driving force of machine learning, with the amount and quality of
training data often being more important for the performance of a system than
architecture and training details. But collecting, processing and annotating
real data at scale is difficult, expensive, and frequently raises additional
privacy, fairness and legal concerns. Synthetic data is a powerful tool with
the potential to address these shortcomings: 1) it is cheap 2) supports rich
ground-truth annotations 3) offers full control over data and 4) can circumvent
or mitigate problems regarding bias, privacy and licensing. Unfortunately,
software tools for effective data generation are less mature than those for
architecture design and training, which leads to fragmented generation efforts.
To address these problems we introduce Kubric, an open-source Python framework
that interfaces with PyBullet and Blender to generate photo-realistic scenes,
with rich annotations, and seamlessly scales to large jobs distributed over
thousands of machines, and generating TBs of data. We demonstrate the
effectiveness of Kubric by presenting a series of 13 different generated
datasets for tasks ranging from studying 3D NeRF models to optical flow
estimation. We release Kubric, the used assets, all of the generation code, as
well as the rendered datasets for reuse and modification.";Klaus Greff<author:sep>Francois Belletti<author:sep>Lucas Beyer<author:sep>Carl Doersch<author:sep>Yilun Du<author:sep>Daniel Duckworth<author:sep>David J. Fleet<author:sep>Dan Gnanapragasam<author:sep>Florian Golemo<author:sep>Charles Herrmann<author:sep>Thomas Kipf<author:sep>Abhijit Kundu<author:sep>Dmitry Lagun<author:sep>Issam Laradji<author:sep> Hsueh-Ti<author:sep> Liu<author:sep>Henning Meyer<author:sep>Yishu Miao<author:sep>Derek Nowrouzezahrai<author:sep>Cengiz Oztireli<author:sep>Etienne Pot<author:sep>Noha Radwan<author:sep>Daniel Rebain<author:sep>Sara Sabour<author:sep>Mehdi S. M. Sajjadi<author:sep>Matan Sela<author:sep>Vincent Sitzmann<author:sep>Austin Stone<author:sep>Deqing Sun<author:sep>Suhani Vora<author:sep>Ziyu Wang<author:sep>Tianhao Wu<author:sep>Kwang Moo Yi<author:sep>Fangcheng Zhong<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2203.03570v1;cs.CV;21 pages, CVPR2022;nerf
2203.01914v2;http://arxiv.org/abs/2203.01914v2;2022-03-03;Playable Environments: Video Manipulation in Space and Time;"We present Playable Environments - a new representation for interactive video
generation and manipulation in space and time. With a single image at inference
time, our novel framework allows the user to move objects in 3D while
generating a video by providing a sequence of desired actions. The actions are
learnt in an unsupervised manner. The camera can be controlled to get the
desired viewpoint. Our method builds an environment state for each frame, which
can be manipulated by our proposed action module and decoded back to the image
space with volumetric rendering. To support diverse appearances of objects, we
extend neural radiance fields with style-based modulation. Our method trains on
a collection of various monocular videos requiring only the estimated camera
parameters and 2D object locations. To set a challenging benchmark, we
introduce two large scale video datasets with significant camera movements. As
evidenced by our experiments, playable environments enable several creative
applications not attainable by prior video synthesis works, including playable
3D video generation, stylization and manipulation. Further details, code and
examples are available at
https://willi-menapace.github.io/playable-environments-website";Willi Menapace<author:sep>Stéphane Lathuilière<author:sep>Aliaksandr Siarohin<author:sep>Christian Theobalt<author:sep>Sergey Tulyakov<author:sep>Vladislav Golyanik<author:sep>Elisa Ricci;http://arxiv.org/pdf/2203.01914v2;cs.CV;CVPR 2022;
2203.01913v2;http://arxiv.org/abs/2203.01913v2;2022-03-03;NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance  Fields;"Thin, reflective objects such as forks and whisks are common in our daily
lives, but they are particularly challenging for robot perception because it is
hard to reconstruct them using commodity RGB-D cameras or multi-view stereo
techniques. While traditional pipelines struggle with objects like these,
Neural Radiance Fields (NeRFs) have recently been shown to be remarkably
effective for performing view synthesis on objects with thin structures or
reflective materials. In this paper we explore the use of NeRF as a new source
of supervision for robust robot vision systems. In particular, we demonstrate
that a NeRF representation of a scene can be used to train dense object
descriptors. We use an optimized NeRF to extract dense correspondences between
multiple views of an object, and then use these correspondences as training
data for learning a view-invariant representation of the object. NeRF's usage
of a density field allows us to reformulate the correspondence problem with a
novel distribution-of-depths formulation, as opposed to the conventional
approach of using a depth map. Dense correspondence models supervised with our
method significantly outperform off-the-shelf learned descriptors by 106%
(PCK@3px metric, more than doubling performance) and outperform our baseline
supervised with multi-view stereo by 29%. Furthermore, we demonstrate the
learned dense descriptors enable robots to perform accurate 6-degree of freedom
(6-DoF) pick and place of thin and reflective objects.";Lin Yen-Chen<author:sep>Pete Florence<author:sep>Jonathan T. Barron<author:sep>Tsung-Yi Lin<author:sep>Alberto Rodriguez<author:sep>Phillip Isola;http://arxiv.org/pdf/2203.01913v2;cs.RO;ICRA 2022, Website: https://yenchenlin.me/nerf-supervision/;nerf
2203.01762v2;http://arxiv.org/abs/2203.01762v2;2022-03-03;NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural  Radiance Fields;"Deep learning has shown great potential for modeling the physical dynamics of
complex particle systems such as fluids. Existing approaches, however, require
the supervision of consecutive particle properties, including positions and
velocities. In this paper, we consider a partially observable scenario known as
fluid dynamics grounding, that is, inferring the state transitions and
interactions within the fluid particle systems from sequential visual
observations of the fluid surface. We propose a differentiable two-stage
network named NeuroFluid. Our approach consists of (i) a particle-driven neural
renderer, which involves fluid physical properties into the volume rendering
function, and (ii) a particle transition model optimized to reduce the
differences between the rendered and the observed images. NeuroFluid provides
the first solution to unsupervised learning of particle-based fluid dynamics by
training these two models jointly. It is shown to reasonably estimate the
underlying physics of fluids with different initial shapes, viscosity, and
densities.";Shanyan Guan<author:sep>Huayu Deng<author:sep>Yunbo Wang<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2203.01762v2;cs.LG;ICML 2022, the project page: https://syguan96.github.io/NeuroFluid/;
2203.01414v3;http://arxiv.org/abs/2203.01414v3;2022-03-01;ICARUS: A Specialized Architecture for Neural Radiance Fields Rendering;"The practical deployment of Neural Radiance Fields (NeRF) in rendering
applications faces several challenges, with the most critical one being low
rendering speed on even high-end graphic processing units (GPUs). In this
paper, we present ICARUS, a specialized accelerator architecture tailored for
NeRF rendering. Unlike GPUs using general purpose computing and memory
architectures for NeRF, ICARUS executes the complete NeRF pipeline using
dedicated plenoptic cores (PLCore) consisting of a positional encoding unit
(PEU), a multi-layer perceptron (MLP) engine, and a volume rendering unit
(VRU). A PLCore takes in positions \& directions and renders the corresponding
pixel colors without any intermediate data going off-chip for temporary storage
and exchange, which can be time and power consuming. To implement the most
expensive component of NeRF, i.e., the MLP, we transform the fully connected
operations to approximated reconfigurable multiple constant multiplications
(MCMs), where common subexpressions are shared across different multiplications
to improve the computation efficiency. We build a prototype ICARUS using
Synopsys HAPS-80 S104, a field programmable gate array (FPGA)-based prototyping
system for large-scale integrated circuits and systems design. We evaluate the
power-performance-area (PPA) of a PLCore using 40nm LP CMOS technology. Working
at 400 MHz, a single PLCore occupies 16.5 $mm^2$ and consumes 282.8 mW,
translating to 0.105 uJ/sample. The results are compared with those of GPU and
tensor processing unit (TPU) implementations.";Chaolin Rao<author:sep>Huangjie Yu<author:sep>Haochuan Wan<author:sep>Jindong Zhou<author:sep>Yueyang Zheng<author:sep>Yu Ma<author:sep>Anpei Chen<author:sep>Minye Wu<author:sep>Binzhe Yuan<author:sep>Pingqiang Zhou<author:sep>Xin Lou<author:sep>Jingyi Yu;http://arxiv.org/pdf/2203.01414v3;cs.AR;;nerf
2202.13162v1;http://arxiv.org/abs/2202.13162v1;2022-02-26;Pix2NeRF: Unsupervised Conditional $π$-GAN for Single Image to Neural  Radiance Fields Translation;"We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object
or a scene of a specific class, conditioned on a single input image. This is a
challenging task, as training NeRF requires multiple views of the same scene,
coupled with corresponding poses, which are hard to obtain. Our method is based
on $\pi$-GAN, a generative model for unconditional 3D-aware image synthesis,
which maps random latent codes to radiance fields of a class of objects. We
jointly optimize (1) the $\pi$-GAN objective to utilize its high-fidelity
3D-aware generation and (2) a carefully designed reconstruction objective. The
latter includes an encoder coupled with $\pi$-GAN generator to form an
auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is
unsupervised, capable of being trained with independent images without 3D,
multi-view, or pose supervision. Applications of our pipeline include 3d avatar
generation, object-centric novel view synthesis with a single input image, and
3d-aware super-resolution, to name a few.";Shengqu Cai<author:sep>Anton Obukhov<author:sep>Dengxin Dai<author:sep>Luc Van Gool;http://arxiv.org/pdf/2202.13162v1;cs.CV;16 pages, 10 figures;nerf
2202.11855v3;http://arxiv.org/abs/2202.11855v3;2022-02-24;Learning Multi-Object Dynamics with Compositional Neural Radiance Fields;"We present a method to learn compositional multi-object dynamics models from
image observations based on implicit object encoders, Neural Radiance Fields
(NeRFs), and graph neural networks. NeRFs have become a popular choice for
representing scenes due to their strong 3D prior. However, most NeRF approaches
are trained on a single scene, representing the whole scene with a global
model, making generalization to novel scenes, containing different numbers of
objects, challenging. Instead, we present a compositional, object-centric
auto-encoder framework that maps multiple views of the scene to a set of latent
vectors representing each object separately. The latent vectors parameterize
individual NeRFs from which the scene can be reconstructed. Based on those
latent vectors, we train a graph neural network dynamics model in the latent
space to achieve compositionality for dynamics prediction. A key feature of our
approach is that the latent vectors are forced to encode 3D information through
the NeRF decoder, which enables us to incorporate structural priors in learning
the dynamics models, making long-term predictions more stable compared to
several baselines. Simulated and real world experiments show that our method
can model and learn the dynamics of compositional scenes including rigid and
deformable objects. Video: https://dannydriess.github.io/compnerfdyn/";Danny Driess<author:sep>Zhiao Huang<author:sep>Yunzhu Li<author:sep>Russ Tedrake<author:sep>Marc Toussaint;http://arxiv.org/pdf/2202.11855v3;cs.CV;v3: real robot exp;nerf
2202.08614v2;http://arxiv.org/abs/2202.08614v2;2022-02-17;Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time;"Implicit neural representations such as Neural Radiance Field (NeRF) have
focused mainly on modeling static objects captured under multi-view settings
where real-time rendering can be achieved with smart data structures, e.g.,
PlenOctree. In this paper, we present a novel Fourier PlenOctree (FPO)
technique to tackle efficient neural modeling and real-time rendering of
dynamic scenes captured under the free-view video (FVV) setting. The key idea
in our FPO is a novel combination of generalized NeRF, PlenOctree
representation, volumetric fusion and Fourier transform. To accelerate FPO
construction, we present a novel coarse-to-fine fusion scheme that leverages
the generalizable NeRF technique to generate the tree via spatial blending. To
tackle dynamic scenes, we tailor the implicit network to model the Fourier
coefficients of timevarying density and color attributes. Finally, we construct
the FPO and train the Fourier coefficients directly on the leaves of a union
PlenOctree structure of the dynamic sequence. We show that the resulting FPO
enables compact memory overload to handle dynamic objects and supports
efficient fine-tuning. Extensive experiments show that the proposed method is
3000 times faster than the original NeRF and achieves over an order of
magnitude acceleration over SOTA while preserving high visual quality for the
free-viewpoint rendering of unseen dynamic scenes.";Liao Wang<author:sep>Jiakai Zhang<author:sep>Xinhang Liu<author:sep>Fuqiang Zhao<author:sep>Yanshun Zhang<author:sep>Yingliang Zhang<author:sep>Minye Wu<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2202.08614v2;cs.CV;Project page: https://aoliao12138.github.io/FPO/;nerf
2202.06088v1;http://arxiv.org/abs/2202.06088v1;2022-02-12;NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing;"Some of the most exciting experiences that Metaverse promises to offer, for
instance, live interactions with virtual characters in virtual environments,
require real-time photo-realistic rendering. 3D reconstruction approaches to
rendering, active or passive, still require extensive cleanup work to fix the
meshes or point clouds. In this paper, we present a neural volumography
technique called neural volumetric video or NeuVV to support immersive,
interactive, and spatial-temporal rendering of volumetric video contents with
photo-realism and in real-time. The core of NeuVV is to efficiently encode a
dynamic neural radiance field (NeRF) into renderable and editable primitives.
We introduce two types of factorization schemes: a hyper-spherical harmonics
(HH) decomposition for modeling smooth color variations over space and time and
a learnable basis representation for modeling abrupt density and color changes
caused by motion. NeuVV factorization can be integrated into a Video Octree
(VOctree) analogous to PlenOctree to significantly accelerate training while
reducing memory overhead. Real-time NeuVV rendering further enables a class of
immersive content editing tools. Specifically, NeuVV treats each VOctree as a
primitive and implements volume-based depth ordering and alpha blending to
realize spatial-temporal compositions for content re-purposing. For example, we
demonstrate positioning varied manifestations of the same performance at
different 3D locations with different timing, adjusting color/texture of the
performer's clothing, casting spotlight shadows and synthesizing distance
falloff lighting, etc, all at an interactive speed. We further develop a hybrid
neural-rasterization rendering framework to support consumer-level VR headsets
so that the aforementioned volumetric video viewing and editing, for the first
time, can be conducted immersively in virtual 3D space.";Jiakai Zhang<author:sep>Liao Wang<author:sep>Xinhang Liu<author:sep>Fuqiang Zhao<author:sep>Minzhang Li<author:sep>Haizhao Dai<author:sep>Boyuan Zhang<author:sep>Wei Yang<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2202.06088v1;cs.CV;;nerf
2202.04879v1;http://arxiv.org/abs/2202.04879v1;2022-02-10;PVSeRF: Joint Pixel-, Voxel- and Surface-Aligned Radiance Field for  Single-Image Novel View Synthesis;"We present PVSeRF, a learning framework that reconstructs neural radiance
fields from single-view RGB images, for novel view synthesis. Previous
solutions, such as pixelNeRF, rely only on pixel-aligned features and suffer
from feature ambiguity issues. As a result, they struggle with the
disentanglement of geometry and appearance, leading to implausible geometries
and blurry results. To address this challenge, we propose to incorporate
explicit geometry reasoning and combine it with pixel-aligned features for
radiance field prediction. Specifically, in addition to pixel-aligned features,
we further constrain the radiance field learning to be conditioned on i)
voxel-aligned features learned from a coarse volumetric grid and ii) fine
surface-aligned features extracted from a regressed point cloud. We show that
the introduction of such geometry-aware features helps to achieve a better
disentanglement between appearance and geometry, i.e. recovering more accurate
geometries and synthesizing higher quality images of novel views. Extensive
experiments against state-of-the-art methods on ShapeNet benchmarks demonstrate
the superiority of our approach for single-image novel view synthesis.";Xianggang Yu<author:sep>Jiapeng Tang<author:sep>Yipeng Qin<author:sep>Chenghong Li<author:sep>Linchao Bao<author:sep>Xiaoguang Han<author:sep>Shuguang Cui;http://arxiv.org/pdf/2202.04879v1;cs.CV;;nerf
2202.05263v1;http://arxiv.org/abs/2202.05263v1;2022-02-10;Block-NeRF: Scalable Large Scene Neural View Synthesis;"We present Block-NeRF, a variant of Neural Radiance Fields that can represent
large-scale environments. Specifically, we demonstrate that when scaling NeRF
to render city-scale scenes spanning multiple blocks, it is vital to decompose
the scene into individually trained NeRFs. This decomposition decouples
rendering time from scene size, enables rendering to scale to arbitrarily large
environments, and allows per-block updates of the environment. We adopt several
architectural changes to make NeRF robust to data captured over months under
different environmental conditions. We add appearance embeddings, learned pose
refinement, and controllable exposure to each individual NeRF, and introduce a
procedure for aligning appearance between adjacent NeRFs so that they can be
seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to
create the largest neural scene representation to date, capable of rendering an
entire neighborhood of San Francisco.";Matthew Tancik<author:sep>Vincent Casser<author:sep>Xinchen Yan<author:sep>Sabeek Pradhan<author:sep>Ben Mildenhall<author:sep>Pratul P. Srinivasan<author:sep>Jonathan T. Barron<author:sep>Henrik Kretzschmar;http://arxiv.org/pdf/2202.05263v1;cs.CV;Project page: https://waymo.com/research/block-nerf/;nerf
2202.01020v3;http://arxiv.org/abs/2202.01020v3;2022-02-02;MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware  CT-Projections from a Single X-ray;"Computed tomography (CT) is an effective medical imaging modality, widely
used in the field of clinical medicine for the diagnosis of various
pathologies. Advances in Multidetector CT imaging technology have enabled
additional functionalities, including generation of thin slice multiplanar
cross-sectional body imaging and 3D reconstructions. However, this involves
patients being exposed to a considerable dose of ionising radiation. Excessive
ionising radiation can lead to deterministic and harmful effects on the body.
This paper proposes a Deep Learning model that learns to reconstruct CT
projections from a few or even a single-view X-ray. This is based on a novel
architecture that builds from neural radiance fields, which learns a continuous
representation of CT scans by disentangling the shape and volumetric depth of
surface and internal anatomical structures from 2D images. Our model is trained
on chest and knee datasets, and we demonstrate qualitative and quantitative
high-fidelity renderings and compare our approach to other recent radiance
field-based methods. Our code and link to our datasets are available at
https://github.com/abrilcf/mednerf";Abril Corona-Figueroa<author:sep>Jonathan Frawley<author:sep>Sam Bond-Taylor<author:sep>Sarath Bethapudi<author:sep>Hubert P. H. Shum<author:sep>Chris G. Willcocks;http://arxiv.org/pdf/2202.01020v3;eess.IV;6 pages, 4 figures, accepted at IEEE EMBC 2022;nerf
2202.00181v3;http://arxiv.org/abs/2202.00181v3;2022-02-01;CLA-NeRF: Category-Level Articulated Neural Radiance Field;"We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field
that can perform view synthesis, part segmentation, and articulated pose
estimation. CLA-NeRF is trained at the object category level using no CAD
models and no depth, but a set of RGB images with ground truth camera poses and
part segments. During inference, it only takes a few RGB views (i.e., few-shot)
of an unseen 3D object instance within the known category to infer the object
part segmentation and the neural radiance field. Given an articulated pose as
input, CLA-NeRF can perform articulation-aware volume rendering to generate the
corresponding RGB image at any camera pose. Moreover, the articulated pose of
an object can be estimated via inverse rendering. In our experiments, we
evaluate the framework across five categories on both synthetic and real-world
data. In all cases, our method shows realistic deformation results and accurate
articulated pose estimation. We believe that both few-shot articulated object
rendering and articulated pose estimation open doors for robots to perceive and
interact with unseen articulated objects.";Wei-Cheng Tseng<author:sep>Hung-Ju Liao<author:sep>Lin Yen-Chen<author:sep>Min Sun;http://arxiv.org/pdf/2202.00181v3;cs.CV;accepted by ICRA 2022;nerf
2201.12204v3;http://arxiv.org/abs/2201.12204v3;2022-01-28;From data to functa: Your data point is a function and you can treat it  like one;"It is common practice in deep learning to represent a measurement of the
world on a discrete grid, e.g. a 2D grid of pixels. However, the underlying
signal represented by these measurements is often continuous, e.g. the scene
depicted in an image. A powerful continuous alternative is then to represent
these measurements using an implicit neural representation, a neural function
trained to output the appropriate measurement value for any input spatial
location. In this paper, we take this idea to its next level: what would it
take to perform deep learning on these functions instead, treating them as
data? In this context we refer to the data as functa, and propose a framework
for deep learning on functa. This view presents a number of challenges around
efficient conversion from data to functa, compact representation of functa, and
effectively solving downstream tasks on functa. We outline a recipe to overcome
these challenges and apply it to a wide range of data modalities including
images, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We
demonstrate that this approach has various compelling properties across data
modalities, in particular on the canonical tasks of generative modeling, data
imputation, novel view synthesis and classification. Code:
https://github.com/deepmind/functa";Emilien Dupont<author:sep>Hyunjik Kim<author:sep>S. M. Ali Eslami<author:sep>Danilo Rezende<author:sep>Dan Rosenbaum;http://arxiv.org/pdf/2201.12204v3;cs.LG;;nerf
2201.08845v7;http://arxiv.org/abs/2201.08845v7;2022-01-21;Point-NeRF: Point-based Neural Radiance Fields;"Volumetric neural rendering methods like NeRF generate high-quality view
synthesis results but are optimized per-scene leading to prohibitive
reconstruction time. On the other hand, deep multi-view stereo methods can
quickly reconstruct scene geometry via direct network inference. Point-NeRF
combines the advantages of these two approaches by using neural 3D point
clouds, with associated neural features, to model a radiance field. Point-NeRF
can be rendered efficiently by aggregating neural point features near scene
surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can
be initialized via direct inference of a pre-trained deep network to produce a
neural point cloud; this point cloud can be finetuned to surpass the visual
quality of NeRF with 30X faster training time. Point-NeRF can be combined with
other 3D reconstruction methods and handles the errors and outliers in such
methods via a novel pruning and growing mechanism. The experiments on the DTU,
the NeRF Synthetics , the ScanNet and the Tanks and Temples datasets
demonstrate Point-NeRF can surpass the existing methods and achieve the
state-of-the-art results.";Qiangeng Xu<author:sep>Zexiang Xu<author:sep>Julien Philip<author:sep>Sai Bi<author:sep>Zhixin Shu<author:sep>Kalyan Sunkavalli<author:sep>Ulrich Neumann;http://arxiv.org/pdf/2201.08845v7;cs.CV;Accepted to CVPR 2022 (Oral);nerf
2201.07786v1;http://arxiv.org/abs/2201.07786v1;2022-01-19;Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation;"Animating high-fidelity video portrait with speech audio is crucial for
virtual reality and digital entertainment. While most previous studies rely on
accurate explicit structural information, recent works explore the implicit
scene representation of Neural Radiance Fields (NeRF) for realistic generation.
In order to capture the inconsistent motions as well as the semantic difference
between human head and torso, some work models them via two individual sets of
NeRF, leading to unnatural results. In this work, we propose Semantic-aware
Speaking Portrait NeRF (SSP-NeRF), which creates delicate audio-driven
portraits using one unified set of NeRF. The proposed model can handle the
detailed local facial semantics and the global head-torso relationship through
two semantic-aware modules. Specifically, we first propose a Semantic-Aware
Dynamic Ray Sampling module with an additional parsing branch that facilitates
audio-driven volume rendering. Moreover, to enable portrait rendering in one
unified neural radiance field, a Torso Deformation module is designed to
stabilize the large-scale non-rigid torso motions. Extensive evaluations
demonstrate that our proposed approach renders more realistic video portraits
compared to previous methods. Project page:
https://alvinliu0.github.io/projects/SSP-NeRF";Xian Liu<author:sep>Yinghao Xu<author:sep>Qianyi Wu<author:sep>Hang Zhou<author:sep>Wayne Wu<author:sep>Bolei Zhou;http://arxiv.org/pdf/2201.07786v1;cs.CV;"12 pages, 3 figures. Project page:
  https://alvinliu0.github.io/projects/SSP-NeRF";nerf
2201.04623v1;http://arxiv.org/abs/2201.04623v1;2022-01-12;Virtual Elastic Objects;"We present Virtual Elastic Objects (VEOs): virtual objects that not only look
like their real-world counterparts but also behave like them, even when subject
to novel interactions. Achieving this presents multiple challenges: not only do
objects have to be captured including the physical forces acting on them, then
faithfully reconstructed and rendered, but also plausible material parameters
found and simulated. To create VEOs, we built a multi-view capture system that
captures objects under the influence of a compressed air stream. Building on
recent advances in model-free, dynamic Neural Radiance Fields, we reconstruct
the objects and corresponding deformation fields. We propose to use a
differentiable, particle-based simulator to use these deformation fields to
find representative material parameters, which enable us to run new
simulations. To render simulated objects, we devise a method for integrating
the simulation results with Neural Radiance Fields. The resulting method is
applicable to a wide range of scenarios: it can handle objects composed of
inhomogeneous material, with very different shapes, and it can simulate
interactions with other virtual objects. We present our results using a newly
collected dataset of 12 objects under a variety of force fields, which will be
shared with the community.";Hsiao-yu Chen<author:sep>Edgar Tretschk<author:sep>Tuur Stuyck<author:sep>Petr Kadlecek<author:sep>Ladislav Kavan<author:sep>Etienne Vouga<author:sep>Christoph Lassner;http://arxiv.org/pdf/2201.04623v1;cs.CV;;
2201.02533v2;http://arxiv.org/abs/2201.02533v2;2022-01-07;NeROIC: Neural Rendering of Objects from Online Image Collections;"We present a novel method to acquire object representations from online image
collections, capturing high-quality geometry and material properties of
arbitrary objects from photographs with varying cameras, illumination, and
backgrounds. This enables various object-centric rendering applications such as
novel-view synthesis, relighting, and harmonized background composition from
challenging in-the-wild input. Using a multi-stage approach extending neural
radiance fields, we first infer the surface geometry and refine the coarsely
estimated initial camera parameters, while leveraging coarse foreground object
masks to improve the training efficiency and geometry quality. We also
introduce a robust normal estimation technique which eliminates the effect of
geometric noise while retaining crucial details. Lastly, we extract surface
material properties and ambient illumination, represented in spherical
harmonics with extensions that handle transient elements, e.g. sharp shadows.
The union of these components results in a highly modular and efficient object
acquisition framework. Extensive evaluations and comparisons demonstrate the
advantages of our approach in capturing high-quality geometry and appearance
properties useful for rendering applications.";Zhengfei Kuang<author:sep>Kyle Olszewski<author:sep>Menglei Chai<author:sep>Zeng Huang<author:sep>Panos Achlioptas<author:sep>Sergey Tulyakov;http://arxiv.org/pdf/2201.02533v2;cs.CV;"SIGGRAPH 2022 (Journal Track). Project page:
  https://formyfamily.github.io/NeROIC/ Code repository:
  https://github.com/snap-research/NeROIC/";
2201.01683v2;http://arxiv.org/abs/2201.01683v2;2022-01-05;Surface-Aligned Neural Radiance Fields for Controllable 3D Human  Synthesis;"We propose a new method for reconstructing controllable implicit 3D human
models from sparse multi-view RGB videos. Our method defines the neural scene
representation on the mesh surface points and signed distances from the surface
of a human body mesh. We identify an indistinguishability issue that arises
when a point in 3D space is mapped to its nearest surface point on a mesh for
learning surface-aligned neural scene representation. To address this issue, we
propose projecting a point onto a mesh surface using a barycentric
interpolation with modified vertex normals. Experiments with the ZJU-MoCap and
Human3.6M datasets show that our approach achieves a higher quality in a
novel-view and novel-pose synthesis than existing methods. We also demonstrate
that our method easily supports the control of body shape and clothes. Project
page: https://pfnet-research.github.io/surface-aligned-nerf/.";Tianhan Xu<author:sep>Yasuhiro Fujita<author:sep>Eiichi Matsumoto;http://arxiv.org/pdf/2201.01683v2;cs.CV;"CVPR 2022. Project page:
  https://pfnet-research.github.io/surface-aligned-nerf/";nerf
2201.00791v1;http://arxiv.org/abs/2201.00791v1;2022-01-03;DFA-NeRF: Personalized Talking Head Generation via Disentangled Face  Attributes Neural Rendering;"While recent advances in deep neural networks have made it possible to render
high-quality images, generating photo-realistic and personalized talking head
remains challenging. With given audio, the key to tackling this task is
synchronizing lip movement and simultaneously generating personalized
attributes like head movement and eye blink. In this work, we observe that the
input audio is highly correlated to lip motion while less correlated to other
personalized attributes (e.g., head movements). Inspired by this, we propose a
novel framework based on neural radiance field to pursue high-fidelity and
personalized talking head generation. Specifically, neural radiance field takes
lip movements features and personalized attributes as two disentangled
conditions, where lip movements are directly predicted from the audio inputs to
achieve lip-synchronized generation. In the meanwhile, personalized attributes
are sampled from a probabilistic model, where we design a Transformer-based
variational autoencoder sampled from Gaussian Process to learn plausible and
natural-looking head pose and eye blink. Experiments on several benchmarks
demonstrate that our method achieves significantly better results than
state-of-the-art methods.";Shunyu Yao<author:sep>RuiZhe Zhong<author:sep>Yichao Yan<author:sep>Guangtao Zhai<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2201.00791v1;cs.CV;;nerf
2112.15399v2;http://arxiv.org/abs/2112.15399v2;2021-12-31;InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering;"We present an information-theoretic regularization technique for few-shot
novel view synthesis based on neural implicit representation. The proposed
approach minimizes potential reconstruction inconsistency that happens due to
insufficient viewpoints by imposing the entropy constraint of the density in
each ray. In addition, to alleviate the potential degenerate issue when all
training images are acquired from almost redundant viewpoints, we further
incorporate the spatially smoothness constraint into the estimated images by
restricting information gains from a pair of rays with slightly different
viewpoints. The main idea of our algorithm is to make reconstructed scenes
compact along individual rays and consistent across rays in the neighborhood.
The proposed regularizers can be plugged into most of existing neural volume
rendering techniques based on NeRF in a straightforward way. Despite its
simplicity, we achieve consistently improved performance compared to existing
neural view synthesis methods by large margins on multiple standard benchmarks.";Mijeong Kim<author:sep>Seonguk Seo<author:sep>Bohyung Han;http://arxiv.org/pdf/2112.15399v2;cs.CV;CVPR 2022, Website: http://cv.snu.ac.kr/research/InfoNeRF;nerf
2112.12390v2;http://arxiv.org/abs/2112.12390v2;2021-12-23;Learning Implicit Body Representations from Double Diffusion Based  Neural Radiance Fields;"In this paper, we present a novel double diffusion based neural radiance
field, dubbed DD-NeRF, to reconstruct human body geometry and render the human
body appearance in novel views from a sparse set of images. We first propose a
double diffusion mechanism to achieve expressive representations of input
images by fully exploiting human body priors and image appearance details at
two levels. At the coarse level, we first model the coarse human body poses and
shapes via an unclothed 3D deformable vertex model as guidance. At the fine
level, we present a multi-view sampling network to capture subtle geometric
deformations and image detailed appearances, such as clothing and hair, from
multiple input views. Considering the sparsity of the two level features, we
diffuse them into feature volumes in the canonical space to construct neural
radiance fields. Then, we present a signed distance function (SDF) regression
network to construct body surfaces from the diffused features. Thanks to our
double diffused representations, our method can even synthesize novel views of
unseen subjects. Experiments on various datasets demonstrate that our approach
outperforms the state-of-the-art in both geometric reconstruction and novel
view synthesis.";Guangming Yao<author:sep>Hongzhi Wu<author:sep>Yi Yuan<author:sep>Lincheng Li<author:sep>Kun Zhou<author:sep>Xin Yu;http://arxiv.org/pdf/2112.12390v2;cs.CV;6 pages, 5 figures;nerf
2112.12761v3;http://arxiv.org/abs/2112.12761v3;2021-12-23;BANMo: Building Animatable 3D Neural Models from Many Casual Videos;"Prior work for articulated 3D shape reconstruction often relies on
specialized sensors (e.g., synchronized multi-camera systems), or pre-built 3D
deformable models (e.g., SMAL or SMPL). Such methods are not able to scale to
diverse sets of objects in the wild. We present BANMo, a method that requires
neither a specialized sensor nor a pre-defined template shape. BANMo builds
high-fidelity, articulated 3D models (including shape and animatable skinning
weights) from many monocular casual videos in a differentiable rendering
framework. While the use of many videos provides more coverage of camera views
and object articulations, they introduce significant challenges in establishing
correspondence across scenes with different backgrounds, illumination
conditions, etc. Our key insight is to merge three schools of thought; (1)
classic deformable shape models that make use of articulated bones and blend
skinning, (2) volumetric neural radiance fields (NeRFs) that are amenable to
gradient-based optimization, and (3) canonical embeddings that generate
correspondences between pixels and an articulated model. We introduce neural
blend skinning models that allow for differentiable and invertible articulated
deformations. When combined with canonical embeddings, such models allow us to
establish dense correspondences across videos that can be self-supervised with
cycle consistency. On real and synthetic datasets, BANMo shows higher-fidelity
3D reconstructions than prior works for humans and animals, with the ability to
render realistic images from novel viewpoints and poses. Project webpage:
banmo-www.github.io .";Gengshan Yang<author:sep>Minh Vo<author:sep>Natalia Neverova<author:sep>Deva Ramanan<author:sep>Andrea Vedaldi<author:sep>Hanbyul Joo;http://arxiv.org/pdf/2112.12761v3;cs.CV;CVPR 2022 camera-ready version (last update: May 2022);nerf
2112.10759v2;http://arxiv.org/abs/2112.10759v2;2021-12-20;3D-aware Image Synthesis via Learning Structural and Textural  Representations;"Making generative models 3D-aware bridges the 2D image space and the 3D
physical world yet remains challenging. Recent attempts equip a Generative
Adversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D
coordinates to pixel values, as a 3D prior. However, the implicit function in
NeRF has a very local receptive field, making the generator hard to become
aware of the global structure. Meanwhile, NeRF is built on volume rendering
which can be too costly to produce high-resolution results, increasing the
optimization difficulty. To alleviate these two problems, we propose a novel
framework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis,
through explicitly learning a structural representation and a textural
representation. We first learn a feature volume to represent the underlying
structure, which is then converted to a feature field using a NeRF-like model.
The feature field is further accumulated into a 2D feature map as the textural
representation, followed by a neural renderer for appearance synthesis. Such a
design enables independent control of the shape and the appearance. Extensive
experiments on a wide range of datasets show that our approach achieves
sufficiently higher image quality and better 3D control than the previous
methods.";Yinghao Xu<author:sep>Sida Peng<author:sep>Ceyuan Yang<author:sep>Yujun Shen<author:sep>Bolei Zhou;http://arxiv.org/pdf/2112.10759v2;cs.CV;"CVPR 2022 camera-ready, Project page:
  https://genforce.github.io/volumegan/";nerf
2112.10703v2;http://arxiv.org/abs/2112.10703v2;2021-12-20;Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual  Fly-Throughs;"We use neural radiance fields (NeRFs) to build interactive 3D environments
from large-scale visual captures spanning buildings or even multiple city
blocks collected primarily from drones. In contrast to single object scenes (on
which NeRFs are traditionally evaluated), our scale poses multiple challenges
including (1) the need to model thousands of images with varying lighting
conditions, each of which capture only a small subset of the scene, (2)
prohibitively large model capacities that make it infeasible to train on a
single GPU, and (3) significant challenges for fast rendering that would enable
interactive fly-throughs.
  To address these challenges, we begin by analyzing visibility statistics for
large-scale scenes, motivating a sparse network structure where parameters are
specialized to different regions of the scene. We introduce a simple geometric
clustering algorithm for data parallelism that partitions training images (or
rather pixels) into different NeRF submodules that can be trained in parallel.
  We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as
well as against our own drone footage, improving training speed by 3x and PSNR
by 12%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and
introduce a novel method that exploits temporal coherence. Our technique
achieves a 40x speedup over conventional NeRF rendering while remaining within
0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.";Haithem Turki<author:sep>Deva Ramanan<author:sep>Mahadev Satyanarayanan;http://arxiv.org/pdf/2112.10703v2;cs.CV;"CVPR 2022 Project page: https://meganerf.cmusatyalab.org GitHub:
  https://github.com/cmusatyalab/mega-nerf";nerf
2112.10203v2;http://arxiv.org/abs/2112.10203v2;2021-12-19;HVTR: Hybrid Volumetric-Textural Rendering for Human Avatars;"We propose a novel neural rendering pipeline, Hybrid Volumetric-Textural
Rendering (HVTR), which synthesizes virtual human avatars from arbitrary poses
efficiently and at high quality. First, we learn to encode articulated human
motions on a dense UV manifold of the human body surface. To handle complicated
motions (e.g., self-occlusions), we then leverage the encoded information on
the UV manifold to construct a 3D volumetric representation based on a dynamic
pose-conditioned neural radiance field. While this allows us to represent 3D
geometry with changing topology, volumetric rendering is computationally heavy.
Hence we employ only a rough volumetric representation using a pose-conditioned
downsampled neural radiance field (PD-NeRF), which we can render efficiently at
low resolutions. In addition, we learn 2D textural features that are fused with
rendered volumetric features in image space. The key advantage of our approach
is that we can then convert the fused features into a high-resolution,
high-quality avatar by a fast GAN-based textural renderer. We demonstrate that
hybrid rendering enables HVTR to handle complicated motions, render
high-quality avatars under user-controlled poses/shapes and even loose
clothing, and most importantly, be efficient at inference time. Our
experimental results also demonstrate state-of-the-art quantitative results.";Tao Hu<author:sep>Tao Yu<author:sep>Zerong Zheng<author:sep>He Zhang<author:sep>Yebin Liu<author:sep>Matthias Zwicker;http://arxiv.org/pdf/2112.10203v2;cs.CV;"Accepted to 3DV 2022. See more results at
  https://www.cs.umd.edu/~taohu/hvtr/ Demo:
  https://www.youtube.com/watch?v=LE0-YpbLlkY";nerf
2112.09061v1;http://arxiv.org/abs/2112.09061v1;2021-12-16;Solving Inverse Problems with NerfGANs;"We introduce a novel framework for solving inverse problems using NeRF-style
generative models. We are interested in the problem of 3-D scene reconstruction
given a single 2-D image and known camera parameters. We show that naively
optimizing the latent space leads to artifacts and poor novel view rendering.
We attribute this problem to volume obstructions that are clear in the 3-D
geometry and become visible in the renderings of novel views. We propose a
novel radiance field regularization method to obtain better 3-D surfaces and
improved novel views given single view observations. Our method naturally
extends to general inverse problems including inpainting where one observes
only partially a single view. We experimentally evaluate our method, achieving
visual improvements and performance boosts over the baselines in a wide range
of tasks. Our method achieves $30-40\%$ MSE reduction and $15-25\%$ reduction
in LPIPS loss compared to the previous state of the art.";Giannis Daras<author:sep>Wen-Sheng Chu<author:sep>Abhishek Kumar<author:sep>Dmitry Lagun<author:sep>Alexandros G. Dimakis;http://arxiv.org/pdf/2112.09061v1;cs.CV;16 pages, 18 figures;nerf
2112.08867v3;http://arxiv.org/abs/2112.08867v3;2021-12-16;GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation;"3D-aware image generative modeling aims to generate 3D-consistent images with
explicitly controllable camera poses. Recent works have shown promising results
by training neural radiance field (NeRF) generators on unstructured 2D images,
but still can not generate highly-realistic images with fine details. A
critical reason is that the high memory and computation cost of volumetric
representation learning greatly restricts the number of point samples for
radiance integration during training. Deficient sampling not only limits the
expressive power of the generator to handle fine details but also impedes
effective GAN training due to the noise caused by unstable Monte Carlo
sampling. We propose a novel approach that regulates point sampling and
radiance field learning on 2D manifolds, embodied as a set of learned implicit
surfaces in the 3D volume. For each viewing ray, we calculate ray-surface
intersections and accumulate their radiance generated by the network. By
training and rendering such radiance manifolds, our generator can produce high
quality images with realistic fine details and strong visual 3D consistency.";Yu Deng<author:sep>Jiaolong Yang<author:sep>Jianfeng Xiang<author:sep>Xin Tong;http://arxiv.org/pdf/2112.08867v3;cs.CV;CVPR2022 Oral. Project page: https://yudeng.github.io/GRAM/;nerf
2112.05637v3;http://arxiv.org/abs/2112.05637v3;2021-12-10;HeadNeRF: A Real-time NeRF-based Parametric Head Model;"In this paper, we propose HeadNeRF, a novel NeRF-based parametric head model
that integrates the neural radiance field to the parametric representation of
the human head. It can render high fidelity head images in real-time on modern
GPUs, and supports directly controlling the generated images' rendering pose
and various semantic attributes. Different from existing related parametric
models, we use the neural radiance fields as a novel 3D proxy instead of the
traditional 3D textured mesh, which makes that HeadNeRF is able to generate
high fidelity images. However, the computationally expensive rendering process
of the original NeRF hinders the construction of the parametric NeRF model. To
address this issue, we adopt the strategy of integrating 2D neural rendering to
the rendering process of NeRF and design novel loss terms. As a result, the
rendering speed of HeadNeRF can be significantly accelerated, and the rendering
time of one frame is reduced from 5s to 25ms. The well designed loss terms also
improve the rendering accuracy, and the fine-level details of the human head,
such as the gaps between teeth, wrinkles, and beards, can be represented and
synthesized by HeadNeRF. Extensive experimental results and several
applications demonstrate its effectiveness. The trained parametric model is
available at https://github.com/CrisHY1995/headnerf.";Yang Hong<author:sep>Bo Peng<author:sep>Haiyao Xiao<author:sep>Ligang Liu<author:sep>Juyong Zhang;http://arxiv.org/pdf/2112.05637v3;cs.CV;"Accepted by CVPR2022. Project page:
  https://crishy1995.github.io/HeadNeRF-Project/";nerf
2112.05504v4;http://arxiv.org/abs/2112.05504v4;2021-12-10;BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale  Scene Rendering;"Neural radiance fields (NeRF) has achieved outstanding performance in
modeling 3D objects and controlled scenes, usually under a single scale. In
this work, we focus on multi-scale cases where large changes in imagery are
observed at drastically different scales. This scenario vastly exists in
real-world 3D environments, such as city scenes, with views ranging from
satellite level that captures the overview of a city, to ground level imagery
showing complex details of an architecture; and can also be commonly identified
in landscape and delicate minecraft 3D models. The wide span of viewing
positions within these scenes yields multi-scale renderings with very different
levels of detail, which poses great challenges to neural radiance field and
biases it towards compromised results. To address these issues, we introduce
BungeeNeRF, a progressive neural radiance field that achieves level-of-detail
rendering across drastically varied scales. Starting from fitting distant views
with a shallow base block, as training progresses, new blocks are appended to
accommodate the emerging details in the increasingly closer views. The strategy
progressively activates high-frequency channels in NeRF's positional encoding
inputs and successively unfolds more complex details as the training proceeds.
We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale
scenes with drastically varying views on multiple data sources (city models,
synthetic, and drone captured data) and its support for high-quality rendering
in different levels of detail.";Yuanbo Xiangli<author:sep>Linning Xu<author:sep>Xingang Pan<author:sep>Nanxuan Zhao<author:sep>Anyi Rao<author:sep>Christian Theobalt<author:sep>Bo Dai<author:sep>Dahua Lin;http://arxiv.org/pdf/2112.05504v4;cs.CV;"Accepted to ECCV22; Previous version: CityNeRF: Building NeRF at City
  Scale; Project page can be found in https://city-super.github.io/citynerf";nerf
2112.05140v2;http://arxiv.org/abs/2112.05140v2;2021-12-09;NeRF for Outdoor Scene Relighting;"Photorealistic editing of outdoor scenes from photographs requires a profound
understanding of the image formation process and an accurate estimation of the
scene geometry, reflectance and illumination. A delicate manipulation of the
lighting can then be performed while keeping the scene albedo and geometry
unaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene
relighting based on neural radiance fields. In contrast to the prior art, our
technique allows simultaneous editing of both scene illumination and camera
viewpoint using only a collection of outdoor photos shot in uncontrolled
settings. Moreover, it enables direct control over the scene illumination, as
defined through a spherical harmonics model. For evaluation, we collect a new
benchmark dataset of several outdoor sites photographed from multiple
viewpoints and at different times. For each time, a 360 degree environment map
is captured together with a colour-calibration chequerboard to allow accurate
numerical evaluations on real data against ground truth. Comparisons against
SoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at
higher quality and with realistic self-shadowing reproduction. Our method and
the dataset are publicly available at https://4dqv.mpi-inf.mpg.de/NeRF-OSR/.";Viktor Rudnev<author:sep>Mohamed Elgharib<author:sep>William Smith<author:sep>Lingjie Liu<author:sep>Vladislav Golyanik<author:sep>Christian Theobalt;http://arxiv.org/pdf/2112.05140v2;cs.CV;"22 pages, 10 figures, 2 tables; ECCV 2022; project web page:
  https://4dqv.mpi-inf.mpg.de/NeRF-OSR/";nerf
2112.04812v3;http://arxiv.org/abs/2112.04812v3;2021-12-09;Deep Visual Constraints: Neural Implicit Models for Manipulation  Planning from Visual Input;"Manipulation planning is the problem of finding a sequence of robot
configurations that involves interactions with objects in the scene, e.g.,
grasping and placing an object, or more general tool-use. To achieve such
interactions, traditional approaches require hand-engineering of object
representations and interaction constraints, which easily becomes tedious when
complex objects/interactions are considered. Inspired by recent advances in 3D
modeling, e.g. NeRF, we propose a method to represent objects as continuous
functions upon which constraint features are defined and jointly trained. In
particular, the proposed pixel-aligned representation is directly inferred from
images with known camera geometry and naturally acts as a perception component
in the whole manipulation pipeline, thereby enabling long-horizon planning only
from visual input. Project page:
https://sites.google.com/view/deep-visual-constraints";Jung-Su Ha<author:sep>Danny Driess<author:sep>Marc Toussaint;http://arxiv.org/pdf/2112.04812v3;cs.RO;IEEE Robotics and Automation Letters (RA-L) 2022;nerf
2112.05131v1;http://arxiv.org/abs/2112.05131v1;2021-12-09;Plenoxels: Radiance Fields without Neural Networks;"We introduce Plenoxels (plenoptic voxels), a system for photorealistic view
synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical
harmonics. This representation can be optimized from calibrated images via
gradient methods and regularization without any neural components. On standard,
benchmark tasks, Plenoxels are optimized two orders of magnitude faster than
Neural Radiance Fields with no loss in visual quality.";Alex Yu<author:sep>Sara Fridovich-Keil<author:sep>Matthew Tancik<author:sep>Qinhong Chen<author:sep>Benjamin Recht<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2112.05131v1;cs.CV;For video and code, please see https://alexyu.net/plenoxels;
2112.05139v3;http://arxiv.org/abs/2112.05139v3;2021-12-09;CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields;"We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural
radiance fields (NeRF). By leveraging the joint language-image embedding space
of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose
a unified framework that allows manipulating NeRF in a user-friendly way, using
either a short text prompt or an exemplar image. Specifically, to combine the
novel view synthesis capability of NeRF and the controllable manipulation
ability of latent representations from generative models, we introduce a
disentangled conditional NeRF architecture that allows individual control over
both shape and appearance. This is achieved by performing the shape
conditioning via applying a learned deformation field to the positional
encoding and deferring color conditioning to the volumetric rendering stage. To
bridge this disentangled latent representation to the CLIP embedding, we design
two code mappers that take a CLIP embedding as input and update the latent
codes to reflect the targeted editing. The mappers are trained with a
CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we
propose an inverse optimization method that accurately projects an input image
to the latent codes for manipulation to enable editing on real images. We
evaluate our approach by extensive experiments on a variety of text prompts and
exemplar images and also provide an intuitive interface for interactive
editing. Our implementation is available at
https://cassiepython.github.io/clipnerf/";Can Wang<author:sep>Menglei Chai<author:sep>Mingming He<author:sep>Dongdong Chen<author:sep>Jing Liao;http://arxiv.org/pdf/2112.05139v3;cs.CV;To Appear at CVPR 2022;nerf
2112.04312v3;http://arxiv.org/abs/2112.04312v3;2021-12-08;Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural  Human Rendering;"In this work we develop a generalizable and efficient Neural Radiance Field
(NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under
settings with sparse camera views. Though existing NeRF-based methods can
synthesize rather realistic details for human body, they tend to produce poor
results when the input has self-occlusion, especially for unseen humans under
sparse views. Moreover, these methods often require a large number of sampling
points for rendering, which leads to low efficiency and limits their real-world
applicability. To address these challenges, we propose a Geometry-guided
Progressive NeRF (GP-NeRF). In particular, to better tackle self-occlusion, we
devise a geometry-guided multi-view feature integration approach that utilizes
the estimated geometry prior to integrate the incomplete information from input
views and construct a complete geometry volume for the target human body.
Meanwhile, for achieving higher rendering efficiency, we introduce a
progressive rendering pipeline through geometry guidance, which leverages the
geometric feature volume and the predicted density values to progressively
reduce the number of sampling points and speed up the rendering process.
Experiments on the ZJU-MoCap and THUman datasets show that our method
outperforms the state-of-the-arts significantly across multiple generalization
settings, while the time cost is reduced > 70% via applying our efficient
progressive rendering pipeline.";Mingfei Chen<author:sep>Jianfeng Zhang<author:sep>Xiangyu Xu<author:sep>Lijuan Liu<author:sep>Yujun Cai<author:sep>Jiashi Feng<author:sep>Shuicheng Yan;http://arxiv.org/pdf/2112.04312v3;cs.CV;;nerf
2112.03907v1;http://arxiv.org/abs/2112.03907v1;2021-12-07;Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance  Fields;"Neural Radiance Fields (NeRF) is a popular view synthesis technique that
represents a scene as a continuous volumetric function, parameterized by
multilayer perceptrons that provide the volume density and view-dependent
emitted radiance at each location. While NeRF-based techniques excel at
representing fine geometric structures with smoothly varying view-dependent
appearance, they often fail to accurately capture and reproduce the appearance
of glossy surfaces. We address this limitation by introducing Ref-NeRF, which
replaces NeRF's parameterization of view-dependent outgoing radiance with a
representation of reflected radiance and structures this function using a
collection of spatially-varying scene properties. We show that together with a
regularizer on normal vectors, our model significantly improves the realism and
accuracy of specular reflections. Furthermore, we show that our model's
internal representation of outgoing radiance is interpretable and useful for
scene editing.";Dor Verbin<author:sep>Peter Hedman<author:sep>Ben Mildenhall<author:sep>Todd Zickler<author:sep>Jonathan T. Barron<author:sep>Pratul P. Srinivasan;http://arxiv.org/pdf/2112.03907v1;cs.CV;Project page: https://dorverbin.github.io/refnerf/;nerf
2112.03517v1;http://arxiv.org/abs/2112.03517v1;2021-12-07;CG-NeRF: Conditional Generative Neural Radiance Fields;"While recent NeRF-based generative models achieve the generation of diverse
3D-aware images, these approaches have limitations when generating images that
contain user-specified characteristics. In this paper, we propose a novel
model, referred to as the conditional generative neural radiance fields
(CG-NeRF), which can generate multi-view images reflecting extra input
conditions such as images or texts. While preserving the common characteristics
of a given input condition, the proposed model generates diverse images in fine
detail. We propose: 1) a novel unified architecture which disentangles the
shape and appearance from a condition given in various forms and 2) the
pose-consistent diversity loss for generating multimodal outputs while
maintaining consistency of the view. Experimental results show that the
proposed method maintains consistent image quality on various condition types
and achieves superior fidelity and diversity compared to existing NeRF-based
generative models.";Kyungmin Jo<author:sep>Gyumin Shim<author:sep>Sanghun Jung<author:sep>Soyoung Yang<author:sep>Jaegul Choo;http://arxiv.org/pdf/2112.03517v1;cs.CV;;nerf
2112.03288v2;http://arxiv.org/abs/2112.03288v2;2021-12-06;Dense Depth Priors for Neural Radiance Fields from Sparse Input Views;"Neural radiance fields (NeRF) encode a scene into a neural representation
that enables photo-realistic rendering of novel views. However, a successful
reconstruction from RGB images requires a large number of input views taken
under static conditions - typically up to a few hundred images for room-size
scenes. Our method aims to synthesize novel views of whole rooms from an order
of magnitude fewer images. To this end, we leverage dense depth priors in order
to constrain the NeRF optimization. First, we take advantage of the sparse
depth data that is freely available from the structure from motion (SfM)
preprocessing step used to estimate camera poses. Second, we use depth
completion to convert these sparse points into dense depth maps and uncertainty
estimates, which are used to guide NeRF optimization. Our method enables
data-efficient novel view synthesis on challenging indoor scenes, using as few
as 18 images for an entire scene.";Barbara Roessle<author:sep>Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Pratul P. Srinivasan<author:sep>Matthias Nießner;http://arxiv.org/pdf/2112.03288v2;cs.CV;"CVPR 2022, project page:
  https://barbararoessle.github.io/dense_depth_priors_nerf/ , video:
  https://youtu.be/zzkvvdcvksc";nerf
2112.02789v3;http://arxiv.org/abs/2112.02789v3;2021-12-06;HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs;"Recent neural human representations can produce high-quality multi-view
rendering but require using dense multi-view inputs and costly training. They
are hence largely limited to static models as training each frame is
infeasible. We present HumanNeRF - a generalizable neural representation - for
high-fidelity free-view synthesis of dynamic humans. Analogous to how IBRNet
assists NeRF by avoiding per-scene training, HumanNeRF employs an aggregated
pixel-alignment feature across multi-view inputs along with a pose embedded
non-rigid deformation field for tackling dynamic motions. The raw HumanNeRF can
already produce reasonable rendering on sparse video inputs of unseen subjects
and camera settings. To further improve the rendering quality, we augment our
solution with an appearance blending module for combining the benefits of both
neural volumetric rendering and neural texture blending. Extensive experiments
on various multi-view dynamic human datasets demonstrate the generalizability
and effectiveness of our approach in synthesizing photo-realistic free-view
humans under challenging motions and with very sparse camera view inputs.";Fuqiang Zhao<author:sep>Wei Yang<author:sep>Jiakai Zhang<author:sep>Pei Lin<author:sep>Yingliang Zhang<author:sep>Jingyi Yu<author:sep>Lan Xu;http://arxiv.org/pdf/2112.02789v3;cs.CV;https://zhaofuq.github.io/humannerf/;nerf
2112.02308v2;http://arxiv.org/abs/2112.02308v2;2021-12-04;MoFaNeRF: Morphable Facial Neural Radiance Field;"We propose a parametric model that maps free-view images into a vector space
of coded facial shape, expression and appearance with a neural radiance field,
namely Morphable Facial NeRF. Specifically, MoFaNeRF takes the coded facial
shape, expression and appearance along with space coordinate and view direction
as input to an MLP, and outputs the radiance of the space point for
photo-realistic image synthesis. Compared with conventional 3D morphable models
(3DMM), MoFaNeRF shows superiority in directly synthesizing photo-realistic
facial details even for eyes, mouths, and beards. Also, continuous face
morphing can be easily achieved by interpolating the input shape, expression
and appearance codes. By introducing identity-specific modulation and texture
encoder, our model synthesizes accurate photometric details and shows strong
representation ability. Our model shows strong ability on multiple applications
including image-based fitting, random generation, face rigging, face editing,
and novel view synthesis. Experiments show that our method achieves higher
representation ability than previous parametric models, and achieves
competitive performance in several applications. To the best of our knowledge,
our work is the first facial parametric model built upon a neural radiance
field that can be used in fitting, generation and manipulation. The code and
data is available at https://github.com/zhuhao-nju/mofanerf.";Yiyu Zhuang<author:sep>Hao Zhu<author:sep>Xusen Sun<author:sep>Xun Cao;http://arxiv.org/pdf/2112.02308v2;cs.CV;"accepted to ECCV2022; code available at
  http://github.com/zhuhao-nju/mofanerf";nerf
2112.01983v2;http://arxiv.org/abs/2112.01983v2;2021-12-03;CoNeRF: Controllable Neural Radiance Fields;"We extend neural 3D representations to allow for intuitive and interpretable
user control beyond novel view rendering (i.e. camera control). We allow the
user to annotate which part of the scene one wishes to control with just a
small number of mask annotations in the training images. Our key idea is to
treat the attributes as latent variables that are regressed by the neural
network given the scene encoding. This leads to a few-shot learning framework,
where attributes are discovered automatically by the framework, when
annotations are not provided. We apply our method to various scenes with
different types of controllable attributes (e.g. expression control on human
faces, or state control in movement of inanimate objects). Overall, we
demonstrate, to the best of our knowledge, for the first time novel view and
novel attribute re-rendering of scenes from a single video.";Kacper Kania<author:sep>Kwang Moo Yi<author:sep>Marek Kowalski<author:sep>Tomasz Trzciński<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2112.01983v2;cs.CV;Project page: https://conerf.github.io/;nerf
2112.01759v3;http://arxiv.org/abs/2112.01759v3;2021-12-03;NeRF-SR: High-Quality Neural Radiance Fields using Supersampling;"We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis
with mostly low-resolution (LR) inputs. Our method is built upon Neural
Radiance Fields (NeRF) that predicts per-point density and color with a
multi-layer perceptron. While producing images at arbitrary scales, NeRF
struggles with resolutions that go beyond observed images. Our key insight is
that NeRF benefits from 3D consistency, which means an observed pixel absorbs
information from nearby views. We first exploit it by a supersampling strategy
that shoots multiple rays at each image pixel, which further enforces
multi-view constraint at a sub-pixel level. Then, we show that NeRF-SR can
further boost the performance of supersampling by a refinement network that
leverages the estimated depth at hand to hallucinate details from related
patches on only one HR reference image. Experiment results demonstrate that
NeRF-SR generates high-quality results for novel view synthesis at HR on both
synthetic and real-world datasets without any external information.";Chen Wang<author:sep>Xian Wu<author:sep>Yuan-Chen Guo<author:sep>Song-Hai Zhang<author:sep>Yu-Wing Tai<author:sep>Shi-Min Hu;http://arxiv.org/pdf/2112.01759v3;cs.CV;"Accepted to MM 2022. Project Page:
  https://cwchenwang.github.io/NeRF-SR";nerf
2112.01517v3;http://arxiv.org/abs/2112.01517v3;2021-12-02;Efficient Neural Radiance Fields for Interactive Free-viewpoint Video;"This paper aims to tackle the challenge of efficiently producing interactive
free-viewpoint videos. Some recent works equip neural radiance fields with
image encoders, enabling them to generalize across scenes. When processing
dynamic scenes, they can simply treat each video frame as an individual scene
and perform novel view synthesis to generate free-viewpoint videos. However,
their rendering process is slow and cannot support interactive applications. A
major factor is that they sample lots of points in empty space when inferring
radiance fields. We propose a novel scene representation, called ENeRF, for the
fast creation of interactive free-viewpoint videos. Specifically, given
multi-view images at one frame, we first build the cascade cost volume to
predict the coarse geometry of the scene. The coarse geometry allows us to
sample few points near the scene surface, thereby significantly improving the
rendering speed. This process is fully differentiable, enabling us to jointly
learn the depth prediction and radiance field networks from RGB images.
Experiments on multiple benchmarks show that our approach exhibits competitive
performance while being at least 60 times faster than previous generalizable
radiance field methods.";Haotong Lin<author:sep>Sida Peng<author:sep>Zhen Xu<author:sep>Yunzhi Yan<author:sep>Qing Shuai<author:sep>Hujun Bao<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2112.01517v3;cs.CV;"SIGGRAPH Asia 2022; Project page: https://zju3dv.github.io/enerf/";nerf
2112.01422v2;http://arxiv.org/abs/2112.01422v2;2021-12-02;3D-Aware Semantic-Guided Generative Model for Human Synthesis;"Generative Neural Radiance Field (GNeRF) models, which extract implicit 3D
representations from 2D images, have recently been shown to produce realistic
images representing rigid/semi-rigid objects, such as human faces or cars.
However, they usually struggle to generate high-quality images representing
non-rigid objects, such as the human body, which is of a great interest for
many computer graphics applications. This paper proposes a 3D-aware
Semantic-Guided Generative Model (3D-SGAN) for human image synthesis, which
combines a GNeRF with a texture generator. The former learns an implicit 3D
representation of the human body and outputs a set of 2D semantic segmentation
masks. The latter transforms these semantic masks into a real image, adding a
realistic texture to the human appearance. Without requiring additional 3D
information, our model can learn 3D human representations with a
photo-realistic, controllable generation. Our experiments on the DeepFashion
dataset show that 3D-SGAN significantly outperforms the most recent baselines.
The code is available at https://github.com/zhangqianhui/3DSGAN";Jichao Zhang<author:sep>Enver Sangineto<author:sep>Hao Tang<author:sep>Aliaksandr Siarohin<author:sep>Zhun Zhong<author:sep>Nicu Sebe<author:sep>Wei Wang;http://arxiv.org/pdf/2112.01422v2;cs.CV;ECCV 2022. 29 pages;nerf
2112.01523v3;http://arxiv.org/abs/2112.01523v3;2021-12-02;Learning Neural Light Fields with Ray-Space Embedding Networks;"Neural radiance fields (NeRFs) produce state-of-the-art view synthesis
results. However, they are slow to render, requiring hundreds of network
evaluations per pixel to approximate a volume rendering integral. Baking NeRFs
into explicit data structures enables efficient rendering, but results in a
large increase in memory footprint and, in many cases, a quality reduction. In
this paper, we propose a novel neural light field representation that, in
contrast, is compact and directly predicts integrated radiance along rays. Our
method supports rendering with a single network evaluation per pixel for small
baseline light field datasets and can also be applied to larger baselines with
only a few evaluations per pixel. At the core of our approach is a ray-space
embedding network that maps the 4D ray-space manifold into an intermediate,
interpolable latent space. Our method achieves state-of-the-art quality on
dense forward-facing datasets such as the Stanford Light Field dataset. In
addition, for forward-facing scenes with sparser inputs we achieve results that
are competitive with NeRF-based approaches in terms of quality while providing
a better speed/quality/memory trade-off with far fewer network evaluations.";Benjamin Attal<author:sep>Jia-Bin Huang<author:sep>Michael Zollhoefer<author:sep>Johannes Kopf<author:sep>Changil Kim;http://arxiv.org/pdf/2112.01523v3;cs.CV;"CVPR 2022 camera ready revision. Major changes include: 1. Additional
  comparison to NeX on Stanford, RealFF, Shiny datasets 2. Experiment on 360
  degree lego bulldozer scene in the appendix, using Pluecker parameterization
  3. Moving student-teacher results to the appendix 4. Clarity edits -- in
  particular, making it clear that our Stanford evaluation *does not* use
  subdivision";nerf
2112.01455v2;http://arxiv.org/abs/2112.01455v2;2021-12-02;Zero-Shot Text-Guided Object Generation with Dream Fields;"We combine neural rendering with multi-modal image and text representations
to synthesize diverse 3D objects solely from natural language descriptions. Our
method, Dream Fields, can generate the geometry and color of a wide range of
objects without 3D supervision. Due to the scarcity of diverse, captioned 3D
data, prior methods only generate objects from a handful of categories, such as
ShapeNet. Instead, we guide generation with image-text models pre-trained on
large datasets of captioned images from the web. Our method optimizes a Neural
Radiance Field from many camera views so that rendered images score highly with
a target caption according to a pre-trained CLIP model. To improve fidelity and
visual quality, we introduce simple geometric priors, including
sparsity-inducing transmittance regularization, scene bounds, and new MLP
architectures. In experiments, Dream Fields produce realistic, multi-view
consistent object geometry and color from a variety of natural language
captions.";Ajay Jain<author:sep>Ben Mildenhall<author:sep>Jonathan T. Barron<author:sep>Pieter Abbeel<author:sep>Ben Poole;http://arxiv.org/pdf/2112.01455v2;cs.CV;CVPR 2022. 13 pages. Website: https://ajayj.com/dreamfields;
2112.00724v1;http://arxiv.org/abs/2112.00724v1;2021-12-01;RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from  Sparse Inputs;"Neural Radiance Fields (NeRF) have emerged as a powerful representation for
the task of novel view synthesis due to their simplicity and state-of-the-art
performance. Though NeRF can produce photorealistic renderings of unseen
viewpoints when many input views are available, its performance drops
significantly when this number is reduced. We observe that the majority of
artifacts in sparse input scenarios are caused by errors in the estimated scene
geometry, and by divergent behavior at the start of training. We address this
by regularizing the geometry and appearance of patches rendered from unobserved
viewpoints, and annealing the ray sampling space during training. We
additionally use a normalizing flow model to regularize the color of unobserved
viewpoints. Our model outperforms not only other methods that optimize over a
single scene, but in many cases also conditional models that are extensively
pre-trained on large multi-view datasets.";Michael Niemeyer<author:sep>Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Mehdi S. M. Sajjadi<author:sep>Andreas Geiger<author:sep>Noha Radwan;http://arxiv.org/pdf/2112.00724v1;cs.CV;"Project page available at
  https://m-niemeyer.github.io/regnerf/index.html";nerf
2111.15246v3;http://arxiv.org/abs/2111.15246v3;2021-11-30;Hallucinated Neural Radiance Fields in the Wild;"Neural Radiance Fields (NeRF) has recently gained popularity for its
impressive novel view synthesis ability. This paper studies the problem of
hallucinated NeRF: i.e., recovering a realistic NeRF at a different time of day
from a group of tourism images. Existing solutions adopt NeRF with a
controllable appearance embedding to render novel views under various
conditions, but they cannot render view-consistent images with an unseen
appearance. To solve this problem, we present an end-to-end framework for
constructing a hallucinated NeRF, dubbed as Ha-NeRF. Specifically, we propose
an appearance hallucination module to handle time-varying appearances and
transfer them to novel views. Considering the complex occlusions of tourism
images, we introduce an anti-occlusion module to decompose the static subjects
for visibility accurately. Experimental results on synthetic data and real
tourism photo collections demonstrate that our method can hallucinate the
desired appearances and render occlusion-free images from different views. The
project and supplementary materials are available at
https://rover-xingyu.github.io/Ha-NeRF/.";Xingyu Chen<author:sep>Qi Zhang<author:sep>Xiaoyu Li<author:sep>Yue Chen<author:sep>Ying Feng<author:sep>Xuan Wang<author:sep>Jue Wang;http://arxiv.org/pdf/2111.15246v3;cs.CV;"Accepted by CVPR 2022. Project website:
  https://rover-xingyu.github.io/Ha-NeRF/";nerf
2111.15234v2;http://arxiv.org/abs/2111.15234v2;2021-11-30;NeRFReN: Neural Radiance Fields with Reflections;"Neural Radiance Fields (NeRF) has achieved unprecedented view synthesis
quality using coordinate-based neural scene representations. However, NeRF's
view dependency can only handle simple reflections like highlights but cannot
deal with complex reflections such as those from glass and mirrors. In these
scenarios, NeRF models the virtual image as real geometries which leads to
inaccurate depth estimation, and produces blurry renderings when the multi-view
consistency is violated as the reflected objects may only be seen under some of
the viewpoints. To overcome these issues, we introduce NeRFReN, which is built
upon NeRF to model scenes with reflections. Specifically, we propose to split a
scene into transmitted and reflected components, and model the two components
with separate neural radiance fields. Considering that this decomposition is
highly under-constrained, we exploit geometric priors and apply
carefully-designed training strategies to achieve reasonable decomposition
results. Experiments on various self-captured scenes show that our method
achieves high-quality novel view synthesis and physically sound depth
estimation results while enabling scene editing applications.";Yuan-Chen Guo<author:sep>Di Kang<author:sep>Linchao Bao<author:sep>Yu He<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2111.15234v2;cs.CV;"Accepted to CVPR 2022. Project page:
  https://bennyguo.github.io/nerfren/";nerf
2111.15552v1;http://arxiv.org/abs/2111.15552v1;2021-11-30;NeuSample: Neural Sample Field for Efficient View Synthesis;"Neural radiance fields (NeRF) have shown great potentials in representing 3D
scenes and synthesizing novel views, but the computational overhead of NeRF at
the inference stage is still heavy. To alleviate the burden, we delve into the
coarse-to-fine, hierarchical sampling procedure of NeRF and point out that the
coarse stage can be replaced by a lightweight module which we name a neural
sample field. The proposed sample field maps rays into sample distributions,
which can be transformed into point coordinates and fed into radiance fields
for volume rendering. The overall framework is named as NeuSample. We perform
experiments on Realistic Synthetic 360$^{\circ}$ and Real Forward-Facing, two
popular 3D scene sets, and show that NeuSample achieves better rendering
quality than NeRF while enjoying a faster inference speed. NeuSample is further
compressed with a proposed sample field extraction method towards a better
trade-off between quality and speed.";Jiemin Fang<author:sep>Lingxi Xie<author:sep>Xinggang Wang<author:sep>Xiaopeng Zhang<author:sep>Wenyu Liu<author:sep>Qi Tian;http://arxiv.org/pdf/2111.15552v1;cs.CV;Project page: https://jaminfong.cn/neusample/;nerf
2111.15490v2;http://arxiv.org/abs/2111.15490v2;2021-11-30;FENeRF: Face Editing in Neural Radiance Fields;"Previous portrait image generation methods roughly fall into two categories:
2D GANs and 3D-aware GANs. 2D GANs can generate high fidelity portraits but
with low view consistency. 3D-aware GAN methods can maintain view consistency
but their generated images are not locally editable. To overcome these
limitations, we propose FENeRF, a 3D-aware generator that can produce
view-consistent and locally-editable portrait images. Our method uses two
decoupled latent codes to generate corresponding facial semantics and texture
in a spatial aligned 3D volume with shared geometry. Benefiting from such
underlying 3D representation, FENeRF can jointly render the boundary-aligned
image and semantic mask and use the semantic mask to edit the 3D volume via GAN
inversion. We further show such 3D representation can be learned from widely
available monocular image and semantic mask pairs. Moreover, we reveal that
joint learning semantics and texture helps to generate finer geometry. Our
experiments demonstrate that FENeRF outperforms state-of-the-art methods in
various face editing tasks.";Jingxiang Sun<author:sep>Xuan Wang<author:sep>Yong Zhang<author:sep>Xiaoyu Li<author:sep>Qi Zhang<author:sep>Yebin Liu<author:sep>Jue Wang;http://arxiv.org/pdf/2111.15490v2;cs.CV;Accepted to CVPR 2022. Project: https://mrtornado24.github.io/FENeRF/;nerf
2111.14292v2;http://arxiv.org/abs/2111.14292v2;2021-11-29;Deblur-NeRF: Neural Radiance Fields from Blurry Images;"Neural Radiance Field (NeRF) has gained considerable attention recently for
3D scene reconstruction and novel view synthesis due to its remarkable
synthesis quality. However, image blurriness caused by defocus or motion, which
often occurs when capturing scenes in the wild, significantly degrades its
reconstruction quality. To address this problem, We propose Deblur-NeRF, the
first method that can recover a sharp NeRF from blurry input. We adopt an
analysis-by-synthesis approach that reconstructs blurry views by simulating the
blurring process, thus making NeRF robust to blurry inputs. The core of this
simulation is a novel Deformable Sparse Kernel (DSK) module that models
spatially-varying blur kernels by deforming a canonical sparse kernel at each
spatial location. The ray origin of each kernel point is jointly optimized,
inspired by the physical blurring process. This module is parameterized as an
MLP that has the ability to be generalized to various blur types. Jointly
optimizing the NeRF and the DSK module allows us to restore a sharp NeRF. We
demonstrate that our method can be used on both camera motion blur and defocus
blur: the two most common types of blur in real scenes. Evaluation results on
both synthetic and real-world data show that our method outperforms several
baselines. The synthetic and real datasets along with the source code is
publicly available at https://limacv.github.io/deblurnerf/";Li Ma<author:sep>Xiaoyu Li<author:sep>Jing Liao<author:sep>Qi Zhang<author:sep>Xuan Wang<author:sep>Jue Wang<author:sep>Pedro V. Sander;http://arxiv.org/pdf/2111.14292v2;cs.CV;accepted in CVPR2022;nerf
2111.14643v1;http://arxiv.org/abs/2111.14643v1;2021-11-29;Urban Radiance Fields;"The goal of this work is to perform 3D reconstruction and novel view
synthesis from data captured by scanning platforms commonly deployed for world
mapping in urban outdoor environments (e.g., Street View). Given a sequence of
posed RGB images and lidar sweeps acquired by cameras and scanners moving
through an outdoor scene, we produce a model from which 3D surfaces can be
extracted and novel RGB images can be synthesized. Our approach extends Neural
Radiance Fields, which has been demonstrated to synthesize realistic novel
images for small scenes in controlled settings, with new methods for leveraging
asynchronously captured lidar data, for addressing exposure variation between
captured images, and for leveraging predicted image segmentations to supervise
densities on rays pointing at the sky. Each of these three extensions provides
significant performance improvements in experiments on Street View data. Our
system produces state-of-the-art 3D surface reconstructions and synthesizes
higher quality novel views in comparison to both traditional methods
(e.g.~COLMAP) and recent neural representations (e.g.~Mip-NeRF).";Konstantinos Rematas<author:sep>Andrew Liu<author:sep>Pratul P. Srinivasan<author:sep>Jonathan T. Barron<author:sep>Andrea Tagliasacchi<author:sep>Thomas Funkhouser<author:sep>Vittorio Ferrari;http://arxiv.org/pdf/2111.14643v1;cs.CV;Project: https://urban-radiance-fields.github.io/;nerf
2111.14451v4;http://arxiv.org/abs/2111.14451v4;2021-11-29;HDR-NeRF: High Dynamic Range Neural Radiance Fields;"We present High Dynamic Range Neural Radiance Fields (HDR-NeRF) to recover an
HDR radiance field from a set of low dynamic range (LDR) views with different
exposures. Using the HDR-NeRF, we are able to generate both novel HDR views and
novel LDR views under different exposures. The key to our method is to model
the physical imaging process, which dictates that the radiance of a scene point
transforms to a pixel value in the LDR image with two implicit functions: a
radiance field and a tone mapper. The radiance field encodes the scene radiance
(values vary from 0 to +infty), which outputs the density and radiance of a ray
by giving corresponding ray origin and ray direction. The tone mapper models
the mapping process that a ray hitting on the camera sensor becomes a pixel
value. The color of the ray is predicted by feeding the radiance and the
corresponding exposure time into the tone mapper. We use the classic volume
rendering technique to project the output radiance, colors, and densities into
HDR and LDR images, while only the input LDR images are used as the
supervision. We collect a new forward-facing HDR dataset to evaluate the
proposed method. Experimental results on synthetic and real-world scenes
validate that our method can not only accurately control the exposures of
synthesized views but also render views with a high dynamic range.";Xin Huang<author:sep>Qi Zhang<author:sep>Ying Feng<author:sep>Hongdong Li<author:sep>Xuan Wang<author:sep>Qing Wang;http://arxiv.org/pdf/2111.14451v4;cs.CV;"Accepted to CVPR 2022. Project page:
  https://xhuangcv.github.io/hdr-nerf/";nerf
2111.13539v2;http://arxiv.org/abs/2111.13539v2;2021-11-26;GeoNeRF: Generalizing NeRF with Geometry Priors;"We present GeoNeRF, a generalizable photorealistic novel view synthesis
method based on neural radiance fields. Our approach consists of two main
stages: a geometry reasoner and a renderer. To render a novel view, the
geometry reasoner first constructs cascaded cost volumes for each nearby source
view. Then, using a Transformer-based attention mechanism and the cascaded cost
volumes, the renderer infers geometry and appearance, and renders detailed
images via classical volume rendering techniques. This architecture, in
particular, allows sophisticated occlusion reasoning, gathering information
from consistent source views. Moreover, our method can easily be fine-tuned on
a single scene, and renders competitive results with per-scene optimized neural
rendering methods with a fraction of computational cost. Experiments show that
GeoNeRF outperforms state-of-the-art generalizable neural rendering models on
various synthetic and real datasets. Lastly, with a slight modification to the
geometry reasoner, we also propose an alternative model that adapts to RGBD
images. This model directly exploits the depth information often available
thanks to depth sensors. The implementation code is available at
https://www.idiap.ch/paper/geonerf.";Mohammad Mahdi Johari<author:sep>Yann Lepoittevin<author:sep>François Fleuret;http://arxiv.org/pdf/2111.13539v2;cs.CV;CVPR2022;nerf
2111.13679v1;http://arxiv.org/abs/2111.13679v1;2021-11-26;NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw  Images;"Neural Radiance Fields (NeRF) is a technique for high quality novel view
synthesis from a collection of posed input images. Like most view synthesis
methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images
have been processed by a lossy camera pipeline that smooths detail, clips
highlights, and distorts the simple noise distribution of raw sensor data. We
modify NeRF to instead train directly on linear raw images, preserving the
scene's full dynamic range. By rendering raw output images from the resulting
NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In
addition to changing the camera viewpoint, we can manipulate focus, exposure,
and tonemapping after the fact. Although a single raw image appears
significantly more noisy than a postprocessed one, we show that NeRF is highly
robust to the zero-mean distribution of raw noise. When optimized over many
noisy raw inputs (25-200), NeRF produces a scene representation so accurate
that its rendered novel views outperform dedicated single and multi-image deep
raw denoisers run on the same wide baseline input images. As a result, our
method, which we call RawNeRF, can reconstruct scenes from extremely noisy
images captured in near-darkness.";Ben Mildenhall<author:sep>Peter Hedman<author:sep>Ricardo Martin-Brualla<author:sep>Pratul Srinivasan<author:sep>Jonathan T. Barron;http://arxiv.org/pdf/2111.13679v1;cs.CV;Project page: https://bmild.github.io/rawnerf/;nerf
2111.13112v1;http://arxiv.org/abs/2111.13112v1;2021-11-25;VaxNeRF: Revisiting the Classic for Voxel-Accelerated Neural Radiance  Field;"Neural Radiance Field (NeRF) is a popular method in data-driven 3D
reconstruction. Given its simplicity and high quality rendering, many NeRF
applications are being developed. However, NeRF's big limitation is its slow
speed. Many attempts are made to speeding up NeRF training and inference,
including intricate code-level optimization and caching, use of sophisticated
data structures, and amortization through multi-task and meta learning. In this
work, we revisit the basic building blocks of NeRF through the lens of classic
techniques before NeRF. We propose Voxel-Accelearated NeRF (VaxNeRF),
integrating NeRF with visual hull, a classic 3D reconstruction technique only
requiring binary foreground-background pixel labels per image. Visual hull,
which can be optimized in about 10 seconds, can provide coarse in-out field
separation to omit substantial amounts of network evaluations in NeRF. We
provide a clean fully-pythonic, JAX-based implementation on the popular JaxNeRF
codebase, consisting of only about 30 lines of code changes and a modular
visual hull subroutine, and achieve about 2-8x faster learning on top of the
highly-performative JaxNeRF baseline with zero degradation in rendering
quality. With sufficient compute, this effectively brings down full NeRF
training from hours to 30 minutes. We hope VaxNeRF -- a careful combination of
a classic technique with a deep method (that arguably replaced it) -- can
empower and accelerate new NeRF extensions and applications, with its
simplicity, portability, and reliable performance gains. Codes are available at
https://github.com/naruya/VaxNeRF .";Naruya Kondo<author:sep>Yuya Ikeda<author:sep>Andrea Tagliasacchi<author:sep>Yutaka Matsuo<author:sep>Yoichi Ochiai<author:sep>Shixiang Shane Gu;http://arxiv.org/pdf/2111.13112v1;cs.CV;;nerf
2111.12077v3;http://arxiv.org/abs/2111.12077v3;2021-11-23;Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields;"Though neural radiance fields (NeRF) have demonstrated impressive view
synthesis results on objects and small bounded regions of space, they struggle
on ""unbounded"" scenes, where the camera may point in any direction and content
may exist at any distance. In this setting, existing NeRF-like models often
produce blurry or low-resolution renderings (due to the unbalanced detail and
scale of nearby and distant objects), are slow to train, and may exhibit
artifacts due to the inherent ambiguity of the task of reconstructing a large
scene from a small set of images. We present an extension of mip-NeRF (a NeRF
variant that addresses sampling and aliasing) that uses a non-linear scene
parameterization, online distillation, and a novel distortion-based regularizer
to overcome the challenges presented by unbounded scenes. Our model, which we
dub ""mip-NeRF 360"" as we target scenes in which the camera rotates 360 degrees
around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is
able to produce realistic synthesized views and detailed depth maps for highly
intricate, unbounded real-world scenes.";Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Dor Verbin<author:sep>Pratul P. Srinivasan<author:sep>Peter Hedman;http://arxiv.org/pdf/2111.12077v3;cs.CV;https://jonbarron.info/mipnerf360/;nerf
2111.11215v2;http://arxiv.org/abs/2111.11215v2;2021-11-22;Direct Voxel Grid Optimization: Super-fast Convergence for Radiance  Fields Reconstruction;"We present a super-fast convergence approach to reconstructing the per-scene
radiance field from a set of images that capture the scene with known poses.
This task, which is often applied to novel view synthesis, is recently
revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality
and flexibility. However, NeRF and its variants require a lengthy training time
ranging from hours to days for a single scene. In contrast, our approach
achieves NeRF-comparable quality and converges rapidly from scratch in less
than 15 minutes with a single GPU. We adopt a representation consisting of a
density voxel grid for scene geometry and a feature voxel grid with a shallow
network for complex view-dependent appearance. Modeling with explicit and
discretized volume representations is not new, but we propose two simple yet
non-trivial techniques that contribute to fast convergence speed and
high-quality output. First, we introduce the post-activation interpolation on
voxel density, which is capable of producing sharp surfaces in lower grid
resolution. Second, direct voxel density optimization is prone to suboptimal
geometry solutions, so we robustify the optimization process by imposing
several priors. Finally, evaluation on five inward-facing benchmarks shows that
our method matches, if not surpasses, NeRF's quality, yet it only takes about
15 minutes to train from scratch for a new scene.";Cheng Sun<author:sep>Min Sun<author:sep>Hwann-Tzong Chen;http://arxiv.org/pdf/2111.11215v2;cs.CV;"Project page at https://sunset1995.github.io/dvgo/ ; Code at
  https://github.com/sunset1995/DirectVoxGO";nerf
2111.09996v2;http://arxiv.org/abs/2111.09996v2;2021-11-19;LOLNeRF: Learn from One Look;"We present a method for learning a generative 3D model based on neural
radiance fields, trained solely from data with only single views of each
object. While generating realistic images is no longer a difficult task,
producing the corresponding 3D structure such that they can be rendered from
different views is non-trivial. We show that, unlike existing methods, one does
not need multi-view data to achieve this goal. Specifically, we show that by
reconstructing many images aligned to an approximate canonical pose with a
single network conditioned on a shared latent space, you can learn a space of
radiance fields that models shape and appearance for a class of objects. We
demonstrate this by training models to reconstruct object categories using
datasets that contain only one view of each subject without depth or geometry
information. Our experiments show that we achieve state-of-the-art results in
novel view synthesis and high-quality results for monocular depth prediction.";Daniel Rebain<author:sep>Mark Matthews<author:sep>Kwang Moo Yi<author:sep>Dmitry Lagun<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2111.09996v2;cs.CV;See https://lolnerf.github.io for additional results;nerf
2111.10427v2;http://arxiv.org/abs/2111.10427v2;2021-11-19;DIVeR: Real-time and Accurate Neural Radiance Fields with Deterministic  Integration for Volume Rendering;"DIVeR builds on the key ideas of NeRF and its variants -- density models and
volume rendering -- to learn 3D object models that can be rendered
realistically from small numbers of images. In contrast to all previous NeRF
methods, DIVeR uses deterministic rather than stochastic estimates of the
volume rendering integral. DIVeR's representation is a voxel based field of
features. To compute the volume rendering integral, a ray is broken into
intervals, one per voxel; components of the volume rendering integral are
estimated from the features for each interval using an MLP, and the components
are aggregated. As a result, DIVeR can render thin translucent structures that
are missed by other integrators. Furthermore, DIVeR's representation has
semantics that is relatively exposed compared to other such methods -- moving
feature vectors around in the voxel space results in natural edits. Extensive
qualitative and quantitative comparisons to current state-of-the-art methods
show that DIVeR produces models that (1) render at or above state-of-the-art
quality, (2) are very small without being baked, (3) render very fast without
being baked, and (4) can be edited in natural ways.";Liwen Wu<author:sep>Jae Yong Lee<author:sep>Anand Bhattad<author:sep>Yuxiong Wang<author:sep>David Forsyth;http://arxiv.org/pdf/2111.10427v2;cs.CV;;nerf
2111.08988v1;http://arxiv.org/abs/2111.08988v1;2021-11-17;LVAC: Learned Volumetric Attribute Compression for Point Clouds using  Coordinate Based Networks;"We consider the attributes of a point cloud as samples of a vector-valued
volumetric function at discrete positions. To compress the attributes given the
positions, we compress the parameters of the volumetric function. We model the
volumetric function by tiling space into blocks, and representing the function
over each block by shifts of a coordinate-based, or implicit, neural network.
Inputs to the network include both spatial coordinates and a latent vector per
block. We represent the latent vectors using coefficients of the
region-adaptive hierarchical transform (RAHT) used in the MPEG geometry-based
point cloud codec G-PCC. The coefficients, which are highly compressible, are
rate-distortion optimized by back-propagation through a rate-distortion
Lagrangian loss in an auto-decoder configuration. The result outperforms RAHT
by 2--4 dB. This is the first work to compress volumetric functions represented
by local coordinate-based neural networks. As such, we expect it to be
applicable beyond point clouds, for example to compression of high-resolution
neural radiance fields.";Berivan Isik<author:sep>Philip A. Chou<author:sep>Sung Jin Hwang<author:sep>Nick Johnston<author:sep>George Toderici;http://arxiv.org/pdf/2111.08988v1;cs.GR;30 pages, 29 figures;
2111.04237v1;http://arxiv.org/abs/2111.04237v1;2021-11-08;Template NeRF: Towards Modeling Dense Shape Correspondences from  Category-Specific Object Images;"We present neural radiance fields (NeRF) with templates, dubbed
Template-NeRF, for modeling appearance and geometry and generating dense shape
correspondences simultaneously among objects of the same category from only
multi-view posed images, without the need of either 3D supervision or
ground-truth correspondence knowledge. The learned dense correspondences can be
readily used for various image-based tasks such as keypoint detection, part
segmentation, and texture transfer that previously require specific model
designs. Our method can also accommodate annotation transfer in a one or
few-shot manner, given only one or a few instances of the category. Using
periodic activation and feature-wise linear modulation (FiLM) conditioning, we
introduce deep implicit templates on 3D data into the 3D-aware image synthesis
pipeline NeRF. By representing object instances within the same category as
shape and appearance variation of a shared NeRF template, our proposed method
can achieve dense shape correspondences reasoning on images for a wide range of
object classes. We demonstrate the results and applications on both synthetic
and real-world data with competitive results compared with other methods based
on 3D information.";Jianfei Guo<author:sep>Zhiyuan Yang<author:sep>Xi Lin<author:sep>Qingfu Zhang;http://arxiv.org/pdf/2111.04237v1;cs.CV;10 pages, 8 figures;nerf
2110.14373v1;http://arxiv.org/abs/2110.14373v1;2021-10-27;Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition;"Decomposing a scene into its shape, reflectance and illumination is a
fundamental problem in computer vision and graphics. Neural approaches such as
NeRF have achieved remarkable success in view synthesis, but do not explicitly
perform decomposition and instead operate exclusively on radiance (the product
of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform
decomposition but struggle to accurately recover detailed illumination, thereby
significantly limiting realism. We propose a novel reflectance decomposition
network that can estimate shape, BRDF, and per-image illumination given a set
of object images captured under varying illumination. Our key technique is a
novel illumination integration network called Neural-PIL that replaces a costly
illumination integral operation in the rendering with a simple network query.
In addition, we also learn deep low-dimensional priors on BRDF and illumination
representations using novel smooth manifold auto-encoders. Our decompositions
can result in considerably better BRDF and light estimates enabling more
accurate novel view-synthesis and relighting compared to prior art. Project
page: https://markboss.me/publication/2021-neural-pil/";Mark Boss<author:sep>Varun Jampani<author:sep>Raphael Braun<author:sep>Ce Liu<author:sep>Jonathan T. Barron<author:sep>Hendrik P. A. Lensch;http://arxiv.org/pdf/2110.14373v1;cs.CV;"Project page: https://markboss.me/publication/2021-neural-pil/ Video:
  https://youtu.be/AsdAR5u3vQ8 - Accepted at NeurIPS 2021";nerf
2110.14217v1;http://arxiv.org/abs/2110.14217v1;2021-10-27;Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects;"The ability to grasp and manipulate transparent objects is a major challenge
for robots. Existing depth cameras have difficulty detecting, localizing, and
inferring the geometry of such objects. We propose using neural radiance fields
(NeRF) to detect, localize, and infer the geometry of transparent objects with
sufficient accuracy to find and grasp them securely. We leverage NeRF's
view-independent learned density, place lights to increase specular
reflections, and perform a transparency-aware depth-rendering that we feed into
the Dex-Net grasp planner. We show how additional lights create specular
reflections that improve the quality of the depth map, and test a setup for a
robot workcell equipped with an array of cameras to perform transparent object
manipulation. We also create synthetic and real datasets of transparent objects
in real-world settings, including singulated objects, cluttered tables, and the
top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are
able to reliably compute robust grasps on transparent objects, achieving 90%
and 100% grasp success rates in physical experiments on an ABB YuMi, on objects
where baseline methods fail.";Jeffrey Ichnowski<author:sep>Yahav Avigal<author:sep>Justin Kerr<author:sep>Ken Goldberg;http://arxiv.org/pdf/2110.14217v1;cs.RO;"11 pages, 9 figures, to be published in the Conference on Robot
  Learning (CoRL) 2021";nerf
2110.13746v2;http://arxiv.org/abs/2110.13746v2;2021-10-26;H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction  of Humans in Motion;"We present neural radiance fields for rendering and temporal (4D)
reconstruction of humans in motion (H-NeRF), as captured by a sparse set of
cameras or even from a monocular video. Our approach combines ideas from neural
scene representation, novel-view synthesis, and implicit statistical geometric
human representations, coupled using novel loss functions. Instead of learning
a radiance field with a uniform occupancy prior, we constrain it by a
structured implicit human body model, represented using signed distance
functions. This allows us to robustly fuse information from sparse views and
generalize well beyond the poses or views observed in training. Moreover, we
apply geometric constraints to co-learn the structure of the observed subject
-- including both body and clothing -- and to regularize the radiance field to
geometrically plausible solutions. Extensive experiments on multiple datasets
demonstrate the robustness and the accuracy of our approach, its generalization
capabilities significantly outside a small training set of poses and views, and
statistical extrapolation beyond the observed shape.";Hongyi Xu<author:sep>Thiemo Alldieck<author:sep>Cristian Sminchisescu;http://arxiv.org/pdf/2110.13746v2;cs.CV;;nerf
2110.12993v1;http://arxiv.org/abs/2110.12993v1;2021-10-25;Neural Relightable Participating Media Rendering;"Learning neural radiance fields of a scene has recently allowed realistic
novel view synthesis of the scene, but they are limited to synthesize images
under the original fixed lighting condition. Therefore, they are not flexible
for the eagerly desired tasks like relighting, scene editing and scene
composition. To tackle this problem, several recent methods propose to
disentangle reflectance and illumination from the radiance field. These methods
can cope with solid objects with opaque surfaces but participating media are
neglected. Also, they take into account only direct illumination or at most
one-bounce indirect illumination, thus suffer from energy loss due to ignoring
the high-order indirect illumination. We propose to learn neural
representations for participating media with a complete simulation of global
illumination. We estimate direct illumination via ray tracing and compute
indirect illumination with spherical harmonics. Our approach avoids computing
the lengthy indirect bounces and does not suffer from energy loss. Our
experiments on multiple scenes show that our approach achieves superior visual
quality and numerical performance compared to state-of-the-art methods, and it
can generalize to deal with solid objects with opaque surfaces as well.";Quan Zheng<author:sep>Gurprit Singh<author:sep>Hans-Peter Seidel;http://arxiv.org/pdf/2110.12993v1;cs.CV;Accepted to NeurIPS 2021;
2110.09788v1;http://arxiv.org/abs/2110.09788v1;2021-10-19;CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent  Pixel Synthesis;"The style-based GAN (StyleGAN) architecture achieved state-of-the-art results
for generating high-quality images, but it lacks explicit and precise control
over camera poses. The recently proposed NeRF-based GANs made great progress
towards 3D-aware generators, but they are unable to generate high-quality
images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that
is composed of a shallow NeRF network and a deep implicit neural representation
(INR) network. The generator synthesizes each pixel value independently without
any spatial convolution or upsampling operation. In addition, we diagnose the
problem of mirror symmetry that implies a suboptimal solution and solve it by
introducing an auxiliary discriminator. Trained on raw, single-view images,
CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of
6.97 for images at the $256\times256$ resolution on FFHQ. We also demonstrate
several interesting directions for CIPS-3D such as transfer learning and
3D-aware face stylization. The synthesis results are best viewed as videos, so
we recommend the readers to check our github project at
https://github.com/PeterouZh/CIPS-3D";Peng Zhou<author:sep>Lingxi Xie<author:sep>Bingbing Ni<author:sep>Qi Tian;http://arxiv.org/pdf/2110.09788v1;cs.CV;3D-aware GANs based on NeRF, https://github.com/PeterouZh/CIPS-3D;nerf
2110.08985v1;http://arxiv.org/abs/2110.08985v1;2021-10-18;StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image  Synthesis;"We propose StyleNeRF, a 3D-aware generative model for photo-realistic
high-resolution image synthesis with high multi-view consistency, which can be
trained on unstructured 2D images. Existing approaches either cannot synthesize
high-resolution images with fine details or yield noticeable 3D-inconsistent
artifacts. In addition, many of them lack control over style attributes and
explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF)
into a style-based generator to tackle the aforementioned challenges, i.e.,
improving rendering efficiency and 3D consistency for high-resolution image
generation. We perform volume rendering only to produce a low-resolution
feature map and progressively apply upsampling in 2D to address the first
issue. To mitigate the inconsistencies caused by 2D upsampling, we propose
multiple designs, including a better upsampler and a new regularization loss.
With these designs, StyleNeRF can synthesize high-resolution images at
interactive rates while preserving 3D consistency at high quality. StyleNeRF
also enables control of camera poses and different levels of styles, which can
generalize to unseen views. It also supports challenging tasks, including
zoom-in and-out, style mixing, inversion, and semantic editing.";Jiatao Gu<author:sep>Lingjie Liu<author:sep>Peng Wang<author:sep>Christian Theobalt;http://arxiv.org/pdf/2110.08985v1;cs.CV;24 pages, 19 figures. Project page: http://jiataogu.me/style_nerf/;nerf
2110.07604v3;http://arxiv.org/abs/2110.07604v3;2021-10-14;NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in  the Wild;"Recent history has seen a tremendous growth of work exploring implicit
representations of geometry and radiance, popularized through Neural Radiance
Fields (NeRF). Such works are fundamentally based on a (implicit) volumetric
representation of occupancy, allowing them to model diverse scene structure
including translucent objects and atmospheric obscurants. But because the vast
majority of real-world scenes are composed of well-defined surfaces, we
introduce a surface analog of such implicit models called Neural Reflectance
Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface
that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions.
Even more importantly, surface parameterizations allow NeRS to learn (neural)
bidirectional surface reflectance functions (BRDFs) that factorize
view-dependent appearance into environmental illumination, diffuse color
(albedo), and specular ""shininess."" Finally, rather than illustrating our
results on synthetic scenes or controlled in-the-lab capture, we assemble a
novel dataset of multi-view images from online marketplaces for selling goods.
Such ""in-the-wild"" multi-view image sets pose a number of challenges, including
a small number of views with unknown/rough camera estimates. We demonstrate
that surface-based neural reconstructions enable learning from such data,
outperforming volumetric neural rendering-based reconstructions. We hope that
NeRS serves as a first step toward building scalable, high-quality libraries of
real-world shape, materials, and illumination. The project page with code and
video visualizations can be found at https://jasonyzhang.com/ners.";Jason Y. Zhang<author:sep>Gengshan Yang<author:sep>Shubham Tulsiani<author:sep>Deva Ramanan;http://arxiv.org/pdf/2110.07604v3;cs.CV;In NeurIPS 2021. v2-3: Fixed minor typos;nerf
2110.06558v1;http://arxiv.org/abs/2110.06558v1;2021-10-13;LENS: Localization enhanced by NeRF synthesis;"Neural Radiance Fields (NeRF) have recently demonstrated photo-realistic
results for the task of novel view synthesis. In this paper, we propose to
apply novel view synthesis to the robot relocalization problem: we demonstrate
improvement of camera pose regression thanks to an additional synthetic dataset
rendered by the NeRF class of algorithm. To avoid spawning novel views in
irrelevant places we selected virtual camera locations from NeRF internal
representation of the 3D geometry of the scene. We further improved
localization accuracy of pose regressors using synthesized realistic and
geometry consistent images as data augmentation during training. At the time of
publication, our approach improved state of the art with a 60% lower error on
Cambridge Landmarks and 7-scenes datasets. Hence, the resulting accuracy
becomes comparable to structure-based methods, without any architecture
modification or domain adaptation constraints. Since our method allows almost
infinite generation of training data, we investigated limitations of camera
pose regression depending on size and distribution of data used for training on
public benchmarks. We concluded that pose regression accuracy is mostly bounded
by relatively small and biased datasets rather than capacity of the pose
regression model to solve the localization task.";Arthur Moreau<author:sep>Nathan Piasco<author:sep>Dzmitry Tsishkou<author:sep>Bogdan Stanciulescu<author:sep>Arnaud de La Fortelle;http://arxiv.org/pdf/2110.06558v1;cs.CV;Accepted at CoRL 2021;nerf
2110.05594v1;http://arxiv.org/abs/2110.05594v1;2021-10-11;Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo;"We present a modern solution to the multi-view photometric stereo problem
(MVPS). Our work suitably exploits the image formation model in a MVPS
experimental setup to recover the dense 3D reconstruction of an object from
images. We procure the surface orientation using a photometric stereo (PS)
image formation model and blend it with a multi-view neural radiance field
representation to recover the object's surface geometry. Contrary to the
previous multi-staged framework to MVPS, where the position, iso-depth
contours, or orientation measurements are estimated independently and then
fused later, our method is simple to implement and realize. Our method performs
neural rendering of multi-view images while utilizing surface normals estimated
by a deep photometric stereo network. We render the MVPS images by considering
the object's surface normals for each 3D sample point along the viewing
direction rather than explicitly using the density gradient in the volume space
via 3D occupancy information. We optimize the proposed neural radiance field
representation for the MVPS setup efficiently using a fully connected deep
network to recover the 3D geometry of an object. Extensive evaluation on the
DiLiGenT-MV benchmark dataset shows that our method performs better than the
approaches that perform only PS or only multi-view stereo (MVS) and provides
comparable results against the state-of-the-art multi-stage fusion methods.";Berk Kaya<author:sep>Suryansh Kumar<author:sep>Francesco Sarno<author:sep>Vittorio Ferrari<author:sep>Luc Van Gool;http://arxiv.org/pdf/2110.05594v1;cs.CV;Accepted for publication at IEEE/CVF WACV 2022. 18 pages;
2110.00276v1;http://arxiv.org/abs/2110.00276v1;2021-10-01;TyXe: Pyro-based Bayesian neural nets for Pytorch;"We introduce TyXe, a Bayesian neural network library built on top of Pytorch
and Pyro. Our leading design principle is to cleanly separate architecture,
prior, inference and likelihood specification, allowing for a flexible workflow
where users can quickly iterate over combinations of these components. In
contrast to existing packages TyXe does not implement any layer classes, and
instead relies on architectures defined in generic Pytorch code. TyXe then
provides modular choices for canonical priors, variational guides, inference
techniques, and layer selections for a Bayesian treatment of the specified
architecture. Sampling tricks for variance reduction, such as local
reparameterization or flipout, are implemented as effect handlers, which can be
applied independently of other specifications. We showcase the ease of use of
TyXe to explore Bayesian versions of popular models from various libraries: toy
regression with a pure Pytorch neural network; large-scale image classification
with torchvision ResNets; graph neural networks based on DGL; and Neural
Radiance Fields built on top of Pytorch3D. Finally, we provide convenient
abstractions for variational continual learning. In all cases the change from a
deterministic to a Bayesian neural network comes with minimal modifications to
existing code, offering a broad range of researchers and practitioners alike
practical access to uncertainty estimation techniques. The library is available
at https://github.com/TyXe-BDL/TyXe.";Hippolyt Ritter<author:sep>Theofanis Karaletsos;http://arxiv.org/pdf/2110.00276v1;stat.ML;Previously presented at PROBPROG 2020;
2110.00168v2;http://arxiv.org/abs/2110.00168v2;2021-10-01;Vision-Only Robot Navigation in a Neural Radiance World;"Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm
for the representation of natural, complex 3D scenes. NeRFs represent
continuous volumetric density and RGB values in a neural network, and generate
photo-realistic images from unseen camera viewpoints through ray tracing. We
propose an algorithm for navigating a robot through a 3D environment
represented as a NeRF using only an on-board RGB camera for localization. We
assume the NeRF for the scene has been pre-trained offline, and the robot's
objective is to navigate through unoccupied space in the NeRF to reach a goal
pose. We introduce a trajectory optimization algorithm that avoids collisions
with high-density regions in the NeRF based on a discrete time version of
differential flatness that is amenable to constraining the robot's full pose
and control inputs. We also introduce an optimization based filtering method to
estimate 6DoF pose and velocities for the robot in the NeRF given only an
onboard RGB camera. We combine the trajectory planner with the pose filter in
an online replanning loop to give a vision-based robot navigation pipeline. We
present simulation results with a quadrotor robot navigating through a jungle
gym environment, the inside of a church, and Stonehenge using only an RGB
camera. We also demonstrate an omnidirectional ground robot navigating through
the church, requiring it to reorient to fit through the narrow gap. Videos of
this work can be found at https://mikh3x4.github.io/nerf-navigation/ .";Michal Adamkiewicz<author:sep>Timothy Chen<author:sep>Adam Caccavale<author:sep>Rachel Gardner<author:sep>Preston Culbertson<author:sep>Jeannette Bohg<author:sep>Mac Schwager;http://arxiv.org/pdf/2110.00168v2;cs.RO;;nerf
2109.15271v2;http://arxiv.org/abs/2109.15271v2;2021-09-30;TöRF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis;"Neural networks can represent and accurately reconstruct radiance fields for
static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes
captured with monocular video, with promising performance. However, the
monocular setting is known to be an under-constrained problem, and so methods
rely on data-driven priors for reconstructing dynamic content. We replace these
priors with measurements from a time-of-flight (ToF) camera, and introduce a
neural representation based on an image formation model for continuous-wave ToF
cameras. Instead of working with processed depth maps, we model the raw ToF
sensor measurements to improve reconstruction quality and avoid issues with low
reflectance regions, multi-path interference, and a sensor's limited
unambiguous depth range. We show that this approach improves robustness of
dynamic scene reconstruction to erroneous calibration and large motions, and
discuss the benefits and limitations of integrating RGB+ToF sensors that are
now available on modern smartphones.";Benjamin Attal<author:sep>Eliot Laidlaw<author:sep>Aaron Gokaslan<author:sep>Changil Kim<author:sep>Christian Richardt<author:sep>James Tompkin<author:sep>Matthew O'Toole;http://arxiv.org/pdf/2109.15271v2;cs.CV;"Accepted to NeurIPS 2021. Web page: https://imaging.cs.cmu.edu/torf/
  NeurIPS camera ready updates -- added quantitative comparisons to new
  methods, visual side-by-side comparisons performed on larger baseline camera
  sequences";nerf
2109.07448v1;http://arxiv.org/abs/2109.07448v1;2021-09-15;Neural Human Performer: Learning Generalizable Radiance Fields for Human  Performance Rendering;"In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary
human performance using sparse multi-view cameras. Recently, several works have
addressed this problem by learning person-specific neural radiance fields
(NeRF) to capture the appearance of a particular human. In parallel, some work
proposed to use pixel-aligned features to generalize radiance fields to
arbitrary new scenes and objects. Adopting such generalization approaches to
humans, however, is highly challenging due to the heavy occlusions and dynamic
articulations of body parts. To tackle this, we propose Neural Human Performer,
a novel approach that learns generalizable neural radiance fields based on a
parametric human body model for robust performance capture. Specifically, we
first introduce a temporal transformer that aggregates tracked visual features
based on the skeletal body motion over time. Moreover, a multi-view transformer
is proposed to perform cross-attention between the temporally-fused features
and the pixel-aligned features at each time step to integrate observations on
the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets
show that our method significantly outperforms recent generalizable NeRF
methods on unseen identities and poses. The video results and code are
available at https://youngjoongunc.github.io/nhp.";Youngjoong Kwon<author:sep>Dahun Kim<author:sep>Duygu Ceylan<author:sep>Henry Fuchs;http://arxiv.org/pdf/2109.07448v1;cs.CV;;nerf
2109.02123v3;http://arxiv.org/abs/2109.02123v3;2021-09-05;Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit  3D Representations;"Neural Radiance Fields (NeRF) has become a popular framework for learning
implicit 3D representations and addressing different tasks such as novel-view
synthesis or depth-map estimation. However, in downstream applications where
decisions need to be made based on automatic predictions, it is critical to
leverage the confidence associated with the model estimations. Whereas
uncertainty quantification is a long-standing problem in Machine Learning, it
has been largely overlooked in the recent NeRF literature. In this context, we
propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of
standard NeRF that learns a probability distribution over all the possible
radiance fields modeling the scene. This distribution allows to quantify the
uncertainty associated with the scene information provided by the model. S-NeRF
optimization is posed as a Bayesian learning problem which is efficiently
addressed using the Variational Inference framework. Exhaustive experiments
over benchmark datasets demonstrate that S-NeRF is able to provide more
reliable predictions and confidence values than generic approaches previously
proposed for uncertainty estimation in other domains.";Jianxiong Shen<author:sep>Adria Ruiz<author:sep>Antonio Agudo<author:sep>Francesc Moreno-Noguer;http://arxiv.org/pdf/2109.02123v3;cs.CV;;nerf
2109.01847v1;http://arxiv.org/abs/2109.01847v1;2021-09-04;Learning Object-Compositional Neural Radiance Field for Editable Scene  Rendering;"Implicit neural rendering techniques have shown promising results for novel
view synthesis. However, existing methods usually encode the entire scene as a
whole, which is generally not aware of the object identity and limits the
ability to the high-level editing tasks such as moving or adding furniture. In
this paper, we present a novel neural scene rendering system, which learns an
object-compositional neural radiance field and produces realistic rendering
with editing capability for a clustered and real-world scene. Specifically, we
design a novel two-pathway architecture, in which the scene branch encodes the
scene geometry and appearance, and the object branch encodes each standalone
object conditioned on learnable object activation codes. To survive the
training in heavily cluttered scenes, we propose a scene-guided training
strategy to solve the 3D space ambiguity in the occluded regions and learn
sharp boundaries for each object. Extensive experiments demonstrate that our
system not only achieves competitive performance for static scene novel-view
synthesis, but also produces realistic rendering for object-level editing.";Bangbang Yang<author:sep>Yinda Zhang<author:sep>Yinghao Xu<author:sep>Yijin Li<author:sep>Han Zhou<author:sep>Hujun Bao<author:sep>Guofeng Zhang<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2109.01847v1;cs.CV;"Accepted to ICCV 2021. Project Page:
  https://zju3dv.github.io/object_nerf";
2109.01750v1;http://arxiv.org/abs/2109.01750v1;2021-09-03;CodeNeRF: Disentangled Neural Radiance Fields for Object Categories;"CodeNeRF is an implicit 3D neural representation that learns the variation of
object shapes and textures across a category and can be trained, from a set of
posed images, to synthesize novel views of unseen objects. Unlike the original
NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture
by learning separate embeddings. At test time, given a single unposed image of
an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and
appearance codes via optimization. Unseen objects can be reconstructed from a
single image, and then rendered from new viewpoints or their shape and texture
edited by varying the latent codes. We conduct experiments on the SRN
benchmark, which show that CodeNeRF generalises well to unseen objects and
achieves on-par performance with methods that require known camera pose at test
time. Our results on real-world images demonstrate that CodeNeRF can bridge the
sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}";Wonbong Jang<author:sep>Lourdes Agapito;http://arxiv.org/pdf/2109.01750v1;cs.GR;10 pages, 15 figures, ICCV 2021;nerf
2109.01129v3;http://arxiv.org/abs/2109.01129v3;2021-09-02;NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor  Multi-view Stereo;"In this work, we present a new multi-view depth estimation method that
utilizes both conventional reconstruction and learning-based priors over the
recently proposed neural radiance fields (NeRF). Unlike existing neural network
based optimization method that relies on estimated correspondences, our method
directly optimizes over implicit volumes, eliminating the challenging step of
matching pixels in indoor scenes. The key to our approach is to utilize the
learning-based priors to guide the optimization process of NeRF. Our system
firstly adapts a monocular depth network over the target scene by finetuning on
its sparse SfM+MVS reconstruction from COLMAP. Then, we show that the
shape-radiance ambiguity of NeRF still exists in indoor environments and
propose to address the issue by employing the adapted depth priors to monitor
the sampling process of volume rendering. Finally, a per-pixel confidence map
acquired by error computation on the rendered image can be used to further
improve the depth quality. Experiments show that our proposed framework
significantly outperforms state-of-the-art methods on indoor scenes, with
surprising findings presented on the effectiveness of correspondence-based
optimization and NeRF-based optimization over the adapted depth priors. In
addition, we show that the guided optimization scheme does not sacrifice the
original synthesis capability of neural radiance fields, improving the
rendering quality on both seen and novel views. Code is available at
https://github.com/weiyithu/NerfingMVS.";Yi Wei<author:sep>Shaohui Liu<author:sep>Yongming Rao<author:sep>Wang Zhao<author:sep>Jiwen Lu<author:sep>Jie Zhou;http://arxiv.org/pdf/2109.01129v3;cs.CV;"To appear in ICCV 2021 (Oral). Project page:
  https://weiyithu.github.io/NerfingMVS/";nerf
2108.13826v2;http://arxiv.org/abs/2108.13826v2;2021-08-31;Self-Calibrating Neural Radiance Fields;"In this work, we propose a camera self-calibration algorithm for generic
cameras with arbitrary non-linear distortions. We jointly learn the geometry of
the scene and the accurate camera parameters without any calibration objects.
Our camera model consists of a pinhole model, a fourth order radial distortion,
and a generic noise model that can learn arbitrary non-linear camera
distortions. While traditional self-calibration algorithms mostly rely on
geometric constraints, we additionally incorporate photometric consistency.
This requires learning the geometry of the scene, and we use Neural Radiance
Fields (NeRF). We also propose a new geometric loss function, viz., projected
ray distance loss, to incorporate geometric consistency for complex non-linear
camera models. We validate our approach on standard real image datasets and
demonstrate that our model can learn the camera intrinsics and extrinsics
(pose) from scratch without COLMAP initialization. Also, we show that learning
accurate camera models in a differentiable manner allows us to improve PSNR
over baselines. Our module is an easy-to-use plugin that can be applied to NeRF
variants to improve performance. The code and data are currently available at
https://github.com/POSTECH-CVLab/SCNeRF.";Yoonwoo Jeong<author:sep>Seokjun Ahn<author:sep>Christopher Choy<author:sep>Animashree Anandkumar<author:sep>Minsu Cho<author:sep>Jaesik Park;http://arxiv.org/pdf/2108.13826v2;cs.CV;"Accepted in ICCV21, Project Page:
  https://postech-cvlab.github.io/SCNeRF/";nerf
2108.05577v1;http://arxiv.org/abs/2108.05577v1;2021-08-12;iButter: Neural Interactive Bullet Time Generator for Human  Free-viewpoint Rendering;"Generating ``bullet-time'' effects of human free-viewpoint videos is critical
for immersive visual effects and VR/AR experience. Recent neural advances still
lack the controllable and interactive bullet-time design ability for human
free-viewpoint rendering, especially under the real-time, dynamic and general
setting for our trajectory-aware task. To fill this gap, in this paper we
propose a neural interactive bullet-time generator (iButter) for
photo-realistic human free-viewpoint rendering from dense RGB streams, which
enables flexible and interactive design for human bullet-time visual effects.
Our iButter approach consists of a real-time preview and design stage as well
as a trajectory-aware refinement stage. During preview, we propose an
interactive bullet-time design approach by extending the NeRF rendering to a
real-time and dynamic setting and getting rid of the tedious per-scene
training. To this end, our bullet-time design stage utilizes a hybrid training
set, light-weight network design and an efficient silhouette-based sampling
strategy. During refinement, we introduce an efficient trajectory-aware scheme
within 20 minutes, which jointly encodes the spatial, temporal consistency and
semantic cues along the designed trajectory, achieving photo-realistic
bullet-time viewing experience of human activities. Extensive experiments
demonstrate the effectiveness of our approach for convenient interactive
bullet-time design and photo-realistic human free-viewpoint video generation.";Liao Wang<author:sep>Ziyu Wang<author:sep>Pei Lin<author:sep>Yuheng Jiang<author:sep>Xin Suo<author:sep>Minye Wu<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2108.05577v1;cs.CV;Accepted by ACM MM 2021;nerf
2108.04886v1;http://arxiv.org/abs/2108.04886v1;2021-08-10;Differentiable Surface Rendering via Non-Differentiable Sampling;"We present a method for differentiable rendering of 3D surfaces that supports
both explicit and implicit representations, provides derivatives at occlusion
boundaries, and is fast and simple to implement. The method first samples the
surface using non-differentiable rasterization, then applies differentiable,
depth-aware point splatting to produce the final image. Our approach requires
no differentiable meshing or rasterization steps, making it efficient for large
3D models and applicable to isosurfaces extracted from implicit surface
definitions. We demonstrate the effectiveness of our method for implicit-,
mesh-, and parametric-surface-based inverse rendering and neural-network
training applications. In particular, we show for the first time efficient,
differentiable rendering of an isosurface extracted from a neural radiance
field (NeRF), and demonstrate surface-based, rather than volume-based,
rendering of a NeRF.";Forrester Cole<author:sep>Kyle Genova<author:sep>Avneesh Sud<author:sep>Daniel Vlasic<author:sep>Zhoutong Zhang;http://arxiv.org/pdf/2108.04886v1;cs.GR;Accepted to ICCV 2021;nerf
2108.04913v1;http://arxiv.org/abs/2108.04913v1;2021-08-10;FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face  Animation;"This paper presents a neural rendering method for controllable portrait video
synthesis. Recent advances in volumetric neural rendering, such as neural
radiance fields (NeRF), has enabled the photorealistic novel view synthesis of
static scenes with impressive results. However, modeling dynamic and
controllable objects as part of a scene with such scene representations is
still challenging. In this work, we design a system that enables both novel
view synthesis for portrait video, including the human subject and the scene
background, and explicit control of the facial expressions through a
low-dimensional expression representation. We leverage the expression space of
a 3D morphable face model (3DMM) to represent the distribution of human facial
expressions, and use it to condition the NeRF volumetric function. Furthermore,
we impose a spatial prior brought by 3DMM fitting to guide the network to learn
disentangled control for scene appearance and facial actions. We demonstrate
the effectiveness of our method on free view synthesis of portrait videos with
expression controls. To train a scene, our method only requires a short video
of a subject captured by a mobile device.";ShahRukh Athar<author:sep>Zhixin Shu<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2108.04913v1;cs.CV;version 1.0.0;nerf
2108.03880v2;http://arxiv.org/abs/2108.03880v2;2021-08-09;NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis;"Multi-View Stereo (MVS) is a core task in 3D computer vision. With the surge
of novel deep learning methods, learned MVS has surpassed the accuracy of
classical approaches, but still relies on building a memory intensive dense
cost volume. Novel View Synthesis (NVS) is a parallel line of research and has
recently seen an increase in popularity with Neural Radiance Field (NeRF)
models, which optimize a per scene radiance field. However, NeRF methods do not
generalize to novel scenes and are slow to train and test. We propose to bridge
the gap between these two methodologies with a novel network that can recover
3D scene geometry as a distance function, together with high-resolution color
images. Our method uses only a sparse set of images as input and can generalize
well to novel scenes. Additionally, we propose a coarse-to-fine sphere tracing
approach in order to significantly increase speed. We show on various datasets
that our method reaches comparable accuracy to per-scene optimized methods
while being able to generalize and running significantly faster. We provide the
source code at https://github.com/AIS-Bonn/neural_mvs";Radu Alexandru Rosu<author:sep>Sven Behnke;http://arxiv.org/pdf/2108.03880v2;cs.CV;"Accepted for International Joint Conference on Neural Networks
  (IJCNN) 2022. Code available at https://github.com/AIS-Bonn/neural_mvs";nerf
2107.11024v2;http://arxiv.org/abs/2107.11024v2;2021-07-23;A Deep Signed Directional Distance Function for Object Shape  Representation;"Neural networks that map 3D coordinates to signed distance function (SDF) or
occupancy values have enabled high-fidelity implicit representations of object
shape. This paper develops a new shape model that allows synthesizing novel
distance views by optimizing a continuous signed directional distance function
(SDDF). Similar to deep SDF models, our SDDF formulation can represent whole
categories of shapes and complete or interpolate across shapes from partial
input data. Unlike an SDF, which measures distance to the nearest surface in
any direction, an SDDF measures distance in a given direction. This allows
training an SDDF model without 3D shape supervision, using only distance
measurements, readily available from depth camera or Lidar sensors. Our model
also removes post-processing steps like surface extraction or rendering by
directly predicting distance at arbitrary locations and viewing directions.
Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which
train high-capacity black-box models, our model encodes by construction the
property that SDDF values decrease linearly along the viewing direction. This
structure constraint not only results in dimensionality reduction but also
provides analytical confidence about the accuracy of SDDF predictions,
regardless of the distance to the object surface.";Ehsan Zobeidi<author:sep>Nikolay Atanasov;http://arxiv.org/pdf/2107.11024v2;cs.CV;;
2107.04004v2;http://arxiv.org/abs/2107.04004v2;2021-07-08;3D Neural Scene Representations for Visuomotor Control;"Humans have a strong intuitive understanding of the 3D environment around us.
The mental model of the physics in our brain applies to objects of different
materials and enables us to perform a wide range of manipulation tasks that are
far beyond the reach of current robots. In this work, we desire to learn models
for dynamic 3D scenes purely from 2D visual observations. Our model combines
Neural Radiance Fields (NeRF) and time contrastive learning with an
autoencoding framework, which learns viewpoint-invariant 3D-aware scene
representations. We show that a dynamics model, constructed over the learned
representation space, enables visuomotor control for challenging manipulation
tasks involving both rigid bodies and fluids, where the target is specified in
a viewpoint different from what the robot operates on. When coupled with an
auto-decoding framework, it can even support goal specification from camera
viewpoints that are outside the training distribution. We further demonstrate
the richness of the learned 3D dynamics model by performing future prediction
and novel view synthesis. Finally, we provide detailed ablation studies
regarding different system designs and qualitative analysis of the learned
representations.";Yunzhu Li<author:sep>Shuang Li<author:sep>Vincent Sitzmann<author:sep>Pulkit Agrawal<author:sep>Antonio Torralba;http://arxiv.org/pdf/2107.04004v2;cs.RO;"Accepted to Conference on Robot Learning (CoRL 2021) as Oral
  Presentation. The first two authors contributed equally. Project Page:
  https://3d-representation-learning.github.io/nerf-dy/";nerf
2107.02791v2;http://arxiv.org/abs/2107.02791v2;2021-07-06;Depth-supervised NeRF: Fewer Views and Faster Training for Free;"A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting
incorrect geometries when given an insufficient number of input views. One
potential reason is that standard volumetric rendering does not enforce the
constraint that most of a scene's geometry consist of empty space and opaque
surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised
Neural Radiance Fields), a loss for learning radiance fields that takes
advantage of readily-available depth supervision. We leverage the fact that
current NeRF pipelines require images with known camera poses that are
typically estimated by running structure-from-motion (SFM). Crucially, SFM also
produces sparse 3D points that can be used as ""free"" depth supervision during
training: we add a loss to encourage the distribution of a ray's terminating
depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can
render better images given fewer training views while training 2-3x faster.
Further, we show that our loss is compatible with other recently proposed NeRF
methods, demonstrating that depth is a cheap and easily digestible supervisory
signal. And finally, we find that DS-NeRF can support other types of depth
supervision such as scanned depth sensors and RGB-D reconstruction outputs.";Kangle Deng<author:sep>Andrew Liu<author:sep>Jun-Yan Zhu<author:sep>Deva Ramanan;http://arxiv.org/pdf/2107.02791v2;cs.CV;"Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:
  https://github.com/dunbar12138/DSNeRF";nerf
2106.13870v2;http://arxiv.org/abs/2106.13870v2;2021-06-25;Scene Uncertainty and the Wellington Posterior of Deterministic Image  Classifiers;"We propose a method to estimate the uncertainty of the outcome of an image
classifier on a given input datum. Deep neural networks commonly used for image
classification are deterministic maps from an input image to an output class.
As such, their outcome on a given datum involves no uncertainty, so we must
specify what variability we are referring to when defining, measuring and
interpreting uncertainty, and attributing ""confidence"" to the outcome. To this
end, we introduce the Wellington Posterior, which is the distribution of
outcomes that would have been obtained in response to data that could have been
generated by the same scene that produced the given image. Since there are
infinitely many scenes that could have generated any given image, the
Wellington Posterior involves inductive transfer from scenes other than the one
portrayed. We explore the use of data augmentation, dropout, ensembling,
single-view reconstruction, and model linearization to compute a Wellington
Posterior. Additional methods include the use of conditional generative models
such as generative adversarial networks, neural radiance fields, and
conditional prior networks. We test these methods against the empirical
posterior obtained by performing inference on multiple images of the same
underlying scene. These developments are only a small step towards assessing
the reliability of deep network classifiers in a manner that is compatible with
safety-critical applications and human interpretation.";Stephanie Tsuei<author:sep>Aditya Golatkar<author:sep>Stefano Soatto;http://arxiv.org/pdf/2106.13870v2;cs.CV;;
2106.13629v2;http://arxiv.org/abs/2106.13629v2;2021-06-25;Animatable Neural Radiance Fields from Monocular RGB Videos;"We present animatable neural radiance fields (animatable NeRF) for detailed
human avatar creation from monocular videos. Our approach extends neural
radiance fields (NeRF) to the dynamic scenes with human movements via
introducing explicit pose-guided deformation while learning the scene
representation network. In particular, we estimate the human pose for each
frame and learn a constant canonical space for the detailed human template,
which enables natural shape deformation from the observation space to the
canonical space under the explicit control of the pose parameters. To
compensate for inaccurate pose estimation, we introduce the pose refinement
strategy that updates the initial pose during the learning process, which not
only helps to learn more accurate human reconstruction but also accelerates the
convergence. In experiments we show that the proposed approach achieves 1)
implicit human geometry and appearance reconstruction with high-quality
details, 2) photo-realistic rendering of the human from novel views, and 3)
animation of the human with novel poses.";Jianchuan Chen<author:sep>Ying Zhang<author:sep>Di Kang<author:sep>Xuefei Zhe<author:sep>Linchao Bao<author:sep>Xu Jia<author:sep>Huchuan Lu;http://arxiv.org/pdf/2106.13629v2;cs.CV;12 pages, 12 figures;nerf
2106.13228v2;http://arxiv.org/abs/2106.13228v2;2021-06-24;HyperNeRF: A Higher-Dimensional Representation for Topologically Varying  Neural Radiance Fields;"Neural Radiance Fields (NeRF) are able to reconstruct scenes with
unprecedented fidelity, and various recent works have extended NeRF to handle
dynamic scenes. A common approach to reconstruct such non-rigid scenes is
through the use of a learned deformation field mapping from coordinates in each
input image into a canonical template coordinate space. However, these
deformation-based approaches struggle to model changes in topology, as
topological changes require a discontinuity in the deformation field, but these
deformation fields are necessarily continuous. We address this limitation by
lifting NeRFs into a higher dimensional space, and by representing the 5D
radiance field corresponding to each individual input image as a slice through
this ""hyper-space"". Our method is inspired by level set methods, which model
the evolution of surfaces as slices through a higher dimensional surface. We
evaluate our method on two tasks: (i) interpolating smoothly between ""moments"",
i.e., configurations of the scene, seen in the input images while maintaining
visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
that our method, which we dub HyperNeRF, outperforms existing methods on both
tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for
interpolation and 8.6% for novel-view synthesis, as measured by LPIPS.
Additional videos, results, and visualizations are available at
https://hypernerf.github.io.";Keunhong Park<author:sep>Utkarsh Sinha<author:sep>Peter Hedman<author:sep>Jonathan T. Barron<author:sep>Sofien Bouaziz<author:sep>Dan B Goldman<author:sep>Ricardo Martin-Brualla<author:sep>Steven M. Seitz;http://arxiv.org/pdf/2106.13228v2;cs.CV;SIGGRAPH Asia 2021, Project page: https://hypernerf.github.io/;nerf
2106.10859v1;http://arxiv.org/abs/2106.10859v1;2021-06-21;Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single  Panorama;"We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first
method to the application of parallax-enabled novel panoramic view synthesis.
Recent works for novel view synthesis focus on perspective images with limited
field-of-view and require sufficient pictures captured in a specific condition.
Conversely, OmniNeRF can generate panorama images for unknown viewpoints given
a single equirectangular image as training data. To this end, we propose to
augment the single RGB-D panorama by projecting back and forth between a 3D
world and different 2D panoramic coordinates at different virtual camera
positions. By doing so, we are able to optimize an Omnidirectional Neural
Radiance Field with visible pixels collecting from omnidirectional viewing
angles at a fixed center for the estimation of new viewing angles from varying
camera positions. As a result, the proposed OmniNeRF achieves convincing
renderings of novel panoramic views that exhibit the parallax effect. We
showcase the effectiveness of each of our proposals on both synthetic and
real-world datasets.";Ching-Yu Hsu<author:sep>Cheng Sun<author:sep>Hwann-Tzong Chen;http://arxiv.org/pdf/2106.10859v1;cs.CV;;nerf
2106.10689v3;http://arxiv.org/abs/2106.10689v3;2021-06-20;NeuS: Learning Neural Implicit Surfaces by Volume Rendering for  Multi-view Reconstruction;"We present a novel neural surface reconstruction method, called NeuS, for
reconstructing objects and scenes with high fidelity from 2D image inputs.
Existing neural surface reconstruction approaches, such as DVR and IDR, require
foreground mask as supervision, easily get trapped in local minima, and
therefore struggle with the reconstruction of objects with severe
self-occlusion or thin structures. Meanwhile, recent neural methods for novel
view synthesis, such as NeRF and its variants, use volume rendering to produce
a neural scene representation with robustness of optimization, even for highly
complex objects. However, extracting high-quality surfaces from this learned
implicit representation is difficult because there are not sufficient surface
constraints in the representation. In NeuS, we propose to represent a surface
as the zero-level set of a signed distance function (SDF) and develop a new
volume rendering method to train a neural SDF representation. We observe that
the conventional volume rendering method causes inherent geometric errors (i.e.
bias) for surface reconstruction, and therefore propose a new formulation that
is free of bias in the first order of approximation, thus leading to more
accurate surface reconstruction even without the mask supervision. Experiments
on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the
state-of-the-arts in high-quality surface reconstruction, especially for
objects and scenes with complex structures and self-occlusion.";Peng Wang<author:sep>Lingjie Liu<author:sep>Yuan Liu<author:sep>Christian Theobalt<author:sep>Taku Komura<author:sep>Wenping Wang;http://arxiv.org/pdf/2106.10689v3;cs.CV;23 pages;nerf
2106.05264v1;http://arxiv.org/abs/2106.05264v1;2021-06-09;NeRF in detail: Learning to sample for view synthesis;"Neural radiance fields (NeRF) methods have demonstrated impressive novel view
synthesis performance. The core approach is to render individual rays by
querying a neural network at points sampled along the ray to obtain the density
and colour of the sampled points, and integrating this information using the
rendering equation. Since dense sampling is computationally prohibitive, a
common solution is to perform coarse-to-fine sampling.
  In this work we address a clear limitation of the vanilla coarse-to-fine
approach -- that it is based on a heuristic and not trained end-to-end for the
task at hand. We introduce a differentiable module that learns to propose
samples and their importance for the fine network, and consider and compare
multiple alternatives for its neural architecture. Training the proposal module
from scratch can be unstable due to lack of supervision, so an effective
pre-training strategy is also put forward. The approach, named `NeRF in detail'
(NeRF-ID), achieves superior view synthesis quality over NeRF and the
state-of-the-art on the synthetic Blender benchmark and on par or better
performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the
predicted sample importance, a 25% saving in computation can be achieved
without significantly sacrificing the rendering quality.";Relja Arandjelović<author:sep>Andrew Zisserman;http://arxiv.org/pdf/2106.05264v1;cs.CV;;nerf
2106.02019v2;http://arxiv.org/abs/2106.02019v2;2021-06-03;Neural Actor: Neural Free-view Synthesis of Human Actors with Pose  Control;"We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.";Lingjie Liu<author:sep>Marc Habermann<author:sep>Viktor Rudnev<author:sep>Kripasindhu Sarkar<author:sep>Jiatao Gu<author:sep>Christian Theobalt;http://arxiv.org/pdf/2106.02019v2;cs.CV;;
2106.01970v2;http://arxiv.org/abs/2106.01970v2;2021-06-03;NeRFactor: Neural Factorization of Shape and Reflectance Under an  Unknown Illumination;"We address the problem of recovering the shape and spatially-varying
reflectance of an object from multi-view images (and their camera poses) of an
object illuminated by one unknown lighting condition. This enables the
rendering of novel views of the object under arbitrary environment lighting and
editing of the object's material properties. The key to our approach, which we
call Neural Radiance Factorization (NeRFactor), is to distill the volumetric
geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020]
representation of the object into a surface representation and then jointly
refine the geometry while solving for the spatially-varying reflectance and
environment lighting. Specifically, NeRFactor recovers 3D neural fields of
surface normals, light visibility, albedo, and Bidirectional Reflectance
Distribution Functions (BRDFs) without any supervision, using only a
re-rendering loss, simple smoothness priors, and a data-driven BRDF prior
learned from real-world BRDF measurements. By explicitly modeling light
visibility, NeRFactor is able to separate shadows from albedo and synthesize
realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor
is able to recover convincing 3D models for free-viewpoint relighting in this
challenging and underconstrained capture setup for both synthetic and real
scenes. Qualitative and quantitative experiments show that NeRFactor
outperforms classic and deep learning-based state of the art across various
tasks. Our videos, code, and data are available at
people.csail.mit.edu/xiuming/projects/nerfactor/.";Xiuming Zhang<author:sep>Pratul P. Srinivasan<author:sep>Boyang Deng<author:sep>Paul Debevec<author:sep>William T. Freeman<author:sep>Jonathan T. Barron;http://arxiv.org/pdf/2106.01970v2;cs.CV;"Camera-ready version for SIGGRAPH Asia 2021. Project Page:
  https://people.csail.mit.edu/xiuming/projects/nerfactor/";nerf
2105.13016v3;http://arxiv.org/abs/2105.13016v3;2021-05-27;Stylizing 3D Scene via Implicit Representation and HyperNetwork;"In this work, we aim to address the 3D scene stylization problem - generating
stylized images of the scene at arbitrary novel view angles. A straightforward
solution is to combine existing novel view synthesis and image/video style
transfer approaches, which often leads to blurry results or inconsistent
appearance. Inspired by the high-quality results of the neural radiance fields
(NeRF) method, we propose a joint framework to directly render novel views with
the desired style. Our framework consists of two components: an implicit
representation of the 3D scene with the neural radiance fields model, and a
hypernetwork to transfer the style information into the scene representation.
In particular, our implicit representation model disentangles the scene into
the geometry and appearance branches, and the hypernetwork learns to predict
the parameters of the appearance branch from the reference style image. To
alleviate the training difficulties and memory burden, we propose a two-stage
training procedure and a patch sub-sampling approach to optimize the style and
content losses with the neural radiance fields model. After optimization, our
model is able to render consistent novel views at arbitrary view angles with
arbitrary style. Both quantitative evaluation and human subject study have
demonstrated that the proposed method generates faithful stylization results
with consistent appearance across different views.";Pei-Ze Chiang<author:sep>Meng-Shiun Tsai<author:sep>Hung-Yu Tseng<author:sep>Wei-sheng Lai<author:sep>Wei-Chen Chiu;http://arxiv.org/pdf/2105.13016v3;cs.CV;"Accepted to WACV2022; Project page:
  https://ztex08010518.github.io/3dstyletransfer/";nerf
2105.09103v1;http://arxiv.org/abs/2105.09103v1;2021-05-19;Recursive-NeRF: An Efficient and Dynamically Growing NeRF;"View synthesis methods using implicit continuous shape representations
learned from a set of images, such as the Neural Radiance Field (NeRF) method,
have gained increasing attention due to their high quality imagery and
scalability to high resolution. However, the heavy computation required by its
volumetric approach prevents NeRF from being useful in practice; minutes are
taken to render a single image of a few megapixels. Now, an image of a scene
can be rendered in a level-of-detail manner, so we posit that a complicated
region of the scene should be represented by a large neural network while a
small neural network is capable of encoding a simple region, enabling a balance
between efficiency and quality. Recursive-NeRF is our embodiment of this idea,
providing an efficient and adaptive rendering and training approach for NeRF.
The core of Recursive-NeRF learns uncertainties for query coordinates,
representing the quality of the predicted color and volumetric intensity at
each level. Only query coordinates with high uncertainties are forwarded to the
next level to a bigger neural network with a more powerful representational
capability. The final rendered image is a composition of results from neural
networks of all levels. Our evaluation on three public datasets shows that
Recursive-NeRF is more efficient than NeRF while providing state-of-the-art
quality. The code will be available at https://github.com/Gword/Recursive-NeRF.";Guo-Wei Yang<author:sep>Wen-Yang Zhou<author:sep>Hao-Yang Peng<author:sep>Dun Liang<author:sep>Tai-Jiang Mu<author:sep>Shi-Min Hu;http://arxiv.org/pdf/2105.09103v1;cs.CV;11 pages, 12 figures;nerf
2105.06466v2;http://arxiv.org/abs/2105.06466v2;2021-05-13;Editing Conditional Radiance Fields;"A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.";Steven Liu<author:sep>Xiuming Zhang<author:sep>Zhoutong Zhang<author:sep>Richard Zhang<author:sep>Jun-Yan Zhu<author:sep>Bryan Russell;http://arxiv.org/pdf/2105.06466v2;cs.CV;"Code: https://github.com/stevliu/editnerf Website:
  http://editnerf.csail.mit.edu/, v2 updated figure 8 and included additional
  details";nerf
2105.06468v1;http://arxiv.org/abs/2105.06468v1;2021-05-13;Dynamic View Synthesis from Dynamic Monocular Video;"We present an algorithm for generating novel views at arbitrary viewpoints
and any input time step given a monocular video of a dynamic scene. Our work
builds upon recent advances in neural implicit representation and uses
continuous and differentiable functions for modeling the time-varying structure
and the appearance of the scene. We jointly train a time-invariant static NeRF
and a time-varying dynamic NeRF, and learn how to blend the results in an
unsupervised manner. However, learning this implicit function from a single
video is highly ill-posed (with infinitely many solutions that match the input
video). To resolve the ambiguity, we introduce regularization losses to
encourage a more physically plausible solution. We show extensive quantitative
and qualitative results of dynamic view synthesis from casually captured
videos.";Chen Gao<author:sep>Ayush Saraf<author:sep>Johannes Kopf<author:sep>Jia-Bin Huang;http://arxiv.org/pdf/2105.06468v1;cs.CV;Project webpage: https://free-view-video.github.io/;nerf
2105.05994v1;http://arxiv.org/abs/2105.05994v1;2021-05-12;Neural Trajectory Fields for Dynamic Novel View Synthesis;"Recent approaches to render photorealistic views from a limited set of
photographs have pushed the boundaries of our interactions with pictures of
static scenes. The ability to recreate moments, that is, time-varying
sequences, is perhaps an even more interesting scenario, but it remains largely
unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for
dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input
sequence for each point in space. This allows us to enforce consistency between
any two frames in the sequence, which results in high quality reconstruction,
particularly in dynamic regions.";Chaoyang Wang<author:sep>Ben Eckart<author:sep>Simon Lucey<author:sep>Orazio Gallo;http://arxiv.org/pdf/2105.05994v1;cs.CV;;nerf
2105.06405v1;http://arxiv.org/abs/2105.06405v1;2021-05-11;Vision-based Neural Scene Representations for Spacecraft;"In advanced mission concepts with high levels of autonomy, spacecraft need to
internally model the pose and shape of nearby orbiting objects. Recent works in
neural scene representations show promising results for inferring generic
three-dimensional scenes from optical images. Neural Radiance Fields (NeRF)
have shown success in rendering highly specular surfaces using a large number
of images and their pose. More recently, Generative Radiance Fields (GRAF)
achieved full volumetric reconstruction of a scene from unposed images only,
thanks to the use of an adversarial framework to train a NeRF. In this paper,
we compare and evaluate the potential of NeRF and GRAF to render novel views
and extract the 3D shape of two different spacecraft, the Soil Moisture and
Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube
sat. Considering the best performances of both models, we observe that NeRF has
the ability to render more accurate images regarding the material specularity
of the spacecraft and its pose. For its part, GRAF generates precise novel
views with accurate details even when parts of the satellites are shadowed
while having the significant advantage of not needing any information about the
relative pose.";Anne Mergy<author:sep>Gurvan Lecuyer<author:sep>Dawa Derksen<author:sep>Dario Izzo;http://arxiv.org/pdf/2105.06405v1;cs.CV;;nerf
2105.03120v1;http://arxiv.org/abs/2105.03120v1;2021-05-07;Neural 3D Scene Compression via Model Compression;"Rendering 3D scenes requires access to arbitrary viewpoints from the scene.
Storage of such a 3D scene can be done in two ways; (1) storing 2D images taken
from the 3D scene that can reconstruct the scene back through interpolations,
or (2) storing a representation of the 3D scene itself that already encodes
views from all directions. So far, traditional 3D compression methods have
focused on the first type of storage and compressed the original 2D images with
image compression techniques. With this approach, the user first decodes the
stored 2D images and then renders the 3D scene. However, this separated
procedure is inefficient since a large amount of 2D images have to be stored.
In this work, we take a different approach and compress a functional
representation of 3D scenes. In particular, we introduce a method to compress
3D scenes by compressing the neural networks that represent the scenes as
neural radiance fields. Our method provides more efficient storage of 3D scenes
since it does not store 2D images -- which are redundant when we render the
scene from the neural functional representation.";Berivan Isik;http://arxiv.org/pdf/2105.03120v1;cs.CV;Stanford CS 231A Final Project, 2021. WiCV at CVPR 2021;
2105.02872v2;http://arxiv.org/abs/2105.02872v2;2021-05-06;Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies;"This paper addresses the challenge of reconstructing an animatable human
model from a multi-view video. Some recent works have proposed to decompose a
non-rigidly deforming scene into a canonical neural radiance field and a set of
deformation fields that map observation-space points to the canonical space,
thereby enabling them to learn the dynamic scene from images. However, they
represent the deformation field as translational vector field or SE(3) field,
which makes the optimization highly under-constrained. Moreover, these
representations cannot be explicitly controlled by input motions. Instead, we
introduce neural blend weight fields to produce the deformation fields. Based
on the skeleton-driven deformation, blend weight fields are used with 3D human
skeletons to generate observation-to-canonical and canonical-to-observation
correspondences. Since 3D human skeletons are more observable, they can
regularize the learning of deformation fields. Moreover, the learned blend
weight fields can be combined with input skeletal motions to generate new
deformation fields to animate the human model. Experiments show that our
approach significantly outperforms recent human synthesis methods. The code and
supplementary materials are available at
https://zju3dv.github.io/animatable_nerf/.";Sida Peng<author:sep>Junting Dong<author:sep>Qianqian Wang<author:sep>Shangzhan Zhang<author:sep>Qing Shuai<author:sep>Xiaowei Zhou<author:sep>Hujun Bao;http://arxiv.org/pdf/2105.02872v2;cs.CV;"Accepted to ICCV 2021. The first two authors contributed equally to
  this paper. Project page: https://zju3dv.github.io/animatable_nerf/";nerf
2104.14786v1;http://arxiv.org/abs/2104.14786v1;2021-04-30;Editable Free-viewpoint Video Using a Layered Neural Representation;"Generating free-viewpoint videos is critical for immersive VR/AR experience
but recent neural advances still lack the editing ability to manipulate the
visual perception for large dynamic scenes. To fill this gap, in this paper we
propose the first approach for editable photo-realistic free-viewpoint video
generation for large-scale dynamic scenes using only sparse 16 cameras. The
core of our approach is a new layered neural representation, where each dynamic
entity including the environment itself is formulated into a space-time
coherent neural layered radiance representation called ST-NeRF. Such layered
representation supports fully perception and realistic manipulation of the
dynamic scene whilst still supporting a free viewing experience in a wide
range. In our ST-NeRF, the dynamic entity/layer is represented as continuous
functions, which achieves the disentanglement of location, deformation as well
as the appearance of the dynamic entity in a continuous and self-supervised
manner. We propose a scene parsing 4D label map tracking to disentangle the
spatial information explicitly, and a continuous deform module to disentangle
the temporal motion implicitly. An object-aware volume rendering scheme is
further introduced for the re-assembling of all the neural layers. We adopt a
novel layered loss and motion-aware ray sampling strategy to enable efficient
training for a large dynamic scene with multiple performers, Our framework
further enables a variety of editing functions, i.e., manipulating the scale
and location, duplicating or retiming individual neural layers to create
numerous visual effects while preserving high realism. Extensive experiments
demonstrate the effectiveness of our approach to achieve high-quality,
photo-realistic, and editable free-viewpoint video generation for dynamic
scenes.";Jiakai Zhang<author:sep>Xinhang Liu<author:sep>Xinyi Ye<author:sep>Fuqiang Zhao<author:sep>Yanshun Zhang<author:sep>Minye Wu<author:sep>Yingliang Zhang<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2104.14786v1;cs.CV;;nerf
2104.09877v1;http://arxiv.org/abs/2104.09877v1;2021-04-20;Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry;"We present a new generic method for shadow-aware multi-view satellite
photogrammetry of Earth Observation scenes. Our proposed method, the Shadow
Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric
representation learning. For each scene, we train S-NeRF using very high
spatial resolution optical images taken from known viewing angles. The learning
requires no labels or shape priors: it is self-supervised by an image
reconstruction loss. To accommodate for changing light source conditions both
from a directional light source (the Sun) and a diffuse light source (the sky),
we extend the NeRF approach in two ways. First, direct illumination from the
Sun is modeled via a local light source visibility field. Second, indirect
illumination from a diffuse light source is learned as a non-local color field
as a function of the position of the Sun. Quantitatively, the combination of
these factors reduces the altitude and color errors in shaded areas, compared
to NeRF. The S-NeRF methodology not only performs novel view synthesis and full
3D shape estimation, it also enables shadow detection, albedo synthesis, and
transient object filtering, without any explicit shape supervision.";Dawa Derksen<author:sep>Dario Izzo;http://arxiv.org/pdf/2104.09877v1;cs.CV;Accepted to CVPR2021 - EarthVision;nerf
2104.10078v2;http://arxiv.org/abs/2104.10078v2;2021-04-20;UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for  Multi-View Reconstruction;"Neural implicit 3D representations have emerged as a powerful paradigm for
reconstructing surfaces from multi-view images and synthesizing novel views.
Unfortunately, existing methods such as DVR or IDR require accurate per-pixel
object masks as supervision. At the same time, neural radiance fields have
revolutionized novel view synthesis. However, NeRF's estimated volume density
does not admit accurate surface reconstruction. Our key insight is that
implicit surface models and radiance fields can be formulated in a unified way,
enabling both surface and volume rendering using the same model. This unified
perspective enables novel, more efficient sampling procedures and the ability
to reconstruct accurate surfaces without input masks. We compare our method on
the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments
demonstrate that we outperform NeRF in terms of reconstruction quality while
performing on par with IDR without requiring masks.";Michael Oechsle<author:sep>Songyou Peng<author:sep>Andreas Geiger;http://arxiv.org/pdf/2104.10078v2;cs.CV;ICCV 2021 oral;nerf
2104.08418v1;http://arxiv.org/abs/2104.08418v1;2021-04-17;FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category  Modelling;"We investigate the use of Neural Radiance Fields (NeRF) to learn high quality
3D object category models from collections of input images. In contrast to
previous work, we are able to do this whilst simultaneously separating
foreground objects from their varying backgrounds. We achieve this via a
2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a
geometrically constant background and a deformable foreground that represents
the object category. We show that this method can learn accurate 3D object
category models using only photometric supervision and casually captured images
of the objects. Additionally, our 2-part decomposition allows the model to
perform accurate and crisp amodal segmentation. We quantitatively evaluate our
method with view synthesis and image fidelity metrics, using synthetic,
lab-captured, and in-the-wild data. Our results demonstrate convincing 3D
object category modelling that exceed the performance of existing methods.";Christopher Xie<author:sep>Keunhong Park<author:sep>Ricardo Martin-Brualla<author:sep>Matthew Brown;http://arxiv.org/pdf/2104.08418v1;cs.CV;;nerf
2104.06935v1;http://arxiv.org/abs/2104.06935v1;2021-04-14;Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views  of Novel Scenes;"Recent neural view synthesis methods have achieved impressive quality and
realism, surpassing classical pipelines which rely on multi-view
reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a
single scene with a neural network and require dense multi-view inputs. Testing
on a new scene requires re-training from scratch, which takes 2-3 days. In this
work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis
approach that is trained end-to-end, generalizes to new scenes, and requires
only sparse views at test time. The core idea is a neural architecture inspired
by classical multi-view stereo methods, which estimates surface points by
finding similar image regions in stereo images. In SRF, we predict color and
density for each 3D point given an encoding of its stereo correspondence in the
input images. The encoding is implicitly learned by an ensemble of pair-wise
similarities -- emulating classical stereo. Experiments show that SRF learns
structure instead of overfitting on a scene. We train on multiple scenes of the
DTU dataset and generalize to new ones without re-training, requiring only 10
sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning
further improve the results, achieving significantly sharper, more detailed
results than scene-specific models. The code, model, and videos are available
at https://virtualhumans.mpi-inf.mpg.de/srf/.";Julian Chibane<author:sep>Aayush Bansal<author:sep>Verica Lazova<author:sep>Gerard Pons-Moll;http://arxiv.org/pdf/2104.06935v1;cs.CV;"IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2021";nerf
2104.06405v2;http://arxiv.org/abs/2104.06405v2;2021-04-13;BARF: Bundle-Adjusting Neural Radiance Fields;"Neural Radiance Fields (NeRF) have recently gained a surge of interest within
the computer vision community for its power to synthesize photorealistic novel
views of real-world scenes. One limitation of NeRF, however, is its requirement
of accurate camera poses to learn the scene representations. In this paper, we
propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from
imperfect (or even unknown) camera poses -- the joint problem of learning
neural 3D representations and registering camera frames. We establish a
theoretical connection to classical image alignment and show that
coarse-to-fine registration is also applicable to NeRF. Furthermore, we show
that na\""ively applying positional encoding in NeRF has a negative impact on
registration with a synthesis-based objective. Experiments on synthetic and
real-world data show that BARF can effectively optimize the neural scene
representations and resolve large camera pose misalignment at the same time.
This enables view synthesis and localization of video sequences from unknown
camera poses, opening up new avenues for visual localization systems (e.g.
SLAM) and potential applications for dense 3D mapping and reconstruction.";Chen-Hsuan Lin<author:sep>Wei-Chiu Ma<author:sep>Antonio Torralba<author:sep>Simon Lucey;http://arxiv.org/pdf/2104.06405v2;cs.CV;"Accepted to ICCV 2021 as oral presentation (project page & code:
  https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF)";nerf
2104.04532v3;http://arxiv.org/abs/2104.04532v3;2021-04-09;Neural RGB-D Surface Reconstruction;"Obtaining high-quality 3D reconstructions of room-scale scenes is of
paramount importance for upcoming applications in AR or VR. These range from
mixed reality applications for teleconferencing, virtual measuring, virtual
room planing, to robotic applications. While current volume-based view
synthesis methods that use neural radiance fields (NeRFs) show promising
results in reproducing the appearance of an object or scene, they do not
reconstruct an actual surface. The volumetric representation of the surface
based on densities leads to artifacts when a surface is extracted using
Marching Cubes, since during optimization, densities are accumulated along the
ray and are not used at a single sample point in isolation. Instead of this
volumetric representation of the surface, we propose to represent the surface
using an implicit function (truncated signed distance function). We show how to
incorporate this representation in the NeRF framework, and extend it to use
depth measurements from a commodity RGB-D sensor, such as a Kinect. In
addition, we propose a pose and camera refinement technique which improves the
overall reconstruction quality. In contrast to concurrent work on integrating
depth priors in NeRF which concentrates on novel view synthesis, our approach
is able to reconstruct high-quality, metrical 3D reconstructions.";Dejan Azinović<author:sep>Ricardo Martin-Brualla<author:sep>Dan B Goldman<author:sep>Matthias Nießner<author:sep>Justus Thies;http://arxiv.org/pdf/2104.04532v3;cs.CV;"CVPR'22; Project page:
  https://dazinovic.github.io/neural-rgbd-surface-reconstruction/ Video:
  https://youtu.be/iWuSowPsC3g";nerf
2104.02607v2;http://arxiv.org/abs/2104.02607v2;2021-04-06;MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror  Catadioptric Imaging;"Photo-realistic neural reconstruction and rendering of the human portrait are
critical for numerous VR/AR applications. Still, existing solutions inherently
rely on multi-view capture settings, and the one-shot solution to get rid of
the tedious multi-view synchronization and calibration remains extremely
challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait
free-viewpoint rendering approach using a catadioptric imaging system with
multiple sphere mirrors and a single high-resolution digital camera, which is
the first to combine neural radiance field with catadioptric imaging so as to
enable one-shot photo-realistic human portrait reconstruction and rendering, in
a low-cost and casual capture setting. More specifically, we propose a
light-weight catadioptric system design with a sphere mirror array to enable
diverse ray sampling in the continuous 3D space as well as an effective online
calibration for the camera and the mirror array. Our catadioptric imaging
system can be easily deployed with a low budget and the casual capture ability
for convenient daily usages. We introduce a novel neural warping radiance field
representation to learn a continuous displacement field that implicitly
compensates for the misalignment due to our flexible system setting. We further
propose a density regularization scheme to leverage the inherent geometry
information from the catadioptric data in a self-supervision manner, which not
only improves the training efficiency but also provides more effective density
supervision for higher rendering quality. Extensive experiments demonstrate the
effectiveness and robustness of our scheme to achieve one-shot photo-realistic
and high-quality appearance free-viewpoint rendering for human portrait scenes.";Ziyu Wang<author:sep>Liao Wang<author:sep>Fuqiang Zhao<author:sep>Minye Wu<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2104.02607v2;cs.CV;;nerf
2104.01772v1;http://arxiv.org/abs/2104.01772v1;2021-04-05;Convolutional Neural Opacity Radiance Fields;"Photo-realistic modeling and rendering of fuzzy objects with complex opacity
are critical for numerous immersive VR/AR applications, but it suffers from
strong view-dependent brightness, color. In this paper, we propose a novel
scheme to generate opacity radiance fields with a convolutional neural renderer
for fuzzy objects, which is the first to combine both explicit opacity
supervision and convolutional mechanism into the neural radiance field
framework so as to enable high-quality appearance and global consistent alpha
mattes generation in arbitrary novel views. More specifically, we propose an
efficient sampling strategy along with both the camera rays and image plane,
which enables efficient radiance field sampling and learning in a patch-wise
manner, as well as a novel volumetric feature integration scheme that generates
per-patch hybrid feature embeddings to reconstruct the view-consistent
fine-detailed appearance and opacity output. We further adopt a patch-wise
adversarial training scheme to preserve both high-frequency appearance and
opacity details in a self-supervised framework. We also introduce an effective
multi-view image capture system to capture high-quality color and alpha maps
for challenging fuzzy objects. Extensive experiments on existing and our new
challenging fuzzy object dataset demonstrate that our method achieves
photo-realistic, globally consistent, and fined detailed appearance and opacity
free-viewpoint rendering for various fuzzy objects.";Haimin Luo<author:sep>Anpei Chen<author:sep>Qixuan Zhang<author:sep>Bai Pang<author:sep>Minye Wu<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2104.01772v1;cs.CV;;
2104.01148v1;http://arxiv.org/abs/2104.01148v1;2021-04-02;Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation;"We present ObSuRF, a method which turns a single image of a scene into a 3D
model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF
corresponding to a different object. A single forward pass of an encoder
network outputs a set of latent vectors describing the objects in the scene.
These vectors are used independently to condition a NeRF decoder, defining the
geometry and appearance of each object. We make learning more computationally
efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs
without explicit ray marching. After confirming that the model performs equal
or better than state of the art on three 2D image segmentation benchmarks, we
apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a
novel dataset in which scenes are populated by ShapeNet models. We find that
after training ObSuRF on RGB-D views of training scenes, it is capable of not
only recovering the 3D geometry of a scene depicted in a single input image,
but also to segment it into objects, despite receiving no supervision in that
regard.";Karl Stelzner<author:sep>Kristian Kersting<author:sep>Adam R. Kosiorek;http://arxiv.org/pdf/2104.01148v1;cs.CV;"15 pages, 3 figures. For project page with videos, see
  http://stelzner.github.io/obsurf/";nerf
2104.00677v1;http://arxiv.org/abs/2104.00677v1;2021-04-01;Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis;"We present DietNeRF, a 3D neural scene representation estimated from a few
images. Neural Radiance Fields (NeRF) learn a continuous volumetric
representation of a scene through multi-view consistency, and can be rendered
from novel viewpoints by ray casting. While NeRF has an impressive ability to
reconstruct geometry and fine details given many images, up to 100 for
challenging 360{\deg} scenes, it often finds a degenerate solution to its image
reconstruction objective when only a few input views are available. To improve
few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic
consistency loss that encourages realistic renderings at novel poses. DietNeRF
is trained on individual scenes to (1) correctly render given input views from
the same pose, and (2) match high-level semantic attributes across different,
random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary
poses. We extract these semantics using a pre-trained visual encoder such as
CLIP, a Vision Transformer trained on hundreds of millions of diverse
single-view, 2D photographs mined from the web with natural language
supervision. In experiments, DietNeRF improves the perceptual quality of
few-shot view synthesis when learned from scratch, can render novel views with
as few as one observed image when pre-trained on a multi-view dataset, and
produces plausible completions of completely unobserved regions.";Ajay Jain<author:sep>Matthew Tancik<author:sep>Pieter Abbeel;http://arxiv.org/pdf/2104.00677v1;cs.CV;Project website: https://www.ajayj.com/dietnerf;nerf
2104.00587v1;http://arxiv.org/abs/2104.00587v1;2021-04-01;NeRF-VAE: A Geometry Aware 3D Scene Generative Model;"We propose NeRF-VAE, a 3D scene generative model that incorporates geometric
structure via NeRF and differentiable volume rendering. In contrast to NeRF,
our model takes into account shared structure across scenes, and is able to
infer the structure of a novel scene -- without the need to re-train -- using
amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts
previous generative models with convolution-based rendering which lacks
geometric structure. Our model is a VAE that learns a distribution over
radiance fields by conditioning them on a latent scene representation. We show
that, once trained, NeRF-VAE is able to infer and render
geometrically-consistent scenes from previously unseen 3D environments using
very few input images. We further demonstrate that NeRF-VAE generalizes well to
out-of-distribution cameras, while convolutional models do not. Finally, we
introduce and study an attention-based conditioning mechanism of NeRF-VAE's
decoder, which improves model performance.";Adam R. Kosiorek<author:sep>Heiko Strathmann<author:sep>Daniel Zoran<author:sep>Pol Moreno<author:sep>Rosalia Schneider<author:sep>Soňa Mokrá<author:sep>Danilo J. Rezende;http://arxiv.org/pdf/2104.00587v1;stat.ML;17 pages, 15 figures, under review;nerf
2103.17269v1;http://arxiv.org/abs/2103.17269v1;2021-03-31;CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields;"Tremendous progress in deep generative models has led to photorealistic image
synthesis. While achieving compelling results, most approaches operate in the
two-dimensional image domain, ignoring the three-dimensional nature of our
world. Several recent works therefore propose generative models which are
3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to
the image plane. This leads to impressive 3D consistency, but incorporating
such a bias comes at a price: the camera needs to be modeled as well. Current
approaches assume fixed intrinsics and a predefined prior over camera pose
ranges. As a result, parameter tuning is typically required for real-world
data, and results degrade if the data distribution is not matched. Our key
hypothesis is that learning a camera generator jointly with the image generator
leads to a more principled approach to 3D-aware image synthesis. Further, we
propose to decompose the scene into a background and foreground model, leading
to more efficient and disentangled scene representations. While training from
raw, unposed image collections, we learn a 3D- and camera-aware generative
model which faithfully recovers not only the image but also the camera data
distribution. At test time, our model generates images with explicit control
over the camera as well as the shape and appearance of the scene.";Michael Niemeyer<author:sep>Andreas Geiger;http://arxiv.org/pdf/2103.17269v1;cs.CV;;
2103.16365v2;http://arxiv.org/abs/2103.16365v2;2021-03-30;FoV-NeRF: Foveated Neural Radiance Fields for Virtual Reality;"Virtual Reality (VR) is becoming ubiquitous with the rise of consumer
displays and commercial VR platforms. Such displays require low latency and
high quality rendering of synthetic imagery with reduced compute overheads.
Recent advances in neural rendering showed promise of unlocking new
possibilities in 3D computer graphics via image-based representations of
virtual or physical environments. Specifically, the neural radiance fields
(NeRF) demonstrated that photo-realistic quality and continuous view changes of
3D scenes can be achieved without loss of view-dependent effects. While NeRF
can significantly benefit rendering for VR applications, it faces unique
challenges posed by high field-of-view, high resolution, and
stereoscopic/egocentric viewing, typically causing low quality and high latency
of the rendered images. In VR, this not only harms the interaction experience
but may also cause sickness. To tackle these problems toward
six-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first
gaze-contingent 3D neural representation and view synthesis method. We
incorporate the human psychophysics of visual- and stereo-acuity into an
egocentric neural representation of 3D scenery. We then jointly optimize the
latency/performance and visual quality while mutually bridging human perception
and neural scene synthesis to achieve perceptually high-quality immersive
interaction. We conducted both objective analysis and subjective studies to
evaluate the effectiveness of our approach. We find that our method
significantly reduces latency (up to 99% time reduction compared with NeRF)
without loss of high-fidelity rendering (perceptually identical to
full-resolution ground truth). The presented approach may serve as the first
step toward future VR/AR systems that capture, teleport, and visualize remote
environments in real-time.";Nianchen Deng<author:sep>Zhenyi He<author:sep>Jiannan Ye<author:sep>Budmonde Duinkharjav<author:sep>Praneeth Chakravarthula<author:sep>Xubo Yang<author:sep>Qi Sun;http://arxiv.org/pdf/2103.16365v2;cs.GR;9 pages;nerf
2103.15595v2;http://arxiv.org/abs/2103.15595v2;2021-03-29;MVSNeRF: Fast Generalizable Radiance Field Reconstruction from  Multi-View Stereo;"We present MVSNeRF, a novel neural rendering approach that can efficiently
reconstruct neural radiance fields for view synthesis. Unlike prior works on
neural radiance fields that consider per-scene optimization on densely captured
images, we propose a generic deep neural network that can reconstruct radiance
fields from only three nearby input views via fast network inference. Our
approach leverages plane-swept cost volumes (widely used in multi-view stereo)
for geometry-aware scene reasoning, and combines this with physically based
volume rendering for neural radiance field reconstruction. We train our network
on real objects in the DTU dataset, and test it on three different datasets to
evaluate its effectiveness and generalizability. Our approach can generalize
across scenes (even indoor scenes, completely different from our training
scenes of objects) and generate realistic view synthesis results using only
three input images, significantly outperforming concurrent works on
generalizable radiance field reconstruction. Moreover, if dense images are
captured, our estimated radiance field representation can be easily fine-tuned;
this leads to fast per-scene reconstruction with higher rendering quality and
substantially less optimization time than NeRF.";Anpei Chen<author:sep>Zexiang Xu<author:sep>Fuqiang Zhao<author:sep>Xiaoshuai Zhang<author:sep>Fanbo Xiang<author:sep>Jingyi Yu<author:sep>Hao Su;http://arxiv.org/pdf/2103.15595v2;cs.CV;"Project Page: https://apchenstu.github.io/mvsnerf/
  Code:https://github.com/apchenstu/mvsnerf";nerf
2103.15606v3;http://arxiv.org/abs/2103.15606v3;2021-03-29;GNeRF: GAN-based Neural Radiance Field without Posed Camera;"We introduce GNeRF, a framework to marry Generative Adversarial Networks
(GAN) with Neural Radiance Field (NeRF) reconstruction for the complex
scenarios with unknown and even randomly initialized camera poses. Recent
NeRF-based advances have gained popularity for remarkable realistic novel view
synthesis. However, most of them heavily rely on accurate camera poses
estimation, while few recent methods can only optimize the unknown camera poses
in roughly forward-facing scenes with relatively short camera trajectories and
require rough camera poses initialization. Differently, our GNeRF only utilizes
randomly initialized poses for complex outside-in scenarios. We propose a novel
two-phases end-to-end framework. The first phase takes the use of GANs into the
new realm for optimizing coarse camera poses and radiance fields jointly, while
the second phase refines them with additional photometric loss. We overcome
local minima using a hybrid and iterative optimization scheme. Extensive
experiments on a variety of synthetic and natural scenes demonstrate the
effectiveness of GNeRF. More impressively, our approach outperforms the
baselines favorably in those scenes with repeated patterns or even low textures
that are regarded as extremely challenging before.";Quan Meng<author:sep>Anpei Chen<author:sep>Haimin Luo<author:sep>Minye Wu<author:sep>Hao Su<author:sep>Lan Xu<author:sep>Xuming He<author:sep>Jingyi Yu;http://arxiv.org/pdf/2103.15606v3;cs.CV;ICCV 2021 (Oral);nerf
2103.15875v2;http://arxiv.org/abs/2103.15875v2;2021-03-29;In-Place Scene Labelling and Understanding with Implicit Scene  Representation;"Semantic labelling is highly correlated with geometry and radiance
reconstruction, as scene entities with similar shape and appearance are more
likely to come from similar classes. Recent implicit neural reconstruction
techniques are appealing as they do not require prior training data, but the
same fully self-supervised approach is not possible for semantics because
labels are human-defined properties.
  We extend neural radiance fields (NeRF) to jointly encode semantics with
appearance and geometry, so that complete and accurate 2D semantic labels can
be achieved using a small amount of in-place annotations specific to the scene.
The intrinsic multi-view consistency and smoothness of NeRF benefit semantics
by enabling sparse labels to efficiently propagate. We show the benefit of this
approach when labels are either sparse or very noisy in room-scale scenes. We
demonstrate its advantageous properties in various interesting applications
such as an efficient scene labelling tool, novel semantic view synthesis, label
denoising, super-resolution, label interpolation and multi-view semantic label
fusion in visual semantic mapping systems.";Shuaifeng Zhi<author:sep>Tristan Laidlow<author:sep>Stefan Leutenegger<author:sep>Andrew J. Davison;http://arxiv.org/pdf/2103.15875v2;cs.CV;"Camera ready version. To be published in Proceedings of IEEE
  International Conference on Computer Vision (ICCV 2021) as Oral Presentation.
  Project page with more videos: https://shuaifengzhi.com/Semantic-NeRF/";nerf
2103.14910v3;http://arxiv.org/abs/2103.14910v3;2021-03-27;MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis;"In this paper, we propose MINE to perform novel view synthesis and depth
estimation via dense 3D reconstruction from a single image. Our approach is a
continuous depth generalization of the Multiplane Images (MPI) by introducing
the NEural radiance fields (NeRF). Given a single image as input, MINE predicts
a 4-channel image (RGB and volume density) at arbitrary depth values to jointly
reconstruct the camera frustum and fill in occluded contents. The reconstructed
and inpainted frustum can then be easily rendered into novel RGB or depth views
using differentiable rendering. Extensive experiments on RealEstate10K, KITTI
and Flowers Light Fields show that our MINE outperforms state-of-the-art by a
large margin in novel view synthesis. We also achieve competitive results in
depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our
source code is available at https://github.com/vincentfung13/MINE";Jiaxin Li<author:sep>Zijian Feng<author:sep>Qi She<author:sep>Henghui Ding<author:sep>Changhu Wang<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2103.14910v3;cs.CV;ICCV 2021. Main paper and supplementary materials;nerf
2103.14645v1;http://arxiv.org/abs/2103.14645v1;2021-03-26;Baking Neural Radiance Fields for Real-Time View Synthesis;"Neural volumetric representations such as Neural Radiance Fields (NeRF) have
emerged as a compelling technique for learning to represent 3D scenes from
images with the goal of rendering photorealistic images of the scene from
unobserved viewpoints. However, NeRF's computational requirements are
prohibitive for real-time applications: rendering views from a trained NeRF
requires querying a multilayer perceptron (MLP) hundreds of times per ray. We
present a method to train a NeRF, then precompute and store (i.e. ""bake"") it as
a novel representation called a Sparse Neural Radiance Grid (SNeRG) that
enables real-time rendering on commodity hardware. To achieve this, we
introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid
representation with learned feature vectors. The resulting scene representation
retains NeRF's ability to render fine geometric details and view-dependent
appearance, is compact (averaging less than 90 MB per scene), and can be
rendered in real-time (higher than 30 frames per second on a laptop GPU).
Actual screen captures are shown in our video.";Peter Hedman<author:sep>Pratul P. Srinivasan<author:sep>Ben Mildenhall<author:sep>Jonathan T. Barron<author:sep>Paul Debevec;http://arxiv.org/pdf/2103.14645v1;cs.CV;Project page: https://nerf.live;nerf
2103.13744v2;http://arxiv.org/abs/2103.13744v2;2021-03-25;KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs;"NeRF synthesizes novel views of a scene with unprecedented quality by fitting
a neural radiance field to RGB images. However, NeRF requires querying a deep
Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering
times, even on modern GPUs. In this paper, we demonstrate that real-time
rendering is possible by utilizing thousands of tiny MLPs instead of one single
large MLP. In our setting, each individual MLP only needs to represent parts of
the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining
this divide-and-conquer strategy with further optimizations, rendering is
accelerated by three orders of magnitude compared to the original NeRF model
without incurring high storage costs. Further, using teacher-student
distillation for training, we show that this speed-up can be achieved without
sacrificing visual quality.";Christian Reiser<author:sep>Songyou Peng<author:sep>Yiyi Liao<author:sep>Andreas Geiger;http://arxiv.org/pdf/2103.13744v2;cs.CV;"ICCV 2021. Code, pretrained models and an interactive viewer are
  available at https://github.com/creiser/kilonerf/";nerf
2103.14024v2;http://arxiv.org/abs/2103.14024v2;2021-03-25;PlenOctrees for Real-time Rendering of Neural Radiance Fields;"We introduce a method to render Neural Radiance Fields (NeRFs) in real time
using PlenOctrees, an octree-based 3D representation which supports
view-dependent effects. Our method can render 800x800 images at more than 150
FPS, which is over 3000 times faster than conventional NeRFs. We do so without
sacrificing quality while preserving the ability of NeRFs to perform
free-viewpoint rendering of scenes with arbitrary geometry and view-dependent
effects. Real-time performance is achieved by pre-tabulating the NeRF into a
PlenOctree. In order to preserve view-dependent effects such as specularities,
we factorize the appearance via closed-form spherical basis functions.
Specifically, we show that it is possible to train NeRFs to predict a spherical
harmonic representation of radiance, removing the viewing direction as an input
to the neural network. Furthermore, we show that PlenOctrees can be directly
optimized to further minimize the reconstruction loss, which leads to equal or
better quality compared to competing methods. Moreover, this octree
optimization step can be used to reduce the training time, as we no longer need
to wait for the NeRF training to converge fully. Our real-time neural rendering
approach may potentially enable new applications such as 6-DOF industrial and
product visualizations, as well as next generation AR/VR systems. PlenOctrees
are amenable to in-browser rendering as well; please visit the project page for
the interactive online demo, as well as video and code:
https://alexyu.net/plenoctrees";Alex Yu<author:sep>Ruilong Li<author:sep>Matthew Tancik<author:sep>Hao Li<author:sep>Ren Ng<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2103.14024v2;cs.CV;ICCV 2021 (Oral);nerf
2103.13415v3;http://arxiv.org/abs/2103.13415v3;2021-03-24;Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance  Fields;"The rendering procedure used by neural radiance fields (NeRF) samples a scene
with a single ray per pixel and may therefore produce renderings that are
excessively blurred or aliased when training or testing images observe scene
content at different resolutions. The straightforward solution of supersampling
by rendering with multiple rays per pixel is impractical for NeRF, because
rendering each ray requires querying a multilayer perceptron hundreds of times.
Our solution, which we call ""mip-NeRF"" (a la ""mipmap""), extends NeRF to
represent the scene at a continuously-valued scale. By efficiently rendering
anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable
aliasing artifacts and significantly improves NeRF's ability to represent fine
details, while also being 7% faster than NeRF and half the size. Compared to
NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with
NeRF and by 60% on a challenging multiscale variant of that dataset that we
present. Mip-NeRF is also able to match the accuracy of a brute-force
supersampled NeRF on our multiscale dataset while being 22x faster.";Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Matthew Tancik<author:sep>Peter Hedman<author:sep>Ricardo Martin-Brualla<author:sep>Pratul P. Srinivasan;http://arxiv.org/pdf/2103.13415v3;cs.CV;;nerf
2103.12716v2;http://arxiv.org/abs/2103.12716v2;2021-03-23;UltraSR: Spatial Encoding is a Missing Key for Implicit Image  Function-based Arbitrary-Scale Super-Resolution;"The recent success of NeRF and other related implicit neural representation
methods has opened a new path for continuous image representation, where pixel
values no longer need to be looked up from stored discrete 2D arrays but can be
inferred from neural network models on a continuous spatial domain. Although
the recent work LIIF has demonstrated that such novel approaches can achieve
good performance on the arbitrary-scale super-resolution task, their upscaled
images frequently show structural distortion due to the inaccurate prediction
of high-frequency textures. In this work, we propose UltraSR, a simple yet
effective new network design based on implicit image functions in which we
deeply integrated spatial coordinates and periodic encoding with the implicit
neural representation. Through extensive experiments and ablation studies, we
show that spatial encoding is a missing key toward the next-stage
high-performing implicit image function. Our UltraSR sets new state-of-the-art
performance on the DIV2K benchmark under all super-resolution scales compared
to previous state-of-the-art methods. UltraSR also achieves superior
performance on other standard benchmark datasets in which it outperforms prior
works in almost all experiments.";Xingqian Xu<author:sep>Zhangyang Wang<author:sep>Humphrey Shi;http://arxiv.org/pdf/2103.12716v2;cs.CV;;nerf
2103.11078v3;http://arxiv.org/abs/2103.11078v3;2021-03-20;AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis;"Generating high-fidelity talking head video by fitting with the input audio
sequence is a challenging problem that receives considerable attentions
recently. In this paper, we address this problem with the aid of neural scene
representation networks. Our method is completely different from existing
methods that rely on intermediate representations like 2D landmarks or 3D face
models to bridge the gap between audio input and video output. Specifically,
the feature of input audio signal is directly fed into a conditional implicit
function to generate a dynamic neural radiance field, from which a
high-fidelity talking-head video corresponding to the audio signal is
synthesized using volume rendering. Another advantage of our framework is that
not only the head (with hair) region is synthesized as previous methods did,
but also the upper body is generated via two individual neural radiance fields.
Experimental results demonstrate that our novel framework can (1) produce
high-fidelity and natural results, and (2) support free adjustment of audio
signals, viewing directions, and background images. Code is available at
https://github.com/YudongGuo/AD-NeRF.";Yudong Guo<author:sep>Keyu Chen<author:sep>Sen Liang<author:sep>Yong-Jin Liu<author:sep>Hujun Bao<author:sep>Juyong Zhang;http://arxiv.org/pdf/2103.11078v3;cs.CV;"Project: https://yudongguo.github.io/ADNeRF/ Code:
  https://github.com/YudongGuo/AD-NeRF";nerf
2103.10380v2;http://arxiv.org/abs/2103.10380v2;2021-03-18;FastNeRF: High-Fidelity Neural Rendering at 200FPS;"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can
be used to encode complex 3D environments that can be rendered
photorealistically from novel viewpoints. Rendering these images is very
computationally demanding and recent improvements are still a long way from
enabling interactive rates, even on high-end hardware. Motivated by scenarios
on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based
system capable of rendering high fidelity photorealistic images at 200Hz on a
high-end consumer GPU. The core of our method is a graphics-inspired
factorization that allows for (i) compactly caching a deep radiance map at each
position in space, (ii) efficiently querying that map using ray directions to
estimate the pixel values in the rendered image. Extensive experiments show
that the proposed method is 3000 times faster than the original NeRF algorithm
and at least an order of magnitude faster than existing work on accelerating
NeRF, while maintaining visual quality and extensibility.";Stephan J. Garbin<author:sep>Marek Kowalski<author:sep>Matthew Johnson<author:sep>Jamie Shotton<author:sep>Julien Valentin;http://arxiv.org/pdf/2103.10380v2;cs.CV;"main paper: 10 pages, 6 figures; supplementary: 10 pages, 17 figures";nerf
2103.03231v4;http://arxiv.org/abs/2103.03231v4;2021-03-04;DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields  using Depth Oracle Networks;"The recent research explosion around implicit neural representations, such as
NeRF, shows that there is immense potential for implicitly storing high-quality
scene and lighting information in compact neural networks. However, one major
limitation preventing the use of NeRF in real-time rendering applications is
the prohibitive computational cost of excessive network evaluations along each
view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural
representations closer to practical rendering of synthetic content in real-time
applications, such as games and virtual reality. We show that the number of
samples required for each view ray can be significantly reduced when samples
are placed around surfaces in the scene without compromising image quality. To
this end, we propose a depth oracle network that predicts ray sample locations
for each view ray with a single network evaluation. We show that using a
classification network around logarithmically discretized and spherically
warped depth values is essential to encode surface locations rather than
directly estimating depth. The combination of these techniques leads to DONeRF,
our compact dual network design with a depth oracle network as its first step
and a locally sampled shading network for ray accumulation. With DONeRF, we
reduce the inference costs by up to 48x compared to NeRF when conditioning on
available ground truth depth information. Compared to concurrent acceleration
methods for raymarching-based neural representations, DONeRF does not require
additional memory for explicit caching or acceleration structures, and can
render interactively (20 frames per second) on a single GPU.";Thomas Neff<author:sep>Pascal Stadlbauer<author:sep>Mathias Parger<author:sep>Andreas Kurz<author:sep>Joerg H. Mueller<author:sep>Chakravarty R. Alla Chaitanya<author:sep>Anton Kaplanyan<author:sep>Markus Steinberger;http://arxiv.org/pdf/2103.03231v4;cs.CV;"Accepted to EGSR 2021 in the CGF track; Project website:
  https://depthoraclenerf.github.io/";nerf
2103.02597v2;http://arxiv.org/abs/2103.02597v2;2021-03-03;Neural 3D Video Synthesis from Multi-view Video;"We propose a novel approach for 3D video synthesis that is able to represent
multi-view video recordings of a dynamic real-world scene in a compact, yet
expressive representation that enables high-quality view synthesis and motion
interpolation. Our approach takes the high quality and compactness of static
neural radiance fields in a new direction: to a model-free, dynamic setting. At
the core of our approach is a novel time-conditioned neural radiance field that
represents scene dynamics using a set of compact latent codes. We are able to
significantly boost the training speed and perceptual quality of the generated
imagery by a novel hierarchical training scheme in combination with ray
importance sampling. Our learned representation is highly compact and able to
represent a 10 second 30 FPS multiview video recording by 18 cameras with a
model size of only 28MB. We demonstrate that our method can render
high-fidelity wide-angle novel views at over 1K resolution, even for complex
and dynamic scenes. We perform an extensive qualitative and quantitative
evaluation that shows that our approach outperforms the state of the art.
Project website: https://neural-3d-video.github.io/.";Tianye Li<author:sep>Mira Slavcheva<author:sep>Michael Zollhoefer<author:sep>Simon Green<author:sep>Christoph Lassner<author:sep>Changil Kim<author:sep>Tanner Schmidt<author:sep>Steven Lovegrove<author:sep>Michael Goesele<author:sep>Richard Newcombe<author:sep>Zhaoyang Lv;http://arxiv.org/pdf/2103.02597v2;cs.CV;"Accepted as an oral presentation for CVPR 2022. Project website:
  https://neural-3d-video.github.io/";
2103.01954v2;http://arxiv.org/abs/2103.01954v2;2021-03-02;Mixture of Volumetric Primitives for Efficient Neural Rendering;"Real-time rendering and animation of humans is a core function in games,
movies, and telepresence applications. Existing methods have a number of
drawbacks we aim to address with our work. Triangle meshes have difficulty
modeling thin structures like hair, volumetric representations like Neural
Volumes are too low-resolution given a reasonable memory budget, and
high-resolution implicit representations like Neural Radiance Fields are too
slow for use in real-time applications. We present Mixture of Volumetric
Primitives (MVP), a representation for rendering dynamic 3D content that
combines the completeness of volumetric representations with the efficiency of
primitive-based rendering, e.g., point-based or mesh-based methods. Our
approach achieves this by leveraging spatially shared computation with a
deconvolutional architecture and by minimizing computation in empty regions of
space with volumetric primitives that can move to cover only occupied regions.
Our parameterization supports the integration of correspondence and tracking
constraints, while being robust to areas where classical tracking fails, such
as around thin or translucent structures and areas with large topological
variability. MVP is a hybrid that generalizes both volumetric and
primitive-based representations. Through a series of extensive experiments we
demonstrate that it inherits the strengths of each, while avoiding many of
their limitations. We also compare our approach to several state-of-the-art
methods and demonstrate that MVP produces superior results in terms of quality
and runtime performance.";Stephen Lombardi<author:sep>Tomas Simon<author:sep>Gabriel Schwartz<author:sep>Michael Zollhoefer<author:sep>Yaser Sheikh<author:sep>Jason Saragih;http://arxiv.org/pdf/2103.01954v2;cs.GR;"13 pages; SIGGRAPH 2021";
2102.07064v4;http://arxiv.org/abs/2102.07064v4;2021-02-14;NeRF--: Neural Radiance Fields Without Known Camera Parameters;"Considering the problem of novel view synthesis (NVS) from only a set of 2D
images, we simplify the training process of Neural Radiance Field (NeRF) on
forward-facing scenes by removing the requirement of known or pre-computed
camera parameters, including both intrinsics and 6DoF poses. To this end, we
propose NeRF$--$, with three contributions: First, we show that the camera
parameters can be jointly optimised as learnable parameters with NeRF training,
through a photometric reconstruction; Second, to benchmark the camera parameter
estimation and the quality of novel view renderings, we introduce a new dataset
of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset
(BLEFF); Third, we conduct extensive analyses to understand the training
behaviours under various camera motions, and show that in most scenarios, the
joint optimisation pipeline can recover accurate camera parameters and achieve
comparable novel view synthesis quality as those trained with COLMAP
pre-computed camera parameters. Our code and data are available at
https://nerfmm.active.vision.";Zirui Wang<author:sep>Shangzhe Wu<author:sep>Weidi Xie<author:sep>Min Chen<author:sep>Victor Adrian Prisacariu;http://arxiv.org/pdf/2102.07064v4;cs.CV;"Project page see https://nerfmm.active.vision. Add a break point
  analysis experiment and release a BLEFF dataset";nerf
2102.06199v3;http://arxiv.org/abs/2102.06199v3;2021-02-11;A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape,  Appearance, and Pose;"While deep learning reshaped the classical motion capture pipeline with
feed-forward networks, generative models are required to recover fine alignment
via iterative refinement. Unfortunately, the existing models are usually
hand-crafted or learned in controlled conditions, only applicable to limited
domains. We propose a method to learn a generative neural body model from
unlabelled monocular videos by extending Neural Radiance Fields (NeRFs). We
equip them with a skeleton to apply to time-varying and articulated motion. A
key insight is that implicit models require the inverse of the forward
kinematics used in explicit surface models. Our reparameterization defines
spatial latent variables relative to the pose of body parts and thereby
overcomes ill-posed inverse operations with an overparameterization. This
enables learning volumetric body shape and appearance from scratch while
jointly refining the articulated pose; all without ground truth labels for
appearance, pose, or 3D shape on the input videos. When used for
novel-view-synthesis and motion capture, our neural model improves accuracy on
diverse datasets. Project website: https://lemonatsu.github.io/anerf/ .";Shih-Yang Su<author:sep>Frank Yu<author:sep>Michael Zollhoefer<author:sep>Helge Rhodin;http://arxiv.org/pdf/2102.06199v3;cs.CV;NeurIPS 2021. Project website: https://lemonatsu.github.io/anerf/;nerf
2102.04281v3;http://arxiv.org/abs/2102.04281v3;2021-02-08;Conditions de Kan sur les nerfs des $ω$-catégories;"We show that the Street nerve of a strict $\omega$-category $C$ is a Kan
complex (respectively a quasi-category) if and only if the $n$-cells of $C$ for
$n\geq 1$ (respectively $n> 1$) are weakly invertible. Moreover, we equip
$\mathcal{N}(C)$ with a structure of saturated complicial set where the
$n$-simplices correspond to morphisms from the $n^{th}$ oriental to $C$ sending
the unique non-trivial $n$-cell of the domain to a weakly invertible cell of
$C$.";Félix Loubaton;http://arxiv.org/pdf/2102.04281v3;math.CT;52 pages, in French;nerf
2101.02697v1;http://arxiv.org/abs/2101.02697v1;2021-01-07;PVA: Pixel-aligned Volumetric Avatars;"Acquisition and rendering of photo-realistic human heads is a highly
challenging research problem of particular importance for virtual telepresence.
Currently, the highest quality is achieved by volumetric approaches trained in
a person specific manner on multi-view data. These models better represent fine
structure, such as hair, compared to simpler mesh-based models. Volumetric
models typically employ a global code to represent facial expressions, such
that they can be driven by a small set of animation parameters. While such
architectures achieve impressive rendering quality, they can not easily be
extended to the multi-identity setting. In this paper, we devise a novel
approach for predicting volumetric avatars of the human head given just a small
number of inputs. We enable generalization across identities by a novel
parameterization that combines neural radiance fields with local, pixel-aligned
features extracted directly from the inputs, thus sidestepping the need for
very deep or complex networks. Our approach is trained in an end-to-end manner
solely based on a photometric re-rendering loss without requiring explicit 3D
supervision.We demonstrate that our approach outperforms the existing state of
the art in terms of quality and is able to generate faithful facial expressions
in a multi-identity setting.";Amit Raj<author:sep>Michael Zollhoefer<author:sep>Tomas Simon<author:sep>Jason Saragih<author:sep>Shunsuke Saito<author:sep>James Hays<author:sep>Stephen Lombardi;http://arxiv.org/pdf/2101.02697v1;cs.CV;Project page located at https://volumetric-avatars.github.io/;
2101.00373v3;http://arxiv.org/abs/2101.00373v3;2021-01-02;Non-line-of-Sight Imaging via Neural Transient Fields;"We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging.
Previous solutions have sought to explicitly recover the 3D geometry (e.g., as
point clouds) or voxel density (e.g., within a pre-defined volume) of the
hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF)
approach, we use a multi-layer perceptron (MLP) to represent the neural
transient field or NeTF. However, NeTF measures the transient over spherical
wavefronts rather than the radiance along lines. We therefore formulate a
spherical volume NeTF reconstruction pipeline, applicable to both confocal and
non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of
viewpoints (scanning spots) and the sampling is highly uneven. We thus
introduce a Monte Carlo technique to improve the robustness in the
reconstruction. Comprehensive experiments on synthetic and real datasets
demonstrate NeTF provides higher quality reconstruction and preserves fine
details largely missing in the state-of-the-art.";Siyuan Shen<author:sep>Zi Wang<author:sep>Ping Liu<author:sep>Zhengqing Pan<author:sep>Ruiqian Li<author:sep>Tian Gao<author:sep>Shiying Li<author:sep>Jingyi Yu;http://arxiv.org/pdf/2101.00373v3;eess.IV;;nerf
2101.01602v1;http://arxiv.org/abs/2101.01602v1;2020-12-22;STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in  Motion with Neural Rendering;"We present STaR, a novel method that performs Self-supervised Tracking and
Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos
without any manual annotation. Recent work has shown that neural networks are
surprisingly effective at the task of compressing many views of a scene into a
learned function which maps from a viewing ray to an observed radiance value
via volume rendering. Unfortunately, these methods lose all their predictive
power once any object in the scene has moved. In this work, we explicitly model
rigid motion of objects in the context of neural representations of radiance
fields. We show that without any additional human specified supervision, we can
reconstruct a dynamic scene with a single rigid object in motion by
simultaneously decomposing it into its two constituent parts and encoding each
with its own neural representation. We achieve this by jointly optimizing the
parameters of two neural radiance fields and a set of rigid poses which align
the two fields at each frame. On both synthetic and real world datasets, we
demonstrate that our method can render photorealistic novel views, where
novelty is measured on both spatial and temporal axes. Our factored
representation furthermore enables animation of unseen object motion.";Wentao Yuan<author:sep>Zhaoyang Lv<author:sep>Tanner Schmidt<author:sep>Steven Lovegrove;http://arxiv.org/pdf/2101.01602v1;cs.CV;;
2012.12247v4;http://arxiv.org/abs/2012.12247v4;2020-12-22;Non-Rigid Neural Radiance Fields: Reconstruction and Novel View  Synthesis of a Dynamic Scene From Monocular Video;"We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and
novel view synthesis approach for general non-rigid dynamic scenes. Our
approach takes RGB images of a dynamic scene as input (e.g., from a monocular
video recording), and creates a high-quality space-time geometry and appearance
representation. We show that a single handheld consumer-grade camera is
sufficient to synthesize sophisticated renderings of a dynamic scene from novel
virtual camera views, e.g. a `bullet-time' video effect. NR-NeRF disentangles
the dynamic scene into a canonical volume and its deformation. Scene
deformation is implemented as ray bending, where straight rays are deformed
non-rigidly. We also propose a novel rigidity network to better constrain rigid
regions of the scene, leading to more stable results. The ray bending and
rigidity network are trained without explicit supervision. Our formulation
enables dense correspondence estimation across views and time, and compelling
video editing applications such as motion exaggeration. Our code will be open
sourced.";Edgar Tretschk<author:sep>Ayush Tewari<author:sep>Vladislav Golyanik<author:sep>Michael Zollhöfer<author:sep>Christoph Lassner<author:sep>Christian Theobalt;http://arxiv.org/pdf/2012.12247v4;cs.CV;"Project page (incl. supplemental videos and code):
  https://vcai.mpi-inf.mpg.de/projects/nonrigid_nerf/ or
  https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/";nerf
2101.05204v2;http://arxiv.org/abs/2101.05204v2;2020-12-17;Neural Volume Rendering: NeRF And Beyond;"Besides the COVID-19 pandemic and political upheaval in the US, 2020 was also
the year in which neural volume rendering exploded onto the scene, triggered by
the impressive NeRF paper by Mildenhall et al. (2020). Both of us have tried to
capture this excitement, Frank on a blog post (Dellaert, 2020) and Yen-Chen in
a Github collection (Yen-Chen, 2020). This note is an annotated bibliography of
the relevant papers, and we posted the associated bibtex file on the
repository.";Frank Dellaert<author:sep>Lin Yen-Chen;http://arxiv.org/pdf/2101.05204v2;cs.CV;"Blog: https://dellaert.github.io/NeRF/ Bibtex:
  https://github.com/yenchenlin/awesome-NeRF";nerf
2012.09955v1;http://arxiv.org/abs/2012.09955v1;2020-12-17;Learning Compositional Radiance Fields of Dynamic Human Heads;"Photorealistic rendering of dynamic humans is an important ability for
telepresence systems, virtual shopping, synthetic data generation, and more.
Recently, neural rendering methods, which combine techniques from computer
graphics and machine learning, have created high-fidelity models of humans and
objects. Some of these methods do not produce results with high-enough fidelity
for driveable human models (Neural Volumes) whereas others have extremely long
rendering times (NeRF). We propose a novel compositional 3D representation that
combines the best of previous methods to produce both higher-resolution and
faster results. Our representation bridges the gap between discrete and
continuous volumetric representations by combining a coarse 3D-structure-aware
grid of animation codes with a continuous learned scene function that maps
every position and its corresponding local animation code to its view-dependent
emitted radiance and local volume density. Differentiable volume rendering is
employed to compute photo-realistic novel views of the human head and upper
body as well as to train our novel representation end-to-end using only 2D
supervision. In addition, we show that the learned dynamic radiance field can
be used to synthesize novel unseen expressions based on a global animation
code. Our approach achieves state-of-the-art results for synthesizing novel
views of dynamic human heads and the upper body.";Ziyan Wang<author:sep>Timur Bagautdinov<author:sep>Stephen Lombardi<author:sep>Tomas Simon<author:sep>Jason Saragih<author:sep>Jessica Hodgins<author:sep>Michael Zollhöfer;http://arxiv.org/pdf/2012.09955v1;cs.CV;;nerf
2012.08503v1;http://arxiv.org/abs/2012.08503v1;2020-12-15;Object-Centric Neural Scene Rendering;"We present a method for composing photorealistic scenes from captured images
of objects. Our work builds upon neural radiance fields (NeRFs), which
implicitly model the volumetric density and directionally-emitted radiance of a
scene. While NeRFs synthesize realistic pictures, they only model static scenes
and are closely tied to specific imaging conditions. This property makes NeRFs
hard to generalize to new scenarios, including new lighting or new arrangements
of objects. Instead of learning a scene radiance field as a NeRF does, we
propose to learn object-centric neural scattering functions (OSFs), a
representation that models per-object light transport implicitly using a
lighting- and view-dependent neural network. This enables rendering scenes even
when objects or lights move, without retraining. Combined with a volumetric
path tracing procedure, our framework is capable of rendering both intra- and
inter-object light transport effects including occlusions, specularities,
shadows, and indirect illumination. We evaluate our approach on scene
composition and show that it generalizes to novel illumination conditions,
producing photorealistic, physically accurate renderings of multi-object
scenes.";Michelle Guo<author:sep>Alireza Fathi<author:sep>Jiajun Wu<author:sep>Thomas Funkhouser;http://arxiv.org/pdf/2012.08503v1;cs.CV;"Summary Video: https://youtu.be/NtR7xgxSL1U Project Webpage:
  https://shellguo.com/osf";nerf
2012.05903v2;http://arxiv.org/abs/2012.05903v2;2020-12-10;Portrait Neural Radiance Fields from a Single Image;"We present a method for estimating Neural Radiance Fields (NeRF) from a
single headshot portrait. While NeRF has demonstrated high-quality view
synthesis, it requires multiple images of static scenes and thus impractical
for casual captures and moving subjects. In this work, we propose to pretrain
the weights of a multilayer perceptron (MLP), which implicitly models the
volumetric density and colors, with a meta-learning framework using a light
stage portrait dataset. To improve the generalization to unseen faces, we train
the MLP in the canonical coordinate space approximated by 3D face morphable
models. We quantitatively evaluate the method using controlled captures and
demonstrate the generalization to real portrait images, showing favorable
results against state-of-the-arts.";Chen Gao<author:sep>Yichang Shih<author:sep>Wei-Sheng Lai<author:sep>Chia-Kai Liang<author:sep>Jia-Bin Huang;http://arxiv.org/pdf/2012.05903v2;cs.CV;Project webpage: https://portrait-nerf.github.io/;nerf
2012.05877v3;http://arxiv.org/abs/2012.05877v3;2020-12-10;INeRF: Inverting Neural Radiance Fields for Pose Estimation;"We present iNeRF, a framework that performs mesh-free pose estimation by
""inverting"" a Neural RadianceField (NeRF). NeRFs have been shown to be
remarkably effective for the task of view synthesis - synthesizing
photorealistic novel views of real-world scenes or objects. In this work, we
investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,
RGB-only 6DoF pose estimation - given an image, find the translation and
rotation of a camera relative to a 3D object or scene. Our method assumes that
no object mesh models are available during either training or test time.
Starting from an initial pose estimate, we use gradient descent to minimize the
residual between pixels rendered from a NeRF and pixels in an observed image.
In our experiments, we first study 1) how to sample rays during pose refinement
for iNeRF to collect informative gradients and 2) how different batch sizes of
rays affect iNeRF on a synthetic dataset. We then show that for complex
real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating
the camera poses of novel images and using these images as additional training
data for NeRF. Finally, we show iNeRF can perform category-level object pose
estimation, including object instances not seen during training, with RGB
images by inverting a NeRF model inferred from a single view.";Lin Yen-Chen<author:sep>Pete Florence<author:sep>Jonathan T. Barron<author:sep>Alberto Rodriguez<author:sep>Phillip Isola<author:sep>Tsung-Yi Lin;http://arxiv.org/pdf/2012.05877v3;cs.CV;IROS 2021, Website: http://yenchenlin.me/inerf/;nerf
2012.03065v1;http://arxiv.org/abs/2012.03065v1;2020-12-05;Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar  Reconstruction;"We present dynamic neural radiance fields for modeling the appearance and
dynamics of a human face. Digitally modeling and reconstructing a talking human
is a key building-block for a variety of applications. Especially, for
telepresence applications in AR or VR, a faithful reproduction of the
appearance including novel viewpoints or head-poses is required. In contrast to
state-of-the-art approaches that model the geometry and material properties
explicitly, or are purely image-based, we introduce an implicit representation
of the head based on scene representation networks. To handle the dynamics of
the face, we combine our scene representation network with a low-dimensional
morphable model which provides explicit control over pose and expressions. We
use volumetric rendering to generate images from this hybrid representation and
demonstrate that such a dynamic neural scene representation can be learned from
monocular input data only, without the need of a specialized capture setup. In
our experiments, we show that this learned volumetric representation allows for
photo-realistic image generation that surpasses the quality of state-of-the-art
video-based reenactment methods.";Guy Gafni<author:sep>Justus Thies<author:sep>Michael Zollhöfer<author:sep>Matthias Nießner;http://arxiv.org/pdf/2012.03065v1;cs.CV;"Video: https://youtu.be/m7oROLdQnjk | Project page:
  https://gafniguy.github.io/4D-Facial-Avatars/";
2012.02190v3;http://arxiv.org/abs/2012.02190v3;2020-12-03;pixelNeRF: Neural Radiance Fields from One or Few Images;"We propose pixelNeRF, a learning framework that predicts a continuous neural
scene representation conditioned on one or few input images. The existing
approach for constructing neural radiance fields involves optimizing the
representation to every scene independently, requiring many calibrated views
and significant compute time. We take a step towards resolving these
shortcomings by introducing an architecture that conditions a NeRF on image
inputs in a fully convolutional manner. This allows the network to be trained
across multiple scenes to learn a scene prior, enabling it to perform novel
view synthesis in a feed-forward manner from a sparse set of views (as few as
one). Leveraging the volume rendering approach of NeRF, our model can be
trained directly from images with no explicit 3D supervision. We conduct
extensive experiments on ShapeNet benchmarks for single image novel view
synthesis tasks with held-out objects as well as entire unseen categories. We
further demonstrate the flexibility of pixelNeRF by demonstrating it on
multi-object ShapeNet scenes and real scenes from the DTU dataset. In all
cases, pixelNeRF outperforms current state-of-the-art baselines for novel view
synthesis and single image 3D reconstruction. For the video and code, please
visit the project website: https://alexyu.net/pixelnerf";Alex Yu<author:sep>Vickie Ye<author:sep>Matthew Tancik<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2012.02190v3;cs.CV;CVPR 2021;nerf
2011.13961v1;http://arxiv.org/abs/2011.13961v1;2020-11-27;D-NeRF: Neural Radiance Fields for Dynamic Scenes;"Neural rendering techniques combining machine learning with geometric
reasoning have arisen as one of the most promising approaches for synthesizing
novel views of a scene from a sparse set of images. Among these, stands out the
Neural radiance fields (NeRF), which trains a deep network to map 5D input
coordinates (representing spatial location and viewing direction) into a volume
density and view-dependent emitted radiance. However, despite achieving an
unprecedented level of photorealism on the generated images, NeRF is only
applicable to static scenes, where the same spatial location can be queried
from different images. In this paper we introduce D-NeRF, a method that extends
neural radiance fields to a dynamic domain, allowing to reconstruct and render
novel images of objects under rigid and non-rigid motions from a \emph{single}
camera moving around the scene. For this purpose we consider time as an
additional input to the system, and split the learning process in two main
stages: one that encodes the scene into a canonical space and another that maps
this canonical representation into the deformed scene at a particular time.
Both mappings are simultaneously learned using fully-connected networks. Once
the networks are trained, D-NeRF can render novel images, controlling both the
camera view and the time variable, and thus, the object movement. We
demonstrate the effectiveness of our approach on scenes with objects under
rigid, articulated and non-rigid motions. Code, model weights and the dynamic
scenes dataset will be released.";Albert Pumarola<author:sep>Enric Corona<author:sep>Gerard Pons-Moll<author:sep>Francesc Moreno-Noguer;http://arxiv.org/pdf/2011.13961v1;cs.CV;;nerf
2011.12948v5;http://arxiv.org/abs/2011.12948v5;2020-11-25;Nerfies: Deformable Neural Radiance Fields;"We present the first method capable of photorealistically reconstructing
deformable scenes using photos/videos captured casually from mobile phones. Our
approach augments neural radiance fields (NeRF) by optimizing an additional
continuous volumetric deformation field that warps each observed point into a
canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone
to local minima, and propose a coarse-to-fine optimization method for
coordinate-based models that allows for more robust optimization. By adapting
principles from geometry processing and physical simulation to NeRF-like
models, we propose an elastic regularization of the deformation field that
further improves robustness. We show that our method can turn casually captured
selfie photos/videos into deformable NeRF models that allow for photorealistic
renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We
evaluate our method by collecting time-synchronized data using a rig with two
mobile phones, yielding train/validation images of the same pose at different
viewpoints. We show that our method faithfully reconstructs non-rigidly
deforming scenes and reproduces unseen views with high fidelity.";Keunhong Park<author:sep>Utkarsh Sinha<author:sep>Jonathan T. Barron<author:sep>Sofien Bouaziz<author:sep>Dan B Goldman<author:sep>Steven M. Seitz<author:sep>Ricardo Martin-Brualla;http://arxiv.org/pdf/2011.12948v5;cs.CV;ICCV 2021, Project page with videos: https://nerfies.github.io/;nerf
2011.12490v1;http://arxiv.org/abs/2011.12490v1;2020-11-25;DeRF: Decomposed Radiance Fields;"With the advent of Neural Radiance Fields (NeRF), neural networks can now
render novel views of a 3D scene with quality that fools the human eye. Yet,
generating these images is very computationally intensive, limiting their
applicability in practical scenarios. In this paper, we propose a technique
based on spatial decomposition capable of mitigating this issue. Our key
observation is that there are diminishing returns in employing larger (deeper
and/or wider) networks. Hence, we propose to spatially decompose a scene and
dedicate smaller networks for each decomposed part. When working together,
these networks can render the whole scene. This allows us near-constant
inference time regardless of the number of decomposed parts. Moreover, we show
that a Voronoi spatial decomposition is preferable for this purpose, as it is
provably compatible with the Painter's Algorithm for efficient and GPU-friendly
rendering. Our experiments show that for real-world scenes, our method provides
up to 3x more efficient inference than NeRF (with the same rendering quality),
or an improvement of up to 1.0~dB in PSNR (for the same inference cost).";Daniel Rebain<author:sep>Wei Jiang<author:sep>Soroosh Yazdani<author:sep>Ke Li<author:sep>Kwang Moo Yi<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2011.12490v1;cs.CV;;nerf
2010.07492v2;http://arxiv.org/abs/2010.07492v2;2020-10-15;NeRF++: Analyzing and Improving Neural Radiance Fields;"Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a
variety of capture settings, including 360 capture of bounded scenes and
forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer
perceptrons (MLPs) representing view-invariant opacity and view-dependent color
volumes to a set of training images, and samples novel views based on volume
rendering techniques. In this technical report, we first remark on radiance
fields and their potential ambiguities, namely the shape-radiance ambiguity,
and analyze NeRF's success in avoiding such ambiguities. Second, we address a
parametrization issue involved in applying NeRF to 360 captures of objects
within large-scale, unbounded 3D scenes. Our method improves view synthesis
fidelity in this challenging scenario. Code is available at
https://github.com/Kai-46/nerfplusplus.";Kai Zhang<author:sep>Gernot Riegler<author:sep>Noah Snavely<author:sep>Vladlen Koltun;http://arxiv.org/pdf/2010.07492v2;cs.CV;"Code is available at https://github.com/Kai-46/nerfplusplus; fix a
  minor formatting issue in Fig. 4";nerf
2008.02268v3;http://arxiv.org/abs/2008.02268v3;2020-08-05;NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo  Collections;"We present a learning-based method for synthesizing novel views of complex
scenes using only unstructured collections of in-the-wild photographs. We build
on Neural Radiance Fields (NeRF), which uses the weights of a multilayer
perceptron to model the density and color of a scene as a function of 3D
coordinates. While NeRF works well on images of static subjects captured under
controlled settings, it is incapable of modeling many ubiquitous, real-world
phenomena in uncontrolled images, such as variable illumination or transient
occluders. We introduce a series of extensions to NeRF to address these issues,
thereby enabling accurate reconstructions from unstructured image collections
taken from the internet. We apply our system, dubbed NeRF-W, to internet photo
collections of famous landmarks, and demonstrate temporally consistent novel
view renderings that are significantly closer to photorealism than the prior
state of the art.";Ricardo Martin-Brualla<author:sep>Noha Radwan<author:sep>Mehdi S. M. Sajjadi<author:sep>Jonathan T. Barron<author:sep>Alexey Dosovitskiy<author:sep>Daniel Duckworth;http://arxiv.org/pdf/2008.02268v3;cs.CV;"Project website: https://nerf-w.github.io. Ricardo Martin-Brualla,
  Noha Radwan, and Mehdi S. M. Sajjadi contributed equally to this work.
  Updated with results for three additional scenes";nerf
2007.11571v2;http://arxiv.org/abs/2007.11571v2;2020-07-22;Neural Sparse Voxel Fields;"Photo-realistic free-viewpoint rendering of real-world scenes using classical
computer graphics techniques is challenging, because it requires the difficult
step of capturing detailed appearance and geometry models. Recent studies have
demonstrated promising results by learning scene representations that
implicitly encode both geometry and appearance without 3D supervision. However,
existing approaches in practice often show blurry renderings caused by the
limited network capacity or the difficulty in finding accurate intersections of
camera rays with the scene geometry. Synthesizing high-resolution imagery from
these representations often requires time-consuming optical ray marching. In
this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene
representation for fast and high-quality free-viewpoint rendering. NSVF defines
a set of voxel-bounded implicit fields organized in a sparse voxel octree to
model local properties in each cell. We progressively learn the underlying
voxel structures with a differentiable ray-marching operation from only a set
of posed RGB images. With the sparse voxel octree structure, rendering novel
views can be accelerated by skipping the voxels containing no relevant scene
content. Our method is typically over 10 times faster than the state-of-the-art
(namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving
higher quality results. Furthermore, by utilizing an explicit sparse voxel
representation, our method can easily be applied to scene editing and scene
composition. We also demonstrate several challenging tasks, including
multi-scene learning, free-viewpoint rendering of a moving human, and
large-scale scene rendering. Code and data are available at our website:
https://github.com/facebookresearch/NSVF.";Lingjie Liu<author:sep>Jiatao Gu<author:sep>Kyaw Zaw Lin<author:sep>Tat-Seng Chua<author:sep>Christian Theobalt;http://arxiv.org/pdf/2007.11571v2;cs.CV;20 pages, in progress;nerf
2003.08934v2;http://arxiv.org/abs/2003.08934v2;2020-03-19;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis;"We present a method that achieves state-of-the-art results for synthesizing
novel views of complex scenes by optimizing an underlying continuous volumetric
scene function using a sparse set of input views. Our algorithm represents a
scene using a fully-connected (non-convolutional) deep network, whose input is
a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing
direction $(\theta, \phi)$) and whose output is the volume density and
view-dependent emitted radiance at that spatial location. We synthesize views
by querying 5D coordinates along camera rays and use classic volume rendering
techniques to project the output colors and densities into an image. Because
volume rendering is naturally differentiable, the only input required to
optimize our representation is a set of images with known camera poses. We
describe how to effectively optimize neural radiance fields to render
photorealistic novel views of scenes with complicated geometry and appearance,
and demonstrate results that outperform prior work on neural rendering and view
synthesis. View synthesis results are best viewed as videos, so we urge readers
to view our supplementary video for convincing comparisons.";Ben Mildenhall<author:sep>Pratul P. Srinivasan<author:sep>Matthew Tancik<author:sep>Jonathan T. Barron<author:sep>Ravi Ramamoorthi<author:sep>Ren Ng;http://arxiv.org/pdf/2003.08934v2;cs.CV;"ECCV 2020 (oral). Project page with videos and code:
  http://tancik.com/nerf";nerf
1403.5969v1;http://arxiv.org/abs/1403.5969v1;2014-03-24;Random Matrices and Erasure Robust Frames;"Data erasure can often occur in communication. Guarding against erasures
involves redundancy in data representation. Mathematically this may be achieved
by redundancy through the use of frames. One way to measure the robustness of a
frame against erasures is to examine the worst case condition number of the
frame with a certain number of vectors erased from the frame. The term {\em
numerically erasure-robust frames (NERFs)} was introduced in \cite{FicMix12} to
give a more precise characterization of erasure robustness of frames. In the
paper the authors established that random frames whose entries are drawn
independently from the standard normal distribution can be robust against up to
approximately 15\% erasures, and asked whether there exist frames that are
robust against erasures of more than 50\%. In this paper we show that with very
high probability random frames are, independent of the dimension, robust
against any amount of erasures as long as the number of remaining vectors is at
least $1+\delta$ times the dimension for some $\delta_0>0$. This is the best
possible result, and it also implies that the proportion of erasures can
arbitrarily close to 1 while still maintaining robustness. Our result depends
crucially on a new estimate for the smallest singular value of a rectangular
random matrix with independent standard normal entries.";Yang Wang;http://arxiv.org/pdf/1403.5969v1;cs.IT;;nerf
1210.0139v1;http://arxiv.org/abs/1210.0139v1;2012-09-29;Group-theoretic constructions of erasure-robust frames;"In the field of compressed sensing, a key problem remains open: to explicitly
construct matrices with the restricted isometry property (RIP) whose
performance rivals those generated using random matrix theory. In short, RIP
involves estimating the singular values of a combinatorially large number of
submatrices, seemingly requiring an enormous amount of computation in even
low-dimensional examples. In this paper, we consider a similar problem
involving submatrix singular value estimation, namely the problem of explicitly
constructing numerically erasure robust frames (NERFs). Such frames are the
latest invention in a long line of research concerning the design of linear
encoders that are robust against data loss. We begin by focusing on a subtle
difference between the definition of a NERF and that of an RIP matrix, one that
allows us to introduce a new computational trick for quickly estimating NERF
bounds. In short, we estimate these bounds by evaluating the frame analysis
operator at every point of an epsilon-net for the unit sphere. We then borrow
ideas from the theory of group frames to construct explicit frames and
epsilon-nets with such high degrees of symmetry that the requisite number of
operator evaluations is greatly reduced. We conclude with numerical results,
using these new ideas to quickly produce decent estimates of NERF bounds which
would otherwise take an eternity. Though the more important RIP problem remains
open, this work nevertheless demonstrates the feasibility of exploiting
symmetry to greatly reduce the computational burden of similar combinatorial
linear algebra problems.";Matthew Fickus<author:sep>John Jasper<author:sep>Dustin G. Mixon<author:sep>Jesse Peterson;http://arxiv.org/pdf/1210.0139v1;math.FA;;nerf
