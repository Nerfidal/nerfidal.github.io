id;link;pub_date;title;abstract;authors;pdf_link;category;comments;tags
2404.15264v1;http://arxiv.org/abs/2404.15264v1;2024-04-23;TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via  Gaussian Splatting;"Radiance fields have demonstrated impressive performance in synthesizing
lifelike 3D talking heads. However, due to the difficulty in fitting steep
appearance changes, the prevailing paradigm that presents facial motions by
directly modifying point appearance may lead to distortions in dynamic regions.
To tackle this challenge, we introduce TalkingGaussian, a deformation-based
radiance fields framework for high-fidelity talking head synthesis. Leveraging
the point-based Gaussian Splatting, facial motions can be represented in our
method by applying smooth and continuous deformations to persistent Gaussian
primitives, without requiring to learn the difficult appearance change like
previous methods. Due to this simplification, precise facial motions can be
synthesized while keeping a highly intact facial feature. Under such a
deformation paradigm, we further identify a face-mouth motion inconsistency
that would affect the learning of detailed speaking motions. To address this
conflict, we decompose the model into two branches separately for the face and
inside mouth areas, therefore simplifying the learning tasks to help
reconstruct more accurate motion and structure of the mouth region. Extensive
experiments demonstrate that our method renders high-quality lip-synchronized
talking head videos, with better facial fidelity and higher efficiency compared
with previous methods.";Jiahe Li<author:sep>Jiawei Zhang<author:sep>Xiao Bai<author:sep>Jin Zheng<author:sep>Xin Ning<author:sep>Jun Zhou<author:sep>Lin Gu;http://arxiv.org/pdf/2404.15264v1;cs.CV;Project page: https://fictionarry.github.io/TalkingGaussian/;gaussian splatting
2404.15259v1;http://arxiv.org/abs/2404.15259v1;2024-04-23;FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient  Descent;"This paper introduces FlowMap, an end-to-end differentiable method that
solves for precise camera poses, camera intrinsics, and per-frame dense depth
of a video sequence. Our method performs per-video gradient-descent
minimization of a simple least-squares objective that compares the optical flow
induced by depth, intrinsics, and poses against correspondences obtained via
off-the-shelf optical flow and point tracking. Alongside the use of point
tracks to encourage long-term geometric consistency, we introduce
differentiable re-parameterizations of depth, intrinsics, and pose that are
amenable to first-order optimization. We empirically show that camera
parameters and dense depth recovered by our method enable photo-realistic novel
view synthesis on 360-degree trajectories using Gaussian Splatting. Our method
not only far outperforms prior gradient-descent based bundle adjustment
methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM
method, on the downstream task of 360-degree novel view synthesis (even though
our method is purely gradient-descent based, fully differentiable, and presents
a complete departure from conventional SfM).";Cameron Smith<author:sep>David Charatan<author:sep>Ayush Tewari<author:sep>Vincent Sitzmann;http://arxiv.org/pdf/2404.15259v1;cs.CV;Project website: https://cameronosmith.github.io/flowmap/;gaussian splatting
2404.13896v2;http://arxiv.org/abs/2404.13896v2;2024-04-22;CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with  Complex Trajectory;"Neural radiance field (NeRF) has achieved impressive results in high-quality
3D scene reconstruction. However, NeRF heavily relies on precise camera poses.
While recent works like BARF have introduced camera pose optimization within
NeRF, their applicability is limited to simple trajectory scenes. Existing
methods struggle while tackling complex trajectories involving large rotations.
To address this limitation, we propose CT-NeRF, an incremental reconstruction
optimization pipeline using only RGB images without pose and depth input. In
this pipeline, we first propose a local-global bundle adjustment under a pose
graph connecting neighboring frames to enforce the consistency between poses to
escape the local minima caused by only pose consistency with the scene
structure. Further, we instantiate the consistency between poses as a
reprojected geometric image distance constraint resulting from pixel-level
correspondences between input image pairs. Through the incremental
reconstruction, CT-NeRF enables the recovery of both camera poses and scene
structure and is capable of handling scenes with complex trajectories. We
evaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and
Free-Dataset, which feature complex trajectories. Results show CT-NeRF
outperforms existing methods in novel view synthesis and pose estimation
accuracy.";Yunlong Ran<author:sep>Yanxu Li<author:sep>Qi Ye<author:sep>Yuchi Huo<author:sep>Zechun Bai<author:sep>Jiahao Sun<author:sep>Jiming Chen;http://arxiv.org/pdf/2404.13896v2;cs.CV;;nerf
2404.14037v1;http://arxiv.org/abs/2404.14037v1;2024-04-22;GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian  Splatting;"Recent works on audio-driven talking head synthesis using Neural Radiance
Fields (NeRF) have achieved impressive results. However, due to inadequate pose
and expression control caused by NeRF implicit representation, these methods
still have some limitations, such as unsynchronized or unnatural lip movements,
and visual jitter and artifacts. In this paper, we propose GaussianTalker, a
novel method for audio-driven talking head synthesis based on 3D Gaussian
Splatting. With the explicit representation property of 3D Gaussians, intuitive
control of the facial motion is achieved by binding Gaussians to 3D facial
models. GaussianTalker consists of two modules, Speaker-specific Motion
Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator
achieves accurate lip movements specific to the target speaker through
universalized audio feature extraction and customized lip motion generation.
Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance
facial detail representation via a latent pose, delivering stable and realistic
rendered videos. Extensive experimental results suggest that GaussianTalker
outperforms existing state-of-the-art methods in talking head synthesis,
delivering precise lip synchronization and exceptional visual quality. Our
method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU,
significantly exceeding the threshold for real-time rendering performance, and
can potentially be deployed on other hardware platforms.";Hongyun Yu<author:sep>Zhan Qu<author:sep>Qihang Yu<author:sep>Jianchuan Chen<author:sep>Zhonghua Jiang<author:sep>Zhiwen Chen<author:sep>Shengyu Zhang<author:sep>Jimin Xu<author:sep>Fei Wu<author:sep>Chengfei Lv<author:sep>Gang Yu;http://arxiv.org/pdf/2404.14037v1;cs.CV;;nerf
2404.13921v1;http://arxiv.org/abs/2404.13921v1;2024-04-22;NeRF-DetS: Enhancing Multi-View 3D Object Detection with  Sampling-adaptive Network of Continuous NeRF-based Representation;"As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and
3D perception, demonstrating that perceptual tasks can benefit from novel view
synthesis methods like NeRF, significantly improving the performance of indoor
multi-view 3D object detection. Using the geometry MLP of NeRF to direct the
attention of detection head to crucial parts and incorporating self-supervised
loss from novel view rendering contribute to the achieved improvement. To
better leverage the notable advantages of the continuous representation through
neural rendering in space, we introduce a novel 3D perception network
structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level
Sampling-Adaptive Network, making the sampling process adaptively from coarse
to fine. Also, we propose a superior multi-view information fusion method,
known as Multi-head Weighted Fusion. This fusion approach efficiently addresses
the challenge of losing multi-view information when using arithmetic mean,
while keeping low computational costs. NeRF-DetS outperforms competitive
NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement
in mAP@.25 and mAP@.50, respectively.";Chi Huang<author:sep>Xinyang Li<author:sep>Shengchuan Zhang<author:sep>Liujuan Cao<author:sep>Rongrong Ji;http://arxiv.org/pdf/2404.13921v1;cs.CV;;nerf
2404.14410v1;http://arxiv.org/abs/2404.14410v1;2024-04-22;Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D  Glimpses;"In this paper, we present a method to reconstruct the world and multiple
dynamic humans in 3D from a monocular video input. As a key idea, we represent
both the world and multiple humans via the recently emerging 3D Gaussian
Splatting (3D-GS) representation, enabling to conveniently and efficiently
compose and render them together. In particular, we address the scenarios with
severely limited and sparse observations in 3D human reconstruction, a common
challenge encountered in the real world. To tackle this challenge, we introduce
a novel approach to optimize the 3D-GS representation in a canonical space by
fusing the sparse cues in the common space, where we leverage a pre-trained 2D
diffusion model to synthesize unseen views while keeping the consistency with
the observed 2D appearances. We demonstrate our method can reconstruct
high-quality animatable 3D humans in various challenging examples, in the
presence of occlusion, image crops, few-shot, and extremely sparse
observations. After reconstruction, our method is capable of not only rendering
the scene in any novel views at arbitrary time instances, but also editing the
3D scene by removing individual humans or applying different motions for each
human. Through various experiments, we demonstrate the quality and efficiency
of our methods over alternative existing approaches.";Inhee Lee<author:sep>Byungjun Kim<author:sep>Hanbyul Joo;http://arxiv.org/pdf/2404.14410v1;cs.CV;The project page is available at https://snuvclab.github.io/gtu/;
2404.14249v1;http://arxiv.org/abs/2404.14249v1;2024-04-22;CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and  View-consistent 3D Semantic Understanding;"The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time
synthesis of novel views in 3D scenes. Currently, it primarily focuses on
geometry and appearance modeling, while lacking the semantic understanding of
scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from
Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to
efficiently comprehend 3D environments without annotated semantic data. In
specific, rather than straightforwardly learning and rendering high-dimensional
semantic features of 3D Gaussians, which significantly diminishes the
efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC
exploits the inherent unified semantics within objects to learn compact yet
effective semantic representations of 3D Gaussians, enabling highly efficient
rendering (>100 FPS). Additionally, to address the semantic ambiguity, caused
by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we
introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the
multi-view consistency originated from the 3D model. 3DCS imposes cross-view
semantic consistency constraints by leveraging refined, self-predicted
pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing
precise and view-consistent segmentation results. Extensive experiments
demonstrate that our method remarkably outperforms existing state-of-the-art
approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on
Replica and ScanNet datasets, respectively, while maintaining real-time
rendering speed. Furthermore, our approach exhibits superior performance even
with sparse input data, verifying the robustness of our method.";Guibiao Liao<author:sep>Jiankun Li<author:sep>Zhenyu Bao<author:sep>Xiaoqing Ye<author:sep>Jingdong Wang<author:sep>Qing Li<author:sep>Kanglin Liu;http://arxiv.org/pdf/2404.14249v1;cs.CV;https://github.com/gbliao/CLIP-GS;gaussian splatting
2404.13816v1;http://arxiv.org/abs/2404.13816v1;2024-04-22;Neural Radiance Field in Autonomous Driving: A Survey;"Neural Radiance Field (NeRF) has garnered significant attention from both
academia and industry due to its intrinsic advantages, particularly its
implicit representation and novel view synthesis capabilities. With the rapid
advancements in deep learning, a multitude of methods have emerged to explore
the potential applications of NeRF in the domain of Autonomous Driving (AD).
However, a conspicuous void is apparent within the current literature. To
bridge this gap, this paper conducts a comprehensive survey of NeRF's
applications in the context of AD. Our survey is structured to categorize
NeRF's applications in Autonomous Driving (AD), specifically encompassing
perception, 3D reconstruction, simultaneous localization and mapping (SLAM),
and simulation. We delve into in-depth analysis and summarize the findings for
each application category, and conclude by providing insights and discussions
on future directions in this field. We hope this paper serves as a
comprehensive reference for researchers in this domain. To the best of our
knowledge, this is the first survey specifically focused on the applications of
NeRF in the Autonomous Driving domain.";Lei He<author:sep>Leheng Li<author:sep>Wenchao Sun<author:sep>Zeyu Han<author:sep>Yichen Liu<author:sep>Sifa Zheng<author:sep>Jianqiang Wang<author:sep>Keqiang Li;http://arxiv.org/pdf/2404.13816v1;cs.CV;;nerf
2404.13711v1;http://arxiv.org/abs/2404.13711v1;2024-04-21;ArtNeRF: A Stylized Neural Field for 3D-Aware Cartoonized Face Synthesis;"Recent advances in generative visual models and neural radiance fields have
greatly boosted 3D-aware image synthesis and stylization tasks. However,
previous NeRF-based work is limited to single scene stylization, training a
model to generate 3D-aware cartoon faces with arbitrary styles remains
unsolved. We propose ArtNeRF, a novel face stylization framework derived from
3D-aware GAN to tackle this problem. In this framework, we utilize an
expressive generator to synthesize stylized faces and a triple-branch
discriminator module to improve the visual quality and style consistency of the
generated faces. Specifically, a style encoder based on contrastive learning is
leveraged to extract robust low-dimensional embeddings of style images,
empowering the generator with the knowledge of various styles. To smooth the
training process of cross-domain transfer learning, we propose an adaptive
style blending module which helps inject style information and allows users to
freely tune the level of stylization. We further introduce a neural rendering
module to achieve efficient real-time rendering of images with higher
resolutions. Extensive experiments demonstrate that ArtNeRF is versatile in
generating high-quality 3D-aware cartoon faces with arbitrary styles.";Zichen Tang<author:sep>Hongyu Yang;http://arxiv.org/pdf/2404.13711v1;cs.CV;;nerf
2404.13541v1;http://arxiv.org/abs/2404.13541v1;2024-04-21;Generalizable Novel-View Synthesis using a Stereo Camera;"In this paper, we propose the first generalizable view synthesis approach
that specifically targets multi-view stereo-camera images. Since recent stereo
matching has demonstrated accurate geometry prediction, we introduce stereo
matching into novel-view synthesis for high-quality geometry reconstruction. To
this end, this paper proposes a novel framework, dubbed StereoNeRF, which
integrates stereo matching into a NeRF-based generalizable view synthesis
approach. StereoNeRF is equipped with three key components to effectively
exploit stereo matching in novel-view synthesis: a stereo feature extractor, a
depth-guided plane-sweeping, and a stereo depth loss. Moreover, we propose the
StereoNVS dataset, the first multi-view dataset of stereo-camera images,
encompassing a wide variety of both real and synthetic scenes. Our experimental
results demonstrate that StereoNeRF surpasses previous approaches in
generalizable view synthesis.";Haechan Lee<author:sep>Wonjoon Jin<author:sep>Seung-Hwan Baek<author:sep>Sunghyun Cho;http://arxiv.org/pdf/2404.13541v1;cs.CV;"Accepted to CVPR 2024. Project page URL:
  https://jinwonjoon.github.io/stereonerf/";nerf
2404.13679v1;http://arxiv.org/abs/2404.13679v1;2024-04-21;GScream: Learning 3D Geometry and Feature Consistent Gaussian Splatting  for Object Removal;"This paper tackles the intricate challenge of object removal to update the
radiance field using the 3D Gaussian Splatting. The main challenges of this
task lie in the preservation of geometric consistency and the maintenance of
texture coherence in the presence of the substantial discrete nature of
Gaussian primitives. We introduce a robust framework specifically designed to
overcome these obstacles. The key insight of our approach is the enhancement of
information exchange among visible and invisible areas, facilitating content
restoration in terms of both geometry and texture. Our methodology begins with
optimizing the positioning of Gaussian primitives to improve geometric
consistency across both removed and visible areas, guided by an online
registration process informed by monocular depth estimation. Following this, we
employ a novel feature propagation mechanism to bolster texture coherence,
leveraging a cross-attention design that bridges sampling Gaussians from both
uncertain and certain areas. This innovative approach significantly refines the
texture coherence within the final radiance field. Extensive experiments
validate that our method not only elevates the quality of novel view synthesis
for scenes undergoing object removal but also showcases notable efficiency
gains in training and rendering speeds.";Yuxin Wang<author:sep>Qianyi Wu<author:sep>Guofeng Zhang<author:sep>Dan Xu;http://arxiv.org/pdf/2404.13679v1;cs.CV;Project Page: https://w-ted.github.io/publications/gscream;gaussian splatting
2404.13437v1;http://arxiv.org/abs/2404.13437v1;2024-04-20;High-fidelity Endoscopic Image Synthesis by Utilizing Depth-guided  Neural Surfaces;"In surgical oncology, screening colonoscopy plays a pivotal role in providing
diagnostic assistance, such as biopsy, and facilitating surgical navigation,
particularly in polyp detection. Computer-assisted endoscopic surgery has
recently gained attention and amalgamated various 3D computer vision
techniques, including camera localization, depth estimation, surface
reconstruction, etc. Neural Radiance Fields (NeRFs) and Neural Implicit
Surfaces (NeuS) have emerged as promising methodologies for deriving accurate
3D surface models from sets of registered images, addressing the limitations of
existing colon reconstruction approaches stemming from constrained camera
movement.
  However, the inadequate tissue texture representation and confused scale
problem in monocular colonoscopic image reconstruction still impede the
progress of the final rendering results. In this paper, we introduce a novel
method for colon section reconstruction by leveraging NeuS applied to
endoscopic images, supplemented by a single frame of depth map. Notably, we
pioneered the exploration of utilizing only one frame depth map in
photorealistic reconstruction and neural rendering applications while this
single depth map can be easily obtainable from other monocular depth estimation
networks with an object scale. Through rigorous experimentation and validation
on phantom imagery, our approach demonstrates exceptional accuracy in
completely rendering colon sections, even capturing unseen portions of the
surface. This breakthrough opens avenues for achieving stable and consistently
scaled reconstructions, promising enhanced quality in cancer screening
procedures and treatment interventions.";Baoru Huang<author:sep>Yida Wang<author:sep>Anh Nguyen<author:sep>Daniel Elson<author:sep>Francisco Vasconcelos<author:sep>Danail Stoyanov;http://arxiv.org/pdf/2404.13437v1;cs.CV;;nerf
2404.13346v1;http://arxiv.org/abs/2404.13346v1;2024-04-20;EC-SLAM: Real-time Dense Neural RGB-D SLAM System with Effectively  Constrained Global Bundle Adjustment;"We introduce EC-SLAM, a real-time dense RGB-D simultaneous localization and
mapping (SLAM) system utilizing Neural Radiance Fields (NeRF). Although recent
NeRF-based SLAM systems have demonstrated encouraging outcomes, they have yet
to completely leverage NeRF's capability to constrain pose optimization. By
employing an effectively constrained global bundle adjustment (BA) strategy,
our system makes use of NeRF's implicit loop closure correction capability.
This improves the tracking accuracy by reinforcing the constraints on the
keyframes that are most pertinent to the optimized current frame. In addition,
by implementing a feature-based and uniform sampling strategy that minimizes
the number of ineffective constraint points for pose optimization, we mitigate
the effects of random sampling in NeRF. EC-SLAM utilizes sparse parametric
encodings and the truncated signed distance field (TSDF) to represent the map
in order to facilitate efficient fusion, resulting in reduced model parameters
and accelerated convergence velocity. A comprehensive evaluation conducted on
the Replica, ScanNet, and TUM datasets showcases cutting-edge performance,
including enhanced reconstruction accuracy resulting from precise pose
estimation, 21 Hz run time, and tracking precision improvements of up to 50\%.
The source code is available at https://github.com/Lightingooo/EC-SLAM.";Guanghao Li<author:sep>Qi Chen<author:sep>YuXiang Yan<author:sep>Jian Pu;http://arxiv.org/pdf/2404.13346v1;cs.RO;;nerf
2404.12970v1;http://arxiv.org/abs/2404.12970v1;2024-04-19;FlyNeRF: NeRF-Based Aerial Mapping for High-Quality 3D Scene  Reconstruction;"Current methods for 3D reconstruction and environmental mapping frequently
face challenges in achieving high precision, highlighting the need for
practical and effective solutions. In response to this issue, our study
introduces FlyNeRF, a system integrating Neural Radiance Fields (NeRF) with
drone-based data acquisition for high-quality 3D reconstruction. Utilizing
unmanned aerial vehicle (UAV) for capturing images and corresponding spatial
coordinates, the obtained data is subsequently used for the initial NeRF-based
3D reconstruction of the environment. Further evaluation of the reconstruction
render quality is accomplished by the image evaluation neural network developed
within the scope of our system. According to the results of the image
evaluation module, an autonomous algorithm determines the position for
additional image capture, thereby improving the reconstruction quality. The
neural network introduced for render quality assessment demonstrates an
accuracy of 97%. Furthermore, our adaptive methodology enhances the overall
reconstruction quality, resulting in an average improvement of 2.5 dB in Peak
Signal-to-Noise Ratio (PSNR) for the 10% quantile. The FlyNeRF demonstrates
promising results, offering advancements in such fields as environmental
monitoring, surveillance, and digital twins, where high-fidelity 3D
reconstructions are crucial.";Maria Dronova<author:sep>Vladislav Cheremnykh<author:sep>Alexey Kotcov<author:sep>Aleksey Fedoseev<author:sep>Dzmitry Tsetserukou;http://arxiv.org/pdf/2404.12970v1;cs.RO;;nerf
2404.12777v1;http://arxiv.org/abs/2404.12777v1;2024-04-19;EfficientGS: Streamlining Gaussian Splatting for Large-Scale  High-Resolution Scene Representation;"In the domain of 3D scene representation, 3D Gaussian Splatting (3DGS) has
emerged as a pivotal technology. However, its application to large-scale,
high-resolution scenes (exceeding 4k$\times$4k pixels) is hindered by the
excessive computational requirements for managing a large number of Gaussians.
Addressing this, we introduce 'EfficientGS', an advanced approach that
optimizes 3DGS for high-resolution, large-scale scenes. We analyze the
densification process in 3DGS and identify areas of Gaussian
over-proliferation. We propose a selective strategy, limiting Gaussian increase
to key primitives, thereby enhancing the representational efficiency.
Additionally, we develop a pruning mechanism to remove redundant Gaussians,
those that are merely auxiliary to adjacent ones. For further enhancement, we
integrate a sparse order increment for Spherical Harmonics (SH), designed to
alleviate storage constraints and reduce training overhead. Our empirical
evaluations, conducted on a range of datasets including extensive 4K+ aerial
images, demonstrate that 'EfficientGS' not only expedites training and
rendering times but also achieves this with a model size approximately tenfold
smaller than conventional 3DGS while maintaining high rendering fidelity.";Wenkai Liu<author:sep>Tao Guan<author:sep>Bin Zhu<author:sep>Lili Ju<author:sep>Zikai Song<author:sep>Dan Li<author:sep>Yuesong Wang<author:sep>Wei Yang;http://arxiv.org/pdf/2404.12777v1;cs.CV;;gaussian splatting
2404.12888v1;http://arxiv.org/abs/2404.12888v1;2024-04-19;Learn2Talk: 3D Talking Face Learns from 2D Talking Face;"Speech-driven facial animation methods usually contain two main classes, 3D
and 2D talking face, both of which attract considerable research attention in
recent years. However, to the best of our knowledge, the research on 3D talking
face does not go deeper as 2D talking face, in the aspect of
lip-synchronization (lip-sync) and speech perception. To mind the gap between
the two sub-fields, we propose a learning framework named Learn2Talk, which can
construct a better 3D talking face network by exploiting two expertise points
from the field of 2D talking face. Firstly, inspired by the audio-video sync
network, a 3D sync-lip expert model is devised for the pursuit of lip-sync
between audio and 3D facial motion. Secondly, a teacher model selected from 2D
talking face methods is used to guide the training of the audio-to-3D motions
regression network to yield more 3D vertex accuracy. Extensive experiments show
the advantages of the proposed framework in terms of lip-sync, vertex accuracy
and speech perception, compared with state-of-the-arts. Finally, we show two
applications of the proposed framework: audio-visual speech recognition and
speech-driven 3D Gaussian Splatting based avatar animation.";Yixiang Zhuang<author:sep>Baoping Cheng<author:sep>Yao Cheng<author:sep>Yuntao Jin<author:sep>Renshuai Liu<author:sep>Chengyang Li<author:sep>Xuan Cheng<author:sep>Jing Liao<author:sep>Juncong Lin;http://arxiv.org/pdf/2404.12888v1;cs.CV;;gaussian splatting
2404.12547v2;http://arxiv.org/abs/2404.12547v2;2024-04-18;Does Gaussian Splatting need SFM Initialization?;"3D Gaussian Splatting has recently been embraced as a versatile and effective
method for scene reconstruction and novel view synthesis, owing to its
high-quality results and compatibility with hardware rasterization. Despite its
advantages, Gaussian Splatting's reliance on high-quality point cloud
initialization by Structure-from-Motion (SFM) algorithms is a significant
limitation to be overcome. To this end, we investigate various initialization
strategies for Gaussian Splatting and delve into how volumetric reconstructions
from Neural Radiance Fields (NeRF) can be utilized to bypass the dependency on
SFM data. Our findings demonstrate that random initialization can perform much
better if carefully designed and that by employing a combination of improved
initialization strategies and structure distillation from low-cost NeRF models,
it is possible to achieve equivalent results, or at times even superior, to
those obtained from SFM initialization.";Yalda Foroutan<author:sep>Daniel Rebain<author:sep>Kwang Moo Yi<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2404.12547v2;cs.CV;14 pages, 6 figures;gaussian splatting<tag:sep>nerf
2404.12379v2;http://arxiv.org/abs/2404.12379v2;2024-04-18;Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular  Videos;"Modern 3D engines and graphics pipelines require mesh as a memory-efficient
representation, which allows efficient rendering, geometry processing, texture
editing, and many other downstream operations. However, it is still highly
difficult to obtain high-quality mesh in terms of structure and detail from
monocular visual observations. The problem becomes even more challenging for
dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh
(DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh
given a single monocular video. Our work leverages the recent advancement in 3D
Gaussian Splatting to construct the mesh sequence with temporal consistency
from a video. Building on top of this representation, DG-Mesh recovers
high-quality meshes from the Gaussian points and can track the mesh vertices
over time, which enables applications such as texture editing on dynamic
objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly
distributed Gaussians, resulting better mesh reconstruction through mesh-guided
densification and pruning on the deformed Gaussians. By applying
cycle-consistent deformation between the canonical and the deformed space, we
can project the anchored Gaussian back to the canonical space and optimize
Gaussians across all time frames. During the evaluation on different datasets,
DG-Mesh provides significantly better mesh reconstruction and rendering than
baselines. Project page: https://www.liuisabella.com/DG-Mesh/";Isabella Liu<author:sep>Hao Su<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2404.12379v2;cs.CV;Project page: https://www.liuisabella.com/DG-Mesh/;gaussian splatting
2404.12385v1;http://arxiv.org/abs/2404.12385v1;2024-04-18;MeshLRM: Large Reconstruction Model for High-Quality Mesh;"We propose MeshLRM, a novel LRM-based approach that can reconstruct a
high-quality mesh from merely four input images in less than one second.
Different from previous large reconstruction models (LRMs) that focus on
NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction
and rendering within the LRM framework. This allows for end-to-end mesh
reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.
Moreover, we improve the LRM architecture by simplifying several complex
designs in previous LRMs. MeshLRM's NeRF initialization is sequentially trained
with low- and high-resolution images; this new LRM training strategy enables
significantly faster convergence and thereby leads to better quality with less
compute. Our approach achieves state-of-the-art mesh reconstruction from
sparse-view inputs and also allows for many downstream applications, including
text-to-3D and single-image-to-3D generation. Project page:
https://sarahweiii.github.io/meshlrm/";Xinyue Wei<author:sep>Kai Zhang<author:sep>Sai Bi<author:sep>Hao Tan<author:sep>Fujun Luan<author:sep>Valentin Deschaintre<author:sep>Kalyan Sunkavalli<author:sep>Hao Su<author:sep>Zexiang Xu;http://arxiv.org/pdf/2404.12385v1;cs.CV;;nerf
2404.11897v1;http://arxiv.org/abs/2404.11897v1;2024-04-18;AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height  Large-scale Outdoor Scene Rendering;"Existing neural radiance fields (NeRF)-based novel view synthesis methods for
large-scale outdoor scenes are mainly built on a single altitude. Moreover,
they often require a priori camera shooting height and scene scope, leading to
inefficient and impractical applications when camera altitude changes. In this
work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce
the training cost of building good reconstructions by synthesizing
free-viewpoint images based on varying altitudes of scenes. Specifically, to
tackle the detail variation problem from low altitude (drone-level) to high
altitude (satellite-level), a source image selection method and an
attention-based feature fusion approach are developed to extract and fuse the
most relevant features of target view from multi-height images for
high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF
achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only
requires a half hour of training time to reach the competitive PSNR as compared
to the latest BungeeNeRF.";Jingfeng Guo<author:sep>Xiaohan Zhang<author:sep>Baozhu Zhao<author:sep>Qi Liu;http://arxiv.org/pdf/2404.11897v1;cs.CV;;nerf
2404.11852v1;http://arxiv.org/abs/2404.11852v1;2024-04-18;Cicero: Addressing Algorithmic and Architectural Bottlenecks in Neural  Rendering by Radiance Warping and Memory Optimizations;"Neural Radiance Field (NeRF) is widely seen as an alternative to traditional
physically-based rendering. However, NeRF has not yet seen its adoption in
resource-limited mobile systems such as Virtual and Augmented Reality (VR/AR),
because it is simply extremely slow. On a mobile Volta GPU, even the
state-of-the-art NeRF models generally execute only at 0.8 FPS. We show that
the main performance bottlenecks are both algorithmic and architectural. We
introduce, CICERO, to tame both forms of inefficiencies. We first introduce two
algorithms, one fundamentally reduces the amount of work any NeRF model has to
execute, and the other eliminates irregular DRAM accesses. We then describe an
on-chip data layout strategy that eliminates SRAM bank conflicts. A pure
software implementation of CICERO offers an 8.0x speed-up and 7.9x energy
saving over a mobile Volta GPU. When compared to a baseline with a dedicated
DNN accelerator, our speed-up and energy reduction increase to 28.2x and 37.8x,
respectively - all with minimal quality loss (less than 1.0 dB peak
signal-to-noise ratio reduction).";Yu Feng<author:sep>Zihan Liu<author:sep>Jingwen Leng<author:sep>Minyi Guo<author:sep>Yuhao Zhu;http://arxiv.org/pdf/2404.11852v1;cs.AR;;nerf
2404.11285v1;http://arxiv.org/abs/2404.11285v1;2024-04-17;Novel View Synthesis for Cinematic Anatomy on Mobile and Immersive  Displays;"Interactive photorealistic visualization of 3D anatomy (i.e., Cinematic
Anatomy) is used in medical education to explain the structure of the human
body. It is currently restricted to frontal teaching scenarios, where the
demonstrator needs a powerful GPU and high-speed access to a large storage
device where the dataset is hosted. We demonstrate the use of novel view
synthesis via compressed 3D Gaussian splatting to overcome this restriction and
to enable students to perform cinematic anatomy on lightweight mobile devices
and in virtual reality environments. We present an automatic approach for
finding a set of images that captures all potentially seen structures in the
data. By mixing closeup views with images from a distance, the splat
representation can recover structures up to the voxel resolution. The use of
Mip-Splatting enables smooth transitions when the focal length is increased.
Even for GB datasets, the final renderable representation can usually be
compressed to less than 70 MB, enabling interactive rendering on low-end
devices using rasterization.";Simon Niedermayr<author:sep>Christoph Neuhauser<author:sep>Kaloian Petkov<author:sep>Klaus Engel<author:sep>RÃ¼diger Westermann;http://arxiv.org/pdf/2404.11285v1;cs.GR;;gaussian splatting
2404.11358v2;http://arxiv.org/abs/2404.11358v2;2024-04-17;DeblurGS: Gaussian Splatting for Camera Motion Blur;"Although significant progress has been made in reconstructing sharp 3D scenes
from motion-blurred images, a transition to real-world applications remains
challenging. The primary obstacle stems from the severe blur which leads to
inaccuracies in the acquisition of initial camera poses through
Structure-from-Motion, a critical aspect often overlooked by previous
approaches. To address this challenge, we propose DeblurGS, a method to
optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the
noisy camera pose initialization. We restore a fine-grained sharp scene by
leveraging the remarkable reconstruction capability of 3D Gaussian Splatting.
Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry
observation and synthesizes corresponding blurry renderings for the
optimization process. Furthermore, we propose Gaussian Densification Annealing
strategy to prevent the generation of inaccurate Gaussians at erroneous
locations during the early training stages when camera motion is still
imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves
state-of-the-art performance in deblurring and novel view synthesis for
real-world and synthetic benchmark datasets, as well as field-captured blurry
smartphone videos.";Jeongtaek Oh<author:sep>Jaeyoung Chung<author:sep>Dongwoo Lee<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2404.11358v2;cs.CV;;gaussian splatting
2404.11419v1;http://arxiv.org/abs/2404.11419v1;2024-04-17;SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping;"We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose
a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM
(NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing
NeRF-SLAM systems consistently exhibit inferior tracking performance compared
to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via
image alignment and photometric bundle-adjustment. Such optimization processes
are difficult to optimize due to the narrow basin of attraction of the
optimization loss in image space (local minima) and the lack of initial
correspondences. We mitigate these limitations by implementing a Gaussian
pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking
optimization strategy. Furthermore, NeRF systems encounter challenges in
converging to the right geometry with limited input views. While prior
approaches use a Signed-Distance Function (SDF)-based NeRF and directly
supervise SDF values by approximating ground truth SDF through depth
measurements, this often results in suboptimal geometry. In contrast, our
method employs a volume density representation and introduces a novel KL
regularizer on the ray termination distribution, constraining scene geometry to
consist of empty space and opaque surfaces. Our solution implements both local
and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate
(KL regularizer) SLAM solution. We conduct experiments on multiple datasets
(ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in
reconstruction accuracy.";Vincent Cartillier<author:sep>Grant Schindler<author:sep>Irfan Essa;http://arxiv.org/pdf/2404.11419v1;cs.CV;;nerf
2404.11401v1;http://arxiv.org/abs/2404.11401v1;2024-04-17;RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled  Neural Rendering;"We propose RainyScape, an unsupervised framework for reconstructing clean
scenes from a collection of multi-view rainy images. RainyScape consists of two
main modules: a neural rendering module and a rain-prediction module that
incorporates a predictor network and a learnable latent embedding that captures
the rain characteristics of the scene. Specifically, based on the spectral bias
property of neural networks, we first optimize the neural rendering pipeline to
obtain a low-frequency scene representation. Subsequently, we jointly optimize
the two modules, driven by the proposed adaptive direction-sensitive
gradient-based reconstruction loss, which encourages the network to distinguish
between scene details and rain streaks, facilitating the propagation of
gradients to the relevant components. Extensive experiments on both the classic
neural radiance field and the recently proposed 3D Gaussian splatting
demonstrate the superiority of our method in effectively eliminating rain
streaks and rendering clean images, achieving state-of-the-art performance. The
constructed high-quality dataset and source code will be publicly available.";Xianqiang Lyu<author:sep>Hui Liu<author:sep>Junhui Hou;http://arxiv.org/pdf/2404.11401v1;cs.CV;;gaussian splatting
2404.10441v1;http://arxiv.org/abs/2404.10441v1;2024-04-16;1st Place Solution for ICCV 2023 OmniObject3D Challenge: Sparse-View  Reconstruction;"In this report, we present the 1st place solution for ICCV 2023 OmniObject3D
Challenge: Sparse-View Reconstruction. The challenge aims to evaluate
approaches for novel view synthesis and surface reconstruction using only a few
posed images of each object. We utilize Pixel-NeRF as the basic model, and
apply depth supervision as well as coarse-to-fine positional encoding. The
experiments demonstrate the effectiveness of our approach in improving
sparse-view reconstruction quality. We ranked first in the final test with a
PSNR of 25.44614.";Hang Du<author:sep>Yaping Xue<author:sep>Weidong Dai<author:sep>Xuejun Yan<author:sep>Jingjing Wang;http://arxiv.org/pdf/2404.10441v1;cs.CV;;nerf
2404.10318v1;http://arxiv.org/abs/2404.10318v1;2024-04-16;SRGS: Super-Resolution 3D Gaussian Splatting;"Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel
explicit 3D representation. This approach relies on the representation power of
Gaussian primitives to provide a high-quality rendering. However, primitives
optimized at low resolution inevitably exhibit sparsity and texture deficiency,
posing a challenge for achieving high-resolution novel view synthesis (HRNVS).
To address this problem, we propose Super-Resolution 3D Gaussian Splatting
(SRGS) to perform the optimization in a high-resolution (HR) space. The
sub-pixel constraint is introduced for the increased viewpoints in HR space,
exploiting the sub-pixel cross-view information of the multiple low-resolution
(LR) views. The gradient accumulated from more viewpoints will facilitate the
densification of primitives. Furthermore, a pre-trained 2D super-resolution
model is integrated with the sub-pixel constraint, enabling these dense
primitives to learn faithful texture features. In general, our method focuses
on densification and texture learning to effectively enhance the representation
ability of primitives. Experimentally, our method achieves high rendering
quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on
challenging datasets such as Mip-NeRF 360 and Tanks & Temples. Related codes
will be released upon acceptance.";Xiang Feng<author:sep>Yongbo He<author:sep>Yubo Wang<author:sep>Yan Yang<author:sep>Zhenzhong Kuang<author:sep>Yu Jun<author:sep>Jianping Fan<author:sep>Jiajun ding;http://arxiv.org/pdf/2404.10318v1;cs.CV;submit ACM MM 2024;gaussian splatting<tag:sep>nerf
2404.10603v1;http://arxiv.org/abs/2404.10603v1;2024-04-16;Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences;"Leveraging multi-view diffusion models as priors for 3D optimization have
alleviated the problem of 3D consistency, e.g., the Janus face problem or the
content drift problem, in zero-shot text-to-3D models. However, the 3D
geometric fidelity of the output remains an unresolved issue; albeit the
rendered 2D views are realistic, the underlying geometry may contain errors
such as unreasonable concavities. In this work, we propose CorrespondentDream,
an effective method to leverage annotation-free, cross-view correspondences
yielded from the diffusion U-Net to provide additional 3D prior to the NeRF
optimization process. We find that these correspondences are strongly
consistent with human perception, and by adopting it in our loss design, we are
able to produce NeRF models with geometries that are more coherent with common
sense, e.g., more smoothed object surface, yielding higher 3D fidelity. We
demonstrate the efficacy of our approach through various comparative
qualitative results and a solid user study.";Seungwook Kim<author:sep>Kejie Li<author:sep>Xueqing Deng<author:sep>Yichun Shi<author:sep>Minsu Cho<author:sep>Peng Wang;http://arxiv.org/pdf/2404.10603v1;cs.CV;25 pages, 22 figures, accepted to CVPR 2024;nerf
2404.10625v1;http://arxiv.org/abs/2404.10625v1;2024-04-16;Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks;"NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or
GIRAFFE have shown very high rendering quality under large representational
variety. However, rendering with Neural Radiance Fields poses challenges for 3D
applications: First, the significant computational demands of NeRF rendering
preclude its use on low-power devices, such as mobiles and VR/AR headsets.
Second, implicit representations based on neural networks are difficult to
incorporate into explicit 3D scenes, such as VR environments or video games. 3D
Gaussian Splatting (3DGS) overcomes these limitations by providing an explicit
3D representation that can be rendered efficiently at high frame rates. In this
work, we present a novel approach that combines the high rendering quality of
NeRF-based 3D-aware GANs with the flexibility and computational advantages of
3DGS. By training a decoder that maps implicit NeRF representations to explicit
3D Gaussian Splatting attributes, we can integrate the representational
diversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting
for the first time. Additionally, our approach allows for a high resolution GAN
inversion and real-time GAN editing with 3D Gaussian Splatting scenes.";Florian Barthel<author:sep>Arian Beckmann<author:sep>Wieland Morgenstern<author:sep>Anna Hilsmann<author:sep>Peter Eisert;http://arxiv.org/pdf/2404.10625v1;cs.CV;CVPRW;gaussian splatting<tag:sep>nerf
2404.10484v1;http://arxiv.org/abs/2404.10484v1;2024-04-16;AbsGS: Recovering Fine Details for 3D Gaussian Splatting;"3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with
differentiable rasterization to achieve high-quality novel view synthesis
results while providing advanced real-time rendering performance. However, due
to the flaw of its adaptive density control strategy in 3D-GS, it frequently
suffers from over-reconstruction issue in intricate scenes containing
high-frequency details, leading to blurry rendered images. The underlying
reason for the flaw has still been under-explored. In this work, we present a
comprehensive analysis of the cause of aforementioned artifacts, namely
gradient collision, which prevents large Gaussians in over-reconstructed
regions from splitting. To address this issue, we propose the novel
homodirectional view-space positional gradient as the criterion for
densification. Our strategy efficiently identifies large Gaussians in
over-reconstructed regions, and recovers fine details by splitting. We evaluate
our proposed method on various challenging datasets. The experimental results
indicate that our approach achieves the best rendering quality with reduced or
similar memory consumption. Our method is easy to implement and can be
incorporated into a wide variety of most recent Gaussian Splatting-based
methods. We will open source our codes upon formal publication. Our project
page is available at: https://ty424.github.io/AbsGS.github.io/";Zongxin Ye<author:sep>Wenyu Li<author:sep>Sidun Liu<author:sep>Peng Qiao<author:sep>Yong Dou;http://arxiv.org/pdf/2404.10484v1;cs.CV;;gaussian splatting
2404.10272v1;http://arxiv.org/abs/2404.10272v1;2024-04-16;Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using  VDB Grid and Hierarchical Ray Traversal;"Transmittance estimators such as Occupancy Grid (OG) can accelerate the
training and rendering of Neural Radiance Field (NeRF) by predicting important
samples that contributes much to the generated image. However, OG manages
occupied regions in the form of the dense binary grid, in which there are many
blocks with the same values that cause redundant examination of voxels'
emptiness in ray-tracing. In our work, we introduce two techniques to improve
the efficiency of ray-tracing in trained OG without fine-tuning. First, we
replace the dense grids with VDB grids to reduce the spatial redundancy.
Second, we use hierarchical digital differential analyzer (HDDA) to efficiently
trace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF
360 datasets show that our proposed method successfully accelerates rendering
NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in
average, compared to a fast implementation of OG, NerfAcc, without losing the
quality of rendered images.";Yoshio Kato<author:sep>Shuhei Tarashima;http://arxiv.org/pdf/2404.10272v1;cs.CV;"Short paper for CVPR Neural Rendering Intelligence Workshop 2024.
  Code: https://github.com/Yosshi999/faster-occgrid";nerf
2404.10772v1;http://arxiv.org/abs/2404.10772v1;2024-04-16;Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in  Unbounded Scenes;"Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view
synthesis results, while allowing the rendering of high-resolution images in
real-time. However, leveraging 3D Gaussians for surface reconstruction poses
significant challenges due to the explicit and disconnected nature of 3D
Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel
approach for efficient, high-quality, and compact surface reconstruction in
unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of
3D Gaussians, enabling direct geometry extraction from 3D Gaussians by
identifying its levelset, without resorting to Poisson reconstruction or TSDF
fusion as in previous work. We approximate the surface normal of Gaussians as
the normal of the ray-Gaussian intersection plane, enabling the application of
regularization that significantly enhances geometry. Furthermore, we develop an
efficient geometry extraction method utilizing marching tetrahedra, where the
tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's
complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based
methods in surface reconstruction and novel view synthesis. Further, it
compares favorably to, or even outperforms, neural implicit methods in both
quality and speed.";Zehao Yu<author:sep>Torsten Sattler<author:sep>Andreas Geiger;http://arxiv.org/pdf/2404.10772v1;cs.CV;"Project page:
  https://niujinshuchong.github.io/gaussian-opacity-fields";gaussian splatting
2404.09458v1;http://arxiv.org/abs/2404.09458v1;2024-04-15;CompGS: Efficient 3D Scene Representation via Compressed Gaussian  Splatting;"Gaussian splatting, renowned for its exceptional rendering quality and
efficiency, has emerged as a prominent technique in 3D scene representation.
However, the substantial data volume of Gaussian splatting impedes its
practical utility in real-world applications. Herein, we propose an efficient
3D scene representation, named Compressed Gaussian Splatting (CompGS), which
harnesses compact Gaussian primitives for faithful 3D scene modeling with a
remarkably reduced data size. To ensure the compactness of Gaussian primitives,
we devise a hybrid primitive structure that captures predictive relationships
between each other. Then, we exploit a small set of anchor primitives for
prediction, allowing the majority of primitives to be encapsulated into highly
compact residual forms. Moreover, we develop a rate-constrained optimization
scheme to eliminate redundancies within such hybrid primitives, steering our
CompGS towards an optimal trade-off between bitrate consumption and
representation efficacy. Experimental results show that the proposed CompGS
significantly outperforms existing methods, achieving superior compactness in
3D scene representation without compromising model accuracy and rendering
quality. Our code will be released on GitHub for further research.";Xiangrui Liu<author:sep>Xinju Wu<author:sep>Pingping Zhang<author:sep>Shiqi Wang<author:sep>Zhu Li<author:sep>Sam Kwong;http://arxiv.org/pdf/2404.09458v1;cs.CV;Submitted to a conference;gaussian splatting
2404.09591v1;http://arxiv.org/abs/2404.09591v1;2024-04-15;3D Gaussian Splatting as Markov Chain Monte Carlo;"While 3D Gaussian Splatting has recently become popular for neural rendering,
current methods rely on carefully engineered cloning and splitting strategies
for placing Gaussians, which does not always generalize and may lead to
poor-quality renderings. In addition, for real-world scenes, they rely on a
good initial point cloud to perform well. In this work, we rethink 3D Gaussians
as random samples drawn from an underlying probability distribution describing
the physical representation of the scene -- in other words, Markov Chain Monte
Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates are
strikingly similar to a Stochastic Langevin Gradient Descent (SGLD) update. As
with MCMC, samples are nothing but past visit locations, adding new Gaussians
under our framework can simply be realized without heuristics as placing
Gaussians at existing Gaussian locations. To encourage using fewer Gaussians
for efficiency, we introduce an L1-regularizer on the Gaussians. On various
standard evaluation scenes, we show that our method provides improved rendering
quality, easy control over the number of Gaussians, and robustness to
initialization.";Shakiba Kheradmand<author:sep>Daniel Rebain<author:sep>Gopal Sharma<author:sep>Weiwei Sun<author:sep>Jeff Tseng<author:sep>Hossam Isack<author:sep>Abhishek Kar<author:sep>Andrea Tagliasacchi<author:sep>Kwang Moo Yi;http://arxiv.org/pdf/2404.09591v1;cs.CV;;gaussian splatting
2404.09748v1;http://arxiv.org/abs/2404.09748v1;2024-04-15;LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted  Gaussian Primitives;"Large garages are ubiquitous yet intricate scenes in our daily lives, posing
challenges characterized by monotonous colors, repetitive patterns, reflective
surfaces, and transparent vehicle glass. Conventional Structure from Motion
(SfM) methods for camera pose estimation and 3D reconstruction fail in these
environments due to poor correspondence construction. To address these
challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting
approach for large-scale garage modeling and rendering. We develop a handheld
scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate
accurate LiDAR and image data scanning. With this Polar device, we present a
GarageWorld dataset consisting of five expansive garage scenes with diverse
geometric structures and will release the dataset to the community for further
research. We demonstrate that the collected LiDAR point cloud by the Polar
device enhances a suite of 3D Gaussian splatting algorithms for garage scene
modeling and rendering. We also propose a novel depth regularizer for 3D
Gaussian splatting algorithm training, effectively eliminating floating
artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian
renderer for real-time viewing on web-based devices. Additionally, we explore a
hybrid representation that combines the advantages of traditional mesh in
depicting simple geometry and colors (e.g., walls and the ground) with modern
3D Gaussian representations capturing complex details and high-frequency
textures. This strategy achieves an optimal balance between memory performance
and rendering quality. Experimental results on our dataset, along with
ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering
quality and resource efficiency.";Jiadi Cui<author:sep>Junming Cao<author:sep>Yuhui Zhong<author:sep>Liao Wang<author:sep>Fuqiang Zhao<author:sep>Penghao Wang<author:sep>Yifan Chen<author:sep>Zhipeng He<author:sep>Lan Xu<author:sep>Yujiao Shi<author:sep>Yingliang Zhang<author:sep>Jingyi Yu;http://arxiv.org/pdf/2404.09748v1;cs.CV;Project Page: https://jdtsui.github.io/letsgo/;gaussian splatting
2404.09833v1;http://arxiv.org/abs/2404.09833v1;2024-04-15;Video2Game: Real-time, Interactive, Realistic and Browser-Compatible  Environment from a Single Video;"Creating high-quality and interactive virtual environments, such as games and
simulators, often involves complex and costly manual modeling processes. In
this paper, we present Video2Game, a novel approach that automatically converts
videos of real-world scenes into realistic and interactive game environments.
At the heart of our system are three core components:(i) a neural radiance
fields (NeRF) module that effectively captures the geometry and visual
appearance of the scene; (ii) a mesh module that distills the knowledge from
NeRF for faster rendering; and (iii) a physics module that models the
interactions and physical dynamics among the objects. By following the
carefully designed pipeline, one can construct an interactable and actionable
digital replica of the real world. We benchmark our system on both indoor and
large-scale outdoor scenes. We show that we can not only produce
highly-realistic renderings in real-time, but also build interactive games on
top.";Hongchi Xia<author:sep>Zhi-Hao Lin<author:sep>Wei-Chiu Ma<author:sep>Shenlong Wang;http://arxiv.org/pdf/2404.09833v1;cs.CV;CVPR 2024. Project page (with code): https://video2game.github.io/;nerf
2404.09995v1;http://arxiv.org/abs/2404.09995v1;2024-04-15;Taming Latent Diffusion Model for Neural Radiance Field Inpainting;"Neural Radiance Field (NeRF) is a representation for 3D reconstruction from
multi-view images. Despite some recent work showing preliminary success in
editing a reconstructed NeRF with diffusion prior, they remain struggling to
synthesize reasonable geometry in completely uncovered regions. One major
reason is the high diversity of synthetic contents from the diffusion model,
which hinders the radiance field from converging to a crisp and deterministic
geometry. Moreover, applying latent diffusion models on real data often yields
a textural shift incoherent to the image condition due to auto-encoding errors.
These two problems are further reinforced with the use of pixel-distance
losses. To address these issues, we propose tempering the diffusion model's
stochasticity with per-scene customization and mitigating the textural shift
with masked adversarial training. During the analyses, we also found the
commonly used pixel and perceptual losses are harmful in the NeRF inpainting
task. Through rigorous experiments, our framework yields state-of-the-art NeRF
inpainting results on various real-world scenes. Project page:
https://hubert0527.github.io/MALD-NeRF";Chieh Hubert Lin<author:sep>Changil Kim<author:sep>Jia-Bin Huang<author:sep>Qinbo Li<author:sep>Chih-Yao Ma<author:sep>Johannes Kopf<author:sep>Ming-Hsuan Yang<author:sep>Hung-Yu Tseng;http://arxiv.org/pdf/2404.09995v1;cs.CV;Project page: https://hubert0527.github.io/MALD-NeRF;nerf
2404.09412v1;http://arxiv.org/abs/2404.09412v1;2024-04-15;DeferredGS: Decoupled and Editable Gaussian Splatting with Deferred  Shading;"Reconstructing and editing 3D objects and scenes both play crucial roles in
computer graphics and computer vision. Neural radiance fields (NeRFs) can
achieve realistic reconstruction and editing results but suffer from
inefficiency in rendering. Gaussian splatting significantly accelerates
rendering by rasterizing Gaussian ellipsoids. However, Gaussian splatting
utilizes a single Spherical Harmonic (SH) function to model both texture and
lighting, limiting independent editing capabilities of these components.
Recently, attempts have been made to decouple texture and lighting with the
Gaussian splatting representation but may fail to produce plausible geometry
and decomposition results on reflective scenes. Additionally, the forward
shading technique they employ introduces noticeable blending artifacts during
relighting, as the geometry attributes of Gaussians are optimized under the
original illumination and may not be suitable for novel lighting conditions. To
address these issues, we introduce DeferredGS, a method for decoupling and
editing the Gaussian splatting representation using deferred shading. To
achieve successful decoupling, we model the illumination with a learnable
environment map and define additional attributes such as texture parameters and
normal direction on Gaussians, where the normal is distilled from a jointly
trained signed distance function. More importantly, we apply deferred shading,
resulting in more realistic relighting effects compared to previous methods.
Both qualitative and quantitative experiments demonstrate the superior
performance of DeferredGS in novel view synthesis and editing tasks.";Tong Wu<author:sep>Jia-Mu Sun<author:sep>Yu-Kun Lai<author:sep>Yuewen Ma<author:sep>Leif Kobbelt<author:sep>Lin Gao;http://arxiv.org/pdf/2404.09412v1;cs.CV;;gaussian splatting<tag:sep>nerf
2404.09271v1;http://arxiv.org/abs/2404.09271v1;2024-04-14;VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field;"Visual relocalization is a key technique to autonomous driving, robotics, and
virtual/augmented reality. After decades of explorations, absolute pose
regression (APR), scene coordinate regression (SCR), and hierarchical methods
(HMs) have become the most popular frameworks. However, in spite of high
efficiency, APRs and SCRs have limited accuracy especially in large-scale
outdoor scenes; HMs are accurate but need to store a large number of 2D
descriptors for matching, resulting in poor efficiency. In this paper, we
propose an efficient and accurate framework, called VRS-NeRF, for visual
relocalization with sparse neural radiance field. Precisely, we introduce an
explicit geometric map (EGM) for 3D map representation and an implicit learning
map (ILM) for sparse patches rendering. In this localization process, EGP
provides priors of spare 2D points and ILM utilizes these sparse points to
render patches with sparse NeRFs for matching. This allows us to discard a
large number of 2D descriptors so as to reduce the map size. Moreover,
rendering patches only for useful points rather than all pixels in the whole
image reduces the rendering time significantly. This framework inherits the
accuracy of HMs and discards their low efficiency. Experiments on 7Scenes,
CambridgeLandmarks, and Aachen datasets show that our method gives much better
accuracy than APRs and SCRs, and close performance to HMs but is much more
efficient.";Fei Xue<author:sep>Ignas Budvytis<author:sep>Daniel Olmeda Reino<author:sep>Roberto Cipolla;http://arxiv.org/pdf/2404.09271v1;cs.CV;source code https://github.com/feixue94/vrs-nerf;nerf
2404.09227v1;http://arxiv.org/abs/2404.09227v1;2024-04-14;DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation  Modeling;"Recent progress in text-to-3D creation has been propelled by integrating the
potent prior of Diffusion Models from text-to-image generation into the 3D
domain. Nevertheless, generating 3D scenes characterized by multiple instances
and intricate arrangements remains challenging. In this study, we present
DreamScape, a method for creating highly consistent 3D scenes solely from
textual descriptions, leveraging the strong 3D representation capabilities of
Gaussian Splatting and the complex arrangement abilities of large language
models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene
representation, consisting of semantic primitives (objects) and their spatial
transformations and relationships derived directly from text prompts using
LLMs. This compositional representation allows for local-to-global optimization
of the entire scene. A progressive scale control is tailored during local
object generation, ensuring that objects of different sizes and densities adapt
to the scene, which addresses training instability issue arising from simple
blending in the subsequent global optimization stage. To mitigate potential
biases of LLM priors, we model collision relationships between objects at the
global level, enhancing physical correctness and overall realism. Additionally,
to generate pervasive objects like rain and snow distributed extensively across
the scene, we introduce a sparse initialization and densification strategy.
Experiments demonstrate that DreamScape offers high usability and
controllability, enabling the generation of high-fidelity 3D scenes from only
text prompts and achieving state-of-the-art performance compared to other
methods.";Xuening Yuan<author:sep>Hongyu Yang<author:sep>Yueming Zhao<author:sep>Di Huang;http://arxiv.org/pdf/2404.09227v1;cs.CV;;gaussian splatting
2404.09105v1;http://arxiv.org/abs/2404.09105v1;2024-04-14;EGGS: Edge Guided Gaussian Splatting for Radiance Fields;"The Gaussian splatting methods are getting popular. However, their loss
function only contains the $\ell_1$ norm and the structural similarity between
the rendered and input images, without considering the edges in these images.
It is well-known that the edges in an image provide important information.
Therefore, in this paper, we propose an Edge Guided Gaussian Splatting (EGGS)
method that leverages the edges in the input images. More specifically, we give
the edge region a higher weight than the flat region. With such edge guidance,
the resulting Gaussian particles focus more on the edges instead of the flat
regions. Moreover, such edge guidance does not crease the computation cost
during the training and rendering stage. The experiments confirm that such
simple edge-weighted loss function indeed improves about $1\sim2$ dB on several
difference data sets. With simply plugging in the edge guidance, the proposed
method can improve all Gaussian splatting methods in different scenarios, such
as human head modeling, building 3D reconstruction, etc.";Yuanhao Gong;http://arxiv.org/pdf/2404.09105v1;cs.CV;;gaussian splatting
2404.08966v2;http://arxiv.org/abs/2404.08966v2;2024-04-13;LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via  Eulerian Motion Field;"Cinemagraph is a unique form of visual media that combines elements of still
photography and subtle motion to create a captivating experience. However, the
majority of videos generated by recent works lack depth information and are
confined to the constraints of 2D image space. In this paper, inspired by
significant progress in the field of novel view synthesis (NVS) achieved by 3D
Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from
2D image space to 3D space using 3D Gaussian modeling. To achieve this, we
first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from
multi-view images of static scenes,incorporating shape regularization terms to
prevent blurring or artifacts caused by object deformation. We then adopt an
autoencoder tailored for 3D Gaussian to project it into feature space. To
maintain the local continuity of the scene, we devise SuperGaussian for
clustering based on the acquired features. By calculating the similarity
between clusters and employing a two-stage estimation method, we derive an
Eulerian motion field to describe velocities across the entire scene. The 3D
Gaussian points then move within the estimated Eulerian motion field. Through
bidirectional animation techniques, we ultimately generate a 3D Cinemagraph
that exhibits natural and seamlessly loopable dynamics. Experiment results
validate the effectiveness of our approach, demonstrating high-quality and
visually appealing scene generation. The project is available at
https://pokerlishao.github.io/LoopGaussian/.";Jiyang Li<author:sep>Lechao Cheng<author:sep>Zhangye Wang<author:sep>Tingting Mu<author:sep>Jingxuan He;http://arxiv.org/pdf/2404.08966v2;cs.CV;10 pages;gaussian splatting
2404.08252v1;http://arxiv.org/abs/2404.08252v1;2024-04-12;MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based  Monocular Guidance;"The latest regularized Neural Radiance Field (NeRF) approaches produce poor
geometry and view extrapolation for multiview stereo (MVS) benchmarks such as
ETH3D. In this paper, we aim to create 3D models that provide accurate geometry
and view synthesis, partially closing the large geometric performance gap
between NeRF and traditional MVS methods. We propose a patch-based approach
that effectively leverages monocular surface normal and relative depth
predictions. The patch-based ray sampling also enables the appearance
regularization of normalized cross-correlation (NCC) and structural similarity
(SSIM) between randomly sampled virtual and training views. We further show
that ""density restrictions"" based on sparse structure-from-motion points can
help greatly improve geometric accuracy with a slight drop in novel view
synthesis metrics. Our experiments show 4x the performance of RegNeRF and 8x
that of FreeNeRF on average F1@2cm for ETH3D MVS benchmark, suggesting a
fruitful research direction to improve the geometric accuracy of NeRF-based
models, and sheds light on a potential future approach to enable NeRF-based
optimization to eventually outperform traditional MVS.";Yuqun Wu<author:sep>Jae Yong Lee<author:sep>Chuhang Zou<author:sep>Shenlong Wang<author:sep>Derek Hoiem;http://arxiv.org/pdf/2404.08252v1;cs.CV;26 pages, 15 figures;nerf
2404.08449v2;http://arxiv.org/abs/2404.08449v2;2024-04-12;OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering;"Rendering dynamic 3D human from monocular videos is crucial for various
applications such as virtual reality and digital entertainment. Most methods
assume the people is in an unobstructed scene, while various objects may cause
the occlusion of body parts in real-life scenarios. Previous method utilizing
NeRF for surface rendering to recover the occluded areas, but it requiring more
than one day to train and several seconds to render, failing to meet the
requirements of real-time interactive applications. To address these issues, we
propose OccGaussian based on 3D Gaussian Splatting, which can be trained within
6 minutes and produces high-quality human renderings up to 160 FPS with
occluded input. OccGaussian initializes 3D Gaussian distributions in the
canonical space, and we perform occlusion feature query at occluded regions,
the aggregated pixel-align feature is extracted to compensate for the missing
information. Then we use Gaussian Feature MLP to further process the feature
along with the occlusion-aware loss functions to better perceive the occluded
area. Extensive experiments both in simulated and real-world occlusions,
demonstrate that our method achieves comparable or even superior performance
compared to the state-of-the-art method. And we improving training and
inference speeds by 250x and 800x, respectively. Our code will be available for
research purposes.";Jingrui Ye<author:sep>Zongkai Zhang<author:sep>Yujiao Jiang<author:sep>Qingmin Liao<author:sep>Wenming Yang<author:sep>Zongqing Lu;http://arxiv.org/pdf/2404.08449v2;cs.CV;;gaussian splatting<tag:sep>nerf
2404.08312v1;http://arxiv.org/abs/2404.08312v1;2024-04-12;GPN: Generative Point-based NeRF;"Scanning real-life scenes with modern registration devices typically gives
incomplete point cloud representations, primarily due to the limitations of
partial scanning, 3D occlusions, and dynamic light conditions. Recent works on
processing incomplete point clouds have always focused on point cloud
completion. However, these approaches do not ensure consistency between the
completed point cloud and the captured images regarding color and geometry. We
propose using Generative Point-based NeRF (GPN) to reconstruct and repair a
partial cloud by fully utilizing the scanning images and the corresponding
reconstructed cloud. The repaired point cloud can achieve multi-view
consistency with the captured images at high spatial resolution. For the
finetunes of a single scene, we optimize the global latent condition by
incorporating an Auto-Decoder architecture while retaining multi-view
consistency. As a result, the generated point clouds are smooth, plausible, and
geometrically consistent with the partial scanning images. Extensive
experiments on ShapeNet demonstrate that our works achieve competitive
performances to the other state-of-the-art point cloud-based neural scene
rendering and editing performances.";Haipeng Wang;http://arxiv.org/pdf/2404.08312v1;cs.CV;;nerf
2404.07474v1;http://arxiv.org/abs/2404.07474v1;2024-04-11;G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images;"Novel view synthesis aims to generate new view images of a given view image
collection. Recent attempts address this problem relying on 3D geometry priors
(e.g., shapes, sizes, and positions) learned from multi-view images. However,
such methods encounter the following limitations: 1) they require a set of
multi-view images as training data for a specific scene (e.g., face, car or
chair), which is often unavailable in many real-world scenarios; 2) they fail
to extract the geometry priors from single-view images due to the lack of
multi-view supervision. In this paper, we propose a Geometry-enhanced NeRF
(G-NeRF), which seeks to enhance the geometry priors by a geometry-guided
multi-view synthesis approach, followed by a depth-aware training. In the
synthesis process, inspired that existing 3D GAN models can unconditionally
synthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D
GAN models, such as EG3D, as a free source to provide geometry priors through
synthesizing multi-view data. Simultaneously, to further improve the geometry
quality of the synthetic data, we introduce a truncation method to effectively
sample latent codes within 3D GAN models. To tackle the absence of multi-view
supervision for single-view images, we design the depth-aware training
approach, incorporating a depth-aware discriminator to guide geometry priors
through depth maps. Experiments demonstrate the effectiveness of our method in
terms of both qualitative and quantitative results.";Zixiong Huang<author:sep>Qi Chen<author:sep>Libo Sun<author:sep>Yifan Yang<author:sep>Naizhou Wang<author:sep>Mingkui Tan<author:sep>Qi Wu;http://arxiv.org/pdf/2404.07474v1;cs.CV;CVPR 2024 Accepted Paper;nerf
2404.07933v1;http://arxiv.org/abs/2404.07933v1;2024-04-11;Boosting Self-Supervision for Single-View Scene Completion via Knowledge  Distillation;"Inferring scene geometry from images via Structure from Motion is a
long-standing and fundamental problem in computer vision. While classical
approaches and, more recently, depth map predictions only focus on the visible
parts of a scene, the task of scene completion aims to reason about geometry
even in occluded regions. With the popularity of neural radiance fields
(NeRFs), implicit representations also became popular for scene completion by
predicting so-called density fields. Unlike explicit approaches. e.g.
voxel-based methods, density fields also allow for accurate depth prediction
and novel-view synthesis via image-based rendering. In this work, we propose to
fuse the scene reconstruction from multiple images and distill this knowledge
into a more accurate single-view scene reconstruction. To this end, we propose
Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed
images, trained fully self-supervised only from image data. Using knowledge
distillation, we use MVBTS to train a single-view scene completion network via
direct supervision called KDBTS. It achieves state-of-the-art performance on
occupancy prediction, especially in occluded regions.";Keonhee Han<author:sep>Dominik Muhle<author:sep>Felix Wimbauer<author:sep>Daniel Cremers;http://arxiv.org/pdf/2404.07933v1;cs.CV;;nerf
2404.07993v1;http://arxiv.org/abs/2404.07993v1;2024-04-11;Connecting NeRFs, Images, and Text;"Neural Radiance Fields (NeRFs) have emerged as a standard framework for
representing 3D scenes and objects, introducing a novel data type for
information exchange and storage. Concurrently, significant progress has been
made in multimodal representation learning for text and image data. This paper
explores a novel research direction that aims to connect the NeRF modality with
other modalities, similar to established methodologies for images and text. To
this end, we propose a simple framework that exploits pre-trained models for
NeRF representations alongside multimodal models for text and image processing.
Our framework learns a bidirectional mapping between NeRF embeddings and those
obtained from corresponding images and text. This mapping unlocks several novel
and useful applications, including NeRF zero-shot classification and NeRF
retrieval from images or text.";Francesco Ballerini<author:sep>Pierluigi Zama Ramirez<author:sep>Roberto Mirabella<author:sep>Samuele Salti<author:sep>Luigi Di Stefano;http://arxiv.org/pdf/2404.07993v1;cs.CV;Accepted at CVPRW-INRV 2024;nerf
2404.07762v2;http://arxiv.org/abs/2404.07762v2;2024-04-11;NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous  Driving;"We present a versatile NeRF-based simulator for testing autonomous driving
(AD) software systems, designed with a focus on sensor-realistic closed-loop
evaluation and the creation of safety-critical scenarios. The simulator learns
from sequences of real-world driving sensor data and enables reconfigurations
and renderings of new, unseen scenarios. In this work, we use our simulator to
test the responses of AD models to safety-critical scenarios inspired by the
European New Car Assessment Programme (Euro NCAP). Our evaluation reveals that,
while state-of-the-art end-to-end planners excel in nominal driving scenarios
in an open-loop setting, they exhibit critical flaws when navigating our
safety-critical scenarios in a closed-loop setting. This highlights the need
for advancements in the safety and real-world usability of end-to-end planners.
By publicly releasing our simulator and scenarios as an easy-to-run evaluation
suite, we invite the research community to explore, refine, and validate their
AD models in controlled, yet highly configurable and challenging
sensor-realistic environments. Code and instructions can be found at
https://github.com/wljungbergh/NeuroNCAP";William Ljungbergh<author:sep>Adam Tonderski<author:sep>Joakim Johnander<author:sep>Holger Caesar<author:sep>Kalle ÃstrÃ¶m<author:sep>Michael Felsberg<author:sep>Christoffer Petersson;http://arxiv.org/pdf/2404.07762v2;cs.CV;;nerf
2404.07991v1;http://arxiv.org/abs/2404.07991v1;2024-04-11;GoMAvatar: Efficient Animatable Human Modeling from Monocular Video  Using Gaussians-on-Mesh;"We introduce GoMAvatar, a novel approach for real-time, memory-efficient,
high-quality animatable human modeling. GoMAvatar takes as input a single
monocular video to create a digital avatar capable of re-articulation in new
poses and real-time rendering from novel viewpoints, while seamlessly
integrating with rasterization-based graphics pipelines. Central to our method
is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering
quality and speed of Gaussian splatting with geometry modeling and
compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and
various YouTube videos. GoMAvatar matches or surpasses current monocular human
modeling algorithms in rendering quality and significantly outperforms them in
computational efficiency (43 FPS) while being memory-efficient (3.63 MB per
subject).";Jing Wen<author:sep>Xiaoming Zhao<author:sep>Zhongzheng Ren<author:sep>Alexander G. Schwing<author:sep>Shenlong Wang;http://arxiv.org/pdf/2404.07991v1;cs.CV;"CVPR 2024; project page: https://wenj.github.io/GoMAvatar/";gaussian splatting
2404.07199v1;http://arxiv.org/abs/2404.07199v1;2024-04-10;RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth  Diffusion;"We introduce RealmDreamer, a technique for generation of general
forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D
Gaussian Splatting representation to match complex text prompts. We initialize
these splats by utilizing the state-of-the-art text-to-image generators,
lifting their samples into 3D, and computing the occlusion volume. We then
optimize this representation across multiple views as a 3D inpainting task with
image-conditional diffusion models. To learn correct geometric structure, we
incorporate a depth diffusion model by conditioning on the samples from the
inpainting model, giving rich geometric structure. Finally, we finetune the
model using sharpened samples from image generators. Notably, our technique
does not require video or multi-view data and can synthesize a variety of
high-quality 3D scenes in different styles, consisting of multiple objects. Its
generality additionally allows 3D synthesis from a single image.";Jaidev Shriram<author:sep>Alex Trevithick<author:sep>Lingjie Liu<author:sep>Ravi Ramamoorthi;http://arxiv.org/pdf/2404.07199v1;cs.CV;Project Page: https://realmdreamer.github.io/;gaussian splatting
2404.06926v1;http://arxiv.org/abs/2404.06926v1;2024-04-10;Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D  Gaussian Splatting;"We present a real-time LiDAR-Inertial-Camera SLAM system with 3D Gaussian
Splatting as the mapping backend. Leveraging robust pose estimates from our
LiDAR-Inertial-Camera odometry, Coco-LIC, an incremental photo-realistic
mapping system is proposed in this paper. We initialize 3D Gaussians from
colorized LiDAR points and optimize them using differentiable rendering powered
by 3D Gaussian Splatting. Meticulously designed strategies are employed to
incrementally expand the Gaussian map and adaptively control its density,
ensuring high-quality mapping with real-time capability. Experiments conducted
in diverse scenarios demonstrate the superior performance of our method
compared to existing radiance-field-based SLAM systems.";Xiaolei Lang<author:sep>Laijian Li<author:sep>Hang Zhang<author:sep>Feng Xiong<author:sep>Mu Xu<author:sep>Yong Liu<author:sep>Xingxing Zuo<author:sep>Jiajun Lv;http://arxiv.org/pdf/2404.06926v1;cs.RO;Submitted to IROS 2024;gaussian splatting
2404.06903v1;http://arxiv.org/abs/2404.06903v1;2024-04-10;DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic  Gaussian Splatting;"The increasing demand for virtual reality applications has highlighted the
significance of crafting immersive 3D assets. We present a text-to-3D
360$^{\circ}$ scene generation pipeline that facilitates the creation of
comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of
minutes. Our approach utilizes the generative power of a 2D diffusion model and
prompt self-refinement to create a high-quality and globally coherent panoramic
image. This image acts as a preliminary ""flat"" (2D) scene representation.
Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to
enable real-time exploration. To produce consistent 3D geometry, our pipeline
constructs a spatially coherent structure by aligning the 2D monocular depth
into a globally optimized point cloud. This point cloud serves as the initial
state for the centroids of 3D Gaussians. In order to address invisible issues
inherent in single-view inputs, we impose semantic and geometric constraints on
both synthesized and input camera views as regularizations. These guide the
optimization of Gaussians, aiding in the reconstruction of unseen regions. In
summary, our method offers a globally consistent 3D scene within a
360$^{\circ}$ perspective, providing an enhanced immersive experience over
existing techniques. Project website at: http://dreamscene360.github.io/";Shijie Zhou<author:sep>Zhiwen Fan<author:sep>Dejia Xu<author:sep>Haoran Chang<author:sep>Pradyumna Chari<author:sep>Tejas Bharadwaj<author:sep>Suya You<author:sep>Zhangyang Wang<author:sep>Achuta Kadambi;http://arxiv.org/pdf/2404.06903v1;cs.CV;;gaussian splatting
2404.06832v1;http://arxiv.org/abs/2404.06832v1;2024-04-10;SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection;"Detecting anomalies in images has become a well-explored problem in both
academia and industry. State-of-the-art algorithms are able to detect defects
in increasingly difficult settings and data modalities. However, most current
methods are not suited to address 3D objects captured from differing poses.
While solutions using Neural Radiance Fields (NeRFs) have been proposed, they
suffer from excessive computation requirements, which hinder real-world
usability. For this reason, we propose the novel 3D Gaussian splatting-based
framework SplatPose which, given multi-view images of a 3D object, accurately
estimates the pose of unseen views in a differentiable manner, and detects
anomalies in them. We achieve state-of-the-art results in both training and
inference speed, and detection performance, even when using less training data
than competing methods. We thoroughly evaluate our framework using the recently
proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly
detection (MAD) data set.";Mathis Kruse<author:sep>Marco Rudolph<author:sep>Dominik Woiwode<author:sep>Bodo Rosenhahn;http://arxiv.org/pdf/2404.06832v1;cs.CV;Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024;gaussian splatting<tag:sep>nerf
2404.06753v1;http://arxiv.org/abs/2404.06753v1;2024-04-10;MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D  Reconstruction of Indoor Scenes from Monocular RGB Views;"Current monocular 3D scene reconstruction (3DR) works are either
fully-supervised, or not generalizable, or implicit in 3D representation. We
propose a novel framework - MonoSelfRecon that for the first time achieves
explicit 3D mesh reconstruction for generalizable indoor scenes with monocular
RGB views by purely self-supervision on voxel-SDF (signed distance function).
MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and
a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF
in self-supervision. We propose novel self-supervised losses, which not only
support pure self-supervision, but can be used together with supervised signals
to further boost supervised training. Our experiments show that ""MonoSelfRecon""
trained in pure self-supervision outperforms current best self-supervised
indoor depth estimation models and is comparable to 3DR models trained in fully
supervision with depth annotations. MonoSelfRecon is not restricted by specific
model design, which can be used to any models with voxel-SDF for purely
self-supervised manner.";Runfa Li<author:sep>Upal Mahbub<author:sep>Vasudev Bhaskaran<author:sep>Truong Nguyen;http://arxiv.org/pdf/2404.06753v1;cs.CV;;nerf
2404.06710v3;http://arxiv.org/abs/2404.06710v3;2024-04-10;SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike  Camera;"One of the most critical factors in achieving sharp Novel View Synthesis
(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) is the quality of the training images. However,
Conventional RGB cameras are susceptible to motion blur. In contrast,
neuromorphic cameras like event and spike cameras inherently capture more
comprehensive temporal information, which can provide a sharp representation of
the scene as additional training data. Recent methods have explored the
integration of event cameras to improve the quality of NVS. The event-RGB
approaches have some limitations, such as high training costs and the inability
to work effectively in the background. Instead, our study introduces a new
method that uses the spike camera to overcome these limitations. By considering
texture reconstruction from spike streams as ground truth, we design the
Texture from Spike (TfS) loss. Since the spike camera relies on temporal
integration instead of temporal differentiation used by event cameras, our
proposed TfS loss maintains manageable training costs. It handles foreground
objects with backgrounds simultaneously. We also provide a real-world dataset
captured with our spike-RGB camera system to facilitate future research
endeavors. We conduct extensive experiments using synthetic and real-world
datasets to demonstrate that our design can enhance novel view synthesis across
NeRF and 3DGS. The code and dataset will be made available for public access.";Gaole Dai<author:sep>Zhenyu Wang<author:sep>Qinwen Xu<author:sep>Ming Lu<author:sep>Wen Chen<author:sep>Boxin Shi<author:sep>Shanghang Zhang<author:sep>Tiejun Huang;http://arxiv.org/pdf/2404.06710v3;cs.CV;;gaussian splatting<tag:sep>nerf
2404.06814v1;http://arxiv.org/abs/2404.06814v1;2024-04-10;Zero-shot Point Cloud Completion Via 2D Priors;"3D point cloud completion is designed to recover complete shapes from
partially observed point clouds. Conventional completion methods typically
depend on extensive point cloud data for training %, with their effectiveness
often constrained to object categories similar to those seen during training.
In contrast, we propose a zero-shot framework aimed at completing partially
observed point clouds across any unseen categories. Leveraging point rendering
via Gaussian Splatting, we develop techniques of Point Cloud Colorization and
Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion
models to infer missing regions. Experimental results on both synthetic and
real-world scanned point clouds demonstrate that our approach outperforms
existing methods in completing a variety of objects without any requirement for
specific training data.";Tianxin Huang<author:sep>Zhiwen Yan<author:sep>Yuyang Zhao<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2404.06814v1;cs.CV;;gaussian splatting
2404.06727v1;http://arxiv.org/abs/2404.06727v1;2024-04-10;Bayesian NeRF: Quantifying Uncertainty with Volume Density in Neural  Radiance Fields;"We present the Bayesian Neural Radiance Field (NeRF), which explicitly
quantifies uncertainty in geometric volume structures without the need for
additional networks, making it adept for challenging observations and
uncontrolled images. NeRF diverges from traditional geometric methods by
offering an enriched scene representation, rendering color and density in 3D
space from various viewpoints. However, NeRF encounters limitations in relaxing
uncertainties by using geometric structure information, leading to inaccuracies
in interpretation under insufficient real-world observations. Recent research
efforts aimed at addressing this issue have primarily relied on empirical
methods or auxiliary networks. To fundamentally address this issue, we propose
a series of formulational extensions to NeRF. By introducing generalized
approximations and defining density-related uncertainty, our method seamlessly
extends to manage uncertainty not only for RGB but also for depth, without the
need for additional networks or empirical assumptions. In experiments we show
that our method significantly enhances performance on RGB and depth images in
the comprehensive dataset, demonstrating the reliability of the Bayesian NeRF
approach to quantifying uncertainty based on the geometric structure.";Sibeak Lee<author:sep>Kyeongsu Kang<author:sep>Hyeonwoo Yu;http://arxiv.org/pdf/2404.06727v1;cs.CV;;nerf
2404.06109v1;http://arxiv.org/abs/2404.06109v1;2024-04-09;Revising Densification in Gaussian Splatting;"In this paper, we address the limitations of Adaptive Density Control (ADC)
in 3D Gaussian Splatting (3DGS), a scene representation method achieving
high-quality, photorealistic results for novel view synthesis. ADC has been
introduced for automatic 3D point primitive management, controlling
densification and pruning, however, with certain limitations in the
densification logic. Our main contribution is a more principled, pixel-error
driven formulation for density control in 3DGS, leveraging an auxiliary,
per-pixel error function as the criterion for densification. We further
introduce a mechanism to control the total number of primitives generated per
scene and correct a bias in the current opacity handling strategy of ADC during
cloning operations. Our approach leads to consistent quality improvements
across a variety of benchmark scenes, without sacrificing the method's
efficiency.";Samuel Rota BulÃ²<author:sep>Lorenzo Porzi<author:sep>Peter Kontschieder;http://arxiv.org/pdf/2404.06109v1;cs.CV;;gaussian splatting
2404.06246v1;http://arxiv.org/abs/2404.06246v1;2024-04-09;GHNeRF: Learning Generalizable Human Features with Efficient Neural  Radiance Fields;"Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising
results in 3D scene representations, including 3D human representations.
However, these representations often lack crucial information on the underlying
human pose and structure, which is crucial for AR/VR applications and games. In
this paper, we introduce a novel approach, termed GHNeRF, designed to address
these limitations by learning 2D/3D joint locations of human subjects with NeRF
representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract
essential human features from 2D images, which are then incorporated into the
NeRF framework in order to encode human biomechanic features. This allows our
network to simultaneously learn biomechanic features, such as joint locations,
along with human geometry and texture. To assess the effectiveness of our
method, we conduct a comprehensive comparison with state-of-the-art human NeRF
techniques and joint estimation algorithms. Our results show that GHNeRF can
achieve state-of-the-art results in near real-time.";Arnab Dey<author:sep>Di Yang<author:sep>Rohith Agaram<author:sep>Antitza Dantcheva<author:sep>Andrew I. Comport<author:sep>Srinath Sridhar<author:sep>Jean Martinet;http://arxiv.org/pdf/2404.06246v1;cs.CV;;nerf
2404.06270v2;http://arxiv.org/abs/2404.06270v2;2024-04-09;3D Geometry-aware Deformable Gaussian Splatting for Dynamic View  Synthesis;"In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting
method for dynamic view synthesis. Existing neural radiance fields (NeRF) based
solutions learn the deformation in an implicit manner, which cannot incorporate
3D scene geometry. Therefore, the learned deformation is not necessarily
geometrically coherent, which results in unsatisfactory dynamic view synthesis
and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new
representation of the 3D scene, building upon which the 3D geometry could be
exploited in learning the complex 3D deformation. Specifically, the scenes are
represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized
to move and rotate over time to model the deformation. To enforce the 3D scene
geometry constraint during deformation, we explicitly extract 3D geometry
features and integrate them in learning the 3D deformation. In this way, our
solution achieves 3D geometry-aware deformation modeling, which enables
improved dynamic view synthesis and 3D dynamic reconstruction. Extensive
experimental results on both synthetic and real datasets prove the superiority
of our solution, which achieves new state-of-the-art performance.
  The project is available at https://npucvr.github.io/GaGS/";Zhicheng Lu<author:sep>Xiang Guo<author:sep>Le Hui<author:sep>Tianrui Chen<author:sep>Min Yang<author:sep>Xiao Tang<author:sep>Feng Zhu<author:sep>Yuchao Dai;http://arxiv.org/pdf/2404.06270v2;cs.CV;Accepted by CVPR 2024. Project page: https://npucvr.github.io/GaGS/;gaussian splatting<tag:sep>nerf
2404.06128v1;http://arxiv.org/abs/2404.06128v1;2024-04-09;Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for  Realistic Endoscopic Reconstruction;"Within colorectal cancer diagnostics, conventional colonoscopy techniques
face critical limitations, including a limited field of view and a lack of
depth information, which can impede the detection of precancerous lesions.
Current methods struggle to provide comprehensive and accurate 3D
reconstructions of the colonic surface which can help minimize the missing
regions and reinspection for pre-cancerous polyps. Addressing this, we
introduce 'Gaussian Pancakes', a method that leverages 3D Gaussian Splatting
(3D GS) combined with a Recurrent Neural Network-based Simultaneous
Localization and Mapping (RNNSLAM) system. By introducing geometric and depth
regularization into the 3D GS framework, our approach ensures more accurate
alignment of Gaussians with the colon surface, resulting in smoother 3D
reconstructions with novel viewing of detailed textures and structures.
Evaluations across three diverse datasets show that Gaussian Pancakes enhances
novel view synthesis quality, surpassing current leading methods with a 18%
boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster
rendering and more than 10X shorter training times, making it a practical tool
for real-time applications. Hence, this holds promise for achieving clinical
translation for better detection and diagnosis of colorectal cancer.";Sierra Bonilla<author:sep>Shuai Zhang<author:sep>Dimitrios Psychogyios<author:sep>Danail Stoyanov<author:sep>Francisco Vasconcelos<author:sep>Sophia Bano;http://arxiv.org/pdf/2404.06128v1;cs.CV;12 pages, 5 figures;gaussian splatting
2404.06429v1;http://arxiv.org/abs/2404.06429v1;2024-04-09;Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion;"Benefiting from the rapid development of 2D diffusion models, 3D content
creation has made significant progress recently. One promising solution
involves the fine-tuning of pre-trained 2D diffusion models to harness their
capacity for producing multi-view images, which are then lifted into accurate
3D models via methods like fast-NeRFs or large reconstruction models. However,
as inconsistency still exists and limited generated resolution, the generation
results of such methods still lack intricate textures and complex geometries.
To solve this problem, we propose Magic-Boost, a multi-view conditioned
diffusion model that significantly refines coarse generative results through a
brief period of SDS optimization ($\sim15$min). Compared to the previous text
or single image based diffusion models, Magic-Boost exhibits a robust
capability to generate images with high consistency from pseudo synthesized
multi-view images. It provides precise SDS guidance that well aligns with the
identity of the input images, enriching the local detail in both geometry and
texture of the initial generative results. Extensive experiments show
Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D
assets with rich geometric and textural details. (Project Page:
https://magic-research.github.io/magic-boost/)";Fan Yang<author:sep>Jianfeng Zhang<author:sep>Yichun Shi<author:sep>Bowen Chen<author:sep>Chenxu Zhang<author:sep>Huichao Zhang<author:sep>Xiaofeng Yang<author:sep>Jiashi Feng<author:sep>Guosheng Lin;http://arxiv.org/pdf/2404.06429v1;cs.CV;;nerf
2404.06152v1;http://arxiv.org/abs/2404.06152v1;2024-04-09;HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields;"In recent advancements in novel view synthesis, generalizable Neural Radiance
Fields (NeRF) based methods applied to human subjects have shown remarkable
results in generating novel views from few images. However, this generalization
ability cannot capture the underlying structural features of the skeleton
shared across all instances. Building upon this, we introduce HFNeRF: a novel
generalizable human feature NeRF aimed at generating human biomechanic features
using a pre-trained image encoder. While previous human NeRF methods have shown
promising results in the generation of photorealistic virtual avatars, such
methods lack underlying human structure or biomechanic features such as
skeleton or joint information that are crucial for downstream applications
including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D
pre-trained foundation models toward learning human features in 3D using neural
rendering, and then volume rendering towards generating 2D feature maps. We
evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as
features. The proposed method is fully differentiable, allowing to successfully
learn color, geometry, and human skeleton in a simultaneous manner. This paper
presents preliminary results of HFNeRF, illustrating its potential in
generating realistic virtual avatars with biomechanic features using NeRF.";Arnab Dey<author:sep>Di Yang<author:sep>Antitza Dantcheva<author:sep>Jean Martinet;http://arxiv.org/pdf/2404.06152v1;cs.CV;;nerf
2404.06091v1;http://arxiv.org/abs/2404.06091v1;2024-04-09;Hash3D: Training-free Acceleration for 3D Generation;"The evolution of 3D generative modeling has been notably propelled by the
adoption of 2D diffusion models. Despite this progress, the cumbersome
optimization process per se presents a critical hurdle to efficiency. In this
paper, we introduce Hash3D, a universal acceleration for 3D generation without
model training. Central to Hash3D is the insight that feature-map redundancy is
prevalent in images rendered from camera positions and diffusion time-steps in
close proximity. By effectively hashing and reusing these feature maps across
neighboring timesteps and camera angles, Hash3D substantially prevents
redundant calculations, thus accelerating the diffusion model's inference in 3D
generation tasks. We achieve this through an adaptive grid-based hashing.
Surprisingly, this feature-sharing mechanism not only speed up the generation
but also enhances the smoothness and view consistency of the synthesized 3D
objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models,
demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency
by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian
splatting largely speeds up 3D model creation, reducing text-to-3D processing
to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The
project page is at https://adamdad.github.io/hash3D/.";Xingyi Yang<author:sep>Xinchao Wang;http://arxiv.org/pdf/2404.06091v1;cs.CV;https://adamdad.github.io/hash3D/;
2404.05163v1;http://arxiv.org/abs/2404.05163v1;2024-04-08;Semantic Flow: Learning Semantic Field of Dynamic Scenes from Monocular  Videos;"In this work, we pioneer Semantic Flow, a neural semantic representation of
dynamic scenes from monocular videos. In contrast to previous NeRF methods that
reconstruct dynamic scenes from the colors and volume densities of individual
points, Semantic Flow learns semantics from continuous flows that contain rich
3D motion information. As there is 2D-to-3D ambiguity problem in the viewing
direction when extracting 3D flow features from 2D video frames, we consider
the volume densities as opacity priors that describe the contributions of flow
features to the semantics on the frames. More specifically, we first learn a
flow network to predict flows in the dynamic scene, and propose a flow feature
aggregation module to extract flow features from video frames. Then, we propose
a flow attention module to extract motion information from flow features, which
is followed by a semantic network to output semantic logits of flows. We
integrate the logits with volume densities in the viewing direction to
supervise the flow features with semantic labels on video frames. Experimental
results show that our model is able to learn from multiple dynamic scenes and
supports a series of new tasks such as instance-level scene editing, semantic
completions, dynamic scene tracking and semantic adaption on novel scenes.
Codes are available at https://github.com/tianfr/Semantic-Flow/.";Fengrui Tian<author:sep>Yueqi Duan<author:sep>Angtian Wang<author:sep>Jianfei Guo<author:sep>Shaoyi Du;http://arxiv.org/pdf/2404.05163v1;cs.CV;"Accepted by ICLR 2024, Codes are available at
  https://github.com/tianfr/Semantic-Flow/";nerf
2404.05220v1;http://arxiv.org/abs/2404.05220v1;2024-04-08;StylizedGS: Controllable Stylization for 3D Gaussian Splatting;"With the rapid development of XR, 3D generation and editing are becoming more
and more important, among which, stylization is an important tool of 3D
appearance editing. It can achieve consistent 3D artistic stylization given a
single reference style image and thus is a user-friendly editing way. However,
recent NeRF-based 3D stylization methods face efficiency issues that affect the
actual user experience and the implicit nature limits its ability to transfer
the geometric pattern styles. Additionally, the ability for artists to exert
flexible control over stylized scenes is considered highly desirable, fostering
an environment conducive to creative exploration. In this paper, we introduce
StylizedGS, a 3D neural style transfer framework with adaptable control over
perceptual factors based on 3D Gaussian Splatting (3DGS) representation. The
3DGS brings the benefits of high efficiency. We propose a GS filter to
eliminate floaters in the reconstruction which affects the stylization effects
before stylization. Then the nearest neighbor-based style loss is introduced to
achieve stylization by fine-tuning the geometry and color parameters of 3DGS,
while a depth preservation loss with other regularizations is proposed to
prevent the tampering of geometry content. Moreover, facilitated by specially
designed losses, StylizedGS enables users to control color, stylized scale and
regions during the stylization to possess customized capabilities. Our method
can attain high-quality stylization results characterized by faithful
brushstrokes and geometric consistency with flexible controls. Extensive
experiments across various scenes and styles demonstrate the effectiveness and
efficiency of our method concerning both stylization quality and inference FPS.";Dingxi Zhang<author:sep>Zhuoxun Chen<author:sep>Yu-Jie Yuan<author:sep>Fang-Lue Zhang<author:sep>Zhenliang He<author:sep>Shiguang Shan<author:sep>Lin Gao;http://arxiv.org/pdf/2404.05220v1;cs.CV;;gaussian splatting<tag:sep>nerf
2404.05236v1;http://arxiv.org/abs/2404.05236v1;2024-04-08;Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation;"Recently, a surge of 3D style transfer methods has been proposed that
leverage the scene reconstruction power of a pre-trained neural radiance field
(NeRF). To successfully stylize a scene this way, one must first reconstruct a
photo-realistic radiance field from collected images of the scene. However,
when only sparse input views are available, pre-trained few-shot NeRFs often
suffer from high-frequency artifacts, which are generated as a by-product of
high-frequency details for improving reconstruction quality. Is it possible to
generate more faithful stylized scenes from sparse inputs by directly
optimizing encoding-based scene representation with target style? In this
paper, we consider the stylization of sparse-view scenes in terms of
disentangling content semantics and style textures. We propose a coarse-to-fine
sparse-view scene stylization framework, where a novel hierarchical
encoding-based neural representation is designed to generate high-quality
stylized scenes directly from implicit scene representations. We also propose a
new optimization strategy with content strength annealing to achieve realistic
stylization and better content preservation. Extensive experiments demonstrate
that our method can achieve high-quality stylization of sparse-view scenes and
outperforms fine-tuning-based baselines in terms of stylization quality and
efficiency.";Y. Wang<author:sep>A. Gao<author:sep>Y. Gong<author:sep>Y. Zeng;http://arxiv.org/pdf/2404.05236v1;cs.CV;;nerf
2404.04875v1;http://arxiv.org/abs/2404.04875v1;2024-04-07;NeRF2Points: Large-Scale Point Cloud Generation From Street Views'  Radiance Field Optimization;"Neural Radiance Fields (NeRF) have emerged as a paradigm-shifting methodology
for the photorealistic rendering of objects and environments, enabling the
synthesis of novel viewpoints with remarkable fidelity. This is accomplished
through the strategic utilization of object-centric camera poses characterized
by significant inter-frame overlap. This paper explores a compelling,
alternative utility of NeRF: the derivation of point clouds from aggregated
urban landscape imagery. The transmutation of street-view data into point
clouds is fraught with complexities, attributable to a nexus of interdependent
variables. First, high-quality point cloud generation hinges on precise camera
poses, yet many datasets suffer from inaccuracies in pose metadata. Also, the
standard approach of NeRF is ill-suited for the distinct characteristics of
street-view data from autonomous vehicles in vast, open settings. Autonomous
vehicle cameras often record with limited overlap, leading to blurring,
artifacts, and compromised pavement representation in NeRF-based point clouds.
In this paper, we present NeRF2Points, a tailored NeRF variant for urban point
cloud synthesis, notable for its high-quality output from RGB inputs alone. Our
paper is supported by a bespoke, high-resolution 20-kilometer urban street
dataset, designed for point cloud generation and evaluation. NeRF2Points
adeptly navigates the inherent challenges of NeRF-based point cloud synthesis
through the implementation of the following strategic innovations: (1)
Integration of Weighted Iterative Geometric Optimization (WIGO) and Structure
from Motion (SfM) for enhanced camera pose accuracy, elevating street-view data
precision. (2) Layered Perception and Integrated Modeling (LPiM) is designed
for distinct radiance field modeling in urban environments, resulting in
coherent point cloud representations.";Peng Tu<author:sep>Xun Zhou<author:sep>Mingming Wang<author:sep>Xiaojun Yang<author:sep>Bo Peng<author:sep>Ping Chen<author:sep>Xiu Su<author:sep>Yawen Huang<author:sep>Yefeng Zheng<author:sep>Chang Xu;http://arxiv.org/pdf/2404.04875v1;cs.CV;18 pages;nerf
2404.04913v1;http://arxiv.org/abs/2404.04913v1;2024-04-07;CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality  Novel-view Synthesis;"Neural Radiance Fields (NeRF) have achieved huge success in effectively
capturing and representing 3D objects and scenes. However, several factors have
impeded its further proliferation as next-generation 3D media. To establish a
ubiquitous presence in everyday media formats, such as images and videos, it is
imperative to devise a solution that effectively fulfills three key objectives:
fast encoding and decoding time, compact model sizes, and high-quality
renderings. Despite significant advancements, a comprehensive algorithm that
adequately addresses all objectives has yet to be fully realized. In this work,
we present CodecNeRF, a neural codec for NeRF representations, consisting of a
novel encoder and decoder architecture that can generate a NeRF representation
in a single forward pass. Furthermore, inspired by the recent
parameter-efficient finetuning approaches, we develop a novel finetuning method
to efficiently adapt the generated NeRF representations to a new test instance,
leading to high-quality image renderings and compact code sizes. The proposed
CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF,
achieved unprecedented compression performance of more than 150x and 20x
reduction in encoding time while maintaining (or improving) the image quality
on widely used 3D object datasets, such as ShapeNet and Objaverse.";Gyeongjin Kang<author:sep>Younggeun Lee<author:sep>Eunbyung Park;http://arxiv.org/pdf/2404.04913v1;cs.CV;"34 pages, 22 figures, Project page:
  https://gynjn.github.io/Codec-NeRF/";nerf
2404.04880v1;http://arxiv.org/abs/2404.04880v1;2024-04-07;GauU-Scene V2: Expanse Lidar Image Dataset Shows Unreliable Geometric  Reconstruction Using Gaussian Splatting and NeRF;"We introduce a novel large-scale scene reconstruction benchmark that utilizes
newly developed 3D representation approaches: Gaussian Splatting and Neural
Radiance Fields, on our expansive GauU-Scene V2 dataset. GauU-Scene V2
encompasses over 6.5 square kilometers and features a comprehensive RGB dataset
coupled with LiDAR ground truth. This dataset offers a unique blend of urban
and academic environments for advanced spatial analysis, covering more than 6.5
km2. We also provide detailed supplementary information on data collection
protocols. Furthermore, we present an easy-to-follow pipeline to align the
COLMAP sparse point cloud with the detailed LiDAR dataset. Our evaluation of
U-Scene, which includes a detailed analysis across various novel viewpoints
using image-based metrics such as SSIM, LPIPS, and PSNR, shows contradictory
results when applying geometric-based metrics, such as Chamfer distance. This
leads to doubts about the reliability of current image-based measurement
matrices and geometric extraction methods on Gaussian Splatting. We also make
the dataset available on the following anonymous project page";Butian Xiong<author:sep>Nanjun Zheng<author:sep>Zhen Li;http://arxiv.org/pdf/2404.04880v1;cs.CV;8 pages(No reference) 6 figures 4 tabs;gaussian splatting<tag:sep>nerf
2404.04908v1;http://arxiv.org/abs/2404.04908v1;2024-04-07;Dual-Camera Smooth Zoom on Mobile Phones;"When zooming between dual cameras on a mobile, noticeable jumps in geometric
content and image color occur in the preview, inevitably affecting the user's
zoom experience. In this work, we introduce a new task, ie, dual-camera smooth
zoom (DCSZ) to achieve a smooth zoom preview. The frame interpolation (FI)
technique is a potential solution but struggles with ground-truth collection.
To address the issue, we suggest a data factory solution where continuous
virtual cameras are assembled to generate DCSZ data by rendering reconstructed
3D models of the scene. In particular, we propose a novel dual-camera smooth
zoom Gaussian Splatting (ZoomGS), where a camera-specific encoding is
introduced to construct a specific 3D model for each virtual camera. With the
proposed data factory, we construct a synthetic dataset for DCSZ, and we
utilize it to fine-tune FI models. In addition, we collect real-world dual-zoom
images without ground-truth for evaluation. Extensive experiments are conducted
with multiple FI methods. The results show that the fine-tuned FI models
achieve a significant performance improvement over the original ones on DCSZ
task. The datasets, codes, and pre-trained models will be publicly available.";Renlong Wu<author:sep>Zhilu Zhang<author:sep>Yu Yang<author:sep>Wangmeng Zuo;http://arxiv.org/pdf/2404.04908v1;cs.CV;24;gaussian splatting
2404.04526v1;http://arxiv.org/abs/2404.04526v1;2024-04-06;DATENeRF: Depth-Aware Text-based Editing of NeRFs;"Recent advancements in diffusion models have shown remarkable proficiency in
editing 2D images based on text prompts. However, extending these techniques to
edit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual
2D frames can result in inconsistencies across multiple views. Our crucial
insight is that a NeRF scene's geometry can serve as a bridge to integrate
these 2D edits. Utilizing this geometry, we employ a depth-conditioned
ControlNet to enhance the coherence of each 2D image modification. Moreover, we
introduce an inpainting approach that leverages the depth information of NeRF
scenes to distribute 2D edits across different images, ensuring robustness
against errors and resampling challenges. Our results reveal that this
methodology achieves more consistent, lifelike, and detailed edits than
existing leading methods for text-driven NeRF scene editing.";Sara Rojas<author:sep>Julien Philip<author:sep>Kai Zhang<author:sep>Sai Bi<author:sep>Fujun Luan<author:sep>Bernard Ghanem<author:sep>Kalyan Sunkavall;http://arxiv.org/pdf/2404.04526v1;cs.CV;"14 pages, Conference paper, 3D Scene Editing, Neural Rendering,
  Diffusion Models";nerf
2404.04687v1;http://arxiv.org/abs/2404.04687v1;2024-04-06;Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion;"Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent
technique in computer vision and graphics for reconstructing 3D scenes. GS
represents a scene as a set of 3D Gaussians with varying opacities and employs
a computationally efficient splatting operation along with analytical
derivatives to compute the 3D Gaussian parameters given scene images captured
from various viewpoints. Unfortunately, capturing surround view ($360^{\circ}$
viewpoint) images is impossible or impractical in many real-world imaging
scenarios, including underwater imaging, rooms inside a building, and
autonomous navigation. In these restricted baseline imaging scenarios, the GS
algorithm suffers from a well-known 'missing cone' problem, which results in
poor reconstruction along the depth axis. In this manuscript, we demonstrate
that using transient data (from sonars) allows us to address the missing cone
problem by sampling high-frequency data along the depth axis. We extend the
Gaussian splatting algorithms for two commonly used sonars and propose fusion
algorithms that simultaneously utilize RGB camera data and sonar data. Through
simulations, emulations, and hardware experiments across various imaging
scenarios, we show that the proposed fusion algorithms lead to significantly
better novel view synthesis (5 dB improvement in PSNR) and 3D geometry
reconstruction (60% lower Chamfer distance).";Ziyuan Qu<author:sep>Omkar Vengurlekar<author:sep>Mohamad Qadri<author:sep>Kevin Zhang<author:sep>Michael Kaess<author:sep>Christopher Metzler<author:sep>Suren Jayasuriya<author:sep>Adithya Pediredla;http://arxiv.org/pdf/2404.04687v1;cs.CV;;gaussian splatting
2404.04211v1;http://arxiv.org/abs/2404.04211v1;2024-04-05;Robust Gaussian Splatting;"In this paper, we address common error sources for 3D Gaussian Splatting
(3DGS) including blur, imperfect camera poses, and color inconsistencies, with
the goal of improving its robustness for practical applications like
reconstructions from handheld phone captures. Our main contribution involves
modeling motion blur as a Gaussian distribution over camera poses, allowing us
to address both camera pose refinement and motion blur correction in a unified
way. Additionally, we propose mechanisms for defocus blur compensation and for
addressing color in-consistencies caused by ambient light, shadows, or due to
camera-related factors like varying white balancing settings. Our proposed
solutions integrate in a seamless way with the 3DGS formulation while
maintaining its benefits in terms of training efficiency and rendering speed.
We experimentally validate our contributions on relevant benchmark datasets
including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and
thus consistent improvements over relevant baselines.";FranÃ§ois Darmon<author:sep>Lorenzo Porzi<author:sep>Samuel Rota-BulÃ²<author:sep>Peter Kontschieder;http://arxiv.org/pdf/2404.04211v1;cs.CV;;gaussian splatting<tag:sep>nerf
2404.03613v1;http://arxiv.org/abs/2404.03613v1;2024-04-04;Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian  Splatting;"As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view
synthesis, it is a natural extension to deform a canonical 3DGS to multiple
frames. However, previous works fail to accurately reconstruct dynamic scenes,
especially 1) static parts moving along nearby dynamic parts, and 2) some
dynamic areas are blurry. We attribute the failure to the wrong design of the
deformation field, which is built as a coordinate-based function. This approach
is problematic because 3DGS is a mixture of multiple fields centered at the
Gaussians, not just a single coordinate-based framework. To resolve this
problem, we define the deformation as a function of per-Gaussian embeddings and
temporal embeddings. Moreover, we decompose deformations as coarse and fine
deformations to model slow and fast movements, respectively. Also, we introduce
an efficient training strategy for faster convergence and higher quality.
Project page: https://jeongminb.github.io/e-d3dgs/";Jeongmin Bae<author:sep>Seoha Kim<author:sep>Youngsik Yun<author:sep>Hahyun Lee<author:sep>Gun Bang<author:sep>Youngjung Uh;http://arxiv.org/pdf/2404.03613v1;cs.CV;Preprint;gaussian splatting
2404.03654v2;http://arxiv.org/abs/2404.03654v2;2024-04-04;RaFE: Generative Radiance Fields Restoration;"NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel
view synthesis and 3D reconstruction, but its performance is sensitive to input
image quality, which struggles to achieve high-fidelity rendering when provided
with low-quality sparse input viewpoints. Previous methods for NeRF restoration
are tailored for specific degradation type, ignoring the generality of
restoration. To overcome this limitation, we propose a generic radiance fields
restoration pipeline, named RaFE, which applies to various types of
degradations, such as low resolution, blurriness, noise, compression artifacts,
or their combinations. Our approach leverages the success of off-the-shelf 2D
restoration methods to recover the multi-view images individually. Instead of
reconstructing a blurred NeRF by averaging inconsistencies, we introduce a
novel approach using Generative Adversarial Networks (GANs) for NeRF generation
to better accommodate the geometric and appearance inconsistencies present in
the multi-view images. Specifically, we adopt a two-level tri-plane
architecture, where the coarse level remains fixed to represent the low-quality
NeRF, and a fine-level residual tri-plane to be added to the coarse level is
modeled as a distribution with GAN to capture potential variations in
restoration. We validate RaFE on both synthetic and real cases for various
restoration tasks, demonstrating superior performance in both quantitative and
qualitative evaluations, surpassing other 3D restoration methods specific to
single task. Please see our project website
https://zkaiwu.github.io/RaFE-Project/.";Zhongkai Wu<author:sep>Ziyu Wan<author:sep>Jing Zhang<author:sep>Jing Liao<author:sep>Dong Xu;http://arxiv.org/pdf/2404.03654v2;cs.CV;Project Page: https://zkaiwu.github.io/RaFE;nerf
2404.03650v1;http://arxiv.org/abs/2404.03650v1;2024-04-04;OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features  and Rendered Novel Views;"Large visual-language models (VLMs), like CLIP, enable open-set image
segmentation to segment arbitrary concepts from an image in a zero-shot manner.
This goes beyond the traditional closed-set assumption, i.e., where models can
only segment classes from a pre-defined training set. More recently, first
works on open-set segmentation in 3D scenes have appeared in the literature.
These methods are heavily influenced by closed-set 3D convolutional approaches
that process point clouds or polygon meshes. However, these 3D scene
representations do not align well with the image-based nature of the
visual-language models. Indeed, point cloud and 3D meshes typically have a
lower resolution than images and the reconstructed 3D scene geometry might not
project well to the underlying 2D image sequences used to compute pixel-aligned
CLIP features. To address these challenges, we propose OpenNeRF which naturally
operates on posed images and directly encodes the VLM features within the NeRF.
This is similar in spirit to LERF, however our work shows that using pixel-wise
VLM features (instead of global CLIP features) results in an overall less
complex architecture without the need for additional DINO regularization. Our
OpenNeRF further leverages NeRF's ability to render novel views and extract
open-set VLM features from areas that are not well observed in the initial
posed images. For 3D point cloud segmentation on the Replica dataset, OpenNeRF
outperforms recent open-vocabulary methods such as LERF and OpenScene by at
least +4.9 mIoU.";Francis Engelmann<author:sep>Fabian Manhardt<author:sep>Michael Niemeyer<author:sep>Keisuke Tateno<author:sep>Marc Pollefeys<author:sep>Federico Tombari;http://arxiv.org/pdf/2404.03650v1;cs.CV;ICLR 2024, Project page: https://opennerf.github.io;nerf
2404.03736v1;http://arxiv.org/abs/2404.03736v1;2024-04-04;SC4D: Sparse-Controlled Video-to-4D Generation and Motion Transfer;"Recent advances in 2D/3D generative models enable the generation of dynamic
3D objects from a single-view video. Existing approaches utilize score
distillation sampling to form the dynamic scene as dynamic NeRF or dense 3D
Gaussians. However, these methods struggle to strike a balance among reference
view alignment, spatio-temporal consistency, and motion fidelity under
single-view conditions due to the implicit nature of NeRF or the intricate
dense Gaussian motion prediction. To address these issues, this paper proposes
an efficient, sparse-controlled video-to-4D framework named SC4D, that
decouples motion and appearance to achieve superior video-to-4D generation.
Moreover, we introduce Adaptive Gaussian (AG) initialization and Gaussian
Alignment (GA) loss to mitigate shape degeneration issue, ensuring the fidelity
of the learned motion and shape. Comprehensive experimental results demonstrate
that our method surpasses existing methods in both quality and efficiency. In
addition, facilitated by the disentangled modeling of motion and appearance of
SC4D, we devise a novel application that seamlessly transfers the learned
motion onto a diverse array of 4D entities according to textual descriptions.";Zijie Wu<author:sep>Chaohui Yu<author:sep>Yanqin Jiang<author:sep>Chenjie Cao<author:sep>Fan Wang<author:sep>Xiang Bai;http://arxiv.org/pdf/2404.03736v1;cs.CV;Project Page: https://sc4d.github.io/;nerf
2404.03202v2;http://arxiv.org/abs/2404.03202v2;2024-04-04;OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field  Reconstruction using Omnidirectional Images;"Photorealistic reconstruction relying on 3D Gaussian Splatting has shown
promising potential in robotics. However, the current 3D Gaussian Splatting
system only supports radiance field reconstruction using undistorted
perspective images. In this paper, we present OmniGS, a novel omnidirectional
Gaussian splatting system, to take advantage of omnidirectional images for fast
radiance field reconstruction. Specifically, we conduct a theoretical analysis
of spherical camera model derivatives in 3D Gaussian Splatting. According to
the derivatives, we then implement a new GPU-accelerated omnidirectional
rasterizer that directly splats 3D Gaussians onto the equirectangular screen
space for omnidirectional image rendering. As a result, we realize
differentiable optimization of the radiance field without the requirement of
cube-map rectification or tangent-plane approximation. Extensive experiments
conducted in egocentric and roaming scenarios demonstrate that our method
achieves state-of-the-art reconstruction quality and high rendering speed using
omnidirectional images. To benefit the research community, the code will be
made publicly available once the paper is published.";Longwei Li<author:sep>Huajian Huang<author:sep>Sai-Kit Yeung<author:sep>Hui Cheng;http://arxiv.org/pdf/2404.03202v2;cs.CV;7 pages, 4 figures;gaussian splatting
2404.03349v1;http://arxiv.org/abs/2404.03349v1;2024-04-04;VF-NeRF: Viewshed Fields for Rigid NeRF Registration;"3D scene registration is a fundamental problem in computer vision that seeks
the best 6-DoF alignment between two scenes. This problem was extensively
investigated in the case of point clouds and meshes, but there has been
relatively limited work regarding Neural Radiance Fields (NeRF). In this paper,
we consider the problem of rigid registration between two NeRFs when the
position of the original cameras is not given. Our key novelty is the
introduction of Viewshed Fields (VF), an implicit function that determines, for
each 3D point, how likely it is to be viewed by the original cameras. We
demonstrate how VF can help in the various stages of NeRF registration, with an
extensive evaluation showing that VF-NeRF achieves SOTA results on various
datasets with different capturing approaches such as LLFF and Objaverese.";Leo Segre<author:sep>Shai Avidan;http://arxiv.org/pdf/2404.03349v1;cs.CV;;nerf
2404.03126v1;http://arxiv.org/abs/2404.03126v1;2024-04-04;GaSpCT: Gaussian Splatting for Novel CT Projection View Synthesis;"We present GaSpCT, a novel view synthesis and 3D scene representation method
used to generate novel projection views for Computer Tomography (CT) scans. We
adapt the Gaussian Splatting framework to enable novel view synthesis in CT
based on limited sets of 2D image projections and without the need for
Structure from Motion (SfM) methodologies. Therefore, we reduce the total
scanning duration and the amount of radiation dose the patient receives during
the scan. We adapted the loss function to our use-case by encouraging a
stronger background and foreground distinction using two sparsity promoting
regularizers: a beta loss and a total variation (TV) loss. Finally, we
initialize the Gaussian locations across the 3D space using a uniform prior
distribution of where the brain's positioning would be expected to be within
the field of view. We evaluate the performance of our model using brain CT
scans from the Parkinson's Progression Markers Initiative (PPMI) dataset and
demonstrate that the rendered novel views closely match the original projection
views of the simulated scan, and have better performance than other implicit 3D
scene representations methodologies. Furthermore, we empirically observe
reduced training time compared to neural network based image synthesis for
sparse-view CT image reconstruction. Finally, the memory requirements of the
Gaussian Splatting representations are reduced by 17% compared to the
equivalent voxel grid image representations.";Emmanouil Nikolakakis<author:sep>Utkarsh Gupta<author:sep>Jonathan Vengosh<author:sep>Justin Bui<author:sep>Razvan Marinescu;http://arxiv.org/pdf/2404.03126v1;eess.IV;Under Review Process for MICCAI 2024;gaussian splatting
2404.02514v1;http://arxiv.org/abs/2404.02514v1;2024-04-03;Freditor: High-Fidelity and Transferable NeRF Editing by Frequency  Decomposition;"This paper enables high-fidelity, transferable NeRF editing by frequency
decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D
scenes while suffering from blurry results, and fail to capture detailed
structures caused by the inconsistency between 2D editings. Our critical
insight is that low-frequency components of images are more
multiview-consistent after editing compared with their high-frequency parts.
Moreover, the appearance style is mainly exhibited on the low-frequency
components, and the content details especially reside in high-frequency parts.
This motivates us to perform editing on low-frequency components, which results
in high-fidelity edited scenes. In addition, the editing is performed in the
low-frequency feature space, enabling stable intensity control and novel scene
transfer. Comprehensive experiments conducted on photorealistic datasets
demonstrate the superior performance of high-fidelity and transferable NeRF
editing. The project page is at \url{https://aigc3d.github.io/freditor}.";Yisheng He<author:sep>Weihao Yuan<author:sep>Siyu Zhu<author:sep>Zilong Dong<author:sep>Liefeng Bo<author:sep>Qixing Huang;http://arxiv.org/pdf/2404.02514v1;cs.CV;;nerf
2404.02788v1;http://arxiv.org/abs/2404.02788v1;2024-04-03;GenN2N: Generative NeRF2NeRF Translation;"We present GenN2N, a unified NeRF-to-NeRF translation framework for various
NeRF translation tasks such as text-driven NeRF editing, colorization,
super-resolution, inpainting, etc. Unlike previous methods designed for
individual translation tasks with task-specific schemes, GenN2N achieves all
these NeRF editing tasks by employing a plug-and-play image-to-image translator
to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF
space. Since the 3D consistency of 2D edits may not be assured, we propose to
model the distribution of the underlying 3D edits through a generative model
that can cover all possible edited NeRFs. To model the distribution of 3D
edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes
images while decoding NeRFs. The latent space is trained to align with a
Gaussian distribution and the NeRFs are supervised through an adversarial loss
on its renderings. To ensure the latent code does not depend on 2D viewpoints
but truly reflects the 3D edits, we also regularize the latent code through a
contrastive learning scheme. Extensive experiments on various editing tasks
show GenN2N, as a universal framework, performs as well or better than
task-specific specialists while possessing flexible generative power. More
results on our project page: https://xiangyueliu.github.io/GenN2N/";Xiangyue Liu<author:sep>Han Xue<author:sep>Kunming Luo<author:sep>Ping Tan<author:sep>Li Yi;http://arxiv.org/pdf/2404.02788v1;cs.CV;"Accepted to CVPR 2024. Project page:
  https://xiangyueliu.github.io/GenN2N/";nerf
2404.02410v1;http://arxiv.org/abs/2404.02410v1;2024-04-03;TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding  Autonomous Driving Scenes;"Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize
3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR
data capabilities but also overlooks the potential advantages of fusing LiDAR
with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera
Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both
LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and
novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D
mesh) and implicit (hierarchical octree feature) 3D representation derived from
LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D
Gaussian's properties are not only initialized in alignment with the 3D mesh
which provides more completed 3D shape and color information, but are also
endowed with broader contextual information through retrieved octree implicit
features. During the Gaussian Splatting optimization process, the 3D mesh
offers dense depth information as supervision, which enhances the training
process by learning of a robust geometry. Comprehensive evaluations conducted
on the Waymo Open Dataset and nuScenes Dataset validate our method's
state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our
method demonstrates fast training and achieves real-time RGB and depth
rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in
resolution of 1600x900 (nuScenes) in urban scenarios.";Cheng Zhao<author:sep>Su Sun<author:sep>Ruoyu Wang<author:sep>Yuliang Guo<author:sep>Jun-Jun Wan<author:sep>Zhou Huang<author:sep>Xinyu Huang<author:sep>Yingjie Victor Chen<author:sep>Liu Ren;http://arxiv.org/pdf/2404.02410v1;cs.CV;;gaussian splatting
2404.02742v1;http://arxiv.org/abs/2404.02742v1;2024-04-03;LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis;"Although neural radiance fields (NeRFs) have achieved triumphs in image novel
view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS
methods employ a simple shift from image NVS methods while ignoring the dynamic
nature and the large-scale reconstruction problem of LiDAR point clouds. In
light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for
novel space-time LiDAR view synthesis. In consideration of the sparsity and
large-scale characteristics, we design a 4D hybrid representation combined with
multi-planar and grid features to achieve effective reconstruction in a
coarse-to-fine manner. Furthermore, we introduce geometric constraints derived
from point clouds to improve temporal consistency. For the realistic synthesis
of LiDAR point clouds, we incorporate the global optimization of ray-drop
probability to preserve cross-region patterns. Extensive experiments on
KITTI-360 and NuScenes datasets demonstrate the superiority of our method in
accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes
are available at https://github.com/ispc-lab/LiDAR4D.";Zehan Zheng<author:sep>Fan Lu<author:sep>Weiyi Xue<author:sep>Guang Chen<author:sep>Changjun Jiang;http://arxiv.org/pdf/2404.02742v1;cs.CV;"Accepted by CVPR 2024. Project Page:
  https://dyfcalid.github.io/LiDAR4D";nerf
2404.02617v1;http://arxiv.org/abs/2404.02617v1;2024-04-03;Neural Radiance Fields with Torch Units;"Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction
methods widely used in industrial applications. Although prevalent methods
achieve considerable improvements in small-scale scenes, accomplishing
reconstruction in complex and large-scale scenes is still challenging. First,
the background in complex scenes shows a large variance among different views.
Second, the current inference pattern, $i.e.$, a pixel only relies on an
individual camera ray, fails to capture contextual information. To solve these
problems, we propose to enlarge the ray perception field and build up the
sample points interactions. In this paper, we design a novel inference pattern
that encourages a single camera ray possessing more contextual information, and
models the relationship among sample points on each camera ray. To hold
contextual information,a camera ray in our proposed method can render a patch
of pixels simultaneously. Moreover, we replace the MLP in neural radiance field
models with distance-aware convolutions to enhance the feature propagation
among sample points from the same camera ray. To summarize, as a torchlight, a
ray in our proposed method achieves rendering a patch of image. Thus, we call
the proposed method, Torch-NeRF. Extensive experiments on KITTI-360 and LLFF
show that the Torch-NeRF exhibits excellent performance.";Bingnan Ni<author:sep>Huanyu Wang<author:sep>Dongfeng Bai<author:sep>Minghe Weng<author:sep>Dexin Qi<author:sep>Weichao Qiu<author:sep>Bingbing Liu;http://arxiv.org/pdf/2404.02617v1;cs.CV;;nerf
2404.01810v1;http://arxiv.org/abs/2404.01810v1;2024-04-02;Surface Reconstruction from Gaussian Splatting via Novel Stereo Views;"The Gaussian splatting for radiance field rendering method has recently
emerged as an efficient approach for accurate scene representation. It
optimizes the location, size, color, and shape of a cloud of 3D Gaussian
elements to visually match, after projection, or splatting, a set of given
images taken from various viewing directions. And yet, despite the proximity of
Gaussian elements to the shape boundaries, direct surface reconstruction of
objects in the scene is a challenge.
  We propose a novel approach for surface reconstruction from Gaussian
splatting models. Rather than relying on the Gaussian elements' locations as a
prior for surface reconstruction, we leverage the superior novel-view synthesis
capabilities of 3DGS. To that end, we use the Gaussian splatting model to
render pairs of stereo-calibrated novel views from which we extract depth
profiles using a stereo matching method. We then combine the extracted RGB-D
images into a geometrically consistent surface. The resulting reconstruction is
more accurate and shows finer details when compared to other methods for
surface reconstruction from Gaussian splatting models, while requiring
significantly less compute time compared to other surface reconstruction
methods.
  We performed extensive testing of the proposed method on in-the-wild scenes,
taken by a smartphone, showcasing its superior reconstruction abilities.
Additionally, we tested the proposed method on the Tanks and Temples benchmark,
and it has surpassed the current leading method for surface reconstruction from
Gaussian splatting models. Project page: https://gs2mesh.github.io/.";Yaniv Wolf<author:sep>Amit Bracha<author:sep>Ron Kimmel;http://arxiv.org/pdf/2404.01810v1;cs.CV;Project Page: https://gs2mesh.github.io/;gaussian splatting
2404.02185v1;http://arxiv.org/abs/2404.02185v1;2024-04-02;NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for  Memory-Efficient Scene Representation;"The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene
modeling and novel-view synthesis. As a kind of visual media for 3D scene
representation, compression with high rate-distortion performance is an eternal
target. Motivated by advances in neural compression and neural field
representation, we propose NeRFCodec, an end-to-end NeRF compression framework
that integrates non-linear transform, quantization, and entropy coding for
memory-efficient scene representation. Since training a non-linear transform
directly on a large scale of NeRF feature planes is impractical, we discover
that pre-trained neural 2D image codec can be utilized for compressing the
features when adding content-specific parameters. Specifically, we reuse neural
2D image codec but modify its encoder and decoder heads, while keeping the
other parts of the pre-trained decoder frozen. This allows us to train the full
pipeline via supervision of rendering loss and entropy loss, yielding the
rate-distortion balance by updating the content-specific parameters. At test
time, the bitstreams containing latent code, feature decoder head, and other
side information are transmitted for communication. Experimental results
demonstrate our method outperforms existing NeRF compression methods, enabling
high-quality novel view synthesis with a memory budget of 0.5 MB.";Sicheng Li<author:sep>Hao Li<author:sep>Yiyi Liao<author:sep>Lu Yu;http://arxiv.org/pdf/2404.02185v1;cs.CV;Accepted at CVPR2024. The source code will be released;nerf
2404.02155v1;http://arxiv.org/abs/2404.02155v1;2024-04-02;Alpha Invariance: On Inverse Scaling Between Distance and Volume Density  in Neural Radiance Fields;"Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of
volumetric densities in neural radiance fields, i.e., the densities double when
scene size is halved, and vice versa. We call this property alpha invariance.
For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing
both distance and volume densities in log space, and 2) a
discretization-agnostic initialization strategy to guarantee high ray
transmittance. We revisit a few popular radiance field models and find that
these systems use various heuristics to deal with issues arising from scene
scaling. We test their behaviors and show our recipe to be more robust.";Joshua Ahn<author:sep>Haochen Wang<author:sep>Raymond A. Yeh<author:sep>Greg Shakhnarovich;http://arxiv.org/pdf/2404.02155v1;cs.CV;CVPR 2024. project page https://pals.ttic.edu/p/alpha-invariance;nerf
2404.01812v1;http://arxiv.org/abs/2404.01812v1;2024-04-02;Uncertainty-aware Active Learning of NeRF-based Object Models for Robot  Manipulators using Visual and Re-orientation Actions;"Manipulating unseen objects is challenging without a 3D representation, as
objects generally have occluded surfaces. This requires physical interaction
with objects to build their internal representations. This paper presents an
approach that enables a robot to rapidly learn the complete 3D model of a given
object for manipulation in unfamiliar orientations. We use an ensemble of
partially constructed NeRF models to quantify model uncertainty to determine
the next action (a visual or re-orientation action) by optimizing
informativeness and feasibility. Further, our approach determines when and how
to grasp and re-orient an object given its partial NeRF model and re-estimates
the object pose to rectify misalignments introduced during the interaction.
Experiments with a simulated Franka Emika Robot Manipulator operating in a
tabletop environment with benchmark objects demonstrate an improvement of (i)
14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth
reconstruction of the object surface (F-score) and (iii) 71% in the task
success rate of manipulating objects a-priori unseen orientations/stable
configurations in the scene; over current methods. The project page can be
found here: https://actnerf.github.io.";Saptarshi Dasgupta<author:sep>Akshat Gupta<author:sep>Shreshth Tuli<author:sep>Rohan Paul;http://arxiv.org/pdf/2404.01812v1;cs.RO;"This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible";nerf
2404.01133v1;http://arxiv.org/abs/2404.01133v1;2024-04-01;CityGaussian: Real-time High-quality Large-Scale Scene Rendering with  Gaussians;"The advancement of real-time 3D scene reconstruction and novel view synthesis
has been significantly propelled by 3D Gaussian Splatting (3DGS). However,
effectively training large-scale 3DGS and rendering it in real-time across
various scales remains challenging. This paper introduces CityGaussian
(CityGS), which employs a novel divide-and-conquer training approach and
Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and
rendering. Specifically, the global scene prior and adaptive training data
selection enables efficient training and seamless fusion. Based on fused
Gaussian primitives, we generate different detail levels through compression,
and realize fast rendering across various scales through the proposed
block-wise detail levels selection and aggregation strategy. Extensive
experimental results on large-scale scenes demonstrate that our approach
attains state-of-theart rendering quality, enabling consistent real-time
rendering of largescale scenes across vastly different scales. Our project page
is available at https://dekuliutesla.github.io/citygs/.";Yang Liu<author:sep>He Guan<author:sep>Chuanchen Luo<author:sep>Lue Fan<author:sep>Junran Peng<author:sep>Zhaoxiang Zhang;http://arxiv.org/pdf/2404.01133v1;cs.CV;Project Page: https://dekuliutesla.github.io/citygs/;gaussian splatting
2404.00923v1;http://arxiv.org/abs/2404.00923v1;2024-04-01;MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,  Depth, and Inertial Measurements;"Simultaneous localization and mapping is essential for position tracking and
scene understanding. 3D Gaussian-based map representations enable
photorealistic reconstruction and real-time rendering of scenes using multiple
posed cameras. We show for the first time that using 3D Gaussians for map
representation with unposed camera images and inertial measurements can enable
accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural
radiance field-based representations by enabling faster rendering, scale
awareness, and improved trajectory tracking. Our framework enables
keyframe-based mapping and tracking utilizing loss functions that incorporate
relative pose transformations from pre-integrated inertial measurements, depth
estimates, and measures of photometric rendering quality. We also release a
multi-modal dataset, UT-MM, collected from a mobile robot equipped with a
camera and an inertial measurement unit. Experimental evaluation on several
scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking
and 5% improvement in photometric rendering quality compared to the current
3DGS SLAM state-of-the-art, while allowing real-time rendering of a
high-resolution dense 3D map. Project Webpage:
https://vita-group.github.io/MM3DGS-SLAM";Lisong C. Sun<author:sep>Neel P. Bhatt<author:sep>Jonathan C. Liu<author:sep>Zhiwen Fan<author:sep>Zhangyang Wang<author:sep>Todd E. Humphreys<author:sep>Ufuk Topcu;http://arxiv.org/pdf/2404.00923v1;cs.CV;Project Webpage: https://vita-group.github.io/MM3DGS-SLAM;gaussian splatting
2404.01241v2;http://arxiv.org/abs/2404.01241v2;2024-04-01;StructLDM: Structured Latent Diffusion for 3D Human Generation;"Recent 3D human generative models have achieved remarkable progress by
learning 3D-aware GANs from 2D images. However, existing 3D human generative
methods model humans in a compact 1D latent space, ignoring the articulated
structure and semantics of human body topology. In this paper, we explore more
expressive and higher-dimensional latent space for 3D human modeling and
propose StructLDM, a diffusion-based unconditional 3D human generative model,
which is learned from 2D images. StructLDM solves the challenges imposed due to
the high-dimensional growth of latent space with three key designs: 1) A
semantic structured latent space defined on the dense surface manifold of a
statistical human body template. 2) A structured 3D-aware auto-decoder that
factorizes the global latent space into several semantic body parts
parameterized by a set of conditional structured local NeRFs anchored to the
body template, which embeds the properties learned from the 2D training data
and can be decoded to render view-consistent humans under different poses and
clothing styles. 3) A structured latent diffusion model for generative human
appearance sampling. Extensive experiments validate StructLDM's
state-of-the-art generation performance and illustrate the expressiveness of
the structured latent space over the well-adopted 1D latent space. Notably,
StructLDM enables different levels of controllable 3D human generation and
editing, including pose/view/shape control, and high-level tasks including
compositional generations, part-aware clothing editing, 3D virtual try-on, etc.
Our project page is at: https://taohuumd.github.io/projects/StructLDM/.";Tao Hu<author:sep>Fangzhou Hong<author:sep>Ziwei Liu;http://arxiv.org/pdf/2404.01241v2;cs.CV;Project page: https://taohuumd.github.io/projects/StructLDM/;nerf
2404.00992v1;http://arxiv.org/abs/2404.00992v1;2024-04-01;SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency  Guidance;"Neural Radiance Field (NeRF) technology has made significant strides in
creating novel viewpoints. However, its effectiveness is hampered when working
with sparsely available views, often leading to performance dips due to
overfitting. FreeNeRF attempts to overcome this limitation by integrating
implicit geometry regularization, which incrementally improves both geometry
and textures. Nonetheless, an initial low positional encoding bandwidth results
in the exclusion of high-frequency elements. The quest for a holistic approach
that simultaneously addresses overfitting and the preservation of
high-frequency details remains ongoing. This study introduces a novel feature
matching based sparse geometry regularization module. This module excels in
pinpointing high-frequency keypoints, thereby safeguarding the integrity of
fine details. Through progressive refinement of geometry and textures across
NeRF iterations, we unveil an effective few-shot neural rendering architecture,
designated as SGCNeRF, for enhanced novel view synthesis. Our experiments
demonstrate that SGCNeRF not only achieves superior geometry-consistent
outcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in
PSNR on the LLFF and DTU datasets, respectively.";Yuru Xiao<author:sep>Xianming Liu<author:sep>Deming Zhai<author:sep>Kui Jiang<author:sep>Junjun Jiang<author:sep>Xiangyang Ji;http://arxiv.org/pdf/2404.00992v1;cs.CV;;nerf
2404.01400v1;http://arxiv.org/abs/2404.01400v1;2024-04-01;NVINS: Robust Visual Inertial Navigation Fused with NeRF-augmented  Camera Pose Regressor and Uncertainty Quantification;"In recent years, Neural Radiance Fields (NeRF) have emerged as a powerful
tool for 3D reconstruction and novel view synthesis. However, the computational
cost of NeRF rendering and degradation in quality due to the presence of
artifacts pose significant challenges for its application in real-time and
robust robotic tasks, especially on embedded systems. This paper introduces a
novel framework that integrates NeRF-derived localization information with
Visual-Inertial Odometry(VIO) to provide a robust solution for robotic
navigation in a real-time. By training an absolute pose regression network with
augmented image data rendered from a NeRF and quantifying its uncertainty, our
approach effectively counters positional drift and enhances system reliability.
We also establish a mathematically sound foundation for combining visual
inertial navigation with camera localization neural networks, considering
uncertainty under a Bayesian framework. Experimental validation in the
photorealistic simulation environment demonstrates significant improvements in
accuracy compared to a conventional VIO approach.";Juyeop Han<author:sep>Lukas Lao Beyer<author:sep>Guilherme V. Cavalheiro<author:sep>Sertac Karaman;http://arxiv.org/pdf/2404.01400v1;cs.RO;8 pages, 5 figures, 2 tables;nerf
2404.00891v1;http://arxiv.org/abs/2404.00891v1;2024-04-01;Marrying NeRF with Feature Matching for One-step Pose Estimation;"Given the image collection of an object, we aim at building a real-time
image-based pose estimation method, which requires neither its CAD model nor
hours of object-specific training. Recent NeRF-based methods provide a
promising solution by directly optimizing the pose from pixel loss between
rendered and target images. However, during inference, they require long
converging time, and suffer from local minima, making them impractical for
real-time robot applications. We aim at solving this problem by marrying image
matching with NeRF. With 2D matches and depth rendered by NeRF, we directly
solve the pose in one step by building 2D-3D correspondences between target and
initial view, thus allowing for real-time prediction. Moreover, to improve the
accuracy of 2D-3D correspondences, we propose a 3D consistent point mining
strategy, which effectively discards unfaithful points reconstruted by NeRF.
Moreover, current NeRF-based methods naively optimizing pixel loss fail at
occluded images. Thus, we further propose a 2D matches based sampling strategy
to preclude the occluded area. Experimental results on representative datasets
prove that our method outperforms state-of-the-art methods, and improves
inference efficiency by 90x, achieving real-time prediction at 6 FPS.";Ronghan Chen<author:sep>Yang Cong<author:sep>Yu Ren;http://arxiv.org/pdf/2404.00891v1;cs.CV;ICRA, 2024. Video https://www.youtube.com/watch?v=70fgUobOFWo;nerf
2404.01300v1;http://arxiv.org/abs/2404.01300v1;2024-04-01;NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields;"Neural fields excel in computer vision and robotics due to their ability to
understand the 3D visual world such as inferring semantics, geometry, and
dynamics. Given the capabilities of neural fields in densely representing a 3D
scene from 2D images, we ask the question: Can we scale their self-supervised
pretraining, specifically using masked autoencoders, to generate effective 3D
representations from posed RGB images. Owing to the astounding success of
extending transformers to novel data modalities, we employ standard 3D Vision
Transformers to suit the unique formulation of NeRFs. We leverage NeRF's
volumetric grid as a dense input to the transformer, contrasting it with other
3D representations such as pointclouds where the information density can be
uneven, and the representation is irregular. Due to the difficulty of applying
masked autoencoders to an implicit representation, such as NeRF, we opt for
extracting an explicit representation that canonicalizes scenes across domains
by employing the camera trajectory for sampling. Our goal is made possible by
masking random patches from NeRF's radiance and density grid and employing a
standard 3D Swin Transformer to reconstruct the masked patches. In doing so,
the model can learn the semantic and spatial structure of complete scenes. We
pretrain this representation at scale on our proposed curated posed-RGB data,
totaling over 1.6 million images. Once pretrained, the encoder is used for
effective 3D transfer learning. Our novel self-supervised pretraining for
NeRFs, NeRF-MAE, scales remarkably well and improves performance on various
challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,
NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF
scene understanding baselines on Front3D and ScanNet datasets with an absolute
performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.";Muhammad Zubair Irshad<author:sep>Sergey Zakahrov<author:sep>Vitor Guizilini<author:sep>Adrien Gaidon<author:sep>Zsolt Kira<author:sep>Rares Ambrus;http://arxiv.org/pdf/2404.01300v1;cs.CV;29 pages, 13 figures. Project Page: https://nerf-mae.github.io/;nerf
2404.00874v1;http://arxiv.org/abs/2404.00874v1;2024-04-01;DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF;"We present DiSR-NeRF, a diffusion-guided framework for view-consistent
super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement
for high-resolution (HR) reference images by leveraging existing powerful 2D
super-resolution models. Nonetheless, independent SR 2D images are often
inconsistent across different views. We thus propose Iterative 3D
Synchronization (I3DS) to mitigate the inconsistency problem via the inherent
multi-view consistency property of NeRF. Specifically, our I3DS alternates
between upscaling low-resolution (LR) rendered images with diffusion models,
and updating the underlying 3D representation with standard NeRF training. We
further introduce Renoised Score Distillation (RSD), a novel score-distillation
objective for 2D image resolution. Our RSD combines features from ancestral
sampling and Score Distillation Sampling (SDS) to generate sharp images that
are also LR-consistent. Qualitative and quantitative results on both synthetic
and real-world datasets demonstrate that our DiSR-NeRF can achieve better
results on NeRF super-resolution compared with existing works. Code and video
results available at the project website.";Jie Long Lee<author:sep>Chen Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2404.00874v1;cs.CV;;nerf
2404.00987v1;http://arxiv.org/abs/2404.00987v1;2024-04-01;FlexiDreamer: Single Image-to-3D Generation with FlexiCubes;"3D content generation from text prompts or single images has made remarkable
progress in quality and speed recently. One of its dominant paradigms involves
generating consistent multi-view images followed by a sparse-view
reconstruction. However, due to the challenge of directly deforming the mesh
representation to approach the target topology, most methodologies learn an
implicit representation (such as NeRF) during the sparse-view reconstruction
and acquire the target mesh by a post-processing extraction. Although the
implicit representation can effectively model rich 3D information, its training
typically entails a long convergence time. In addition, the post-extraction
operation from the implicit field also leads to undesirable visual artifacts.
In this paper, we propose FlexiDreamer, a novel single image-to-3d generation
framework that reconstructs the target mesh in an end-to-end manner. By
leveraging a flexible gradient-based extraction known as FlexiCubes, our method
circumvents the defects brought by the post-processing and facilitates a direct
acquisition of the target mesh. Furthermore, we incorporate a multi-resolution
hash grid encoding scheme that progressively activates the encoding levels into
the implicit field in FlexiCubes to help capture geometric details for per-step
optimization. Notably, FlexiDreamer recovers a dense 3D structure from a
single-view image in approximately 1 minute on a single NVIDIA A100 GPU,
outperforming previous methodologies by a large margin.";Ruowen Zhao<author:sep>Zhengyi Wang<author:sep>Yikai Wang<author:sep>Zihan Zhou<author:sep>Jun Zhu;http://arxiv.org/pdf/2404.00987v1;cs.CV;project page:https://flexidreamer.github.io;nerf
2404.01053v1;http://arxiv.org/abs/2404.01053v1;2024-04-01;HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior;"We present HAHA - a novel approach for animatable human avatar generation
from monocular input videos. The proposed method relies on learning the
trade-off between the use of Gaussian splatting and a textured mesh for
efficient and high fidelity rendering. We demonstrate its efficiency to animate
and render full-body human avatars controlled via the SMPL-X parametric model.
Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh
where it is necessary, like hair and out-of-mesh clothing. This results in a
minimal number of Gaussians being used to represent the full avatar, and
reduced rendering artifacts. This allows us to handle the animation of small
body parts such as fingers that are traditionally disregarded. We demonstrate
the effectiveness of our approach on two open datasets: SnapshotPeople and
X-Humans. Our method demonstrates on par reconstruction quality to the
state-of-the-art on SnapshotPeople, while using less than a third of Gaussians.
HAHA outperforms previous state-of-the-art on novel poses from X-Humans both
quantitatively and qualitatively.";David Svitov<author:sep>Pietro Morerio<author:sep>Lourdes Agapito<author:sep>Alessio Del Bue;http://arxiv.org/pdf/2404.01053v1;cs.CV;;gaussian splatting
2404.00875v2;http://arxiv.org/abs/2404.00875v2;2024-04-01;DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable  Primitive Assembly;"We present a differentiable rendering framework to learn structured 3D
abstractions in the form of primitive assemblies from sparse RGB images
capturing a 3D object. By leveraging differentiable volume rendering, our
method does not require 3D supervision. Architecturally, our network follows
the general pipeline of an image-conditioned neural radiance field (NeRF)
exemplified by pixelNeRF for color prediction. As our core contribution, we
introduce differential primitive assembly (DPA) into NeRF to output a 3D
occupancy field in place of density prediction, where the predicted occupancies
serve as opacity values for volume rendering. Our network, coined DPA-Net,
produces a union of convexes, each as an intersection of convex quadric
primitives, to approximate the target 3D object, subject to an abstraction loss
and a masking loss, both defined in the image space upon volume rendering. With
test-time adaptation and additional sampling and loss designs aimed at
improving the accuracy and compactness of the obtained assemblies, our method
demonstrates superior performance over state-of-the-art alternatives for 3D
primitive abstraction from sparse views.";Fenggen Yu<author:sep>Yiming Qian<author:sep>Xu Zhang<author:sep>Francisca Gil-Ureta<author:sep>Brian Jackson<author:sep>Eric Bennett<author:sep>Hao Zhang;http://arxiv.org/pdf/2404.00875v2;cs.CV;14 pages;nerf
2404.01296v1;http://arxiv.org/abs/2404.01296v1;2024-04-01;MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space;"We introduce a novel framework for 3D human avatar generation and
personalization, leveraging text prompts to enhance user engagement and
customization. Central to our approach are key innovations aimed at overcoming
the challenges in photo-realistic avatar synthesis. Firstly, we utilize a
conditional Neural Radiance Fields (NeRF) model, trained on a large-scale
unannotated multi-view dataset, to create a versatile initial solution space
that accelerates and diversifies avatar generation. Secondly, we develop a
geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models,
to ensure superior view invariance and enable direct optimization of avatar
geometry. These foundational ideas are complemented by our optimization
pipeline built on Variational Score Distillation (VSD), which mitigates texture
loss and over-saturation issues. As supported by our extensive experiments,
these strategies collectively enable the creation of custom avatars with
unparalleled visual quality and better adherence to input text prompts. You can
find more results and videos in our website:
https://syntec-research.github.io/MagicMirror";Armand Comas-MassaguÃ©<author:sep>Di Qiu<author:sep>Menglei Chai<author:sep>Marcel BÃ¼hler<author:sep>Amit Raj<author:sep>Ruiqi Gao<author:sep>Qiangeng Xu<author:sep>Mark Matthews<author:sep>Paulo Gotardo<author:sep>Octavia Camps<author:sep>Sergio Orts-Escolano<author:sep>Thabo Beeler;http://arxiv.org/pdf/2404.01296v1;cs.CV;;nerf
2404.01168v1;http://arxiv.org/abs/2404.01168v1;2024-04-01;Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting;"3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the
realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much
like its predecessor Neural Radiance Fields (NeRF), struggles to accurately
model physical reflections, particularly in mirrors that are ubiquitous in
real-world scenes. This oversight mistakenly perceives reflections as separate
entities that physically exist, resulting in inaccurate reconstructions and
inconsistent reflective properties across varied viewpoints. To address this
pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework
devised to master the intricacies of mirror geometries and reflections, paving
the way for the generation of realistically depicted mirror reflections. By
ingeniously incorporating mirror attributes into the 3DGS and leveraging the
principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to
observe from behind the mirror, enriching the realism of scene renderings.
Extensive assessments, spanning both synthetic and real-world scenes, showcase
our method's ability to render novel views with enhanced fidelity in real-time,
surpassing the state-of-the-art Mirror-NeRF specifically within the challenging
mirror regions. Our code will be made publicly available for reproducible
research.";Jiarui Meng<author:sep>Haijie Li<author:sep>Yanmin Wu<author:sep>Qiankun Gao<author:sep>Shuzhou Yang<author:sep>Jian Zhang<author:sep>Siwei Ma;http://arxiv.org/pdf/2404.01168v1;cs.CV;22 pages, 7 figures;gaussian splatting<tag:sep>nerf
2404.00674v1;http://arxiv.org/abs/2404.00674v1;2024-03-31;Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated  Objects;"We present Knowledge NeRF to synthesize novel views for dynamic
scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering
them from arbitrary perspectives is a challenging problem with applications in
various domains. Previous dynamic NeRF methods learn the deformation of
articulated objects from monocular videos. However, qualities of their
reconstructed scenes are limited.To clearly reconstruct dynamic scenes, we
propose a new framework by considering two frames at a time.We pretrain a NeRF
model for an articulated object.When articulated objects moves, Knowledge NeRF
learns to generate novel views at the new state by incorporating past knowledge
in the pretrained NeRF model with minimal observations in the present state. We
propose a projection module to adapt NeRF for dynamic scenes, learning the
correspondence between pretrained knowledge base and current states.
Experimental results demonstrate the effectiveness of our method in
reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge
NeRF is a new pipeline and promising solution for novel view synthesis in
dynamic articulated objects. The data and implementation are publicly available
at https://github.com/RussRobin/Knowledge_NeRF.";Wenxiao Cai<author:sep>Xinyue LeiÄ±nst<author:sep>Xinyu He<author:sep>Junming Leo Chen<author:sep>Yangang Wang;http://arxiv.org/pdf/2404.00674v1;cs.CV;;nerf
2404.00714v1;http://arxiv.org/abs/2404.00714v1;2024-03-31;Neural Radiance Field-based Visual Rendering: A Comprehensive Review;"In recent years, Neural Radiance Fields (NeRF) has made remarkable progress
in the field of computer vision and graphics, providing strong technical
support for solving key tasks including 3D scene understanding, new perspective
synthesis, human body reconstruction, robotics, and so on, the attention of
academics to this research result is growing. As a revolutionary neural
implicit field representation, NeRF has caused a continuous research boom in
the academic community. Therefore, the purpose of this review is to provide an
in-depth analysis of the research literature on NeRF within the past two years,
to provide a comprehensive academic perspective for budding researchers. In
this paper, the core architecture of NeRF is first elaborated in detail,
followed by a discussion of various improvement strategies for NeRF, and case
studies of NeRF in diverse application scenarios, demonstrating its practical
utility in different domains. In terms of datasets and evaluation metrics, This
paper details the key resources needed for NeRF model training. Finally, this
paper provides a prospective discussion on the future development trends and
potential challenges of NeRF, aiming to provide research inspiration for
researchers in the field and to promote the further development of related
technologies.";Mingyuan Yao<author:sep>Yukang Huo<author:sep>Yang Ran<author:sep>Qingbin Tian<author:sep>Ruifeng Wang<author:sep>Haihua Wang;http://arxiv.org/pdf/2404.00714v1;cs.CV;35 pages, 22 figures, 14 tables, 18 formulas;nerf
2404.00769v1;http://arxiv.org/abs/2404.00769v1;2024-03-31;An Active Perception Game for Robust Autonomous Exploration;"We formulate active perception for an autonomous agent that explores an
unknown environment as a two-player zero-sum game: the agent aims to maximize
information gained from the environment while the environment aims to minimize
the information gained by the agent. In each episode, the environment reveals a
set of actions with their potentially erroneous information gain. In order to
select the best action, the robot needs to recover the true information gain
from the erroneous one. The robot does so by minimizing the discrepancy between
its estimate of information gain and the true information gain it observes
after taking the action. We propose an online convex optimization algorithm
that achieves sub-linear expected regret $O(T^{3/4})$ for estimating the
information gain. We also provide a bound on the regret of active perception
performed by any (near-)optimal prediction and trajectory selection algorithms.
We evaluate this approach using semantic neural radiance fields (NeRFs) in
simulated realistic 3D environments to show that the robot can discover up to
12% more objects using the improved estimate of the information gain. On the
M3ED dataset, the proposed algorithm reduced the error of information gain
prediction in occupancy map by over 67%. In real-world experiments using
occupancy maps on a Jackal ground robot, we show that this approach can
calculate complicated trajectories that efficiently explore all occluded
regions.";Siming He<author:sep>Yuezhan Tao<author:sep>Igor Spasojevic<author:sep>Vijay Kumar<author:sep>Pratik Chaudhari;http://arxiv.org/pdf/2404.00769v1;cs.RO;;nerf
2404.00345v1;http://arxiv.org/abs/2404.00345v1;2024-03-30;MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview  and Text;"The generation of 3D scenes from user-specified conditions offers a promising
avenue for alleviating the production burden in 3D applications. Previous
studies required significant effort to realize the desired scene, owing to
limited control conditions. We propose a method for controlling and generating
3D scenes under multimodal conditions using partial images, layout information
represented in the top view, and text prompts. Combining these conditions to
generate a 3D scene involves the following significant difficulties: (1) the
creation of large datasets, (2) reflection on the interaction of multimodal
conditions, and (3) domain dependence of the layout conditions. We decompose
the process of 3D scene generation into 2D image generation from the given
conditions and 3D scene generation from 2D images. 2D image generation is
achieved by fine-tuning a pretrained text-to-image model with a small
artificial dataset of partial images and layouts, and 3D scene generation is
achieved by layout-conditioned depth estimation and neural radiance fields
(NeRF), thereby avoiding the creation of large datasets. The use of a common
representation of spatial information using 360-degree images allows for the
consideration of multimodal condition interactions and reduces the domain
dependence of the layout control. The experimental results qualitatively and
quantitatively demonstrated that the proposed method can generate 3D scenes in
diverse domains, from indoor to outdoor, according to multimodal conditions.";Takayuki Hara<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2404.00345v1;cs.CV;Project Page: https://hara012.github.io/MaGRITTe-project;nerf
2404.00409v1;http://arxiv.org/abs/2404.00409v1;2024-03-30;3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting;"In this paper, we present an implicit surface reconstruction method with 3D
Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D
reconstruction with intricate details while inheriting the high efficiency and
rendering quality of 3DGS. The key insight is incorporating an implicit signed
distance field (SDF) within 3D Gaussians to enable them to be aligned and
jointly optimized. First, we introduce a differentiable SDF-to-opacity
transformation function that converts SDF values into corresponding Gaussians'
opacities. This function connects the SDF and 3D Gaussians, allowing for
unified optimization and enforcing surface constraints on the 3D Gaussians.
During learning, optimizing the 3D Gaussians provides supervisory signals for
SDF learning, enabling the reconstruction of intricate details. However, this
only provides sparse supervisory signals to the SDF at locations occupied by
Gaussians, which is insufficient for learning a continuous SDF. Then, to
address this limitation, we incorporate volumetric rendering and align the
rendered geometric attributes (depth, normal) with those derived from 3D
Gaussians. This consistency regularization introduces supervisory signals to
locations not covered by discrete 3D Gaussians, effectively eliminating
redundant surfaces outside the Gaussian sampling range. Our extensive
experimental results demonstrate that our 3DGSR method enables high-quality 3D
surface reconstruction while preserving the efficiency and rendering quality of
3DGS. Besides, our method competes favorably with leading surface
reconstruction techniques while offering a more efficient learning process and
much better rendering qualities. The code will be available at
https://github.com/CVMI-Lab/3DGSR.";Xiaoyang Lyu<author:sep>Yang-Tian Sun<author:sep>Yi-Hua Huang<author:sep>Xiuzhe Wu<author:sep>Ziyi Yang<author:sep>Yilun Chen<author:sep>Jiangmiao Pang<author:sep>Xiaojuan Qi;http://arxiv.org/pdf/2404.00409v1;cs.CV;;gaussian splatting
2403.20153v1;http://arxiv.org/abs/2403.20153v1;2024-03-29;Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D  Generative Prior;"Recent methods for audio-driven talking head synthesis often optimize neural
radiance fields (NeRF) on a monocular talking portrait video, leveraging its
capability to render high-fidelity and 3D-consistent novel-view frames.
However, they often struggle to reconstruct complete face geometry due to the
absence of comprehensive 3D information in the input monocular videos. In this
paper, we introduce a novel audio-driven talking head synthesis framework,
called Talk3D, that can faithfully reconstruct its plausible facial geometries
by effectively adopting the pre-trained 3D-aware generative prior. Given the
personalized 3D generative model, we present a novel audio-guided attention
U-Net architecture that predicts the dynamic face variations in the NeRF space
driven by audio. Furthermore, our model is further modulated by audio-unrelated
conditioning tokens which effectively disentangle variations unrelated to audio
features. Compared to existing methods, our method excels in generating
realistic facial geometries even under extreme head poses. We also conduct
extensive experiments showing our approach surpasses state-of-the-art
benchmarks in terms of both quantitative and qualitative evaluations.";Jaehoon Ko<author:sep>Kyusun Cho<author:sep>Joungbin Lee<author:sep>Heeji Yoon<author:sep>Sangmin Lee<author:sep>Sangjun Ahn<author:sep>Seungryong Kim;http://arxiv.org/pdf/2403.20153v1;cs.CV;Project page: https://ku-cvlab.github.io/Talk3D/;nerf
2403.20159v1;http://arxiv.org/abs/2403.20159v1;2024-03-29;HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation  in Urban Scenes;"Online dense mapping of urban scenes forms a fundamental cornerstone for
scene understanding and navigation of autonomous vehicles. Recent advancements
in mapping methods are mainly based on NeRF, whose rendering speed is too slow
to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering
speed hundreds of times faster than NeRF, holds greater potential in online
dense mapping. However, integrating 3DGS into a street-view dense mapping
framework still faces two challenges, including incomplete reconstruction due
to the absence of geometric information beyond the LiDAR coverage area and
extensive computation for reconstruction in large urban scenes. To this end, we
propose HGS-Mapping, an online dense mapping framework in unbounded large-scale
scenes. To attain complete construction, our framework introduces Hybrid
Gaussian Representation, which models different parts of the entire scene using
Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian
initialization mechanism and an adaptive update method to achieve high-fidelity
and rapid reconstruction. To the best of our knowledge, we are the first to
integrate Gaussian representation into online dense mapping of urban scenes.
Our approach achieves SOTA reconstruction accuracy while only employing 66%
number of Gaussians, leading to 20% faster reconstruction speed.";Ke Wu<author:sep>Kaizhao Zhang<author:sep>Zhiwei Zhang<author:sep>Shanshuai Yuan<author:sep>Muer Tie<author:sep>Julong Wei<author:sep>Zijun Xu<author:sep>Jieru Zhao<author:sep>Zhongxue Gan<author:sep>Wenchao Ding;http://arxiv.org/pdf/2403.20159v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.19920v1;http://arxiv.org/abs/2403.19920v1;2024-03-29;MI-NeRF: Learning a Single Face NeRF from Multiple Identities;"In this work, we introduce a method that learns a single dynamic neural
radiance field (NeRF) from monocular talking face videos of multiple
identities. NeRFs have shown remarkable results in modeling the 4D dynamics and
appearance of human faces. However, they require per-identity optimization.
Although recent approaches have proposed techniques to reduce the training and
rendering time, increasing the number of identities can be expensive. We
introduce MI-NeRF (multi-identity NeRF), a single unified network that models
complex non-rigid facial motion for multiple identities, using only monocular
videos of arbitrary length. The core premise in our method is to learn the
non-linear interactions between identity and non-identity specific information
with a multiplicative module. By training on multiple videos simultaneously,
MI-NeRF not only reduces the total training time compared to standard
single-identity NeRFs, but also demonstrates robustness in synthesizing novel
expressions for any input identity. We present results for both facial
expression transfer and talking face video synthesis. Our method can be further
personalized for a target identity given only a short video.";Aggelina Chatziagapi<author:sep>Grigorios G. Chrysos<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2403.19920v1;cs.CV;Project page: https://aggelinacha.github.io/MI-NeRF/;nerf
2403.20079v1;http://arxiv.org/abs/2403.20079v1;2024-03-29;SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior;"Novel View Synthesis (NVS) for street scenes play a critical role in the
autonomous driving simulation. The current mainstream technique to achieve it
is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS). Although thrilling progress has been made, when handling
street scenes, current methods struggle to maintain rendering quality at the
viewpoint that deviates significantly from the training viewpoints. This issue
stems from the sparse training views captured by a fixed camera on a moving
vehicle. To tackle this problem, we propose a novel approach that enhances the
capacity of 3DGS by leveraging prior from a Diffusion Model along with
complementary multi-modal data. Specifically, we first fine-tune a Diffusion
Model by adding images from adjacent frames as condition, meanwhile exploiting
depth data from LiDAR point clouds to supply additional spatial information.
Then we apply the Diffusion Model to regularize the 3DGS at unseen views during
training. Experimental results validate the effectiveness of our method
compared with current state-of-the-art models, and demonstrate its advance in
rendering images from broader views.";Zhongrui Yu<author:sep>Haoran Wang<author:sep>Jinze Yang<author:sep>Hanzhang Wang<author:sep>Zeke Xie<author:sep>Yunfeng Cai<author:sep>Jiale Cao<author:sep>Zhong Ji<author:sep>Mingming Sun;http://arxiv.org/pdf/2403.20079v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.20013v1;http://arxiv.org/abs/2403.20013v1;2024-03-29;DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal;"When capturing images through the glass during rainy or snowy weather
conditions, the resulting images often contain waterdrops adhered on the glass
surface, and these waterdrops significantly degrade the image quality and
performance of many computer vision algorithms. To tackle these limitations, we
propose a method to reconstruct the clear 3D scene implicitly from multi-view
images degraded by waterdrops. Our method exploits an attention network to
predict the location of waterdrops and then train a Neural Radiance Fields to
recover the 3D scene implicitly. By leveraging the strong scene representation
capabilities of NeRF, our method can render high-quality novel-view images with
waterdrops removed. Extensive experimental results on both synthetic and real
datasets show that our method is able to generate clear 3D scenes and
outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal
methods.";Yunhao Li<author:sep>Jing Wu<author:sep>Lingzhe Zhao<author:sep>Peidong Liu;http://arxiv.org/pdf/2403.20013v1;cs.CV;;nerf
2403.20032v1;http://arxiv.org/abs/2403.20032v1;2024-03-29;HO-Gaussian: Hybrid Optimization of 3D Gaussian Splatting for Urban  Scenes;"The rapid growth of 3D Gaussian Splatting (3DGS) has revolutionized neural
rendering, enabling real-time production of high-quality renderings. However,
the previous 3DGS-based methods have limitations in urban scenes due to
reliance on initial Structure-from-Motion(SfM) points and difficulties in
rendering distant, sky and low-texture areas. To overcome these challenges, we
propose a hybrid optimization method named HO-Gaussian, which combines a
grid-based volume with the 3DGS pipeline. HO-Gaussian eliminates the dependency
on SfM point initialization, allowing for rendering of urban scenes, and
incorporates the Point Densitification to enhance rendering quality in
problematic regions during training. Furthermore, we introduce Gaussian
Direction Encoding as an alternative for spherical harmonics in the rendering
pipeline, which enables view-dependent color representation. To account for
multi-camera systems, we introduce neural warping to enhance object consistency
across different cameras. Experimental results on widely used autonomous
driving datasets demonstrate that HO-Gaussian achieves photo-realistic
rendering in real-time on multi-camera urban datasets.";Zhuopeng Li<author:sep>Yilin Zhang<author:sep>Chenming Wu<author:sep>Jianke Zhu<author:sep>Liangjun Zhang;http://arxiv.org/pdf/2403.20032v1;cs.CV;;gaussian splatting
2403.19985v1;http://arxiv.org/abs/2403.19985v1;2024-03-29;Stable Surface Regularization for Fast Few-Shot NeRF;"This paper proposes an algorithm for synthesizing novel views under few-shot
setup. The main concept is to develop a stable surface regularization technique
called Annealing Signed Distance Function (ASDF), which anneals the surface in
a coarse-to-fine manner to accelerate convergence speed. We observe that the
Eikonal loss - which is a widely known geometric regularization - requires
dense training signal to shape different level-sets of SDF, leading to
low-fidelity results under few-shot training. In contrast, the proposed surface
regularization successfully reconstructs scenes and produce high-fidelity
geometry with stable training. Our method is further accelerated by utilizing
grid representation and monocular geometric priors. Finally, the proposed
approach is up to 45 times faster than existing few-shot novel view synthesis
methods, and it produces comparable results in the ScanNet dataset and
NeRF-Real dataset.";Byeongin Joung<author:sep>Byeong-Uk Lee<author:sep>Jaesung Choe<author:sep>Ukcheol Shin<author:sep>Minjun Kang<author:sep>Taeyeop Lee<author:sep>In So Kweon<author:sep>Kuk-Jin Yoon;http://arxiv.org/pdf/2403.19985v1;cs.CV;3DV 2024;nerf
2403.20275v1;http://arxiv.org/abs/2403.20275v1;2024-03-29;Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for  Reconstructing Challenging Surfaces;"Touch and vision go hand in hand, mutually enhancing our ability to
understand the world. From a research perspective, the problem of mixing touch
and vision is underexplored and presents interesting challenges. To this end,
we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data
(local depth maps) with multi-view vision data to achieve surface
reconstruction and novel view synthesis. Our method optimises 3D Gaussian
primitives to accurately model the object's geometry at points of contact. By
creating a framework that decreases the transmittance at touch locations, we
achieve a refined surface reconstruction, ensuring a uniformly smooth depth
map. Touch is particularly useful when considering non-Lambertian objects (e.g.
shiny or reflective surfaces) since contemporary methods tend to fail to
reconstruct with fidelity specular highlights. By combining vision and tactile
sensing, we achieve more accurate geometry reconstructions with fewer images
than prior methods. We conduct evaluation on objects with glossy and reflective
surfaces and demonstrate the effectiveness of our approach, offering
significant improvements in reconstruction quality.";Mauro Comi<author:sep>Alessio Tonioni<author:sep>Max Yang<author:sep>Jonathan Tremblay<author:sep>Valts Blukis<author:sep>Yijiong Lin<author:sep>Nathan F. Lepora<author:sep>Laurence Aitchison;http://arxiv.org/pdf/2403.20275v1;cs.CV;17 pages;gaussian splatting
2403.20018v1;http://arxiv.org/abs/2403.20018v1;2024-03-29;SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image;"In this paper, we explore the potential of Snapshot Compressive Imaging (SCI)
technique for recovering the underlying 3D scene representation from a single
temporal compressed image. SCI is a cost-effective method that enables the
recording of high-dimensional data, such as hyperspectral or temporal
information, into a single image using low-cost 2D imaging sensors. To achieve
this, a series of specially designed 2D masks are usually employed, which not
only reduces storage requirements but also offers potential privacy protection.
Inspired by this, to take one step further, our approach builds upon the
powerful 3D scene representation capabilities of neural radiance fields (NeRF).
Specifically, we formulate the physical imaging process of SCI as part of the
training of NeRF, allowing us to exploit its impressive performance in
capturing complex scene structures. To assess the effectiveness of our method,
we conduct extensive evaluations using both synthetic data and real data
captured by our SCI system. Extensive experimental results demonstrate that our
proposed approach surpasses the state-of-the-art methods in terms of image
reconstruction and novel view image synthesis. Moreover, our method also
exhibits the ability to restore high frame-rate multi-view consistent images by
leveraging SCI and the rendering capabilities of NeRF. The code is available at
https://github.com/WU-CVGL/SCINeRF.";Yunhao Li<author:sep>Xiaodong Wang<author:sep>Ping Wang<author:sep>Xin Yuan<author:sep>Peidong Liu;http://arxiv.org/pdf/2403.20018v1;eess.IV;;nerf
2403.20034v1;http://arxiv.org/abs/2403.20034v1;2024-03-29;NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking  With Depth Completion and Denoising;"In recent years, there have been significant advancements in 3D
reconstruction and dense RGB-D SLAM systems. One notable development is the
application of Neural Radiance Fields (NeRF) in these systems, which utilizes
implicit neural representation to encode 3D scenes. This extension of NeRF to
SLAM has shown promising results. However, the depth images obtained from
consumer-grade RGB-D sensors are often sparse and noisy, which poses
significant challenges for 3D reconstruction and affects the accuracy of the
representation of the scene geometry. Moreover, the original hierarchical
feature grid with occupancy value is inaccurate for scene geometry
representation. Furthermore, the existing methods select random pixels for
camera tracking, which leads to inaccurate localization and is not robust in
real-world indoor environments. To this end, we present NeSLAM, an advanced
framework that achieves accurate and dense depth estimation, robust camera
tracking, and realistic synthesis of novel views. First, a depth completion and
denoising network is designed to provide dense geometry prior and guide the
neural implicit representation optimization. Second, the occupancy scene
representation is replaced with Signed Distance Field (SDF) hierarchical scene
representation for high-quality reconstruction and view synthesis. Furthermore,
we also propose a NeRF-based self-supervised feature tracking algorithm for
robust real-time tracking. Experiments on various indoor datasets demonstrate
the effectiveness and accuracy of the system in reconstruction, tracking
quality, and novel view synthesis.";Tianchen Deng<author:sep>Yanbo Wang<author:sep>Hongle Xie<author:sep>Hesheng Wang<author:sep>Jingchuan Wang<author:sep>Danwei Wang<author:sep>Weidong Chen;http://arxiv.org/pdf/2403.20034v1;cs.CV;;nerf
2403.20309v1;http://arxiv.org/abs/2403.20309v1;2024-03-29;InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40  Seconds;"While novel view synthesis (NVS) has made substantial progress in 3D computer
vision, it typically requires an initial estimation of camera intrinsics and
extrinsics from dense viewpoints. This pre-processing is usually conducted via
a Structure-from-Motion (SfM) pipeline, a procedure that can be slow and
unreliable, particularly in sparse-view scenarios with insufficient matched
features for accurate reconstruction. In this work, we integrate the strengths
of point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with
end-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved
issues in NVS under unconstrained settings, which encompasses pose-free and
sparse view challenges. Our framework, InstantSplat, unifies dense stereo
priors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview &
pose-free images in less than 1 minute. Specifically, InstantSplat comprises a
Coarse Geometric Initialization (CGI) module that swiftly establishes a
preliminary scene structure and camera parameters across all training views,
utilizing globally-aligned 3D point maps derived from a pre-trained dense
stereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO)
module, which jointly optimizes the 3D Gaussian attributes and the initialized
poses with pose regularization. Experiments conducted on the large-scale
outdoor Tanks & Temples datasets demonstrate that InstantSplat significantly
improves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error
(ATE) by 80%. These establish InstantSplat as a viable solution for scenarios
involving posefree and sparse-view conditions. Project page:
instantsplat.github.io.";Zhiwen Fan<author:sep>Wenyan Cong<author:sep>Kairun Wen<author:sep>Kevin Wang<author:sep>Jian Zhang<author:sep>Xinghao Ding<author:sep>Danfei Xu<author:sep>Boris Ivanovic<author:sep>Marco Pavone<author:sep>Georgios Pavlakos<author:sep>Zhangyang Wang<author:sep>Yue Wang;http://arxiv.org/pdf/2403.20309v1;cs.CV;;gaussian splatting
2403.19243v1;http://arxiv.org/abs/2403.19243v1;2024-03-28;Sine Activated Low-Rank Matrices for Parameter Efficient Learning;"Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model accuracy. Our
method proves to be an adaptable enhancement for existing low-rank models, as
evidenced by its successful application in Vision Transformers (ViT), Large
Language Models (LLMs), Neural Radiance Fields (NeRF), and 3D shape modeling.
This demonstrates the wide-ranging potential and efficiency of our proposed
technique.";Yiping Ji<author:sep>Hemanth Saratchandran<author:sep>Cameron Gordon<author:sep>Zeyu Zhang<author:sep>Simon Lucey;http://arxiv.org/pdf/2403.19243v1;cs.LG;The first two authors contributed equally;nerf
2403.19780v1;http://arxiv.org/abs/2403.19780v1;2024-03-28;Mitigating Motion Blur in Neural Radiance Fields with Events and Frames;"Neural Radiance Fields (NeRFs) have shown great potential in novel view
synthesis. However, they struggle to render sharp images when the data used for
training is affected by motion blur. On the other hand, event cameras excel in
dynamic scenes as they measure brightness changes with microsecond resolution
and are thus only marginally affected by blur. Recent methods attempt to
enhance NeRF reconstructions under camera motion by fusing frames and events.
However, they face challenges in recovering accurate color content or constrain
the NeRF to a set of predefined camera poses, harming reconstruction quality in
challenging conditions. This paper proposes a novel formulation addressing
these issues by leveraging both model- and learning-based modules. We
explicitly model the blur formation process, exploiting the event double
integral as an additional model-based prior. Additionally, we model the
event-pixel response using an end-to-end learnable response function, allowing
our method to adapt to non-idealities in the real event-camera sensor. We show,
on synthetic and real data, that the proposed approach outperforms existing
deblur NeRFs that use only frames as well as those that combine frames and
events by +6.13dB and +2.48dB, respectively.";Marco Cannici<author:sep>Davide Scaramuzza;http://arxiv.org/pdf/2403.19780v1;cs.CV;"IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2024";nerf
2403.19495v1;http://arxiv.org/abs/2403.19495v1;2024-03-28;CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians;"The field of 3D reconstruction from images has rapidly evolved in the past
few years, first with the introduction of Neural Radiance Field (NeRF) and more
recently with 3D Gaussian Splatting (3DGS). The latter provides a significant
edge over NeRF in terms of the training and inference speed, as well as the
reconstruction quality. Although 3DGS works well for dense input images, the
unstructured point-cloud like representation quickly overfits to the more
challenging setup of extremely sparse input images (e.g., 3 images), creating a
representation that appears as a jumble of needles from novel views. To address
this issue, we propose regularized optimization and depth-based initialization.
Our key idea is to introduce a structured Gaussian representation that can be
controlled in 2D image space. We then constraint the Gaussians, in particular
their position, and prevent them from moving independently during optimization.
Specifically, we introduce single and multiview constraints through an implicit
convolutional decoder and a total variation loss, respectively. With the
coherency introduced to the Gaussians, we further constrain the optimization
through a flow-based loss function. To support our regularized optimization, we
propose an approach to initialize the Gaussians using monocular depth estimates
at each input view. We demonstrate significant improvements compared to the
state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.";Avinash Paliwal<author:sep>Wei Ye<author:sep>Jinhui Xiong<author:sep>Dmytro Kotovenko<author:sep>Rakesh Ranjan<author:sep>Vikas Chandra<author:sep>Nima Khademi Kalantari;http://arxiv.org/pdf/2403.19495v1;cs.CV;Project page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS;gaussian splatting<tag:sep>nerf
2403.19607v1;http://arxiv.org/abs/2403.19607v1;2024-03-28;SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent  Objects;"Acquiring accurate depth information of transparent objects using
off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and
Robotics. Depth estimation/completion methods are typically employed and
trained on datasets with quality depth labels acquired from either simulation,
additional sensors or specialized data collection setups and known 3d models.
However, acquiring reliable depth information for datasets at scale is not
straightforward, limiting training scalability and generalization. Neural
Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide
success in novel view synthesis and shape recovery. However, heuristics and
controlled environments (lights, backgrounds, etc) are often required to
accurately capture specular surfaces. In this paper, we propose using Visual
Foundation Models (VFMs) for segmentation in a zero-shot, label-free way to
guide the NeRF reconstruction process for these objects via the simultaneous
reconstruction of semantic fields and extensions to increase robustness. Our
proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant
performance on depth completion datasets for transparent objects and robotic
grasping.";Avinash Ummadisingu<author:sep>Jongkeum Choi<author:sep>Koki Yamane<author:sep>Shimpei Masuda<author:sep>Naoki Fukaya<author:sep>Kuniyuki Takahashi;http://arxiv.org/pdf/2403.19607v1;cs.RO;"8 pages. An accompanying video is available at
  https://www.youtube.com/watch?v=S4NCoUq4bmE";nerf
2403.19655v1;http://arxiv.org/abs/2403.19655v1;2024-03-28;GaussianCube: Structuring Gaussian Splatting using Optimal Transport for  3D Generative Modeling;"3D Gaussian Splatting (GS) have achieved considerable improvement over Neural
Radiance Fields in terms of 3D fitting fidelity and rendering speed. However,
this unstructured representation with scattered Gaussians poses a significant
challenge for generative modeling. To address the problem, we introduce
GaussianCube, a structured GS representation that is both powerful and
efficient for generative modeling. We achieve this by first proposing a
modified densification-constrained GS fitting algorithm which can yield
high-quality fitting results using a fixed number of free Gaussians, and then
re-arranging the Gaussians into a predefined voxel grid via Optimal Transport.
The structured grid representation allows us to use standard 3D U-Net as our
backbone in diffusion generative modeling without elaborate designs. Extensive
experiments conducted on ShapeNet and OmniObject3D show that our model achieves
state-of-the-art generation results both qualitatively and quantitatively,
underscoring the potential of GaussianCube as a powerful and versatile 3D
representation.";Bowen Zhang<author:sep>Yiji Cheng<author:sep>Jiaolong Yang<author:sep>Chunyu Wang<author:sep>Feng Zhao<author:sep>Yansong Tang<author:sep>Dong Chen<author:sep>Baining Guo;http://arxiv.org/pdf/2403.19655v1;cs.CV;Project Page: https://gaussiancube.github.io/;gaussian splatting
2403.19319v1;http://arxiv.org/abs/2403.19319v1;2024-03-28;Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field  Representation and Generation;"We present Mesh2NeRF, an approach to derive ground-truth radiance fields from
textured meshes for 3D generation tasks. Many 3D generative approaches
represent 3D scenes as radiance fields for training. Their ground-truth
radiance fields are usually fitted from multi-view renderings from a
large-scale synthetic 3D dataset, which often results in artifacts due to
occlusions or under-fitting issues. In Mesh2NeRF, we propose an analytic
solution to directly obtain ground-truth radiance fields from 3D meshes,
characterizing the density field with an occupancy function featuring a defined
surface thickness, and determining view-dependent color through a reflection
function considering both the mesh and environment lighting. Mesh2NeRF extracts
accurate radiance fields which provides direct supervision for training
generative NeRFs and single scene representation. We validate the effectiveness
of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in
PSNR for view synthesis in single scene representation on the ABO dataset, a
0.69 PSNR enhancement in the single-view conditional generation of ShapeNet
Cars, and notably improved mesh extraction from NeRF in the unconditional
generation of Objaverse Mugs.";Yujin Chen<author:sep>Yinyu Nie<author:sep>Benjamin Ummenhofer<author:sep>Reiner Birkl<author:sep>Michael Paulitsch<author:sep>Matthias MÃ¼ller<author:sep>Matthias NieÃner;http://arxiv.org/pdf/2403.19319v1;cs.CV;"Project page: https://terencecyj.github.io/projects/Mesh2NeRF/ Video:
  https://youtu.be/oufv1N3f7iY";nerf
2403.19586v1;http://arxiv.org/abs/2403.19586v1;2024-03-28;TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D  DSA Rendering;"Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical
imaging technique that provides a series of 2D images captured at different
stages and angles during the process of contrast agent filling blood vessels.
It plays a significant role in the diagnosis of cerebrovascular diseases.
Improving the rendering quality and speed under sparse sampling is important
for observing the status and location of lesions. The current methods exhibit
inadequate rendering quality in sparse views and suffer from slow rendering
speed. To overcome these limitations, we propose TOGS, a Gaussian splatting
method with opacity offset over time, which can effectively improve the
rendering quality and speed of 4D DSA. We introduce an opacity offset table for
each Gaussian to model the temporal variations in the radiance of the contrast
agent. By interpolating the opacity offset table, the opacity variation of the
Gaussian at different time points can be determined. This enables us to render
the 2D DSA image at that specific moment. Additionally, we introduced a Smooth
loss term in the loss function to mitigate overfitting issues that may arise in
the model when dealing with sparse view scenarios. During the training phase,
we randomly prune Gaussians, thereby reducing the storage overhead of the
model. The experimental results demonstrate that compared to previous methods,
this model achieves state-of-the-art reconstruction quality under the same
number of training views. Additionally, it enables real-time rendering while
maintaining low storage overhead. The code will be publicly available.";Shuai Zhang<author:sep>Huangxuan Zhao<author:sep>Zhenghong Zhou<author:sep>Guanjun Wu<author:sep>Chuansheng Zheng<author:sep>Xinggang Wang<author:sep>Wenyu Liu;http://arxiv.org/pdf/2403.19586v1;cs.CV;;gaussian splatting
2403.19615v1;http://arxiv.org/abs/2403.19615v1;2024-03-28;SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing;"In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian
Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs
modifying the training procedure of Gaussian splatting, our method functions at
test-time and is training-free. Specifically, SA-GS can be applied to any
pretrained Gaussian splatting field as a plugin to significantly improve the
field's anti-alising performance. The core technique is to apply 2D
scale-adaptive filters to each Gaussian during test time. As pointed out by
Mip-Splatting, observing Gaussians at different frequencies leads to mismatches
between the Gaussian scales during training and testing. Mip-Splatting resolves
this issue using 3D smoothing and 2D Mip filters, which are unfortunately not
aware of testing frequency. In this work, we show that a 2D scale-adaptive
filter that is informed of testing frequency can effectively match the Gaussian
scale, thus making the Gaussian primitive distribution remain consistent across
different testing frequencies. When scale inconsistency is eliminated, sampling
rates smaller than the scene frequency result in conventional jaggedness, and
we propose to integrate the projected 2D Gaussian within each pixel during
testing. This integration is actually a limiting case of super-sampling, which
significantly improves anti-aliasing performance over vanilla Gaussian
Splatting. Through extensive experiments using various settings and both
bounded and unbounded scenes, we show SA-GS performs comparably with or better
than Mip-Splatting. Note that super-sampling and integration are only effective
when our scale-adaptive filtering is activated. Our codes, data and models are
available at https://github.com/zsy1987/SA-GS.";Xiaowei Song<author:sep>Jv Zheng<author:sep>Shiran Yuan<author:sep>Huan-ang Gao<author:sep>Jingwei Zhao<author:sep>Xiang He<author:sep>Weihao Gu<author:sep>Hao Zhao;http://arxiv.org/pdf/2403.19615v1;cs.CV;"Project page: https://kevinsong729.github.io/project-pages/SA-GS/
  Code: https://github.com/zsy1987/SA-GS";gaussian splatting
2403.19632v1;http://arxiv.org/abs/2403.19632v1;2024-03-28;GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond;"We present GauStudio, a novel modular framework for modeling 3D Gaussian
Splatting (3DGS) to provide standardized, plug-and-play components for users to
easily customize and implement a 3DGS pipeline. Supported by our framework, we
propose a hybrid Gaussian representation with foreground and skyball background
models. Experiments demonstrate this representation reduces artifacts in
unbounded outdoor scenes and improves novel view synthesis. Finally, we propose
Gaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuse
approach for high-fidelity mesh reconstruction from 3DGS inputs without
fine-tuning. Overall, our GauStudio framework, hybrid representation, and GauS
approach enhance 3DGS modeling and rendering capabilities, enabling
higher-quality novel view synthesis and surface reconstruction.";Chongjie Ye<author:sep>Yinyu Nie<author:sep>Jiahao Chang<author:sep>Yuantao Chen<author:sep>Yihao Zhi<author:sep>Xiaoguang Han;http://arxiv.org/pdf/2403.19632v1;cs.CV;Code: https://github.com/GAP-LAB-CUHK-SZ/gaustudio;gaussian splatting
2403.18784v2;http://arxiv.org/abs/2403.18784v2;2024-03-27;SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable  Surface;"We present SplatFace, a novel Gaussian splatting framework designed for 3D
human face reconstruction without reliance on accurate pre-determined geometry.
Our method is designed to simultaneously deliver both high-quality novel view
rendering and accurate 3D mesh reconstructions. We incorporate a generic 3D
Morphable Model (3DMM) to provide a surface geometric structure, making it
possible to reconstruct faces with a limited set of input images. We introduce
a joint optimization strategy that refines both the Gaussians and the morphable
surface through a synergistic non-rigid alignment process. A novel distance
metric, splat-to-surface, is proposed to improve alignment by considering both
the Gaussian position and covariance. The surface information is also utilized
to incorporate a world-space densification process, resulting in superior
reconstruction quality. Our experimental analysis demonstrates that the
proposed method is competitive with both other Gaussian splatting techniques in
novel view synthesis and other 3D reconstruction methods in producing 3D face
meshes with high geometric precision.";Jiahao Luo<author:sep>Jing Liu<author:sep>James Davis;http://arxiv.org/pdf/2403.18784v2;cs.CV;;gaussian splatting
2403.18795v2;http://arxiv.org/abs/2403.18795v2;2024-03-27;Gamba: Marry Gaussian Splatting with Mamba for single view 3D  reconstruction;"We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.";Qiuhong Shen<author:sep>Xuanyu Yi<author:sep>Zike Wu<author:sep>Pan Zhou<author:sep>Hanwang Zhang<author:sep>Shuicheng Yan<author:sep>Xinchao Wang;http://arxiv.org/pdf/2403.18795v2;cs.CV;;gaussian splatting<tag:sep>nerf
2403.18711v1;http://arxiv.org/abs/2403.18711v1;2024-03-27;SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable  Transient-Free 3D reconstruction from Satellite Imagery;"Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.";Camille Billouard<author:sep>Dawa Derksen<author:sep>Emmanuelle Sarrazin<author:sep>Bruno Vallet;http://arxiv.org/pdf/2403.18711v1;cs.CV;"5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP";nerf
2403.18476v1;http://arxiv.org/abs/2403.18476v1;2024-03-27;Modeling uncertainty for Gaussian Splatting;"We present Stochastic Gaussian Splatting (SGS): the first framework for
uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the
novel-view synthesis field by achieving impressive reconstruction quality at a
fraction of the computational cost of Neural Radiance Fields (NeRF). However,
contrary to the latter, it still lacks the ability to provide information about
the confidence associated with their outputs. To address this limitation, in
this paper, we introduce a Variational Inference-based approach that seamlessly
integrates uncertainty prediction into the common rendering pipeline of GS.
Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new
term in the loss function, enabling optimization of uncertainty estimation
alongside image reconstruction. Experimental results on the LLFF dataset
demonstrate that our method outperforms existing approaches in terms of both
image rendering quality and uncertainty estimation accuracy. Overall, our
framework equips practitioners with valuable insights into the reliability of
synthesized views, facilitating safer decision-making in real-world
applications.";Luca Savant<author:sep>Diego Valsesia<author:sep>Enrico Magli;http://arxiv.org/pdf/2403.18476v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.17822v1;http://arxiv.org/abs/2403.17822v1;2024-03-26;DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing;"3D Gaussian splatting, a novel differentiable rendering technique, has
achieved state-of-the-art novel view synthesis results with high rendering
speeds and relatively low training times. However, its performance on scenes
commonly seen in indoor datasets is poor due to the lack of geometric
constraints during optimization. We extend 3D Gaussian splatting with depth and
normal cues to tackle challenging indoor datasets and showcase techniques for
efficient mesh extraction, an important downstream application. Specifically,
we regularize the optimization procedure with depth information, enforce local
smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians
supervised by normal cues to achieve better alignment with the true scene
geometry. We improve depth estimation and novel view synthesis results over
baselines and show how this simple yet effective regularization technique can
be used to directly extract meshes from the Gaussian representation yielding
more physically accurate reconstructions on indoor scenes. Our code will be
released in https://github.com/maturk/dn-splatter.";Matias Turkulainen<author:sep>Xuqian Ren<author:sep>Iaroslav Melekhov<author:sep>Otto Seiskari<author:sep>Esa Rahtu<author:sep>Juho Kannala;http://arxiv.org/pdf/2403.17822v1;cs.CV;;gaussian splatting
2403.17898v1;http://arxiv.org/abs/2403.17898v1;2024-03-26;Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D  Gaussians;"The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering
fidelity and efficiency compared to NeRF-based neural scene representations.
While demonstrating the potential for real-time rendering, 3D-GS encounters
rendering bottlenecks in large scenes with complex details due to an excessive
number of Gaussian primitives located within the viewing frustum. This
limitation is particularly noticeable in zoom-out views and can lead to
inconsistent rendering speeds in scenes with varying details. Moreover, it
often struggles to capture the corresponding level of details at different
scales with its heuristic density control operation. Inspired by the
Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an
LOD-structured 3D Gaussian approach supporting level-of-detail decomposition
for scene representation that contributes to the final rendering results. Our
model dynamically selects the appropriate level from the set of
multi-resolution anchor points, ensuring consistent rendering performance with
adaptive LOD adjustments while maintaining high-fidelity rendering results.";Kerui Ren<author:sep>Lihan Jiang<author:sep>Tao Lu<author:sep>Mulin Yu<author:sep>Linning Xu<author:sep>Zhangkai Ni<author:sep>Bo Dai;http://arxiv.org/pdf/2403.17898v1;cs.CV;Project page: https://city-super.github.io/octree-gs/;gaussian splatting<tag:sep>nerf
2403.17888v1;http://arxiv.org/abs/2403.17888v1;2024-03-26;2D Gaussian Splatting for Geometrically Accurate Radiance Fields;"3D Gaussian Splatting (3DGS) has recently revolutionized radiance field
reconstruction, achieving high quality novel view synthesis and fast rendering
speed without baking. However, 3DGS fails to accurately represent surfaces due
to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian
Splatting (2DGS), a novel approach to model and reconstruct geometrically
accurate radiance fields from multi-view images. Our key idea is to collapse
the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D
Gaussians, 2D Gaussians provide view-consistent geometry while modeling
surfaces intrinsically. To accurately recover thin surfaces and achieve stable
optimization, we introduce a perspective-accurate 2D splatting process
utilizing ray-splat intersection and rasterization. Additionally, we
incorporate depth distortion and normal consistency terms to further enhance
the quality of the reconstructions. We demonstrate that our differentiable
renderer allows for noise-free and detailed geometry reconstruction while
maintaining competitive appearance quality, fast training speed, and real-time
rendering. Our code will be made publicly available.";Binbin Huang<author:sep>Zehao Yu<author:sep>Anpei Chen<author:sep>Andreas Geiger<author:sep>Shenghua Gao;http://arxiv.org/pdf/2403.17888v1;cs.CV;12 pages, 12 figures;gaussian splatting
2403.17537v1;http://arxiv.org/abs/2403.17537v1;2024-03-26;NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using  Heuristics-Guided Segmentation;"Neural Radiance Field (NeRF) has been widely recognized for its excellence in
novel view synthesis and 3D scene reconstruction. However, their effectiveness
is inherently tied to the assumption of static scenes, rendering them
susceptible to undesirable artifacts when confronted with transient distractors
such as moving objects or shadows. In this work, we propose a novel paradigm,
namely ""Heuristics-Guided Segmentation"" (HuGS), which significantly enhances
the separation of static scenes from transient distractors by harmoniously
combining the strengths of hand-crafted heuristics and state-of-the-art
segmentation models, thus significantly transcending the limitations of
previous solutions. Furthermore, we delve into the meticulous design of
heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based
heuristics and color residual heuristics, catering to a diverse range of
texture profiles. Extensive experiments demonstrate the superiority and
robustness of our method in mitigating transient distractors for NeRFs trained
in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.";Jiahao Chen<author:sep>Yipeng Qin<author:sep>Lingjie Liu<author:sep>Jiangbo Lu<author:sep>Guanbin Li;http://arxiv.org/pdf/2403.17537v1;cs.CV;To appear in CVPR2024;nerf
2403.16885v1;http://arxiv.org/abs/2403.16885v1;2024-03-25;CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance  Fields from Sparse Inputs;"Neural Radiance Fields (NeRF) have shown impressive capabilities for
photorealistic novel view synthesis when trained on dense inputs. However, when
trained on sparse inputs, NeRF typically encounters issues of incorrect density
or color predictions, mainly due to insufficient coverage of the scene causing
partial and sparse supervision, thus leading to significant performance
degradation. While existing works mainly consider ray-level consistency to
construct 2D learning regularization based on rendered color, depth, or
semantics on image planes, in this paper we propose a novel approach that
models 3D spatial field consistency to improve NeRF's performance with sparse
inputs. Specifically, we first adopt a voxel-based ray sampling strategy to
ensure that the sampled rays intersect with a certain voxel in 3D space. We
then randomly sample additional points within the voxel and apply a Transformer
to infer the properties of other points on each ray, which are then
incorporated into the volume rendering. By backpropagating through the
rendering loss, we enhance the consistency among neighboring points.
Additionally, we propose to use a contrastive loss on the encoder output of the
Transformer to further improve consistency within each voxel. Experiments
demonstrate that our method yields significant improvement over different
radiance fields in the sparse inputs setting, and achieves comparable
performance with current works.";Yingji Zhong<author:sep>Lanqing Hong<author:sep>Zhenguo Li<author:sep>Dan Xu;http://arxiv.org/pdf/2403.16885v1;cs.CV;"The paper is accepted by CVPR 2024. Project page is available at
  https://zhongyingji.github.io/CVT-xRF";nerf
2403.17001v1;http://arxiv.org/abs/2403.17001v1;2024-03-25;VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation;"Recent innovations on text-to-3D generation have featured Score Distillation
Sampling (SDS), which enables the zero-shot learning of implicit 3D models
(NeRF) by directly distilling prior knowledge from 2D diffusion models.
However, current SDS-based models still struggle with intricate text prompts
and commonly result in distorted 3D models with unrealistic textures or
cross-view inconsistency issues. In this work, we introduce a novel Visual
Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the
visual appearance knowledge in 2D visual prompt to boost text-to-3D generation.
Instead of solely supervising SDS with text prompt, VP3D first capitalizes on
2D diffusion model to generate a high-quality image from input text, which
subsequently acts as visual prompt to strengthen SDS optimization with explicit
visual appearance. Meanwhile, we couple the SDS optimization with additional
differentiable reward function that encourages rendering images of 3D models to
better visually align with 2D visual prompt and semantically match with text
prompt. Through extensive experiments, we show that the 2D Visual Prompt in our
VP3D significantly eases the learning of visual appearance of 3D models and
thus leads to higher visual fidelity with more detailed textures. It is also
appealing in view that when replacing the self-generating visual prompt with a
given reference image, VP3D is able to trigger a new task of stylized
text-to-3D generation. Our project page is available at
https://vp3d-cvpr24.github.io.";Yang Chen<author:sep>Yingwei Pan<author:sep>Haibo Yang<author:sep>Ting Yao<author:sep>Tao Mei;http://arxiv.org/pdf/2403.17001v1;cs.CV;"CVPR 2024; Project page: https://vp3d-cvpr24.github.io";nerf
2403.16410v1;http://arxiv.org/abs/2403.16410v1;2024-03-25;Spike-NeRF: Neural Radiance Field Based On Spike Camera;"As a neuromorphic sensor with high temporal resolution, spike cameras offer
notable advantages over traditional cameras in high-speed vision applications
such as high-speed optical estimation, depth estimation, and object tracking.
Inspired by the success of the spike camera, we proposed Spike-NeRF, the first
Neural Radiance Field derived from spike data, to achieve 3D reconstruction and
novel viewpoint synthesis of high-speed scenes. Instead of the multi-view
images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike
streams captured by a moving spike camera in a very short time. To reconstruct
a correct and stable 3D scene from high-frequency but unstable spike data, we
devised spike masks along with a distinctive loss function. We evaluate our
method qualitatively and numerically on several challenging synthetic scenes
generated by blender with the spike camera simulator. Our results demonstrate
that Spike-NeRF produces more visually appealing results than the existing
methods and the baseline we proposed in high-speed scenes. Our code and data
will be released soon.";Yijia Guo<author:sep>Yuanxi Bai<author:sep>Liwen Hu<author:sep>Mianzhi Liu<author:sep>Ziyi Guo<author:sep>Lei Ma<author:sep>Tiejun Huang;http://arxiv.org/pdf/2403.16410v1;cs.CV;This paper is accepted by ICME2024;nerf
2403.17237v1;http://arxiv.org/abs/2403.17237v1;2024-03-25;DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric  Diffusion;"We present DreamPolisher, a novel Gaussian Splatting based method with
geometric guidance, tailored to learn cross-view consistency and intricate
detail from textual descriptions. While recent progress on text-to-3D
generation methods have been promising, prevailing methods often fail to ensure
view-consistency and textural richness. This problem becomes particularly
noticeable for methods that work with text input alone. To address this, we
propose a two-stage Gaussian Splatting based approach that enforces geometric
consistency among views. Initially, a coarse 3D generation undergoes refinement
via geometric optimization. Subsequently, we use a ControlNet driven refiner
coupled with the geometric consistency term to improve both texture fidelity
and overall consistency of the generated 3D asset. Empirical evaluations across
diverse textual prompts spanning various object categories demonstrate the
efficacy of DreamPolisher in generating consistent and realistic 3D objects,
aligning closely with the semantics of the textual instructions.";Yuanze Lin<author:sep>Ronald Clark<author:sep>Philip Torr;http://arxiv.org/pdf/2403.17237v1;cs.CV;Project webpage: https://yuanze-lin.me/DreamPolisher_page/;gaussian splatting
2403.16964v1;http://arxiv.org/abs/2403.16964v1;2024-03-25;GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction;"Presenting a 3D scene from multiview images remains a core and long-standing
challenge in computer vision and computer graphics. Two main requirements lie
in rendering and reconstruction. Notably, SOTA rendering quality is usually
achieved with neural volumetric rendering techniques, which rely on aggregated
point/primitive-wise color and neglect the underlying scene geometry. Learning
of neural implicit surfaces is sparked from the success of neural rendering.
Current works either constrain the distribution of density fields or the shape
of primitives, resulting in degraded rendering quality and flaws on the learned
scene surfaces. The efficacy of such methods is limited by the inherent
constraints of the chosen neural representation, which struggles to capture
fine surface details, especially for larger, more intricate scenes. To address
these issues, we introduce GSDF, a novel dual-branch architecture that combines
the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS)
representation with neural Signed Distance Fields (SDF). The core idea is to
leverage and enhance the strengths of each branch while alleviating their
limitation through mutual guidance and joint supervision. We show on diverse
scenes that our design unlocks the potential for more accurate and detailed
surface reconstructions, and at the meantime benefits 3DGS rendering with
structures that are more aligned with the underlying geometry.";Mulin Yu<author:sep>Tao Lu<author:sep>Linning Xu<author:sep>Lihan Jiang<author:sep>Yuanbo Xiangli<author:sep>Bo Dai;http://arxiv.org/pdf/2403.16964v1;cs.CV;Project page: https://city-super.github.io/GSDF;gaussian splatting
2403.16092v1;http://arxiv.org/abs/2403.16092v1;2024-03-24;Are NeRFs ready for autonomous driving? Towards closing the  real-to-simulation gap;"Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing
autonomous driving (AD) research, offering scalable closed-loop simulation and
data augmentation capabilities. However, to trust the results achieved in
simulation, one needs to ensure that AD systems perceive real and rendered data
in the same way. Although the performance of rendering methods is increasing,
many scenarios will remain inherently challenging to reconstruct faithfully. To
this end, we propose a novel perspective for addressing the real-to-simulated
data gap. Rather than solely focusing on improving rendering fidelity, we
explore simple yet effective methods to enhance perception model robustness to
NeRF artifacts without compromising performance on real data. Moreover, we
conduct the first large-scale investigation into the real-to-simulated data gap
in an AD setting using a state-of-the-art neural rendering technique.
Specifically, we evaluate object detectors and an online mapping model on real
and simulated data, and study the effects of different pre-training strategies.
Our results show notable improvements in model robustness to simulated data,
even improving real-world performance in some cases. Last, we delve into the
correlation between the real-to-simulated gap and image reconstruction metrics,
identifying FID and LPIPS as strong indicators.";Carl LindstrÃ¶m<author:sep>Georg Hess<author:sep>Adam Lilja<author:sep>Maryam Fatemi<author:sep>Lars Hammarstrand<author:sep>Christoffer Petersson<author:sep>Lennart Svensson;http://arxiv.org/pdf/2403.16092v1;cs.CV;;nerf
2403.16080v2;http://arxiv.org/abs/2403.16080v2;2024-03-24;PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic  Human Modeling;"High-quality human reconstruction and photo-realistic rendering of a dynamic
scene is a long-standing problem in computer vision and graphics. Despite
considerable efforts invested in developing various capture systems and
reconstruction algorithms, recent advancements still struggle with loose or
oversized clothing and overly complex poses. In part, this is due to the
challenges of acquiring high-quality human datasets. To facilitate the
development of these fields, in this paper, we present PKU-DyMVHumans, a
versatile human-centric dataset for high-fidelity reconstruction and rendering
of dynamic human scenarios from dense multi-view videos. It comprises 8.2
million frames captured by more than 56 synchronized cameras across diverse
scenarios. These sequences comprise 32 human subjects across 45 different
scenarios, each with a high-detailed appearance and realistic human motion.
Inspired by recent advancements in neural radiance field (NeRF)-based scene
representations, we carefully set up an off-the-shelf framework that is easy to
provide those state-of-the-art NeRF-based implementations and benchmark on
PKU-DyMVHumans dataset. It is paving the way for various applications like
fine-grained foreground/background decomposition, high-quality human
reconstruction and photo-realistic novel view synthesis of a dynamic scene.
Extensive studies are performed on the benchmark, demonstrating new
observations and challenges that emerge from using such high-fidelity dynamic
data. The dataset is available at: https://pku-dymvhumans.github.io.";Xiaoyun Zheng<author:sep>Liwei Liao<author:sep>Xufeng Li<author:sep>Jianbo Jiao<author:sep>Rongjie Wang<author:sep>Feng Gao<author:sep>Shiqi Wang<author:sep>Ronggang Wang;http://arxiv.org/pdf/2403.16080v2;cs.CV;;nerf
2403.16043v1;http://arxiv.org/abs/2403.16043v1;2024-03-24;Semantic Is Enough: Only Semantic Information For NeRF Reconstruction;"Recent research that combines implicit 3D representation with semantic
information, like Semantic-NeRF, has proven that NeRF model could perform
excellently in rendering 3D structures with semantic labels. This research aims
to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing
solely on semantic output and removing the RGB output component. We reformulate
the model and its training procedure to leverage only the cross-entropy loss
between the model semantic output and the ground truth semantic images,
removing the colour data traditionally used in the original Semantic-NeRF
approach. We then conduct a series of identical experiments using the original
and the modified Semantic-NeRF model. Our primary objective is to obverse the
impact of this modification on the model performance by Semantic-NeRF, focusing
on tasks such as scene understanding, object detection, and segmentation. The
results offer valuable insights into the new way of rendering the scenes and
provide an avenue for further research and development in semantic-focused 3D
scene understanding.";Ruibo Wang<author:sep>Song Zhang<author:sep>Ping Huang<author:sep>Donghai Zhang<author:sep>Wei Yan;http://arxiv.org/pdf/2403.16043v1;cs.CV;;nerf
2403.16095v1;http://arxiv.org/abs/2403.16095v1;2024-03-24;CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D  Gaussian Field;"Recently neural radiance fields (NeRF) have been widely exploited as 3D
representations for dense simultaneous localization and mapping (SLAM). Despite
their notable successes in surface modeling and novel view synthesis, existing
NeRF-based methods are hindered by their computationally intensive and
time-consuming volume rendering pipeline. This paper presents an efficient
dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D
Gaussian field with high consistency and geometric stability. Through an
in-depth analysis of Gaussian Splatting, we propose several techniques to
construct a consistent and stable 3D Gaussian field suitable for tracking and
mapping. Additionally, a novel depth uncertainty model is proposed to ensure
the selection of valuable Gaussian primitives during optimization, thereby
improving tracking efficiency and accuracy. Experiments on various datasets
demonstrate that CG-SLAM achieves superior tracking and mapping performance
with a notable tracking speed of up to 15 Hz. We will make our source code
publicly available. Project page: https://zju3dv.github.io/cg-slam.";Jiarui Hu<author:sep>Xianhao Chen<author:sep>Boyin Feng<author:sep>Guanglin Li<author:sep>Liangjing Yang<author:sep>Hujun Bao<author:sep>Guofeng Zhang<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2403.16095v1;cs.CV;Project Page: https://zju3dv.github.io/cg-slam;gaussian splatting<tag:sep>nerf
2403.16292v1;http://arxiv.org/abs/2403.16292v1;2024-03-24;latentSplat: Autoencoding Variational Gaussians for Fast Generalizable  3D Reconstruction;"We present latentSplat, a method to predict semantic Gaussians in a 3D latent
space that can be splatted and decoded by a light-weight generative 2D
architecture. Existing methods for generalizable 3D reconstruction either do
not enable fast inference of high resolution novel views due to slow volume
rendering, or are limited to interpolation of close input views, even in
simpler settings with a single central object, where 360-degree generalization
is possible. In this work, we combine a regression-based approach with a
generative model, moving towards both of these capabilities within the same
method, trained purely on readily available real video data. The core of our
method are variational 3D Gaussians, a representation that efficiently encodes
varying uncertainty within a latent space consisting of 3D feature Gaussians.
From these Gaussians, specific instances can be sampled and rendered via
efficient Gaussian splatting and a fast, generative decoder network. We show
that latentSplat outperforms previous works in reconstruction quality and
generalization, while being fast and scalable to high-resolution data.";Christopher Wewer<author:sep>Kevin Raj<author:sep>Eddy Ilg<author:sep>Bernt Schiele<author:sep>Jan Eric Lenssen;http://arxiv.org/pdf/2403.16292v1;cs.CV;Project website: https://geometric-rl.mpi-inf.mpg.de/latentsplat/;gaussian splatting
2403.16224v1;http://arxiv.org/abs/2403.16224v1;2024-03-24;Inverse Rendering of Glossy Objects via the Neural Plenoptic Function  and Radiance Fields;"Inverse rendering aims at recovering both geometry and materials of objects.
It provides a more compatible reconstruction for conventional rendering
engines, compared with the neural radiance fields (NeRFs). On the other hand,
existing NeRF-based inverse rendering methods cannot handle glossy objects with
local light interactions well, as they typically oversimplify the illumination
as a 2D environmental map, which assumes infinite lights only. Observing the
superiority of NeRFs in recovering radiance fields, we propose a novel 5D
Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more
accurate lighting-object interactions can be formulated via the rendering
equation. We also design a material-aware cone sampling strategy to efficiently
integrate lights inside the BRDF lobes with the help of pre-filtered radiance
fields. Our method has two stages: the geometry of the target object and the
pre-filtered environmental radiance fields are reconstructed in the first
stage, and materials of the target object are estimated in the second stage
with the proposed NeP and material-aware cone sampling strategy. Extensive
experiments on the proposed real-world and synthetic datasets demonstrate that
our method can reconstruct high-fidelity geometry/materials of challenging
glossy objects with complex lighting interactions from nearby objects. Project
webpage: https://whyy.site/paper/nep";Haoyuan Wang<author:sep>Wenbo Hu<author:sep>Lei Zhu<author:sep>Rynson W. H. Lau;http://arxiv.org/pdf/2403.16224v1;cs.CV;CVPR 2024 paper. Project webpage https://whyy.site/paper/nep;nerf
2403.15981v2;http://arxiv.org/abs/2403.15981v2;2024-03-24;Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance  Fields;"Accurate collection of plant phenotyping is critical to optimising
sustainable farming practices in precision agriculture. Traditional phenotyping
in controlled laboratory environments, while valuable, falls short in
understanding plant growth under real-world conditions. Emerging sensor and
digital technologies offer a promising approach for direct phenotyping of
plants in farm environments. This study investigates a learning-based
phenotyping method using the Neural Radiance Field to achieve accurate in-situ
phenotyping of pepper plants in greenhouse environments. To quantitatively
evaluate the performance of this method, traditional point cloud registration
on 3D scanning data is implemented for comparison. Experimental result shows
that NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the
3D scanning methods. The mean distance error between the scanner-based method
and the NeRF-based method is 0.865mm. This study shows that the learning-based
NeRF method achieves similar accuracy to 3D scanning-based methods but with
improved scalability and robustness.";Junhong Zhao<author:sep>Wei Ying<author:sep>Yaoqiang Pan<author:sep>Zhenfeng Yi<author:sep>Chao Chen<author:sep>Kewei Hu<author:sep>Hanwen Kang;http://arxiv.org/pdf/2403.15981v2;cs.CV;;nerf
2403.16141v1;http://arxiv.org/abs/2403.16141v1;2024-03-24;Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes;"Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic
scenes often involve explicit modeling of scene dynamics. However, this
approach faces challenges in modeling scene dynamics in urban environments,
where moving objects of various categories and scales are present. In such
settings, it becomes crucial to effectively eliminate moving objects to
accurately reconstruct static backgrounds. Our research introduces an
innovative method, termed here as Entity-NeRF, which combines the strengths of
knowledge-based and statistical strategies. This approach utilizes entity-wise
statistics, leveraging entity segmentation and stationary entity classification
through thing/stuff segmentation. To assess our methodology, we created an
urban scene dataset masked with moving objects. Our comprehensive experiments
demonstrate that Entity-NeRF notably outperforms existing techniques in
removing moving objects and reconstructing static urban backgrounds, both
quantitatively and qualitatively.";Takashi Otonari<author:sep>Satoshi Ikehata<author:sep>Kiyoharu Aizawa;http://arxiv.org/pdf/2403.16141v1;cs.CV;"Accepted by IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2024), Project website:
  https://otonari726.github.io/entitynerf/";nerf
2403.15791v1;http://arxiv.org/abs/2403.15791v1;2024-03-23;DriveEnv-NeRF: Exploration of A NeRF-Based Autonomous Driving  Environment for Real-World Performance Validation;"In this study, we introduce the DriveEnv-NeRF framework, which leverages
Neural Radiance Fields (NeRF) to enable the validation and faithful forecasting
of the efficacy of autonomous driving agents in a targeted real-world scene.
Standard simulator-based rendering often fails to accurately reflect real-world
performance due to the sim-to-real gap, which represents the disparity between
virtual simulations and real-world conditions. To mitigate this gap, we propose
a workflow for building a high-fidelity simulation environment of the targeted
real-world scene using NeRF. This approach is capable of rendering realistic
images from novel viewpoints and constructing 3D meshes for emulating
collisions. The validation of these capabilities through the comparison of
success rates in both simulated and real environments demonstrates the benefits
of using DriveEnv-NeRF as a real-world performance indicator. Furthermore, the
DriveEnv-NeRF framework can serve as a training environment for autonomous
driving agents under various lighting conditions. This approach enhances the
robustness of the agents and reduces performance degradation when deployed to
the target real scene, compared to agents fully trained using the standard
simulator rendering pipeline.";Mu-Yi Shen<author:sep>Chia-Chi Hsu<author:sep>Hao-Yu Hou<author:sep>Yu-Chen Huang<author:sep>Wei-Fang Sun<author:sep>Chia-Che Chang<author:sep>Yu-Lun Liu<author:sep>Chun-Yi Lee;http://arxiv.org/pdf/2403.15791v1;cs.RO;Project page: https://github.com/muyishen2040/DriveEnvNeRF;nerf
2403.15704v1;http://arxiv.org/abs/2403.15704v1;2024-03-23;Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image  Collections;"Novel view synthesis from unconstrained in-the-wild images remains a
meaningful but challenging task. The photometric variation and transient
occluders in those unconstrained images make it difficult to reconstruct the
original scene accurately. Previous approaches tackle the problem by
introducing a global appearance feature in Neural Radiance Fields (NeRF).
However, in the real world, the unique appearance of each tiny point in a scene
is determined by its independent intrinsic material attributes and the varying
environmental impacts it receives. Inspired by this fact, we propose Gaussian
in the wild (GS-W), a method that uses 3D Gaussian points to reconstruct the
scene and introduces separated intrinsic and dynamic appearance feature for
each point, capturing the unchanged scene appearance along with dynamic
variation like illumination and weather. Additionally, an adaptive sampling
strategy is presented to allow each Gaussian point to focus on the local and
detailed information more effectively. We also reduce the impact of transient
occluders using a 2D visibility map. More experiments have demonstrated better
reconstruction quality and details of GS-W compared to previous methods, with a
$1000\times$ increase in rendering speed.";Dongbin Zhang<author:sep>Chuming Wang<author:sep>Weitao Wang<author:sep>Peihao Li<author:sep>Minghan Qin<author:sep>Haoqian Wang;http://arxiv.org/pdf/2403.15704v1;cs.CV;14 pages, 5 figures;gaussian splatting<tag:sep>nerf
2403.15705v1;http://arxiv.org/abs/2403.15705v1;2024-03-23;UPNeRF: A Unified Framework for Monocular 3D Object Reconstruction and  Pose Estimation;"Monocular 3D reconstruction for categorical objects heavily relies on
accurately perceiving each object's pose. While gradient-based optimization
within a NeRF framework updates initially given poses, this paper highlights
that such a scheme fails when the initial pose even moderately deviates from
the true pose. Consequently, existing methods often depend on a third-party 3D
object to provide an initial object pose, leading to increased complexity and
generalization issues. To address these challenges, we present UPNeRF, a
Unified framework integrating Pose estimation and NeRF-based reconstruction,
bringing us closer to real-time monocular 3D object reconstruction. UPNeRF
decouples the object's dimension estimation and pose refinement to resolve the
scale-depth ambiguity, and introduces an effective projected-box representation
that generalizes well cross different domains. While using a dedicated pose
estimator that smoothly integrates into an object-centric NeRF, UPNeRF is free
from external 3D detectors. UPNeRF achieves state-of-the-art results in both
reconstruction and pose estimation tasks on the nuScenes dataset. Furthermore,
UPNeRF exhibits exceptional Cross-dataset generalization on the KITTI and Waymo
datasets, surpassing prior methods with up to 50% reduction in rotation and
translation error.";Yuliang Guo<author:sep>Abhinav Kumar<author:sep>Cheng Zhao<author:sep>Ruoyu Wang<author:sep>Xinyu Huang<author:sep>Liu Ren;http://arxiv.org/pdf/2403.15705v1;cs.CV;;nerf
2403.15624v1;http://arxiv.org/abs/2403.15624v1;2024-03-22;Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian  Splatting;"Open-vocabulary 3D scene understanding presents a significant challenge in
computer vision, withwide-ranging applications in embodied agents and augmented
reality systems. Previous approaches haveadopted Neural Radiance Fields (NeRFs)
to analyze 3D scenes. In this paper, we introduce SemanticGaussians, a novel
open-vocabulary scene understanding approach based on 3D Gaussian Splatting.
Our keyidea is distilling pre-trained 2D semantics into 3D Gaussians. We design
a versatile projection approachthat maps various 2Dsemantic features from
pre-trained image encoders into a novel semantic component of 3D Gaussians,
withoutthe additional training required by NeRFs. We further build a 3D
semantic network that directly predictsthe semantic component from raw 3D
Gaussians for fast inference. We explore several applications ofSemantic
Gaussians: semantic segmentation on ScanNet-20, where our approach attains a
4.2% mIoU and 4.0%mAcc improvement over prior open-vocabulary scene
understanding counterparts; object part segmentation,sceneediting, and
spatial-temporal segmentation with better qualitative results over 2D and 3D
baselines,highlighting its versatility and effectiveness on supporting diverse
downstream tasks.";Jun Guo<author:sep>Xiaojian Ma<author:sep>Yue Fan<author:sep>Huaping Liu<author:sep>Qing Li;http://arxiv.org/pdf/2403.15624v1;cs.CV;Project page: see https://semantic-gaussians.github.io;gaussian splatting<tag:sep>nerf
2403.14939v1;http://arxiv.org/abs/2403.14939v1;2024-03-22;STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians;"Recent progress in pre-trained diffusion models and 3D generation have
spurred interest in 4D content creation. However, achieving high-fidelity 4D
generation with spatial-temporal consistency remains a challenge. In this work,
we propose STAG4D, a novel framework that combines pre-trained diffusion models
with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing
inspiration from 3D generation techniques, we utilize a multi-view diffusion
model to initialize multi-view images anchoring on the input video frames,
where the video can be either real-world captured or generated by a video
diffusion model. To ensure the temporal consistency of the multi-view sequence
initialization, we introduce a simple yet effective fusion strategy to leverage
the first frame as a temporal anchor in the self-attention computation. With
the almost consistent multi-view sequences, we then apply the score
distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian
spatting is specially crafted for the generation task, where an adaptive
densification strategy is proposed to mitigate the unstable Gaussian gradient
for robust optimization. Notably, the proposed pipeline does not require any
pre-training or fine-tuning of diffusion networks, offering a more accessible
and practical solution for the 4D generation task. Extensive experiments
demonstrate that our method outperforms prior 4D generation works in rendering
quality, spatial-temporal consistency, and generation robustness, setting a new
state-of-the-art for 4D generation from diverse inputs, including text, image,
and video.";Yifei Zeng<author:sep>Yanqin Jiang<author:sep>Siyu Zhu<author:sep>Yuanxun Lu<author:sep>Youtian Lin<author:sep>Hao Zhu<author:sep>Weiming Hu<author:sep>Xun Cao<author:sep>Yao Yao;http://arxiv.org/pdf/2403.14939v1;cs.CV;;gaussian splatting
2403.15272v1;http://arxiv.org/abs/2403.15272v1;2024-03-22;WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization;"Despite the advancements in deep learning for camera relocalization tasks,
obtaining ground truth pose labels required for the training process remains a
costly endeavor. While current weakly supervised methods excel in lightweight
label generation, their performance notably declines in scenarios with sparse
views. In response to this challenge, we introduce WSCLoc, a system capable of
being customized to various deep learning-based relocalization models to
enhance their performance under weakly-supervised and sparse view conditions.
This is realized with two stages. In the initial stage, WSCLoc employs a
multilayer perceptron-based structure called WFT-NeRF to co-optimize image
reconstruction quality and initial pose information. To ensure a stable
learning process, we incorporate temporal information as input. Furthermore,
instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to
explicitly enforce a scale constraint. In the second stage, we co-optimize the
pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by
Time-Encoding based Random View Synthesis and supervised by inter-frame
geometric constraints that consider pose, depth, and RGB information. We
validate our approaches on two publicly available datasets, one outdoor and one
indoor. Our experimental results demonstrate that our weakly-supervised
relocalization solutions achieve superior pose estimation accuracy in
sparse-view scenarios, comparable to state-of-the-art camera relocalization
methods. We will make our code publicly available.";Jialu Wang<author:sep>Kaichen Zhou<author:sep>Andrew Markham<author:sep>Niki Trigoni;http://arxiv.org/pdf/2403.15272v1;cs.CV;;nerf
2403.15530v1;http://arxiv.org/abs/2403.15530v1;2024-03-22;Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian  Splatting;"3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis
results while advancing real-time rendering performance. However, it relies
heavily on the quality of the initial point cloud, resulting in blurring and
needle-like artifacts in areas with insufficient initializing points. This is
mainly attributed to the point cloud growth condition in 3DGS that only
considers the average gradient magnitude of points from observable views,
thereby failing to grow for large Gaussians that are observable for many
viewpoints while many of them are only covered in the boundaries. To this end,
we propose a novel method, named Pixel-GS, to take into account the number of
pixels covered by the Gaussian in each view during the computation of the
growth condition. We regard the covered pixel numbers as the weights to
dynamically average the gradients from different views, such that the growth of
large Gaussians can be prompted. As a result, points within the areas with
insufficient initializing points can be grown more effectively, leading to a
more accurate and detailed reconstruction. In addition, we propose a simple yet
effective strategy to scale the gradient field according to the distance to the
camera, to suppress the growth of floaters near the camera. Extensive
experiments both qualitatively and quantitatively demonstrate that our method
achieves state-of-the-art rendering quality while maintaining real-time
rendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.";Zheng Zhang<author:sep>Wenbo Hu<author:sep>Yixing Lao<author:sep>Tong He<author:sep>Hengshuang Zhao;http://arxiv.org/pdf/2403.15530v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.15124v1;http://arxiv.org/abs/2403.15124v1;2024-03-22;EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic  Surgeries using Gaussian Splatting;"Precise camera tracking, high-fidelity 3D tissue reconstruction, and
real-time online visualization are critical for intrabody medical imaging
devices such as endoscopes and capsule robots. However, existing SLAM
(Simultaneous Localization and Mapping) methods often struggle to achieve both
complete high-quality surgical field reconstruction and efficient computation,
restricting their intraoperative applications among endoscopic surgeries. In
this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic
surgeries, which integrates streamlined Gaussian representation and
differentiable rasterization to facilitate over 100 fps rendering speed during
online camera tracking and tissue reconstructing. Extensive experiments show
that EndoGSLAM achieves a better trade-off between intraoperative availability
and reconstruction quality than traditional or neural SLAM approaches, showing
tremendous potential for endoscopic surgeries. The project page is at
https://EndoGSLAM.loping151.com";Kailing Wang<author:sep>Chen Yang<author:sep>Yuehao Wang<author:sep>Sikuang Li<author:sep>Yan Wang<author:sep>Qi Dou<author:sep>Xiaokang Yang<author:sep>Wei Shen;http://arxiv.org/pdf/2403.15124v1;cs.CV;;gaussian splatting
2403.14627v1;http://arxiv.org/abs/2403.14627v1;2024-03-21;MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images;"We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model
learned from sparse multi-view images. To accurately localize the Gaussian
centers, we propose to build a cost volume representation via plane sweeping in
the 3D space, where the cross-view feature similarities stored in the cost
volume can provide valuable geometry cues to the estimation of depth. We learn
the Gaussian primitives' opacities, covariances, and spherical harmonics
coefficients jointly with the Gaussian centers while only relying on
photometric supervision. We demonstrate the importance of the cost volume
representation in learning feed-forward Gaussian Splatting models via extensive
experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks,
our model achieves state-of-the-art performance with the fastest feed-forward
inference speed (22 fps). Compared to the latest state-of-the-art method
pixelSplat, our model uses $10\times $ fewer parameters and infers more than
$2\times$ faster while providing higher appearance and geometry quality as well
as better cross-dataset generalization.";Yuedong Chen<author:sep>Haofei Xu<author:sep>Chuanxia Zheng<author:sep>Bohan Zhuang<author:sep>Marc Pollefeys<author:sep>Andreas Geiger<author:sep>Tat-Jen Cham<author:sep>Jianfei Cai;http://arxiv.org/pdf/2403.14627v1;cs.CV;"Project page: https://donydchen.github.io/mvsplat Code:
  https://github.com/donydchen/mvsplat";gaussian splatting
2403.14166v1;http://arxiv.org/abs/2403.14166v1;2024-03-21;Mini-Splatting: Representing Scenes with a Constrained Number of  Gaussians;"In this study, we explore the challenge of efficiently representing scenes
with a constrained number of Gaussians. Our analysis shifts from traditional
graphics and 2D computer vision to the perspective of point clouds,
highlighting the inefficient spatial distribution of Gaussian representation as
a key limitation in model performance. To address this, we introduce strategies
for densification including blur split and depth reinitialization, and
simplification through Gaussian binarization and sampling. These techniques
reorganize the spatial positions of the Gaussians, resulting in significant
improvements across various datasets and benchmarks in terms of rendering
quality, resource consumption, and storage compression. Our proposed
Mini-Splatting method integrates seamlessly with the original rasterization
pipeline, providing a strong baseline for future research in
Gaussian-Splatting-based works.";Guangchi Fang<author:sep>Bing Wang;http://arxiv.org/pdf/2403.14166v1;cs.CV;;
2403.14376v1;http://arxiv.org/abs/2403.14376v1;2024-03-21;InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space  Complexity;"The conventional mesh-based Level of Detail (LoD) technique, exemplified by
applications such as Google Earth and many game engines, exhibits the
capability to holistically represent a large scene even the Earth, and achieves
rendering with a space complexity of O(log n). This constrained data
requirement not only enhances rendering efficiency but also facilitates dynamic
data fetching, thereby enabling a seamless 3D navigation experience for users.
In this work, we extend this proven LoD technique to Neural Radiance Fields
(NeRF) by introducing an octree structure to represent the scenes in different
scales. This innovative approach provides a mathematically simple and elegant
representation with a rendering space complexity of O(log n), aligned with the
efficiency of mesh-based LoD techniques. We also present a novel training
strategy that maintains a complexity of O(n). This strategy allows for parallel
training with minimal overhead, ensuring the scalability and efficiency of our
proposed method. Our contribution is not only in extending the capabilities of
existing techniques but also in establishing a foundation for scalable and
efficient large-scale scene representation using NeRF and octree structures.";Jiabin Liang<author:sep>Lanqing Zhang<author:sep>Zhuoran Zhao<author:sep>Xiangyu Xu;http://arxiv.org/pdf/2403.14376v1;cs.CV;;nerf
2403.14244v1;http://arxiv.org/abs/2403.14244v1;2024-03-21;Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering;"The 3D Gaussian splatting method has drawn a lot of attention, thanks to its
high performance in training and high quality of the rendered image. However,
it uses anisotropic Gaussian kernels to represent the scene. Although such
anisotropic kernels have advantages in representing the geometry, they lead to
difficulties in terms of computation, such as splitting or merging two kernels.
In this paper, we propose to use isotropic Gaussian kernels to avoid such
difficulties in the computation, leading to a higher performance method. The
experiments confirm that the proposed method is about {\bf 100X} faster without
losing the geometry representation accuracy. The proposed method can be applied
in a large range applications where the radiance field is needed, such as 3D
reconstruction, view synthesis, and dynamic object modeling.";Yuanhao Gong<author:sep>Lantao Yu<author:sep>Guanghui Yue;http://arxiv.org/pdf/2403.14244v1;cs.CV;;gaussian splatting
2403.14839v1;http://arxiv.org/abs/2403.14839v1;2024-03-21;Hyperspectral Neural Radiance Fields;"Hyperspectral Imagery (HSI) has been used in many applications to
non-destructively determine the material and/or chemical compositions of
samples. There is growing interest in creating 3D hyperspectral
reconstructions, which could provide both spatial and spectral information
while also mitigating common HSI challenges such as non-Lambertian surfaces and
translucent objects. However, traditional 3D reconstruction with HSI is
difficult due to technological limitations of hyperspectral cameras. In recent
years, Neural Radiance Fields (NeRFs) have seen widespread success in creating
high quality volumetric 3D representations of scenes captured by a variety of
camera models. Leveraging recent advances in NeRFs, we propose computing a
hyperspectral 3D reconstruction in which every point in space and view
direction is characterized by wavelength-dependent radiance and transmittance
spectra. To evaluate our approach, a dataset containing nearly 2000
hyperspectral images across 8 scenes and 2 cameras was collected. We perform
comparisons against traditional RGB NeRF baselines and apply ablation testing
with alternative spectra representations. Finally, we demonstrate the potential
of hyperspectral NeRFs for hyperspectral super-resolution and imaging sensor
simulation. We show that our hyperspectral NeRF approach enables creating fast,
accurate volumetric 3D hyperspectral scenes and enables several new
applications and areas for future study.";Gerry Chen<author:sep>Sunil Kumar Narayanan<author:sep>Thomas Gautier Ottou<author:sep>Benjamin Missaoui<author:sep>Harsh Muriki<author:sep>CÃ©dric Pradalier<author:sep>Yongsheng Chen;http://arxiv.org/pdf/2403.14839v1;cs.CV;"Main paper: 15 pages + 2 pages references. Supplemental/Appendix: 6
  pages";nerf
2403.14412v1;http://arxiv.org/abs/2403.14412v1;2024-03-21;CombiNeRF: A Combination of Regularization Techniques for Few-Shot  Neural Radiance Field View Synthesis;"Neural Radiance Fields (NeRFs) have shown impressive results for novel view
synthesis when a sufficiently large amount of views are available. When dealing
with few-shot settings, i.e. with a small set of input views, the training
could overfit those views, leading to artifacts and geometric and chromatic
inconsistencies in the resulting rendering. Regularization is a valid solution
that helps NeRF generalization. On the other hand, each of the most recent NeRF
regularization techniques aim to mitigate a specific rendering problem.
Starting from this observation, in this paper we propose CombiNeRF, a framework
that synergically combines several regularization techniques, some of them
novel, in order to unify the benefits of each. In particular, we regularize
single and neighboring rays distributions and we add a smoothness term to
regularize near geometries. After these geometric approaches, we propose to
exploit Lipschitz regularization to both NeRF density and color networks and to
use encoding masks for input features regularization. We show that CombiNeRF
outperforms the state-of-the-art methods with few-shot settings in several
publicly available datasets. We also present an ablation study on the LLFF and
NeRF-Synthetic datasets that support the choices made. We release with this
paper the open-source implementation of our framework.";Matteo Bonotto<author:sep>Luigi Sarrocco<author:sep>Daniele Evangelista<author:sep>Marco Imperoli<author:sep>Alberto Pretto;http://arxiv.org/pdf/2403.14412v1;cs.CV;"This paper has been accepted for publication at the 2024
  International Conference on 3D Vision (3DV)";nerf
2403.14554v1;http://arxiv.org/abs/2403.14554v1;2024-03-21;Gaussian Frosting: Editable Complex Radiance Fields with Real-Time  Rendering;"We propose Gaussian Frosting, a novel mesh-based representation for
high-quality rendering and editing of complex 3D effects in real-time. Our
approach builds on the recent 3D Gaussian Splatting framework, which optimizes
a set of 3D Gaussians to approximate a radiance field from images. We propose
first extracting a base mesh from Gaussians during optimization, then building
and refining an adaptive layer of Gaussians with a variable thickness around
the mesh to better capture the fine details and volumetric effects near the
surface, such as hair or grass. We call this layer Gaussian Frosting, as it
resembles a coating of frosting on a cake. The fuzzier the material, the
thicker the frosting. We also introduce a parameterization of the Gaussians to
enforce them to stay inside the frosting layer and automatically adjust their
parameters when deforming, rescaling, editing or animating the mesh. Our
representation allows for efficient rendering using Gaussian splatting, as well
as editing and animation by modifying the base mesh. We demonstrate the
effectiveness of our method on various synthetic and real scenes, and show that
it outperforms existing surface-based approaches. We will release our code and
a web-based viewer as additional contributions. Our project page is the
following: https://anttwo.github.io/frosting/";Antoine GuÃ©don<author:sep>Vincent Lepetit;http://arxiv.org/pdf/2403.14554v1;cs.CV;Project Webpage: https://anttwo.github.io/frosting/;gaussian splatting
2403.14619v1;http://arxiv.org/abs/2403.14619v1;2024-03-21;ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D  Decomposition;"3D decomposition/segmentation still remains a challenge as large-scale 3D
annotated data is not readily available. Contemporary approaches typically
leverage 2D machine-generated segments, integrating them for 3D consistency.
While the majority of these methods are based on NeRFs, they face a potential
weakness that the instance/semantic embedding features derive from independent
MLPs, thus preventing the segmentation network from learning the geometric
details of the objects directly through radiance and density. In this paper, we
propose ClusteringSDF, a novel approach to achieve both segmentation and
reconstruction in 3D via the neural implicit surface representation,
specifically Signal Distance Function (SDF), where the segmentation rendering
is directly integrated with the volume rendering of neural implicit surfaces.
Although based on ObjectSDF++, ClusteringSDF no longer requires the
ground-truth segments for supervision while maintaining the capability of
reconstructing individual object surfaces, but purely with the noisy and
inconsistent labels from pre-trained models.As the core of ClusteringSDF, we
introduce a high-efficient clustering mechanism for lifting the 2D labels to 3D
and the experimental results on the challenging scenes from ScanNet and Replica
datasets show that ClusteringSDF can achieve competitive performance compared
against the state-of-the-art with significantly reduced training time.";Tianhao Wu<author:sep>Chuanxia Zheng<author:sep>Tat-Jen Cham<author:sep>Qianyi Wu;http://arxiv.org/pdf/2403.14619v1;cs.CV;Project Page: https://sm0kywu.github.io/ClusteringSDF/;nerf
2403.14530v1;http://arxiv.org/abs/2403.14530v1;2024-03-21;HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression;"3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel
view synthesis, boasting rapid rendering speed with high fidelity. However, the
substantial Gaussians and their associated attributes necessitate effective
compression techniques. Nevertheless, the sparse and unorganized nature of the
point cloud of Gaussians (or anchors in our paper) presents challenges for
compression. To address this, we make use of the relations between the
unorganized anchors and the structured hash grid, leveraging their mutual
information for context modeling, and propose a Hash-grid Assisted Context
(HAC) framework for highly compact 3DGS representation. Our approach introduces
a binary hash grid to establish continuous spatial consistencies, allowing us
to unveil the inherent spatial relations of anchors through a carefully
designed context model. To facilitate entropy coding, we utilize Gaussian
distributions to accurately estimate the probability of each quantized
attribute, where an adaptive quantization module is proposed to enable
high-precision quantization of these attributes for improved fidelity
restoration. Additionally, we incorporate an adaptive masking strategy to
eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer
to explore context-based compression for 3DGS representation, resulting in a
remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while
simultaneously improving fidelity, and achieving over $11\times$ size reduction
over SOTA 3DGS compression approach Scaffold-GS. Our code is available here:
https://github.com/YihangChen-ee/HAC";Yihang Chen<author:sep>Qianyi Wu<author:sep>Jianfei Cai<author:sep>Mehrtash Harandi<author:sep>Weiyao Lin;http://arxiv.org/pdf/2403.14530v1;cs.CV;"Project Page: https://yihangchen-ee.github.io/project_hac/ Code:
  https://github.com/YihangChen-ee/HAC";gaussian splatting
2403.14370v2;http://arxiv.org/abs/2403.14370v2;2024-03-21;SyncTweedies: A General Generative Framework Based on Synchronized  Diffusions;"We introduce a general framework for generating diverse visual content,
including ambiguous images, panorama images, mesh textures, and Gaussian splat
textures, by synchronizing multiple diffusion processes. We present exhaustive
investigation into all possible scenarios for synchronizing multiple diffusion
processes through a canonical space and analyze their characteristics across
applications. In doing so, we reveal a previously unexplored case: averaging
the outputs of Tweedie's formula while conducting denoising in multiple
instance spaces. This case also provides the best quality with the widest
applicability to downstream tasks. We name this case SyncTweedies. In our
experiments generating visual content aforementioned, we demonstrate the
superior quality of generation by SyncTweedies compared to other
synchronization methods, optimization-based and iterative-update-based methods.";Jaihoon Kim<author:sep>Juil Koo<author:sep>Kyeongmin Yeo<author:sep>Minhyuk Sung;http://arxiv.org/pdf/2403.14370v2;cs.CV;Project page: https://synctweedies.github.io/;
2403.14053v1;http://arxiv.org/abs/2403.14053v1;2024-03-21;Leveraging Thermal Modality to Enhance Reconstruction in Low-Light  Conditions;"Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view
synthesis by learning the implicit volumetric representation of a scene from
multi-view images, which faithfully convey the colorimetric information.
However, sensor noises will contaminate low-value pixel signals, and the lossy
camera image signal processor will further remove near-zero intensities in
extremely dark situations, deteriorating the synthesis performance. Existing
approaches reconstruct low-light scenes from raw images but struggle to recover
texture and boundary details in dark regions. Additionally, they are unsuitable
for high-speed models relying on explicit representations. To address these
issues, we present Thermal-NeRF, which takes thermal and visible raw images as
inputs, considering the thermal camera is robust to the illumination variation
and raw images preserve any possible clues in the dark, to accomplish visible
and thermal view synthesis simultaneously. Also, the first multi-view thermal
and visible dataset (MVTV) is established to support the research on multimodal
NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and
noise smoothing and provides better synthesis performance than previous work.
Finally, we demonstrate that both modalities are beneficial to each other in 3D
reconstruction.";Jiacong Xu<author:sep>Mingqian Liao<author:sep>K Ram Prabhakar<author:sep>Vishal M. Patel;http://arxiv.org/pdf/2403.14053v1;cs.CV;25 pages, 13 figures;nerf
2403.13806v1;http://arxiv.org/abs/2403.13806v1;2024-03-20;RadSplat: Radiance Field-Informed Gaussian Splatting for Robust  Real-Time Rendering with 900+ FPS;"Recent advances in view synthesis and real-time rendering have achieved
photorealistic quality at impressive rendering speeds. While Radiance
Field-based methods achieve state-of-the-art quality in challenging scenarios
such as in-the-wild captures and large-scale scenes, they often suffer from
excessively high compute requirements linked to volumetric rendering. Gaussian
Splatting-based methods, on the other hand, rely on rasterization and naturally
achieve real-time rendering but suffer from brittle optimization heuristics
that underperform on more challenging scenes. In this work, we present
RadSplat, a lightweight method for robust real-time rendering of complex
scenes. Our main contributions are threefold. First, we use radiance fields as
a prior and supervision signal for optimizing point-based scene
representations, leading to improved quality and more robust optimization.
Next, we develop a novel pruning technique reducing the overall point count
while maintaining high quality, leading to smaller and more compact scene
representations with faster inference speeds. Finally, we propose a novel
test-time filtering approach that further accelerates rendering and allows to
scale to larger, house-sized scenes. We find that our method enables
state-of-the-art synthesis of complex captures at 900+ FPS.";Michael Niemeyer<author:sep>Fabian Manhardt<author:sep>Marie-Julie Rakotosaona<author:sep>Michael Oechsle<author:sep>Daniel Duckworth<author:sep>Rama Gosula<author:sep>Keisuke Tateno<author:sep>John Bates<author:sep>Dominik Kaeser<author:sep>Federico Tombari;http://arxiv.org/pdf/2403.13806v1;cs.CV;Project page at https://m-niemeyer.github.io/radsplat/;gaussian splatting
2403.13348v1;http://arxiv.org/abs/2403.13348v1;2024-03-20;MULAN-WC: Multi-Robot Localization Uncertainty-aware Active NeRF with  Wireless Coordination;"This paper presents MULAN-WC, a novel multi-robot 3D reconstruction framework
that leverages wireless signal-based coordination between robots and Neural
Radiance Fields (NeRF). Our approach addresses key challenges in multi-robot 3D
reconstruction, including inter-robot pose estimation, localization uncertainty
quantification, and active best-next-view selection. We introduce a method for
using wireless Angle-of-Arrival (AoA) and ranging measurements to estimate
relative poses between robots, as well as quantifying and incorporating the
uncertainty embedded in the wireless localization of these pose estimates into
the NeRF training loss to mitigate the impact of inaccurate camera poses.
Furthermore, we propose an active view selection approach that accounts for
robot pose uncertainty when determining the next-best views to improve the 3D
reconstruction, enabling faster convergence through intelligent view selection.
Extensive experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our framework in theory and in practice. Leveraging wireless
coordination and localization uncertainty-aware training, MULAN-WC can achieve
high-quality 3d reconstruction which is close to applying the ground truth
camera poses. Furthermore, the quantification of the information gain from a
novel view enables consistent rendering quality improvement with incrementally
captured images by commending the robot the novel view position. Our hardware
experiments showcase the practicality of deploying MULAN-WC to real robotic
systems.";Weiying Wang<author:sep>Victor Cai<author:sep>Stephanie Gil;http://arxiv.org/pdf/2403.13348v1;cs.RO;;nerf
2403.13327v1;http://arxiv.org/abs/2403.13327v1;2024-03-20;Gaussian Splatting on the Move: Blur and Rolling Shutter Compensation  for Natural Camera Motion;"High-quality scene reconstruction and novel view synthesis based on Gaussian
Splatting (3DGS) typically require steady, high-quality photographs, often
impractical to capture with handheld cameras. We present a method that adapts
to camera motion and allows high-quality scene reconstruction with handheld
video data suffering from motion blur and rolling shutter distortion. Our
approach is based on detailed modelling of the physical image formation process
and utilizes velocities estimated using visual-inertial odometry (VIO). Camera
poses are considered non-static during the exposure time of a single image
frame and camera poses are further optimized in the reconstruction process. We
formulate a differentiable rendering pipeline that leverages screen space
approximation to efficiently incorporate rolling-shutter and motion blur
effects into the 3DGS framework. Our results with both synthetic and real data
demonstrate superior performance in mitigating camera motion over existing
methods, thereby advancing 3DGS in naturalistic settings.";Otto Seiskari<author:sep>Jerry Ylilammi<author:sep>Valtteri Kaatrasalo<author:sep>Pekka Rantalankila<author:sep>Matias Turkulainen<author:sep>Juho Kannala<author:sep>Esa Rahtu<author:sep>Arno Solin;http://arxiv.org/pdf/2403.13327v1;cs.CV;Source code available at https://github.com/SpectacularAI/3dgs-deblur;gaussian splatting
2403.13206v1;http://arxiv.org/abs/2403.13206v1;2024-03-19;Depth-guided NeRF Training via Earth Mover's Distance;"Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of
predicted viewpoints. However, the photometric loss often does not provide
enough information to disambiguate between different possible geometries
yielding the same image. Previous work has thus incorporated depth supervision
during NeRF training, leveraging dense predictions from pre-trained depth
networks as pseudo-ground truth. While these depth priors are assumed to be
perfect once filtered for noise, in practice, their accuracy is more
challenging to capture. This work proposes a novel approach to uncertainty in
depth priors for NeRF supervision. Instead of using custom-trained depth or
uncertainty priors, we use off-the-shelf pretrained diffusion models to predict
depth and capture uncertainty during the denoising process. Because we know
that depth priors are prone to errors, we propose to supervise the ray
termination distance distribution with Earth Mover's Distance instead of
enforcing the rendered depth to replicate the depth prior exactly through
L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth
metrics by a large margin while maintaining performance on photometric
measures.";Anita Rau<author:sep>Josiah Aklilu<author:sep>F. Christopher Holsinger<author:sep>Serena Yeung-Levy;http://arxiv.org/pdf/2403.13206v1;cs.CV;Preprint. Under review;nerf
2403.12535v1;http://arxiv.org/abs/2403.12535v1;2024-03-19;High-Fidelity SLAM Using Gaussian Splatting with Rendering-Guided  Densification and Regularized Optimization;"We propose a dense RGBD SLAM system based on 3D Gaussian Splatting that
provides metrically accurate pose tracking and visually realistic
reconstruction. To this end, we first propose a Gaussian densification strategy
based on the rendering loss to map unobserved areas and refine reobserved
areas. Second, we introduce extra regularization parameters to alleviate the
forgetting problem in the continuous mapping problem, where parameters tend to
overfit the latest frame and result in decreasing rendering quality for
previous frames. Both mapping and tracking are performed with Gaussian
parameters by minimizing re-rendering loss in a differentiable way. Compared to
recent neural and concurrently developed gaussian splatting RGBD SLAM
baselines, our method achieves state-of-the-art results on the synthetic
dataset Replica and competitive results on the real-world dataset TUM.";Shuo Sun<author:sep>Malcolm Mielle<author:sep>Achim J. Lilienthal<author:sep>Martin Magnusson;http://arxiv.org/pdf/2403.12535v1;cs.RO;submitted to IROS24;gaussian splatting
2403.12800v1;http://arxiv.org/abs/2403.12800v1;2024-03-19;Learning Neural Volumetric Pose Features for Camera Localization;"We introduce a novel neural volumetric pose feature, termed PoseMap, designed
to enhance camera localization by encapsulating the information between images
and the associated camera poses. Our framework leverages an Absolute Pose
Regression (APR) architecture, together with an augmented NeRF module. This
integration not only facilitates the generation of novel views to enrich the
training dataset but also enables the learning of effective pose features.
Additionally, we extend our architecture for self-supervised online alignment,
allowing our method to be used and fine-tuned for unlabelled images within a
unified framework. Experiments demonstrate that our method achieves 14.28% and
20.51% performance gain on average in indoor and outdoor benchmark scenes,
outperforming existing APR methods with state-of-the-art accuracy.";Jingyu Lin<author:sep>Jiaqi Gu<author:sep>Bojian Wu<author:sep>Lubin Fan<author:sep>Renjie Chen<author:sep>Ligang Liu<author:sep>Jieping Ye;http://arxiv.org/pdf/2403.12800v1;cs.CV;14 pages, 9 figures;nerf
2403.13199v1;http://arxiv.org/abs/2403.13199v1;2024-03-19;DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced  Images;"Neural radiance fields (NeRFs) show potential for transforming images
captured worldwide into immersive 3D visual experiences. However, most of this
captured visual data remains siloed in our camera rolls as these images contain
personal details. Even if made public, the problem of learning 3D
representations of billions of scenes captured daily in a centralized manner is
computationally intractable. Our approach, DecentNeRF, is the first attempt at
decentralized, crowd-sourced NeRFs that require $\sim 10^4\times$ less server
computing for a scene than a centralized approach. Instead of sending the raw
data, our approach requires users to send a 3D representation, distributing the
high computation cost of training centralized NeRFs between the users. It
learns photorealistic scene representations by decomposing users' 3D views into
personal and global NeRFs and a novel optimally weighted aggregation of only
the latter. We validate the advantage of our approach to learn NeRFs with
photorealism and minimal server computation cost on structured synthetic and
real-world photo tourism datasets. We further analyze how secure aggregation of
global NeRFs in DecentNeRF minimizes the undesired reconstruction of personal
content by the server.";Zaid Tasneem<author:sep>Akshat Dave<author:sep>Abhishek Singh<author:sep>Kushagra Tiwary<author:sep>Praneeth Vepakomma<author:sep>Ashok Veeraraghavan<author:sep>Ramesh Raskar;http://arxiv.org/pdf/2403.13199v1;cs.CV;;nerf
2403.12550v2;http://arxiv.org/abs/2403.12550v2;2024-03-19;RGBD GS-ICP SLAM;"Simultaneous Localization and Mapping (SLAM) with dense representation plays
a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)
applications. Recent advancements in dense representation SLAM have highlighted
the potential of leveraging neural scene representation and 3D Gaussian
representation for high-fidelity spatial representation. In this paper, we
propose a novel dense representation SLAM approach with a fusion of Generalized
Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast
to existing methods, we utilize a single Gaussian map for both tracking and
mapping, resulting in mutual benefits. Through the exchange of covariances
between tracking and mapping processes with scale alignment techniques, we
minimize redundant computations and achieve an efficient system. Additionally,
we enhance tracking accuracy and mapping quality through our keyframe selection
methods. Experimental results demonstrate the effectiveness of our approach,
showing an incredibly fast speed up to 107 FPS (for the entire system) and
superior quality of the reconstructed map.";Seongbo Ha<author:sep>Jiung Yeon<author:sep>Hyeonwoo Yu;http://arxiv.org/pdf/2403.12550v2;cs.CV;;gaussian splatting
2403.12365v1;http://arxiv.org/abs/2403.12365v1;2024-03-19;GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation;"Creating 4D fields of Gaussian Splatting from images or videos is a
challenging task due to its under-constrained nature. While the optimization
can draw photometric reference from the input videos or be regulated by
generative models, directly supervising Gaussian motions remains underexplored.
In this paper, we introduce a novel concept, Gaussian flow, which connects the
dynamics of 3D Gaussians and pixel velocities between consecutive frames. The
Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into
the image space. This differentiable process enables direct dynamic supervision
from optical flow. Our method significantly benefits 4D dynamic content
generation and 4D novel view synthesis with Gaussian Splatting, especially for
contents with rich motions that are hard to be handled by existing methods. The
common color drifting issue that happens in 4D generation is also resolved with
improved Guassian dynamics. Superior visual quality on extensive experiments
demonstrates our method's effectiveness. Quantitative and qualitative
evaluations show that our method achieves state-of-the-art results on both
tasks of 4D generation and 4D novel view synthesis. Project page:
https://zerg-overmind.github.io/GaussianFlow.github.io/";Quankai Gao<author:sep>Qiangeng Xu<author:sep>Zhe Cao<author:sep>Ben Mildenhall<author:sep>Wenchao Ma<author:sep>Le Chen<author:sep>Danhang Tang<author:sep>Ulrich Neumann;http://arxiv.org/pdf/2403.12365v1;cs.CV;;gaussian splatting
2403.12722v1;http://arxiv.org/abs/2403.12722v1;2024-03-19;HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting;"Holistic understanding of urban scenes based on RGB images is a challenging
yet important problem. It encompasses understanding both the geometry and
appearance to enable novel view synthesis, parsing semantic labels, and
tracking moving objects. Despite considerable progress, existing approaches
often focus on specific aspects of this task and require additional inputs such
as LiDAR scans or manually annotated 3D bounding boxes. In this paper, we
introduce a novel pipeline that utilizes 3D Gaussian Splatting for holistic
urban scene understanding. Our main idea involves the joint optimization of
geometry, appearance, semantics, and motion using a combination of static and
dynamic 3D Gaussians, where moving object poses are regularized via physical
constraints. Our approach offers the ability to render new viewpoints in
real-time, yielding 2D and 3D semantic information with high accuracy, and
reconstruct dynamic scenes, even in scenarios where 3D bounding box detection
are highly noisy. Experimental results on KITTI, KITTI-360, and Virtual KITTI 2
demonstrate the effectiveness of our approach.";Hongyu Zhou<author:sep>Jiahao Shao<author:sep>Lu Xu<author:sep>Dongfeng Bai<author:sep>Weichao Qiu<author:sep>Bingbing Liu<author:sep>Yue Wang<author:sep>Andreas Geiger<author:sep>Yiyi Liao;http://arxiv.org/pdf/2403.12722v1;cs.CV;Our project page is at https://xdimlab.github.io/hugs_website;gaussian splatting
2403.12682v1;http://arxiv.org/abs/2403.12682v1;2024-03-19;IFFNeRF: Initialisation Free and Fast 6DoF pose estimation from a single  image and a NeRF model;"We introduce IFFNeRF to estimate the six degrees-of-freedom (6DoF) camera
pose of a given image, building on the Neural Radiance Fields (NeRF)
formulation. IFFNeRF is specifically designed to operate in real-time and
eliminates the need for an initial pose guess that is proximate to the sought
solution. IFFNeRF utilizes the Metropolis-Hasting algorithm to sample surface
points from within the NeRF model. From these sampled points, we cast rays and
deduce the color for each ray through pixel-level view synthesis. The camera
pose can then be estimated as the solution to a Least Squares problem by
selecting correspondences between the query image and the resulting bundle. We
facilitate this process through a learned attention mechanism, bridging the
query image embedding with the embedding of parameterized rays, thereby
matching rays pertinent to the image. Through synthetic and real evaluation
settings, we show that our method can improve the angular and translation error
accuracy by 80.1% and 67.3%, respectively, compared to iNeRF while performing
at 34fps on consumer hardware and not requiring the initial pose guess.";Matteo Bortolon<author:sep>Theodore Tsesmelis<author:sep>Stuart James<author:sep>Fabio Poiesi<author:sep>Alessio Del Bue;http://arxiv.org/pdf/2403.12682v1;cs.CV;"Accepted ICRA 2024, Project page:
  https://mbortolon97.github.io/iffnerf/";nerf
2403.12839v1;http://arxiv.org/abs/2403.12839v1;2024-03-19;Global-guided Focal Neural Radiance Field for Large-scale Scene  Rendering;"Neural radiance fields~(NeRF) have recently been applied to render
large-scale scenes. However, their limited model capacity typically results in
blurred rendering results. Existing large-scale NeRFs primarily address this
limitation by partitioning the scene into blocks, which are subsequently
handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and
processed independently, lead to inconsistencies in geometry and appearance
across the scene. Consequently, the rendering quality fails to exhibit
significant improvement despite the expansion of model capacity. In this work,
we present global-guided focal neural radiance field (GF-NeRF) that achieves
high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a
two-stage (Global and Focal) architecture and a global-guided training
strategy. The global stage obtains a continuous representation of the entire
scene while the focal stage decomposes the scene into multiple blocks and
further processes them with distinct sub-encoders. Leveraging this two-stage
architecture, sub-encoders only need fine-tuning based on the global encoder,
thus reducing training complexity in the focal stage while maintaining
scene-wide consistency. Spatial information and error information from the
global stage also benefit the sub-encoders to focus on crucial areas and
effectively capture more details of large-scale scenes. Notably, our approach
does not rely on any prior knowledge about the target scene, attributing
GF-NeRF adaptable to various large-scale scene types, including street-view and
aerial-view scenes. We demonstrate that our method achieves high-fidelity,
natural rendering results on various types of large-scale datasets. Our project
page: https://shaomq2187.github.io/GF-NeRF/";Mingqi Shao<author:sep>Feng Xiong<author:sep>Hang Zhang<author:sep>Shuang Yang<author:sep>Mu Xu<author:sep>Wei Bian<author:sep>Xueqian Wang;http://arxiv.org/pdf/2403.12839v1;cs.CV;;nerf
2403.12957v1;http://arxiv.org/abs/2403.12957v1;2024-03-19;GVGEN: Text-to-3D Generation with Volumetric Representation;"In recent years, 3D Gaussian splatting has emerged as a powerful technique
for 3D reconstruction and generation, known for its fast and high-quality
rendering capabilities. To address these shortcomings, this paper introduces a
novel diffusion-based framework, GVGEN, designed to efficiently generate 3D
Gaussian representations from text input. We propose two innovative
techniques:(1) Structured Volumetric Representation. We first arrange
disorganized 3D Gaussian points as a structured form GaussianVolume. This
transformation allows the capture of intricate texture details within a volume
composed of a fixed number of Gaussians. To better optimize the representation
of these details, we propose a unique pruning and densifying method named the
Candidate Pool Strategy, enhancing detail fidelity through selective
optimization. (2) Coarse-to-fine Generation Pipeline. To simplify the
generation of GaussianVolume and empower the model to generate instances with
detailed 3D geometry, we propose a coarse-to-fine pipeline. It initially
constructs a basic geometric structure, followed by the prediction of complete
Gaussian attributes. Our framework, GVGEN, demonstrates superior performance in
qualitative and quantitative assessments compared to existing 3D generation
methods. Simultaneously, it maintains a fast generation speed ($\sim$7
seconds), effectively striking a balance between quality and efficiency.";Xianglong He<author:sep>Junyi Chen<author:sep>Sida Peng<author:sep>Di Huang<author:sep>Yangguang Li<author:sep>Xiaoshui Huang<author:sep>Chun Yuan<author:sep>Wanli Ouyang<author:sep>Tong He;http://arxiv.org/pdf/2403.12957v1;cs.CV;project page: https://gvgen.github.io/;gaussian splatting
2403.11831v2;http://arxiv.org/abs/2403.11831v2;2024-03-18;BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting;"While neural rendering has demonstrated impressive capabilities in 3D scene
reconstruction and novel view synthesis, it heavily relies on high-quality
sharp images and accurate camera poses. Numerous approaches have been proposed
to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly
encountered in real-world scenarios such as low-light or long-exposure
conditions. However, the implicit representation of NeRF struggles to
accurately recover intricate details from severely motion-blurred images and
cannot achieve real-time rendering. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time
rendering by explicitly optimizing point clouds as Gaussian spheres.
  In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle
Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian
representation and handles severe motion-blurred images with inaccurate camera
poses to achieve high-quality scene reconstruction. Our method models the
physical image formation process of motion-blurred images and jointly learns
the parameters of Gaussians while recovering camera motion trajectories during
exposure time.
  In our experiments, we demonstrate that BAD-Gaussians not only achieves
superior rendering quality compared to previous state-of-the-art deblur neural
rendering methods on both synthetic and real datasets but also enables
real-time rendering capabilities.
  Our project page and source code is available at
https://lingzhezhao.github.io/BAD-Gaussians/";Lingzhe Zhao<author:sep>Peng Wang<author:sep>Peidong Liu;http://arxiv.org/pdf/2403.11831v2;cs.CV;"Project Page and Source Code:
  https://lingzhezhao.github.io/BAD-Gaussians/";gaussian splatting<tag:sep>nerf
2403.11396v1;http://arxiv.org/abs/2403.11396v1;2024-03-18;Beyond Uncertainty: Risk-Aware Active View Acquisition for Safe Robot  Navigation and 3D Scene Understanding with FisherRF;"This work proposes a novel approach to bolster both the robot's risk
assessment and safety measures while deepening its understanding of 3D scenes,
which is achieved by leveraging Radiance Field (RF) models and 3D Gaussian
Splatting. To further enhance these capabilities, we incorporate additional
sampled views from the environment with the RF model. One of our key
contributions is the introduction of Risk-aware Environment Masking (RaEM),
which prioritizes crucial information by selecting the next-best-view that
maximizes the expected information gain. This targeted approach aims to
minimize uncertainties surrounding the robot's path and enhance the safety of
its navigation. Our method offers a dual benefit: improved robot safety and
increased efficiency in risk-aware 3D scene reconstruction and understanding.
Extensive experiments in real-world scenarios demonstrate the effectiveness of
our proposed approach, highlighting its potential to establish a robust and
safety-focused framework for active robot exploration and 3D scene
understanding.";Guangyi Liu<author:sep>Wen Jiang<author:sep>Boshu Lei<author:sep>Vivek Pandey<author:sep>Kostas Daniilidis<author:sep>Nader Motee;http://arxiv.org/pdf/2403.11396v1;cs.RO;;
2403.11577v1;http://arxiv.org/abs/2403.11577v1;2024-03-18;3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal  Calibration;"Reliable multimodal sensor fusion algorithms require accurate spatiotemporal
calibration. Recently, targetless calibration techniques based on implicit
neural representations have proven to provide precise and robust results.
Nevertheless, such methods are inherently slow to train given the high
computational overhead caused by the large number of sampled points required
for volume rendering. With the recent introduction of 3D Gaussian Splatting as
a faster alternative to implicit representation methods, we propose to leverage
this new rendering approach to achieve faster multi-sensor calibration. We
introduce 3DGS-Calib, a new calibration method that relies on the speed and
rendering accuracy of 3D Gaussian Splatting to achieve multimodal
spatiotemporal calibration that is accurate, robust, and with a substantial
speed-up compared to methods relying on implicit neural representations. We
demonstrate the superiority of our proposal with experimental results on
sequences from KITTI-360, a widely used driving dataset.";Quentin Herau<author:sep>Moussab Bennehar<author:sep>Arthur Moreau<author:sep>Nathan Piasco<author:sep>Luis Roldao<author:sep>Dzmitry Tsishkou<author:sep>Cyrille Migniot<author:sep>Pascal Vasseur<author:sep>CÃ©dric Demonceaux;http://arxiv.org/pdf/2403.11577v1;cs.CV;Under review;gaussian splatting
2403.11589v1;http://arxiv.org/abs/2403.11589v1;2024-03-18;UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures  for Human Avatar Modeling;"Reconstructing photo-realistic drivable human avatars from multi-view image
sequences has been a popular and challenging topic in the field of computer
vision and graphics. While existing NeRF-based methods can achieve high-quality
novel view rendering of human models, both training and inference processes are
time-consuming. Recent approaches have utilized 3D Gaussians to represent the
human body, enabling faster training and rendering. However, they undermine the
importance of the mesh guidance and directly predict Gaussians in 3D space with
coarse mesh guidance. This hinders the learning procedure of the Gaussians and
tends to produce blurry textures. Therefore, we propose UV Gaussians, which
models the 3D human body by jointly learning mesh deformations and 2D UV-space
Gaussian textures. We utilize the embedding of UV map to learn Gaussian
textures in 2D space, leveraging the capabilities of powerful 2D networks to
extract features. Additionally, through an independent Mesh network, we
optimize pose-dependent geometric deformations, thereby guiding Gaussian
rendering and significantly enhancing rendering quality. We collect and process
a new dataset of human motion, which includes multi-view images, scanned
models, parametric model registration, and corresponding texture maps.
Experimental results demonstrate that our method achieves state-of-the-art
synthesis of novel view and novel pose. The code and data will be made
available on the homepage https://alex-jyj.github.io/UV-Gaussians/ once the
paper is accepted.";Yujiao Jiang<author:sep>Qingmin Liao<author:sep>Xiaoyu Li<author:sep>Li Ma<author:sep>Qi Zhang<author:sep>Chaopeng Zhang<author:sep>Zongqing Lu<author:sep>Ying Shan;http://arxiv.org/pdf/2403.11589v1;cs.CV;;nerf
2403.11427v1;http://arxiv.org/abs/2403.11427v1;2024-03-18;BAGS: Building Animatable Gaussian Splatting from a Monocular Video with  Diffusion Priors;"Animatable 3D reconstruction has significant applications across various
fields, primarily relying on artists' handcraft creation. Recently, some
studies have successfully constructed animatable 3D models from monocular
videos. However, these approaches require sufficient view coverage of the
object within the input video and typically necessitate significant time and
computational costs for training and rendering. This limitation restricts the
practical applications. In this work, we propose a method to build animatable
3D Gaussian Splatting from monocular video with diffusion priors. The 3D
Gaussian representations significantly accelerate the training and rendering
process, and the diffusion priors allow the method to learn 3D models with
limited viewpoints. We also present the rigid regularization to enhance the
utilization of the priors. We perform an extensive evaluation across various
real-world videos, demonstrating its superior performance compared to the
current state-of-the-art methods.";Tingyang Zhang<author:sep>Qingzhe Gao<author:sep>Weiyu Li<author:sep>Libin Liu<author:sep>Baoquan Chen;http://arxiv.org/pdf/2403.11427v1;cs.CV;https://talegqz.github.io/BAGS/;gaussian splatting
2403.12154v1;http://arxiv.org/abs/2403.12154v1;2024-03-18;ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View  Synthesis;"Thermal scene reconstruction exhibit great potential for applications across
a broad spectrum of fields, including building energy consumption analysis and
non-destructive testing. However, existing methods typically require dense
scene measurements and often rely on RGB images for 3D geometry reconstruction,
with thermal information being projected post-reconstruction. This two-step
strategy, adopted due to the lack of texture in thermal images, can lead to
disparities between the geometry and temperatures of the reconstructed objects
and those of the actual scene. To address this challenge, we propose
ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields,
capable of rendering new RGB and thermal views of a scene jointly. To overcome
the lack of texture in thermal images, we use paired RGB and thermal images to
learn scene density, while distinct networks estimate color and temperature
information. Furthermore, we introduce ThermoScenes, a new dataset to palliate
the lack of available RGB+thermal datasets for scene reconstruction.
Experimental results validate that ThermoNeRF achieves accurate thermal image
synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement
of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a
state-of-the-art NeRF method.";Mariam Hassan<author:sep>Florent Forest<author:sep>Olga Fink<author:sep>Malcolm Mielle;http://arxiv.org/pdf/2403.12154v1;cs.CV;;nerf
2403.11573v2;http://arxiv.org/abs/2403.11573v2;2024-03-18;Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for  Resolving Class-imbalance Problem;"Typical LiDAR-based 3D object detection models are trained in a supervised
manner with real-world data collection, which is often imbalanced over classes
(or long-tailed). To deal with it, augmenting minority-class examples by
sampling ground truth (GT) LiDAR points from a database and pasting them into a
scene of interest is often used, but challenges still remain: inflexibility in
locating GT samples and limited sample diversity. In this work, we propose to
leverage pseudo-LiDAR point clouds generated (at a low cost) from videos
capturing a surround view of miniatures or real-world objects of minor classes.
Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of
three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D
view synthesis model, (ii) object-level domain alignment with LiDAR intensity
estimation and (iii) a hybrid context-aware placement method from ground and
map information. We demonstrate the superiority and generality of our method
through performance improvements in extensive experiments conducted on three
popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the
datasets with large domain gaps captured by different LiDAR configurations. Our
code and data will be publicly available upon publication.";Mincheol Chang<author:sep>Siyeong Lee<author:sep>Jinkyu Kim<author:sep>Namil Kim;http://arxiv.org/pdf/2403.11573v2;cs.CV;28 pages, 12 figures, 11 tables;nerf
2403.11678v1;http://arxiv.org/abs/2403.11678v1;2024-03-18;Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous  Scenes;"We present a method enabling the scaling of NeRFs to learn a large number of
semantically-similar scenes. We combine two techniques to improve the required
training time and memory cost per scene. First, we learn a 3D-aware latent
space in which we train Tri-Plane scene representations, hence reducing the
resolution at which scenes are learned. Moreover, we present a way to share
common information across scenes, hence allowing for a reduction of model
complexity to learn a particular scene. Our method reduces effective per-scene
memory costs by 44% and per-scene time costs by 86% when training 1000 scenes.
Our project page can be found at https://3da-ae.github.io .";Antoine Schnepf<author:sep>Karim Kassab<author:sep>Jean-Yves Franceschi<author:sep>Laurent Caraffa<author:sep>Flavian Vasile<author:sep>Jeremie Mary<author:sep>Andrew Comport<author:sep>ValÃ©rie Gouet-Brunet;http://arxiv.org/pdf/2403.11678v1;cs.CV;;nerf
2403.11453v1;http://arxiv.org/abs/2403.11453v1;2024-03-18;Bridging 3D Gaussian and Mesh for Freeview Video Rendering;"This is only a preview version of GauMesh. Recently, primitive-based
rendering has been proven to achieve convincing results in solving the problem
of modeling and rendering the 3D dynamic scene from 2D images. Despite this, in
the context of novel view synthesis, each type of primitive has its inherent
defects in terms of representation ability. It is difficult to exploit the mesh
to depict the fuzzy geometry. Meanwhile, the point-based splatting (e.g. the 3D
Gaussian Splatting) method usually produces artifacts or blurry pixels in the
area with smooth geometry and sharp textures. As a result, it is difficult,
even not impossible, to represent the complex and dynamic scene with a single
type of primitive. To this end, we propose a novel approach, GauMesh, to bridge
the 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a
sequence of tracked mesh as initialization, our goal is to simultaneously
optimize the mesh geometry, color texture, opacity maps, a set of 3D Gaussians,
and the deformation field. At a specific time, we perform $\alpha$-blending on
the RGB and opacity values based on the merged and re-ordered z-buffers from
mesh and 3D Gaussian rasterizations. This produces the final rendering, which
is supervised by the ground-truth image. Experiments demonstrate that our
approach adapts the appropriate type of primitives to represent the different
parts of the dynamic scene and outperforms all the baseline methods in both
quantitative and qualitative comparisons without losing render speed.";Yuting Xiao<author:sep>Xuan Wang<author:sep>Jiafei Li<author:sep>Hongrui Cai<author:sep>Yanbo Fan<author:sep>Nan Xue<author:sep>Minghui Yang<author:sep>Yujun Shen<author:sep>Shenghua Gao;http://arxiv.org/pdf/2403.11453v1;cs.GR;7 pages;gaussian splatting
2403.11776v1;http://arxiv.org/abs/2403.11776v1;2024-03-18;DVN-SLAM: Dynamic Visual Neural SLAM Based on Local-Global Encoding;"Recent research on Simultaneous Localization and Mapping (SLAM) based on
implicit representation has shown promising results in indoor environments.
However, there are still some challenges: the limited scene representation
capability of implicit encodings, the uncertainty in the rendering process from
implicit representations, and the disruption of consistency by dynamic objects.
To address these challenges, we propose a real-time dynamic visual SLAM system
based on local-global fusion neural implicit representation, named DVN-SLAM. To
improve the scene representation capability, we introduce a local-global fusion
neural implicit representation that enables the construction of an implicit map
while considering both global structure and local details. To tackle
uncertainties arising from the rendering process, we design an information
concentration loss for optimization, aiming to concentrate scene information on
object surfaces. The proposed DVN-SLAM achieves competitive performance in
localization and mapping across multiple datasets. More importantly, DVN-SLAM
demonstrates robustness in dynamic scenes, a trait that sets it apart from
other NeRF-based methods.";Wenhua Wu<author:sep>Guangming Wang<author:sep>Ting Deng<author:sep>Sebastian Aegidius<author:sep>Stuart Shanks<author:sep>Valerio Modugno<author:sep>Dimitrios Kanoulas<author:sep>Hesheng Wang;http://arxiv.org/pdf/2403.11776v1;cs.CV;;nerf
2403.11625v2;http://arxiv.org/abs/2403.11625v2;2024-03-18;GaussNav: Gaussian Splatting for Visual Navigation;"In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to
locate a specific object depicted in a goal image within an unexplored
environment. The primary difficulty of IIN stems from the necessity of
recognizing the target object across varying viewpoints and rejecting potential
distractors.
  Existing map-based navigation methods largely adopt the representation form
of Bird's Eye View (BEV) maps, which, however, lack the representation of
detailed textures in a scene.
  To address the above issues, we propose a new Gaussian Splatting Navigation
(abbreviated as GaussNav) framework for IIN task, which constructs a novel map
representation based on 3D Gaussian Splatting (3DGS).
  The proposed framework enables the agent to not only memorize the geometry
and semantic information of the scene, but also retain the textural features of
objects.
  Our GaussNav framework demonstrates a significant leap in performance,
evidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to
0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset.
  Our code will be made publicly available.";Xiaohan Lei<author:sep>Min Wang<author:sep>Wengang Zhou<author:sep>Houqiang Li;http://arxiv.org/pdf/2403.11625v2;cs.CV;conference;gaussian splatting
2403.11679v1;http://arxiv.org/abs/2403.11679v1;2024-03-18;NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using  3D Gaussian Splatting;"We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D
Gaussian representation, that enables robust 3D semantic mapping, accurate
camera tracking, and high-quality rendering in real-time. In the system, we
propose a Spatially Consistent Feature Fusion model to reduce the effect of
erroneous estimates from pre-trained segmentation head on semantic
reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we
employ a lightweight encoder-decoder to compress the high-dimensional semantic
features into a compact 3D Gaussian representation, mitigating the burden of
excessive memory consumption. Furthermore, we leverage the advantage of 3D
Gaussian splatting, which enables efficient and differentiable novel view
rendering, and propose a Virtual Camera View Pruning method to eliminate
outlier GS points, thereby effectively enhancing the quality of scene
representations. Our NEDS-SLAM method demonstrates competitive performance over
existing dense semantic SLAM methods in terms of mapping and tracking accuracy
on Replica and ScanNet datasets, while also showing excellent capabilities in
3D dense semantic mapping.";Yiming Ji<author:sep>Yang Liu<author:sep>Guanghu Xie<author:sep>Boyu Ma<author:sep>Zongwu Xie;http://arxiv.org/pdf/2403.11679v1;cs.CV;;gaussian splatting
2403.11899v1;http://arxiv.org/abs/2403.11899v1;2024-03-18;GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with  Noisy Polarization Priors;"Learning surfaces from neural radiance field (NeRF) became a rising topic in
Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods
demonstrated their ability to reconstruct accurate 3D shapes of Lambertian
scenes. However, their results on reflective scenes are unsatisfactory due to
the entanglement of specular radiance and complicated geometry. To address the
challenges, we propose a Gaussian-based representation of normals in SDF
fields. Supervised by polarization priors, this representation guides the
learning of geometry behind the specular reflection and captures more details
than existing methods. Moreover, we propose a reweighting strategy in the
optimization process to alleviate the noise issue of polarization priors. To
validate the effectiveness of our design, we capture polarimetric information,
and ground truth meshes in additional reflective scenes with various geometry.
We also evaluated our framework on the PANDORA dataset. Comparisons prove our
method outperforms existing neural 3D reconstruction methods in reflective
scenes by a large margin.";LI Yang<author:sep>WU Ruizheng<author:sep>LI Jiyong<author:sep>CHEN Ying-cong;http://arxiv.org/pdf/2403.11899v1;cs.CV;"Accepted to ICLR 2024 Poster. For the Appendix, please see
  http://yukiumi13.github.io/gnerp_page";nerf
2403.11447v1;http://arxiv.org/abs/2403.11447v1;2024-03-18;Motion-aware 3D Gaussian Splatting for Efficient Dynamic Scene  Reconstruction;"3D Gaussian Splatting (3DGS) has become an emerging tool for dynamic scene
reconstruction. However, existing methods focus mainly on extending static 3DGS
into a time-variant representation, while overlooking the rich motion
information carried by 2D observations, thus suffering from performance
degradation and model redundancy. To address the above problem, we propose a
novel motion-aware enhancement framework for dynamic scene reconstruction,
which mines useful motion cues from optical flow to improve different paradigms
of dynamic 3DGS. Specifically, we first establish a correspondence between 3D
Gaussian movements and pixel-level flow. Then a novel flow augmentation method
is introduced with additional insights into uncertainty and loss collaboration.
Moreover, for the prevalent deformation-based paradigm that presents a harder
optimization problem, a transient-aware deformation auxiliary module is
proposed. We conduct extensive experiments on both multi-view and monocular
scenes to verify the merits of our work. Compared with the baselines, our
method shows significant superiority in both rendering quality and efficiency.";Zhiyang Guo<author:sep>Wengang Zhou<author:sep>Li Li<author:sep>Min Wang<author:sep>Houqiang Li;http://arxiv.org/pdf/2403.11447v1;cs.CV;;gaussian splatting
2403.12198v1;http://arxiv.org/abs/2403.12198v1;2024-03-18;FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo  Endoscopic Videos;"Reconstruction of endoscopic scenes is an important asset for various medical
applications, from post-surgery analysis to educational training. Neural
rendering has recently shown promising results in endoscopic reconstruction
with deforming tissue. However, the setup has been restricted to a static
endoscope, limited deformation, or required an external tracking device to
retrieve camera pose information of the endoscopic camera. With FLex we adress
the challenging setup of a moving endoscope within a highly dynamic environment
of deforming tissue. We propose an implicit scene separation into multiple
overlapping 4D neural radiance fields (NeRFs) and a progressive optimization
scheme jointly optimizing for reconstruction and camera poses from scratch.
This improves the ease-of-use and allows to scale reconstruction capabilities
in time to process surgical videos of 5,000 frames and more; an improvement of
more than ten times compared to the state of the art while being agnostic to
external tracking information. Extensive evaluations on the StereoMIS dataset
show that FLex significantly improves the quality of novel view synthesis while
maintaining competitive pose accuracy.";Florian Philipp Stilz<author:sep>Mert Asim Karaoglu<author:sep>Felix Tristram<author:sep>Nassir Navab<author:sep>Benjamin Busam<author:sep>Alexander Ladikos;http://arxiv.org/pdf/2403.12198v1;cs.CV;;nerf
2403.11460v1;http://arxiv.org/abs/2403.11460v1;2024-03-18;Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning;"In this work, we present Fed3DGS, a scalable 3D reconstruction framework
based on 3D Gaussian splatting (3DGS) with federated learning. Existing
city-scale reconstruction methods typically adopt a centralized approach, which
gathers all data in a central server and reconstructs scenes. The approach
hampers scalability because it places a heavy load on the server and demands
extensive data storage when reconstructing scenes on a scale beyond city-scale.
In pursuit of a more scalable 3D reconstruction, we propose a federated
learning framework with 3DGS, which is a decentralized framework and can
potentially use distributed computational resources across millions of clients.
We tailor a distillation-based model update scheme for 3DGS and introduce
appearance modeling for handling non-IID data in the scenario of 3D
reconstruction with federated learning. We simulate our method on several
large-scale benchmarks, and our method demonstrates rendered image quality
comparable to centralized approaches. In addition, we also simulate our method
with data collected in different seasons, demonstrating that our framework can
reflect changes in the scenes and our appearance modeling captures changes due
to seasonal variations.";Teppei Suzuki;http://arxiv.org/pdf/2403.11460v1;cs.CV;Code: https://github.com/DensoITLab/Fed3DGS;gaussian splatting
2403.11812v1;http://arxiv.org/abs/2403.11812v1;2024-03-18;Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from  Aerial Imagery;"We present a neural radiance field method for urban-scale semantic and
building-level instance segmentation from aerial images by lifting noisy 2D
labels to 3D. This is a challenging problem due to two primary reasons.
Firstly, objects in urban aerial images exhibit substantial variations in size,
including buildings, cars, and roads, which pose a significant challenge for
accurate 2D segmentation. Secondly, the 2D labels generated by existing
segmentation methods suffer from the multi-view inconsistency problem,
especially in the case of aerial images, where each image captures only a small
portion of the entire scene. To overcome these limitations, we first introduce
a scale-adaptive semantic label fusion strategy that enhances the segmentation
of objects of varying sizes by combining labels predicted from different
altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then
introduce a novel cross-view instance label grouping strategy based on the 3D
scene representation to mitigate the multi-view inconsistency problem in the 2D
instance labels. Furthermore, we exploit multi-view reconstructed depth priors
to improve the geometric quality of the reconstructed radiance field, resulting
in enhanced segmentation results. Experiments on multiple real-world
urban-scale datasets demonstrate that our approach outperforms existing
methods, highlighting its effectiveness.";Yuqi Zhang<author:sep>Guanying Chen<author:sep>Jiaxing Chen<author:sep>Shuguang Cui;http://arxiv.org/pdf/2403.11812v1;cs.CV;CVPR 2024: https://zyqz97.github.io/Aerial_Lifting/;nerf
2403.11909v1;http://arxiv.org/abs/2403.11909v1;2024-03-18;RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF;"Recent advances in neural rendering have enabled highly photorealistic 3D
scene reconstruction and novel view synthesis. Despite this progress, current
state-of-the-art methods struggle to reconstruct high frequency detail, due to
factors such as a low-frequency bias of radiance fields and inaccurate camera
calibration. One approach to mitigate this issue is to enhance images
post-rendering. 2D enhancers can be pre-trained to recover some detail but are
agnostic to scene geometry and do not easily generalize to new distributions of
image degradation. Conversely, existing 3D enhancers are able to transfer
detail from nearby training images in a generalizable manner, but suffer from
inaccurate camera calibration and can propagate errors from the geometry into
rendered images. We propose a neural rendering enhancer, RoGUENeRF, which
exploits the best of both paradigms. Our method is pre-trained to learn a
general enhancer while also leveraging information from nearby training images
via robust 3D alignment and geometry-aware fusion. Our approach restores
high-frequency textures while maintaining geometric consistency and is also
robust to inaccurate camera calibration. We show that RoGUENeRF substantially
enhances the rendering quality of a wide range of neural rendering baselines,
e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the
real world 360v2 dataset.";Sibi Catley-Chandar<author:sep>Richard Shaw<author:sep>Gregory Slabaugh<author:sep>Eduardo Perez-Pellitero;http://arxiv.org/pdf/2403.11909v1;cs.CV;;nerf
2403.11868v2;http://arxiv.org/abs/2403.11868v2;2024-03-18;View-Consistent 3D Editing with Gaussian Splatting;"The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,
offering efficient, high-fidelity rendering and enabling precise local
manipulations. Currently, diffusion-based 2D editing models are harnessed to
modify multi-view rendered images, which then guide the editing of 3DGS models.
However, this approach faces a critical issue of multi-view inconsistency,
where the guidance images exhibit significant discrepancies across views,
leading to mode collapse and visual artifacts of 3DGS. To this end, we
introduce View-consistent Editing (VcEdit), a novel framework that seamlessly
incorporates 3DGS into image editing processes, ensuring multi-view consistency
in edited guidance images and effectively mitigating mode collapse issues.
VcEdit employs two innovative consistency modules: the Cross-attention
Consistency Module and the Editing Consistency Module, both designed to reduce
inconsistencies in edited images. By incorporating these consistency modules
into an iterative pattern, VcEdit proficiently resolves the issue of multi-view
inconsistency, facilitating high-quality 3DGS editing across a diverse range of
scenes.";Yuxuan Wang<author:sep>Xuanyu Yi<author:sep>Zike Wu<author:sep>Na Zhao<author:sep>Long Chen<author:sep>Hanwang Zhang;http://arxiv.org/pdf/2403.11868v2;cs.GR;;gaussian splatting
2403.11865v1;http://arxiv.org/abs/2403.11865v1;2024-03-18;Exploring Multi-modal Neural Scene Representations With Applications on  Thermal Imaging;"Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard
for the task of novel view synthesis when trained on a set of RGB images. In
this paper, we conduct a comprehensive evaluation of neural scene
representations, such as NeRFs, in the context of multi-modal learning.
Specifically, we present four different strategies of how to incorporate a
second modality, other than RGB, into NeRFs: (1) training from scratch
independently on both modalities; (2) pre-training on RGB and fine-tuning on
the second modality; (3) adding a second branch; and (4) adding a separate
component to predict (color) values of the additional modality. We chose
thermal imaging as second modality since it strongly differs from RGB in terms
of radiosity, making it challenging to integrate into neural scene
representations. For the evaluation of the proposed strategies, we captured a
new publicly available multi-view dataset, ThermalMix, consisting of six common
objects and about 360 RGB and thermal images in total. We employ cross-modality
calibration prior to data capturing, leading to high-quality alignments between
RGB and thermal images. Our findings reveal that adding a second branch to NeRF
performs best for novel view synthesis on thermal images while also yielding
compelling results on RGB. Finally, we also show that our analysis generalizes
to other modalities, including near-infrared images and depth maps. Project
page: https://mert-o.github.io/ThermalNeRF/.";Mert Ãzer<author:sep>Maximilian Weiherer<author:sep>Martin Hundhausen<author:sep>Bernhard Egger;http://arxiv.org/pdf/2403.11865v1;cs.CV;24 pages, 14 figures;nerf
2403.11364v1;http://arxiv.org/abs/2403.11364v1;2024-03-17;Creating Seamless 3D Maps Using Radiance Fields;"It is desirable to create 3D object models and 3D maps from 2D input images
for applications such as navigation, virtual tourism, and urban planning. The
traditional methods of creating 3D maps, (such as photogrammetry), require a
large number of images and odometry. Additionally, traditional methods have
difficulty with reflective surfaces and specular reflections; windows and
chrome in the scene can be problematic. Google Road View is a familiar
application, which uses traditional methods to fuse a collection of 2D input
images into the illusion of a 3D map. However, Google Road View does not create
an actual 3D object model, only a collection of views. The objective of this
work is to create an actual 3D object model using updated techniques. Neural
Radiance Fields (NeRF[1]) has emerged as a potential solution, offering the
capability to produce more precise and intricate 3D maps. Gaussian Splatting[4]
is another contemporary technique. This investigation compares Neural Radiance
Fields to Gaussian Splatting, and describes some of their inner workings. Our
primary contribution is a method for improving the results of the 3D
reconstructed models. Our results indicate that Gaussian Splatting was superior
to the NeRF technique.";Sai Tarun Sathyan<author:sep>Thomas B. Kinsman;http://arxiv.org/pdf/2403.11364v1;cs.CV;10 pages with figures;gaussian splatting<tag:sep>nerf
2403.11367v1;http://arxiv.org/abs/2403.11367v1;2024-03-17;3DGS-ReLoc: 3D Gaussian Splatting for Map Representation and Visual  ReLocalization;"This paper presents a novel system designed for 3D mapping and visual
relocalization using 3D Gaussian Splatting. Our proposed method uses LiDAR and
camera data to create accurate and visually plausible representations of the
environment. By leveraging LiDAR data to initiate the training of the 3D
Gaussian Splatting map, our system constructs maps that are both detailed and
geometrically accurate. To mitigate excessive GPU memory usage and facilitate
rapid spatial queries, we employ a combination of a 2D voxel map and a KD-tree.
This preparation makes our method well-suited for visual localization tasks,
enabling efficient identification of correspondences between the query image
and the rendered image from the Gaussian Splatting map via normalized
cross-correlation (NCC). Additionally, we refine the camera pose of the query
image using feature-based matching and the Perspective-n-Point (PnP) technique.
The effectiveness, adaptability, and precision of our system are demonstrated
through extensive evaluation on the KITTI360 dataset.";Peng Jiang<author:sep>Gaurav Pandey<author:sep>Srikanth Saripalli;http://arxiv.org/pdf/2403.11367v1;cs.CV;8 pages, 7 figures;gaussian splatting
2403.10119v1;http://arxiv.org/abs/2403.10119v1;2024-03-15;URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural  Radiance Fields;"We propose a novel rolling shutter bundle adjustment method for neural
radiance fields (NeRF), which utilizes the unordered rolling shutter (RS)
images to obtain the implicit 3D representation. Existing NeRF methods suffer
from low-quality images and inaccurate initial camera poses due to the RS
effect in the image, whereas, the previous method that incorporates the RS into
NeRF requires strict sequential data input, limiting its widespread
applicability. In constant, our method recovers the physical formation of RS
images by estimating camera poses and velocities, thereby removing the input
constraints on sequential data. Moreover, we adopt a coarse-to-fine training
strategy, in which the RS epipolar constraints of the pairwise frames in the
scene graph are used to detect the camera poses that fall into local minima.
The poses detected as outliers are corrected by the interpolation method with
neighboring poses. The experimental results validate the effectiveness of our
method over state-of-the-art works and demonstrate that the reconstruction of
3D representations is not constrained by the requirement of video sequence
input.";Bo Xu<author:sep>Ziao Liu<author:sep>Mengqi Guo<author:sep>Jiancheng Li<author:sep>Gim Hee Li;http://arxiv.org/pdf/2403.10119v1;cs.CV;;nerf
2403.10103v1;http://arxiv.org/abs/2403.10103v1;2024-03-15;DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video;"Recent advancements in dynamic neural radiance field methods have yielded
remarkable outcomes. However, these approaches rely on the assumption of sharp
input images. When faced with motion blur, existing dynamic NeRF methods often
struggle to generate high-quality novel views. In this paper, we propose
DyBluRF, a dynamic radiance field approach that synthesizes sharp novel views
from a monocular video affected by motion blur. To account for motion blur in
input images, we simultaneously capture the camera trajectory and object
Discrete Cosine Transform (DCT) trajectories within the scene. Additionally, we
employ a global cross-time rendering approach to ensure consistent temporal
coherence across the entire scene. We curate a dataset comprising diverse
dynamic scenes that are specifically tailored for our task. Experimental
results on our dataset demonstrate that our method outperforms existing
approaches in generating sharp novel views from motion-blurred inputs while
maintaining spatial-temporal consistency of the scene.";Huiqiang Sun<author:sep>Xingyi Li<author:sep>Liao Shen<author:sep>Xinyi Ye<author:sep>Ke Xian<author:sep>Zhiguo Cao;http://arxiv.org/pdf/2403.10103v1;cs.CV;;nerf
2403.10516v1;http://arxiv.org/abs/2403.10516v1;2024-03-15;FeatUp: A Model-Agnostic Framework for Features at Any Resolution;"Deep features are a cornerstone of computer vision research, capturing image
semantics and enabling the community to solve downstream tasks even in the
zero- or few-shot regime. However, these features often lack the spatial
resolution to directly perform dense prediction tasks like segmentation and
depth prediction because models aggressively pool information over large areas.
In this work, we introduce FeatUp, a task- and model-agnostic framework to
restore lost spatial information in deep features. We introduce two variants of
FeatUp: one that guides features with high-resolution signal in a single
forward pass, and one that fits an implicit model to a single image to
reconstruct features at any resolution. Both approaches use a multi-view
consistency loss with deep analogies to NeRFs. Our features retain their
original semantics and can be swapped into existing applications to yield
resolution and performance gains even without re-training. We show that FeatUp
significantly outperforms other feature upsampling and image super-resolution
approaches in class activation map generation, transfer learning for
segmentation and depth prediction, and end-to-end training for semantic
segmentation.";Stephanie Fu<author:sep>Mark Hamilton<author:sep>Laura Brandt<author:sep>Axel Feldman<author:sep>Zhoutong Zhang<author:sep>William T. Freeman;http://arxiv.org/pdf/2403.10516v1;cs.CV;"Accepted to the International Conference on Learning Representations
  (ICLR) 2024";nerf
2403.09981v1;http://arxiv.org/abs/2403.09981v1;2024-03-15;Controllable Text-to-3D Generation via Surface-Aligned Gaussian  Splatting;"While text-to-3D and image-to-3D generation tasks have received considerable
attention, one important but under-explored field between them is controllable
text-to-3D generation, which we mainly focus on in this work. To address this
task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network
architecture designed to enhance existing pre-trained multi-view diffusion
models by integrating additional input conditions, such as edge, depth, normal,
and scribble maps. Our innovation lies in the introduction of a conditioning
module that controls the base diffusion model using both local and global
embeddings, which are computed from the input condition images and camera
poses. Once trained, MVControl is able to offer 3D diffusion guidance for
optimization-based 3D generation. And, 2) we propose an efficient multi-stage
3D generation pipeline that leverages the benefits of recent large
reconstruction models and score distillation algorithm. Building upon our
MVControl architecture, we employ a unique hybrid diffusion guidance method to
direct the optimization process. In pursuit of efficiency, we adopt 3D
Gaussians as our representation instead of the commonly used implicit
representations. We also pioneer the use of SuGaR, a hybrid representation that
binds Gaussians to mesh triangle faces. This approach alleviates the issue of
poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained
geometry on the mesh. Extensive experiments demonstrate that our method
achieves robust generalization and enables the controllable generation of
high-quality 3D content.";Zhiqi Li<author:sep>Yiming Chen<author:sep>Lingzhe Zhao<author:sep>Peidong Liu;http://arxiv.org/pdf/2403.09981v1;cs.CV;Project page: https://lizhiqi49.github.io/MVControl/;
2403.10427v1;http://arxiv.org/abs/2403.10427v1;2024-03-15;SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians;"Implicit neural representation methods have shown impressive advancements in
learning 3D scenes from unstructured in-the-wild photo collections but are
still limited by the large computational cost of volumetric rendering. More
recently, 3D Gaussian Splatting emerged as a much faster alternative with
superior rendering quality and training efficiency, especially for small-scale
and object-centric scenarios. Nevertheless, this technique suffers from poor
performance on unstructured in-the-wild data. To tackle this, we extend over 3D
Gaussian Splatting to handle unstructured image collections. We achieve this by
modeling appearance to seize photometric variations in the rendered images.
Additionally, we introduce a new mechanism to train transient Gaussians to
handle the presence of scene occluders in an unsupervised manner. Experiments
on diverse photo collection scenes and multi-pass acquisition of outdoor
landmarks show the effectiveness of our method over prior works achieving
state-of-the-art results with improved efficiency.";Hiba Dahmani<author:sep>Moussab Bennehar<author:sep>Nathan Piasco<author:sep>Luis Roldao<author:sep>Dzmitry Tsishkou;http://arxiv.org/pdf/2403.10427v1;cs.CV;;gaussian splatting
2403.10297v1;http://arxiv.org/abs/2403.10297v1;2024-03-15;Leveraging Neural Radiance Field in Descriptor Synthesis for Keypoints  Scene Coordinate Regression;"Classical structural-based visual localization methods offer high accuracy
but face trade-offs in terms of storage, speed, and privacy. A recent
innovation, keypoint scene coordinate regression (KSCR) named D2S addresses
these issues by leveraging graph attention networks to enhance keypoint
relationships and predict their 3D coordinates using a simple multilayer
perceptron (MLP). Camera pose is then determined via PnP+RANSAC, using
established 2D-3D correspondences. While KSCR achieves competitive results,
rivaling state-of-the-art image-retrieval methods like HLoc across multiple
benchmarks, its performance is hindered when data samples are limited due to
the deep learning model's reliance on extensive data. This paper proposes a
solution to this challenge by introducing a pipeline for keypoint descriptor
synthesis using Neural Radiance Field (NeRF). By generating novel poses and
feeding them into a trained NeRF model to create new views, our approach
enhances the KSCR's generalization capabilities in data-scarce environments.
The proposed system could significantly improve localization accuracy by up to
50\% and cost only a fraction of time for data synthesis. Furthermore, its
modular design allows for the integration of multiple NeRFs, offering a
versatile and efficient solution for visual localization. The implementation is
publicly available at: https://github.com/ais-lab/DescriptorSynthesis4Feat2Map.";Huy-Hoang Bui<author:sep>Bach-Thuan Bui<author:sep>Dinh-Tuan Tran<author:sep>Joo-Ho Lee;http://arxiv.org/pdf/2403.10297v1;cs.CV;;nerf
2403.10147v1;http://arxiv.org/abs/2403.10147v1;2024-03-15;GGRt: Towards Generalizable 3D Gaussians without Pose Priors in  Real-Time;"This paper presents GGRt, a novel approach to generalizable novel view
synthesis that alleviates the need for real camera poses, complexity in
processing high-resolution images, and lengthy optimization processes, thus
facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in
real-world scenarios. Specifically, we design a novel joint learning framework
that consists of an Iterative Pose Optimization Network (IPO-Net) and a
Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism,
the proposed framework can inherently estimate robust relative pose information
from the image observations and thus primarily alleviate the requirement of
real camera poses. Moreover, we implement a deferred back-propagation mechanism
that enables high-resolution training and inference, overcoming the resolution
constraints of previous methods. To enhance the speed and efficiency, we
further introduce a progressive Gaussian cache module that dynamically adjusts
during training and inference. As the first pose-free generalizable 3D-GS
framework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at
$\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our
method outperforms existing NeRF-based pose-free techniques in terms of
inference speed and effectiveness. It can also approach the real pose-based
3D-GS methods. Our contributions provide a significant leap forward for the
integration of computer vision and computer graphics into practical
applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open
datasets and enabling real-time rendering for immersive experiences.";Hao Li<author:sep>Yuanyuan Gao<author:sep>Dingwen Zhang<author:sep>Chenming Wu<author:sep>Yalun Dai<author:sep>Chen Zhao<author:sep>Haocheng Feng<author:sep>Errui Ding<author:sep>Jingdong Wang<author:sep>Junwei Han;http://arxiv.org/pdf/2403.10147v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.10242v1;http://arxiv.org/abs/2403.10242v1;2024-03-15;FDGaussian: Fast Gaussian Splatting from Single Image via  Geometric-aware Diffusion Model;"Reconstructing detailed 3D objects from single-view images remains a
challenging task due to the limited information available. In this paper, we
introduce FDGaussian, a novel two-stage framework for single-image 3D
reconstruction. Recent methods typically utilize pre-trained 2D diffusion
models to generate plausible novel views from the input image, yet they
encounter issues with either multi-view inconsistency or lack of geometric
fidelity. To overcome these challenges, we propose an orthogonal plane
decomposition mechanism to extract 3D geometric features from the 2D input,
enabling the generation of consistent multi-view images. Moreover, we further
accelerate the state-of-the-art Gaussian Splatting incorporating epipolar
attention to fuse images from different viewpoints. We demonstrate that
FDGaussian generates images with high consistency across different views and
reconstructs high-quality 3D objects, both qualitatively and quantitatively.
More examples can be found at our website https://qjfeng.net/FDGaussian/.";Qijun Feng<author:sep>Zhen Xing<author:sep>Zuxuan Wu<author:sep>Yu-Gang Jiang;http://arxiv.org/pdf/2403.10242v1;cs.CV;;gaussian splatting
2403.10340v1;http://arxiv.org/abs/2403.10340v1;2024-03-15;Thermal-NeRF: Neural Radiance Fields from an Infrared Camera;"In recent years, Neural Radiance Fields (NeRFs) have demonstrated significant
potential in encoding highly-detailed 3D geometry and environmental appearance,
positioning themselves as a promising alternative to traditional explicit
representation for 3D scene reconstruction. However, the predominant reliance
on RGB imaging presupposes ideal lighting conditions: a premise frequently
unmet in robotic applications plagued by poor lighting or visual obstructions.
This limitation overlooks the capabilities of infrared (IR) cameras, which
excel in low-light detection and present a robust alternative under such
adverse scenarios. To tackle these issues, we introduce Thermal-NeRF, the first
method that estimates a volumetric scene representation in the form of a NeRF
solely from IR imaging. By leveraging a thermal mapping and structural thermal
constraint derived from the thermal characteristics of IR imaging, our method
showcasing unparalleled proficiency in recovering NeRFs in visually degraded
scenes where RGB-based methods fall short. We conduct extensive experiments to
demonstrate that Thermal-NeRF can achieve superior quality compared to existing
methods. Furthermore, we contribute a dataset for IR-based NeRF applications,
paving the way for future research in IR NeRF reconstruction.";Tianxiang Ye<author:sep>Qi Wu<author:sep>Junyuan Deng<author:sep>Guoqing Liu<author:sep>Liu Liu<author:sep>Songpengcheng Xia<author:sep>Liang Pang<author:sep>Wenxian Yu<author:sep>Ling Pei;http://arxiv.org/pdf/2403.10340v1;cs.CV;;nerf
2403.10050v1;http://arxiv.org/abs/2403.10050v1;2024-03-15;Texture-GS: Disentangling the Geometry and Texture for 3D Gaussian  Splatting Editing;"3D Gaussian splatting, emerging as a groundbreaking approach, has drawn
increasing attention for its capabilities of high-fidelity reconstruction and
real-time rendering. However, it couples the appearance and geometry of the
scene within the Gaussian attributes, which hinders the flexibility of editing
operations, such as texture swapping. To address this issue, we propose a novel
approach, namely Texture-GS, to disentangle the appearance from the geometry by
representing it as a 2D texture mapped onto the 3D surface, thereby
facilitating appearance editing. Technically, the disentanglement is achieved
by our proposed texture mapping module, which consists of a UV mapping MLP to
learn the UV coordinates for the 3D Gaussian centers, a local Taylor expansion
of the MLP to efficiently approximate the UV coordinates for the ray-Gaussian
intersections, and a learnable texture to capture the fine-grained appearance.
Extensive experiments on the DTU dataset demonstrate that our method not only
facilitates high-fidelity appearance editing but also achieves real-time
rendering on consumer-level devices, e.g. a single RTX 2080 Ti GPU.";Tian-Xing Xu<author:sep>Wenbo Hu<author:sep>Yu-Kun Lai<author:sep>Ying Shan<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2403.10050v1;cs.CV;;gaussian splatting
2403.09973v1;http://arxiv.org/abs/2403.09973v1;2024-03-15;Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive  Experience;"We have built a custom mobile multi-camera large-space dense light field
capture system, which provides a series of high-quality and sufficiently dense
light field images for various scenarios. Our aim is to contribute to the
development of popular 3D scene reconstruction algorithms such as IBRnet, NeRF,
and 3D Gaussian splitting. More importantly, the collected dataset, which is
much denser than existing datasets, may also inspire space-oriented light field
reconstruction, which is potentially different from object-centric 3D
reconstruction, for immersive VR/AR experiences. We utilized a total of 40
GoPro 10 cameras, capturing images of 5k resolution. The number of photos
captured for each scene is no less than 1000, and the average density (view
number within a unit sphere) is 134.68. It is also worth noting that our system
is capable of efficiently capturing large outdoor scenes. Addressing the
current lack of large-space and dense light field datasets, we made efforts to
include elements such as sky, reflections, lights and shadows that are of
interest to researchers in the field of 3D reconstruction during the data
capture process. Finally, we validated the effectiveness of our provided
dataset on three popular algorithms and also integrated the reconstructed 3DGS
results into the Unity engine, demonstrating the potential of utilizing our
datasets to enhance the realism of virtual reality (VR) and create feasible
interactive spaces. The dataset is available at our project website.";Xiaohang Yu<author:sep>Zhengxian Yang<author:sep>Shi Pan<author:sep>Yuqi Han<author:sep>Haoxiang Wang<author:sep>Jun Zhang<author:sep>Shi Yan<author:sep>Borong Lin<author:sep>Lei Yang<author:sep>Tao Yu<author:sep>Lu Fang;http://arxiv.org/pdf/2403.09973v1;cs.CV;;nerf
2403.09875v1;http://arxiv.org/abs/2403.09875v1;2024-03-14;Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting;"In this work, we propose a novel method to supervise 3D Gaussian Splatting
(3DGS) scenes using optical tactile sensors. Optical tactile sensors have
become widespread in their use in robotics for manipulation and object
representation; however, raw optical tactile sensor data is unsuitable to
directly supervise a 3DGS scene. Our representation leverages a Gaussian
Process Implicit Surface to implicitly represent the object, combining many
touches into a unified representation with uncertainty. We merge this model
with a monocular depth estimation network, which is aligned in a two stage
process, coarsely aligning with a depth camera and then finely adjusting to
match our touch data. For every training image, our method produces a
corresponding fused depth and uncertainty map. Utilizing this additional
information, we propose a new loss function, variance weighted depth supervised
loss, for training the 3DGS scene model. We leverage the DenseTact optical
tactile sensor and RealSense RGB-D camera to show that combining touch and
vision in this manner leads to quantitatively and qualitatively better results
than vision or touch alone in a few-view scene syntheses on opaque as well as
on reflective and transparent objects. Please see our project page at
http://armlabstanford.github.io/touch-gs";Aiden Swann<author:sep>Matthew Strong<author:sep>Won Kyung Do<author:sep>Gadiel Sznaier Camps<author:sep>Mac Schwager<author:sep>Monroe Kennedy III;http://arxiv.org/pdf/2403.09875v1;cs.RO;;gaussian splatting
2403.09143v1;http://arxiv.org/abs/2403.09143v1;2024-03-14;A New Split Algorithm for 3D Gaussian Splatting;"3D Gaussian splatting models, as a novel explicit 3D representation, have
been applied in many domains recently, such as explicit geometric editing and
geometry generation. Progress has been rapid. However, due to their mixed
scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred
or needle-like effect near the surface. At the same time, 3D Gaussian splatting
models tend to flatten large untextured regions, yielding a very sparse point
cloud. These problems are caused by the non-uniform nature of 3D Gaussian
splatting models, so in this paper, we propose a new 3D Gaussian splitting
algorithm, which can produce a more uniform and surface-bounded 3D Gaussian
splatting model. Our algorithm splits an $N$-dimensional Gaussian into two
N-dimensional Gaussians. It ensures consistency of mathematical characteristics
and similarity of appearance, allowing resulting 3D Gaussian splatting models
to be more uniform and a better fit to the underlying surface, and thus more
suitable for explicit editing, point cloud extraction and other tasks.
Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form
solution, making it readily applicable to any 3D Gaussian model.";Qiyuan Feng<author:sep>Gengchen Cao<author:sep>Haoxiang Chen<author:sep>Tai-Jiang Mu<author:sep>Ralph R. Martin<author:sep>Shi-Min Hu;http://arxiv.org/pdf/2403.09143v1;cs.GR;11 pages, 10 figures;gaussian splatting
2403.09439v1;http://arxiv.org/abs/2403.09439v1;2024-03-14;3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation;"Text-driven 3D scene generation techniques have made rapid progress in recent
years. Their success is mainly attributed to using existing generative models
to iteratively perform image warping and inpainting to generate 3D scenes.
However, these methods heavily rely on the outputs of existing models, leading
to error accumulation in geometry and appearance that prevent the models from
being used in various scenarios (e.g., outdoor and unreal scenarios). To
address this limitation, we generatively refine the newly generated local views
by querying and aggregating global 3D information, and then progressively
generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF
as a unified representation of the 3D scene to constrain global 3D consistency,
and propose a generative refinement network to synthesize new contents with
higher quality by exploiting the natural image prior from 2D diffusion model as
well as the global 3D information of the current scene. Our extensive
experiments demonstrate that, in comparison to previous methods, our approach
supports wide variety of scene generation and arbitrary camera trajectories
with improved visual quality and 3D consistency.";Frank Zhang<author:sep>Yibo Zhang<author:sep>Quan Zheng<author:sep>Rui Ma<author:sep>Wei Hua<author:sep>Hujun Bao<author:sep>Weiwei Xu<author:sep>Changqing Zou;http://arxiv.org/pdf/2403.09439v1;cs.CV;11 pages, 7 figures;nerf
2403.09419v1;http://arxiv.org/abs/2403.09419v1;2024-03-14;RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban  Scenes;"The task of separating dynamic objects from static environments using NeRFs
has been widely studied in recent years. However, capturing large-scale scenes
still poses a challenge due to their complex geometric structures and
unconstrained dynamics. Without the help of 3D motion cues, previous methods
often require simplified setups with slow camera motion and only a few/single
dynamic actors, leading to suboptimal solutions in most urban setups. To
overcome such limitations, we present RoDUS, a pipeline for decomposing static
and dynamic elements in urban scenes, with thoughtfully separated NeRF models
for moving and non-moving components. Our approach utilizes a robust
kernel-based initialization coupled with 4D semantic information to selectively
guide the learning process. This strategy enables accurate capturing of the
dynamics in the scene, resulting in reduced artifacts caused by NeRF on
background reconstruction, all by using self-supervision. Notably, experimental
evaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of
our method in decomposing challenging urban scenes into precise static and
dynamic components.";Thang-Anh-Quan Nguyen<author:sep>Luis RoldÃ£o<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Dzmitry Tsishkou;http://arxiv.org/pdf/2403.09419v1;cs.CV;;nerf
2403.09577v1;http://arxiv.org/abs/2403.09577v1;2024-03-14;The NeRFect Match: Exploring NeRF Features for Visual Localization;"In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene
representation for visual localization. Recently, NeRF has been employed to
enhance pose regression and scene coordinate regression models by augmenting
the training database, providing auxiliary supervision through rendered images,
or serving as an iterative refinement module. We extend its recognized
advantages -- its ability to provide a compact scene representation with
realistic appearances and accurate geometry -- by exploring the potential of
NeRF's internal features in establishing precise 2D-3D matches for
localization. To this end, we conduct a comprehensive examination of NeRF's
implicit knowledge, acquired through view synthesis, for matching under various
conditions. This includes exploring different matching network architectures,
extracting encoder features at multiple layers, and varying training
configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D
matching function that capitalizes on the internal knowledge of NeRF learned
via view synthesis. Our evaluation of NeRFMatch on standard localization
benchmarks, within a structure-based pipeline, sets a new state-of-the-art for
localization performance on Cambridge Landmarks.";Qunjie Zhou<author:sep>Maxim Maximov<author:sep>Or Litany<author:sep>Laura Leal-TaixÃ©;http://arxiv.org/pdf/2403.09577v1;cs.CV;;nerf
2403.09477v1;http://arxiv.org/abs/2403.09477v1;2024-03-14;VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance  Fields;"Autonomous mobile robots are an increasingly integral part of modern factory
and warehouse operations. Obstacle detection, avoidance and path planning are
critical safety-relevant tasks, which are often solved using expensive LiDAR
sensors and depth cameras. We propose to use cost-effective low-resolution
ranging sensors, such as ultrasonic and infrared time-of-flight sensors by
developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance
Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution
Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from
ultrasonic and infrared sensors and utilizes them to update the occupancy grid
used for ray marching. Experimental evaluation in 2D demonstrates that
VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds
regarding coverage. Notably, in small environments, its accuracy aligns with
that of LiDAR measurements, while in larger ones, it is bounded by the utilized
ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic
and infrared sensors is highly effective when dealing with sparse data and low
view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the
mapping capabilities and increases the training speed by 46% compared to
Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for
cost-effective local mapping in mobile robotics, with potential applications in
safety and navigation tasks. The code can be found at
https://github.com/ethz-asl/virus nerf.";Nicolaj Schmid<author:sep>Cornelius von Einem<author:sep>Cesar Cadena<author:sep>Roland Siegwart<author:sep>Lorenz Hruby<author:sep>Florian Tschopp;http://arxiv.org/pdf/2403.09477v1;cs.RO;;nerf
2403.09079v1;http://arxiv.org/abs/2403.09079v1;2024-03-14;PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF  Priors;"Autonomous vehicles rely extensively on perception systems to navigate and
interpret their surroundings. Despite significant advancements in these systems
recently, challenges persist under conditions like occlusion, extreme lighting,
or in unfamiliar urban areas. Unlike these systems, humans do not solely depend
on immediate observations to perceive the environment. In navigating new
cities, humans gradually develop a preliminary mental map to supplement
real-time perception during subsequent visits. Inspired by this human approach,
we introduce a novel framework, Pre-Sight, that leverages past traversals to
construct static prior memories, enhancing online perception in later
navigations. Our method involves optimizing a city-scale neural radiance field
with data from previous journeys to generate neural priors. These priors, rich
in semantic and geometric details, are derived without manual annotations and
can seamlessly augment various state-of-the-art perception models, improving
their efficacy with minimal additional computational cost. Experimental results
on the nuScenes dataset demonstrate the framework's high compatibility with
diverse online perception models. Specifically, it shows remarkable
improvements in HD-map construction and occupancy prediction tasks,
highlighting its potential as a new perception framework for autonomous driving
systems. Our code will be released at
https://github.com/yuantianyuan01/PreSight.";Tianyuan Yuan<author:sep>Yucheng Mao<author:sep>Jiawei Yang<author:sep>Yicheng Liu<author:sep>Yue Wang<author:sep>Hang Zhao;http://arxiv.org/pdf/2403.09079v1;cs.CV;;nerf
2403.09637v1;http://arxiv.org/abs/2403.09637v1;2024-03-14;GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary  Robotic Grasping;"Constructing a 3D scene capable of accommodating open-ended language queries,
is a pivotal pursuit, particularly within the domain of robotics. Such
technology facilitates robots in executing object manipulations based on human
language directives. To tackle this challenge, some research efforts have been
dedicated to the development of language-embedded implicit fields. However,
implicit fields (e.g. NeRF) encounter limitations due to the necessity of
processing a large number of input views for reconstruction, coupled with their
inherent inefficiencies in inference. Thus, we present the GaussianGrasper,
which utilizes 3D Gaussian Splatting to explicitly represent the scene as a
collection of Gaussian primitives. Our approach takes a limited set of RGB-D
views and employs a tile-based splatting technique to create a feature field.
In particular, we propose an Efficient Feature Distillation (EFD) module that
employs contrastive learning to efficiently and accurately distill language
embeddings derived from foundational models. With the reconstructed geometry of
the Gaussian field, our method enables the pre-trained grasping model to
generate collision-free grasp pose candidates. Furthermore, we propose a
normal-guided grasp module to select the best grasp pose. Through comprehensive
real-world experiments, we demonstrate that GaussianGrasper enables robots to
accurately query and grasp objects with language instructions, providing a new
solution for language-guided manipulation tasks. Data and codes can be
available at https://github.com/MrSecant/GaussianGrasper.";Yuhang Zheng<author:sep>Xiangyu Chen<author:sep>Yupeng Zheng<author:sep>Songen Gu<author:sep>Runyi Yang<author:sep>Bu Jin<author:sep>Pengfei Li<author:sep>Chengliang Zhong<author:sep>Zengmao Wang<author:sep>Lina Liu<author:sep>Chao Yang<author:sep>Dawei Wang<author:sep>Zhen Chen<author:sep>Xiaoxiao Long<author:sep>Meiqing Wang;http://arxiv.org/pdf/2403.09637v1;cs.RO;;gaussian splatting<tag:sep>nerf
2403.09413v1;http://arxiv.org/abs/2403.09413v1;2024-03-14;Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting;"3D Gaussian splatting (3DGS) has recently demonstrated impressive
capabilities in real-time novel view synthesis and 3D reconstruction. However,
3DGS heavily depends on the accurate initialization derived from
Structure-from-Motion (SfM) methods. When trained with randomly initialized
point clouds, 3DGS fails to maintain its ability to produce high-quality
images, undergoing large performance drops of 4-5 dB in PSNR. Through extensive
analysis of SfM initialization in the frequency domain and analysis of a 1D
regression task with multiple 1D Gaussians, we propose a novel optimization
strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D
Gaussian Splatting), that successfully trains 3D Gaussians from random point
clouds. We show the effectiveness of our strategy through quantitative and
qualitative comparisons on multiple datasets, largely improving the performance
in all settings. Our project page and code can be found at
https://ku-cvlab.github.io/RAIN-GS.";Jaewoo Jung<author:sep>Jisang Han<author:sep>Honggyu An<author:sep>Jiwon Kang<author:sep>Seonghoon Park<author:sep>Seungryong Kim;http://arxiv.org/pdf/2403.09413v1;cs.CV;Project Page: https://ku-cvlab.github.io/RAIN-GS;gaussian splatting
2403.08156v1;http://arxiv.org/abs/2403.08156v1;2024-03-13;NeRF-Supervised Feature Point Detection and Description;"Feature point detection and description is the backbone for various computer
vision applications, such as Structure-from-Motion, visual SLAM, and visual
place recognition. While learning-based methods have surpassed traditional
handcrafted techniques, their training often relies on simplistic
homography-based simulations of multi-view perspectives, limiting model
generalisability. This paper introduces a novel approach leveraging neural
radiance fields (NeRFs) for realistic multi-view training data generation. We
create a diverse multi-view dataset using NeRFs, consisting of indoor and
outdoor scenes. Our proposed methodology adapts state-of-the-art feature
detectors and descriptors to train on NeRF-synthesised views supervised by
perspective projective geometry. Our experiments demonstrate that the proposed
methods achieve competitive or superior performance on standard benchmarks for
relative pose estimation, point cloud registration, and homography estimation
while requiring significantly less training data compared to existing
approaches.";Ali Youssef<author:sep>Francisco Vasconcelos;http://arxiv.org/pdf/2403.08156v1;cs.CV;;nerf
2403.08310v1;http://arxiv.org/abs/2403.08310v1;2024-03-13;StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance  Fields;"4D style transfer aims at transferring arbitrary visual style to the
synthesized novel views of a dynamic 4D scene with varying viewpoints and
times. Existing efforts on 3D style transfer can effectively combine the visual
features of style images and neural radiance fields (NeRF) but fail to handle
the 4D dynamic scenes limited by the static scene assumption. Consequently, we
aim to handle the novel challenging problem of 4D style transfer for the first
time, which further requires the consistency of stylized results on dynamic
objects. In this paper, we introduce StyleDyRF, a method that represents the 4D
feature space by deforming a canonical feature volume and learns a linear style
transformation matrix on the feature volume in a data-driven fashion. To obtain
the canonical feature volume, the rays at each time step are deformed with the
geometric prior of a pre-trained dynamic NeRF to render the feature map under
the supervision of pre-trained visual encoders. With the content and style cues
in the canonical feature volume and the style image, we can learn the style
transformation matrix from their covariance matrices with lightweight neural
networks. The learned style transformation matrix can reflect a direct matching
of feature covariance from the content volume to the given style pattern, in
analogy with the optimization of the Gram matrix in traditional 2D neural style
transfer. The experimental results show that our method not only renders 4D
photorealistic style transfer results in a zero-shot manner but also
outperforms existing methods in terms of visual quality and consistency.";Hongbin Xu<author:sep>Weitao Chen<author:sep>Feng Xiao<author:sep>Baigui Sun<author:sep>Wenxiong Kang;http://arxiv.org/pdf/2403.08310v1;cs.CV;"In submission. The code and model are released at:
  https://github.com/ToughStoneX/StyleDyRF";nerf
2403.08733v2;http://arxiv.org/abs/2403.08733v2;2024-03-13;GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting  Editing;"We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed
by the 3D Gaussian Splatting (3DGS).
  Our method first renders a collection of images by using the 3DGS and edits
them by using a pre-trained 2D diffusion model (ControlNet) based on the input
prompt, which is then used to optimise the 3D model.
  Our key contribution is multi-view consistent editing, which enables editing
all images together instead of iteratively editing one image while updating the
3D model as in previous works.
  It leads to faster editing as well as higher visual quality.
  This is achieved by the two terms:
  (a) depth-conditioned editing that enforces geometric consistency across
multi-view images by leveraging naturally consistent depth maps.
  (b) attention-based latent code alignment that unifies the appearance of
edited images by conditioning their editing to several reference views through
self and cross-view attention between images' latent representations.
  Experiments demonstrate that our method achieves faster editing and better
visual results than previous state-of-the-art methods.";Jing Wu<author:sep>Jia-Wang Bian<author:sep>Xinghui Li<author:sep>Guangrun Wang<author:sep>Ian Reid<author:sep>Philip Torr<author:sep>Victor Adrian Prisacariu;http://arxiv.org/pdf/2403.08733v2;cs.CV;17 pages;gaussian splatting
2403.08321v1;http://arxiv.org/abs/2403.08321v1;2024-03-13;ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic  Manipulation;"Performing language-conditioned robotic manipulation tasks in unstructured
environments is highly demanded for general intelligent robots. Conventional
robotic manipulation methods usually learn semantic representation of the
observation for action prediction, which ignores the scene-level spatiotemporal
dynamics for human goal completion. In this paper, we propose a dynamic
Gaussian Splatting method named ManiGaussian for multi-task robotic
manipulation, which mines scene dynamics via future scene reconstruction.
Specifically, we first formulate the dynamic Gaussian Splatting framework that
infers the semantics propagation in the Gaussian embedding space, where the
semantic representation is leveraged to predict the optimal robot action. Then,
we build a Gaussian world model to parameterize the distribution in our dynamic
Gaussian Splatting framework, which provides informative supervision in the
interactive environment via future scene reconstruction. We evaluate our
ManiGaussian on 10 RLBench tasks with 166 variations, and the results
demonstrate our framework can outperform the state-of-the-art methods by 13.1\%
in average success rate.";Guanxing Lu<author:sep>Shiyi Zhang<author:sep>Ziwei Wang<author:sep>Changliu Liu<author:sep>Jiwen Lu<author:sep>Yansong Tang;http://arxiv.org/pdf/2403.08321v1;cs.RO;;gaussian splatting
2403.08498v1;http://arxiv.org/abs/2403.08498v1;2024-03-13;Gaussian Splatting in Style;"Scene stylization extends the work of neural style transfer to three spatial
dimensions. A vital challenge in this problem is to maintain the uniformity of
the stylized appearance across a multi-view setting. A vast majority of the
previous works achieve this by optimizing the scene with a specific style
image. In contrast, we propose a novel architecture trained on a collection of
style images, that at test time produces high quality stylized novel views. Our
work builds up on the framework of 3D Gaussian splatting. For a given scene, we
take the pretrained Gaussians and process them using a multi resolution hash
grid and a tiny MLP to obtain the conditional stylised views. The explicit
nature of 3D Gaussians give us inherent advantages over NeRF-based methods
including geometric consistency, along with having a fast training and
rendering regime. This enables our method to be useful for vast practical use
cases such as in augmented or virtual reality applications. Through our
experiments, we show our methods achieve state-of-the-art performance with
superior visual quality on various indoor and outdoor real-world data.";Abhishek Saroha<author:sep>Mariia Gladkova<author:sep>Cecilia Curreli<author:sep>Tarun Yenamandra<author:sep>Daniel Cremers;http://arxiv.org/pdf/2403.08498v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.08551v2;http://arxiv.org/abs/2403.08551v2;2024-03-13;GaussianImage: 1000 FPS Image Representation and Compression by 2D  Gaussian Splatting;"Implicit neural representations (INRs) recently achieved great success in
image representation and compression, offering high visual quality and fast
rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are
available. However, this requirement often hinders their use on low-end devices
with limited memory. In response, we propose a groundbreaking paradigm of image
representation and compression by 2D Gaussian Splatting, named GaussianImage.
We first introduce 2D Gaussian to represent the image, where each Gaussian has
8 parameters including position, covariance and color. Subsequently, we unveil
a novel rendering algorithm based on accumulated summation. Remarkably, our
method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster
fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation
performance, but also delivers a faster rendering speed of 1500-2000 FPS
regardless of parameter size. Furthermore, we integrate existing vector
quantization technique to build an image codec. Experimental results
demonstrate that our codec attains rate-distortion performance comparable to
compression-based INRs such as COIN and COIN++, while facilitating decoding
speeds of approximately 1000 FPS. Additionally, preliminary proof of concept
shows that our codec surpasses COIN and COIN++ in performance when using
partial bits-back coding.";Xinjie Zhang<author:sep>Xingtong Ge<author:sep>Tongda Xu<author:sep>Dailan He<author:sep>Yan Wang<author:sep>Hongwei Qin<author:sep>Guo Lu<author:sep>Jing Geng<author:sep>Jun Zhang;http://arxiv.org/pdf/2403.08551v2;eess.IV;;gaussian splatting
2403.07547v1;http://arxiv.org/abs/2403.07547v1;2024-03-12;SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields;"Neural radiance fields (NeRF) has attracted considerable attention for their
exceptional ability in synthesizing novel views with high fidelity. However,
the presence of motion blur, resulting from slight camera movements during
extended shutter exposures, poses a significant challenge, potentially
compromising the quality of the reconstructed 3D scenes. While recent studies
have addressed this issue, they do not consider the continuous dynamics of
camera movements during image acquisition, leading to inaccurate scene
reconstruction. Additionally, these methods are plagued by slow training and
rendering speed. To effectively handle these issues, we propose sequential
motion understanding radiance fields (SMURF), a novel approach that employs
neural ordinary differential equation (Neural-ODE) to model continuous camera
motion and leverages the explicit volumetric representation method for faster
training and robustness to motion-blurred input images. The core idea of the
SMURF is continuous motion blurring kernel (CMBK), a unique module designed to
model a continuous camera movements for processing blurry inputs. Our model,
rigorously evaluated against benchmark datasets, demonstrates state-of-the-art
performance both quantitatively and qualitatively.";Jungho Lee<author:sep>Dogyoon Lee<author:sep>Minhyeok Lee<author:sep>Donghyung Kim<author:sep>Sangyoun Lee;http://arxiv.org/pdf/2403.07547v1;cs.CV;"25 pages, 10 figures, Code is available at
  https://github.com/Jho-Yonsei/SMURF";nerf
2403.08125v1;http://arxiv.org/abs/2403.08125v1;2024-03-12;Q-SLAM: Quadric Representations for Monocular SLAM;"Monocular SLAM has long grappled with the challenge of accurately modeling 3D
geometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular
SLAM have shown promise, yet these methods typically focus on novel view
synthesis rather than precise 3D geometry modeling. This focus results in a
significant disconnect between NeRF applications, i.e., novel-view synthesis
and the requirements of SLAM. We identify that the gap results from the
volumetric representations used in NeRF, which are often dense and noisy. In
this study, we propose a novel approach that reimagines volumetric
representations through the lens of quadric forms. We posit that most scene
components can be effectively represented as quadric planes. Leveraging this
assumption, we reshape the volumetric representations with million of cubes by
several quadric planes, which leads to more accurate and efficient modeling of
3D scenes in SLAM contexts. Our method involves two key steps: First, we use
the quadric assumption to enhance coarse depth estimations obtained from
tracking modules, e.g., Droid-SLAM. This step alone significantly improves
depth estimation accuracy. Second, in the subsequent mapping phase, we diverge
from previous NeRF-based SLAM methods that distribute sampling points across
the entire volume space. Instead, we concentrate sampling points around quadric
planes and aggregate them using a novel quadric-decomposed Transformer.
Additionally, we introduce an end-to-end joint optimization strategy that
synchronizes pose estimation with 3D reconstruction.";Chensheng Peng<author:sep>Chenfeng Xu<author:sep>Yue Wang<author:sep>Mingyu Ding<author:sep>Heng Yang<author:sep>Masayoshi Tomizuka<author:sep>Kurt Keutzer<author:sep>Marco Pavone<author:sep>Wei Zhan;http://arxiv.org/pdf/2403.08125v1;cs.CV;;nerf
2403.07494v1;http://arxiv.org/abs/2403.07494v1;2024-03-12;SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM;"We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D
Gaussian representation, that enables accurate 3D semantic mapping, robust
camera tracking, and high-quality rendering in real-time. In this system, we
incorporate semantic feature embedding into 3D Gaussian representation, which
effectively encodes semantic information within the spatial layout of the
environment for precise semantic scene representation. Furthermore, we propose
feature-level loss for updating 3D Gaussian representation, enabling
higher-level guidance for 3D Gaussian optimization. In addition, to reduce
cumulative drift and improve reconstruction accuracy, we introduce
semantic-informed bundle adjustment leveraging semantic associations for joint
optimization of 3D Gaussian representation and camera poses, leading to more
robust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates
superior performance over existing dense semantic SLAM methods in terms of
mapping and tracking accuracy on Replica and ScanNet datasets, while also
showing excellent capabilities in novel-view semantic synthesis and 3D semantic
mapping.";Siting Zhu<author:sep>Renjie Qin<author:sep>Guangming Wang<author:sep>Jiuming Liu<author:sep>Hesheng Wang;http://arxiv.org/pdf/2403.07494v1;cs.RO;;gaussian splatting
2403.07807v1;http://arxiv.org/abs/2403.07807v1;2024-03-12;StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting;"We introduce StyleGaussian, a novel 3D style transfer technique that allows
instant transfer of any image's style to a 3D scene at 10 frames per second
(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style
transfer without compromising its real-time rendering ability and multi-view
consistency. It achieves instant style transfer with three steps: embedding,
transfer, and decoding. Initially, 2D VGG scene features are embedded into
reconstructed 3D Gaussians. Next, the embedded features are transformed
according to a reference style image. Finally, the transformed features are
decoded into the stylized RGB. StyleGaussian has two novel designs. The first
is an efficient feature rendering strategy that first renders low-dimensional
features and then maps them into high-dimensional features while embedding VGG
features. It cuts the memory consumption significantly and enables 3DGS to
render the high-dimensional memory-intensive features. The second is a
K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized
features, it eliminates the 2D CNN operations that compromise strict multi-view
consistency. Extensive experiments show that StyleGaussian achieves instant 3D
stylization with superior stylization quality while preserving real-time
rendering and strict multi-view consistency. Project page:
https://kunhao-liu.github.io/StyleGaussian/";Kunhao Liu<author:sep>Fangneng Zhan<author:sep>Muyu Xu<author:sep>Christian Theobalt<author:sep>Ling Shao<author:sep>Shijian Lu;http://arxiv.org/pdf/2403.07807v1;cs.CV;;gaussian splatting
2403.06908v1;http://arxiv.org/abs/2403.06908v1;2024-03-11;FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization;"3D Gaussian splatting has achieved very impressive performance in real-time
novel view synthesis. However, it often suffers from over-reconstruction during
Gaussian densification where high-variance image regions are covered by a few
large Gaussians only, leading to blur and artifacts in the rendered images. We
design a progressive frequency regularization (FreGS) technique to tackle the
over-reconstruction issue within the frequency space. Specifically, FreGS
performs coarse-to-fine Gaussian densification by exploiting low-to-high
frequency components that can be easily extracted with low-pass and high-pass
filters in the Fourier space. By minimizing the discrepancy between the
frequency spectrum of the rendered image and the corresponding ground truth, it
achieves high-quality Gaussian densification and alleviates the
over-reconstruction of Gaussian splatting effectively. Experiments over
multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and
Deep Blending) show that FreGS achieves superior novel view synthesis and
outperforms the state-of-the-art consistently.";Jiahui Zhang<author:sep>Fangneng Zhan<author:sep>Muyu Xu<author:sep>Shijian Lu<author:sep>Eric Xing;http://arxiv.org/pdf/2403.06908v1;cs.CV;;gaussian splatting<tag:sep>nerf
2403.06505v1;http://arxiv.org/abs/2403.06505v1;2024-03-11;Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis;"The neural radiance field (NeRF) has emerged as a prominent methodology for
synthesizing realistic images of novel views. While neural radiance
representations based on voxels or mesh individually offer distinct advantages,
excelling in either rendering quality or speed, each has limitations in the
other aspect. In response, we propose a pioneering hybrid representation named
Vosh, seamlessly combining both voxel and mesh components in hybrid rendering
for view synthesis. Vosh is meticulously crafted by optimizing the voxel grid
of NeRF, strategically with selected voxels replaced by mesh. Therefore, it
excels in fast rendering scenes with simple geometry and textures through its
mesh component, while simultaneously enabling high-quality rendering in
intricate regions by leveraging voxel component. The flexibility of Vosh is
showcased through the ability to adjust hybrid ratios, providing users the
ability to control the balance between rendering quality and speed based on
flexible usage. Experimental results demonstrates that our method achieves
commendable trade-off between rendering quality and speed, and notably has
real-time performance on mobile devices.";Chenhao Zhang<author:sep>Yongyang Zhou<author:sep>Lei Zhang;http://arxiv.org/pdf/2403.06505v1;cs.CV;;nerf
2403.06912v2;http://arxiv.org/abs/2403.06912v2;2024-03-11;DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with  Global-Local Depth Normalization;"Radiance fields have demonstrated impressive performance in synthesizing
novel views from sparse input views, yet prevailing methods suffer from high
training costs and slow inference speed. This paper introduces DNGaussian, a
depth-regularized framework based on 3D Gaussian radiance fields, offering
real-time and high-quality few-shot novel view synthesis at low costs. Our
motivation stems from the highly efficient representation and surprising
quality of the recent 3D Gaussian Splatting, despite it will encounter a
geometry degradation when input views decrease. In the Gaussian radiance
fields, we find this degradation in scene geometry primarily lined to the
positioning of Gaussian primitives and can be mitigated by depth constraint.
Consequently, we propose a Hard and Soft Depth Regularization to restore
accurate scene geometry under coarse monocular depth supervision while
maintaining a fine-grained color appearance. To further refine detailed
geometry reshaping, we introduce Global-Local Depth Normalization, enhancing
the focus on small local depth changes. Extensive experiments on LLFF, DTU, and
Blender datasets demonstrate that DNGaussian outperforms state-of-the-art
methods, achieving comparable or better results with significantly reduced
memory cost, a $25 \times$ reduction in training time, and over $3000 \times$
faster rendering speed.";Jiahe Li<author:sep>Jiawei Zhang<author:sep>Xiao Bai<author:sep>Jin Zheng<author:sep>Xin Ning<author:sep>Jun Zhou<author:sep>Lin Gu;http://arxiv.org/pdf/2403.06912v2;cs.CV;"Accepted at CVPR 2024. Project page:
  https://fictionarry.github.io/DNGaussian/";gaussian splatting
2403.06394v2;http://arxiv.org/abs/2403.06394v2;2024-03-11;FSViewFusion: Few-Shots View Generation of Novel Objects;"Novel view synthesis has observed tremendous developments since the arrival
of NeRFs. However, Nerf models overfit on a single scene, lacking
generalization to out of distribution objects. Recently, diffusion models have
exhibited remarkable performance on introducing generalization in view
synthesis. Inspired by these advancements, we explore the capabilities of a
pretrained stable diffusion model for view synthesis without explicit 3D
priors. Specifically, we base our method on a personalized text to image model,
Dreambooth, given its strong ability to adapt to specific novel objects with a
few shots. Our research reveals two interesting findings. First, we observe
that Dreambooth can learn the high level concept of a view, compared to
arguably more complex strategies which involve finetuning diffusions on large
amounts of multi-view data. Second, we establish that the concept of a view can
be disentangled and transferred to a novel object irrespective of the original
object's identify from which the views are learnt. Motivated by this, we
introduce a learning strategy, FSViewFusion, which inherits a specific view
through only one image sample of a single scene, and transfers the knowledge to
a novel object, learnt from few shots, using low rank adapters. Through
extensive experiments we demonstrate that our method, albeit simple, is
efficient in generating reliable view samples for in the wild images. Code and
models will be released.";Rukhshanda Hussain<author:sep>Hui Xian Grace Lim<author:sep>Borchun Chen<author:sep>Mubarak Shah<author:sep>Ser Nam Lim;http://arxiv.org/pdf/2403.06394v2;cs.CV;;nerf
2403.06877v1;http://arxiv.org/abs/2403.06877v1;2024-03-11;SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields  for Robotic Inspection;"We present a neural-field-based large-scale reconstruction system that fuses
lidar and vision data to generate high-quality reconstructions that are
geometrically accurate and capture photo-realistic textures. This system adapts
the state-of-the-art neural radiance field (NeRF) representation to also
incorporate lidar data which adds strong geometric constraints on the depth and
surface normals. We exploit the trajectory from a real-time lidar SLAM system
to bootstrap a Structure-from-Motion (SfM) procedure to both significantly
reduce the computation time and to provide metric scale which is crucial for
lidar depth loss. We use submapping to scale the system to large-scale
environments captured over long trajectories. We demonstrate the reconstruction
system with data from a multi-camera, lidar sensor suite onboard a legged
robot, hand-held while scanning building scenes for 600 metres, and onboard an
aerial robot surveying a multi-storey mock disaster site-building. Website:
https://ori-drs.github.io/projects/silvr/";Yifu Tao<author:sep>Yash Bhalgat<author:sep>Lanke Frank Tarimo Fu<author:sep>Matias Mattamala<author:sep>Nived Chebrolu<author:sep>Maurice Fallon;http://arxiv.org/pdf/2403.06877v1;cs.RO;"Accepted at ICRA 2024; Website:
  https://ori-drs.github.io/projects/silvr/";nerf
2403.06092v1;http://arxiv.org/abs/2403.06092v1;2024-03-10;Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View  Synthesis?;"Neural Radiance Field (NeRF) has achieved superior performance for novel view
synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a
volume rendering procedure, however, when fewer known views are given (i.e.,
few-shot view synthesis), the model is prone to overfit the given views. To
handle this issue, previous efforts have been made towards leveraging learned
priors or introducing additional regularizations. In contrast, in this paper,
we for the first time provide an orthogonal method from the perspective of
network structure. Given the observation that trivially reducing the number of
model parameters alleviates the overfitting issue, but at the cost of missing
details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs
(i.e., location and viewing direction) of the vanilla MLP into each layer to
prevent the overfitting issue without harming detailed synthesis. To further
reduce the artifacts, we propose to model colors and volume density separately
and present two regularization terms. Extensive experiments on multiple
datasets demonstrate that: 1) although the proposed mi-MLP is easy to
implement, it is surprisingly effective as it boosts the PSNR of the baseline
from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art
results on a wide range of benchmarks. We will release the code upon
publication.";Hanxin Zhu<author:sep>Tianyu He<author:sep>Xin Li<author:sep>Bingchen Li<author:sep>Zhibo Chen;http://arxiv.org/pdf/2403.06092v1;cs.CV;Accepted by CVPR 2024;nerf
2403.05783v1;http://arxiv.org/abs/2403.05783v1;2024-03-09;Large Generative Model Assisted 3D Semantic Communication;"Semantic Communication (SC) is a novel paradigm for data transmission in 6G.
However, there are several challenges posed when performing SC in 3D scenarios:
1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain
channel estimation. To address these issues, we propose a Generative AI Model
assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor
(3DSE), which employs generative AI models, including Segment Anything Model
(SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D
scenario based on user requirements. The extracted 3D semantics are represented
as multi-perspective images of the goal-oriented 3D object. Then, we present an
Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective
images, in which we use a semantic encoder with two output heads to perform
semantic encoding and mask redundant semantics in the latent semantic space,
respectively. Next, we design a conditional Generative adversarial network and
Diffusion model aided-Channel Estimation (GDCE) to estimate and refine the
Channel State Information (CSI) of physical channels. Finally, simulation
results demonstrate the advantages of the proposed GAM-3DSC system in
effectively transmitting the goal-oriented 3D scenario.";Feibo Jiang<author:sep>Yubo Peng<author:sep>Li Dong<author:sep>Kezhi Wang<author:sep>Kun Yang<author:sep>Cunhua Pan<author:sep>Xiaohu You;http://arxiv.org/pdf/2403.05783v1;cs.IT;13 pages,13 figures,1 table;nerf
2403.05907v1;http://arxiv.org/abs/2403.05907v1;2024-03-09;Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous  Driving;"Recent studies have highlighted the promising application of NeRF in
autonomous driving contexts. However, the complexity of outdoor environments,
combined with the restricted viewpoints in driving scenarios, complicates the
task of precisely reconstructing scene geometry. Such challenges often lead to
diminished quality in reconstructions and extended durations for both training
and rendering. To tackle these challenges, we present Lightning NeRF. It uses
an efficient hybrid scene representation that effectively utilizes the geometry
prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly
improves the novel view synthesis performance of NeRF and reduces computational
overheads. Through evaluations on real-world datasets, such as KITTI-360,
Argoverse2, and our private dataset, we demonstrate that our approach not only
exceeds the current state-of-the-art in novel view synthesis quality but also
achieves a five-fold increase in training speed and a ten-fold improvement in
rendering speed. Codes are available at
https://github.com/VISION-SJTU/Lightning-NeRF .";Junyi Cao<author:sep>Zhichao Li<author:sep>Naiyan Wang<author:sep>Chao Ma;http://arxiv.org/pdf/2403.05907v1;cs.CV;Accepted to ICRA 2024;nerf
2403.05154v1;http://arxiv.org/abs/2403.05154v1;2024-03-08;GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian  Splatting;"We present GSEdit, a pipeline for text-guided 3D object editing based on
Gaussian Splatting models. Our method enables the editing of the style and
appearance of 3D objects without altering their main details, all in a matter
of minutes on consumer hardware. We tackle the problem by leveraging Gaussian
splatting to represent 3D scenes, and we optimize the model while progressively
varying the image supervision by means of a pretrained image-based diffusion
model. The input object may be given as a 3D triangular mesh, or directly
provided as Gaussians from a generative model such as DreamGaussian. GSEdit
ensures consistency across different viewpoints, maintaining the integrity of
the original object's information. Compared to previously proposed methods
relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making
3D editing tasks much faster. Our editing process is refined via the
application of the SDS loss, ensuring that our edits are both precise and
accurate. Our comprehensive evaluation demonstrates that GSEdit effectively
alters object shape and appearance following the given textual instructions
while preserving their coherence and detail.";Francesco Palandra<author:sep>Andrea Sanchietti<author:sep>Daniele Baieri<author:sep>Emanuele RodolÃ ;http://arxiv.org/pdf/2403.05154v1;cs.CV;15 pages, 7 figures;gaussian splatting<tag:sep>nerf
2403.05087v1;http://arxiv.org/abs/2403.05087v1;2024-03-08;SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded  Gaussian Splatting;"We present SplattingAvatar, a hybrid 3D representation of photorealistic
human avatars with Gaussian Splatting embedded on a triangle mesh, which
renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We
disentangle the motion and appearance of a virtual human with explicit mesh
geometry and implicit appearance modeling with Gaussian Splatting. The
Gaussians are defined by barycentric coordinates and displacement on a triangle
mesh as Phong surfaces. We extend lifted optimization to simultaneously
optimize the parameters of the Gaussians while walking on the triangle mesh.
SplattingAvatar is a hybrid representation of virtual humans where the mesh
represents low-frequency motion and surface deformation, while the Gaussians
take over the high-frequency geometry and detailed appearance. Unlike existing
deformation methods that rely on an MLP-based linear blend skinning (LBS) field
for motion, we control the rotation and translation of the Gaussians directly
by mesh, which empowers its compatibility with various animation techniques,
e.g., skeletal animation, blend shapes, and mesh editing. Trainable from
monocular videos for both full-body and head avatars, SplattingAvatar shows
state-of-the-art rendering quality across multiple datasets.";Zhijing Shao<author:sep>Zhaolong Wang<author:sep>Zhuang Li<author:sep>Duotun Wang<author:sep>Xiangru Lin<author:sep>Yu Zhang<author:sep>Mingming Fan<author:sep>Zeyu Wang;http://arxiv.org/pdf/2403.05087v1;cs.GR;"[CVPR 2024] Code and data are available at
  https://github.com/initialneil/SplattingAvatar";gaussian splatting
2403.04116v1;http://arxiv.org/abs/2403.04116v1;2024-03-07;Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis;"X-ray is widely applied for transmission imaging due to its stronger
penetration than natural light. When rendering novel view X-ray projections,
existing methods mainly based on NeRF suffer from long training time and slow
inference speed. In this paper, we propose a 3D Gaussian splatting-based
framework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we
redesign a radiative Gaussian point cloud model inspired by the isotropic
nature of X-ray imaging. Our model excludes the influence of view direction
when learning to predict the radiation intensity of 3D points. Based on this
model, we develop a Differentiable Radiative Rasterization (DRR) with CUDA
implementation. Secondly, we customize an Angle-pose Cuboid Uniform
Initialization (ACUI) strategy that directly uses the parameters of the X-ray
scanner to compute the camera information and then uniformly samples point
positions within a cuboid enclosing the scanned object. Experiments show that
our X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying
less than 15% training time and over 73x inference speed. The application on
sparse-view CT reconstruction also reveals the practical values of our method.
Code and models will be publicly available at
https://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training
process visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .";Yuanhao Cai<author:sep>Yixun Liang<author:sep>Jiahao Wang<author:sep>Angtian Wang<author:sep>Yulun Zhang<author:sep>Xiaokang Yang<author:sep>Zongwei Zhou<author:sep>Alan Yuille;http://arxiv.org/pdf/2403.04116v1;eess.IV;"The first 3D Gaussian Splatting-based method for X-ray 3D
  reconstruction";gaussian splatting<tag:sep>nerf
2403.04115v2;http://arxiv.org/abs/2403.04115v2;2024-03-07;DNAct: Diffusion Guided Multi-Task 3D Policy Learning;"This paper presents DNAct, a language-conditioned multi-task policy framework
that integrates neural rendering pre-training and diffusion training to enforce
multi-modality learning in action sequence spaces. To learn a generalizable
multi-task policy with few demonstrations, the pre-training phase of DNAct
leverages neural rendering to distill 2D semantic features from foundation
models such as Stable Diffusion to a 3D space, which provides a comprehensive
semantic understanding regarding the scene. Consequently, it allows various
applications to challenging robotic tasks requiring rich 3D semantics and
accurate geometry. Furthermore, we introduce a novel approach utilizing
diffusion training to learn a vision and language feature that encapsulates the
inherent multi-modality in the multi-task demonstrations. By reconstructing the
action sequences from different tasks via the diffusion process, the model is
capable of distinguishing different modalities and thus improving the
robustness and the generalizability of the learned representation. DNAct
significantly surpasses SOTA NeRF-based multi-task manipulation approaches with
over 30% improvement in success rate. Project website: dnact.github.io.";Ge Yan<author:sep>Yueh-Hua Wu<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2403.04115v2;cs.RO;;nerf
2403.04926v1;http://arxiv.org/abs/2403.04926v1;2024-03-07;BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel  Modeling;"Recent efforts in using 3D Gaussians for scene reconstruction and novel view
synthesis can achieve impressive results on curated benchmarks; however, images
captured in real life are often blurry. In this work, we analyze the robustness
of Gaussian-Splatting-based methods against various image blur, such as motion
blur, defocus blur, downscaling blur, \etc. Under these degradations,
Gaussian-Splatting-based methods tend to overfit and produce worse results than
Neural-Radiance-Field-based methods. To address this issue, we propose Blur
Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling
capacities such that a 3D-consistent and high quality scene can be
reconstructed despite image-wise blur. Specifically, we model blur by
estimating per-pixel convolution kernels from a Blur Proposal Network (BPN).
BPN is designed to consider spatial, color, and depth variations of the scene
to maximize modeling capacity. Additionally, BPN also proposes a
quality-assessing mask, which indicates regions where blur occur. Finally, we
introduce a coarse-to-fine kernel optimization scheme; this optimization scheme
is fast and avoids sub-optimal solutions due to a sparse point cloud
initialization, which often occurs when we apply Structure-from-Motion on
blurry images. We demonstrate that BAGS achieves photorealistic renderings
under various challenging blur conditions and imaging geometry, while
significantly improving upon existing approaches.";Cheng Peng<author:sep>Yutao Tang<author:sep>Yifan Zhou<author:sep>Nengyu Wang<author:sep>Xijun Liu<author:sep>Deming Li<author:sep>Rama Chellappa;http://arxiv.org/pdf/2403.04926v1;cs.CV;;gaussian splatting
2403.04114v1;http://arxiv.org/abs/2403.04114v1;2024-03-07;Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs;"Deep learning methods for perception are the cornerstone of many robotic
systems. Despite their potential for impressive performance, obtaining
real-world training data is expensive, and can be impractically difficult for
some tasks. Sim-to-real transfer with domain randomization offers a potential
workaround, but often requires extensive manual tuning and results in models
that are brittle to distribution shift between sim and real. In this work, we
introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF
model that is the centerpiece of a real-to-sim pipeline for synthesizing
training data targeted to scenes and objects from the real world. COV-NeRF
extracts objects from real images and composes them into new scenes, generating
photorealistic renderings and many types of 2D and 3D supervision, including
depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the
rendering quality of modern NeRF methods, and can be used to rapidly close the
sim-to-real gap across a variety of perceptual modalities.";Nikhil Mishra<author:sep>Maximilian Sieb<author:sep>Pieter Abbeel<author:sep>Xi Chen;http://arxiv.org/pdf/2403.04114v1;cs.RO;ICRA 2024;nerf
2403.04508v2;http://arxiv.org/abs/2403.04508v2;2024-03-07;Finding Waldo: Towards Efficient Exploration of NeRF Scene Spaces;"Neural Radiance Fields (NeRF) have quickly become the primary approach for 3D
reconstruction and novel view synthesis in recent years due to their remarkable
performance. Despite the huge interest in NeRF methods, a practical use case of
NeRFs has largely been ignored; the exploration of the scene space modelled by
a NeRF. In this paper, for the first time in the literature, we propose and
formally define the scene exploration framework as the efficient discovery of
NeRF model inputs (i.e. coordinates and viewing angles), using which one can
render novel views that adhere to user-selected criteria. To remedy the lack of
approaches addressing scene exploration, we first propose two baseline methods
called Guided-Random Search (GRS) and Pose Interpolation-based Search (PIBS).
We then cast scene exploration as an optimization problem, and propose the
criteria-agnostic Evolution-Guided Pose Search (EGPS) for efficient
exploration. We test all three approaches with various criteria (e.g. saliency
maximization, image quality maximization, photo-composition quality
improvement) and show that our EGPS performs more favourably than other
baselines. We finally highlight key points and limitations, and outline
directions for future research in scene exploration.";Evangelos Skartados<author:sep>Mehmet Kerim Yucel<author:sep>Bruno Manganelli<author:sep>Anastasios Drosou<author:sep>Albert SaÃ -Garriga;http://arxiv.org/pdf/2403.04508v2;cs.CV;Accepted at ACM MMSys'24;nerf
2403.03608v1;http://arxiv.org/abs/2403.03608v1;2024-03-06;GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D  Scene Understanding;"Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance
Fields (NeRF) have emerged as a popular research topic in 3D vision. In this
work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),
which uniquely takes image semantics into the synthesis process so that both
novel view images and the associated semantic maps can be produced for unseen
scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and
Depth-Guided Visual rendering. The former is able to observe multi-view image
inputs to extract semantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs both image and
semantic rendering with improved performances. Our experiments not only confirm
that GSNeRF performs favorably against prior works on both novel-view image and
semantic segmentation synthesis but the effectiveness of our sampling strategy
for visual rendering is further verified.";Zi-Ting Chou<author:sep>Sheng-Yu Huang<author:sep>I-Jieh Liu<author:sep>Yu-Chiang Frank Wang;http://arxiv.org/pdf/2403.03608v1;cs.CV;Accepted by CVPR2024;nerf
2403.03241v1;http://arxiv.org/abs/2403.03241v1;2024-03-05;A Deep Learning Framework for Wireless Radiation Field Reconstruction  and Channel Prediction;"We present NeWRF, a deep learning framework for predicting wireless channels.
Wireless channel prediction is a long-standing problem in the wireless
community and is a key technology for improving the coverage of wireless
network deployments. Today, a wireless deployment is evaluated by a site survey
which is a cumbersome process requiring an experienced engineer to perform
extensive channel measurements. To reduce the cost of site surveys, we develop
NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF).
NeWRF trains a neural network model with a sparse set of channel measurements,
and predicts the wireless channel accurately at any location in the site. We
introduce a series of techniques that integrate wireless propagation properties
into the NeRF framework to account for the fundamental differences between the
behavior of light and wireless signals. We conduct extensive evaluations of our
framework and show that our approach can accurately predict channels at
unvisited locations with significantly lower measurement density than prior
state-of-the-art";Haofan Lu<author:sep>Christopher Vattheuer<author:sep>Baharan Mirzasoleiman<author:sep>Omid Abari;http://arxiv.org/pdf/2403.03241v1;cs.NI;;nerf
2403.02751v1;http://arxiv.org/abs/2403.02751v1;2024-03-05;Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps;"We present Splat-Nav, a navigation pipeline that consists of a real-time safe
planning module and a robust state estimation module designed to operate in the
Gaussian Splatting (GSplat) environment representation, a popular emerging 3D
scene representation from computer vision. We formulate rigorous collision
constraints that can be computed quickly to build a guaranteed-safe polytope
corridor through the map. We then optimize a B-spline trajectory through this
corridor. We also develop a real-time, robust state estimation module by
interpreting the GSplat representation as a point cloud. The module enables the
robot to localize its global pose with zero prior knowledge from RGB-D images
using point cloud alignment, and then track its own pose as it moves through
the scene from RGB images using image-to-point cloud localization. We also
incorporate semantics into the GSplat in order to obtain better images for
localization. All of these modules operate mainly on CPU, freeing up GPU
resources for tasks like real-time scene reconstruction. We demonstrate the
safety and robustness of our pipeline in both simulation and hardware, where we
show re-planning at 5 Hz and pose estimation at 20 Hz, an order of magnitude
faster than Neural Radiance Field (NeRF)-based navigation methods, thereby
enabling real-time navigation.";Timothy Chen<author:sep>Ola Shorinwa<author:sep>Weijia Zeng<author:sep>Joseph Bruno<author:sep>Philip Dames<author:sep>Mac Schwager;http://arxiv.org/pdf/2403.02751v1;cs.RO;;gaussian splatting<tag:sep>nerf
2403.02063v1;http://arxiv.org/abs/2403.02063v1;2024-03-04;Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input  Views;"Novel-view synthesis with sparse input views is important for real-world
applications like AR/VR and autonomous driving. Recent methods have integrated
depth information into NeRFs for sparse input synthesis, leveraging depth prior
for geometric and spatial understanding. However, most existing works tend to
overlook inaccuracies within depth maps and have low time efficiency. To
address these issues, we propose a depth-guided robust and fast point cloud
fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel
grid of features. A point cloud is constructed for each input view,
characterized within the voxel grid using matrices and vectors. We accumulate
the point cloud of each input view to construct the fused point cloud of the
entire scene. Each voxel determines its density and appearance by referring to
the point cloud of the entire scene. Through point cloud fusion and voxel grid
fine-tuning, inaccuracies in depth values are refined or substituted by those
from other views. Moreover, our method can achieve faster reconstruction and
greater compactness through effective vector-matrix decomposition. Experimental
results underline the superior performance and time efficiency of our approach
compared to state-of-the-art baselines.";Shuai Guo<author:sep>Qiuwen Wang<author:sep>Yijie Gao<author:sep>Rong Xie<author:sep>Li Song;http://arxiv.org/pdf/2403.02063v1;cs.CV;;nerf
2403.02265v1;http://arxiv.org/abs/2403.02265v1;2024-03-04;DaReNeRF: Direction-aware Representation for Dynamic Scenes;"Addressing the intricate challenge of modeling and re-rendering dynamic
scenes, most recent approaches have sought to simplify these complexities using
plane-based explicit representations, overcoming the slow training time issues
associated with methods like Neural Radiance Fields (NeRF) and implicit
representations. However, the straightforward decomposition of 4D dynamic
scenes into multiple 2D plane-based representations proves insufficient for
re-rendering high-fidelity scenes with complex motions. In response, we present
a novel direction-aware representation (DaRe) approach that captures scene
dynamics from six different directions. This learned representation undergoes
an inverse dual-tree complex wavelet transformation (DTCWT) to recover
plane-based information. DaReNeRF computes features for each space-time point
by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny
MLP for color regression and leveraging volume rendering in training yield
state-of-the-art performance in novel view synthesis for complex dynamic
scenes. Notably, to address redundancy introduced by the six real and six
imaginary direction-aware wavelet coefficients, we introduce a trainable
masking approach, mitigating storage issues without significant performance
decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared
to prior art while delivering superior performance.";Ange Lou<author:sep>Benjamin Planche<author:sep>Zhongpai Gao<author:sep>Yamin Li<author:sep>Tianyu Luan<author:sep>Hao Ding<author:sep>Terrence Chen<author:sep>Jack Noble<author:sep>Ziyan Wu;http://arxiv.org/pdf/2403.02265v1;cs.CV;Accepted at CVPR 2024. Paper + supplementary material;nerf
2403.01325v1;http://arxiv.org/abs/2403.01325v1;2024-03-02;NeRF-VPT: Learning Novel View Representations with Neural Radiance  Fields via View Prompt Tuning;"Neural Radiance Fields (NeRF) have garnered remarkable success in novel view
synthesis. Nonetheless, the task of generating high-quality images for novel
views persists as a critical challenge. While the existing efforts have
exhibited commendable progress, capturing intricate details, enhancing
textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics
warrant further focused attention and advancement. In this work, we propose
NeRF-VPT, an innovative method for novel view synthesis to address these
challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning
paradigm, wherein RGB information gained from preceding rendering outcomes
serves as instructive visual prompts for subsequent rendering stages, with the
aspiration that the prior knowledge embedded in the prompts can facilitate the
gradual enhancement of rendered image quality. NeRF-VPT only requires sampling
RGB data from previous stage renderings as priors at each training stage,
without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is
plug-and-play and can be readily integrated into existing methods. By
conducting comparative analyses of our NeRF-VPT against several NeRF-based
approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360,
Real Forward-Facing, Replica dataset, and a user-captured dataset, we
substantiate that our NeRF-VPT significantly elevates baseline performance and
proficiently generates more high-quality novel view images than all the
compared state-of-the-art methods. Furthermore, the cascading learning of
NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in
a significant enhancement of accuracy for sparse-view novel view synthesis. The
source code and dataset are available at
\url{https://github.com/Freedomcls/NeRF-VPT}.";Linsheng Chen<author:sep>Guangrun Wang<author:sep>Liuchun Yuan<author:sep>Keze Wang<author:sep>Ken Deng<author:sep>Philip H. S. Torr;http://arxiv.org/pdf/2403.01325v1;cs.CV;AAAI 2024;nerf
2403.01058v1;http://arxiv.org/abs/2403.01058v1;2024-03-02;Neural Field Classifiers via Target Encoding and Classification Loss;"Neural field methods have seen great progress in various long-standing tasks
in computer vision and computer graphics, including novel view synthesis and
geometry reconstruction. As existing neural field methods try to predict some
coordinate-based continuous target values, such as RGB for Neural Radiance
Field (NeRF), all of these methods are regression models and are optimized by
some regression loss. However, are regression models really better than
classification models for neural field methods? In this work, we try to visit
this very fundamental but overlooked question for neural fields from a machine
learning perspective. We successfully propose a novel Neural Field Classifier
(NFC) framework which formulates existing neural field methods as
classification tasks rather than regression tasks. The proposed NFC can easily
transform arbitrary Neural Field Regressor (NFR) into its classification
variant via employing a novel Target Encoding module and optimizing a
classification loss. By encoding a continuous regression target into a
high-dimensional discrete encoding, we naturally formulate a multi-label
classification task. Extensive experiments demonstrate the impressive
effectiveness of NFC at the nearly free extra computational costs. Moreover,
NFC also shows robustness to sparse inputs, corrupted images, and dynamic
scenes.";Xindi Yang<author:sep>Zeke Xie<author:sep>Xiong Zhou<author:sep>Boyu Liu<author:sep>Buhua Liu<author:sep>Yi Liu<author:sep>Haoran Wang<author:sep>Yunfeng Cai<author:sep>Mingming Sun;http://arxiv.org/pdf/2403.01058v1;cs.CV;"ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables";nerf
2403.01137v1;http://arxiv.org/abs/2403.01137v1;2024-03-02;Neural radiance fields-based holography [Invited];"This study presents a novel approach for generating holograms based on the
neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data
is difficult in hologram computation. NeRF is a state-of-the-art technique for
3D light-field reconstruction from 2D images based on volume rendering. The
NeRF can rapidly predict new-view images that do not include a training
dataset. In this study, we constructed a rendering pipeline directly from a 3D
light field generated from 2D images by NeRF for hologram generation using deep
neural networks within a reasonable time. The pipeline comprises three main
components: the NeRF, a depth predictor, and a hologram generator, all
constructed using deep neural networks. The pipeline does not include any
physical calculations. The predicted holograms of a 3D scene viewed from any
direction were computed using the proposed pipeline. The simulation and
experimental results are presented.";Minsung Kang<author:sep>Fan Wang<author:sep>Kai Kumano<author:sep>Tomoyoshi Ito<author:sep>Tomoyoshi Shimobaba;http://arxiv.org/pdf/2403.01137v1;cs.CV;;nerf
2403.00228v1;http://arxiv.org/abs/2403.00228v1;2024-03-01;DISORF: A Distributed Online NeRF Training and Rendering Framework for  Mobile Robots;"We present a framework, DISORF, to enable online 3D reconstruction and
visualization of scenes captured by resource-constrained mobile robots and edge
devices. To address the limited compute capabilities of edge devices and
potentially limited network availability, we design a framework that
efficiently distributes computation between the edge device and remote server.
We leverage on-device SLAM systems to generate posed keyframes and transmit
them to remote servers that can perform high quality 3D reconstruction and
visualization at runtime by leveraging NeRF models. We identify a key challenge
with online NeRF training where naive image sampling strategies can lead to
significant degradation in rendering quality. We propose a novel shifted
exponential frame sampling method that addresses this challenge for online NeRF
training. We demonstrate the effectiveness of our framework in enabling
high-quality real-time reconstruction and visualization of unknown scenes as
they are captured and streamed from cameras in mobile robots and edge devices.";Chunlin Li<author:sep>Ruofan Liang<author:sep>Hanrui Fan<author:sep>Zhengen Zhang<author:sep>Sankeerth Durvasula<author:sep>Nandita Vijaykumar;http://arxiv.org/pdf/2403.00228v1;cs.RO;;nerf
2402.19441v1;http://arxiv.org/abs/2402.19441v1;2024-02-29;3D Gaussian Model for Animation and Texturing;"3D Gaussian Splatting has made a marked impact on neural rendering by
achieving impressive fidelity and performance. Despite this achievement,
however, it is not readily applicable to developing interactive applications.
Real-time applications like XR apps and games require functions such as
animation, UV-mapping, and model editing simultaneously manipulated through the
usage of a 3D model. We propose a modeling that is analogous to typical 3D
models, which we call 3D Gaussian Model (3DGM); it provides a manipulatable
proxy for novel animation and texture transfer. By binding the 3D Gaussians in
texture space and re-projecting them back to world space through implicit shell
mapping, we show how our 3D modeling can serve as a valid rendering methodology
for interactive applications. It is further noted that recently, 3D mesh
reconstruction works have been able to produce high-quality mesh for rendering.
Our work, on the other hand, only requires an approximated geometry for
rendering an object in high fidelity. Applicationwise, we will show that our
proxy-based 3DGM is capable of driving novel animation without animated
training data and texture transferring via UV mapping of the 3D Gaussians. We
believe the result indicates the potential of our work for enabling interactive
applications for 3D Gaussian Splatting.";Xiangzhi Eric Wang<author:sep>Zackary P. T. Sin;http://arxiv.org/pdf/2402.19441v1;cs.GR;;gaussian splatting
2402.18196v1;http://arxiv.org/abs/2402.18196v1;2024-02-28;NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human  Pose Estimation in Top-View Fisheye Images;"Human pose estimation (HPE) in the top-view using fisheye cameras presents a
promising and innovative application domain. However, the availability of
datasets capturing this viewpoint is extremely limited, especially those with
high-quality 2D and 3D keypoint annotations. Addressing this gap, we leverage
the capabilities of Neural Radiance Fields (NeRF) technique to establish a
comprehensive pipeline for generating human pose datasets from existing 2D and
3D datasets, specifically tailored for the top-view fisheye perspective.
Through this pipeline, we create a novel dataset NToP570K (NeRF-powered
Top-view human Pose dataset for fisheye cameras with over 570 thousand images),
and conduct an extensive evaluation of its efficacy in enhancing neural
networks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-B
model achieves an improvement in AP of 33.3 % on our validation set for 2D HPE
after finetuning on our training set. A similarly finetuned HybrIK-Transformer
model gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.";Jingrui Yu<author:sep>Dipankar Nandi<author:sep>Roman Seidel<author:sep>Gangolf Hirtz;http://arxiv.org/pdf/2402.18196v1;cs.CV;;nerf
2402.17364v1;http://arxiv.org/abs/2402.17364v1;2024-02-27;Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis;"Recent works in implicit representations, such as Neural Radiance Fields
(NeRF), have advanced the generation of realistic and animatable head avatars
from video sequences. These implicit methods are still confronted by visual
artifacts and jitters, since the lack of explicit geometric constraints poses a
fundamental challenge in accurately modeling complex facial deformations. In
this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid
representation that encodes explicit dynamic meshes by neural networks to
ensure geometric consistency across various motions and viewpoints. DynTet is
parameterized by the coordinate-based networks which learn signed distance,
deformation, and material texture, anchoring the training data into a
predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently
decodes textured meshes with a consistent topology, enabling fast rendering
through a differentiable rasterizer and supervision via a pixel loss. To
enhance training efficiency, we incorporate classical 3D Morphable Models to
facilitate geometry learning and define a canonical space for simplifying
texture learning. These advantages are readily achievable owing to the
effective geometric representation employed in DynTet. Compared with prior
works, DynTet demonstrates significant improvements in fidelity, lip
synchronization, and real-time performance according to various metrics. Beyond
producing stable and visually appealing synthesis videos, our method also
outputs the dynamic meshes which is promising to enable many emerging
applications.";Zicheng Zhang<author:sep>Ruobing Zheng<author:sep>Ziwen Liu<author:sep>Congying Han<author:sep>Tianqi Li<author:sep>Meng Wang<author:sep>Tiande Guo<author:sep>Jingdong Chen<author:sep>Bonan Li<author:sep>Ming Yang;http://arxiv.org/pdf/2402.17364v1;cs.CV;CVPR 2024;nerf
2402.17768v1;http://arxiv.org/abs/2402.17768v1;2024-02-27;Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning;"A common failure mode for policies trained with imitation is compounding
execution errors at test time. When the learned policy encounters states that
were not present in the expert demonstrations, the policy fails, leading to
degenerate behavior. The Dataset Aggregation, or DAgger approach to this
problem simply collects more data to cover these failure states. However, in
practice, this is often prohibitively expensive. In this work, we propose
Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without
the cost for eye-in-hand imitation learning problems. Instead of collecting new
samples to cover out-of-distribution states, DMD uses recent advances in
diffusion models to create these samples with diffusion models. This leads to
robust performance from few demonstrations. In experiments conducted for
non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a
success rate of 80% with as few as 8 expert demonstrations, where naive
behavior cloning reaches only 20%. DMD also outperform competing NeRF-based
augmentation schemes by 50%.";Xiaoyu Zhang<author:sep>Matthew Chang<author:sep>Pranav Kumar<author:sep>Saurabh Gupta;http://arxiv.org/pdf/2402.17768v1;cs.RO;"for project website with video, see
  https://sites.google.com/view/diffusion-meets-dagger";nerf
2402.17427v1;http://arxiv.org/abs/2402.17427v1;2024-02-27;VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction;"Existing NeRF-based methods for large scene reconstruction often have
limitations in visual quality and rendering speed. While the recent 3D Gaussian
Splatting works well on small-scale and object-centric scenes, scaling it up to
large scenes poses challenges due to limited video memory, long optimization
time, and noticeable appearance variations. To address these challenges, we
present VastGaussian, the first method for high-quality reconstruction and
real-time rendering on large scenes based on 3D Gaussian Splatting. We propose
a progressive partitioning strategy to divide a large scene into multiple
cells, where the training cameras and point cloud are properly distributed with
an airspace-aware visibility criterion. These cells are merged into a complete
scene after parallel optimization. We also introduce decoupled appearance
modeling into the optimization process to reduce appearance variations in the
rendered images. Our approach outperforms existing NeRF-based methods and
achieves state-of-the-art results on multiple large scene datasets, enabling
fast optimization and high-fidelity real-time rendering.";Jiaqi Lin<author:sep>Zhihao Li<author:sep>Xiao Tang<author:sep>Jianzhuang Liu<author:sep>Shiyong Liu<author:sep>Jiayue Liu<author:sep>Yangdi Lu<author:sep>Xiaofei Wu<author:sep>Songcen Xu<author:sep>Youliang Yan<author:sep>Wenming Yang;http://arxiv.org/pdf/2402.17427v1;cs.CV;"Accepted to CVPR 2024. Project website:
  https://vastgaussian.github.io";gaussian splatting<tag:sep>nerf
2402.17115v1;http://arxiv.org/abs/2402.17115v1;2024-02-27;CharNeRF: 3D Character Generation from Concept Art;"3D modeling holds significant importance in the realms of AR/VR and gaming,
allowing for both artistic creativity and practical applications. However, the
process is often time-consuming and demands a high level of skill. In this
paper, we present a novel approach to create volumetric representations of 3D
characters from consistent turnaround concept art, which serves as the standard
input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been
a game-changer in image-based 3D reconstruction, to the best of our knowledge,
there is no known research that optimizes the pipeline for concept art. To
harness the potential of concept art, with its defined body poses and specific
view angles, we propose encoding it as priors for our model. We train the
network to make use of these priors for various 3D points through a learnable
view-direction-attended multi-head self-attention layer. Additionally, we
demonstrate that a combination of ray sampling and surface sampling enhances
the inference capabilities of our network. Our model is able to generate
high-quality 360-degree views of characters. Subsequently, we provide a simple
guideline to better leverage our model to extract the 3D mesh. It is important
to note that our model's inferencing capabilities are influenced by the
training data's characteristics, primarily focusing on characters with a single
head, two arms, and two legs. Nevertheless, our methodology remains versatile
and adaptable to concept art from diverse subject matters, without imposing any
specific assumptions on the data.";Eddy Chu<author:sep>Yiyang Chen<author:sep>Chedy Raissi<author:sep>Anand Bhojan;http://arxiv.org/pdf/2402.17115v1;cs.CV;;nerf
2402.17292v1;http://arxiv.org/abs/2402.17292v1;2024-02-27;DivAvatar: Diverse 3D Avatar Generation with a Single Prompt;"Text-to-Avatar generation has recently made significant strides due to
advancements in diffusion models. However, most existing work remains
constrained by limited diversity, producing avatars with subtle differences in
appearance for a given text prompt. We design DivAvatar, a novel framework that
generates diverse avatars, empowering 3D creatives with a multitude of distinct
and richly varied 3D avatars from a single text prompt. Different from most
existing work that exploits scene-specific 3D representations such as NeRF,
DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse
avatar generation from simply noise sampling in inference time. DivAvatar has
two key designs that help achieve generation diversity and visual quality. The
first is a noise sampling technique during training phase which is critical in
generating diverse appearances. The second is a semantic-aware zoom mechanism
and a novel depth loss, the former producing appearances of high textual
fidelity by separate fine-tuning of specific body parts and the latter
improving geometry quality greatly by smoothing the generated mesh in the
features space. Extensive experiments show that DivAvatar is highly versatile
in generating avatars of diverse appearances.";Weijing Tao<author:sep>Biwen Lei<author:sep>Kunhao Liu<author:sep>Shijian Lu<author:sep>Miaomiao Cui<author:sep>Xuansong Xie<author:sep>Chunyan Miao;http://arxiv.org/pdf/2402.17292v1;cs.CV;;nerf
2402.16366v1;http://arxiv.org/abs/2402.16366v1;2024-02-26;SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field;"Representing the Neural Radiance Field (NeRF) with the explicit voxel grid
(EVG) is a promising direction for improving NeRFs. However, the EVG
representation is not efficient for storage and transmission because of the
terrific memory cost. Current methods for compressing EVG mainly inherit the
methods designed for neural network compression, such as pruning and
quantization, which do not take full advantage of the spatial correlation of
voxels. Inspired by prosperous digital image compression techniques, this paper
proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG
compression. The proposed framework can remove spatial redundancy efficiently
for better compression performance.Moreover, we model the bitrate and design a
novel form of the loss function, where we can jointly optimize compression
ratio and distortion to achieve higher coding efficiency. Extensive experiments
demonstrate that our method can achieve 32% bit saving compared to the
state-of-the-art method VQRF on multiple representative test datasets, with
comparable training time.";Zetian Song<author:sep>Wenhong Duan<author:sep>Yuhuai Zhang<author:sep>Shiqi Wang<author:sep>Siwei Ma<author:sep>Wen Gao;http://arxiv.org/pdf/2402.16366v1;cs.CV;;nerf
2402.16407v1;http://arxiv.org/abs/2402.16407v1;2024-02-26;CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency;"Neural Radiance Field (NeRF) has shown impressive results in novel view
synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR),
thanks to its ability to represent scenes continuously. However, when just a
few input view images are available, NeRF tends to overfit the given views and
thus make the estimated depths of pixels share almost the same value. Unlike
previous methods that conduct regularization by introducing complex priors or
additional supervisions, we propose a simple yet effective method that
explicitly builds depth-aware consistency across input views to tackle this
challenge. Our key insight is that by forcing the same spatial points to be
sampled repeatedly in different input views, we are able to strengthen the
interactions between views and therefore alleviate the overfitting problem. To
achieve this, we build the neural networks on layered representations
(\textit{i.e.}, multiplane images), and the sampling point can thus be
resampled on multiple discrete planes. Furthermore, to regularize the unseen
target views, we constrain the rendered colors and depths from different input
views to be the same. Although simple, extensive experiments demonstrate that
our proposed method can achieve better synthesis quality over state-of-the-art
methods.";Hanxin Zhu<author:sep>Tianyu He<author:sep>Zhibo Chen;http://arxiv.org/pdf/2402.16407v1;cs.CV;"Accepted by IEEE Conference on Virtual Reality and 3D User Interfaces
  (IEEE VR 2024)";nerf
2402.16308v1;http://arxiv.org/abs/2402.16308v1;2024-02-26;DreamUp3D: Object-Centric Generative Models for Single-View 3D Scene  Understanding and Real-to-Sim Transfer;"3D scene understanding for robotic applications exhibits a unique set of
requirements including real-time inference, object-centric latent
representation learning, accurate 6D pose estimation and 3D reconstruction of
objects. Current methods for scene understanding typically rely on a
combination of trained models paired with either an explicit or learnt
volumetric representation, all of which have their own drawbacks and
limitations. We introduce DreamUp3D, a novel Object-Centric Generative Model
(OCGM) designed explicitly to perform inference on a 3D scene informed only by
a single RGB-D image. DreamUp3D is a self-supervised model, trained end-to-end,
and is capable of segmenting objects, providing 3D object reconstructions,
generating object-centric latent representations and accurate per-object 6D
pose estimates. We compare DreamUp3D to baselines including NeRFs, pre-trained
CLIP-features, ObSurf, and ObPose, in a range of tasks including 3D scene
reconstruction, object matching and object pose estimation. Our experiments
show that our model outperforms all baselines by a significant margin in
real-world scenarios displaying its applicability for 3D scene understanding
tasks while meeting the strict demands exhibited in robotics applications.";Yizhe Wu<author:sep>Haitz SÃ¡ez de OcÃ¡riz Borde<author:sep>Jack Collins<author:sep>Oiwi Parker Jones<author:sep>Ingmar Posner;http://arxiv.org/pdf/2402.16308v1;cs.RO;;nerf
2402.16936v1;http://arxiv.org/abs/2402.16936v1;2024-02-26;Disentangled 3D Scene Generation with Layout Learning;"We introduce a method to generate 3D scenes that are disentangled into their
component objects. This disentanglement is unsupervised, relying only on the
knowledge of a large pretrained text-to-image model. Our key insight is that
objects can be discovered by finding parts of a 3D scene that, when rearranged
spatially, still produce valid configurations of the same scene. Concretely,
our method jointly optimizes multiple NeRFs from scratch - each representing
its own object - along with a set of layouts that composite these objects into
scenes. We then encourage these composited scenes to be in-distribution
according to the image generator. We show that despite its simplicity, our
approach successfully generates 3D scenes decomposed into individual objects,
enabling new capabilities in text-to-3D content creation. For results and an
interactive demo, see our project page at https://dave.ml/layoutlearning/";Dave Epstein<author:sep>Ben Poole<author:sep>Ben Mildenhall<author:sep>Alexei A. Efros<author:sep>Aleksander Holynski;http://arxiv.org/pdf/2402.16936v1;cs.CV;;nerf
2402.17797v2;http://arxiv.org/abs/2402.17797v2;2024-02-26;Neural Radiance Fields in Medical Imaging: Challenges and Next Steps;"Neural Radiance Fields (NeRF), as a pioneering technique in computer vision,
offer great potential to revolutionize medical imaging by synthesizing
three-dimensional representations from the projected two-dimensional image
data. However, they face unique challenges when applied to medical
applications. This paper presents a comprehensive examination of applications
of NeRFs in medical imaging, highlighting four imminent challenges, including
fundamental imaging principles, inner structure requirement, object boundary
definition, and color density significance. We discuss current methods on
different organs and discuss related limitations. We also review several
datasets and evaluation metrics and propose several promising directions for
future research.";Xin Wang<author:sep>Shu Hu<author:sep>Heng Fan<author:sep>Hongtu Zhu<author:sep>Xin Li;http://arxiv.org/pdf/2402.17797v2;eess.IV;;nerf
2402.15870v1;http://arxiv.org/abs/2402.15870v1;2024-02-24;Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian  Splatting;"The recent advancements in 3D Gaussian splatting (3D-GS) have not only
facilitated real-time rendering through modern GPU rasterization pipelines but
have also attained state-of-the-art rendering quality. Nevertheless, despite
its exceptional rendering quality and performance on standard datasets, 3D-GS
frequently encounters difficulties in accurately modeling specular and
anisotropic components. This issue stems from the limited ability of spherical
harmonics (SH) to represent high-frequency information. To overcome this
challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic
spherical Gaussian (ASG) appearance field instead of SH for modeling the
view-dependent appearance of each 3D Gaussian. Additionally, we have developed
a coarse-to-fine training strategy to improve learning efficiency and eliminate
floaters caused by overfitting in real-world scenes. Our experimental results
demonstrate that our method surpasses existing approaches in terms of rendering
quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to
model scenes with specular and anisotropic components without increasing the
number of 3D Gaussians. This improvement extends the applicability of 3D GS to
handle intricate scenarios with specular and anisotropic surfaces.";Ziyi Yang<author:sep>Xinyu Gao<author:sep>Yangtian Sun<author:sep>Yihua Huang<author:sep>Xiaoyang Lyu<author:sep>Wen Zhou<author:sep>Shaohui Jiao<author:sep>Xiaojuan Qi<author:sep>Xiaogang Jin;http://arxiv.org/pdf/2402.15870v1;cs.CV;;gaussian splatting
2402.14464v1;http://arxiv.org/abs/2402.14464v1;2024-02-22;NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth  Supervision for Indoor Multi-View 3D Detection;"NeRF-Det has achieved impressive performance in indoor multi-view 3D
detection by innovatively utilizing NeRF to enhance representation learning.
Despite its notable performance, we uncover three decisive shortcomings in its
current design, including semantic ambiguity, inappropriate sampling, and
insufficient utilization of depth supervision. To combat the aforementioned
problems, we present three corresponding solutions: 1) Semantic Enhancement. We
project the freely available 3D segmentation annotations onto the 2D plane and
leverage the corresponding 2D semantic maps as the supervision signal,
significantly enhancing the semantic awareness of multi-view detectors. 2)
Perspective-aware Sampling. Instead of employing the uniform sampling strategy,
we put forward the perspective-aware sampling policy that samples densely near
the camera while sparsely in the distance, more effectively collecting the
valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to
directly regressing the depth values that are difficult to optimize, we divide
the depth range of each scene into a fixed number of ordinal bins and
reformulate the depth prediction as the combination of the classification of
depth bins as well as the regression of the residual depth values, thereby
benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has
exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets.
Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9%
in mAP@0.25 and +3.5% in mAP@0.50. The code will be publicly at
https://github.com/mrsempress/NeRF-Detplusplus.";Chenxi Huang<author:sep>Yuenan Hou<author:sep>Weicai Ye<author:sep>Di Huang<author:sep>Xiaoshui Huang<author:sep>Binbin Lin<author:sep>Deng Cai<author:sep>Wanli Ouyang;http://arxiv.org/pdf/2402.14464v1;cs.CV;7 pages, 2 figures;nerf
2402.14415v1;http://arxiv.org/abs/2402.14415v1;2024-02-22;TaylorGrid: Towards Fast and High-Quality Implicit Field Learning via  Direct Taylor-based Grid Optimization;"Coordinate-based neural implicit representation or implicit fields have been
widely studied for 3D geometry representation or novel view synthesis.
Recently, a series of efforts have been devoted to accelerating the speed and
improving the quality of the coordinate-based implicit field learning. Instead
of learning heavy MLPs to predict the neural implicit values for the query
coordinates, neural voxels or grids combined with shallow MLPs have been
proposed to achieve high-quality implicit field learning with reduced
optimization time. On the other hand, lightweight field representations such as
linear grid have been proposed to further improve the learning speed. In this
paper, we aim for both fast and high-quality implicit field learning, and
propose TaylorGrid, a novel implicit field representation which can be
efficiently computed via direct Taylor expansion optimization on 2D or 3D
grids. As a general representation, TaylorGrid can be adapted to different
implicit fields learning tasks such as SDF learning or NeRF. From extensive
quantitative and qualitative comparisons, TaylorGrid achieves a balance between
the linear grid and neural voxels, showing its superiority in fast and
high-quality implicit field learning.";Renyi Mao<author:sep>Qingshan Xu<author:sep>Peng Zheng<author:sep>Ye Wang<author:sep>Tieru Wu<author:sep>Rui Ma;http://arxiv.org/pdf/2402.14415v1;cs.CV;;nerf
2402.14586v2;http://arxiv.org/abs/2402.14586v2;2024-02-22;FrameNeRF: A Simple and Efficient Framework for Few-shot Novel View  Synthesis;"We present a novel framework, called FrameNeRF, designed to apply
off-the-shelf fast high-fidelity NeRF models with fast training speed and high
rendering quality for few-shot novel view synthesis tasks. The training
stability of fast high-fidelity models is typically constrained to dense views,
making them unsuitable for few-shot novel view synthesis tasks. To address this
limitation, we utilize a regularization model as a data generator to produce
dense views from sparse inputs, facilitating subsequent training of fast
high-fidelity models. Since these dense views are pseudo ground truth generated
by the regularization model, original sparse images are then used to fine-tune
the fast high-fidelity model. This process helps the model learn realistic
details and correct artifacts introduced in earlier stages. By leveraging an
off-the-shelf regularization model and a fast high-fidelity model, our approach
achieves state-of-the-art performance across various benchmark datasets.";Yan Xing<author:sep>Pan Wang<author:sep>Ligang Liu<author:sep>Daolun Li<author:sep>Li Zhang;http://arxiv.org/pdf/2402.14586v2;cs.CV;;nerf
2402.14792v1;http://arxiv.org/abs/2402.14792v1;2024-02-22;Consolidating Attention Features for Multi-view Image Editing;"Large-scale text-to-image models enable a wide range of image editing
techniques, using text prompts or even spatial controls. However, applying
these editing methods to multi-view images depicting a single scene leads to
3D-inconsistent results. In this work, we focus on spatial control-based
geometric manipulations and introduce a method to consolidate the editing
process across various views. We build on two insights: (1) maintaining
consistent features throughout the generative process helps attain consistency
in multi-view editing, and (2) the queries in self-attention layers
significantly influence the image structure. Hence, we propose to improve the
geometric consistency of the edited images by enforcing the consistency of the
queries. To do so, we introduce QNeRF, a neural radiance field trained on the
internal query features of the edited images. Once trained, QNeRF can render
3D-consistent queries, which are then softly injected back into the
self-attention layers during generation, greatly improving multi-view
consistency. We refine the process through a progressive, iterative method that
better consolidates queries across the diffusion timesteps. We compare our
method to a range of existing techniques and demonstrate that it can achieve
better multi-view consistency and higher fidelity to the input scene. These
advantages allow us to train NeRFs with fewer visual artifacts, that are better
aligned with the target geometry.";Or Patashnik<author:sep>Rinon Gal<author:sep>Daniel Cohen-Or<author:sep>Jun-Yan Zhu<author:sep>Fernando De la Torre;http://arxiv.org/pdf/2402.14792v1;cs.CV;"Project Page at
  https://qnerf-consolidation.github.io/qnerf-consolidation/";nerf
2402.14196v1;http://arxiv.org/abs/2402.14196v1;2024-02-22;Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields;"Despite the remarkable achievements of neural radiance fields (NeRF) in
representing 3D scenes and generating novel view images, the aliasing issue,
rendering ""jaggies"" or ""blurry"" images at varying camera distances, remains
unresolved in most existing approaches. The recently proposed mip-NeRF has
addressed this challenge by rendering conical frustums instead of rays.
However, it relies on MLP architecture to represent the radiance fields,
missing out on the fast training speed offered by the latest grid-based
methods. In this work, we present mip-Grid, a novel approach that integrates
anti-aliasing techniques into grid-based representations for radiance fields,
mitigating the aliasing artifacts while enjoying fast training time. The
proposed method generates multi-scale grids by applying simple convolution
operations over a shared grid representation and uses the scale-aware
coordinate to retrieve features at different scales from the generated
multi-scale grids. To test the effectiveness, we integrated the proposed method
into the two recent representative grid-based methods, TensoRF and K-Planes.
Experimental results demonstrate that mip-Grid greatly improves the rendering
performance of both methods and even outperforms mip-NeRF on multi-scale
datasets while achieving significantly faster training time. For code and demo
videos, please see https://stnamjef.github.io/mipgrid.github.io/.";Seungtae Nam<author:sep>Daniel Rho<author:sep>Jong Hwan Ko<author:sep>Eunbyung Park;http://arxiv.org/pdf/2402.14196v1;cs.CV;Accepted to NeurIPS 2023;nerf
2402.14650v1;http://arxiv.org/abs/2402.14650v1;2024-02-22;GaussianPro: 3D Gaussian Splatting with Progressive Propagation;"The advent of 3D Gaussian Splatting (3DGS) has recently brought about a
revolution in the field of neural rendering, facilitating high-quality
renderings at real-time speed. However, 3DGS heavily depends on the initialized
point cloud produced by Structure-from-Motion (SfM) techniques. When tackling
with large-scale scenes that unavoidably contain texture-less surfaces, the SfM
techniques always fail to produce enough points in these surfaces and cannot
provide good initialization for 3DGS. As a result, 3DGS suffers from difficult
optimization and low-quality renderings. In this paper, inspired by classical
multi-view stereo (MVS) techniques, we propose GaussianPro, a novel method that
applies a progressive propagation strategy to guide the densification of the 3D
Gaussians. Compared to the simple split and clone strategies used in 3DGS, our
method leverages the priors of the existing reconstructed geometries of the
scene and patch matching techniques to produce new Gaussians with accurate
positions and orientations. Experiments on both large-scale and small-scale
scenes validate the effectiveness of our method, where our method significantly
surpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in
terms of PSNR.";Kai Cheng<author:sep>Xiaoxiao Long<author:sep>Kaizhi Yang<author:sep>Yao Yao<author:sep>Wei Yin<author:sep>Yuexin Ma<author:sep>Wenping Wang<author:sep>Xuejin Chen;http://arxiv.org/pdf/2402.14650v1;cs.CV;"See the project page for code, data:
  https://kcheng1021.github.io/gaussianpro.github.io";gaussian splatting
2402.13827v1;http://arxiv.org/abs/2402.13827v1;2024-02-21;Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering  of 3D Gaussian Splatting;"3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms
the neural radiance field (NeRF) in terms of both speed and image quality.
3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects
these Gaussians onto the 2D image plane for rendering. However, during the
rendering process, a substantial number of unnecessary 3D Gaussians exist for
the current view direction, resulting in significant computation costs
associated with their identification. In this paper, we propose a computational
reduction technique that quickly identifies unnecessary 3D Gaussians in
real-time for rendering the current view without compromising image quality.
This is accomplished through the offline clustering of 3D Gaussians that are
close in distance, followed by the projection of these clusters onto a 2D image
plane during runtime. Additionally, we analyze the bottleneck associated with
the proposed technique when executed on GPUs and propose an efficient hardware
architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360
dataset, the proposed technique excludes 63% of 3D Gaussians on average before
the 2D image projection, which reduces the overall rendering computation by
almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The
proposed accelerator also achieves a speedup of 10.7x compared to a GPU.";Joongho Jo<author:sep>Hyeongwon Kim<author:sep>Jongsun Park;http://arxiv.org/pdf/2402.13827v1;cs.CV;;gaussian splatting<tag:sep>nerf
2402.13510v1;http://arxiv.org/abs/2402.13510v1;2024-02-21;SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural  Radiance Fields;"The widespread adoption of implicit neural representations, especially Neural
Radiance Fields (NeRF), highlights a growing need for editing capabilities in
implicit 3D models, essential for tasks like scene post-processing and 3D
content creation. Despite previous efforts in NeRF editing, challenges remain
due to limitations in editing flexibility and quality. The key issue is
developing a neural representation that supports local edits for real-time
updates. Current NeRF editing methods, offering pixel-level adjustments or
detailed geometry and color modifications, are mostly limited to static scenes.
This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level
editing in dynamic settings, specifically targeting the D-NeRF network. It
allows for consistent edits across sequences by mapping editing actions to a
specific timeframe, freezing the deformation network responsible for dynamic
scene representation, and using a teacher-student approach to integrate
changes.";Zhentao Huang<author:sep>Yukun Shi<author:sep>Neil Bruce<author:sep>Minglun Gong;http://arxiv.org/pdf/2402.13510v1;cs.CV;;nerf
2402.13255v1;http://arxiv.org/abs/2402.13255v1;2024-02-20;How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey;"Over the past two decades, research in the field of Simultaneous Localization
and Mapping (SLAM) has undergone a significant evolution, highlighting its
critical role in enabling autonomous exploration of unknown environments. This
evolution ranges from hand-crafted methods, through the era of deep learning,
to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D
Gaussian Splatting (3DGS) representations. Recognizing the growing body of
research and the absence of a comprehensive survey on the topic, this paper
aims to provide the first comprehensive overview of SLAM progress through the
lens of the latest advancements in radiance fields. It sheds light on the
background, evolutionary path, inherent strengths and limitations, and serves
as a fundamental reference to highlight the dynamic progress and specific
challenges.";Fabio Tosi<author:sep>Youmin Zhang<author:sep>Ziren Gong<author:sep>Erik SandstrÃ¶m<author:sep>Stefano Mattoccia<author:sep>Martin R. Oswald<author:sep>Matteo Poggi;http://arxiv.org/pdf/2402.13255v1;cs.CV;;gaussian splatting<tag:sep>nerf
2402.13226v2;http://arxiv.org/abs/2402.13226v2;2024-02-20;NeRF Solves Undersampled MRI Reconstruction;"This article presents a novel undersampled magnetic resonance imaging (MRI)
technique that leverages the concept of Neural Radiance Field (NeRF). With
radial undersampling, the corresponding imaging problem can be reformulated
into an image modeling task from sparse-view rendered data; therefore, a high
dimensional MR image is obtainable from undersampled k-space data by taking
advantage of implicit neural representation. A multi-layer perceptron, which is
designed to output an image intensity from a spatial coordinate, learns the MR
physics-driven rendering relation between given measurement data and desired
image. Effective undersampling strategies for high-quality neural
representation are investigated. The proposed method serves two benefits: (i)
The learning is based fully on single undersampled k-space data, not a bunch of
measured data and target image sets. It can be used potentially for diagnostic
MR imaging, such as fetal MRI, where data acquisition is relatively rare or
limited against diversity of clinical images while undersampled reconstruction
is highly demanded. (ii) A reconstructed MR image is a scan-specific
representation highly adaptive to the given k-space measurement. Numerous
experiments validate the feasibility and capability of the proposed approach.";Tae Jun Jang<author:sep>Chang Min Hyun;http://arxiv.org/pdf/2402.13226v2;eess.IV;;nerf
2402.13252v1;http://arxiv.org/abs/2402.13252v1;2024-02-20;Improving Robustness for Joint Optimization of Camera Poses and  Decomposed Low-Rank Tensorial Radiance Fields;"In this paper, we propose an algorithm that allows joint refinement of camera
pose and scene geometry represented by decomposed low-rank tensor, using only
2D images as supervision. First, we conduct a pilot study based on a 1D signal
and relate our findings to 3D scenarios, where the naive joint pose
optimization on voxel-based NeRFs can easily lead to sub-optimal solutions.
Moreover, based on the analysis of the frequency spectrum, we propose to apply
convolutional Gaussian filters on 2D and 3D radiance fields for a
coarse-to-fine training schedule that enables joint camera pose optimization.
Leveraging the decomposition property in decomposed low-rank tensor, our method
achieves an equivalent effect to brute-force 3D convolution with only incurring
little computational overhead. To further improve the robustness and stability
of joint optimization, we also propose techniques of smoothed 2D supervision,
randomly scaled kernel parameters, and edge-guided loss mask. Extensive
quantitative and qualitative evaluations demonstrate that our proposed
framework achieves superior performance in novel view synthesis as well as
rapid convergence for optimization.";Bo-Yu Cheng<author:sep>Wei-Chen Chiu<author:sep>Yu-Lun Liu;http://arxiv.org/pdf/2402.13252v1;cs.CV;"AAAI 2024. Project page:
  https://alex04072000.github.io/Joint-TensoRF/";nerf
2402.12792v1;http://arxiv.org/abs/2402.12792v1;2024-02-20;OccFlowNet: Towards Self-supervised Occupancy Estimation via  Differentiable Rendering and Occupancy Flow;"Semantic occupancy has recently gained significant traction as a prominent 3D
scene representation. However, most existing methods rely on large and costly
datasets with fine-grained 3D voxel labels for training, which limits their
practicality and scalability, increasing the need for self-monitored learning
in this domain. In this work, we present a novel approach to occupancy
estimation inspired by neural radiance field (NeRF) using only 2D labels, which
are considerably easier to acquire. In particular, we employ differentiable
volumetric rendering to predict depth and semantic maps and train a 3D network
based on 2D supervision only. To enhance geometric accuracy and increase the
supervisory signal, we introduce temporal rendering of adjacent time steps.
Additionally, we introduce occupancy flow as a mechanism to handle dynamic
objects in the scene and ensure their temporal consistency. Through extensive
experimentation we demonstrate that 2D supervision only is sufficient to
achieve state-of-the-art performance compared to methods using 3D labels, while
outperforming concurrent 2D approaches. When combining 2D supervision with 3D
labels, temporal rendering and occupancy flow we outperform all previous
occupancy estimation models significantly. We conclude that the proposed
rendering supervision and occupancy flow advances occupancy estimation and
further bridges the gap towards self-supervised learning in this domain.";Simon Boeder<author:sep>Fabian Gigengack<author:sep>Benjamin Risse;http://arxiv.org/pdf/2402.12792v1;cs.CV;;nerf
2402.12184v1;http://arxiv.org/abs/2402.12184v1;2024-02-19;Colorizing Monochromatic Radiance Fields;"Though Neural Radiance Fields (NeRF) can produce colorful 3D representations
of the world by using a set of 2D images, such ability becomes non-existent
when only monochromatic images are provided. Since color is necessary in
representing the world, reproducing color from monochromatic radiance fields
becomes crucial. To achieve this goal, instead of manipulating the
monochromatic radiance fields directly, we consider it as a
representation-prediction task in the Lab color space. By first constructing
the luminance and density representation using monochromatic images, our
prediction stage can recreate color representation on the basis of an image
colorization module. We then reproduce a colorful implicit model through the
representation of luminance, density, and color. Extensive experiments have
been conducted to validate the effectiveness of our approaches. Our project
page: https://liquidammonia.github.io/color-nerf.";Yean Cheng<author:sep>Renjie Wan<author:sep>Shuchen Weng<author:sep>Chengxuan Zhu<author:sep>Yakun Chang<author:sep>Boxin Shi;http://arxiv.org/pdf/2402.12184v1;cs.CV;;nerf
2402.12377v1;http://arxiv.org/abs/2402.12377v1;2024-02-19;Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based  View Synthesis;"While surface-based view synthesis algorithms are appealing due to their low
computational requirements, they often struggle to reproduce thin structures.
In contrast, more expensive methods that model the scene's geometry as a
volumetric density field (e.g. NeRF) excel at reconstructing fine geometric
detail. However, density fields often represent geometry in a ""fuzzy"" manner,
which hinders exact localization of the surface. In this work, we modify
density fields to encourage them to converge towards surfaces, without
compromising their ability to reconstruct thin structures. First, we employ a
discrete opacity grid representation instead of a continuous density field,
which allows opacity values to discontinuously transition from zero to one at
the surface. Second, we anti-alias by casting multiple rays per pixel, which
allows occlusion boundaries and subpixel structures to be modelled without
using semi-transparent voxels. Third, we minimize the binary entropy of the
opacity values, which facilitates the extraction of surface geometry by
encouraging opacity values to binarize towards the end of training. Lastly, we
develop a fusion-based meshing strategy followed by mesh simplification and
appearance model fitting. The compact meshes produced by our model can be
rendered in real-time on mobile devices and achieve significantly higher view
synthesis quality compared to existing mesh-based approaches.";Christian Reiser<author:sep>Stephan Garbin<author:sep>Pratul P. Srinivasan<author:sep>Dor Verbin<author:sep>Richard Szeliski<author:sep>Ben Mildenhall<author:sep>Jonathan T. Barron<author:sep>Peter Hedman<author:sep>Andreas Geiger;http://arxiv.org/pdf/2402.12377v1;cs.CV;Project page at https://binary-opacity-grid.github.io;nerf
2402.11141v1;http://arxiv.org/abs/2402.11141v1;2024-02-17;Semantically-aware Neural Radiance Fields for Visual Scene  Understanding: A Comprehensive Review;"This review thoroughly examines the role of semantically-aware Neural
Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of
over 250 scholarly papers. It explores how NeRFs adeptly infer 3D
representations for both stationary and dynamic objects in a scene. This
capability is pivotal for generating high-quality new viewpoints, completing
missing scene details (inpainting), conducting comprehensive scene segmentation
(panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and
extracting object-centric 3D models. A significant aspect of this study is the
application of semantic labels as viewpoint-invariant functions, which
effectively map spatial coordinates to a spectrum of semantic labels, thus
facilitating the recognition of distinct objects within the scene. Overall,
this survey highlights the progression and diverse applications of
semantically-aware neural radiance fields in the context of visual scene
interpretation.";Thang-Anh-Quan Nguyen<author:sep>Amine Bourki<author:sep>MÃ¡tyÃ¡s Macudzinski<author:sep>Anthony Brunel<author:sep>Mohammed Bennamoun;http://arxiv.org/pdf/2402.11141v1;cs.CV;;nerf
2402.10344v1;http://arxiv.org/abs/2402.10344v1;2024-02-15;Evaluating NeRFs for 3D Plant Geometry Reconstruction in Field  Conditions;"We evaluate different Neural Radiance Fields (NeRFs) techniques for
reconstructing (3D) plants in varied environments, from indoor settings to
outdoor fields. Traditional techniques often struggle to capture the complex
details of plants, which is crucial for botanical and agricultural
understanding. We evaluate three scenarios with increasing complexity and
compare the results with the point cloud obtained using LiDAR as ground truth
data. In the most realistic field scenario, the NeRF models achieve a 74.65% F1
score with 30 minutes of training on the GPU, highlighting the efficiency and
accuracy of NeRFs in challenging environments. These findings not only
demonstrate the potential of NeRF in detailed and realistic 3D plant modeling
but also suggest practical approaches for enhancing the speed and efficiency of
the 3D reconstruction process.";Muhammad Arbab Arshad<author:sep>Talukder Jubery<author:sep>James Afful<author:sep>Anushrut Jignasu<author:sep>Aditya Balu<author:sep>Baskar Ganapathysubramanian<author:sep>Soumik Sarkar<author:sep>Adarsh Krishnamurthy;http://arxiv.org/pdf/2402.10344v1;cs.CV;;nerf
2402.10128v1;http://arxiv.org/abs/2402.10128v1;2024-02-15;GES: Generalized Exponential Splatting for Efficient Radiance Field  Rendering;"Advancements in 3D Gaussian Splatting have significantly accelerated 3D
reconstruction and generation. However, it may require a large number of
Gaussians, which creates a substantial memory footprint. This paper introduces
GES (Generalized Exponential Splatting), a novel representation that employs
Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer
particles to represent a scene and thus significantly outperforming Gaussian
Splatting methods in efficiency with a plug-and-play replacement ability for
Gaussian-based utilities. GES is validated theoretically and empirically in
both principled 1D setup and realistic 3D scenes.
  It is shown to represent signals with sharp edges more accurately, which are
typically challenging for Gaussians due to their inherent low-pass
characteristics. Our empirical analysis demonstrates that GEF outperforms
Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and
parabolic signals), thereby reducing the need for extensive splitting
operations that increase the memory footprint of Gaussian Splatting. With the
aid of a frequency-modulated loss, GES achieves competitive performance in
novel-view synthesis benchmarks while requiring less than half the memory
storage of Gaussian Splatting and increasing the rendering speed by up to 39%.
The code is available on the project website https://abdullahamdi.com/ges .";Abdullah Hamdi<author:sep>Luke Melas-Kyriazi<author:sep>Guocheng Qian<author:sep>Jinjie Mai<author:sep>Ruoshi Liu<author:sep>Carl Vondrick<author:sep>Bernard Ghanem<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2402.10128v1;cs.CV;preprint;gaussian splatting
2402.10259v2;http://arxiv.org/abs/2402.10259v2;2024-02-15;GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object  with Gaussian Splatting;"Reconstructing and rendering 3D objects from highly sparse views is of
critical importance for promoting applications of 3D vision techniques and
improving user experience. However, images from sparse views only contain very
limited 3D information, leading to two significant challenges: 1) Difficulty in
building multi-view consistency as images for matching are too few; 2)
Partially omitted or highly compressed object information as view coverage is
insufficient. To tackle these challenges, we propose GaussianObject, a
framework to represent and render the 3D object with Gaussian splatting, that
achieves high rendering quality with only 4 input images. We first introduce
techniques of visual hull and floater elimination which explicitly inject
structure priors into the initial optimization process for helping build
multi-view consistency, yielding a coarse 3D Gaussian representation. Then we
construct a Gaussian repair model based on diffusion models to supplement the
omitted object information, where Gaussians are further refined. We design a
self-generating strategy to obtain image pairs for training the repair model.
Our GaussianObject is evaluated on several challenging datasets, including
MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction
results from only 4 views and significantly outperforming previous
state-of-the-art methods.";Chen Yang<author:sep>Sikuang Li<author:sep>Jiemin Fang<author:sep>Ruofan Liang<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wei Shen<author:sep>Qi Tian;http://arxiv.org/pdf/2402.10259v2;cs.CV;Project page: https://gaussianobject.github.io/;gaussian splatting<tag:sep>nerf
2402.09325v1;http://arxiv.org/abs/2402.09325v1;2024-02-14;PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames  in Autonomous Driving Environments;"Large-scale 3D scene reconstruction and novel view synthesis are vital for
autonomous vehicles, especially utilizing temporally sparse LiDAR frames.
However, conventional explicit representations remain a significant bottleneck
towards representing the reconstructed and synthetic scenes at unlimited
resolution. Although the recently developed neural radiance fields (NeRF) have
shown compelling results in implicit representations, the problem of
large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR
frames remains unexplored. To bridge this gap, we propose a 3D scene
reconstruction and novel view synthesis framework called parent-child neural
radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF,
the framework implements hierarchical spatial partitioning and multi-level
scene representation, including scene, segment, and point levels. The
multi-level scene representation enhances the efficient utilization of sparse
LiDAR point cloud data and enables the rapid acquisition of an approximate
volumetric scene representation. With extensive experiments, PC-NeRF is proven
to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in
large-scale scenes. Moreover, PC-NeRF can effectively handle situations with
sparse LiDAR frames and demonstrate high deployment efficiency with limited
training epochs. Our approach implementation and the pre-trained models are
available at https://github.com/biter0088/pc-nerf.";Xiuzhong Hu<author:sep>Guangming Xiong<author:sep>Zheng Zang<author:sep>Peng Jia<author:sep>Yuxuan Han<author:sep>Junyi Ma;http://arxiv.org/pdf/2402.09325v1;cs.CV;arXiv admin note: substantial text overlap with arXiv:2310.00874;nerf
2402.08622v1;http://arxiv.org/abs/2402.08622v1;2024-02-13;NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs;"A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry
and appearance of a scene. We here ask the question whether we can transfer the
appearance from a source NeRF onto a target 3D geometry in a semantically
meaningful way, such that the resulting new NeRF retains the target geometry
but has an appearance that is an analogy to the source NeRF. To this end, we
generalize classic image analogies from 2D images to NeRFs. We leverage
correspondence transfer along semantic affinity that is driven by semantic
features from large, pre-trained 2D image models to achieve multi-view
consistent appearance transfer. Our method allows exploring the mix-and-match
product space of 3D geometry and appearance. We show that our method
outperforms traditional stylization-based methods and that a large majority of
users prefer our method over several typical baselines.";Michael Fischer<author:sep>Zhengqin Li<author:sep>Thu Nguyen-Phuoc<author:sep>Aljaz Bozic<author:sep>Zhao Dong<author:sep>Carl Marshall<author:sep>Tobias Ritschel;http://arxiv.org/pdf/2402.08622v1;cs.CV;Project page: https://mfischer-ucl.github.io/nerf_analogies/;nerf
2402.08138v1;http://arxiv.org/abs/2402.08138v1;2024-02-13;H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object  Surface Fields;"Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance
Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D
indoor scene reconstruction. We introduce a novel two-phase learning approach,
H2O-SDF, that discriminates between object and non-object regions within indoor
environments. This method achieves a nuanced balance, carefully preserving the
geometric integrity of room layouts while also capturing intricate surface
details of specific objects. A cornerstone of our two-phase learning framework
is the introduction of the Object Surface Field (OSF), a novel concept designed
to mitigate the persistent vanishing gradient problem that has previously
hindered the capture of high-frequency details in other methods. Our proposed
approach is validated through several experiments that include ablation
studies.";Minyoung Park<author:sep>Mirae Do<author:sep>YeonJae Shin<author:sep>Jaeseok Yoo<author:sep>Jongkwang Hong<author:sep>Joongrock Kim<author:sep>Chul Lee;http://arxiv.org/pdf/2402.08138v1;cs.CV;;nerf
2402.08784v1;http://arxiv.org/abs/2402.08784v1;2024-02-13;Preconditioners for the Stochastic Training of Implicit Neural  Representations;"Implicit neural representations have emerged as a powerful technique for
encoding complex continuous multidimensional signals as neural networks,
enabling a wide range of applications in computer vision, robotics, and
geometry. While Adam is commonly used for training due to its stochastic
proficiency, it entails lengthy training durations. To address this, we explore
alternative optimization techniques for accelerated training without
sacrificing accuracy. Traditional second-order optimizers like L-BFGS are
suboptimal in stochastic settings, making them unsuitable for large-scale data
sets. Instead, we propose stochastic training using curvature-aware diagonal
preconditioners, showcasing their effectiveness across various signal
modalities such as images, shape reconstruction, and Neural Radiance Fields
(NeRF).";Shin-Fang Chng<author:sep>Hemanth Saratchandran<author:sep>Simon Lucey;http://arxiv.org/pdf/2402.08784v1;cs.CV;The first two authors contributed equally;nerf
2402.08682v1;http://arxiv.org/abs/2402.08682v1;2024-02-13;IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality  3D Generation;"Most text-to-3D generators build upon off-the-shelf text-to-image models
trained on billions of images. They use variants of Score Distillation Sampling
(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation
is to fine-tune the 2D generator to be multi-view aware, which can help
distillation or can be combined with reconstruction networks to output 3D
objects directly. In this paper, we further explore the design space of
text-to-3D models. We significantly improve multi-view generation by
considering video instead of image generators. Combined with a 3D
reconstruction algorithm which, by using Gaussian splatting, can optimize a
robust image-based loss, we directly produce high-quality 3D outputs from the
generated views. Our new method, IM-3D, reduces the number of evaluations of
the 2D generator network 10-100x, resulting in a much more efficient pipeline,
better quality, fewer geometric inconsistencies, and higher yield of usable 3D
assets.";Luke Melas-Kyriazi<author:sep>Iro Laina<author:sep>Christian Rupprecht<author:sep>Natalia Neverova<author:sep>Andrea Vedaldi<author:sep>Oran Gafni<author:sep>Filippos Kokkinos;http://arxiv.org/pdf/2402.08682v1;cs.CV;;gaussian splatting
2402.07648v1;http://arxiv.org/abs/2402.07648v1;2024-02-12;DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable  Object Manipulation;"Manipulating deformable objects is a ubiquitous task in household
environments, demanding adequate representation and accurate dynamics
prediction due to the objects' infinite degrees of freedom. This work proposes
DeformNet, which utilizes latent space modeling with a learned 3D
representation model to tackle these challenges effectively. The proposed
representation model combines a PointNet encoder and a conditional neural
radiance field (NeRF), facilitating a thorough acquisition of object
deformations and variations in lighting conditions. To model the complex
dynamics, we employ a recurrent state-space model (RSSM) that accurately
predicts the transformation of the latent representation over time. Extensive
simulation experiments with diverse objectives demonstrate the generalization
capabilities of DeformNet for various deformable object manipulation tasks,
even in the presence of previously unseen goals. Finally, we deploy DeformNet
on an actual UR5 robotic arm to demonstrate its capability in real-world
scenarios.";Chenchang Li<author:sep>Zihao Ai<author:sep>Tong Wu<author:sep>Xiaosa Li<author:sep>Wenbo Ding<author:sep>Huazhe Xu;http://arxiv.org/pdf/2402.07648v1;cs.RO;"7 pages, Submitted to 2024 IEEE International Conference on Robotics
  and Automation (ICRA), Japan, Yokohama";nerf
2402.07181v1;http://arxiv.org/abs/2402.07181v1;2024-02-11;3D Gaussian as a New Vision Era: A Survey;"3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the
field of Computer Graphics, offering explicit scene representation and novel
view synthesis without the reliance on neural networks, such as Neural Radiance
Fields (NeRF). This technique has found diverse applications in areas such as
robotics, urban mapping, autonomous navigation, and virtual reality/augmented
reality, just name a few. Given the growing popularity and expanding research
in 3D Gaussian Splatting, this paper presents a comprehensive survey of
relevant papers from the past year. We organize the survey into taxonomies
based on characteristics and applications, providing an introduction to the
theoretical underpinnings of 3D Gaussian Splatting. Our goal through this
survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a
valuable reference for seminal works in the field, and inspire future research
directions, as discussed in our concluding section.";Ben Fei<author:sep>Jingyi Xu<author:sep>Rui Zhang<author:sep>Qingyuan Zhou<author:sep>Weidong Yang<author:sep>Ying He;http://arxiv.org/pdf/2402.07181v1;cs.CV;;gaussian splatting<tag:sep>nerf
2402.07207v1;http://arxiv.org/abs/2402.07207v1;2024-02-11;GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided  Generative Gaussian Splatting;"We present GALA3D, generative 3D GAussians with LAyout-guided control, for
effective compositional text-to-3D generation. We first utilize large language
models (LLMs) to generate the initial layout and introduce a layout-guided 3D
Gaussian representation for 3D content generation with adaptive geometric
constraints. We then propose an object-scene compositional optimization
mechanism with conditioned diffusion to collaboratively generate realistic 3D
scenes with consistent geometry, texture, scale, and accurate interactions
among multiple objects while simultaneously adjusting the coarse layout priors
extracted from the LLMs to align with the generated scene. Experiments show
that GALA3D is a user-friendly, end-to-end framework for state-of-the-art
scene-level 3D content generation and controllable editing while ensuring the
high fidelity of object-level entities within the scene. Source codes and
models will be available at https://gala3d.github.io/.";Xiaoyu Zhou<author:sep>Xingjian Ran<author:sep>Yajiao Xiong<author:sep>Jinlin He<author:sep>Zhiwei Lin<author:sep>Yongtao Wang<author:sep>Deqing Sun<author:sep>Ming-Hsuan Yang;http://arxiv.org/pdf/2402.07207v1;cs.CV;;gaussian splatting
2402.07310v1;http://arxiv.org/abs/2402.07310v1;2024-02-11;BioNeRF: Biologically Plausible Neural Radiance Fields for View  Synthesis;"This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.";Leandro A. Passos<author:sep>Douglas Rodrigues<author:sep>Danilo Jodas<author:sep>Kelton A. P. Costa<author:sep>JoÃ£o Paulo Papa;http://arxiv.org/pdf/2402.07310v1;cs.CV;;nerf
2402.06390v1;http://arxiv.org/abs/2402.06390v1;2024-02-09;ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake  Generation using NeRF and Gaussian Splatting;"Numerous emerging deep-learning techniques have had a substantial impact on
computer graphics. Among the most promising breakthroughs are the recent rise
of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the
object's shape and color in neural network weights using a handful of images
with known camera positions to generate novel views. In contrast, GS provides
accelerated training and inference without a decrease in rendering quality by
encoding the object's characteristics in a collection of Gaussian
distributions. These two techniques have found many use cases in spatial
computing and other domains. On the other hand, the emergence of deepfake
methods has sparked considerable controversy. Such techniques can have a form
of artificial intelligence-generated videos that closely mimic authentic
footage. Using generative models, they can modify facial features, enabling the
creation of altered identities or facial expressions that exhibit a remarkably
realistic appearance to a real person. Despite these controversies, deepfake
can offer a next-generation solution for avatar creation and gaming when of
desirable quality. To that end, we show how to combine all these emerging
technologies to obtain a more plausible outcome. Our ImplicitDeepfake1 uses the
classical deepfake algorithm to modify all training images separately and then
train NeRF and GS on modified faces. Such relatively simple strategies can
produce plausible 3D deepfake-based avatars.";Georgii Stanishevskii<author:sep>Jakub Steczkiewicz<author:sep>Tomasz Szczepanik<author:sep>SÅawomir Tadeja<author:sep>Jacek Tabor<author:sep>PrzemysÅaw Spurek;http://arxiv.org/pdf/2402.06390v1;cs.CV;;gaussian splatting<tag:sep>nerf
2402.06149v1;http://arxiv.org/abs/2402.06149v1;2024-02-09;HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting;"Creating digital avatars from textual prompts has long been a desirable yet
challenging task. Despite the promising outcomes obtained through 2D diffusion
priors in recent works, current methods face challenges in achieving
high-quality and animated avatars effectively. In this paper, we present
$\textbf{HeadStudio}$, a novel framework that utilizes 3D Gaussian splatting to
generate realistic and animated avatars from text prompts. Our method drives 3D
Gaussians semantically to create a flexible and achievable appearance through
the intermediate FLAME representation. Specifically, we incorporate the FLAME
into both 3D representation and score distillation: 1) FLAME-based 3D Gaussian
splatting, driving 3D Gaussian points by rigging each point to a FLAME mesh. 2)
FLAME-based score distillation sampling, utilizing FLAME-based fine-grained
control signal to guide score distillation from the text prompt. Extensive
experiments demonstrate the efficacy of HeadStudio in generating animatable
avatars from textual prompts, exhibiting visually appealing appearances. The
avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel
views at a resolution of 1024. They can be smoothly controlled by real-world
speech and video. We hope that HeadStudio can advance digital avatar creation
and that the present method can widely be applied across various domains.";Zhenglin Zhou<author:sep>Fan Ma<author:sep>Hehe Fan<author:sep>Yi Yang;http://arxiv.org/pdf/2402.06149v1;cs.CV;9 pages, 8 figures;gaussian splatting
2402.06198v2;http://arxiv.org/abs/2402.06198v2;2024-02-09;GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D  Pretraining from Real-World Data;"3D Shape represented as point cloud has achieve advancements in multimodal
pre-training to align image and language descriptions, which is curial to
object identification, classification, and retrieval. However, the discrete
representations of point cloud lost the object's surface shape information and
creates a gap between rendering results and 2D correspondences. To address this
problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D
Gaussian Splatting) into multimodal pre-training to enhance 3D representation.
GS-CLIP leverages a pre-trained vision-language model for a learned common
visual and textual space on massive real world image-text pairs and then learns
a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel
Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature.
As a general framework for language-image-3D pre-training, GS-CLIP is agnostic
to 3D backbone networks. Experiments on challenging shows that GS-CLIP
significantly improves the state-of-the-art, outperforming the previously best
results.";Haoyuan Li<author:sep>Yanpeng Zhou<author:sep>Yihan Zeng<author:sep>Hang Xu<author:sep>Xiaodan Liang;http://arxiv.org/pdf/2402.06198v2;cs.CV;"The content of the technical report needs to be updated and retracted
  to avoid other impacts";gaussian splatting
2402.04796v1;http://arxiv.org/abs/2402.04796v1;2024-02-07;Mesh-based Gaussian Splatting for Real-time Large-scale Deformation;"Neural implicit representations, including Neural Distance Fields and Neural
Radiance Fields, have demonstrated significant capabilities for reconstructing
surfaces with complicated geometry and topology, and generating novel views of
a scene. Nevertheless, it is challenging for users to directly deform or
manipulate these implicit representations with large deformations in the
real-time fashion. Gaussian Splatting(GS) has recently become a promising
method with explicit geometry for representing static scenes and facilitating
high-quality and real-time synthesis of novel views. However,it cannot be
easily deformed due to the use of discrete Gaussians and lack of explicit
topology. To address this, we develop a novel GS-based method that enables
interactive deformation. Our key idea is to design an innovative mesh-based GS
representation, which is integrated into Gaussian learning and manipulation. 3D
Gaussians are defined over an explicit mesh, and they are bound with each
other: the rendering of 3D Gaussians guides the mesh face split for adaptive
refinement, and the mesh face split directs the splitting of 3D Gaussians.
Moreover, the explicit mesh constraints help regularize the Gaussian
distribution, suppressing poor-quality Gaussians(e.g. misaligned
Gaussians,long-narrow shaped Gaussians), thus enhancing visual quality and
avoiding artifacts during deformation. Based on this representation, we further
introduce a large-scale Gaussian deformation technique to enable deformable GS,
which alters the parameters of 3D Gaussians according to the manipulation of
the associated mesh. Our method benefits from existing mesh deformation
datasets for more realistic data-driven Gaussian deformation. Extensive
experiments show that our approach achieves high-quality reconstruction and
effective deformation, while maintaining the promising rendering results at a
high frame rate(65 FPS on average).";Lin Gao<author:sep>Jie Yang<author:sep>Bo-Tao Zhang<author:sep>Jia-Mu Sun<author:sep>Yu-Jie Yuan<author:sep>Hongbo Fu<author:sep>Yu-Kun Lai;http://arxiv.org/pdf/2402.04796v1;cs.GR;11 pages, 7 figures;gaussian splatting
2402.04648v1;http://arxiv.org/abs/2402.04648v1;2024-02-07;OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language  Foundation Models for 3D Semantic Understanding;"The development of Neural Radiance Fields (NeRFs) has provided a potent
representation for encapsulating the geometric and appearance characteristics
of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D
semantic perception tasks has been a recent focus. However, current methods
that extract semantics directly from Contrastive Language-Image Pretraining
(CLIP) for semantic field learning encounter difficulties due to noisy and
view-inconsistent semantics provided by CLIP. To tackle these limitations, we
propose OV-NeRF, which exploits the potential of pre-trained vision and
language foundation models to enhance semantic field learning through proposed
single-view and cross-view strategies. First, from the single-view perspective,
we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask
proposals derived from SAM to rectify the noisy semantics of each training
view, facilitating accurate semantic field learning. Second, from the
cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy
to address the challenge raised by view-inconsistent semantics. Rather than
invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the
3D consistent semantics generated from the well-trained semantic field itself
for semantic field training, aiming to reduce ambiguity and enhance overall
semantic consistency across different views. Extensive experiments validate our
OV-NeRF outperforms current state-of-the-art methods, achieving a significant
improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet,
respectively. Furthermore, our approach exhibits consistent superior results
across various CLIP configurations, further verifying its robustness.";Guibiao Liao<author:sep>Kaichen Zhou<author:sep>Zhenyu Bao<author:sep>Kanglin Liu<author:sep>Qing Li;http://arxiv.org/pdf/2402.04648v1;cs.CV;;nerf
2402.04554v2;http://arxiv.org/abs/2402.04554v2;2024-02-07;BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial  Imagery;"In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields
(NeRF) designed specifically for reconstructing large-scale scenes using aerial
imagery. Unlike previous research focused on small-scale and object-centric
NeRF reconstruction, our approach addresses multiple challenges, including (1)
Addressing the issue of slow training and rendering associated with large
models. (2) Meeting the computational demands necessitated by modeling a
substantial number of images, requiring extensive resources such as
high-performance GPUs. (3) Overcoming significant artifacts and low visual
fidelity commonly observed in large-scale reconstruction tasks due to limited
model capacity. Specifically, we present a novel bird-view pose-based spatial
decomposition algorithm that decomposes a large aerial image set into multiple
small sets with appropriately sized overlaps, allowing us to train individual
NeRFs of sub-scene. This decomposition approach not only decouples rendering
time from the scene size but also enables rendering to scale seamlessly to
arbitrarily large environments. Moreover, it allows for per-block updates of
the environment, enhancing the flexibility and adaptability of the
reconstruction process. Additionally, we propose a projection-guided novel view
re-rendering strategy, which aids in effectively utilizing the independently
trained sub-scenes to generate superior rendering results. We evaluate our
approach on existing datasets as well as against our own drone footage,
improving reconstruction speed by 10x over classical photogrammetry software
and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with
similar rendering quality.";Huiqing Zhang<author:sep>Yifei Xue<author:sep>Ming Liao<author:sep>Yizhen Lao;http://arxiv.org/pdf/2402.04554v2;cs.CV;;nerf
2402.04829v1;http://arxiv.org/abs/2402.04829v1;2024-02-07;NeRF as Non-Distant Environment Emitter in Physics-based Inverse  Rendering;"Physics-based inverse rendering aims to jointly optimize shape, materials,
and lighting from captured 2D images. Here lighting is an important part of
achieving faithful light transport simulation. While the environment map is
commonly used as the lighting model in inverse rendering, we show that its
distant lighting assumption leads to spatial invariant lighting, which can be
an inaccurate approximation in real-world inverse rendering. We propose to use
NeRF as a spatially varying environment lighting model and build an inverse
rendering pipeline using NeRF as the non-distant environment emitter. By
comparing our method with the environment map on real and synthetic datasets,
we show that our NeRF-based emitter models the scene lighting more accurately
and leads to more accurate inverse rendering. Project page and video:
https://nerfemitterpbir.github.io/.";Jingwang Ling<author:sep>Ruihan Yu<author:sep>Feng Xu<author:sep>Chun Du<author:sep>Shuang Zhao;http://arxiv.org/pdf/2402.04829v1;cs.CV;Project page and video: https://nerfemitterpbir.github.io/;nerf
2402.03723v1;http://arxiv.org/abs/2402.03723v1;2024-02-06;Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos;"Creating controllable 3D human portraits from casual smartphone videos is
highly desirable due to their immense value in AR/VR applications. The recent
development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering
quality and training efficiency. However, it still remains a challenge to
accurately model and disentangle head movements and facial expressions from a
single-view capture to achieve high-quality renderings. In this paper, we
introduce Rig3DGS to address this challenge. We represent the entire scene,
including the dynamic subject, using a set of 3D Gaussians in a canonical
space. Using a set of control signals, such as head pose and expressions, we
transform them to the 3D space with learned deformations to generate the
desired rendering. Our key innovation is a carefully designed deformation
method which is guided by a learnable prior derived from a 3D morphable model.
This approach is highly efficient in training and effective in controlling
facial expressions, head positions, and view synthesis across various captures.
We demonstrate the effectiveness of our learned deformation through extensive
quantitative and qualitative experiments. The project page can be found at
http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html";Alfredo Rivero<author:sep>ShahRukh Athar<author:sep>Zhixin Shu<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2402.03723v1;cs.CV;;gaussian splatting
2402.04081v1;http://arxiv.org/abs/2402.04081v1;2024-02-06;Improved Generalization of Weight Space Networks via Augmentations;"Learning in deep weight spaces (DWS), where neural networks process the
weights of other neural networks, is an emerging research direction, with
applications to 2D and 3D neural fields (INRs, NeRFs), as well as making
inferences about other types of neural networks. Unfortunately, weight space
models tend to suffer from substantial overfitting. We empirically analyze the
reasons for this overfitting and find that a key reason is the lack of
diversity in DWS datasets. While a given object can be represented by many
different weight configurations, typical INR training sets fail to capture
variability across INRs that represent the same object. To address this, we
explore strategies for data augmentation in weight spaces and propose a MixUp
method adapted for weight spaces. We demonstrate the effectiveness of these
methods in two setups. In classification, they improve performance similarly to
having up to 10 times more data. In self-supervised contrastive learning, they
yield substantial 5-10% gains in downstream classification.";Aviv Shamsian<author:sep>Aviv Navon<author:sep>David W. Zhang<author:sep>Yan Zhang<author:sep>Ethan Fetaya<author:sep>Gal Chechik<author:sep>Haggai Maron;http://arxiv.org/pdf/2402.04081v1;cs.LG;Under Review;nerf
2402.03307v2;http://arxiv.org/abs/2402.03307v2;2024-02-05;4D Gaussian Splatting: Towards Efficient Novel View Synthesis for  Dynamic Scenes;"We consider the problem of novel view synthesis (NVS) for dynamic scenes.
Recent neural approaches have accomplished exceptional NVS results for static
3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior
efforts often encode dynamics by learning a canonical space plus implicit or
explicit deformation fields, which struggle in challenging scenarios like
sudden movements or capturing high-fidelity renderings. In this paper, we
introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic
scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D
Gaussian Splatting in static scenes. We model dynamics at each timestamp by
temporally slicing the 4D Gaussians, which naturally compose dynamic 3D
Gaussians and can be seamlessly projected into images. As an explicit
spatial-temporal representation, 4DGS demonstrates powerful capabilities for
modeling complicated dynamics and fine details, especially for scenes with
abrupt motions. We further implement our temporal slicing and splatting
techniques in a highly optimized CUDA acceleration framework, achieving
real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and
583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions
showcase the superior efficiency and effectiveness of 4DGS, which consistently
outperforms existing methods both quantitatively and qualitatively.";Yuanxing Duan<author:sep>Fangyin Wei<author:sep>Qiyu Dai<author:sep>Yuhang He<author:sep>Wenzheng Chen<author:sep>Baoquan Chen;http://arxiv.org/pdf/2402.03307v2;cs.CV;;gaussian splatting
2402.02906v1;http://arxiv.org/abs/2402.02906v1;2024-02-05;ViewFusion: Learning Composable Diffusion Models for Novel View  Synthesis;"Deep learning is providing a wealth of new approaches to the old problem of
novel view synthesis, from Neural Radiance Field (NeRF) based approaches to
end-to-end style architectures. Each approach offers specific strengths but
also comes with specific limitations in their applicability. This work
introduces ViewFusion, a state-of-the-art end-to-end generative approach to
novel view synthesis with unparalleled flexibility. ViewFusion consists in
simultaneously applying a diffusion denoising step to any number of input views
of a scene, then combining the noise gradients obtained for each view with an
(inferred) pixel-weighting mask, ensuring that for each region of the target
scene only the most informative input views are taken into account. Our
approach resolves several limitations of previous approaches by (1) being
trainable and generalizing across multiple scenes and object classes, (2)
adaptively taking in a variable number of pose-free views at both train and
test time, (3) generating plausible views even in severely undetermined
conditions (thanks to its generative nature) -- all while generating views of
quality on par or even better than state-of-the-art methods. Limitations
include not generating a 3D embedding of the scene, resulting in a relatively
slow inference speed, and our method only being tested on the relatively small
dataset NMR. Code is available.";Bernard Spiegl<author:sep>Andrea Perin<author:sep>StÃ©phane Deny<author:sep>Alexander Ilin;http://arxiv.org/pdf/2402.02906v1;cs.CV;;nerf
2402.03246v1;http://arxiv.org/abs/2402.03246v1;2024-02-05;SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM;"Semantic understanding plays a crucial role in Dense Simultaneous
Localization and Mapping (SLAM), facilitating comprehensive scene
interpretation. Recent advancements that integrate Gaussian Splatting into SLAM
systems have demonstrated its effectiveness in generating high-quality
renderings through the use of explicit 3D Gaussian representations. Building on
this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system
grounded in 3D Gaussians, which provides precise 3D semantic segmentation
alongside high-fidelity reconstructions. Specifically, we propose to employ
multi-channel optimization during the mapping process, integrating appearance,
geometric, and semantic constraints with key-frame optimization to enhance
reconstruction quality. Extensive experiments demonstrate that SGS-SLAM
delivers state-of-the-art performance in camera pose estimation, map
reconstruction, and semantic segmentation, outperforming existing methods
meanwhile preserving real-time rendering ability.";Mingrui Li<author:sep>Shuhong Liu<author:sep>Heng Zhou;http://arxiv.org/pdf/2402.03246v1;cs.CV;;gaussian splatting
2402.01915v1;http://arxiv.org/abs/2402.01915v1;2024-02-02;Robust Inverse Graphics via Probabilistic Inference;"How do we infer a 3D scene from a single image in the presence of corruptions
like rain, snow or fog? Straightforward domain randomization relies on knowing
the family of corruptions ahead of time. Here, we propose a Bayesian
approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene
prior and an uninformative uniform corruption prior, making it applicable to a
wide range of corruptions. Given a single image, RIG performs posterior
inference jointly over the scene and the corruption. We demonstrate this idea
by training a neural radiance field (NeRF) scene prior and using a secondary
NeRF to represent the corruptions over which we place an uninformative prior.
RIG, trained only on clean data, outperforms depth estimators and alternative
NeRF approaches that perform point estimation instead of full inference. The
results hold for a number of scene prior architectures based on normalizing
flows and diffusion models. For the latter, we develop reconstruction-guidance
with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is
applicable in the presence of auxiliary latent variables such as the
corruption. RIG demonstrates how scene priors can be used beyond generation
tasks.";Tuan Anh Le<author:sep>Pavel Sountsov<author:sep>Matthew D. Hoffman<author:sep>Ben Lee<author:sep>Brian Patton<author:sep>Rif A. Saurous;http://arxiv.org/pdf/2402.01915v1;cs.CV;;nerf
2402.01380v1;http://arxiv.org/abs/2402.01380v1;2024-02-02;Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate  Distortion Optimization;"Volumetric videos, benefiting from immersive 3D realism and interactivity,
hold vast potential for various applications, while the tremendous data volume
poses significant challenges for compression. Recently, NeRF has demonstrated
remarkable potential in volumetric video compression thanks to its simple
representation and powerful 3D modeling capabilities, where a notable work is
ReRF. However, ReRF separates the modeling from compression process, resulting
in suboptimal compression efficiency. In contrast, in this paper, we propose a
volumetric video compression method based on dynamic NeRF in a more compact
manner. Specifically, we decompose the NeRF representation into the coefficient
fields and the basis fields, incrementally updating the basis fields in the
temporal domain to achieve dynamic modeling. Additionally, we perform
end-to-end joint optimization on the modeling and compression process to
further improve the compression efficiency. Extensive experiments demonstrate
that our method achieves higher compression efficiency compared to ReRF on
various datasets.";Zhiyu Zhang<author:sep>Guo Lu<author:sep>Huanxiong Liang<author:sep>Anni Tang<author:sep>Qiang Hu<author:sep>Li Song;http://arxiv.org/pdf/2402.01380v1;cs.CV;;nerf
2402.01524v1;http://arxiv.org/abs/2402.01524v1;2024-02-02;HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation;"Neural radiance fields (NeRFs) are a widely accepted standard for
synthesizing new 3D object views from a small number of base images. However,
NeRFs have limited generalization properties, which means that we need to use
significant computational resources to train individual architectures for each
item we want to represent. To address this issue, we propose a few-shot
learning approach based on the hypernetwork paradigm that does not require
gradient optimization during inference. The hypernetwork gathers information
from the training data and generates an update for universal weights. As a
result, we have developed an efficient method for generating a high-quality 3D
object representation from a small number of images in a single step. This has
been confirmed by direct comparison with the state-of-the-art solutions and a
comprehensive ablation study.";PaweÅ Batorski<author:sep>Dawid Malarz<author:sep>Marcin PrzewiÄÅºlikowski<author:sep>Marcin Mazur<author:sep>SÅawomir Tadeja<author:sep>PrzemysÅaw Spurek;http://arxiv.org/pdf/2402.01524v1;cs.CV;;nerf
2402.01950v1;http://arxiv.org/abs/2402.01950v1;2024-02-02;ConRF: Zero-shot Stylization of 3D Scenes with Conditioned Radiation  Fields;"Most of the existing works on arbitrary 3D NeRF style transfer required
retraining on each single style condition. This work aims to achieve zero-shot
controlled stylization in 3D scenes utilizing text or visual input as
conditioning factors. We introduce ConRF, a novel method of zero-shot
stylization. Specifically, due to the ambiguity of CLIP features, we employ a
conversion process that maps the CLIP feature space to the style space of a
pre-trained VGG network and then refine the CLIP multi-modal knowledge into a
style transfer neural radiation field. Additionally, we use a 3D volumetric
representation to perform local style transfer. By combining these operations,
ConRF offers the capability to utilize either text or images as references,
resulting in the generation of sequences with novel views enhanced by global or
local stylization. Our experiment demonstrates that ConRF outperforms other
existing methods for 3D scene and single-text stylization in terms of visual
quality.";Xingyu Miao<author:sep>Yang Bai<author:sep>Haoran Duan<author:sep>Fan Wan<author:sep>Yawen Huang<author:sep>Yang Long<author:sep>Yefeng Zheng;http://arxiv.org/pdf/2402.01950v1;cs.CV;;nerf
2402.01485v1;http://arxiv.org/abs/2402.01485v1;2024-02-02;Di-NeRF: Distributed NeRF for Collaborative Learning with Unknown  Relative Poses;"Collaborative mapping of unknown environments can be done faster and more
robustly than a single robot. However, a collaborative approach requires a
distributed paradigm to be scalable and deal with communication issues. This
work presents a fully distributed algorithm enabling a group of robots to
collectively optimize the parameters of a Neural Radiance Field (NeRF). The
algorithm involves the communication of each robot's trained NeRF parameters
over a mesh network, where each robot trains its NeRF and has access to its own
visual data only. Additionally, the relative poses of all robots are jointly
optimized alongside the model parameters, enabling mapping with unknown
relative camera poses. We show that multi-robot systems can benefit from
differentiable and robust 3D reconstruction optimized from multiple NeRFs.
Experiments on real-world and synthetic data demonstrate the efficiency of the
proposed algorithm. See the website of the project for videos of the
experiments and supplementary
material(https://sites.google.com/view/di-nerf/home).";Mahboubeh Asadi<author:sep>Kourosh Zareinia<author:sep>Sajad Saeedi;http://arxiv.org/pdf/2402.01485v1;cs.RO;9 pages, 11 figures, Submitted to IEEE-RA-L;nerf
2402.01459v3;http://arxiv.org/abs/2402.01459v3;2024-02-02;GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting;"Recently, a range of neural network-based methods for image rendering have
been introduced. One such widely-researched neural radiance field (NeRF) relies
on a neural network to represent 3D scenes, allowing for realistic view
synthesis from a small number of 2D images. However, most NeRF models are
constrained by long training and inference times. In comparison, Gaussian
Splatting (GS) is a novel, state-of-the-art technique for rendering points in a
3D scene by approximating their contribution to image pixels through Gaussian
distributions, warranting fast training and swift, real-time rendering. A
drawback of GS is the absence of a well-defined approach for its conditioning
due to the necessity to condition several hundred thousand Gaussian components.
To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which
allows modification of Gaussian components in a similar way as meshes. We
parameterize each Gaussian component by the vertices of the mesh face.
Furthermore, our model needs mesh initialization on input or estimated mesh
during training. We also define Gaussian splats solely based on their location
on the mesh, allowing for automatic adjustments in position, scale, and
rotation during animation. As a result, we obtain a real-time rendering of
editable GS.";Joanna WaczyÅska<author:sep>Piotr Borycki<author:sep>SÅawomir Tadeja<author:sep>Jacek Tabor<author:sep>PrzemysÅaw Spurek;http://arxiv.org/pdf/2402.01459v3;cs.CV;;gaussian splatting<tag:sep>nerf
2402.01217v2;http://arxiv.org/abs/2402.01217v2;2024-02-02;Taming Uncertainty in Sparse-view Generalizable NeRF via Indirect  Diffusion Guidance;"Neural Radiance Fields (NeRF) have demonstrated effectiveness in synthesizing
novel views. However, their reliance on dense inputs and scene-specific
optimization has limited their broader applicability. Generalizable NeRFs
(Gen-NeRF), while intended to address this, often produce blurring artifacts in
unobserved regions with sparse inputs, which are full of uncertainty. In this
paper, we aim to diminish the uncertainty in Gen-NeRF for plausible renderings.
We assume that NeRF's inability to effectively mitigate this uncertainty stems
from its inherent lack of generative capacity. Therefore, we innovatively
propose an Indirect Diffusion-guided NeRF framework, termed ID-NeRF, to address
this uncertainty from a generative perspective by leveraging a distilled
diffusion prior as guidance. Specifically, to avoid model confusion caused by
directly regularizing with inconsistent samplings as in previous methods, our
approach introduces a strategy to indirectly inject the inherently missing
imagination into the learned implicit function through a diffusion-guided
latent space. Empirical evaluation across various benchmarks demonstrates the
superior performance of our approach in handling uncertainty with sparse
inputs.";Yaokun Li<author:sep>Chao Gou<author:sep>Guang Tan;http://arxiv.org/pdf/2402.01217v2;cs.CV;;nerf
2402.00827v1;http://arxiv.org/abs/2402.00827v1;2024-02-01;Emo-Avatar: Efficient Monocular Video Style Avatar through Texture  Rendering;"Artistic video portrait generation is a significant and sought-after task in
the fields of computer graphics and vision. While various methods have been
developed that integrate NeRFs or StyleGANs with instructional editing models
for creating and editing drivable portraits, these approaches face several
challenges. They often rely heavily on large datasets, require extensive
customization processes, and frequently result in reduced image quality. To
address the above problems, we propose the Efficient Monotonic Video Style
Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's
capacity for producing dynamic, drivable portrait videos. We proposed a
two-stage deferred neural rendering pipeline. In the first stage, we utilize
few-shot PTI initialization to initialize the StyleGAN generator through
several extreme poses sampled from the video to capture the consistent
representation of aligned faces from the target portrait. In the second stage,
we propose a Laplacian pyramid for high-frequency texture sampling from UV maps
deformed by dynamic flow of expression for motion-aware texture prior
integration to provide torso features to enhance StyleGAN's ability to generate
complete and upper body for portrait video rendering. Emo-Avatar reduces style
customization time from hours to merely 5 minutes compared with existing
methods. In addition, Emo-Avatar requires only a single reference image for
editing and employs region-aware contrastive learning with semantic invariant
CLIP guidance, ensuring consistent high-resolution output and identity
preservation. Through both quantitative and qualitative assessments, Emo-Avatar
demonstrates superior performance over existing methods in terms of training
efficiency, rendering quality and editability in self- and cross-reenactment.";Pinxin Liu<author:sep>Luchuan Song<author:sep>Daoan Zhang<author:sep>Hang Hua<author:sep>Yunlong Tang<author:sep>Huaijin Tu<author:sep>Jiebo Luo<author:sep>Chenliang Xu;http://arxiv.org/pdf/2402.00827v1;cs.CV;;nerf
2402.00763v1;http://arxiv.org/abs/2402.00763v1;2024-02-01;360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming;"3D Gaussian Splatting (3D-GS) has recently attracted great attention with
real-time and photo-realistic renderings. This technique typically takes
perspective images as input and optimizes a set of 3D elliptical Gaussians by
splatting them onto the image planes, resulting in 2D Gaussians. However,
applying 3D-GS to panoramic inputs presents challenges in effectively modeling
the projection onto the spherical surface of ${360^\circ}$ images using 2D
Gaussians. In practical applications, input panoramas are often sparse, leading
to unreliable initialization of 3D Gaussians and subsequent degradation of
3D-GS quality. In addition, due to the under-constrained geometry of
texture-less planes (e.g., walls and floors), 3D-GS struggles to model these
flat regions with elliptical Gaussians, resulting in significant floaters in
novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$
Gaussian splatting for a limited set of panoramic inputs. Instead of splatting
3D Gaussians directly onto the spherical surface, 360-GS projects them onto the
tangent plane of the unit sphere and then maps them to the spherical
projections. This adaptation enables the representation of the projection using
Gaussians. We guide the optimization of 360-GS by exploiting layout priors
within panoramas, which are simple to obtain and contain strong structural
information about the indoor scene. Our experimental results demonstrate that
360-GS allows panoramic rendering and outperforms state-of-the-art methods with
fewer artifacts in novel view synthesis, thus providing immersive roaming in
indoor scenarios.";Jiayang Bai<author:sep>Letian Huang<author:sep>Jie Guo<author:sep>Wen Gong<author:sep>Yuanqi Li<author:sep>Yanwen Guo;http://arxiv.org/pdf/2402.00763v1;cs.CV;11 pages, 10 figures;gaussian splatting
2402.00752v2;http://arxiv.org/abs/2402.00752v2;2024-02-01;Optimal Projection for 3D Gaussian Splatting;"3D Gaussian Splatting has garnered extensive attention and application in
real-time neural rendering. Concurrently, concerns have been raised about the
limitations of this technology in aspects such as point cloud storage,
performance , and robustness in sparse viewpoints , leading to various
improvements. However, there has been a notable lack of attention to the
projection errors introduced by the local affine approximation inherent in the
splatting itself, and the consequential impact of these errors on the quality
of photo-realistic rendering. This paper addresses the projection error
function of 3D Gaussian Splatting, commencing with the residual error from the
first-order Taylor expansion of the projection function $\phi$. The analysis
establishes a correlation between the error and the Gaussian mean position.
Subsequently, leveraging function optimization theory, this paper analyzes the
function's minima to provide an optimal projection strategy for Gaussian
Splatting referred to Optimal Gaussian Splatting. Experimental validation
further confirms that this projection methodology reduces artifacts, resulting
in a more convincingly realistic rendering.";Letian Huang<author:sep>Jiayang Bai<author:sep>Jie Guo<author:sep>Yanwen Guo;http://arxiv.org/pdf/2402.00752v2;cs.CV;;gaussian splatting
2402.00864v1;http://arxiv.org/abs/2402.00864v1;2024-02-01;ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields;"We introduce ViCA-NeRF, the first view-consistency-aware method for 3D
editing with text instructions. In addition to the implicit neural radiance
field (NeRF) modeling, our key insight is to exploit two sources of
regularization that explicitly propagate the editing information across
different views, thus ensuring multi-view consistency. For geometric
regularization, we leverage the depth information derived from NeRF to
establish image correspondences between different views. For learned
regularization, we align the latent codes in the 2D diffusion model between
edited and unedited images, enabling us to edit key views and propagate the
update throughout the entire scene. Incorporating these two strategies, our
ViCA-NeRF operates in two stages. In the initial stage, we blend edits from
different views to create a preliminary 3D edit. This is followed by a second
stage of NeRF training, dedicated to further refining the scene's appearance.
Experimental results demonstrate that ViCA-NeRF provides more flexible,
efficient (3 times faster) editing with higher levels of consistency and
details, compared with the state of the art. Our code is publicly available.";Jiahua Dong<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2402.00864v1;cs.CV;"Neurips2023; project page: https://github.com/Dongjiahua/VICA-NeRF";nerf
2402.00525v1;http://arxiv.org/abs/2402.00525v1;2024-02-01;StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time  Rendering;"Gaussian Splatting has emerged as a prominent model for constructing 3D
representations from images across diverse domains. However, the efficiency of
the 3D Gaussian Splatting rendering pipeline relies on several simplifications.
Notably, reducing Gaussian to 2D splats with a single view-space depth
introduces popping and blending artifacts during view rotation. Addressing this
issue requires accurate per-pixel depth computation, yet a full per-pixel sort
proves excessively costly compared to a global sort operation. In this paper,
we present a novel hierarchical rasterization approach that systematically
resorts and culls splats with minimal processing overhead. Our software
rasterizer effectively eliminates popping artifacts and view inconsistencies,
as demonstrated through both quantitative and qualitative measurements.
Simultaneously, our method mitigates the potential for cheating view-dependent
effects with popping, ensuring a more authentic representation. Despite the
elimination of cheating, our approach achieves comparable quantitative results
for test images, while increasing the consistency for novel view synthesis in
motion. Due to its design, our hierarchical approach is only 4% slower on
average than the original Gaussian Splatting. Notably, enforcing consistency
enables a reduction in the number of Gaussians by approximately half with
nearly identical quality and view-consistency. Consequently, rendering
performance is nearly doubled, making our approach 1.6x faster than the
original Gaussian Splatting, with a 50% reduction in memory requirements.";Lukas Radl<author:sep>Michael Steiner<author:sep>Mathias Parger<author:sep>Alexander Weinrauch<author:sep>Bernhard Kerbl<author:sep>Markus Steinberger;http://arxiv.org/pdf/2402.00525v1;cs.GR;Video: https://youtu.be/RJQlSORNkr0;gaussian splatting
2401.17857v2;http://arxiv.org/abs/2401.17857v2;2024-01-31;Segment Anything in 3D Gaussians;"3D Gaussian Splatting has emerged as an alternative 3D representation of
Neural Radiance Fields (NeRFs), benefiting from its high-quality rendering
results and real-time rendering speed. Considering the 3D Gaussian
representation remains unparsed, it is necessary first to execute object
segmentation within this domain. Subsequently, scene editing and collision
detection can be performed, proving vital to a multitude of applications, such
as virtual reality (VR), augmented reality (AR), game/movie production, etc. In
this paper, we propose a novel approach to achieve object segmentation in 3D
Gaussian via an interactive procedure without any training process and learned
parameters. We refer to the proposed method as SA-GS, for Segment Anything in
3D Gaussians. Given a set of clicked points in a single input view, SA-GS can
generalize SAM to achieve 3D consistent segmentation via the proposed
multi-view mask generation and view-wise label assignment methods. We also
propose a cross-view label-voting approach to assign labels from different
views. In addition, in order to address the boundary roughness issue of
segmented objects resulting from the non-negligible spatial sizes of 3D
Gaussian located at the boundary, SA-GS incorporates the simple but effective
Gaussian Decomposition scheme. Extensive experiments demonstrate that SA-GS
achieves high-quality 3D segmentation results, which can also be easily applied
for scene editing and collision detection tasks. Codes will be released soon.";Xu Hu<author:sep>Yuxi Wang<author:sep>Lue Fan<author:sep>Junsong Fan<author:sep>Junran Peng<author:sep>Zhen Lei<author:sep>Qing Li<author:sep>Zhaoxiang Zhang;http://arxiv.org/pdf/2401.17857v2;cs.CV;;gaussian splatting<tag:sep>nerf
2401.18075v1;http://arxiv.org/abs/2401.18075v1;2024-01-31;CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting;"We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene
Forecasting, a method for predicting future 3D scenes given past observations,
such as 2D ego-centric images. Our method maps an image to a distribution over
plausible 3D latent scene configurations using a probabilistic encoder, and
predicts the evolution of the hypothesized scenes through time. Our latent
scene representation conditions a global Neural Radiance Field (NeRF) to
represent a 3D scene model, which enables explainable predictions and
straightforward downstream applications. This approach extends beyond previous
neural rendering work by considering complex scenarios of uncertainty in
environmental states and dynamics. We employ a two-stage training of
Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we
auto-regressively predict latent scene representations as a partially
observable Markov decision process, utilizing a mixture density network. We
demonstrate the utility of our method in realistic scenarios using the CARLA
driving simulator, where CARFF can be used to enable efficient trajectory and
contingency planning in complex multi-agent autonomous driving scenarios
involving visual occlusions.";Jiezhi Yang<author:sep>Khushi Desai<author:sep>Charles Packer<author:sep>Harshil Bhatia<author:sep>Nicholas Rhinehart<author:sep>Rowan McAllister<author:sep>Joseph Gonzalez;http://arxiv.org/pdf/2401.18075v1;cs.CV;;nerf
2401.17121v1;http://arxiv.org/abs/2401.17121v1;2024-01-30;Physical Priors Augmented Event-Based 3D Reconstruction;"3D neural implicit representations play a significant component in many
robotic applications. However, reconstructing neural radiance fields (NeRF)
from realistic event data remains a challenge due to the sparsities and the
lack of information when only event streams are available. In this paper, we
utilize motion, geometry, and density priors behind event data to impose strong
physical constraints to augment NeRF training. The proposed novel pipeline can
directly benefit from those priors to reconstruct 3D scenes without additional
inputs. Moreover, we present a novel density-guided patch-based sampling
strategy for robust and efficient learning, which not only accelerates training
procedures but also conduces to expressions of local geometries. More
importantly, we establish the first large dataset for event-based 3D
reconstruction, which contains 101 objects with various materials and
geometries, along with the groundtruth of images and depth maps for all camera
viewpoints, which significantly facilitates other research in the related
fields. The code and dataset will be publicly available at
https://github.com/Mercerai/PAEv3d.";Jiaxu Wang<author:sep>Junhao He<author:sep>Ziyi Zhang<author:sep>Renjing Xu;http://arxiv.org/pdf/2401.17121v1;cs.RO;6 pages, 6 figures, ICRA 2024;nerf
2401.16663v1;http://arxiv.org/abs/2401.16663v1;2024-01-30;VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System  in Virtual Reality;"As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain
momentum, there's a growing focus on the development of engagements with 3D
virtual content. Unfortunately, traditional techniques for content creation,
editing, and interaction within these virtual spaces are fraught with
difficulties. They tend to be not only engineering-intensive but also require
extensive expertise, which adds to the frustration and inefficiency in virtual
object manipulation. Our proposed VR-GS system represents a leap forward in
human-centered 3D content interaction, offering a seamless and intuitive user
experience. By developing a physical dynamics-aware interactive Gaussian
Splatting in a Virtual Reality setting, and constructing a highly efficient
two-level embedding strategy alongside deformable body simulations, VR-GS
ensures real-time execution with highly realistic dynamic responses. The
components of our Virtual Reality system are designed for high efficiency and
effectiveness, starting from detailed scene reconstruction and object
segmentation, advancing through multi-view image in-painting, and extending to
interactive physics-based editing. The system also incorporates real-time
deformation embedding and dynamic shadow casting, ensuring a comprehensive and
engaging virtual experience.Our project page is available at:
https://yingjiang96.github.io/VR-GS/.";Ying Jiang<author:sep>Chang Yu<author:sep>Tianyi Xie<author:sep>Xuan Li<author:sep>Yutao Feng<author:sep>Huamin Wang<author:sep>Minchen Li<author:sep>Henry Lau<author:sep>Feng Gao<author:sep>Yin Yang<author:sep>Chenfanfu Jiang;http://arxiv.org/pdf/2401.16663v1;cs.HC;;gaussian splatting
2401.16144v1;http://arxiv.org/abs/2401.16144v1;2024-01-29;Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance  Fields;"Neural radiance fields (NeRFs) have exhibited potential in synthesizing
high-fidelity views of 3D scenes but the standard training paradigm of NeRF
presupposes an equal importance for each image in the training set. This
assumption poses a significant challenge for rendering specific views
presenting intricate geometries, thereby resulting in suboptimal performance.
In this paper, we take a closer look at the implications of the current
training paradigm and redesign this for more superior rendering quality by
NeRFs. Dividing input views into multiple groups based on their visual
similarities and training individual models on each of these groups enables
each model to specialize on specific regions without sacrificing speed or
efficiency. Subsequently, the knowledge of these specialized models is
aggregated into a single entity via a teacher-student distillation paradigm,
enabling spatial efficiency for online render-ing. Empirically, we evaluate our
novel training framework on two publicly available datasets, namely NeRF
synthetic and Tanks&Temples. Our evaluation demonstrates that our DaC training
pipeline enhances the rendering quality of a state-of-the-art baseline model
while exhibiting convergence to a superior minimum.";Rongkai Ma<author:sep>Leo Lebrat<author:sep>Rodrigo Santa Cruz<author:sep>Gil Avraham<author:sep>Yan Zuo<author:sep>Clinton Fookes<author:sep>Olivier Salvado;http://arxiv.org/pdf/2401.16144v1;cs.CV;;nerf
2401.16416v2;http://arxiv.org/abs/2401.16416v2;2024-01-29;Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian  Splatting;"In the realm of robot-assisted minimally invasive surgery, dynamic scene
reconstruction can significantly enhance downstream tasks and improve surgical
outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to
prominence for their exceptional ability to reconstruct scenes. Nonetheless,
these methods are hampered by slow inference, prolonged training, and
substantial computational demands. Additionally, some rely on stereo depth
estimation, which is often infeasible due to the high costs and logistical
challenges associated with stereo cameras. Moreover, the monocular
reconstruction quality for deformable scenes is currently inadequate. To
overcome these obstacles, we present Endo-4DGS, an innovative, real-time
endoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting
(GS) and requires no ground truth depth data. This method extends 3D GS by
incorporating a temporal component and leverages a lightweight MLP to capture
temporal Gaussian deformations. This effectively facilitates the reconstruction
of dynamic surgical scenes with variable conditions. We also integrate
Depth-Anything to generate pseudo-depth maps from monocular views, enhancing
the depth-guided reconstruction process. Our approach has been validated on two
surgical datasets, where it can effectively render in real-time, compute
efficiently, and reconstruct with remarkable accuracy. These results underline
the vast potential of Endo-4DGS to improve surgical assistance.";Yiming Huang<author:sep>Beilei Cui<author:sep>Long Bai<author:sep>Ziqi Guo<author:sep>Mengya Xu<author:sep>Hongliang Ren;http://arxiv.org/pdf/2401.16416v2;cs.CV;;gaussian splatting<tag:sep>nerf
2401.15318v1;http://arxiv.org/abs/2401.15318v1;2024-01-27;Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting;"We demonstrate the feasibility of integrating physics-based animations of
solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in
virtual scenes reconstructed using 3DGS. Leveraging the coherence of the
Gaussian splatting and position-based dynamics (PBD) in the underlying
representation, we manage rendering, view synthesis, and the dynamics of solids
and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each
Gaussian kernel with an added normal, aligning the kernel's orientation with
the surface normal to refine the PBD simulation. This approach effectively
eliminates spiky noises that arise from rotational deformation in solids. It
also allows us to integrate physically based rendering to augment the dynamic
surface reflections on fluids. Consequently, our framework is capable of
realistically reproducing surface highlights on dynamic fluids and facilitating
interactions between scene objects and fluids from new views. For more
information, please visit our project page at
\url{https://amysteriouscat.github.io/GaussianSplashing/}.";Yutao Feng<author:sep>Xiang Feng<author:sep>Yintong Shang<author:sep>Ying Jiang<author:sep>Chang Yu<author:sep>Zeshun Zong<author:sep>Tianjia Shao<author:sep>Hongzhi Wu<author:sep>Kun Zhou<author:sep>Chenfanfu Jiang<author:sep>Yin Yang;http://arxiv.org/pdf/2401.15318v1;cs.GR;;gaussian splatting
2401.14726v1;http://arxiv.org/abs/2401.14726v1;2024-01-26;3D Reconstruction and New View Synthesis of Indoor Environments based on  a Dual Neural Radiance Field;"Simultaneously achieving 3D reconstruction and new view synthesis for indoor
environments has widespread applications but is technically very challenging.
State-of-the-art methods based on implicit neural functions can achieve
excellent 3D reconstruction results, but their performances on new view
synthesis can be unsatisfactory. The exciting development of neural radiance
field (NeRF) has revolutionized new view synthesis, however, NeRF-based models
can fail to reconstruct clean geometric surfaces. We have developed a dual
neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry
reconstruction and view rendering. Du-NeRF contains two geometric fields, one
derived from the SDF field to facilitate geometric reconstruction and the other
derived from the density field to boost new view synthesis. One of the
innovative features of Du-NeRF is that it decouples a view-independent
component from the density field and uses it as a label to supervise the
learning process of the SDF field. This reduces shape-radiance ambiguity and
enables geometry and color to benefit from each other during the learning
process. Extensive experiments demonstrate that Du-NeRF can significantly
improve the performance of novel view synthesis and 3D reconstruction for
indoor environments and it is particularly effective in constructing areas
containing fine geometries that do not obey multi-view color consistency.";Zhenyu Bao<author:sep>Guibiao Liao<author:sep>Zhongyuan Zhao<author:sep>Kanglin Liu<author:sep>Qing Li<author:sep>Guoping Qiu;http://arxiv.org/pdf/2401.14726v1;cs.CV;20 pages, 8 figures;nerf
2401.14828v1;http://arxiv.org/abs/2401.14828v1;2024-01-26;TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And  Image-Prompts;"Text-driven 3D scene editing has gained significant attention owing to its
convenience and user-friendliness. However, existing methods still lack
accurate control of the specified appearance and location of the editing result
due to the inherent limitations of the text description. To this end, we
propose a 3D scene editing framework, TIPEditor, that accepts both text and
image prompts and a 3D bounding box to specify the editing region. With the
image prompt, users can conveniently specify the detailed appearance/style of
the target content in complement to the text description, enabling accurate
control of the appearance. Specifically, TIP-Editor employs a stepwise 2D
personalization strategy to better learn the representation of the existing
scene and the reference image, in which a localization loss is proposed to
encourage correct object placement as specified by the bounding box.
Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as
the 3D representation to facilitate local editing while keeping the background
unchanged. Extensive experiments have demonstrated that TIP-Editor conducts
accurate editing following the text and image prompts in the specified bounding
box region, consistently outperforming the baselines in editing quality, and
the alignment to the prompts, qualitatively and quantitatively.";Jingyu Zhuang<author:sep>Di Kang<author:sep>Yan-Pei Cao<author:sep>Guanbin Li<author:sep>Liang Lin<author:sep>Ying Shan;http://arxiv.org/pdf/2401.14828v1;cs.CV;Under review;gaussian splatting
2401.14257v2;http://arxiv.org/abs/2401.14257v2;2024-01-25;Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation;"Recently, text-to-3D approaches have achieved high-fidelity 3D content
generation using text description. However, the generated objects are
stochastic and lack fine-grained control. Sketches provide a cheap approach to
introduce such fine-grained control. Nevertheless, it is challenging to achieve
flexible control from these sketches due to their abstraction and ambiguity. In
this paper, we present a multi-view sketch-guided text-to-3D generation
framework (namely, Sketch2NeRF) to add sketch control to 3D generation.
Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable
Diffusion and ControlNet) to supervise the optimization of a 3D scene
represented by a neural radiance field (NeRF). We propose a novel synchronized
generation and reconstruction method to effectively optimize the NeRF. In the
experiments, we collected two kinds of multi-view sketch datasets to evaluate
the proposed method. We demonstrate that our method can synthesize 3D
consistent contents with fine-grained sketch control while being high-fidelity
to text prompts. Extensive results show that our method achieves
state-of-the-art performance in terms of sketch similarity and text alignment.";Minglin Chen<author:sep>Weihao Yuan<author:sep>Yukun Wang<author:sep>Zhe Sheng<author:sep>Yisheng He<author:sep>Zilong Dong<author:sep>Liefeng Bo<author:sep>Yulan Guo;http://arxiv.org/pdf/2401.14257v2;cs.CV;11 pages, 9 figures;nerf
2401.14032v1;http://arxiv.org/abs/2401.14032v1;2024-01-25;GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D  Reconstruction Dataset Using Gaussian Splatting;"We introduce a novel large-scale scene reconstruction benchmark using the
newly developed 3D representation approach, Gaussian Splatting, on our
expansive U-Scene dataset. U-Scene encompasses over one and a half square
kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground
truth. For data acquisition, we employed the Matrix 300 drone equipped with the
high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This
dataset, offers a unique blend of urban and academic environments for advanced
spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with
Gaussian Splatting includes a detailed analysis across various novel
viewpoints. We also juxtapose these results with those derived from our
accurate point cloud dataset, highlighting significant differences that
underscore the importance of combine multi-modal information";Butian Xiong<author:sep>Zhuo Li<author:sep>Zhen Li;http://arxiv.org/pdf/2401.14032v1;cs.CV;IJCAI2024 submit, 8 pages;gaussian splatting
2401.14354v1;http://arxiv.org/abs/2401.14354v1;2024-01-25;Learning Robust Generalizable Radiance Field with Visibility and Feature  Augmented Point Representation;"This paper introduces a novel paradigm for the generalizable neural radiance
field (NeRF). Previous generic NeRF methods combine multiview stereo techniques
with image-based neural rendering for generalization, yielding impressive
results, while suffering from three issues. First, occlusions often result in
inconsistent feature matching. Then, they deliver distortions and artifacts in
geometric discontinuities and locally sharp shapes due to their individual
process of sampled points and rough feature aggregation. Third, their
image-based representations experience severe degradations when source views
are not near enough to the target view. To address challenges, we propose the
first paradigm that constructs the generalizable neural field based on
point-based rather than image-based rendering, which we call the Generalizable
neural Point Field (GPF). Our approach explicitly models visibilities by
geometric priors and augments them with neural features. We propose a novel
nonuniform log sampling strategy to improve both rendering speed and
reconstruction quality. Moreover, we present a learnable kernel spatially
augmented with features for feature aggregations, mitigating distortions at
places with drastically varying geometries. Besides, our representation can be
easily manipulated. Experiments show that our model can deliver better
geometries, view consistencies, and rendering quality than all counterparts and
benchmarks on three datasets in both generalization and finetuning settings,
preliminarily proving the potential of the new paradigm for generalizable NeRF.";Jiaxu Wang<author:sep>Ziyi Zhang<author:sep>Renjing Xu;http://arxiv.org/pdf/2401.14354v1;cs.CV;International Conference on Learning Representations 2024;nerf
2401.13352v1;http://arxiv.org/abs/2401.13352v1;2024-01-24;EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable  Endoscopic Tissues Reconstruction;"The accurate 3D reconstruction of deformable soft body tissues from
endoscopic videos is a pivotal challenge in medical applications such as VR
surgery and medical image analysis. Existing methods often struggle with
accuracy and the ambiguity of hallucinated tissue parts, limiting their
practical utility. In this work, we introduce EndoGaussians, a novel approach
that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This
method marks the first use of Gaussian Splatting in this context, overcoming
the limitations of previous NeRF-based techniques. Our method sets new
state-of-the-art standards, as demonstrated by quantitative assessments on
various endoscope datasets. These advancements make our method a promising tool
for medical professionals, offering more reliable and efficient 3D
reconstructions for practical applications in the medical field.";Yangsen Chen<author:sep>Hao Wang;http://arxiv.org/pdf/2401.13352v1;cs.CV;;gaussian splatting<tag:sep>nerf
2401.12451v1;http://arxiv.org/abs/2401.12451v1;2024-01-23;Methods and strategies for improving the novel view synthesis quality of  neural radiation field;"Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a
scene from 2D images and synthesize realistic novel view images. This
technology has received widespread attention from the industry and has good
application prospects. In response to the problem that the rendering quality of
NeRF images needs to be improved, many researchers have proposed various
methods to improve the rendering quality in the past three years. The latest
relevant papers are classified and reviewed, the technical principles behind
quality improvement are analyzed, and the future evolution direction of quality
improvement methods is discussed. This study can help researchers quickly
understand the current state and evolutionary context of technology in this
field, which is helpful in inspiring the development of more efficient
algorithms and promoting the application of NeRF technology in related fields.";Shun Fang<author:sep>Ming Cui<author:sep>Xing Feng<author:sep>Yanna Lv;http://arxiv.org/pdf/2401.12451v1;cs.CV;;nerf
2401.12568v1;http://arxiv.org/abs/2401.12568v1;2024-01-23;NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for  Talking Face Synthesis;"Talking face synthesis driven by audio is one of the current research
hotspots in the fields of multidimensional signal processing and multimedia.
Neural Radiance Field (NeRF) has recently been brought to this research field
in order to enhance the realism and 3D effect of the generated faces. However,
most existing NeRF-based methods either burden NeRF with complex learning tasks
while lacking methods for supervised multimodal feature fusion, or cannot
precisely map audio to the facial region related to speech movements. These
reasons ultimately result in existing methods generating inaccurate lip shapes.
This paper moves a portion of NeRF learning tasks ahead and proposes a talking
face synthesis method via NeRF with attention-based disentanglement (NeRF-AD).
In particular, an Attention-based Disentanglement module is introduced to
disentangle the face into Audio-face and Identity-face using speech-related
facial action unit (AU) information. To precisely regulate how audio affects
the talking face, we only fuse the Audio-face with audio feature. In addition,
AU information is also utilized to supervise the fusion of these two
modalities. Extensive qualitative and quantitative experiments demonstrate that
our NeRF-AD outperforms state-of-the-art methods in generating realistic
talking face videos, including image quality and lip synchronization. To view
video results, please refer to https://xiaoxingliu02.github.io/NeRF-AD.";Chongke Bi<author:sep>Xiaoxing Liu<author:sep>Zhilei Liu;http://arxiv.org/pdf/2401.12568v1;cs.CV;Accepted by ICASSP 2024;nerf
2401.12900v4;http://arxiv.org/abs/2401.12900v4;2024-01-23;PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar  Animation with 3D Gaussian Splatting;"Despite much progress, achieving real-time high-fidelity head avatar
animation is still difficult and existing methods have to trade-off between
speed and quality. 3DMM based methods often fail to model non-facial structures
such as eyeglasses and hairstyles, while neural implicit models suffer from
deformation inflexibility and rendering inefficiency. Although 3D Gaussian has
been demonstrated to possess promising capability for geometry representation
and radiance field reconstruction, applying 3D Gaussian in head avatar creation
remains a major challenge since it is difficult for 3D Gaussian to model the
head shape variations caused by changing poses and expressions. In this paper,
we introduce PSAvatar, a novel framework for animatable head avatar creation
that utilizes discrete geometric primitive to create a parametric morphable
shape model and employs 3D Gaussian for fine detail representation and high
fidelity rendering. The parametric morphable shape model is a Point-based
Morphable Shape Model (PMSM) which uses points instead of meshes for 3D
representation to achieve enhanced representation flexibility. The PMSM first
converts the FLAME mesh to points by sampling on the surfaces as well as off
the meshes to enable the reconstruction of not only surface-like structures but
also complex geometries such as eyeglasses and hairstyles. By aligning these
points with the head shape in an analysis-by-synthesis manner, the PMSM makes
it possible to utilize 3D Gaussian for fine detail representation and
appearance modeling, thus enabling the creation of high-fidelity avatars. We
show that PSAvatar can reconstruct high-fidelity head avatars of a variety of
subjects and the avatars can be animated in real-time ($\ge$ 25 fps at a
resolution of 512 $\times$ 512 ).";Zhongyuan Zhao<author:sep>Zhenyu Bao<author:sep>Qing Li<author:sep>Guoping Qiu<author:sep>Kanglin Liu;http://arxiv.org/pdf/2401.12900v4;cs.GR;13 pages, 10 figures;gaussian splatting
2401.12456v1;http://arxiv.org/abs/2401.12456v1;2024-01-23;Exploration and Improvement of Nerf-based 3D Scene Editing Techniques;"NeRF's high-quality scene synthesis capability was quickly accepted by
scholars in the years after it was proposed, and significant progress has been
made in 3D scene representation and synthesis. However, the high computational
cost limits intuitive and efficient editing of scenes, making NeRF's
development in the scene editing field facing many challenges. This paper
reviews the preliminary explorations of scholars on NeRF in the scene or object
editing field in recent years, mainly changing the shape and texture of scenes
or objects in new synthesized scenes; through the combination of residual
models such as GaN and Transformer with NeRF, the generalization ability of
NeRF scene editing has been further expanded, including realizing real-time new
perspective editing feedback, multimodal editing of text synthesized 3D scenes,
4D synthesis performance, and in-depth exploration in light and shadow editing,
initially achieving optimization of indirect touch editing and detail
representation in complex scenes. Currently, most NeRF editing methods focus on
the touch points and materials of indirect points, but when dealing with more
complex or larger 3D scenes, it is difficult to balance accuracy, breadth,
efficiency, and quality. Overcoming these challenges may become the direction
of future NeRF 3D scene editing technology.";Shun Fang<author:sep>Ming Cui<author:sep>Xing Feng<author:sep>Yanan Zhang;http://arxiv.org/pdf/2401.12456v1;cs.CV;;nerf
2401.12561v1;http://arxiv.org/abs/2401.12561v1;2024-01-23;EndoGaussian: Gaussian Splatting for Deformable Surgical Scene  Reconstruction;"Reconstructing deformable tissues from endoscopic stereo videos is essential
in many downstream surgical applications. However, existing methods suffer from
slow inference speed, which greatly limits their practical use. In this paper,
we introduce EndoGaussian, a real-time surgical scene reconstruction framework
that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical
scenes as canonical Gaussians and a time-dependent deformation field, which
predicts Gaussian deformations at novel timestamps. Due to the efficient
Gaussian representation and parallel rendering pipeline, our framework
significantly accelerates the rendering speed compared to previous methods. In
addition, we design the deformation field as the combination of a lightweight
encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian
tracking with a minor rendering burden. Furthermore, we design a holistic
Gaussian initialization method to fully leverage the surface distribution
prior, achieved by searching informative points from across the input image
sequence. Experiments on public endoscope datasets demonstrate that our method
can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain)
while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and
the fastest training speed (within 2 min/scene), showing significant promise
for intraoperative surgery applications. Code is available at:
\url{https://yifliu3.github.io/EndoGaussian/}.";Yifan Liu<author:sep>Chenxin Li<author:sep>Chen Yang<author:sep>Yixuan Yuan;http://arxiv.org/pdf/2401.12561v1;cs.CV;;gaussian splatting
2401.11711v1;http://arxiv.org/abs/2401.11711v1;2024-01-22;HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided  Neural Radiance Fields for Sparse View Inputs;"Neural Radiance Fields (NeRF) have garnered considerable attention as a
paradigm for novel view synthesis by learning scene representations from
discrete observations. Nevertheless, NeRF exhibit pronounced performance
degradation when confronted with sparse view inputs, consequently curtailing
its further applicability. In this work, we introduce Hierarchical Geometric,
Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can
address the aforementioned limitation and enhance consistency of geometry,
semantic content, and appearance across different views. We propose
Hierarchical Geometric Guidance (HGG) to incorporate the attachment of
Structure from Motion (SfM), namely sparse depth prior, into the scene
representations. Different from direct depth supervision, HGG samples volume
points from local-to-global geometric regions, mitigating the misalignment
caused by inherent bias in the depth prior. Furthermore, we draw inspiration
from notable variations in semantic consistency observed across images of
different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn
the coarse-to-fine semantic content, which corresponds to the coarse-to-fine
scene representations. Experimental results demonstrate that HG3-NeRF can
outperform other state-of-the-art methods on different standard benchmarks and
achieve high-fidelity synthesis results for sparse view inputs.";Zelin Gao<author:sep>Weichen Dai<author:sep>Yu Zhang;http://arxiv.org/pdf/2401.11711v1;cs.CV;13 pages, 6 figures;nerf
2401.12175v1;http://arxiv.org/abs/2401.12175v1;2024-01-22;Single-View 3D Human Digitalization with Large Reconstruction Models;"In this paper, we introduce Human-LRM, a single-stage feed-forward Large
Reconstruction Model designed to predict human Neural Radiance Fields (NeRF)
from a single image. Our approach demonstrates remarkable adaptability in
training using extensive datasets containing 3D scans and multi-view capture.
Furthermore, to enhance the model's applicability for in-the-wild scenarios
especially with occlusions, we propose a novel strategy that distills
multi-view reconstruction into single-view via a conditional triplane diffusion
model. This generative extension addresses the inherent variations in human
body shapes when observed from a single view, and makes it possible to
reconstruct the full body human from an occluded image. Through extensive
experiments, we show that Human-LRM surpasses previous methods by a significant
margin on several benchmarks.";Zhenzhen Weng<author:sep>Jingyuan Liu<author:sep>Hao Tan<author:sep>Zhan Xu<author:sep>Yang Zhou<author:sep>Serena Yeung-Levy<author:sep>Jimei Yang;http://arxiv.org/pdf/2401.12175v1;cs.CV;;nerf
2401.11985v1;http://arxiv.org/abs/2401.11985v1;2024-01-22;Scaling Face Interaction Graph Networks to Real World Scenes;"Accurately simulating real world object dynamics is essential for various
applications such as robotics, engineering, graphics, and design. To better
capture complex real dynamics such as contact and friction, learned simulators
based on graph networks have recently shown great promise. However, applying
these learned simulators to real scenes comes with two major challenges: first,
scaling learned simulators to handle the complexity of real world scenes which
can involve hundreds of objects each with complicated 3D shapes, and second,
handling inputs from perception rather than 3D state information. Here we
introduce a method which substantially reduces the memory required to run
graph-based learned simulators. Based on this memory-efficient simulation
model, we then present a perceptual interface in the form of editable NeRFs
which can convert real-world scenes into a structured representation that can
be processed by graph network simulator. We show that our method uses
substantially less memory than previous graph-based simulators while retaining
their accuracy, and that the simulators learned in synthetic environments can
be applied to real world scenes captured from multiple camera angles. This
paves the way for expanding the application of learned simulators to settings
where only perceptual information is available at inference time.";Tatiana Lopez-Guevara<author:sep>Yulia Rubanova<author:sep>William F. Whitney<author:sep>Tobias Pfaff<author:sep>Kimberly Stachenfeld<author:sep>Kelsey R. Allen;http://arxiv.org/pdf/2401.11985v1;cs.LG;16 pages, 12 figures;nerf
2401.11535v1;http://arxiv.org/abs/2401.11535v1;2024-01-21;Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting;"Surgical 3D reconstruction is a critical area of research in robotic surgery,
with recent works adopting variants of dynamic radiance fields to achieve
success in 3D reconstruction of deformable tissues from single-viewpoint
videos. However, these methods often suffer from time-consuming optimization or
inferior quality, limiting their adoption in downstream tasks. Inspired by 3D
Gaussian Splatting, a recent trending 3D representation, we present EndoGS,
applying Gaussian Splatting for deformable endoscopic tissue reconstruction.
Specifically, our approach incorporates deformation fields to handle dynamic
scenes, depth-guided supervision to optimize 3D targets with a single
viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a
result, EndoGS reconstructs and renders high-quality deformable endoscopic
tissues from a single-viewpoint video, estimated depth maps, and labeled tool
masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS
achieves superior rendering quality. Code is available at
https://github.com/HKU-MedAI/EndoGS.";Lingting Zhu<author:sep>Zhao Wang<author:sep>Zhenchao Jin<author:sep>Guying Lin<author:sep>Lequan Yu;http://arxiv.org/pdf/2401.11535v1;cs.CV;Work in progress. 10 pages, 4 figures;gaussian splatting
2401.09720v2;http://arxiv.org/abs/2401.09720v2;2024-01-18;GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting;"In this work, we propose a novel clothed human reconstruction method called
GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural
radiance based models, 3D Gaussian Splatting has recently demonstrated great
performance in terms of training time and rendering quality. However, applying
the static 3D Gaussian Splatting model to the dynamic human reconstruction
problem is non-trivial due to complicated non-rigid deformations and rich cloth
details. To address these challenges, our method considers explicit pose-guided
deformation to associate dynamic Gaussians across the canonical space and the
observation space, introducing a physically-based prior with regularized
transformations helps mitigate ambiguity between the two spaces. During the
training process, we further propose a pose refinement strategy to update the
pose regression for compensating the inaccurate initial estimation and a
split-with-scale mechanism to enhance the density of regressed point clouds.
The experiments validate that our method can achieve state-of-the-art
photorealistic novel-view rendering results with high-quality details for
dynamic clothed human bodies, along with explicit geometry reconstruction.";Mengtian Li<author:sep>Shengxiang Yao<author:sep>Zhifeng Xie<author:sep>Keyu Chen;http://arxiv.org/pdf/2401.09720v2;cs.CV;;gaussian splatting
2401.09495v4;http://arxiv.org/abs/2401.09495v4;2024-01-17;IPR-NeRF: Ownership Verification meets Neural Radiance Field;"Neural Radiance Field (NeRF) models have gained significant attention in the
computer vision community in the recent past with state-of-the-art visual
quality and produced impressive demonstrations. Since then, technopreneurs have
sought to leverage NeRF models into a profitable business. Therefore, NeRF
models make it worth the risk of plagiarizers illegally copying,
re-distributing, or misusing those models. This paper proposes a comprehensive
intellectual property (IP) protection framework for the NeRF model in both
black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a
diffusion-based solution is introduced to embed and extract the watermark via a
two-stage optimization process. In the white-box setting, a designated digital
signature is embedded into the weights of the NeRF model by adopting the sign
loss objective. Our extensive experiments demonstrate that not only does our
approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models,
but it is also robust against both ambiguity and removal attacks compared to
prior arts.";Win Kent Ong<author:sep>Kam Woh Ng<author:sep>Chee Seng Chan<author:sep>Yi Zhe Song<author:sep>Tao Xiang;http://arxiv.org/pdf/2401.09495v4;cs.CV;"Error on result tabulation of state of the art method which might
  cause misleading to readers";nerf
2401.08937v1;http://arxiv.org/abs/2401.08937v1;2024-01-17;ICON: Incremental CONfidence for Joint Pose and Radiance Field  Optimization;"Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View
Synthesis (NVS) given a set of 2D images. However, NeRF training requires
accurate camera pose for each input view, typically obtained by
Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax
this constraint, but they still often rely on decent initial poses which they
can refine. Here we aim at removing the requirement for pose initialization. We
present Incremental CONfidence (ICON), an optimization procedure for training
NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate
initial guess for poses. Further, ICON introduces ``confidence"": an adaptive
measure of model quality used to dynamically reweight gradients. ICON relies on
high-confidence poses to learn NeRF, and high-confidence 3D structure (as
encoded by NeRF) to learn poses. We show that ICON, without prior pose
initialization, achieves superior performance in both CO3D and HO3D versus
methods which use SfM pose.";Weiyao Wang<author:sep>Pierre Gleize<author:sep>Hao Tang<author:sep>Xingyu Chen<author:sep>Kevin J Liang<author:sep>Matt Feiszli;http://arxiv.org/pdf/2401.08937v1;cs.CV;;nerf
2401.08045v1;http://arxiv.org/abs/2401.08045v1;2024-01-16;Forging Vision Foundation Models for Autonomous Driving: Challenges,  Methodologies, and Opportunities;"The rise of large foundation models, trained on extensive datasets, is
revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4
showcase their adaptability by extracting intricate patterns and performing
effectively across diverse tasks, thereby serving as potent building blocks for
a wide range of AI applications. Autonomous driving, a vibrant front in AI
applications, remains challenged by the lack of dedicated vision foundation
models (VFMs). The scarcity of comprehensive training data, the need for
multi-sensor integration, and the diverse task-specific architectures pose
significant obstacles to the development of VFMs in this field. This paper
delves into the critical challenge of forging VFMs tailored specifically for
autonomous driving, while also outlining future directions. Through a
systematic analysis of over 250 papers, we dissect essential techniques for VFM
development, including data preparation, pre-training strategies, and
downstream task adaptation. Moreover, we explore key advancements such as NeRF,
diffusion models, 3D Gaussian Splatting, and world models, presenting a
comprehensive roadmap for future research. To empower researchers, we have
built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an
open-access repository constantly updated with the latest advancements in
forging VFMs for autonomous driving.";Xu Yan<author:sep>Haiming Zhang<author:sep>Yingjie Cai<author:sep>Jingming Guo<author:sep>Weichao Qiu<author:sep>Bin Gao<author:sep>Kaiqiang Zhou<author:sep>Yue Zhao<author:sep>Huan Jin<author:sep>Jiantao Gao<author:sep>Zhen Li<author:sep>Lihui Jiang<author:sep>Wei Zhang<author:sep>Hongbo Zhang<author:sep>Dengxin Dai<author:sep>Bingbing Liu;http://arxiv.org/pdf/2401.08045v1;cs.CV;Github Repo: https://github.com/zhanghm1995/Forge_VFM4AD;gaussian splatting<tag:sep>nerf
2401.08742v1;http://arxiv.org/abs/2401.08742v1;2024-01-16;Fast Dynamic 3D Object Generation from a Single-view Video;"Generating dynamic three-dimensional (3D) object from a single-view video is
challenging due to the lack of 4D labeled data. Existing methods extend
text-to-3D pipelines by transferring off-the-shelf image generation models such
as score distillation sampling, but they are slow and expensive to scale (e.g.,
150 minutes per object) due to the need for back-propagating the
information-limited supervision signals through a large pretrained model. To
address this limitation, we propose an efficient video-to-4D object generation
framework called Efficient4D. It generates high-quality spacetime-consistent
images under different camera views, and then uses them as labeled data to
directly train a novel 4D Gaussian splatting model with explicit point cloud
geometry, enabling real-time rendering under continuous camera trajectories.
Extensive experiments on synthetic and real videos show that Efficient4D offers
a remarkable 10-fold increase in speed when compared to prior art alternatives
while preserving the same level of innovative view synthesis quality. For
example, Efficient4D takes only 14 minutes to model a dynamic object.";Zijie Pan<author:sep>Zeyu Yang<author:sep>Xiatian Zhu<author:sep>Li Zhang;http://arxiv.org/pdf/2401.08742v1;cs.CV;Technical report;gaussian splatting
2401.08140v2;http://arxiv.org/abs/2401.08140v2;2024-01-16;ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process;"Neural radiance fields (NeRFs) have gained popularity across various
applications. However, they face challenges in the sparse view setting, lacking
sufficient constraints from volume rendering. Reconstructing and understanding
a 3D scene from sparse and unconstrained cameras is a long-standing problem in
classical computer vision with diverse applications. While recent works have
explored NeRFs in sparse, unconstrained view scenarios, their focus has been
primarily on enhancing reconstruction and novel view synthesis. Our approach
takes a broader perspective by posing the question: ""from where has each point
been seen?"" -- which gates how well we can understand and reconstruct it. In
other words, we aim to determine the origin or provenance of each 3D point and
its associated information under sparse, unconstrained views. We introduce
ProvNeRF, a model that enriches a traditional NeRF representation by
incorporating per-point provenance, modeling likely source locations for each
point. We achieve this by extending implicit maximum likelihood estimation
(IMLE) for stochastic processes. Notably, our method is compatible with any
pre-trained NeRF model and the associated training camera poses. We demonstrate
that modeling per-point provenance offers several advantages, including
uncertainty estimation, criteria-based view selection, and improved novel view
synthesis, compared to state-of-the-art methods. Please visit our project page
at https://provnerf.github.io";Kiyohiro Nakayama<author:sep>Mikaela Angelina Uy<author:sep>Yang You<author:sep>Ke Li<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2401.08140v2;cs.CV;;nerf
2401.07935v1;http://arxiv.org/abs/2401.07935v1;2024-01-15;6-DoF Grasp Pose Evaluation and Optimization via Transfer Learning from  NeRFs;"We address the problem of robotic grasping of known and unknown objects using
implicit behavior cloning. We train a grasp evaluation model from a small
number of demonstrations that outputs higher values for grasp candidates that
are more likely to succeed in grasping. This evaluation model serves as an
objective function, that we maximize to identify successful grasps. Key to our
approach is the utilization of learned implicit representations of visual and
geometric features derived from a pre-trained NeRF. Though trained exclusively
in a simulated environment with simplified objects and 4-DoF top-down grasps,
our evaluation model and optimization procedure demonstrate generalization to
6-DoF grasps and novel objects both in simulation and in real-world settings,
without the need for additional data. Supplementary material is available at:
https://gergely-soti.github.io/grasp";Gergely SÃ³ti<author:sep>Xi Huang<author:sep>Christian Wurll<author:sep>BjÃ¶rn Hein;http://arxiv.org/pdf/2401.07935v1;cs.RO;;nerf
2401.06191v1;http://arxiv.org/abs/2401.06191v1;2024-01-11;TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation;"In recent years, the neural radiance field (NeRF) model has gained popularity
due to its ability to recover complex 3D scenes. Following its success, many
approaches proposed different NeRF representations in order to further improve
both runtime and performance. One such example is Triplane, in which NeRF is
represented using three 2D feature planes. This enables easily using existing
2D neural networks in this framework, e.g., to generate the three planes.
Despite its advantage, the triplane representation lagged behind in its 3D
recovery quality compared to NeRF solutions. In this work, we propose
TriNeRFLet, a 2D wavelet-based multiscale triplane representation for NeRF,
which closes the 3D recovery performance gap and is competitive with current
state-of-the-art methods. Building upon the triplane framework, we also propose
a novel super-resolution (SR) technique that combines a diffusion model with
TriNeRFLet for improving NeRF resolution.";Rajaei Khatib<author:sep>Raja Giryes;http://arxiv.org/pdf/2401.06191v1;cs.CV;webpage link: https://rajaeekh.github.io/trinerflet-web;nerf
2401.06003v1;http://arxiv.org/abs/2401.06003v1;2024-01-11;TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering;"Point-based radiance field rendering has demonstrated impressive results for
novel view synthesis, offering a compelling blend of rendering quality and
computational efficiency. However, also latest approaches in this domain are
not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.
2023] struggles when tasked with rendering highly detailed scenes, due to
blurring and cloudy artifacts. On the other hand, ADOP [R\""uckert et al. 2022]
can accommodate crisper images, but the neural reconstruction network decreases
performance, it grapples with temporal instability and it is unable to
effectively address large gaps in the point cloud.
  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that
combines ideas from both Gaussian Splatting and ADOP. The fundamental concept
behind our novel technique involves rasterizing points into a screen-space
image pyramid, with the selection of the pyramid layer determined by the
projected point size. This approach allows rendering arbitrarily large points
using a single trilinear write. A lightweight neural network is then used to
reconstruct a hole-free image including detail beyond splat resolution.
Importantly, our render pipeline is entirely differentiable, allowing for
automatic optimization of both point sizes and positions.
  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art
methods in terms of rendering quality while maintaining a real-time frame rate
of 60 frames per second on readily available hardware. This performance extends
to challenging scenarios, such as scenes featuring intricate geometry,
expansive landscapes, and auto-exposed footage.";Linus Franke<author:sep>Darius RÃ¼ckert<author:sep>Laura Fink<author:sep>Marc Stamminger;http://arxiv.org/pdf/2401.06003v1;cs.CV;;gaussian splatting
2401.05750v1;http://arxiv.org/abs/2401.05750v1;2024-01-11;GO-NeRF: Generating Virtual Objects in Neural Radiance Fields;"Despite advances in 3D generation, the direct creation of 3D objects within
an existing 3D scene represented as NeRF remains underexplored. This process
requires not only high-quality 3D object generation but also seamless
composition of the generated 3D content into the existing NeRF. To this end, we
propose a new method, GO-NeRF, capable of utilizing scene context for
high-quality and harmonious 3D object generation within an existing NeRF. Our
method employs a compositional rendering formulation that allows the generated
3D objects to be seamlessly composited into the scene utilizing learned
3D-aware opacity maps without introducing unintended scene modification.
Moreover, we also develop tailored optimization objectives and training
strategies to enhance the model's ability to exploit scene context and mitigate
artifacts, such as floaters, originating from 3D object generation within a
scene. Extensive experiments on both feed-forward and $360^o$ scenes show the
superior performance of our proposed GO-NeRF in generating objects harmoniously
composited with surrounding scenes and synthesizing high-quality novel view
images. Project page at {\url{https://daipengwa.github.io/GO-NeRF/}.";Peng Dai<author:sep>Feitong Tan<author:sep>Xin Yu<author:sep>Yinda Zhang<author:sep>Xiaojuan Qi;http://arxiv.org/pdf/2401.05750v1;cs.CV;12 pages;nerf
2401.05925v2;http://arxiv.org/abs/2401.05925v2;2024-01-11;CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with  Dual Feature Fusion;"We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a
method for compact 3D-consistent scene segmentation at fast rendering speed
with only RGB images input. Previous NeRF-based segmentation methods have
relied on time-consuming neural scene optimization. While recent 3D Gaussian
Splatting has notably improved speed, existing Gaussian-based segmentation
methods struggle to produce compact masks, especially in zero-shot
segmentation. This issue probably stems from their straightforward assignment
of learnable parameters to each Gaussian, resulting in a lack of robustness
against cross-view inconsistent 2D machine-generated labels. Our method aims to
address this problem by employing Dual Feature Fusion Network as Gaussians'
segmentation field. Specifically, we first optimize 3D Gaussians under RGB
supervision. After Gaussian Locating, DINO features extracted from images are
applied through explicit unprojection, which are further incorporated with
spatial features from the efficient point cloud processing network. Feature
aggregation is utilized to fuse them in a global-to-local strategy for compact
segmentation features. Experimental results show that our model outperforms
baselines on both semantic and panoptic zero-shot segmentation task, meanwhile
consumes less than 10\% inference time compared to NeRF-based methods. Code and
more results will be available at https://David-Dou.github.io/CoSSegGaussians.";Bin Dou<author:sep>Tianyu Zhang<author:sep>Yongjia Ma<author:sep>Zhaohui Wang<author:sep>Zejian Yuan;http://arxiv.org/pdf/2401.05925v2;cs.CV;Correct writing details;nerf
2401.06052v1;http://arxiv.org/abs/2401.06052v1;2024-01-11;Fast High Dynamic Range Radiance Fields for Dynamic Scenes;"Neural Radiances Fields (NeRF) and their extensions have shown great success
in representing 3D scenes and synthesizing novel-view images. However, most
NeRF methods take in low-dynamic-range (LDR) images, which may lose details,
especially with nonuniform illumination. Some previous NeRF methods attempt to
introduce high-dynamic-range (HDR) techniques but mainly target static scenes.
To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF
framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images
captured with various exposures. A learnable exposure mapping function is
constructed to obtain adaptive exposure values for each image. Based on the
monotonically increasing prior, a camera response function is designed for
stable learning. With the proposed model, high-quality novel-view images at any
time point can be rendered with any desired exposure. We further construct a
dataset containing multiple dynamic scenes captured with diverse exposures for
evaluation. All the datasets and code are available at
\url{https://guanjunwu.github.io/HDR-HexPlane/}.";Guanjun Wu<author:sep>Taoran Yi<author:sep>Jiemin Fang<author:sep>Wenyu Liu<author:sep>Xinggang Wang;http://arxiv.org/pdf/2401.06052v1;cs.CV;3DV 2024. Project page: https://guanjunwu.github.io/HDR-HexPlane;nerf
2401.06116v1;http://arxiv.org/abs/2401.06116v1;2024-01-11;Gaussian Shadow Casting for Neural Characters;"Neural character models can now reconstruct detailed geometry and texture
from video, but they lack explicit shadows and shading, leading to artifacts
when generating novel views and poses or during relighting. It is particularly
difficult to include shadows as they are a global effect and the required
casting of secondary rays is costly. We propose a new shadow model using a
Gaussian density proxy that replaces sampling with a simple analytic formula.
It supports dynamic motion and is tailored for shadow computation, thereby
avoiding the affine projection approximation and sorting required by the
closely related Gaussian splatting. Combined with a deferred neural rendering
model, our Gaussian shadows enable Lambertian shading and shadow casting with
minimal overhead. We demonstrate improved reconstructions, with better
separation of albedo, shading, and shadows in challenging outdoor scenes with
direct sun light and hard shadows. Our method is able to optimize the light
direction without any input from the user. As a result, novel poses have fewer
shadow artifacts and relighting in novel scenes is more realistic compared to
the state-of-the-art methods, providing new ways to pose neural characters in
novel environments, increasing their applicability.";Luis Bolanos<author:sep>Shih-Yang Su<author:sep>Helge Rhodin;http://arxiv.org/pdf/2401.06116v1;cs.CV;14 pages, 13 figures;gaussian splatting
2401.05583v1;http://arxiv.org/abs/2401.05583v1;2024-01-10;Diffusion Priors for Dynamic View Synthesis from Monocular Videos;"Dynamic novel view synthesis aims to capture the temporal evolution of visual
content within videos. Existing methods struggle to distinguishing between
motion and structure, particularly in scenarios where camera poses are either
unknown or constrained compared to object motion. Furthermore, with information
solely from reference images, it is extremely challenging to hallucinate unseen
regions that are occluded or partially observed in the given videos. To address
these issues, we first finetune a pretrained RGB-D diffusion model on the video
frames using a customization technique. Subsequently, we distill the knowledge
from the finetuned model to a 4D representations encompassing both dynamic and
static Neural Radiance Fields (NeRF) components. The proposed pipeline achieves
geometric consistency while preserving the scene identity. We perform thorough
experiments to evaluate the efficacy of the proposed method qualitatively and
quantitatively. Our results demonstrate the robustness and utility of our
approach in challenging cases, further advancing dynamic novel view synthesis.";Chaoyang Wang<author:sep>Peiye Zhuang<author:sep>Aliaksandr Siarohin<author:sep>Junli Cao<author:sep>Guocheng Qian<author:sep>Hsin-Ying Lee<author:sep>Sergey Tulyakov;http://arxiv.org/pdf/2401.05583v1;cs.CV;;nerf
2401.05335v1;http://arxiv.org/abs/2401.05335v1;2024-01-10;InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes;"We introduce InseRF, a novel method for generative object insertion in the
NeRF reconstructions of 3D scenes. Based on a user-provided textual description
and a 2D bounding box in a reference viewpoint, InseRF generates new objects in
3D scenes. Recently, methods for 3D scene editing have been profoundly
transformed, owing to the use of strong priors of text-to-image diffusion
models in 3D generative modeling. Existing methods are mostly effective in
editing 3D scenes via style and appearance changes or removing existing
objects. Generating new objects, however, remains a challenge for such methods,
which we address in this study. Specifically, we propose grounding the 3D
object insertion to a 2D object insertion in a reference view of the scene. The
2D edit is then lifted to 3D using a single-view object reconstruction method.
The reconstructed object is then inserted into the scene, guided by the priors
of monocular depth estimation methods. We evaluate our method on various 3D
scenes and provide an in-depth analysis of the proposed components. Our
experiments with generative insertion of objects in several 3D scenes indicate
the effectiveness of our method compared to the existing methods. InseRF is
capable of controllable and 3D-consistent object insertion without requiring
explicit 3D information as input. Please visit our project page at
https://mohamad-shahbazi.github.io/inserf.";Mohamad Shahbazi<author:sep>Liesbeth Claessens<author:sep>Michael Niemeyer<author:sep>Edo Collins<author:sep>Alessio Tonioni<author:sep>Luc Van Gool<author:sep>Federico Tombari;http://arxiv.org/pdf/2401.05335v1;cs.CV;;nerf
2401.04861v1;http://arxiv.org/abs/2401.04861v1;2024-01-10;CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from  Monocular Video;"The goal of our work is to generate high-quality novel views from monocular
videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have
shown impressive performance by leveraging time-varying dynamic radiation
fields. However, these methods have limitations when it comes to accurately
modeling the motion of complex objects, which can lead to inaccurate and blurry
renderings of details. To address this limitation, we propose a novel approach
that builds upon a recent generalization NeRF, which aggregates nearby views
onto new viewpoints. However, such methods are typically only effective for
static scenes. To overcome this challenge, we introduce a module that operates
in both the time and frequency domains to aggregate the features of object
motion. This allows us to learn the relationship between frames and generate
higher-quality images. Our experiments demonstrate significant improvements
over state-of-the-art methods on dynamic scene datasets. Specifically, our
approach outperforms existing methods in terms of both the accuracy and visual
quality of the synthesized views.";Xingyu Miao<author:sep>Yang Bai<author:sep>Haoran Duan<author:sep>Yawen Huang<author:sep>Fan Wan<author:sep>Yang Long<author:sep>Yefeng Zheng;http://arxiv.org/pdf/2401.04861v1;cs.CV;;nerf
2401.05516v1;http://arxiv.org/abs/2401.05516v1;2024-01-10;FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D  Neural Radiance Fields;"We present FPRF, a feed-forward photorealistic style transfer method for
large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with
arbitrary, multiple style reference images without additional optimization
while preserving multi-view appearance consistency. Prior arts required tedious
per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF
efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D
neural radiance field, which inherits AdaIN's feed-forward stylization
machinery, supporting arbitrary style reference images. Furthermore, FPRF
supports multi-reference stylization with the semantic correspondence matching
and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also
preserves multi-view consistency by applying semantic matching and style
transfer processes directly onto queried features in 3D space. In experiments,
we demonstrate that FPRF achieves favorable photorealistic quality 3D scene
stylization for large-scale scenes with diverse reference images. Project page:
https://kim-geonu.github.io/FPRF/";GeonU Kim<author:sep>Kim Youwang<author:sep>Tae-Hyun Oh;http://arxiv.org/pdf/2401.05516v1;cs.CV;Project page: https://kim-geonu.github.io/FPRF/;
2401.03890v1;http://arxiv.org/abs/2401.03890v1;2024-01-08;A Survey on 3D Gaussian Splatting;"3D Gaussian splatting (3D GS) has recently emerged as a transformative
technique in the explicit radiance field and computer graphics landscape. This
innovative approach, characterized by the utilization of millions of 3D
Gaussians, represents a significant departure from the neural radiance field
(NeRF) methodologies, which predominantly use implicit, coordinate-based models
to map spatial coordinates to pixel values. 3D GS, with its explicit scene
representations and differentiable rendering algorithms, not only promises
real-time rendering capabilities but also introduces unprecedented levels of
control and editability. This positions 3D GS as a potential game-changer for
the next generation of 3D reconstruction and representation. In the present
paper, we provide the first systematic overview of the recent developments and
critical contributions in the domain of 3D GS. We begin with a detailed
exploration of the underlying principles and the driving forces behind the
advent of 3D GS, setting the stage for understanding its significance. A focal
point of our discussion is the practical applicability of 3D GS. By
facilitating real-time performance, 3D GS opens up a plethora of applications,
ranging from virtual reality to interactive media and beyond. This is
complemented by a comparative analysis of leading 3D GS models, evaluated
across various benchmark tasks to highlight their performance and practical
utility. The survey concludes by identifying current challenges and suggesting
potential avenues for future research in this domain. Through this survey, we
aim to provide a valuable resource for both newcomers and seasoned researchers,
fostering further exploration and advancement in applicable and explicit
radiance field representation.";Guikun Chen<author:sep>Wenguan Wang;http://arxiv.org/pdf/2401.03890v1;cs.CV;Ongoing project;gaussian splatting<tag:sep>nerf
2401.03771v1;http://arxiv.org/abs/2401.03771v1;2024-01-08;NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation;"The capabilities of monocular depth estimation (MDE) models are limited by
the availability of sufficient and diverse datasets. In the case of MDE models
for autonomous driving, this issue is exacerbated by the linearity of the
captured data trajectories. We propose a NeRF-based data augmentation pipeline
to introduce synthetic data with more diverse viewing directions into training
datasets and demonstrate the benefits of our approach to model performance and
robustness. Our data augmentation pipeline, which we call ""NeRFmentation"",
trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on
relevant metrics, and uses them to generate synthetic RGB-D images captured
from new viewing directions. In this work, we apply our technique in
conjunction with three state-of-the-art MDE architectures on the popular
autonomous driving dataset KITTI, augmenting its training set of the Eigen
split. We evaluate the resulting performance gain on the original test set, a
separate popular driving set, and our own synthetic test set.";Casimir Feldmann<author:sep>Niall Siegenheim<author:sep>Nikolas Hars<author:sep>Lovro Rabuzin<author:sep>Mert Ertugrul<author:sep>Luca Wolfart<author:sep>Marc Pollefeys<author:sep>Zuria Bauer<author:sep>Martin R. Oswald;http://arxiv.org/pdf/2401.03771v1;cs.CV;;nerf
2401.04099v1;http://arxiv.org/abs/2401.04099v1;2024-01-08;AGG: Amortized Generative 3D Gaussians for Single Image to 3D;"Given the growing need for automatic 3D content creation pipelines, various
3D representations have been studied to generate 3D objects from a single
image. Due to its superior rendering efficiency, 3D Gaussian splatting-based
models have recently excelled in both 3D reconstruction and generation. 3D
Gaussian splatting approaches for image to 3D generation are often
optimization-based, requiring many computationally expensive score-distillation
steps. To overcome these challenges, we introduce an Amortized Generative 3D
Gaussian framework (AGG) that instantly produces 3D Gaussians from a single
image, eliminating the need for per-instance optimization. Utilizing an
intermediate hybrid representation, AGG decomposes the generation of 3D
Gaussian locations and other appearance attributes for joint optimization.
Moreover, we propose a cascaded pipeline that first generates a coarse
representation of the 3D data and later upsamples it with a 3D Gaussian
super-resolution module. Our method is evaluated against existing
optimization-based 3D Gaussian frameworks and sampling-based pipelines
utilizing other 3D representations, where AGG showcases competitive generation
abilities both qualitatively and quantitatively while being several orders of
magnitude faster. Project page: https://ir1d.github.io/AGG/";Dejia Xu<author:sep>Ye Yuan<author:sep>Morteza Mardani<author:sep>Sifei Liu<author:sep>Jiaming Song<author:sep>Zhangyang Wang<author:sep>Arash Vahdat;http://arxiv.org/pdf/2401.04099v1;cs.CV;Project page: https://ir1d.github.io/AGG/;gaussian splatting
2401.03257v1;http://arxiv.org/abs/2401.03257v1;2024-01-06;RustNeRF: Robust Neural Radiance Field with Low-Quality Images;"Recent work on Neural Radiance Fields (NeRF) exploits multi-view 3D
consistency, achieving impressive results in 3D scene modeling and
high-fidelity novel-view synthesis. However, there are limitations. First,
existing methods assume enough high-quality images are available for training
the NeRF model, ignoring real-world image degradation. Second, previous methods
struggle with ambiguity in the training set due to unmodeled inconsistencies
among different views. In this work, we present RustNeRF for real-world
high-quality NeRF. To improve NeRF's robustness under real-world inputs, we
train a 3D-aware preprocessing network that incorporates real-world degradation
modeling. We propose a novel implicit multi-view guidance to address
information loss during image degradation and restoration. Extensive
experiments demonstrate RustNeRF's advantages over existing approaches under
real-world degradation. The code will be released.";Mengfei Li<author:sep>Ming Lu<author:sep>Xiaofang Li<author:sep>Shanghang Zhang;http://arxiv.org/pdf/2401.03257v1;cs.CV;;nerf
2401.03203v1;http://arxiv.org/abs/2401.03203v1;2024-01-06;Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity  Monocular Dense Mapping;"In this paper, we introduce Hi-Map, a novel monocular dense mapping approach
based on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to
achieve efficient and high-fidelity mapping using only posed RGB inputs. Our
method eliminates the need for external depth priors derived from e.g., a depth
estimation model. Our key idea is to represent the scene as a hierarchical
feature grid that encodes the radiance and then factorizes it into feature
planes and vectors. As such, the scene representation becomes simpler and more
generalizable for fast and smooth convergence on new observations. This allows
for efficient computation while alleviating noise patterns by reducing the
complexity of the scene representation. Buttressed by the hierarchical
factorized representation, we leverage the Sign Distance Field (SDF) as a proxy
of rendering for inferring the volume density, demonstrating high mapping
fidelity. Moreover, we introduce a dual-path encoding strategy to strengthen
the photometric cues and further boost the mapping quality, especially for the
distant and textureless regions. Extensive experiments demonstrate our method's
superiority in geometric and textural accuracy over the state-of-the-art
NeRF-based monocular mapping methods.";Tongyan Hua<author:sep>Haotian Bai<author:sep>Zidong Cao<author:sep>Ming Liu<author:sep>Dacheng Tao<author:sep>Lin Wang;http://arxiv.org/pdf/2401.03203v1;cs.CV;;nerf
2401.02620v1;http://arxiv.org/abs/2401.02620v1;2024-01-05;Progress and Prospects in 3D Generative AI: A Technical Overview  including 3D human;"While AI-generated text and 2D images continue to expand its territory, 3D
generation has gradually emerged as a trend that cannot be ignored. Since the
year 2023 an abundant amount of research papers has emerged in the domain of 3D
generation. This growth encompasses not just the creation of 3D objects, but
also the rapid development of 3D character and motion generation. Several key
factors contribute to this progress. The enhanced fidelity in stable diffusion,
coupled with control methods that ensure multi-view consistency, and realistic
human models like SMPL-X, contribute synergistically to the production of 3D
models with remarkable consistency and near-realistic appearances. The
advancements in neural network-based 3D storing and rendering models, such as
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have
accelerated the efficiency and realism of neural rendered models. Furthermore,
the multimodality capabilities of large language models have enabled language
inputs to transcend into human motion outputs. This paper aims to provide a
comprehensive overview and summary of the relevant papers published mostly
during the latter half year of 2023. It will begin by discussing the AI
generated object models in 3D, followed by the generated 3D human models, and
finally, the generated 3D human motions, culminating in a conclusive summary
and a vision for the future.";Song Bai<author:sep>Jie Li;http://arxiv.org/pdf/2401.02620v1;cs.AI;;gaussian splatting<tag:sep>nerf
2401.02588v1;http://arxiv.org/abs/2401.02588v1;2024-01-05;Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting;"The accelerating deployment of spacecraft in orbit have generated interest in
on-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possible unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. This requires robust characterization of the
target's geometry. In this article, we present an approach for mapping
geometries of satellites on orbit based on 3D Gaussian Splatting that can run
on computing resources available on current spaceflight hardware. We
demonstrate model training and 3D rendering performance on a
hardware-in-the-loop satellite mock-up under several realistic lighting and
motion conditions. Our model is shown to be capable of training on-board and
rendering higher quality novel views of an unknown satellite nearly 2 orders of
magnitude faster than previous NeRF-based algorithms. Such on-board
capabilities are critical to enable downstream machine intelligence tasks
necessary for autonomous guidance, navigation, and control tasks.";Van Minh Nguyen<author:sep>Emma Sandidge<author:sep>Trupti Mahendrakar<author:sep>Ryan T. White;http://arxiv.org/pdf/2401.02588v1;cs.CV;11 pages, 5 figures;gaussian splatting<tag:sep>nerf
2401.02616v1;http://arxiv.org/abs/2401.02616v1;2024-01-05;FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face  Video Editing on Dynamic NeRF;"The success of the GAN-NeRF structure has enabled face editing on NeRF to
maintain 3D view consistency. However, achieving simultaneously multi-view
consistency and temporal coherence while editing video sequences remains a
formidable challenge. This paper proposes a novel face video editing
architecture built upon the dynamic face GAN-NeRF structure, which effectively
utilizes video sequences to restore the latent code and 3D face geometry. By
editing the latent code, multi-view consistent editing on the face can be
ensured, as validated by multiview stereo reconstruction on the resulting
edited images in our dynamic NeRF. As the estimation of face geometries occurs
on a frame-by-frame basis, this may introduce a jittering issue. We propose a
stabilizer that maintains temporal coherence by preserving smooth changes of
face expressions in consecutive frames. Quantitative and qualitative analyses
reveal that our method, as the pioneering 4D face video editor, achieves
state-of-the-art performance in comparison to existing 2D or 3D-based
approaches independently addressing identity and motion. Codes will be
released.";Hao Zhang<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2401.02616v1;cs.CV;Our code will be available at: https://github.com/ZHANG1023/FED-NeRF;nerf
2401.02281v1;http://arxiv.org/abs/2401.02281v1;2024-01-04;PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for  6DOF Object Pose Dataset Generation;"We introduce Physically Enhanced Gaussian Splatting Simulation System
(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset
generator based on 3D Gaussian Splatting. Environment and object
representations can be easily obtained using commodity cameras to reconstruct
with Gaussian Splatting. PEGASUS allows the composition of new scenes by
merging the respective underlying Gaussian Splatting point cloud of an
environment with one or multiple objects. Leveraging a physics engine enables
the simulation of natural object placement within a scene through interaction
between meshes extracted for the objects and the environment. Consequently, an
extensive amount of new scenes - static or dynamic - can be created by
combining different environments and objects. By rendering scenes from various
perspectives, diverse data points such as RGB images, depth maps, semantic
masks, and 6DoF object poses can be extracted. Our study demonstrates that
training on data generated by PEGASUS enables pose estimation networks to
successfully transfer from synthetic data to real-world data. Moreover, we
introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This
dataset includes spherical scans that captures images from both object
hemisphere and the Gaussian Splatting reconstruction, making them compatible
with PEGASUS.";Lukas Meyer<author:sep>Floris Erich<author:sep>Yusuke Yoshiyasu<author:sep>Marc Stamminger<author:sep>Noriaki Ando<author:sep>Yukiyasu Domae;http://arxiv.org/pdf/2401.02281v1;cs.CV;Project Page: https://meyerls.github.io/pegasus_web;gaussian splatting
2401.01970v1;http://arxiv.org/abs/2401.01970v1;2024-01-03;FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D  Scene Understanding;"Precisely perceiving the geometric and semantic properties of real-world 3D
objects is crucial for the continued evolution of augmented reality and robotic
applications. To this end, we present \algfull{} (\algname{}), which
incorporates vision-language embeddings of foundation models into 3D Gaussian
Splatting (GS). The key contribution of this work is an efficient method to
reconstruct and represent 3D vision-language models. This is achieved by
distilling feature maps generated from image-based foundation models into those
rendered from our 3D model. To ensure high-quality rendering and fast training,
we introduce a novel scene representation by integrating strengths from both GS
and multi-resolution hash encodings (MHE). Our effective training procedure
also introduces a pixel alignment loss that makes the rendered feature distance
of same semantic entities close, following the pixel-level semantic boundaries.
Our results demonstrate remarkable multi-view semantic consistency,
facilitating diverse downstream tasks, beating state-of-the-art methods by
$\mathbf{10.2}$ percent on open-vocabulary language-based object detection,
despite that we are $\mathbf{851\times}$ faster for inference. This research
explores the intersection of vision, language, and 3D scene representation,
paving the way for enhanced scene understanding in uncontrolled real-world
environments. We plan to release the code upon paper acceptance.";Xingxing Zuo<author:sep>Pouya Samangouei<author:sep>Yunwen Zhou<author:sep>Yan Di<author:sep>Mingyang Li;http://arxiv.org/pdf/2401.01970v1;cs.CV;19 pages, Project page coming soon;gaussian splatting
2401.01647v1;http://arxiv.org/abs/2401.01647v1;2024-01-03;SIGNeRF: Scene Integrated Generation for Neural Radiance Fields;"Advances in image diffusion models have recently led to notable improvements
in the generation of high-quality images. In combination with Neural Radiance
Fields (NeRFs), they enabled new opportunities in 3D generation. However, most
generative 3D approaches are object-centric and applying them to editing
existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel
approach for fast and controllable NeRF scene editing and scene-integrated
object generation. A new generative update strategy ensures 3D consistency
across the edited images, without requiring iterative optimization. We find
that depth-conditioned diffusion models inherently possess the capability to
generate 3D consistent views by requesting a grid of images instead of single
views. Based on these insights, we introduce a multi-view reference sheet of
modified images. Our method updates an image collection consistently based on
the reference sheet and refines the original NeRF with the newly generated
image set in one go. By exploiting the depth conditioning mechanism of the
image diffusion model, we gain fine control over the spatial location of the
edit and enforce shape guidance by a selected region or an external mesh.";Jan-Niklas Dihlmann<author:sep>Andreas Engelhardt<author:sep>Hendrik Lensch;http://arxiv.org/pdf/2401.01647v1;cs.CV;Project Page: https://signerf.jdihlmann.com;nerf
2401.01339v1;http://arxiv.org/abs/2401.01339v1;2024-01-02;Street Gaussians for Modeling Dynamic Urban Scenes;"This paper aims to tackle the problem of modeling dynamic urban street scenes
from monocular videos. Recent methods extend NeRF by incorporating tracked
vehicle poses to animate vehicles, enabling photo-realistic view synthesis of
dynamic urban street scenes. However, significant limitations are their slow
training and rendering speed, coupled with the critical need for high precision
in tracked vehicle poses. We introduce Street Gaussians, a new explicit scene
representation that tackles all these limitations. Specifically, the dynamic
urban street is represented as a set of point clouds equipped with semantic
logits and 3D Gaussians, each associated with either a foreground vehicle or
the background. To model the dynamics of foreground object vehicles, each
object point cloud is optimized with optimizable tracked poses, along with a
dynamic spherical harmonics model for the dynamic appearance. The explicit
representation allows easy composition of object vehicles and background, which
in turn allows for scene editing operations and rendering at 133 FPS
(1066$\times$1600 resolution) within half an hour of training. The proposed
method is evaluated on multiple challenging benchmarks, including KITTI and
Waymo Open datasets. Experiments show that the proposed method consistently
outperforms state-of-the-art methods across all datasets. Furthermore, the
proposed representation delivers performance on par with that achieved using
precise ground-truth poses, despite relying only on poses from an off-the-shelf
tracker. The code is available at https://zju3dv.github.io/street_gaussians/.";Yunzhi Yan<author:sep>Haotong Lin<author:sep>Chenxu Zhou<author:sep>Weijie Wang<author:sep>Haiyang Sun<author:sep>Kun Zhan<author:sep>Xianpeng Lang<author:sep>Xiaowei Zhou<author:sep>Sida Peng;http://arxiv.org/pdf/2401.01339v1;cs.CV;Project page: https://zju3dv.github.io/street_gaussians/;nerf
2401.01216v1;http://arxiv.org/abs/2401.01216v1;2024-01-02;Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable  Noise;"Neural radiance fields (NeRF) have been proposed as an innovative 3D
representation method. While attracting lots of attention, NeRF faces critical
issues such as information confidentiality and security. Steganography is a
technique used to embed information in another object as a means of protecting
information security. Currently, there are few related studies on NeRF
steganography, facing challenges in low steganography quality, model weight
damage, and a limited amount of steganographic information. This paper proposes
a novel NeRF steganography method based on trainable noise: Noise-NeRF.
Furthermore, we propose the Adaptive Pixel Selection strategy and Pixel
Perturbation strategy to improve the steganography quality and efficiency. The
extensive experiments on open-source datasets show that Noise-NeRF provides
state-of-the-art performances in both steganography quality and rendering
quality, as well as effectiveness in super-resolution image steganography.";Qinglong Huang<author:sep>Yong Liao<author:sep>Yanbin Hao<author:sep>Pengyuan Zhou;http://arxiv.org/pdf/2401.01216v1;cs.CV;;nerf
2401.00979v1;http://arxiv.org/abs/2401.00979v1;2024-01-02;3D Visibility-aware Generalizable Neural Radiance Fields for Interacting  Hands;"Neural radiance fields (NeRFs) are promising 3D representations for scenes,
objects, and humans. However, most existing methods require multi-view inputs
and per-scene training, which limits their real-life applications. Moreover,
current methods focus on single-subject cases, leaving scenes of interacting
hands that involve severe inter-hand occlusions and challenging view variations
remain unsolved. To tackle these issues, this paper proposes a generalizable
visibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically,
given an image of interacting hands as input, our VA-NeRF first obtains a
mesh-based representation of hands and extracts their corresponding geometric
and textural features. Subsequently, a feature fusion module that exploits the
visibility of query points and mesh vertices is introduced to adaptively merge
features of both hands, enabling the recovery of features in unseen areas.
Additionally, our VA-NeRF is optimized together with a novel discriminator
within an adversarial learning paradigm. In contrast to conventional
discriminators that predict a single real/fake label for the synthesized image,
the proposed discriminator generates a pixel-wise visibility map, providing
fine-grained supervision for unseen areas and encouraging the VA-NeRF to
improve the visual quality of synthesized images. Experiments on the
Interhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms
conventional NeRFs significantly. Project Page:
\url{https://github.com/XuanHuang0/VANeRF}.";Xuan Huang<author:sep>Hanhui Li<author:sep>Zejun Yang<author:sep>Zhisheng Wang<author:sep>Xiaodan Liang;http://arxiv.org/pdf/2401.00979v1;cs.CV;Accepted by AAAI-24;nerf
2401.00616v2;http://arxiv.org/abs/2401.00616v2;2024-01-01;GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for  One-shot Generalizable Neural Radiance Fields;"In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task
which targets synthesizing photo-realistic novel views given only one reference
image per scene. Previous One-shot Generalizable Neural Radiance Fields
(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,
yet suffer the blurry issue due to the encoder-only architecture that highly
relies on the limited reference image. On the other hand, recent
diffusion-based image-to-3d methods show vivid plausible results via distilling
pre-trained 2D diffusion models into a 3D representation, yet require tedious
per-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a
Generative Detail compensation framework via GAN and Diffusion that is both
inference-time finetuning-free and with vivid plausible details. In detail,
following a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a
One-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer
(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model
into the existing OG-NeRF pipeline for primarily relieving the blurry issue
with in-distribution priors captured from the training dataset, achieving a
good balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at
the fine stage, Diff3DE further leverages the pre-trained image diffusion
models to complement rich out-distribution details while maintaining decent 3D
consistency. Extensive experiments on both the synthetic and real-world
datasets show that GD$^2$-NeRF noticeably improves the details while without
per-scene finetuning.";Xiao Pan<author:sep>Zongxin Yang<author:sep>Shuai Bai<author:sep>Yi Yang;http://arxiv.org/pdf/2401.00616v2;cs.CV;"Reading with Macbook Preview is recommended for best quality;
  Submitted to Journal";nerf
2401.00834v1;http://arxiv.org/abs/2401.00834v1;2024-01-01;Deblurring 3D Gaussian Splatting;"Recent studies in Radiance Fields have paved the robust way for novel view
synthesis with their photorealistic rendering quality. Nevertheless, they
usually employ neural networks and volumetric rendering, which are costly to
train and impede their broad use in various real-time applications due to the
lengthy rendering time. Lately 3D Gaussians splatting-based approach has been
proposed to model the 3D scene, and it achieves remarkable visual quality while
rendering the images in real-time. However, it suffers from severe degradation
in the rendering quality if the training images are blurry. Blurriness commonly
occurs due to the lens defocusing, object motion, and camera shake, and it
inevitably intervenes in clean image acquisition. Several previous studies have
attempted to render clean and sharp images from blurry input images using
neural fields. The majority of those works, however, are designed only for
volumetric rendering-based neural radiance fields and are not straightforwardly
applicable to rasterization-based 3D Gaussian splatting methods. Thus, we
propose a novel real-time deblurring framework, deblurring 3D Gaussian
Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the
covariance of each 3D Gaussian to model the scene blurriness. While deblurring
3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct
fine and sharp details from blurry images. A variety of experiments have been
conducted on the benchmark, and the results have revealed the effectiveness of
our approach for deblurring. Qualitative results are available at
https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/";Byeonghyeon Lee<author:sep>Howoong Lee<author:sep>Xiangyu Sun<author:sep>Usman Ali<author:sep>Eunbyung Park;http://arxiv.org/pdf/2401.00834v1;cs.CV;19 pages, 8 figures;gaussian splatting
2401.00825v1;http://arxiv.org/abs/2401.00825v1;2024-01-01;Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using  Sharpness Prior;"Neural Radiance Fields (NeRF) have shown remarkable performance in neural
rendering-based novel view synthesis. However, NeRF suffers from severe visual
quality degradation when the input images have been captured under imperfect
conditions, such as poor illumination, defocus blurring, and lens aberrations.
Especially, defocus blur is quite common in the images when they are normally
captured using cameras. Although few recent studies have proposed to render
sharp images of considerably high-quality, yet they still face many key
challenges. In particular, those methods have employed a Multi-Layer Perceptron
(MLP) based NeRF, which requires tremendous computational time. To overcome
these shortcomings, this paper proposes a novel technique Sharp-NeRF -- a
grid-based NeRF that renders clean and sharp images from the input blurry
images within half an hour of training. To do so, we used several grid-based
kernels to accurately model the sharpness/blurriness of the scene. The
sharpness level of the pixels is computed to learn the spatially varying blur
kernels. We have conducted experiments on the benchmarks consisting of blurry
images and have evaluated full-reference and non-reference metrics. The
qualitative and quantitative results have revealed that our approach renders
the sharp novel views with vivid colors and fine details, and it has
considerably faster training time than the previous works. Our project page is
available at https://benhenryl.github.io/SharpNeRF/";Byeonghyeon Lee<author:sep>Howoong Lee<author:sep>Usman Ali<author:sep>Eunbyung Park;http://arxiv.org/pdf/2401.00825v1;cs.CV;Accepted to WACV 2024;nerf
2401.00208v1;http://arxiv.org/abs/2401.00208v1;2023-12-30;Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with  Generative Diffusion Models;"Current Neural Radiance Fields (NeRF) can generate photorealistic novel
views. For editing 3D scenes represented by NeRF, with the advent of generative
models, this paper proposes Inpaint4DNeRF to capitalize on state-of-the-art
stable diffusion models (e.g., ControlNet) for direct generation of the
underlying completed background content, regardless of static or dynamic. The
key advantages of this generative approach for NeRF inpainting are twofold.
First, after rough mask propagation, to complete or fill in previously occluded
content, we can individually generate a small subset of completed images with
plausible content, called seed images, from which simple 3D geometry proxies
can be derived. Second and the remaining problem is thus 3D multiview
consistency among all completed images, now guided by the seed images and their
3D proxies. Without other bells and whistles, our generative Inpaint4DNeRF
baseline framework is general which can be readily extended to 4D dynamic
NeRFs, where temporal consistency can be naturally handled in a similar way as
our multiview consistency.";Han Jiang<author:sep>Haosen Sun<author:sep>Ruoxuan Li<author:sep>Chi-Keung Tang<author:sep>Yu-Wing Tai;http://arxiv.org/pdf/2401.00208v1;cs.CV;;nerf
2401.00871v1;http://arxiv.org/abs/2401.00871v1;2023-12-30;PlanarNeRF: Online Learning of Planar Primitives with Neural Radiance  Fields;"Identifying spatially complete planar primitives from visual data is a
crucial task in computer vision. Prior methods are largely restricted to either
2D segment recovery or simplifying 3D structures, even with extensive plane
annotations. We present PlanarNeRF, a novel framework capable of detecting
dense 3D planes through online learning. Drawing upon the neural field
representation, PlanarNeRF brings three major contributions. First, it enhances
3D plane detection with concurrent appearance and geometry knowledge. Second, a
lightweight plane fitting module is proposed to estimate plane parameters.
Third, a novel global memory bank structure with an update mechanism is
introduced, ensuring consistent cross-frame correspondence. The flexible
architecture of PlanarNeRF allows it to function in both 2D-supervised and
self-supervised solutions, in each of which it can effectively learn from
sparse training signals, significantly improving training efficiency. Through
extensive experiments, we demonstrate the effectiveness of PlanarNeRF in
various scenarios and remarkable improvement over existing works.";Zheng Chen<author:sep>Qingan Yan<author:sep>Huangying Zhan<author:sep>Changjiang Cai<author:sep>Xiangyu Xu<author:sep>Yuzhong Huang<author:sep>Weihan Wang<author:sep>Ziyue Feng<author:sep>Lantao Liu<author:sep>Yi Xu;http://arxiv.org/pdf/2401.00871v1;cs.CV;;nerf
2312.17561v1;http://arxiv.org/abs/2312.17561v1;2023-12-29;Informative Rays Selection for Few-Shot Neural Radiance Fields;"Neural Radiance Fields (NeRF) have recently emerged as a powerful method for
image-based 3D reconstruction, but the lengthy per-scene optimization limits
their practical usage, especially in resource-constrained settings. Existing
approaches solve this issue by reducing the number of input views and
regularizing the learned volumetric representation with either complex losses
or additional inputs from other modalities. In this paper, we present KeyNeRF,
a simple yet effective method for training NeRF in few-shot scenarios by
focusing on key informative rays. Such rays are first selected at camera level
by a view selection algorithm that promotes baseline diversity while
guaranteeing scene coverage, then at pixel level by sampling from a probability
distribution based on local image entropy. Our approach performs favorably
against state-of-the-art methods, while requiring minimal changes to existing
NeRF codebases.";Marco Orsingher<author:sep>Anthony Dell'Eva<author:sep>Paolo Zani<author:sep>Paolo Medici<author:sep>Massimo Bertozzi;http://arxiv.org/pdf/2312.17561v1;cs.CV;To appear at VISAPP 2024;nerf
2312.17142v2;http://arxiv.org/abs/2312.17142v2;2023-12-28;DreamGaussian4D: Generative 4D Gaussian Splatting;"Remarkable progress has been made in 4D content generation recently. However,
existing methods suffer from long optimization time, lack of motion
controllability, and a low level of detail. In this paper, we introduce
DreamGaussian4D, an efficient 4D generation framework that builds on 4D
Gaussian Splatting representation. Our key insight is that the explicit
modeling of spatial transformations in Gaussian Splatting makes it more
suitable for the 4D generation setting compared with implicit representations.
DreamGaussian4D reduces the optimization time from several hours to just a few
minutes, allows flexible control of the generated 3D motion, and produces
animated meshes that can be efficiently rendered in 3D engines.";Jiawei Ren<author:sep>Liang Pan<author:sep>Jiaxiang Tang<author:sep>Chi Zhang<author:sep>Ang Cao<author:sep>Gang Zeng<author:sep>Ziwei Liu;http://arxiv.org/pdf/2312.17142v2;cs.CV;"Technical report. Project page is at
  https://jiawei-ren.github.io/projects/dreamgaussian4d Code is at
  https://github.com/jiawei-ren/dreamgaussian4d";gaussian splatting
2312.16457v1;http://arxiv.org/abs/2312.16457v1;2023-12-27;City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web;"NeRF has significantly advanced 3D scene reconstruction, capturing intricate
details across various environments. Existing methods have successfully
leveraged radiance field baking to facilitate real-time rendering of small
scenes. However, when applied to large-scale scenes, these techniques encounter
significant challenges, struggling to provide a seamless real-time experience
due to limited resources in computation, memory, and bandwidth. In this paper,
we propose City-on-Web, which represents the whole scene by partitioning it
into manageable blocks, each with its own Level-of-Detail, ensuring high
fidelity, efficient memory management and fast rendering. Meanwhile, we
carefully design the training and inference process such that the final
rendering result on web is consistent with training. Thanks to our novel
representation and carefully designed training/inference process, we are the
first to achieve real-time rendering of large-scale scenes in
resource-constrained environments. Extensive experimental results demonstrate
that our method facilitates real-time rendering of large-scale scenes on a web
platform, achieving 32FPS at 1080P resolution with an RTX 3060 GPU, while
simultaneously achieving a quality that closely rivals that of state-of-the-art
methods. Project page: https://ustc3dv.github.io/City-on-Web/";Kaiwen Song<author:sep>Juyong Zhang;http://arxiv.org/pdf/2312.16457v1;cs.CV;Project page: https://ustc3dv.github.io/City-on-Web/;nerf
2312.16256v2;http://arxiv.org/abs/2312.16256v2;2023-12-26;DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision;"We have witnessed significant progress in deep learning-based 3D vision,
ranging from neural radiance field (NeRF) based 3D representation learning to
applications in novel view synthesis (NVS). However, existing scene-level
datasets for deep learning-based 3D vision, limited to either synthetic
environments or a narrow selection of real-world scenes, are quite
insufficient. This insufficiency not only hinders a comprehensive benchmark of
existing methods but also caps what could be explored in deep learning-based 3D
analysis. To address this critical gap, we present DL3DV-10K, a large-scale
scene dataset, featuring 51.2 million frames from 10,510 videos captured from
65 types of point-of-interest (POI) locations, covering both bounded and
unbounded scenes, with different levels of reflection, transparency, and
lighting. We conducted a comprehensive benchmark of recent NVS methods on
DL3DV-10K, which revealed valuable insights for future research in NVS. In
addition, we have obtained encouraging results in a pilot study to learn
generalizable NeRF from DL3DV-10K, which manifests the necessity of a
large-scale scene-level dataset to forge a path toward a foundation model for
learning 3D representation. Our DL3DV-10K dataset, benchmark results, and
models will be publicly accessible at https://dl3dv-10k.github.io/DL3DV-10K/.";Lu Ling<author:sep>Yichen Sheng<author:sep>Zhi Tu<author:sep>Wentian Zhao<author:sep>Cheng Xin<author:sep>Kun Wan<author:sep>Lantao Yu<author:sep>Qianyu Guo<author:sep>Zixun Yu<author:sep>Yawen Lu<author:sep>Xuanmao Li<author:sep>Xingpeng Sun<author:sep>Rohan Ashok<author:sep>Aniruddha Mukherjee<author:sep>Hao Kang<author:sep>Xiangrui Kong<author:sep>Gang Hua<author:sep>Tianyi Zhang<author:sep>Bedrich Benes<author:sep>Aniket Bera;http://arxiv.org/pdf/2312.16256v2;cs.CV;;nerf
2312.15942v1;http://arxiv.org/abs/2312.15942v1;2023-12-26;Pano-NeRF: Synthesizing High Dynamic Range Novel Views with Geometry  from Sparse Low Dynamic Range Panoramic Images;"Panoramic imaging research on geometry recovery and High Dynamic Range (HDR)
reconstruction becomes a trend with the development of Extended Reality (XR).
Neural Radiance Fields (NeRF) provide a promising scene representation for both
tasks without requiring extensive prior data. However, in the case of inputting
sparse Low Dynamic Range (LDR) panoramic images, NeRF often degrades with
under-constrained geometry and is unable to reconstruct HDR radiance from LDR
inputs. We observe that the radiance from each pixel in panoramic images can be
modeled as both a signal to convey scene lighting information and a light
source to illuminate other pixels. Hence, we propose the irradiance fields from
sparse LDR panoramic images, which increases the observation counts for
faithful geometry recovery and leverages the irradiance-radiance attenuation
for HDR reconstruction. Extensive experiments demonstrate that the irradiance
fields outperform state-of-the-art methods on both geometry recovery and HDR
reconstruction and validate their effectiveness. Furthermore, we show a
promising byproduct of spatially-varying lighting estimation. The code is
available at https://github.com/Lu-Zhan/Pano-NeRF.";Zhan Lu<author:sep>Qian Zheng<author:sep>Boxin Shi<author:sep>Xudong Jiang;http://arxiv.org/pdf/2312.15942v1;cs.CV;;nerf
2312.16084v1;http://arxiv.org/abs/2312.16084v1;2023-12-26;LangSplat: 3D Language Gaussian Splatting;"Human lives in a 3D world and commonly uses natural language to interact with
a 3D scene. Modeling a 3D language field to support open-ended language queries
in 3D has gained increasing attention recently. This paper introduces
LangSplat, which constructs a 3D language field that enables precise and
efficient open-vocabulary querying within 3D spaces. Unlike existing methods
that ground CLIP language embeddings in a NeRF model, LangSplat advances the
field by utilizing a collection of 3D Gaussians, each encoding language
features distilled from CLIP, to represent the language field. By employing a
tile-based splatting technique for rendering language features, we circumvent
the costly rendering process inherent in NeRF. Instead of directly learning
CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and
then learns language features on the scene-specific latent space, thereby
alleviating substantial memory demands imposed by explicit modeling. Existing
methods struggle with imprecise and vague 3D language fields, which fail to
discern clear boundaries between objects. We delve into this issue and propose
to learn hierarchical semantics using SAM, thereby eliminating the need for
extensively querying the language field across various scales and the
regularization of DINO features. Extensive experiments on open-vocabulary 3D
object localization and semantic segmentation demonstrate that LangSplat
significantly outperforms the previous state-of-the-art method LERF by a large
margin. Notably, LangSplat is extremely efficient, achieving a {\speed}
$\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We
strongly recommend readers to check out our video results at
https://langsplat.github.io";Minghan Qin<author:sep>Wanhua Li<author:sep>Jiawei Zhou<author:sep>Haoqian Wang<author:sep>Hanspeter Pfister;http://arxiv.org/pdf/2312.16084v1;cs.CV;Project Page: https://langsplat.github.io;gaussian splatting<tag:sep>nerf
2312.16047v1;http://arxiv.org/abs/2312.16047v1;2023-12-26;2D-Guided 3D Gaussian Segmentation;"Recently, 3D Gaussian, as an explicit 3D representation method, has
demonstrated strong competitiveness over NeRF (Neural Radiance Fields) in terms
of expressing complex scenes and training duration. These advantages signal a
wide range of applications for 3D Gaussians in 3D understanding and editing.
Meanwhile, the segmentation of 3D Gaussians is still in its infancy. The
existing segmentation methods are not only cumbersome but also incapable of
segmenting multiple objects simultaneously in a short amount of time. In
response, this paper introduces a 3D Gaussian segmentation method implemented
with 2D segmentation as supervision. This approach uses input 2D segmentation
maps to guide the learning of the added 3D Gaussian semantic information, while
nearest neighbor clustering and statistical filtering refine the segmentation
results. Experiments show that our concise method can achieve comparable
performances on mIOU and mAcc for multi-object segmentation as previous
single-object segmentation methods.";Kun Lan<author:sep>Haoran Li<author:sep>Haolin Shi<author:sep>Wenjun Wu<author:sep>Yong Liao<author:sep>Lin Wang<author:sep>Pengyuan Zhou;http://arxiv.org/pdf/2312.16047v1;cs.CV;;nerf
2312.15711v1;http://arxiv.org/abs/2312.15711v1;2023-12-25;Neural BSSRDF: Object Appearance Representation Including Heterogeneous  Subsurface Scattering;"Monte Carlo rendering of translucent objects with heterogeneous scattering
properties is often expensive both in terms of memory and computation. If we do
path tracing and use a high dynamic range lighting environment, the rendering
becomes computationally heavy. We propose a compact and efficient neural method
for representing and rendering the appearance of heterogeneous translucent
objects. The neural representation function resembles a bidirectional
scattering-surface reflectance distribution function (BSSRDF). However,
conventional BSSRDF models assume a planar half-space medium and only surface
variation of the material, which is often not a good representation of the
appearance of real-world objects. Our method represents the BSSRDF of a full
object taking its geometry and heterogeneities into account. This is similar to
a neural radiance field, but our representation works for an arbitrary distant
lighting environment. In a sense, we present a version of neural precomputed
radiance transfer that captures all-frequency relighting of heterogeneous
translucent objects. We use a multi-layer perceptron (MLP) with skip
connections to represent the appearance of an object as a function of spatial
position, direction of observation, and direction of incidence. The latter is
considered a directional light incident across the entire non-self-shadowed
part of the object. We demonstrate the ability of our method to store highly
complex materials while having high accuracy when comparing to reference images
of the represented object in unseen lighting environments. As compared with
path tracing of a heterogeneous light scattering volume behind a refractive
interface, our method more easily enables importance sampling of the directions
of incidence and can be integrated into existing rendering frameworks while
achieving interactive frame rates.";Thomson TG<author:sep>Jeppe Revall Frisvad<author:sep>Ravi Ramamoorthi<author:sep>Henrik Wann Jensen;http://arxiv.org/pdf/2312.15711v1;cs.GR;;
2312.16215v1;http://arxiv.org/abs/2312.16215v1;2023-12-24;SUNDIAL: 3D Satellite Understanding through Direct, Ambient, and Complex  Lighting Decomposition;"3D modeling from satellite imagery is essential in areas of environmental
science, urban planning, agriculture, and disaster response. However,
traditional 3D modeling techniques face unique challenges in the remote sensing
context, including limited multi-view baselines over extensive regions, varying
direct, ambient, and complex illumination conditions, and time-varying scene
changes across captures. In this work, we introduce SUNDIAL, a comprehensive
approach to 3D reconstruction of satellite imagery using neural radiance
fields. We jointly learn satellite scene geometry, illumination components, and
sun direction in this single-model approach, and propose a secondary shadow ray
casting technique to 1) improve scene geometry using oblique sun angles to
render shadows, 2) enable physically-based disentanglement of scene albedo and
illumination, and 3) determine the components of illumination from direct,
ambient (sky), and complex sources. To achieve this, we incorporate lighting
cues and geometric priors from remote sensing literature in a neural rendering
approach, modeling physical properties of satellite scenes such as shadows,
scattered sky illumination, and complex illumination and shading of vegetation
and water. We evaluate the performance of SUNDIAL against existing NeRF-based
techniques for satellite scene modeling and demonstrate improved scene and
lighting disentanglement, novel view and lighting rendering, and geometry and
sun direction estimation on challenging scenes with small baselines, sparse
inputs, and variable illumination.";Nikhil Behari<author:sep>Akshat Dave<author:sep>Kushagra Tiwary<author:sep>William Yang<author:sep>Ramesh Raskar;http://arxiv.org/pdf/2312.16215v1;cs.CV;8 pages, 6 figures;nerf
2312.15253v1;http://arxiv.org/abs/2312.15253v1;2023-12-23;Efficient Deformable Tissue Reconstruction via Orthogonal Neural Plane;"Intraoperative imaging techniques for reconstructing deformable tissues in
vivo are pivotal for advanced surgical systems. Existing methods either
compromise on rendering quality or are excessively computationally intensive,
often demanding dozens of hours to perform, which significantly hinders their
practical application. In this paper, we introduce Fast Orthogonal Plane
(Forplane), a novel, efficient framework based on neural radiance fields (NeRF)
for the reconstruction of deformable tissues. We conceptualize surgical
procedures as 4D volumes, and break them down into static and dynamic fields
comprised of orthogonal neural planes. This factorization iscretizes the
four-dimensional space, leading to a decreased memory usage and faster
optimization. A spatiotemporal importance sampling scheme is introduced to
improve performance in regions with tool occlusion as well as large motions and
accelerate training. An efficient ray marching method is applied to skip
sampling among empty regions, significantly improving inference speed. Forplane
accommodates both binocular and monocular endoscopy videos, demonstrating its
extensive applicability and flexibility. Our experiments, carried out on two in
vivo datasets, the EndoNeRF and Hamlyn datasets, demonstrate the effectiveness
of our framework. In all cases, Forplane substantially accelerates both the
optimization process (by over 100 times) and the inference process (by over 15
times) while maintaining or even improving the quality across a variety of
non-rigid deformations. This significant performance improvement promises to be
a valuable asset for future intraoperative surgical applications. The code of
our project is now available at https://github.com/Loping151/ForPlane.";Chen Yang<author:sep>Kailing Wang<author:sep>Yuehao Wang<author:sep>Qi Dou<author:sep>Xiaokang Yang<author:sep>Wei Shen;http://arxiv.org/pdf/2312.15253v1;cs.CV;;nerf
2312.15258v1;http://arxiv.org/abs/2312.15258v1;2023-12-23;Human101: Training 100+FPS Human Gaussians in 100s from 1 View;"Reconstructing the human body from single-view videos plays a pivotal role in
the virtual reality domain. One prevalent application scenario necessitates the
rapid reconstruction of high-fidelity 3D digital humans while simultaneously
ensuring real-time rendering and interaction. Existing methods often struggle
to fulfill both requirements. In this paper, we introduce Human101, a novel
framework adept at producing high-fidelity dynamic 3D human reconstructions
from 1-view videos by training 3D Gaussians in 100 seconds and rendering in
100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which
provides an explicit and efficient representation of 3D humans. Standing apart
from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric
Forward Gaussian Animation method to deform the parameters of 3D Gaussians,
thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an
impressive 60+ FPS and rendering 512-resolution images at 100+ FPS).
Experimental results indicate that our approach substantially eclipses current
methods, clocking up to a 10 times surge in frames per second and delivering
comparable or superior rendering quality. Code and demos will be released at
https://github.com/longxiang-ai/Human101.";Mingwei Li<author:sep>Jiachen Tao<author:sep>Zongxin Yang<author:sep>Yi Yang;http://arxiv.org/pdf/2312.15258v1;cs.CV;Website: https://github.com/longxiang-ai/Human101;gaussian splatting<tag:sep>nerf
2312.16197v1;http://arxiv.org/abs/2312.16197v1;2023-12-23;INFAMOUS-NeRF: ImproviNg FAce MOdeling Using Semantically-Aligned  Hypernetworks with Neural Radiance Fields;"We propose INFAMOUS-NeRF, an implicit morphable face model that introduces
hypernetworks to NeRF to improve the representation power in the presence of
many training subjects. At the same time, INFAMOUS-NeRF resolves the classic
hypernetwork tradeoff of representation power and editability by learning
semantically-aligned latent spaces despite the subject-specific models, all
without requiring a large pretrained model. INFAMOUS-NeRF further introduces a
novel constraint to improve NeRF rendering along the face boundary. Our
constraint can leverage photometric surface rendering and multi-view
supervision to guide surface color prediction and improve rendering near the
surface. Finally, we introduce a novel, loss-guided adaptive sampling method
for more effective NeRF training by reducing the sampling redundancy. We show
quantitatively and qualitatively that our method achieves higher representation
power than prior face modeling methods in both controlled and in-the-wild
settings. Code and models will be released upon publication.";Andrew Hou<author:sep>Feng Liu<author:sep>Zhiyuan Ren<author:sep>Michel Sarkis<author:sep>Ning Bi<author:sep>Yiying Tong<author:sep>Xiaoming Liu;http://arxiv.org/pdf/2312.16197v1;cs.CV;;nerf
2312.15242v1;http://arxiv.org/abs/2312.15242v1;2023-12-23;CaLDiff: Camera Localization in NeRF via Pose Diffusion;"With the widespread use of NeRF-based implicit 3D representation, the need
for camera localization in the same representation becomes manifestly apparent.
Doing so not only simplifies the localization process -- by avoiding an
outside-the-NeRF-based localization -- but also has the potential to offer the
benefit of enhanced localization. This paper studies the problem of localizing
cameras in NeRF using a diffusion model for camera pose adjustment. More
specifically, given a pre-trained NeRF model, we train a diffusion model that
iteratively updates randomly initialized camera poses, conditioned upon the
image to be localized. At test time, a new camera is localized in two steps:
first, coarse localization using the proposed pose diffusion process, followed
by local refinement steps of a pose inversion process in NeRF. In fact, the
proposed camera localization by pose diffusion (CaLDiff) method also integrates
the pose inversion steps within the diffusion process. Such integration offers
significantly better localization, thanks to our downstream refinement-aware
diffusion process. Our exhaustive experiments on challenging real-world data
validate our method by providing significantly better results than the compared
methods and the established baselines. Our source code will be made publicly
available.";Rashik Shrestha<author:sep>Bishad Koju<author:sep>Abhigyan Bhusal<author:sep>Danda Pani Paudel<author:sep>FranÃ§ois Rameau;http://arxiv.org/pdf/2312.15242v1;cs.CV;;nerf
2312.15059v1;http://arxiv.org/abs/2312.15059v1;2023-12-22;Deformable 3D Gaussian Splatting for Animatable Human Avatars;"Recent advances in neural radiance fields enable novel view synthesis of
photo-realistic images in dynamic settings, which can be applied to scenarios
with human animation. Commonly used implicit backbones to establish accurate
models, however, require many input views and additional annotations such as
human masks, UV maps and depth maps. In this work, we propose ParDy-Human
(Parameterized Dynamic Human Avatar), a fully explicit approach to construct a
digital avatar from as little as a single monocular sequence. ParDy-Human
introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D
Gaussians are deformed by a human pose model to animate the avatar. Our method
is composed of two parts: A first module that deforms canonical 3D Gaussians
according to SMPL vertices and a consecutive module that further takes their
designed joint encodings and predicts per Gaussian deformations to deal with
dynamics beyond SMPL vertex deformations. Images are then synthesized by a
rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic
human avatars which requires significantly fewer training views and images. Our
avatars learning is free of additional annotations such as masks and can be
trained with variable backgrounds while inferring full-resolution images
efficiently even on consumer hardware. We provide experimental evidence to show
that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and
THUman4.0 datasets both quantitatively and visually.";HyunJun Jung<author:sep>Nikolas Brasch<author:sep>Jifei Song<author:sep>Eduardo Perez-Pellitero<author:sep>Yiren Zhou<author:sep>Zhihao Li<author:sep>Nassir Navab<author:sep>Benjamin Busam;http://arxiv.org/pdf/2312.15059v1;cs.CV;;gaussian splatting
2312.14915v1;http://arxiv.org/abs/2312.14915v1;2023-12-22;PoseGen: Learning to Generate 3D Human Pose Dataset with NeRF;"This paper proposes an end-to-end framework for generating 3D human pose
datasets using Neural Radiance Fields (NeRF). Public datasets generally have
limited diversity in terms of human poses and camera viewpoints, largely due to
the resource-intensive nature of collecting 3D human pose data. As a result,
pose estimators trained on public datasets significantly underperform when
applied to unseen out-of-distribution samples. Previous works proposed
augmenting public datasets by generating 2D-3D pose pairs or rendering a large
amount of random data. Such approaches either overlook image rendering or
result in suboptimal datasets for pre-trained models. Here we propose PoseGen,
which learns to generate a dataset (human 3D poses and images) with a feedback
loss from a given pre-trained pose estimator. In contrast to prior art, our
generated data is optimized to improve the robustness of the pre-trained model.
The objective of PoseGen is to learn a distribution of data that maximizes the
prediction error of a given pre-trained model. As the learned data distribution
contains OOD samples of the pre-trained model, sampling data from such a
distribution for further fine-tuning a pre-trained model improves the
generalizability of the model. This is the first work that proposes NeRFs for
3D human data generation. NeRFs are data-driven and do not require 3D scans of
humans. Therefore, using NeRF for data generation is a new direction for
convenient user-specific data generation. Our extensive experiments show that
the proposed PoseGen improves two baseline models (SPIN and HybrIK) on four
datasets with an average 6% relative improvement.";Mohsen Gholami<author:sep>Rabab Ward<author:sep>Z. Jane Wang;http://arxiv.org/pdf/2312.14915v1;cs.CV;;nerf
2312.14664v1;http://arxiv.org/abs/2312.14664v1;2023-12-22;Density Uncertainty Quantification with NeRF-Ensembles: Impact of Data  and Scene Constraints;"In the fields of computer graphics, computer vision and photogrammetry,
Neural Radiance Fields (NeRFs) are a major topic driving current research and
development. However, the quality of NeRF-generated 3D scene reconstructions
and subsequent surface reconstructions, heavily relies on the network output,
particularly the density. Regarding this critical aspect, we propose to utilize
NeRF-Ensembles that provide a density uncertainty estimate alongside the mean
density. We demonstrate that data constraints such as low-quality images and
poses lead to a degradation of the training process, increased density
uncertainty and decreased predicted density. Even with high-quality input data,
the density uncertainty varies based on scene constraints such as acquisition
constellations, occlusions and material properties. NeRF-Ensembles not only
provide a tool for quantifying the uncertainty but exhibit two promising
advantages: Enhanced robustness and artifact removal. Through the utilization
of NeRF-Ensembles instead of single NeRFs, small outliers are removed, yielding
a smoother output with improved completeness of structures. Furthermore,
applying percentile-based thresholds on density uncertainty outliers proves to
be effective for the removal of large (foggy) artifacts in post-processing. We
conduct our methodology on 3 different datasets: (i) synthetic benchmark
dataset, (ii) real benchmark dataset, (iii) real data under realistic recording
conditions and sensors.";Miriam JÃ¤ger<author:sep>Steven Landgraf<author:sep>Boris Jutzi;http://arxiv.org/pdf/2312.14664v1;cs.CV;21 pages, 12 figures, 5 tables;nerf
2312.13528v1;http://arxiv.org/abs/2312.13528v1;2023-12-21;DyBluRF: Dynamic Deblurring Neural Radiance Fields for Blurry Monocular  Video;"Video view synthesis, allowing for the creation of visually appealing frames
from arbitrary viewpoints and times, offers immersive viewing experiences.
Neural radiance fields, particularly NeRF, initially developed for static
scenes, have spurred the creation of various methods for video view synthesis.
However, the challenge for video view synthesis arises from motion blur, a
consequence of object or camera movement during exposure, which hinders the
precise synthesis of sharp spatio-temporal views. In response, we propose a
novel dynamic deblurring NeRF framework for blurry monocular video, called
DyBluRF, consisting of an Interleave Ray Refinement (IRR) stage and a Motion
Decomposition-based Deblurring (MDD) stage. Our DyBluRF is the first that
addresses and handles the novel view synthesis for blurry monocular video. The
IRR stage jointly reconstructs dynamic 3D scenes and refines the inaccurate
camera pose information to combat imprecise pose information extracted from the
given blurry frames. The MDD stage is a novel incremental latent sharp-rays
prediction (ILSP) approach for the blurry monocular video frames by decomposing
the latent sharp rays into global camera motion and local object motion
components. Extensive experimental results demonstrate that our DyBluRF
outperforms qualitatively and quantitatively the very recent state-of-the-art
methods. Our project page including source codes and pretrained model are
publicly available at https://kaist-viclab.github.io/dyblurf-site/.";Minh-Quan Viet Bui<author:sep>Jongmin Park<author:sep>Jihyong Oh<author:sep>Munchurl Kim;http://arxiv.org/pdf/2312.13528v1;cs.CV;"The first three authors contributed equally to this work. Please
  visit our project page at https://kaist-viclab.github.io/dyblurf-site/";nerf
2312.13832v1;http://arxiv.org/abs/2312.13832v1;2023-12-21;SyncDreamer for 3D Reconstruction of Endangered Animal Species with NeRF  and NeuS;"The main aim of this study is to demonstrate how innovative view synthesis
and 3D reconstruction techniques can be used to create models of endangered
species using monocular RGB images. To achieve this, we employed SyncDreamer to
produce unique perspectives and NeuS and NeRF to reconstruct 3D
representations. We chose four different animals, including the oriental stork,
frog, dragonfly, and tiger, as our subjects for this study. Our results show
that the combination of SyncDreamer, NeRF, and NeuS techniques can successfully
create 3D models of endangered animals. However, we also observed that NeuS
produced blurry images, while NeRF generated sharper but noisier images. This
study highlights the potential of modeling endangered animals and offers a new
direction for future research in this field. By showcasing the effectiveness of
these advanced techniques, we hope to encourage further exploration and
development of techniques for preserving and studying endangered species.";Ahmet Haydar Ornek<author:sep>Deniz Sen<author:sep>Esmanur Civil;http://arxiv.org/pdf/2312.13832v1;cs.CV;8 figures;nerf
2312.14239v1;http://arxiv.org/abs/2312.14239v1;2023-12-21;PlatoNeRF: 3D Reconstruction in Plato's Cave via Single-View Two-Bounce  Lidar;"3D reconstruction from a single-view is challenging because of the ambiguity
from monocular cues and lack of information about occluded regions. Neural
radiance fields (NeRF), while popular for view synthesis and 3D reconstruction,
are typically reliant on multi-view images. Existing methods for single-view 3D
reconstruction with NeRF rely on either data priors to hallucinate views of
occluded regions, which may not be physically accurate, or shadows observed by
RGB cameras, which are difficult to detect in ambient light and low albedo
backgrounds. We propose using time-of-flight data captured by a single-photon
avalanche diode to overcome these limitations. Our method models two-bounce
optical paths with NeRF, using lidar transient data for supervision. By
leveraging the advantages of both NeRF and two-bounce light measured by lidar,
we demonstrate that we can reconstruct visible and occluded geometry without
data priors or reliance on controlled ambient lighting or scene albedo. In
addition, we demonstrate improved generalization under practical constraints on
sensor spatial- and temporal-resolution. We believe our method is a promising
direction as single-photon lidars become ubiquitous on consumer devices, such
as phones, tablets, and headsets.";Tzofi Klinghoffer<author:sep>Xiaoyu Xiang<author:sep>Siddharth Somasundaram<author:sep>Yuchen Fan<author:sep>Christian Richardt<author:sep>Ramesh Raskar<author:sep>Rakesh Ranjan;http://arxiv.org/pdf/2312.14239v1;cs.CV;Project Page: https://platonerf.github.io/;nerf
2312.13494v1;http://arxiv.org/abs/2312.13494v1;2023-12-21;Visual Tomography: Physically Faithful Volumetric Models of Partially  Translucent Objects;"When created faithfully from real-world data, Digital 3D representations of
objects can be useful for human or computer-assisted analysis. Such models can
also serve for generating training data for machine learning approaches in
settings where data is difficult to obtain or where too few training data
exists, e.g. by providing novel views or images in varying conditions. While
the vast amount of visual 3D reconstruction approaches focus on non-physical
models, textured object surfaces or shapes, in this contribution we propose a
volumetric reconstruction approach that obtains a physical model including the
interior of partially translucent objects such as plankton or insects. Our
technique photographs the object under different poses in front of a bright
white light source and computes absorption and scattering per voxel. It can be
interpreted as visual tomography that we solve by inverse raytracing. We
additionally suggest a method to convert non-physical NeRF media into a
physically-based volumetric grid for initialization and illustrate the
usefulness of the approach using two real-world plankton validation sets, the
lab-scanned models being finally also relighted and virtually submerged in a
scenario with augmented medium and illumination conditions. Please visit the
project homepage at www.marine.informatik.uni-kiel.de/go/vito";David Nakath<author:sep>Xiangyu Weng<author:sep>Mengkun She<author:sep>Kevin KÃ¶ser;http://arxiv.org/pdf/2312.13494v1;cs.CV;Accepted for publication at 3DV '24;nerf
2312.13980v1;http://arxiv.org/abs/2312.13980v1;2023-12-21;Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion  Models with RL Finetuning;"Recent advancements in the text-to-3D task leverage finetuned text-to-image
diffusion models to generate multi-view images, followed by NeRF
reconstruction. Yet, existing supervised finetuned (SFT) diffusion models still
suffer from multi-view inconsistency and the resulting NeRF artifacts. Although
training longer with SFT improves consistency, it also causes distribution
shift, which reduces diversity and realistic details. We argue that the SFT of
multi-view diffusion models resembles the instruction finetuning stage of the
LLM alignment pipeline and can benefit from RL finetuning (RLFT) methods.
Essentially, RLFT methods optimize models beyond their SFT data distribution by
using their own outputs, effectively mitigating distribution shift. To this
end, we introduce Carve3D, a RLFT method coupled with the Multi-view
Reconstruction Consistency (MRC) metric, to improve the consistency of
multi-view diffusion models. To compute MRC on a set of multi-view images, we
compare them with their corresponding renderings of the reconstructed NeRF at
the same viewpoints. We validate the robustness of MRC with extensive
experiments conducted under controlled inconsistency levels. We enhance the
base RLFT algorithm to stabilize the training process, reduce distribution
shift, and identify scaling laws. Through qualitative and quantitative
experiments, along with a user study, we demonstrate Carve3D's improved
multi-view consistency, the resulting superior NeRF reconstruction quality, and
minimal distribution shift compared to longer SFT. Project webpage:
https://desaixie.github.io/carve-3d.";Desai Xie<author:sep>Jiahao Li<author:sep>Hao Tan<author:sep>Xin Sun<author:sep>Zhixin Shu<author:sep>Yi Zhou<author:sep>Sai Bi<author:sep>SÃ¶ren Pirk<author:sep>Arie E. Kaufman;http://arxiv.org/pdf/2312.13980v1;cs.CV;Project webpage: https://desaixie.github.io/carve-3d;nerf
2312.14124v1;http://arxiv.org/abs/2312.14124v1;2023-12-21;Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance  Generation;"Controllable generation of 3D assets is important for many practical
applications like content creation in movies, games and engineering, as well as
in AR/VR. Recently, diffusion models have shown remarkable results in
generation quality of 3D objects. However, none of the existing models enable
disentangled generation to control the shape and appearance separately. For the
first time, we present a suitable representation for 3D diffusion models to
enable such disentanglement by introducing a hybrid point cloud and neural
radiance field approach. We model a diffusion process over point positions
jointly with a high-dimensional feature space for a local density and radiance
decoder. While the point positions represent the coarse shape of the object,
the point features allow modeling the geometry and appearance details. This
disentanglement enables us to sample both independently and therefore to
control both separately. Our approach sets a new state of the art in generation
compared to previous disentanglement-capable methods by reduced FID scores of
30-90% and is on-par with other non disentanglement-capable state-of-the art
methods.";Philipp SchrÃ¶ppel<author:sep>Christopher Wewer<author:sep>Jan Eric Lenssen<author:sep>Eddy Ilg<author:sep>Thomas Brox;http://arxiv.org/pdf/2312.14124v1;cs.CV;;
2312.13763v2;http://arxiv.org/abs/2312.13763v2;2023-12-21;Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed  Diffusion Models;"Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.";Huan Ling<author:sep>Seung Wook Kim<author:sep>Antonio Torralba<author:sep>Sanja Fidler<author:sep>Karsten Kreis;http://arxiv.org/pdf/2312.13763v2;cs.CV;"Project page:
  https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/";gaussian splatting
2312.13729v2;http://arxiv.org/abs/2312.13729v2;2023-12-21;Gaussian Splatting with NeRF-based Color and Opacity;"Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of
neural networks to capture the intricacies of 3D objects. By encoding the shape
and color information within neural network weights, NeRFs excel at producing
strikingly sharp novel views of 3D objects. Recently, numerous generalizations
of NeRFs utilizing generative models have emerged, expanding its versatility.
In contrast, Gaussian Splatting (GS) offers a similar renders quality with
faster training and inference as it does not need neural networks to work. We
encode information about the 3D objects in the set of Gaussian distributions
that can be rendered in 3D similarly to classical meshes. Unfortunately, GS are
difficult to condition since they usually require circa hundred thousand
Gaussian components. To mitigate the caveats of both models, we propose a
hybrid model that uses GS representation of the 3D object's shape and
NeRF-based encoding of color and opacity. Our model uses Gaussian distributions
with trainable positions (i.e. means of Gaussian), shape (i.e. covariance of
Gaussian), color and opacity, and neural network, which takes parameters of
Gaussian and viewing direction to produce changes in color and opacity.
Consequently, our model better describes shadows, light reflections, and
transparency of 3D objects.";Dawid Malarz<author:sep>Weronika Smolak<author:sep>Jacek Tabor<author:sep>SÅawomir Tadeja<author:sep>PrzemysÅaw Spurek;http://arxiv.org/pdf/2312.13729v2;cs.CV;;gaussian splatting<tag:sep>nerf
2312.14154v1;http://arxiv.org/abs/2312.14154v1;2023-12-21;Virtual Pets: Animatable Animal Generation in 3D Scenes;"Toward unlocking the potential of generative models in immersive 4D
experiences, we introduce Virtual Pet, a novel pipeline to model realistic and
diverse motions for target animal species within a 3D environment. To
circumvent the limited availability of 3D motion data aligned with
environmental geometry, we leverage monocular internet videos and extract
deformable NeRF representations for the foreground and static NeRF
representations for the background. For this, we develop a reconstruction
strategy, encompassing species-level shared template learning and per-video
fine-tuning. Utilizing the reconstructed data, we then train a conditional 3D
motion model to learn the trajectory and articulation of foreground animals in
the context of 3D backgrounds. We showcase the efficacy of our pipeline with
comprehensive qualitative and quantitative evaluations using cat videos. We
also demonstrate versatility across unseen cats and indoor environments,
producing temporally coherent 4D outputs for enriched virtual experiences.";Yen-Chi Cheng<author:sep>Chieh Hubert Lin<author:sep>Chaoyang Wang<author:sep>Yash Kant<author:sep>Sergey Tulyakov<author:sep>Alexander Schwing<author:sep>Liangyan Gui<author:sep>Hsin-Ying Lee;http://arxiv.org/pdf/2312.14154v1;cs.CV;Preprint. Project page: https://yccyenchicheng.github.io/VirtualPets/;nerf
2312.13308v1;http://arxiv.org/abs/2312.13308v1;2023-12-20;SWAGS: Sampling Windows Adaptively for Dynamic 3D Gaussian Splatting;"Novel view synthesis has shown rapid progress recently, with methods capable
of producing evermore photo-realistic results. 3D Gaussian Splatting has
emerged as a particularly promising method, producing high-quality renderings
of static scenes and enabling interactive viewing at real-time frame rates.
However, it is currently limited to static scenes only. In this work, we extend
3D Gaussian Splatting to reconstruct dynamic scenes. We model the dynamics of a
scene using a tunable MLP, which learns the deformation field from a canonical
space to a set of 3D Gaussians per frame. To disentangle the static and dynamic
parts of the scene, we learn a tuneable parameter for each Gaussian, which
weighs the respective MLP parameters to focus attention on the dynamic parts.
This improves the model's ability to capture dynamics in scenes with an
imbalance of static to dynamic regions. To handle scenes of arbitrary length
whilst maintaining high rendering quality, we introduce an adaptive window
sampling strategy to partition the sequence into windows based on the amount of
movement in the sequence. We train a separate dynamic Gaussian Splatting model
for each window, allowing the canonical representation to change, thus enabling
the reconstruction of scenes with significant geometric or topological changes.
Temporal consistency is enforced using a fine-tuning step with self-supervising
consistency loss on randomly sampled novel views. As a result, our method
produces high-quality renderings of general dynamic scenes with competitive
quantitative performance, which can be viewed in real-time with our dynamic
interactive viewer.";Richard Shaw<author:sep>Jifei Song<author:sep>Arthur Moreau<author:sep>Michal Nazarczuk<author:sep>Sibi Catley-Chandar<author:sep>Helisa Dhamo<author:sep>Eduardo Perez-Pellitero;http://arxiv.org/pdf/2312.13308v1;cs.CV;;gaussian splatting
2312.13324v1;http://arxiv.org/abs/2312.13324v1;2023-12-20;ShowRoom3D: Text to High-Quality 3D Room Generation Using 3D Priors;"We introduce ShowRoom3D, a three-stage approach for generating high-quality
3D room-scale scenes from texts. Previous methods using 2D diffusion priors to
optimize neural radiance fields for generating room-scale scenes have shown
unsatisfactory quality. This is primarily attributed to the limitations of 2D
priors lacking 3D awareness and constraints in the training methodology. In
this paper, we utilize a 3D diffusion prior, MVDiffusion, to optimize the 3D
room-scale scene. Our contributions are in two aspects. Firstly, we propose a
progressive view selection process to optimize NeRF. This involves dividing the
training process into three stages, gradually expanding the camera sampling
scope. Secondly, we propose the pose transformation method in the second stage.
It will ensure MVDiffusion provide the accurate view guidance. As a result,
ShowRoom3D enables the generation of rooms with improved structural integrity,
enhanced clarity from any view, reduced content repetition, and higher
consistency across different perspectives. Extensive experiments demonstrate
that our method, significantly outperforms state-of-the-art approaches by a
large margin in terms of user study.";Weijia Mao<author:sep>Yan-Pei Cao<author:sep>Jia-Wei Liu<author:sep>Zhongcong Xu<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2312.13324v1;cs.CV;;nerf
2312.12726v1;http://arxiv.org/abs/2312.12726v1;2023-12-20;Reducing Shape-Radiance Ambiguity in Radiance Fields with a Closed-Form  Color Estimation Method;"Neural radiance field (NeRF) enables the synthesis of cutting-edge realistic
novel view images of a 3D scene. It includes density and color fields to model
the shape and radiance of a scene, respectively. Supervised by the photometric
loss in an end-to-end training manner, NeRF inherently suffers from the
shape-radiance ambiguity problem, i.e., it can perfectly fit training views but
does not guarantee decoupling the two fields correctly. To deal with this
issue, existing works have incorporated prior knowledge to provide an
independent supervision signal for the density field, including total variation
loss, sparsity loss, distortion loss, etc. These losses are based on general
assumptions about the density field, e.g., it should be smooth, sparse, or
compact, which are not adaptive to a specific scene. In this paper, we propose
a more adaptive method to reduce the shape-radiance ambiguity. The key is a
rendering method that is only based on the density field. Specifically, we
first estimate the color field based on the density field and posed images in a
closed form. Then NeRF's rendering process can proceed. We address the problems
in estimating the color field, including occlusion and non-uniformly
distributed views. Afterward, it is applied to regularize NeRF's density field.
As our regularization is guided by photometric loss, it is more adaptive
compared to existing ones. Experimental results show that our method improves
the density field of NeRF both qualitatively and quantitatively. Our code is
available at https://github.com/qihangGH/Closed-form-color-field.";Qihang Fang<author:sep>Yafei Song<author:sep>Keqiang Li<author:sep>Liefeng Bo;http://arxiv.org/pdf/2312.12726v1;cs.CV;This work has been published in NeurIPS 2023;nerf
2312.13332v2;http://arxiv.org/abs/2312.13332v2;2023-12-20;Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM;"The opacity of rigid 3D scenes with opaque surfaces is considered to be of a
binary type. However, we observed that this property is not followed by the
existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this
prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization
through the volumetric rendering function does not facilitate easy integration
of the desired prior. Instead, we observed that the opacity of ternary-type
(TT) is well supported. In this work, we study why ternary-type opacity is
well-suited and desired for the task at hand. In particular, we provide
theoretical insights into the process of jointly optimizing radiance and
opacity through the volumetric rendering process. Through exhaustive
experiments on benchmark datasets, we validate our claim and provide insights
into the optimization process, which we believe will unleash the potential of
RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple
yet novel visual odometry scheme that uses a hybrid combination of volumetric
and warping-based image renderings. More specifically, the proposed hybrid
odometry (HO) additionally uses image warping-based coarse odometry, leading up
to an order of magnitude final speed-up. Furthermore, we show that the proposed
TT and HO well complement each other, offering state-of-the-art results on
benchmark datasets in terms of both speed and accuracy.";Junru Lin<author:sep>Asen Nachkov<author:sep>Songyou Peng<author:sep>Luc Van Gool<author:sep>Danda Pani Paudel;http://arxiv.org/pdf/2312.13332v2;cs.CV;;nerf
2312.13102v1;http://arxiv.org/abs/2312.13102v1;2023-12-20;SpecNeRF: Gaussian Directional Encoding for Specular Reflections;"Neural radiance fields have achieved remarkable performance in modeling the
appearance of 3D scenes. However, existing approaches still struggle with the
view-dependent appearance of glossy surfaces, especially under complex lighting
of indoor environments. Unlike existing methods, which typically assume distant
lighting like an environment map, we propose a learnable Gaussian directional
encoding to better model the view-dependent effects under near-field lighting
conditions. Importantly, our new directional encoding captures the
spatially-varying nature of near-field lighting and emulates the behavior of
prefiltered environment maps. As a result, it enables the efficient evaluation
of preconvolved specular color at any 3D location with varying roughness
coefficients. We further introduce a data-driven geometry prior that helps
alleviate the shape radiance ambiguity in reflection modeling. We show that our
Gaussian directional encoding and geometry prior significantly improve the
modeling of challenging specular reflections in neural radiance fields, which
helps decompose appearance into more physically meaningful components.";Li Ma<author:sep>Vasu Agrawal<author:sep>Haithem Turki<author:sep>Changil Kim<author:sep>Chen Gao<author:sep>Pedro Sander<author:sep>Michael ZollhÃ¶fer<author:sep>Christian Richardt;http://arxiv.org/pdf/2312.13102v1;cs.CV;Project page: https://limacv.github.io/SpecNeRF_web/;nerf
2312.13471v1;http://arxiv.org/abs/2312.13471v1;2023-12-20;NeRF-VO: Real-Time Sparse Visual Odometry with Neural Radiance Fields;"We introduce a novel monocular visual odometry (VO) system, NeRF-VO, that
integrates learning-based sparse visual odometry for low-latency camera
tracking and a neural radiance scene representation for sophisticated dense
reconstruction and novel view synthesis. Our system initializes camera poses
using sparse visual odometry and obtains view-dependent dense geometry priors
from a monocular depth prediction network. We harmonize the scale of poses and
dense geometry, treating them as supervisory cues to train a neural implicit
scene representation. NeRF-VO demonstrates exceptional performance in both
photometric and geometric fidelity of the scene representation by jointly
optimizing a sliding window of keyframed poses and the underlying dense
geometry, which is accomplished through training the radiance field with volume
rendering. We surpass state-of-the-art methods in pose estimation accuracy,
novel view synthesis fidelity, and dense reconstruction quality across a
variety of synthetic and real-world datasets, while achieving a higher camera
tracking frequency and consuming less GPU memory.";Jens Naumann<author:sep>Binbin Xu<author:sep>Stefan Leutenegger<author:sep>Xingxing Zuo;http://arxiv.org/pdf/2312.13471v1;cs.CV;10 tables, 4 figures;nerf
2312.13285v1;http://arxiv.org/abs/2312.13285v1;2023-12-20;UniSDF: Unifying Neural Representations for High-Fidelity 3D  Reconstruction of Complex Scenes with Reflections;"Neural 3D scene representations have shown great potential for 3D
reconstruction from 2D images. However, reconstructing real-world captures of
complex scenes still remains a challenge. Existing generic 3D reconstruction
methods often struggle to represent fine geometric details and do not
adequately model reflective surfaces of large-scale scenes. Techniques that
explicitly focus on reflective surfaces can model complex and detailed
reflections by exploiting better reflection parameterizations. However, we
observe that these methods are often not robust in real unbounded scenarios
where non-reflective as well as reflective components are present. In this
work, we propose UniSDF, a general purpose 3D reconstruction method that can
reconstruct large complex scenes with reflections. We investigate both
view-based as well as reflection-based color prediction parameterization
techniques and find that explicitly blending these representations in 3D space
enables reconstruction of surfaces that are more geometrically accurate,
especially for reflective surfaces. We further combine this representation with
a multi-resolution grid backbone that is trained in a coarse-to-fine manner,
enabling faster reconstructions than prior methods. Extensive experiments on
object-level datasets DTU, Shiny Blender as well as unbounded datasets Mip-NeRF
360 and Ref-NeRF real demonstrate that our method is able to robustly
reconstruct complex large-scale scenes with fine details and reflective
surfaces. Please see our project page at
https://fangjinhuawang.github.io/UniSDF.";Fangjinhua Wang<author:sep>Marie-Julie Rakotosaona<author:sep>Michael Niemeyer<author:sep>Richard Szeliski<author:sep>Marc Pollefeys<author:sep>Federico Tombari;http://arxiv.org/pdf/2312.13285v1;cs.CV;Project page: https://fangjinhuawang.github.io/UniSDF;nerf
2312.13277v1;http://arxiv.org/abs/2312.13277v1;2023-12-20;Deep Learning on 3D Neural Fields;"In recent years, Neural Fields (NFs) have emerged as an effective tool for
encoding diverse continuous signals such as images, videos, audio, and 3D
shapes. When applied to 3D data, NFs offer a solution to the fragmentation and
limitations associated with prevalent discrete representations. However, given
that NFs are essentially neural networks, it remains unclear whether and how
they can be seamlessly integrated into deep learning pipelines for solving
downstream tasks. This paper addresses this research problem and introduces
nf2vec, a framework capable of generating a compact latent representation for
an input NF in a single inference pass. We demonstrate that nf2vec effectively
embeds 3D objects represented by the input NFs and showcase how the resulting
embeddings can be employed in deep learning pipelines to successfully address
various tasks, all while processing exclusively NFs. We test this framework on
several NFs used to represent 3D surfaces, such as unsigned/signed distance and
occupancy fields. Moreover, we demonstrate the effectiveness of our approach
with more complex NFs that encompass both geometry and appearance of 3D objects
such as neural radiance fields.";Pierluigi Zama Ramirez<author:sep>Luca De Luigi<author:sep>Daniele Sirocchi<author:sep>Adriano Cardace<author:sep>Riccardo Spezialetti<author:sep>Francesco Ballerini<author:sep>Samuele Salti<author:sep>Luigi Di Stefano;http://arxiv.org/pdf/2312.13277v1;cs.CV;"Extended version of the paper ""Deep Learning on Implicit Neural
  Representations of Shapes"" that was presented at ICLR 2023. arXiv admin note:
  text overlap with arXiv:2302.05438";
2312.13150v1;http://arxiv.org/abs/2312.13150v1;2023-12-20;Splatter Image: Ultra-Fast Single-View 3D Reconstruction;"We introduce the Splatter Image, an ultra-fast approach for monocular 3D
object reconstruction which operates at 38 FPS. Splatter Image is based on
Gaussian Splatting, which has recently brought real-time rendering, fast
training, and excellent scaling to multi-view reconstruction. For the first
time, we apply Gaussian Splatting in a monocular reconstruction setting. Our
approach is learning-based, and, at test time, reconstruction only requires the
feed-forward evaluation of a neural network. The main innovation of Splatter
Image is the surprisingly straightforward design: it uses a 2D image-to-image
network to map the input image to one 3D Gaussian per pixel. The resulting
Gaussians thus have the form of an image, the Splatter Image. We further extend
the method to incorporate more than one image as input, which we do by adding
cross-view attention. Owning to the speed of the renderer (588 FPS), we can use
a single GPU for training while generating entire images at each iteration in
order to optimize perceptual metrics like LPIPS. On standard benchmarks, we
demonstrate not only fast reconstruction but also better results than recent
and much more expensive baselines in terms of PSNR, LPIPS, and other metrics.";Stanislaw Szymanowicz<author:sep>Christian Rupprecht<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2312.13150v1;cs.CV;"Project page: https://szymanowiczs.github.io/splatter-image.html .
  Code: https://github.com/szymanowiczs/splatter-image";gaussian splatting
2312.12122v1;http://arxiv.org/abs/2312.12122v1;2023-12-19;ZS-SRT: An Efficient Zero-Shot Super-Resolution Training Method for  Neural Radiance Fields;"Neural Radiance Fields (NeRF) have achieved great success in the task of
synthesizing novel views that preserve the same resolution as the training
views. However, it is challenging for NeRF to synthesize high-quality
high-resolution novel views with low-resolution training data. To solve this
problem, we propose a zero-shot super-resolution training framework for NeRF.
This framework aims to guide the NeRF model to synthesize high-resolution novel
views via single-scene internal learning rather than requiring any external
high-resolution training data. Our approach consists of two stages. First, we
learn a scene-specific degradation mapping by performing internal learning on a
pretrained low-resolution coarse NeRF. Second, we optimize a super-resolution
fine NeRF by conducting inverse rendering with our mapping function so as to
backpropagate the gradients from low-resolution 2D space into the
super-resolution 3D sampling space. Then, we further introduce a temporal
ensemble strategy in the inference phase to compensate for the scene estimation
errors. Our method is featured on two points: (1) it does not consume
high-resolution views or additional scene data to train super-resolution NeRF;
(2) it can speed up the training process by adopting a coarse-to-fine strategy.
By conducting extensive experiments on public datasets, we have qualitatively
and quantitatively demonstrated the effectiveness of our method.";Xiang Feng<author:sep>Yongbo He<author:sep>Yubo Wang<author:sep>Chengkai Wang<author:sep>Zhenzhong Kuang<author:sep>Jiajun Ding<author:sep>Feiwei Qin<author:sep>Jun Yu<author:sep>Jianping Fan;http://arxiv.org/pdf/2312.12122v1;cs.CV;;nerf
2312.13299v1;http://arxiv.org/abs/2312.13299v1;2023-12-19;Compact 3D Scene Representation via Self-Organizing Gaussian Grids;"3D Gaussian Splatting has recently emerged as a highly promising technique
for modeling of static 3D scenes. In contrast to Neural Radiance Fields, it
utilizes efficient rasterization allowing for very fast rendering at
high-quality. However, the storage size is significantly higher, which hinders
practical deployment, e.g.~on resource constrained devices. In this paper, we
introduce a compact scene representation organizing the parameters of 3D
Gaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a
drastic reduction in storage requirements without compromising visual quality
during rendering. Central to our idea is the explicit exploitation of
perceptual redundancies present in natural scenes. In essence, the inherent
nature of a scene allows for numerous permutations of Gaussian parameters to
equivalently represent it. To this end, we propose a novel highly parallel
algorithm that regularly arranges the high-dimensional Gaussian parameters into
a 2D grid while preserving their neighborhood structure. During training, we
further enforce local smoothness between the sorted parameters in the grid. The
uncompressed Gaussians use the same structure as 3DGS, ensuring a seamless
integration with established renderers. Our method achieves a reduction factor
of 8x to 26x in size for complex scenes with no increase in training time,
marking a substantial leap forward in the domain of 3D scene distribution and
consumption. Additional information can be found on our project page:
https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/";Wieland Morgenstern<author:sep>Florian Barthel<author:sep>Anna Hilsmann<author:sep>Peter Eisert;http://arxiv.org/pdf/2312.13299v1;cs.CV;;gaussian splatting
2312.12036v2;http://arxiv.org/abs/2312.12036v2;2023-12-19;LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks  in Cluttered Tabletop Environments;"Instructing a robot to complete an everyday task within our homes has been a
long-standing challenge for robotics. While recent progress in
language-conditioned imitation learning and offline reinforcement learning has
demonstrated impressive performance across a wide range of tasks, they are
typically limited to short-horizon tasks -- not reflective of those a home
robot would be expected to complete. While existing architectures have the
potential to learn these desired behaviours, the lack of the necessary
long-horizon, multi-step datasets for real robotic systems poses a significant
challenge. To this end, we present the Long-Horizon Manipulation (LHManip)
dataset comprising 200 episodes, demonstrating 20 different manipulation tasks
via real robot teleoperation. The tasks entail multiple sub-tasks, including
grasping, pushing, stacking and throwing objects in highly cluttered
environments. Each task is paired with a natural language instruction and
multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the
dataset comprises 176,278 observation-action pairs which form part of the Open
X-Embodiment dataset. The full LHManip dataset is made publicly available at
https://github.com/fedeceola/LHManip.";Federico Ceola<author:sep>Lorenzo Natale<author:sep>Niko SÃ¼nderhauf<author:sep>Krishan Rana;http://arxiv.org/pdf/2312.12036v2;cs.RO;Submitted to IJRR;nerf
2312.11841v3;http://arxiv.org/abs/2312.11841v3;2023-12-19;MixRT: Mixed Neural Representations For Real-Time NeRF Rendering;"Neural Radiance Field (NeRF) has emerged as a leading technique for novel
view synthesis, owing to its impressive photorealistic reconstruction and
rendering capability. Nevertheless, achieving real-time NeRF rendering in
large-scale scenes has presented challenges, often leading to the adoption of
either intricate baked mesh representations with a substantial number of
triangles or resource-intensive ray marching in baked representations. We
challenge these conventions, observing that high-quality geometry, represented
by meshes with substantial triangles, is not necessary for achieving
photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF
representation that includes a low-quality mesh, a view-dependent displacement
map, and a compressed NeRF model. This design effectively harnesses the
capabilities of existing graphics hardware, thus enabling real-time NeRF
rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering
framework, our proposed MixRT attains real-time rendering speeds on edge
devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),
better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360
datasets), and a smaller storage size (less than 80% compared to
state-of-the-art methods).";Chaojian Li<author:sep>Bichen Wu<author:sep>Peter Vajda<author:sep> Yingyan<author:sep> Lin;http://arxiv.org/pdf/2312.11841v3;cs.CV;Accepted by 3DV'24. Project Page: https://licj15.github.io/MixRT/;nerf
2312.11774v1;http://arxiv.org/abs/2312.11774v1;2023-12-19;Text-Image Conditioned Diffusion for Consistent Text-to-3D Generation;"By lifting the pre-trained 2D diffusion models into Neural Radiance Fields
(NeRFs), text-to-3D generation methods have made great progress. Many
state-of-the-art approaches usually apply score distillation sampling (SDS) to
optimize the NeRF representations, which supervises the NeRF optimization with
pre-trained text-conditioned 2D diffusion models such as Imagen. However, the
supervision signal provided by such pre-trained diffusion models only depends
on text prompts and does not constrain the multi-view consistency. To inject
the cross-view consistency into diffusion priors, some recent works finetune
the 2D diffusion model with multi-view data, but still lack fine-grained view
coherence. To tackle this challenge, we incorporate multi-view image conditions
into the supervision signal of NeRF optimization, which explicitly enforces
fine-grained view consistency. With such stronger supervision, our proposed
text-to-3D method effectively mitigates the generation of floaters (due to
excessive densities) and completely empty spaces (due to insufficient
densities). Our quantitative evaluations on the T$^3$Bench dataset demonstrate
that our method achieves state-of-the-art performance over existing text-to-3D
methods. We will make the code publicly available.";Yuze He<author:sep>Yushi Bai<author:sep>Matthieu Lin<author:sep>Jenny Sheng<author:sep>Yubin Hu<author:sep>Qi Wang<author:sep>Yu-Hui Wen<author:sep>Yong-Jin Liu;http://arxiv.org/pdf/2312.11774v1;cs.CV;;nerf
2312.12337v2;http://arxiv.org/abs/2312.12337v2;2023-12-19;pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable  Generalizable 3D Reconstruction;"We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D
radiance fields parameterized by 3D Gaussian primitives from pairs of images.
Our model features real-time and memory-efficient rendering for scalable
training as well as fast 3D reconstruction at inference time. To overcome local
minima inherent to sparse and locally supported representations, we predict a
dense probability distribution over 3D and sample Gaussian means from that
probability distribution. We make this sampling operation differentiable via a
reparameterization trick, allowing us to back-propagate gradients through the
Gaussian splatting representation. We benchmark our method on wide-baseline
novel view synthesis on the real-world RealEstate10k and ACID datasets, where
we outperform state-of-the-art light field transformers and accelerate
rendering by 2.5 orders of magnitude while reconstructing an interpretable and
editable 3D radiance field.";David Charatan<author:sep>Sizhe Li<author:sep>Andrea Tagliasacchi<author:sep>Vincent Sitzmann;http://arxiv.org/pdf/2312.12337v2;cs.CV;Project page: https://dcharatan.github.io/pixelsplat;gaussian splatting
2312.10921v1;http://arxiv.org/abs/2312.10921v1;2023-12-18;AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head  Synthesis;"Audio-driven talking head synthesis is a promising topic with wide
applications in digital human, film making and virtual reality. Recent
NeRF-based approaches have shown superiority in quality and fidelity compared
to previous studies. However, when it comes to few-shot talking head
generation, a practical scenario where only few seconds of talking video is
available for one identity, two limitations emerge: 1) they either have no base
model, which serves as a facial prior for fast convergence, or ignore the
importance of audio when building the prior; 2) most of them overlook the
degree of correlation between different face regions and audio, e.g., mouth is
audio related, while ear is audio independent. In this paper, we present Audio
Enhanced Neural Radiance Field (AE-NeRF) to tackle the above issues, which can
generate realistic portraits of a new speaker with fewshot dataset.
Specifically, we introduce an Audio Aware Aggregation module into the feature
fusion stage of the reference scheme, where the weight is determined by the
similarity of audio between reference and target image. Then, an Audio-Aligned
Face Generation strategy is proposed to model the audio related and audio
independent regions respectively, with a dual-NeRF framework. Extensive
experiments have shown AE-NeRF surpasses the state-of-the-art on image
fidelity, audio-lip synchronization, and generalization ability, even in
limited training set or training iterations.";Dongze Li<author:sep>Kang Zhao<author:sep>Wei Wang<author:sep>Bo Peng<author:sep>Yingya Zhang<author:sep>Jing Dong<author:sep>Tieniu Tan;http://arxiv.org/pdf/2312.10921v1;cs.CV;Accepted by AAAI 2024;nerf
2312.11458v1;http://arxiv.org/abs/2312.11458v1;2023-12-18;GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View  Synthesis;"We propose a method for dynamic scene reconstruction using deformable 3D
Gaussians that is tailored for monocular video. Building upon the efficiency of
Gaussian splatting, our approach extends the representation to accommodate
dynamic elements via a deformable set of Gaussians residing in a canonical
space, and a time-dependent deformation field defined by a multi-layer
perceptron (MLP). Moreover, under the assumption that most natural scenes have
large regions that remain static, we allow the MLP to focus its
representational power by additionally including a static Gaussian point cloud.
The concatenated dynamic and static point clouds form the input for the
Gaussian Splatting rasterizer, enabling real-time rendering. The differentiable
pipeline is optimized end-to-end with a self-supervised rendering loss. Our
method achieves results that are comparable to state-of-the-art dynamic neural
radiance field methods while allowing much faster optimization and rendering.
Project website: https://lynl7130.github.io/gaufre/index.html";Yiqing Liang<author:sep>Numair Khan<author:sep>Zhengqin Li<author:sep>Thu Nguyen-Phuoc<author:sep>Douglas Lanman<author:sep>James Tompkin<author:sep>Lei Xiao;http://arxiv.org/pdf/2312.11458v1;cs.CV;10 pages, 8 figures, 4 tables;gaussian splatting
2312.11461v1;http://arxiv.org/abs/2312.11461v1;2023-12-18;GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning;"Gaussian splatting has emerged as a powerful 3D representation that harnesses
the advantages of both explicit (mesh) and implicit (NeRF) 3D representations.
In this paper, we seek to leverage Gaussian splatting to generate realistic
animatable avatars from textual descriptions, addressing the limitations (e.g.,
flexibility and efficiency) imposed by mesh or NeRF-based representations.
However, a naive application of Gaussian splatting cannot generate high-quality
animatable avatars and suffers from learning instability; it also cannot
capture fine avatar geometries and often leads to degenerate body parts. To
tackle these problems, we first propose a primitive-based 3D Gaussian
representation where Gaussians are defined inside pose-driven primitives to
facilitate animation. Second, to stabilize and amortize the learning of
millions of Gaussians, we propose to use neural implicit fields to predict the
Gaussian attributes (e.g., colors). Finally, to capture fine avatar geometries
and extract detailed meshes, we propose a novel SDF-based implicit mesh
learning approach for 3D Gaussians that regularizes the underlying geometries
and extracts highly detailed textured meshes. Our proposed method, GAvatar,
enables the large-scale generation of diverse animatable avatars using only
text prompts. GAvatar significantly surpasses existing methods in terms of both
appearance and geometry quality, and achieves extremely fast rendering (100
fps) at 1K resolution.";Ye Yuan<author:sep>Xueting Li<author:sep>Yangyi Huang<author:sep>Shalini De Mello<author:sep>Koki Nagano<author:sep>Jan Kautz<author:sep>Umar Iqbal;http://arxiv.org/pdf/2312.11461v1;cs.CV;Project website: https://nvlabs.github.io/GAvatar;gaussian splatting<tag:sep>nerf
2312.10649v1;http://arxiv.org/abs/2312.10649v1;2023-12-17;PNeRFLoc: Visual Localization with Point-based Neural Radiance Fields;"Due to the ability to synthesize high-quality novel views, Neural Radiance
Fields (NeRF) have been recently exploited to improve visual localization in a
known environment. However, the existing methods mostly utilize NeRFs for data
augmentation to improve the regression model training, and the performance on
novel viewpoints and appearances is still limited due to the lack of geometric
constraints. In this paper, we propose a novel visual localization framework,
\ie, PNeRFLoc, based on a unified point-based representation. On the one hand,
PNeRFLoc supports the initial pose estimation by matching 2D and 3D feature
points as traditional structure-based methods; on the other hand, it also
enables pose refinement with novel view synthesis using rendering-based
optimization. Specifically, we propose a novel feature adaption module to close
the gaps between the features for visual localization and neural rendering. To
improve the efficacy and efficiency of neural rendering-based optimization, we
also develop an efficient rendering-based framework with a warping loss
function. Furthermore, several robustness techniques are developed to handle
illumination changes and dynamic objects for outdoor scenarios. Experiments
demonstrate that PNeRFLoc performs the best on synthetic data when the NeRF
model can be well learned and performs on par with the SOTA method on the
visual localization benchmark datasets.";Boming Zhao<author:sep>Luwei Yang<author:sep>Mao Mao<author:sep>Hujun Bao<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2312.10649v1;cs.CV;Accepted to AAAI 2024;nerf
2312.10422v2;http://arxiv.org/abs/2312.10422v2;2023-12-16;Learning Dense Correspondence for NeRF-Based Face Reenactment;"Face reenactment is challenging due to the need to establish dense
correspondence between various face representations for motion transfer. Recent
studies have utilized Neural Radiance Field (NeRF) as fundamental
representation, which further enhanced the performance of multi-view face
reenactment in photo-realism and 3D consistency. However, establishing dense
correspondence between different face NeRFs is non-trivial, because implicit
representations lack ground-truth correspondence annotations like mesh-based 3D
parametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning
3DMM space with NeRF-based face representations can realize motion control, it
is sub-optimal for their limited face-only modeling and low identity fidelity.
Therefore, we are inspired to ask: Can we learn the dense correspondence
between different NeRF-based face representations without a 3D parametric model
prior? To address this challenge, we propose a novel framework, which adopts
tri-planes as fundamental NeRF representation and decomposes face tri-planes
into three components: canonical tri-planes, identity deformations, and motion.
In terms of motion control, our key contribution is proposing a Plane
Dictionary (PlaneDict) module, which efficiently maps the motion conditions to
a linear weighted addition of learnable orthogonal plane bases. To the best of
our knowledge, our framework is the first method that achieves one-shot
multi-view face reenactment without a 3D parametric model prior. Extensive
experiments demonstrate that we produce better results in fine-grained motion
control and identity preservation than previous methods.";Songlin Yang<author:sep>Wei Wang<author:sep>Yushi Lan<author:sep>Xiangyu Fan<author:sep>Bo Peng<author:sep>Lei Yang<author:sep>Jing Dong;http://arxiv.org/pdf/2312.10422v2;cs.CV;"Accepted by Proceedings of the AAAI Conference on Artificial
  Intelligence, 2024";nerf
2312.10034v1;http://arxiv.org/abs/2312.10034v1;2023-12-15;SlimmeRF: Slimmable Radiance Fields;"Neural Radiance Field (NeRF) and its variants have recently emerged as
successful methods for novel view synthesis and 3D scene reconstruction.
However, most current NeRF models either achieve high accuracy using large
model sizes, or achieve high memory-efficiency by trading off accuracy. This
limits the applicable scope of any single model, since high-accuracy models
might not fit in low-memory devices, and memory-efficient models might not
satisfy high-quality requirements. To this end, we present SlimmeRF, a model
that allows for instant test-time trade-offs between model size and accuracy
through slimming, thus making the model simultaneously suitable for scenarios
with different computing budgets. We achieve this through a newly proposed
algorithm named Tensorial Rank Incrementation (TRaIn) which increases the rank
of the model's tensorial representation gradually during training. We also
observe that our model allows for more effective trade-offs in sparse-view
scenarios, at times even achieving higher accuracy after being slimmed. We
credit this to the fact that erroneous information such as floaters tend to be
stored in components corresponding to higher ranks. Our implementation is
available at https://github.com/Shiran-Yuan/SlimmeRF.";Shiran Yuan<author:sep>Hao Zhao;http://arxiv.org/pdf/2312.10034v1;cs.CV;"3DV 2024 Oral, Project Page: https://shiran-yuan.github.io/SlimmeRF/,
  Code: https://github.com/Shiran-Yuan/SlimmeRF/";nerf
2312.09558v1;http://arxiv.org/abs/2312.09558v1;2023-12-15;Towards Transferable Targeted 3D Adversarial Attack in the Physical  World;"Compared with transferable untargeted attacks, transferable targeted
adversarial attacks could specify the misclassification categories of
adversarial samples, posing a greater threat to security-critical tasks. In the
meanwhile, 3D adversarial samples, due to their potential of multi-view
robustness, can more comprehensively identify weaknesses in existing deep
learning systems, possessing great application value. However, the field of
transferable targeted 3D adversarial attacks remains vacant. The goal of this
work is to develop a more effective technique that could generate transferable
targeted 3D adversarial examples, filling the gap in this field. To achieve
this goal, we design a novel framework named TT3D that could rapidly
reconstruct from few multi-view images into Transferable Targeted 3D textured
meshes. While existing mesh-based texture optimization methods compute
gradients in the high-dimensional mesh space and easily fall into local optima,
leading to unsatisfactory transferability and distinct distortions, TT3D
innovatively performs dual optimization towards both feature grid and
Multi-layer Perceptron (MLP) parameters in the grid-based NeRF space, which
significantly enhances black-box transferability while enjoying naturalness.
Experimental results show that TT3D not only exhibits superior cross-model
transferability but also maintains considerable adaptability across different
renders and vision tasks. More importantly, we produce 3D adversarial examples
with 3D printing techniques in the real world and verify their robust
performance under various scenarios.";Yao Huang<author:sep>Yinpeng Dong<author:sep>Shouwei Ruan<author:sep>Xiao Yang<author:sep>Hang Su<author:sep>Xingxing Wei;http://arxiv.org/pdf/2312.09558v1;cs.CV;11 pages, 7 figures;nerf
2312.09913v1;http://arxiv.org/abs/2312.09913v1;2023-12-15;LAENeRF: Local Appearance Editing for Neural Radiance Fields;"Due to the omnipresence of Neural Radiance Fields (NeRFs), the interest
towards editable implicit 3D representations has surged over the last years.
However, editing implicit or hybrid representations as used for NeRFs is
difficult due to the entanglement of appearance and geometry encoded in the
model parameters. Despite these challenges, recent research has shown first
promising steps towards photorealistic and non-photorealistic appearance edits.
The main open issues of related work include limited interactivity, a lack of
support for local edits and large memory requirements, rendering them less
useful in practice. We address these limitations with LAENeRF, a unified
framework for photorealistic and non-photorealistic appearance editing of
NeRFs. To tackle local editing, we leverage a voxel grid as starting point for
region selection. We learn a mapping from expected ray terminations to final
output color, which can optionally be supervised by a style loss, resulting in
a framework which can perform photorealistic and non-photorealistic appearance
editing of selected regions. Relying on a single point per ray for our mapping,
we limit memory requirements and enable fast optimization. To guarantee
interactivity, we compose the output color using a set of learned, modifiable
base colors, composed with additive layer mixing. Compared to concurrent work,
LAENeRF enables recoloring and stylization while keeping processing time low.
Furthermore, we demonstrate that our approach surpasses baseline methods both
quantitatively and qualitatively.";Lukas Radl<author:sep>Michael Steiner<author:sep>Andreas Kurz<author:sep>Markus Steinberger;http://arxiv.org/pdf/2312.09913v1;cs.CV;Project website: https://r4dl.github.io/LAENeRF/;nerf
2312.09780v1;http://arxiv.org/abs/2312.09780v1;2023-12-15;RANRAC: Robust Neural Scene Representations via Random Ray Consensus;"We introduce RANRAC, a robust reconstruction algorithm for 3D objects
handling occluded and distracted images, which is a particularly challenging
scenario that prior robust reconstruction methods cannot deal with. Our
solution supports single-shot reconstruction by involving light-field networks,
and is also applicable to photo-realistic, robust, multi-view reconstruction
from real-world images based on neural radiance fields. While the algorithm
imposes certain limitations on the scene representation and, thereby, the
supported scene types, it reliably detects and excludes inconsistent
perspectives, resulting in clean images without floating artifacts. Our
solution is based on a fuzzy adaption of the random sample consensus paradigm,
enabling its application to large scale models. We interpret the minimal number
of samples to determine the model parameters as a tunable hyperparameter. This
is applicable, as a cleaner set of samples improves reconstruction quality.
Further, this procedure also handles outliers. Especially for conditioned
models, it can result in the same local minimum in the latent space as would be
obtained with a completely clean set. We report significant improvements for
novel-view synthesis in occluded scenarios, of up to 8dB PSNR compared to the
baseline.";Benno Buschmann<author:sep>Andreea Dogaru<author:sep>Elmar Eisemann<author:sep>Michael Weinmann<author:sep>Bernhard Egger;http://arxiv.org/pdf/2312.09780v1;cs.CV;;
2312.11535v2;http://arxiv.org/abs/2312.11535v2;2023-12-15;Customize-It-3D: High-Quality 3D Creation from A Single Image Using  Subject-Specific Knowledge Prior;"In this paper, we present a novel two-stage approach that fully utilizes the
information provided by the reference image to establish a customized knowledge
prior for image-to-3D generation. While previous approaches primarily rely on a
general diffusion prior, which struggles to yield consistent results with the
reference image, we propose a subject-specific and multi-modal diffusion model.
This model not only aids NeRF optimization by considering the shading mode for
improved geometry but also enhances texture from the coarse results to achieve
superior refinement. Both aspects contribute to faithfully aligning the 3D
content with the subject. Extensive experiments showcase the superiority of our
method, Customize-It-3D, outperforming previous works by a substantial margin.
It produces faithful 360-degree reconstructions with impressive visual quality,
making it well-suited for various applications, including text-to-3D creation.";Nan Huang<author:sep>Ting Zhang<author:sep>Yuhui Yuan<author:sep>Dong Chen<author:sep>Shanghang Zhang;http://arxiv.org/pdf/2312.11535v2;cs.CV;Project Page: https://nnanhuang.github.io/projects/customize-it-3d/;nerf
2312.11537v2;http://arxiv.org/abs/2312.11537v2;2023-12-15;FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple  Super-Resolution Pipeline;"Super-resolution (SR) techniques have recently been proposed to upscale the
outputs of neural radiance fields (NeRF) and generate high-quality images with
enhanced inference speeds. However, existing NeRF+SR methods increase training
overhead by using extra input features, loss functions, and/or expensive
training procedures such as knowledge distillation. In this paper, we aim to
leverage SR for efficiency gains without costly training or architectural
changes. Specifically, we build a simple NeRF+SR pipeline that directly
combines existing modules, and we propose a lightweight augmentation technique,
random patch sampling, for training. Compared to existing NeRF+SR methods, our
pipeline mitigates the SR computing overhead and can be trained up to 23x
faster, making it feasible to run on consumer devices such as the Apple
MacBook. Experiments show our pipeline can upscale NeRF outputs by 2-4x while
maintaining high quality, increasing inference speeds by up to 18x on an NVIDIA
V100 GPU and 12.8x on an M1 Pro chip. We conclude that SR can be a simple but
effective technique for improving the efficiency of NeRF models for consumer
devices.";Chien-Yu Lin<author:sep>Qichen Fu<author:sep>Thomas Merth<author:sep>Karren Yang<author:sep>Anurag Ranjan;http://arxiv.org/pdf/2312.11537v2;cs.CV;WACV 2024 (Oral);nerf
2312.09743v1;http://arxiv.org/abs/2312.09743v1;2023-12-15;SLS4D: Sparse Latent Space for 4D Novel View Synthesis;"Neural radiance field (NeRF) has achieved great success in novel view
synthesis and 3D representation for static scenarios. Existing dynamic NeRFs
usually exploit a locally dense grid to fit the deformation field; however,
they fail to capture the global dynamics and concomitantly yield models of
heavy parameters. We observe that the 4D space is inherently sparse. Firstly,
the deformation field is sparse in spatial but dense in temporal due to the
continuity of of motion. Secondly, the radiance field is only valid on the
surface of the underlying scene, usually occupying a small fraction of the
whole space. We thus propose to represent the 4D scene using a learnable sparse
latent space, a.k.a. SLS4D. Specifically, SLS4D first uses dense learnable time
slot features to depict the temporal space, from which the deformation field is
fitted with linear multi-layer perceptions (MLP) to predict the displacement of
a 3D position at any time. It then learns the spatial features of a 3D position
using another sparse latent space. This is achieved by learning the adaptive
weights of each latent code with the attention mechanism. Extensive experiments
demonstrate the effectiveness of our SLS4D: it achieves the best 4D novel view
synthesis using only about $6\%$ parameters of the most recent work.";Qi-Yuan Feng<author:sep>Hao-Xiang Chen<author:sep>Qun-Ce Xu<author:sep>Tai-Jiang Mu;http://arxiv.org/pdf/2312.09743v1;cs.CV;10 pages, 6 figures;nerf
2312.09682v1;http://arxiv.org/abs/2312.09682v1;2023-12-15;Exploring the Feasibility of Generating Realistic 3D Models of  Endangered Species Using DreamGaussian: An Analysis of Elevation Angle's  Impact on Model Generation;"Many species face the threat of extinction. It's important to study these
species and gather information about them as much as possible to preserve
biodiversity. Due to the rarity of endangered species, there is a limited
amount of data available, making it difficult to apply data requiring
generative AI methods to this domain. We aim to study the feasibility of
generating consistent and real-like 3D models of endangered animals using
limited data. Such a phenomenon leads us to utilize zero-shot stable diffusion
models that can generate a 3D model out of a single image of the target
species. This paper investigates the intricate relationship between elevation
angle and the output quality of 3D model generation, focusing on the innovative
approach presented in DreamGaussian. DreamGaussian, a novel framework utilizing
Generative Gaussian Splatting along with novel mesh extraction and refinement
algorithms, serves as the focal point of our study. We conduct a comprehensive
analysis, analyzing the effect of varying elevation angles on DreamGaussian's
ability to reconstruct 3D scenes accurately. Through an empirical evaluation,
we demonstrate how changes in elevation angle impact the generated images'
spatial coherence, structural integrity, and perceptual realism. We observed
that giving a correct elevation angle with the input image significantly
affects the result of the generated 3D model. We hope this study to be
influential for the usability of AI to preserve endangered animals; while the
penultimate aim is to obtain a model that can output biologically consistent 3D
models via small samples, the qualitative interpretation of an existing
state-of-the-art model such as DreamGaussian will be a step forward in our
goal.";Selcuk Anil Karatopak<author:sep>Deniz Sen;http://arxiv.org/pdf/2312.09682v1;cs.CV;;gaussian splatting
2312.09305v1;http://arxiv.org/abs/2312.09305v1;2023-12-14;Stable Score Distillation for High-Quality 3D Generation;"Score Distillation Sampling (SDS) has exhibited remarkable performance in
conditional 3D content generation. However, a comprehensive understanding of
the SDS formulation is still lacking, hindering the development of 3D
generation. In this work, we present an interpretation of SDS as a combination
of three functional components: mode-disengaging, mode-seeking and
variance-reducing terms, and analyze the properties of each. We show that
problems such as over-smoothness and color-saturation result from the intrinsic
deficiency of the supervision terms and reveal that the variance-reducing term
introduced by SDS is sub-optimal. Additionally, we shed light on the adoption
of large Classifier-Free Guidance (CFG) scale for 3D generation. Based on the
analysis, we propose a simple yet effective approach named Stable Score
Distillation (SSD) which strategically orchestrates each term for high-quality
3D generation. Extensive experiments validate the efficacy of our approach,
demonstrating its ability to generate high-fidelity 3D content without
succumbing to issues such as over-smoothness and over-saturation, even under
low CFG conditions with the most challenging NeRF representation.";Boshi Tang<author:sep>Jianan Wang<author:sep>Zhiyong Wu<author:sep>Lei Zhang;http://arxiv.org/pdf/2312.09305v1;cs.CV;;nerf
2312.09228v2;http://arxiv.org/abs/2312.09228v2;2023-12-14;3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting;"We introduce an approach that creates animatable human avatars from monocular
videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural
radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image
synthesis but often require days of training, and are extremely slow at
inference time. Recently, the community has explored fast grid structures for
efficient training of clothed avatars. Albeit being extremely fast at training,
these methods can barely achieve an interactive rendering frame rate with
around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a
non-rigid deformation network to reconstruct animatable clothed human avatars
that can be trained within 30 minutes and rendered at real-time frame rates
(50+ FPS). Given the explicit nature of our representation, we further
introduce as-isometric-as-possible regularizations on both the Gaussian mean
vectors and the covariance matrices, enhancing the generalization of our model
on highly articulated unseen poses. Experimental results show that our method
achieves comparable and even better performance compared to state-of-the-art
approaches on animatable avatar creation from a monocular input, while being
400x and 250x faster in training and inference, respectively.";Zhiyin Qian<author:sep>Shaofei Wang<author:sep>Marko Mihajlovic<author:sep>Andreas Geiger<author:sep>Siyu Tang;http://arxiv.org/pdf/2312.09228v2;cs.CV;Project page: https://neuralbodies.github.io/3DGS-Avatar;gaussian splatting<tag:sep>nerf
2312.08760v1;http://arxiv.org/abs/2312.08760v1;2023-12-14;CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental  Learning;"Neural Radiance Fields (NeRF) have demonstrated impressive performance in
novel view synthesis. However, NeRF and most of its variants still rely on
traditional complex pipelines to provide extrinsic and intrinsic camera
parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF,
directly treat camera parameters as learnable and estimate them through
differential volume rendering. However, these methods work for forward-looking
scenes with slight motions and fail to tackle the rotation scenario in
practice. To overcome this limitation, we propose a novel \underline{c}amera
parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally
reconstructs 3D representations and recovers the camera parameters inspired by
incremental structure from motion (SfM). Given a sequence of images, CF-NeRF
estimates the camera parameters of images one by one and reconstructs the scene
through initialization, implicit localization, and implicit optimization. To
evaluate our method, we use a challenging real-world dataset NeRFBuster which
provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF
is robust to camera rotation and achieves state-of-the-art results without
providing prior information and constraints.";Qingsong Yan<author:sep>Qiang Wang<author:sep>Kaiyong Zhao<author:sep>Jie Chen<author:sep>Bo Li<author:sep>Xiaowen Chu<author:sep>Fei Deng;http://arxiv.org/pdf/2312.08760v1;cs.CV;"Accepted at the Thirty-Eighth AAAI Conference on Artificial
  Intelligence (AAAI24)";nerf
2312.09147v2;http://arxiv.org/abs/2312.09147v2;2023-12-14;Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D  Reconstruction with Transformers;"Recent advancements in 3D reconstruction from single images have been driven
by the evolution of generative models. Prominent among these are methods based
on Score Distillation Sampling (SDS) and the adaptation of diffusion models in
the 3D domain. Despite their progress, these techniques often face limitations
due to slow optimization or rendering processes, leading to extensive training
and optimization times. In this paper, we introduce a novel approach for
single-view reconstruction that efficiently generates a 3D model from a single
image via feed-forward inference. Our method utilizes two transformer-based
networks, namely a point decoder and a triplane decoder, to reconstruct 3D
objects using a hybrid Triplane-Gaussian intermediate representation. This
hybrid representation strikes a balance, achieving a faster rendering speed
compared to implicit representations while simultaneously delivering superior
rendering quality than explicit representations. The point decoder is designed
for generating point clouds from single images, offering an explicit
representation which is then utilized by the triplane decoder to query Gaussian
features for each point. This design choice addresses the challenges associated
with directly regressing explicit 3D Gaussian attributes characterized by their
non-structural nature. Subsequently, the 3D Gaussians are decoded by an MLP to
enable rapid rendering through splatting. Both decoders are built upon a
scalable, transformer-based architecture and have been efficiently trained on
large-scale 3D datasets. The evaluations conducted on both synthetic datasets
and real-world images demonstrate that our method not only achieves higher
quality but also ensures a faster runtime in comparison to previous
state-of-the-art techniques. Please see our project page at
https://zouzx.github.io/TriplaneGaussian/.";Zi-Xin Zou<author:sep>Zhipeng Yu<author:sep>Yuan-Chen Guo<author:sep>Yangguang Li<author:sep>Ding Liang<author:sep>Yan-Pei Cao<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2312.09147v2;cs.CV;Project Page: https://zouzx.github.io/TriplaneGaussian/;gaussian splatting
2312.09313v2;http://arxiv.org/abs/2312.09313v2;2023-12-14;LatentEditor: Text Driven Local Editing of 3D Scenes;"While neural fields have made significant strides in view synthesis and scene
reconstruction, editing them poses a formidable challenge due to their implicit
encoding of geometry and texture information from multi-view inputs. In this
paper, we introduce \textsc{LatentEditor}, an innovative framework designed to
empower users with the ability to perform precise and locally controlled
editing of neural fields using text prompts. Leveraging denoising diffusion
models, we successfully embed real-world scenes into the latent space,
resulting in a faster and more adaptable NeRF backbone for editing compared to
traditional methods. To enhance editing precision, we introduce a delta score
to calculate the 2D mask in the latent space that serves as a guide for local
modifications while preserving irrelevant regions. Our novel pixel-level
scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the
disparity between IP2P conditional and unconditional noise predictions in the
latent space. The edited latents conditioned on the 2D masks are then
iteratively updated in the training set to achieve 3D local editing. Our
approach achieves faster editing speeds and superior output quality compared to
existing 3D editing models, bridging the gap between textual instructions and
high-quality 3D scene editing in latent space. We show the superiority of our
approach on four benchmark 3D datasets, LLFF, IN2N, NeRFStudio and NeRF-Art.";Umar Khalid<author:sep>Hasan Iqbal<author:sep>Nazmul Karim<author:sep>Jing Hua<author:sep>Chen Chen;http://arxiv.org/pdf/2312.09313v2;cs.CV;Project Page: https://latenteditor.github.io/;nerf
2312.09249v1;http://arxiv.org/abs/2312.09249v1;2023-12-14;ZeroRF: Fast Sparse View 360Â° Reconstruction with Zero Pretraining;"We present ZeroRF, a novel per-scene optimization method addressing the
challenge of sparse view 360{\deg} reconstruction in neural field
representations. Current breakthroughs like Neural Radiance Fields (NeRF) have
demonstrated high-fidelity image synthesis but struggle with sparse input
views. Existing methods, such as Generalizable NeRFs and per-scene optimization
approaches, face limitations in data dependency, computational cost, and
generalization across diverse scenarios. To overcome these challenges, we
propose ZeroRF, whose key idea is to integrate a tailored Deep Image Prior into
a factorized NeRF representation. Unlike traditional methods, ZeroRF
parametrizes feature grids with a neural network generator, enabling efficient
sparse view 360{\deg} reconstruction without any pretraining or additional
regularization. Extensive experiments showcase ZeroRF's versatility and
superiority in terms of both quality and speed, achieving state-of-the-art
results on benchmark datasets. ZeroRF's significance extends to applications in
3D content generation and editing. Project page:
https://sarahweiii.github.io/zerorf/";Ruoxi Shi<author:sep>Xinyue Wei<author:sep>Cheng Wang<author:sep>Hao Su;http://arxiv.org/pdf/2312.09249v1;cs.CV;Project page: https://sarahweiii.github.io/zerorf/;nerf
2312.09093v2;http://arxiv.org/abs/2312.09093v2;2023-12-14;Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption;"The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered
methodology, entangling the aspects of illumination and material reflectance
into emission solely from 3D points. This simplified rendering approach
presents challenges in accurately modeling images captured under adverse
lighting conditions, such as low light or over-exposure. Motivated by the
ancient Greek emission theory that posits visual perception as a result of rays
emanating from the eyes, we slightly refine the conventional NeRF framework to
train NeRF under challenging light conditions and generate normal-light
condition novel views unsupervised. We introduce the concept of a ""Concealing
Field,"" which assigns transmittance values to the surrounding air to account
for illumination effects. In dark scenarios, we assume that object emissions
maintain a standard lighting level but are attenuated as they traverse the air
during the rendering process. Concealing Field thus compel NeRF to learn
reasonable density and colour estimations for objects even in dimly lit
situations. Similarly, the Concealing Field can mitigate over-exposed emissions
during the rendering stage. Furthermore, we present a comprehensive multi-view
dataset captured under challenging illumination conditions for evaluation. Our
code and dataset available at https://github.com/cuiziteng/Aleth-NeRF";Ziteng Cui<author:sep>Lin Gu<author:sep>Xiao Sun<author:sep>Xianzheng Ma<author:sep>Yu Qiao<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2312.09093v2;cs.CV;"AAAI 2024, code available at https://github.com/cuiziteng/Aleth-NeRF
  Modified version of previous paper arXiv:2303.05807";nerf
2312.09031v1;http://arxiv.org/abs/2312.09031v1;2023-12-14;iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via  Comparing and Matching;"We present a method named iComMa to address the 6D pose estimation problem in
computer vision. The conventional pose estimation methods typically rely on the
target's CAD model or necessitate specific network training tailored to
particular object classes. Some existing methods address mesh-free 6D pose
estimation by employing the inversion of a Neural Radiance Field (NeRF), aiming
to overcome the aforementioned constraints. However, it still suffers from
adverse initializations. By contrast, we model the pose estimation as the
problem of inverting the 3D Gaussian Splatting (3DGS) with both the comparing
and matching loss. In detail, a render-and-compare strategy is adopted for the
precise estimation of poses. Additionally, a matching module is designed to
enhance the model's robustness against adverse initializations by minimizing
the distances between 2D keypoints. This framework systematically incorporates
the distinctive characteristics and inherent rationale of render-and-compare
and matching-based approaches. This comprehensive consideration equips the
framework to effectively address a broader range of intricate and challenging
scenarios, including instances with substantial angular deviations, all while
maintaining a high level of prediction accuracy. Experimental results
demonstrate the superior precision and robustness of our proposed jointly
optimized framework when evaluated on synthetic and complex real-world data in
challenging scenarios.";Yuan Sun<author:sep>Xuan Wang<author:sep>Yunfan Zhang<author:sep>Jie Zhang<author:sep>Caigui Jiang<author:sep>Yu Guo<author:sep>Fei Wang;http://arxiv.org/pdf/2312.09031v1;cs.CV;10 pages, 5 figures;gaussian splatting<tag:sep>nerf
2312.09005v1;http://arxiv.org/abs/2312.09005v1;2023-12-14;Scene 3-D Reconstruction System in Scattering Medium;"The research on neural radiance fields for new view synthesis has experienced
explosive growth with the development of new models and extensions. The NERF
algorithm, suitable for underwater scenes or scattering media, is also
evolving. Existing underwater 3D reconstruction systems still face challenges
such as extensive training time and low rendering efficiency. This paper
proposes an improved underwater 3D reconstruction system to address these
issues and achieve rapid, high-quality 3D reconstruction.To begin with, we
enhance underwater videos captured by a monocular camera to correct the poor
image quality caused by the physical properties of the water medium while
ensuring consistency in enhancement across adjacent frames. Subsequently, we
perform keyframe selection on the video frames to optimize resource utilization
and eliminate the impact of dynamic objects on the reconstruction results. The
selected keyframes, after pose estimation using COLMAP, undergo a
three-dimensional reconstruction improvement process using neural radiance
fields based on multi-resolution hash coding for model construction and
rendering.";Zhuoyifan Zhang<author:sep>Lu Zhang<author:sep>Liang Wang<author:sep>Haoming Wu;http://arxiv.org/pdf/2312.09005v1;cs.CV;;nerf
2312.09243v1;http://arxiv.org/abs/2312.09243v1;2023-12-14;OccNeRF: Self-Supervised Multi-Camera Occupancy Prediction with Neural  Radiance Fields;"As a fundamental task of vision-based perception, 3D occupancy prediction
reconstructs 3D structures of surrounding environments. It provides detailed
information for autonomous driving planning and navigation. However, most
existing methods heavily rely on the LiDAR point clouds to generate occupancy
ground truth, which is not available in the vision-based system. In this paper,
we propose an OccNeRF method for self-supervised multi-camera occupancy
prediction. Different from bounded 3D occupancy labels, we need to consider
unbounded scenes with raw image supervision. To solve the issue, we
parameterize the reconstructed occupancy fields and reorganize the sampling
strategy. The neural rendering is adopted to convert occupancy fields to
multi-camera depth maps, supervised by multi-frame photometric consistency.
Moreover, for semantic occupancy prediction, we design several strategies to
polish the prompts and filter the outputs of a pretrained open-vocabulary 2D
segmentation model. Extensive experiments for both self-supervised depth
estimation and semantic occupancy prediction tasks on nuScenes dataset
demonstrate the effectiveness of our method.";Chubin Zhang<author:sep>Juncheng Yan<author:sep>Yi Wei<author:sep>Jiaxin Li<author:sep>Li Liu<author:sep>Yansong Tang<author:sep>Yueqi Duan<author:sep>Jiwen Lu;http://arxiv.org/pdf/2312.09243v1;cs.CV;Code: https://github.com/LinShan-Bin/OccNeRF;nerf
2312.08692v1;http://arxiv.org/abs/2312.08692v1;2023-12-14;SpectralNeRF: Physically Based Spectral Rendering with Neural Radiance  Field;"In this paper, we propose SpectralNeRF, an end-to-end Neural Radiance Field
(NeRF)-based architecture for high-quality physically based rendering from a
novel spectral perspective. We modify the classical spectral rendering into two
main steps, 1) the generation of a series of spectrum maps spanning different
wavelengths, 2) the combination of these spectrum maps for the RGB output. Our
SpectralNeRF follows these two steps through the proposed multi-layer
perceptron (MLP)-based architecture (SpectralMLP) and Spectrum Attention UNet
(SAUNet). Given the ray origin and the ray direction, the SpectralMLP
constructs the spectral radiance field to obtain spectrum maps of novel views,
which are then sent to the SAUNet to produce RGB images of white-light
illumination. Applying NeRF to build up the spectral rendering is a more
physically-based way from the perspective of ray-tracing. Further, the spectral
radiance fields decompose difficult scenes and improve the performance of
NeRF-based methods. Comprehensive experimental results demonstrate the proposed
SpectralNeRF is superior to recent NeRF-based methods when synthesizing new
views on synthetic and real datasets. The codes and datasets are available at
https://github.com/liru0126/SpectralNeRF.";Ru Li<author:sep>Jia Liu<author:sep>Guanghui Liu<author:sep>Shengping Zhang<author:sep>Bing Zeng<author:sep>Shuaicheng Liu;http://arxiv.org/pdf/2312.08692v1;cs.CV;Accepted by AAAI 2024;nerf
2312.08892v1;http://arxiv.org/abs/2312.08892v1;2023-12-14;VaLID: Variable-Length Input Diffusion for Novel View Synthesis;"Novel View Synthesis (NVS), which tries to produce a realistic image at the
target view given source view images and their corresponding poses, is a
fundamental problem in 3D Vision. As this task is heavily under-constrained,
some recent work, like Zero123, tries to solve this problem with generative
modeling, specifically using pre-trained diffusion models. Although this
strategy generalizes well to new scenes, compared to neural radiance
field-based methods, it offers low levels of flexibility. For example, it can
only accept a single-view image as input, despite realistic applications often
offering multiple input images. This is because the source-view images and
corresponding poses are processed separately and injected into the model at
different stages. Thus it is not trivial to generalize the model into
multi-view source images, once they are available. To solve this issue, we try
to process each pose image pair separately and then fuse them as a unified
visual representation which will be injected into the model to guide image
synthesis at the target-views. However, inconsistency and computation costs
increase as the number of input source-view images increases. To solve these
issues, the Multi-view Cross Former module is proposed which maps
variable-length input data to fix-size output data. A two-stage training
strategy is introduced to further improve the efficiency during training time.
Qualitative and quantitative evaluation over multiple datasets demonstrates the
effectiveness of the proposed method against previous approaches. The code will
be released according to the acceptance.";Shijie Li<author:sep>Farhad G. Zanjani<author:sep>Haitam Ben Yahia<author:sep>Yuki M. Asano<author:sep>Juergen Gall<author:sep>Amirhossein Habibian;http://arxiv.org/pdf/2312.08892v1;cs.CV;paper and supplementary material;
2312.09095v2;http://arxiv.org/abs/2312.09095v2;2023-12-14;ColNeRF: Collaboration for Generalizable Sparse Input Neural Radiance  Field;"Neural Radiance Fields (NeRF) have demonstrated impressive potential in
synthesizing novel views from dense input, however, their effectiveness is
challenged when dealing with sparse input. Existing approaches that incorporate
additional depth or semantic supervision can alleviate this issue to an extent.
However, the process of supervision collection is not only costly but also
potentially inaccurate, leading to poor performance and generalization ability
in diverse scenarios. In our work, we introduce a novel model: the
Collaborative Neural Radiance Fields (ColNeRF) designed to work with sparse
input. The collaboration in ColNeRF includes both the cooperation between
sparse input images and the cooperation between the output of the neural
radiation field. Through this, we construct a novel collaborative module that
aligns information from various views and meanwhile imposes self-supervised
constraints to ensure multi-view consistency in both geometry and appearance. A
Collaborative Cross-View Volume Integration module (CCVI) is proposed to
capture complex occlusions and implicitly infer the spatial location of
objects. Moreover, we introduce self-supervision of target rays projected in
multiple directions to ensure geometric and color consistency in adjacent
regions. Benefiting from the collaboration at the input and output ends,
ColNeRF is capable of capturing richer and more generalized scene
representation, thereby facilitating higher-quality results of the novel view
synthesis. Extensive experiments demonstrate that ColNeRF outperforms
state-of-the-art sparse input generalizable NeRF methods. Furthermore, our
approach exhibits superiority in fine-tuning towards adapting to new scenes,
achieving competitive performance compared to per-scene optimized NeRF-based
methods while significantly reducing computational costs. Our code is available
at: https://github.com/eezkni/ColNeRF.";Zhangkai Ni<author:sep>Peiqi Yang<author:sep>Wenhan Yang<author:sep>Hanli Wang<author:sep>Lin Ma<author:sep>Sam Kwong;http://arxiv.org/pdf/2312.09095v2;cs.CV;;nerf
2312.08094v1;http://arxiv.org/abs/2312.08094v1;2023-12-13;3DGEN: A GAN-based approach for generating novel 3D models from image  data;"The recent advances in text and image synthesis show a great promise for the
future of generative models in creative fields. However, a less explored area
is the one of 3D model generation, with a lot of potential applications to game
design, video production, and physical product design. In our paper, we present
3DGEN, a model that leverages the recent work on both Neural Radiance Fields
for object reconstruction and GAN-based image generation. We show that the
proposed architecture can generate plausible meshes for objects of the same
category as the training images and compare the resulting meshes with the
state-of-the-art baselines, leading to visible uplifts in generation quality.";Antoine Schnepf<author:sep>Flavian Vasile<author:sep>Ugo Tanielian;http://arxiv.org/pdf/2312.08094v1;cs.CV;"Submitted to NeurIPS 2022 Machine Learning for Creativity and Design
  Workshop";
2312.08118v1;http://arxiv.org/abs/2312.08118v1;2023-12-13;Neural Radiance Fields for Transparent Object Using Visual Hull;"Unlike opaque object, novel view synthesis of transparent object is a
challenging task, because transparent object refracts light of background
causing visual distortions on the transparent object surface along the
viewpoint change. Recently introduced Neural Radiance Fields (NeRF) is a view
synthesis method. Thanks to its remarkable performance improvement, lots of
following applications based on NeRF in various topics have been developed.
However, if an object with a different refractive index is included in a scene
such as transparent object, NeRF shows limited performance because refracted
light ray at the surface of the transparent object is not appropriately
considered. To resolve the problem, we propose a NeRF-based method consisting
of the following three steps: First, we reconstruct a three-dimensional shape
of a transparent object using visual hull. Second, we simulate the refraction
of the rays inside of the transparent object according to Snell's law. Last, we
sample points through refracted rays and put them into NeRF. Experimental
evaluation results demonstrate that our method addresses the limitation of
conventional NeRF with transparent objects.";Heechan Yoon<author:sep>Seungkyu Lee;http://arxiv.org/pdf/2312.08118v1;cs.CV;;nerf
2312.08136v1;http://arxiv.org/abs/2312.08136v1;2023-12-13;ProNeRF: Learning Efficient Projection-Aware Ray Sampling for  Fine-Grained Implicit Neural Radiance Fields;"Recent advances in neural rendering have shown that, albeit slow, implicit
compact models can learn a scene's geometries and view-dependent appearances
from multiple views. To maintain such a small memory footprint but achieve
faster inference times, recent works have adopted `sampler' networks that
adaptively sample a small subset of points along each ray in the implicit
neural radiance fields. Although these methods achieve up to a 10$\times$
reduction in rendering time, they still suffer from considerable quality
degradation compared to the vanilla NeRF. In contrast, we propose ProNeRF,
which provides an optimal trade-off between memory footprint (similar to NeRF),
speed (faster than HyperReel), and quality (better than K-Planes). ProNeRF is
equipped with a novel projection-aware sampling (PAS) network together with a
new training strategy for ray exploration and exploitation, allowing for
efficient fine-grained particle sampling. Our ProNeRF yields state-of-the-art
metrics, being 15-23x faster with 0.65dB higher PSNR than NeRF and yielding
0.95dB higher PSNR than the best published sampler-based method, HyperReel. Our
exploration and exploitation training strategy allows ProNeRF to learn the full
scenes' color and density distributions while also learning efficient ray
sampling focused on the highest-density regions. We provide extensive
experimental results that support the effectiveness of our method on the widely
adopted forward-facing and 360 datasets, LLFF and Blender, respectively.";Juan Luis Gonzalez Bello<author:sep>Minh-Quan Viet Bui<author:sep>Munchurl Kim;http://arxiv.org/pdf/2312.08136v1;cs.CV;"Visit our project website at
  https://kaist-viclab.github.io/pronerf-site/";nerf
2312.08012v1;http://arxiv.org/abs/2312.08012v1;2023-12-13;uSF: Learning Neural Semantic Field with Uncertainty;"Recently, there has been an increased interest in NeRF methods which
reconstruct differentiable representation of three-dimensional scenes. One of
the main limitations of such methods is their inability to assess the
confidence of the model in its predictions. In this paper, we propose a new
neural network model for the formation of extended vector representations,
called uSF, which allows the model to predict not only color and semantic label
of each point, but also estimate the corresponding values of uncertainty. We
show that with a small number of images available for training, a model
quantifying uncertainty performs better than a model without such
functionality. Code of the uSF approach is publicly available at
https://github.com/sevashasla/usf/.";Vsevolod Skorokhodov<author:sep>Darya Drozdova<author:sep>Dmitry Yudin;http://arxiv.org/pdf/2312.08012v1;cs.CV;12 pages, 4 figures;nerf
2312.07920v1;http://arxiv.org/abs/2312.07920v1;2023-12-13;DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic  Autonomous Driving Scenes;"We present DrivingGaussian, an efficient and effective framework for
surrounding dynamic autonomous driving scenes. For complex scenes with moving
objects, we first sequentially and progressively model the static background of
the entire scene with incremental static 3D Gaussians. We then leverage a
composite dynamic Gaussian graph to handle multiple moving objects,
individually reconstructing each object and restoring their accurate positions
and occlusion relationships within the scene. We further use a LiDAR prior for
Gaussian Splatting to reconstruct scenes with greater details and maintain
panoramic consistency. DrivingGaussian outperforms existing methods in driving
scene reconstruction and enables photorealistic surround-view synthesis with
high-fidelity and multi-camera consistency. The source code and trained models
will be released.";Xiaoyu Zhou<author:sep>Zhiwei Lin<author:sep>Xiaojun Shan<author:sep>Yongtao Wang<author:sep>Deqing Sun<author:sep>Ming-Hsuan Yang;http://arxiv.org/pdf/2312.07920v1;cs.CV;;gaussian splatting
2312.06946v1;http://arxiv.org/abs/2312.06946v1;2023-12-12;WaterHE-NeRF: Water-ray Tracing Neural Radiance Fields for Underwater  Scene Reconstruction;"Neural Radiance Field (NeRF) technology demonstrates immense potential in
novel viewpoint synthesis tasks, due to its physics-based volumetric rendering
process, which is particularly promising in underwater scenes. Addressing the
limitations of existing underwater NeRF methods in handling light attenuation
caused by the water medium and the lack of real Ground Truth (GT) supervision,
this study proposes WaterHE-NeRF. We develop a new water-ray tracing field by
Retinex theory that precisely encodes color, density, and illuminance
attenuation in three-dimensional space. WaterHE-NeRF, through its illuminance
attenuation mechanism, generates both degraded and clear multi-view images and
optimizes image restoration by combining reconstruction loss with Wasserstein
distance. Additionally, the use of histogram equalization (HE) as pseudo-GT
enhances the network's accuracy in preserving original details and color
distribution. Extensive experiments on real underwater datasets and synthetic
datasets validate the effectiveness of WaterHE-NeRF. Our code will be made
publicly available.";Jingchun Zhou<author:sep>Tianyu Liang<author:sep>Zongxin He<author:sep>Dehuan Zhang<author:sep>Weishi Zhang<author:sep>Xianping Fu<author:sep>Chongyi Li;http://arxiv.org/pdf/2312.06946v1;cs.CV;;nerf
2312.07504v1;http://arxiv.org/abs/2312.07504v1;2023-12-12;COLMAP-Free 3D Gaussian Splatting;"While neural rendering has led to impressive advances in scene reconstruction
and novel view synthesis, it relies heavily on accurately pre-computed camera
poses. To relax this constraint, multiple efforts have been made to train
Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the
implicit representations of NeRFs provide extra challenges to optimize the 3D
structure and camera poses at the same time. On the other hand, the recently
proposed 3D Gaussian Splatting provides new opportunities given its explicit
point cloud representations. This paper leverages both the explicit geometric
representation and the continuity of the input video stream to perform novel
view synthesis without any SfM preprocessing. We process the input frames in a
sequential manner and progressively grow the 3D Gaussians set by taking one
input frame at a time, without the need to pre-compute the camera poses. Our
method significantly improves over previous approaches in view synthesis and
camera pose estimation under large motion changes. Our project page is
https://oasisyang.github.io/colmap-free-3dgs";Yang Fu<author:sep>Sifei Liu<author:sep>Amey Kulkarni<author:sep>Jan Kautz<author:sep>Alexei A. Efros<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2312.07504v1;cs.CV;Project Page: https://oasisyang.github.io/colmap-free-3dgs;gaussian splatting<tag:sep>nerf
2312.07246v1;http://arxiv.org/abs/2312.07246v1;2023-12-12;Unifying Correspondence, Pose and NeRF for Pose-Free Novel View  Synthesis from Stereo Pairs;"This work delves into the task of pose-free novel view synthesis from stereo
pairs, a challenging and pioneering task in 3D vision. Our innovative
framework, unlike any before, seamlessly integrates 2D correspondence matching,
camera pose estimation, and NeRF rendering, fostering a synergistic enhancement
of these tasks. We achieve this through designing an architecture that utilizes
a shared representation, which serves as a foundation for enhanced 3D geometry
understanding. Capitalizing on the inherent interplay between the tasks, our
unified framework is trained end-to-end with the proposed training strategy to
improve overall model accuracy. Through extensive evaluations across diverse
indoor and outdoor scenes from two real-world datasets, we demonstrate that our
approach achieves substantial improvement over previous methodologies,
especially in scenarios characterized by extreme viewpoint changes and the
absence of accurate camera poses.";Sunghwan Hong<author:sep>Jaewoo Jung<author:sep>Heeseong Shin<author:sep>Jiaolong Yang<author:sep>Seungryong Kim<author:sep>Chong Luo;http://arxiv.org/pdf/2312.07246v1;cs.CV;Project page: https://ku-cvlab.github.io/CoPoNeRF/;nerf
2312.06657v1;http://arxiv.org/abs/2312.06657v1;2023-12-11;Learning Naturally Aggregated Appearance for Efficient 3D Editing;"Neural radiance fields, which represent a 3D scene as a color field and a
density field, have demonstrated great progress in novel view synthesis yet are
unfavorable for editing due to the implicitness. In view of such a deficiency,
we propose to replace the color field with an explicit 2D appearance
aggregation, also called canonical image, with which users can easily customize
their 3D editing via 2D image processing. To avoid the distortion effect and
facilitate convenient editing, we complement the canonical image with a
projection field that maps 3D points onto 2D pixels for texture lookup. This
field is carefully initialized with a pseudo canonical camera model and
optimized with offset regularity to ensure naturalness of the aggregated
appearance. Extensive experimental results on three datasets suggest that our
representation, dubbed AGAP, well supports various ways of 3D editing (e.g.,
stylization, interactive drawing, and content extraction) with no need of
re-optimization for each case, demonstrating its generalizability and
efficiency. Project page is available at https://felixcheng97.github.io/AGAP/.";Ka Leong Cheng<author:sep>Qiuyu Wang<author:sep>Zifan Shi<author:sep>Kecheng Zheng<author:sep>Yinghao Xu<author:sep>Hao Ouyang<author:sep>Qifeng Chen<author:sep>Yujun Shen;http://arxiv.org/pdf/2312.06657v1;cs.CV;"Project Webpage: https://felixcheng97.github.io/AGAP/, Code:
  https://github.com/felixcheng97/AGAP";
2312.05283v1;http://arxiv.org/abs/2312.05283v1;2023-12-11;Nuvo: Neural UV Mapping for Unruly 3D Representations;"Existing UV mapping algorithms are designed to operate on well-behaved
meshes, instead of the geometry representations produced by state-of-the-art 3D
reconstruction and generation techniques. As such, applying these methods to
the volume densities recovered by neural radiance fields and related techniques
(or meshes triangulated from such fields) results in texture atlases that are
too fragmented to be useful for tasks such as view synthesis or appearance
editing. We present a UV mapping method designed to operate on geometry
produced by 3D reconstruction and generation techniques. Instead of computing a
mapping defined on a mesh's vertices, our method Nuvo uses a neural field to
represent a continuous UV mapping, and optimizes it to be a valid and
well-behaved mapping for just the set of visible points, i.e. only points that
affect the scene's appearance. We show that our model is robust to the
challenges posed by ill-behaved geometry, and that it produces editable UV
mappings that can represent detailed appearance.";Pratul P. Srinivasan<author:sep>Stephan J. Garbin<author:sep>Dor Verbin<author:sep>Jonathan T. Barron<author:sep>Ben Mildenhall;http://arxiv.org/pdf/2312.05283v1;cs.CV;Project page at https://pratulsrinivasan.github.io/nuvo;
2312.06439v1;http://arxiv.org/abs/2312.06439v1;2023-12-11;DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior;"3D generation has raised great attention in recent years. With the success of
text-to-image diffusion models, the 2D-lifting technique becomes a promising
route to controllable 3D generation. However, these methods tend to present
inconsistent geometry, which is also known as the Janus problem. We observe
that the problem is caused mainly by two aspects, i.e., viewpoint bias in 2D
diffusion models and overfitting of the optimization objective. To address it,
we propose a two-stage 2D-lifting framework, namely DreamControl, which
optimizes coarse NeRF scenes as 3D self-prior and then generates fine-grained
objects with control-based score distillation. Specifically, adaptive viewpoint
sampling and boundary integrity metric are proposed to ensure the consistency
of generated priors. The priors are then regarded as input conditions to
maintain reasonable geometries, in which conditional LoRA and weighted score
are further proposed to optimize detailed textures. DreamControl can generate
high-quality 3D content in terms of both geometry consistency and texture
fidelity. Moreover, our control-based optimization guidance is applicable to
more downstream tasks, including user-guided generation and 3D animation. The
project page is available at https://github.com/tyhuang0428/DreamControl.";Tianyu Huang<author:sep>Yihan Zeng<author:sep>Zhilu Zhang<author:sep>Wan Xu<author:sep>Hang Xu<author:sep>Songcen Xu<author:sep>Rynson W. H. Lau<author:sep>Wangmeng Zuo;http://arxiv.org/pdf/2312.06439v1;cs.CV;;nerf
2312.06642v1;http://arxiv.org/abs/2312.06642v1;2023-12-11;CorresNeRF: Image Correspondence Priors for Neural Radiance Fields;"Neural Radiance Fields (NeRFs) have achieved impressive results in novel view
synthesis and surface reconstruction tasks. However, their performance suffers
under challenging scenarios with sparse input views. We present CorresNeRF, a
novel method that leverages image correspondence priors computed by
off-the-shelf methods to supervise NeRF training. We design adaptive processes
for augmentation and filtering to generate dense and high-quality
correspondences. The correspondences are then used to regularize NeRF training
via the correspondence pixel reprojection and depth loss terms. We evaluate our
methods on novel view synthesis and surface reconstruction tasks with
density-based and SDF-based NeRF models on different datasets. Our method
outperforms previous methods in both photometric and geometric metrics. We show
that this simple yet effective technique of using correspondence priors can be
applied as a plug-and-play module across different NeRF variants. The project
page is at https://yxlao.github.io/corres-nerf.";Yixing Lao<author:sep>Xiaogang Xu<author:sep>Zhipeng Cai<author:sep>Xihui Liu<author:sep>Hengshuang Zhao;http://arxiv.org/pdf/2312.06642v1;cs.CV;;nerf
2312.06741v1;http://arxiv.org/abs/2312.06741v1;2023-12-11;Gaussian Splatting SLAM;"We present the first application of 3D Gaussian Splatting to incremental 3D
reconstruction using a single moving monocular or RGB-D camera. Our
Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps,
utilises Gaussians as the only 3D representation, unifying the required
representation for accurate, efficient tracking, mapping, and high-quality
rendering. Several innovations are required to continuously reconstruct 3D
scenes with high fidelity from a live camera. First, to move beyond the
original 3DGS algorithm, which requires accurate poses from an offline
Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using
direct optimisation against the 3D Gaussians, and show that this enables fast
and robust tracking with a wide basin of convergence. Second, by utilising the
explicit nature of the Gaussians, we introduce geometric verification and
regularisation to handle the ambiguities occurring in incremental 3D dense
reconstruction. Finally, we introduce a full SLAM system which not only
achieves state-of-the-art results in novel view synthesis and trajectory
estimation, but also reconstruction of tiny and even transparent objects.";Hidenobu Matsuki<author:sep>Riku Murai<author:sep>Paul H. J. Kelly<author:sep>Andrew J. Davison;http://arxiv.org/pdf/2312.06741v1;cs.CV;"First two authors contributed equally to this work. Project Page:
  https://rmurai.co.uk/projects/GaussianSplattingSLAM/ Video:
  https://www.youtube.com/watch?v=x604ghp9R_Q&ab_channel=DysonRoboticsLaboratoryatImperialCollege";gaussian splatting
2401.08633v1;http://arxiv.org/abs/2401.08633v1;2023-12-11;Creating Visual Effects with Neural Radiance Fields;"We present a pipeline for integrating NeRFs into traditional compositing VFX
pipelines using Nerfstudio, an open-source framework for training and rendering
NeRFs. Our approach involves using Blender, a widely used open-source 3D
creation software, to align camera paths and composite NeRF renders with meshes
and other NeRFs, allowing for seamless integration of NeRFs into traditional
VFX pipelines. Our NeRF Blender add-on allows for more controlled camera
trajectories of photorealistic scenes, compositing meshes and other
environmental effects with NeRFs, and compositing multiple NeRFs in a single
scene.This approach of generating NeRF aligned camera paths can be adapted to
other 3D tool sets and workflows, enabling a more seamless integration of NeRFs
into visual effects and film production. Documentation can be found here:
https://docs.nerf.studio/extensions/blender_addon.html";Cyrus Vachha;http://arxiv.org/pdf/2401.08633v1;cs.CV;2 pages, 4 figures;nerf
2312.05855v1;http://arxiv.org/abs/2312.05855v1;2023-12-10;NeVRF: Neural Video-based Radiance Fields for Long-duration Sequences;"Adopting Neural Radiance Fields (NeRF) to long-duration dynamic sequences has
been challenging. Existing methods struggle to balance between quality and
storage size and encounter difficulties with complex scene changes such as
topological changes and large motions. To tackle these issues, we propose a
novel neural video-based radiance fields (NeVRF) representation. NeVRF marries
neural radiance field with image-based rendering to support photo-realistic
novel view synthesis on long-duration dynamic inward-looking scenes. We
introduce a novel multi-view radiance blending approach to predict radiance
directly from multi-view videos. By incorporating continual learning
techniques, NeVRF can efficiently reconstruct frames from sequential data
without revisiting previous frames, enabling long-duration free-viewpoint
video. Furthermore, with a tailored compression approach, NeVRF can compactly
represent dynamic scenes, making dynamic radiance fields more practical in
real-world scenarios. Our extensive experiments demonstrate the effectiveness
of NeVRF in enabling long-duration sequence rendering, sequential data
reconstruction, and compact data storage.";Minye Wu<author:sep>Tinne Tuytelaars;http://arxiv.org/pdf/2312.05855v1;cs.CV;11 pages, 12 figures;nerf
2312.05873v1;http://arxiv.org/abs/2312.05873v1;2023-12-10;Learning for CasADi: Data-driven Models in Numerical Optimization;"While real-world problems are often challenging to analyze analytically, deep
learning excels in modeling complex processes from data. Existing optimization
frameworks like CasADi facilitate seamless usage of solvers but face challenges
when integrating learned process models into numerical optimizations. To
address this gap, we present the Learning for CasADi (L4CasADi) framework,
enabling the seamless integration of PyTorch-learned models with CasADi for
efficient and potentially hardware-accelerated numerical optimization. The
applicability of L4CasADi is demonstrated with two tutorial examples: First, we
optimize a fish's trajectory in a turbulent river for energy efficiency where
the turbulent flow is represented by a PyTorch model. Second, we demonstrate
how an implicit Neural Radiance Field environment representation can be easily
leveraged for optimal control with L4CasADi. L4CasADi, along with examples and
documentation, is available under MIT license at
https://github.com/Tim-Salzmann/l4casadi";Tim Salzmann<author:sep>Jon Arrizabalaga<author:sep>Joel Andersson<author:sep>Marco Pavone<author:sep>Markus Ryll;http://arxiv.org/pdf/2312.05873v1;eess.SY;;
2312.05748v1;http://arxiv.org/abs/2312.05748v1;2023-12-10;IL-NeRF: Incremental Learning for Neural Radiance Fields with Camera  Pose Alignment;"Neural radiance fields (NeRF) is a promising approach for generating
photorealistic images and representing complex scenes. However, when processing
data sequentially, it can suffer from catastrophic forgetting, where previous
data is easily forgotten after training with new data. Existing incremental
learning methods using knowledge distillation assume that continuous data
chunks contain both 2D images and corresponding camera pose parameters,
pre-estimated from the complete dataset. This poses a paradox as the necessary
camera pose must be estimated from the entire dataset, even though the data
arrives sequentially and future chunks are inaccessible. In contrast, we focus
on a practical scenario where camera poses are unknown. We propose IL-NeRF, a
novel framework for incremental NeRF training, to address this challenge.
IL-NeRF's key idea lies in selecting a set of past camera poses as references
to initialize and align the camera poses of incoming image data. This is
followed by a joint optimization of camera poses and replay-based NeRF
distillation. Our experiments on real-world indoor and outdoor scenes show that
IL-NeRF handles incremental NeRF training and outperforms the baselines by up
to $54.04\%$ in rendering quality.";Letian Zhang<author:sep>Ming Li<author:sep>Chen Chen<author:sep>Jie Xu;http://arxiv.org/pdf/2312.05748v1;cs.CV;;nerf
2312.05941v1;http://arxiv.org/abs/2312.05941v1;2023-12-10;ASH: Animatable Gaussian Splats for Efficient and Photoreal Human  Rendering;"Real-time rendering of photorealistic and controllable human avatars stands
as a cornerstone in Computer Vision and Graphics. While recent advances in
neural implicit rendering have unlocked unprecedented photorealism for digital
avatars, real-time performance has mostly been demonstrated for static scenes
only. To address this, we propose ASH, an animatable Gaussian splatting
approach for photorealistic rendering of dynamic humans in real-time. We
parameterize the clothed human as animatable 3D Gaussians, which can be
efficiently splatted into image space to generate the final rendering. However,
naively learning the Gaussian parameters in 3D space poses a severe challenge
in terms of compute. Instead, we attach the Gaussians onto a deformable
character model, and learn their parameters in 2D texture space, which allows
leveraging efficient 2D convolutional architectures that easily scale with the
required number of Gaussians. We benchmark ASH with competing methods on
pose-controllable avatars, demonstrating that our method outperforms existing
real-time methods by a large margin and shows comparable or even better results
than offline methods.";Haokai Pang<author:sep>Heming Zhu<author:sep>Adam Kortylewski<author:sep>Christian Theobalt<author:sep>Marc Habermann;http://arxiv.org/pdf/2312.05941v1;cs.CV;"13 pages, 7 figures. For project page, see
  https://vcai.mpi-inf.mpg.de/projects/ash/";gaussian splatting
2312.06713v1;http://arxiv.org/abs/2312.06713v1;2023-12-10;TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint  Video;"Neural Radiance Fields (NeRF) revolutionize the realm of visual media by
providing photorealistic Free-Viewpoint Video (FVV) experiences, offering
viewers unparalleled immersion and interactivity. However, the technology's
significant storage requirements and the computational complexity involved in
generation and rendering currently limit its broader application. To close this
gap, this paper presents Temporal Tri-Plane Radiance Fields (TeTriRF), a novel
technology that significantly reduces the storage size for Free-Viewpoint Video
(FVV) while maintaining low-cost generation and rendering. TeTriRF introduces a
hybrid representation with tri-planes and voxel grids to support scaling up to
long-duration sequences and scenes with complex motions or rapid changes. We
propose a group training scheme tailored to achieving high training efficiency
and yielding temporally consistent, low-entropy scene representations.
Leveraging these properties of the representations, we introduce a compression
pipeline with off-the-shelf video codecs, achieving an order of magnitude less
storage size compared to the state-of-the-art. Our experiments demonstrate that
TeTriRF can achieve competitive quality with a higher compression rate.";Minye Wu<author:sep>Zehao Wang<author:sep>Georgios Kouros<author:sep>Tinne Tuytelaars;http://arxiv.org/pdf/2312.06713v1;cs.CV;13 pages, 11 figures;nerf
2312.05664v1;http://arxiv.org/abs/2312.05664v1;2023-12-09;CoGS: Controllable Gaussian Splatting;"Capturing and re-animating the 3D structure of articulated objects present
significant barriers. On one hand, methods requiring extensively calibrated
multi-view setups are prohibitively complex and resource-intensive, limiting
their practical applicability. On the other hand, while single-camera Neural
Radiance Fields (NeRFs) offer a more streamlined approach, they have excessive
training and rendering costs. 3D Gaussian Splatting would be a suitable
alternative but for two reasons. Firstly, existing methods for 3D dynamic
Gaussians require synchronized multi-view cameras, and secondly, the lack of
controllability in dynamic scenarios. We present CoGS, a method for
Controllable Gaussian Splatting, that enables the direct manipulation of scene
elements, offering real-time control of dynamic scenes without the prerequisite
of pre-computing control signals. We evaluated CoGS using both synthetic and
real-world datasets that include dynamic objects that differ in degree of
difficulty. In our evaluations, CoGS consistently outperformed existing dynamic
and controllable neural representations in terms of visual fidelity.";Heng Yu<author:sep>Joel Julin<author:sep>ZoltÃ¡n Ã. Milacski<author:sep>Koichiro Niinuma<author:sep>LÃ¡szlÃ³ A. Jeni;http://arxiv.org/pdf/2312.05664v1;cs.CV;10 pages, in submission;gaussian splatting<tag:sep>nerf
2312.06686v1;http://arxiv.org/abs/2312.06686v1;2023-12-09;Robo360: A 3D Omnispective Multi-Material Robotic Manipulation Dataset;"Building robots that can automate labor-intensive tasks has long been the
core motivation behind the advancements in computer vision and the robotics
community. Recent interest in leveraging 3D algorithms, particularly neural
fields, has led to advancements in robot perception and physical understanding
in manipulation scenarios. However, the real world's complexity poses
significant challenges. To tackle these challenges, we present Robo360, a
dataset that features robotic manipulation with a dense view coverage, which
enables high-quality 3D neural representation learning, and a diverse set of
objects with various physical and optical properties and facilitates research
in various object manipulation and physical world modeling tasks. We confirm
the effectiveness of our dataset using existing dynamic NeRF and evaluate its
potential in learning multi-view policies. We hope that Robo360 can open new
research directions yet to be explored at the intersection of understanding the
physical world in 3D and robot control.";Litian Liang<author:sep>Liuyu Bian<author:sep>Caiwei Xiao<author:sep>Jialin Zhang<author:sep>Linghao Chen<author:sep>Isabella Liu<author:sep>Fanbo Xiang<author:sep>Zhiao Huang<author:sep>Hao Su;http://arxiv.org/pdf/2312.06686v1;cs.CV;;nerf
2312.05572v1;http://arxiv.org/abs/2312.05572v1;2023-12-09;R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid  Landmarks Encoding and Progressive Multilayer Conditioning;"Dynamic NeRFs have recently garnered growing attention for 3D talking
portrait synthesis. Despite advances in rendering speed and visual quality,
challenges persist in enhancing efficiency and effectiveness. We present
R2-Talker, an efficient and effective framework enabling realistic real-time
talking head synthesis. Specifically, using multi-resolution hash grids, we
introduce a novel approach for encoding facial landmarks as conditional
features. This approach losslessly encodes landmark structures as conditional
features, decoupling input diversity, and conditional spaces by mapping
arbitrary landmarks to a unified feature space. We further propose a scheme of
progressive multilayer conditioning in the NeRF rendering pipeline for
effective conditional feature fusion. Our new approach has the following
advantages as demonstrated by extensive experiments compared with the
state-of-the-art works: 1) The lossless input encoding enables acquiring more
precise features, yielding superior visual quality. The decoupling of inputs
and conditional spaces improves generalizability. 2) The fusing of conditional
features and MLP outputs at each MLP layer enhances conditional impact,
resulting in more accurate lip synthesis and better visual quality. 3) It
compactly structures the fusion of conditional features, significantly
enhancing computational efficiency.";Zhiling Ye<author:sep>LiangGuo Zhang<author:sep>Dingheng Zeng<author:sep>Quan Lu<author:sep>Ning Jiang;http://arxiv.org/pdf/2312.05572v1;cs.CV;;nerf
2312.05330v1;http://arxiv.org/abs/2312.05330v1;2023-12-08;Multi-view Inversion for 3D-aware Generative Adversarial Networks;"Current 3D GAN inversion methods for human heads typically use only one
single frontal image to reconstruct the whole 3D head model. This leaves out
meaningful information when multi-view data or dynamic videos are available.
Our method builds on existing state-of-the-art 3D GAN inversion techniques to
allow for consistent and simultaneous inversion of multiple views of the same
subject. We employ a multi-latent extension to handle inconsistencies present
in dynamic face videos to re-synthesize consistent 3D representations from the
sequence. As our method uses additional information about the target subject,
we observe significant enhancements in both geometric accuracy and image
quality, particularly when rendering from wide viewing angles. Moreover, we
demonstrate the editability of our inverted 3D renderings, which distinguishes
them from NeRF-based scene reconstructions.";Florian Barthel<author:sep>Anna Hilsmann<author:sep>Peter Eisert;http://arxiv.org/pdf/2312.05330v1;cs.CV;;nerf
2312.05311v1;http://arxiv.org/abs/2312.05311v1;2023-12-08;360Â° Volumetric Portrait Avatar;"We propose 360{\deg} Volumetric Portrait (3VP) Avatar, a novel method for
reconstructing 360{\deg} photo-realistic portrait avatars of human subjects
solely based on monocular video inputs. State-of-the-art monocular avatar
reconstruction methods rely on stable facial performance capturing. However,
the common usage of 3DMM-based facial tracking has its limits; side-views can
hardly be captured and it fails, especially, for back-views, as required inputs
like facial landmarks or human parsing masks are missing. This results in
incomplete avatar reconstructions that only cover the frontal hemisphere. In
contrast to this, we propose a template-based tracking of the torso, head and
facial expressions which allows us to cover the appearance of a human subject
from all sides. Thus, given a sequence of a subject that is rotating in front
of a single camera, we train a neural volumetric representation based on neural
radiance fields. A key challenge to construct this representation is the
modeling of appearance changes, especially, in the mouth region (i.e., lips and
teeth). We, therefore, propose a deformation-field-based blend basis which
allows us to interpolate between different appearance states. We evaluate our
approach on captured real-world data and compare against state-of-the-art
monocular reconstruction methods. In contrast to those, our method is the first
monocular technique that reconstructs an entire 360{\deg} avatar.";Jalees Nehvi<author:sep>Berna Kabadayi<author:sep>Julien Valentin<author:sep>Justus Thies;http://arxiv.org/pdf/2312.05311v1;cs.CV;Project page: https://jalees018.github.io/3VP-Avatar/;
2312.04820v1;http://arxiv.org/abs/2312.04820v1;2023-12-08;Learn to Optimize Denoising Scores for 3D Generation: A Unified and  Improved Diffusion Prior on NeRF and 3D Gaussian Splatting;"We propose a unified framework aimed at enhancing the diffusion priors for 3D
generation tasks. Despite the critical importance of these tasks, existing
methodologies often struggle to generate high-caliber results. We begin by
examining the inherent limitations in previous diffusion priors. We identify a
divergence between the diffusion priors and the training procedures of
diffusion models that substantially impairs the quality of 3D generation. To
address this issue, we propose a novel, unified framework that iteratively
optimizes both the 3D model and the diffusion prior. Leveraging the different
learnable parameters of the diffusion prior, our approach offers multiple
configurations, affording various trade-offs between performance and
implementation complexity. Notably, our experimental results demonstrate that
our method markedly surpasses existing techniques, establishing new
state-of-the-art in the realm of text-to-3D generation. Furthermore, our
approach exhibits impressive performance on both NeRF and the newly introduced
3D Gaussian Splatting backbones. Additionally, our framework yields insightful
contributions to the understanding of recent score distillation methods, such
as the VSD and DDS loss.";Xiaofeng Yang<author:sep>Yiwen Chen<author:sep>Cheng Chen<author:sep>Chi Zhang<author:sep>Yi Xu<author:sep>Xulei Yang<author:sep>Fayao Liu<author:sep>Guosheng Lin;http://arxiv.org/pdf/2312.04820v1;cs.CV;;gaussian splatting<tag:sep>nerf
2312.05239v1;http://arxiv.org/abs/2312.05239v1;2023-12-08;SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational  Score Distillation;"Despite their ability to generate high-resolution and diverse images from
text prompts, text-to-image diffusion models often suffer from slow iterative
sampling processes. Model distillation is one of the most effective directions
to accelerate these models. However, previous distillation methods fail to
retain the generation quality while requiring a significant amount of images
for training, either from real data or synthetically generated by the teacher
model. In response to this limitation, we present a novel image-free
distillation scheme named $\textbf{SwiftBrush}$. Drawing inspiration from
text-to-3D synthesis, in which a 3D neural radiance field that aligns with the
input prompt can be obtained from a 2D text-to-image diffusion prior via a
specialized loss without the use of any 3D data ground-truth, our approach
re-purposes that same loss for distilling a pretrained multi-step text-to-image
model to a student network that can generate high-fidelity images with just a
single inference step. In spite of its simplicity, our model stands as one of
the first one-step text-to-image generators that can produce images of
comparable quality to Stable Diffusion without reliance on any training image
data. Remarkably, SwiftBrush achieves an FID score of $\textbf{16.67}$ and a
CLIP score of $\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive
results or even substantially surpassing existing state-of-the-art distillation
techniques.";Thuan Hoang Nguyen<author:sep>Anh Tran;http://arxiv.org/pdf/2312.05239v1;cs.CV;Project Page: https://thuanz123.github.io/swiftbrush/;
2312.04784v1;http://arxiv.org/abs/2312.04784v1;2023-12-08;Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular  Video;"Recent advancements in 3D avatar generation excel with multi-view supervision
for photorealistic models. However, monocular counterparts lag in quality
despite broader applicability. We propose ReCaLab to close this gap. ReCaLab is
a fully-differentiable pipeline that learns high-fidelity 3D human avatars from
just a single RGB video. A pose-conditioned deformable NeRF is optimized to
volumetrically represent a human subject in canonical T-pose. The canonical
representation is then leveraged to efficiently associate viewpoint-agnostic
textures using 2D-3D correspondences. This enables to separately generate
albedo and shading which jointly compose an RGB prediction. The design allows
to control intermediate results for human pose, body shape, texture, and
lighting with text prompts. An image-conditioned diffusion model thereby helps
to animate appearance and pose of the 3D avatar to create video sequences with
previously unseen human motion. Extensive experiments show that ReCaLab
outperforms previous monocular approaches in terms of image quality for image
synthesis tasks. ReCaLab even outperforms multi-view methods that leverage up
to 19x more synchronized videos for the task of novel pose rendering. Moreover,
natural language offers an intuitive user interface for creative manipulation
of 3D human avatars.";Yuchen Rao<author:sep>Eduardo Perez Pellitero<author:sep>Benjamin Busam<author:sep>Yiren Zhou<author:sep>Jifei Song;http://arxiv.org/pdf/2312.04784v1;cs.CV;Video link: https://youtu.be/Oz83z1es2J4;nerf
2312.05161v1;http://arxiv.org/abs/2312.05161v1;2023-12-08;TriHuman : A Real-time and Controllable Tri-plane Representation for  Detailed Human Geometry and Appearance Synthesis;"Creating controllable, photorealistic, and geometrically detailed digital
doubles of real humans solely from video data is a key challenge in Computer
Graphics and Vision, especially when real-time performance is required. Recent
methods attach a neural radiance field (NeRF) to an articulated structure,
e.g., a body model or a skeleton, to map points into a pose canonical space
while conditioning the NeRF on the skeletal pose. These approaches typically
parameterize the neural field with a multi-layer perceptron (MLP) leading to a
slow runtime. To address this drawback, we propose TriHuman a novel
human-tailored, deformable, and efficient tri-plane representation, which
achieves real-time performance, state-of-the-art pose-controllable geometry
synthesis as well as photorealistic rendering quality. At the core, we
non-rigidly warp global ray samples into our undeformed tri-plane texture
space, which effectively addresses the problem of global points being mapped to
the same tri-plane locations. We then show how such a tri-plane feature
representation can be conditioned on the skeletal motion to account for dynamic
appearance and geometry changes. Our results demonstrate a clear step towards
higher quality in terms of geometry and appearance modeling of humans as well
as runtime performance.";Heming Zhu<author:sep>Fangneng Zhan<author:sep>Christian Theobalt<author:sep>Marc Habermann;http://arxiv.org/pdf/2312.05161v1;cs.CV;;nerf
2312.04565v1;http://arxiv.org/abs/2312.04565v1;2023-12-07;MuRF: Multi-Baseline Radiance Fields;"We present Multi-Baseline Radiance Fields (MuRF), a general feed-forward
approach to solving sparse view synthesis under multiple different baseline
settings (small and large baselines, and different number of input views). To
render a target novel view, we discretize the 3D space into planes parallel to
the target image plane, and accordingly construct a target view frustum volume.
Such a target volume representation is spatially aligned with the target view,
which effectively aggregates relevant information from the input views for
high-quality rendering. It also facilitates subsequent radiance field
regression with a convolutional network thanks to its axis-aligned nature. The
3D context modeled by the convolutional network enables our method to synthesis
sharper scene structures than prior works. Our MuRF achieves state-of-the-art
performance across multiple different baseline settings and diverse scenarios
ranging from simple objects (DTU) to complex indoor and outdoor scenes
(RealEstate10K and LLFF). We also show promising zero-shot generalization
abilities on the Mip-NeRF 360 dataset, demonstrating the general applicability
of MuRF.";Haofei Xu<author:sep>Anpei Chen<author:sep>Yuedong Chen<author:sep>Christos Sakaridis<author:sep>Yulun Zhang<author:sep>Marc Pollefeys<author:sep>Andreas Geiger<author:sep>Fisher Yu;http://arxiv.org/pdf/2312.04565v1;cs.CV;Project page: https://haofeixu.github.io/murf/;nerf
2312.04558v1;http://arxiv.org/abs/2312.04558v1;2023-12-07;MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar;"The ability to animate photo-realistic head avatars reconstructed from
monocular portrait video sequences represents a crucial step in bridging the
gap between the virtual and real worlds. Recent advancements in head avatar
techniques, including explicit 3D morphable meshes (3DMM), point clouds, and
neural implicit representation have been exploited for this ongoing research.
However, 3DMM-based methods are constrained by their fixed topologies,
point-based approaches suffer from a heavy training burden due to the extensive
quantity of points involved, and the last ones suffer from limitations in
deformation flexibility and rendering efficiency. In response to these
challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head
Avatar), a novel approach that harnesses 3D Gaussian point representation
coupled with a Gaussian deformation field to learn explicit head avatars from
monocular portrait videos. We define our head avatars with Gaussian points
characterized by adaptable shapes, enabling flexible topology. These points
exhibit movement with a Gaussian deformation field in alignment with the target
pose and expression of a person, facilitating efficient deformation.
Additionally, the Gaussian points have controllable shape, size, color, and
opacity combined with Gaussian splatting, allowing for efficient training and
rendering. Experiments demonstrate the superior performance of our method,
which achieves state-of-the-art results among previous methods.";Yufan Chen<author:sep>Lizhen Wang<author:sep>Qijing Li<author:sep>Hongjiang Xiao<author:sep>Shengping Zhang<author:sep>Hongxun Yao<author:sep>Yebin Liu;http://arxiv.org/pdf/2312.04558v1;cs.CV;"The link to our projectpage is
  https://yufan1012.github.io/MonoGaussianAvatar";gaussian splatting
2312.04106v1;http://arxiv.org/abs/2312.04106v1;2023-12-07;Identity-Obscured Neural Radiance Fields: Privacy-Preserving 3D Facial  Reconstruction;"Neural radiance fields (NeRF) typically require a complete set of images
taken from multiple camera perspectives to accurately reconstruct geometric
details. However, this approach raise significant privacy concerns in the
context of facial reconstruction. The critical need for privacy protection
often leads invidividuals to be reluctant in sharing their facial images, due
to fears of potential misuse or security risks. Addressing these concerns, we
propose a method that leverages privacy-preserving images for reconstructing 3D
head geometry within the NeRF framework. Our method stands apart from
traditional facial reconstruction techniques as it does not depend on RGB
information from images containing sensitive facial data. Instead, it
effectively generates plausible facial geometry using a series of
identity-obscured inputs, thereby protecting facial privacy.";Jiayi Kong<author:sep>Baixin Xu<author:sep>Xurui Song<author:sep>Chen Qian<author:sep>Jun Luo<author:sep>Ying He;http://arxiv.org/pdf/2312.04106v1;cs.CV;;nerf
2312.04337v1;http://arxiv.org/abs/2312.04337v1;2023-12-07;Multi-View Unsupervised Image Generation with Cross Attention Guidance;"The growing interest in novel view synthesis, driven by Neural Radiance Field
(NeRF) models, is hindered by scalability issues due to their reliance on
precisely annotated multi-view images. Recent models address this by
fine-tuning large text2image diffusion models on synthetic multi-view data.
Despite robust zero-shot generalization, they may need post-processing and can
face quality issues due to the synthetic-real domain gap. This paper introduces
a novel pipeline for unsupervised training of a pose-conditioned diffusion
model on single-category datasets. With the help of pretrained self-supervised
Vision Transformers (DINOv2), we identify object poses by clustering the
dataset through comparing visibility and locations of specific object parts.
The pose-conditioned diffusion model, trained on pose labels, and equipped with
cross-frame attention at inference time ensures cross-view consistency, that is
further aided by our novel hard-attention guidance. Our model, MIRAGE,
surpasses prior work in novel view synthesis on real images. Furthermore,
MIRAGE is robust to diverse textures and geometries, as demonstrated with our
experiments on synthetic images generated with pretrained Stable Diffusion.";Llukman Cerkezi<author:sep>Aram Davtyan<author:sep>Sepehr Sameni<author:sep>Paolo Favaro;http://arxiv.org/pdf/2312.04337v1;cs.CV;;nerf
2312.04654v1;http://arxiv.org/abs/2312.04654v1;2023-12-07;NeuSD: Surface Completion with Multi-View Text-to-Image Diffusion;"We present a novel method for 3D surface reconstruction from multiple images
where only a part of the object of interest is captured. Our approach builds on
two recent developments: surface reconstruction using neural radiance fields
for the reconstruction of the visible parts of the surface, and guidance of
pre-trained 2D diffusion models in the form of Score Distillation Sampling
(SDS) to complete the shape in unobserved regions in a plausible manner. We
introduce three components. First, we suggest employing normal maps as a pure
geometric representation for SDS instead of color renderings which are
entangled with the appearance information. Second, we introduce the freezing of
the SDS noise during training which results in more coherent gradients and
better convergence. Third, we propose Multi-View SDS as a way to condition the
generation of the non-observable part of the surface without fine-tuning or
making changes to the underlying 2D Stable Diffusion model. We evaluate our
approach on the BlendedMVS dataset demonstrating significant qualitative and
quantitative improvements over competing methods.";Savva Ignatyev<author:sep>Daniil Selikhanovych<author:sep>Oleg Voynov<author:sep>Yiqun Wang<author:sep>Peter Wonka<author:sep>Stamatios Lefkimmiatis<author:sep>Evgeny Burnaev;http://arxiv.org/pdf/2312.04654v1;cs.CV;;
2312.04564v1;http://arxiv.org/abs/2312.04564v1;2023-12-07;EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS;"Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view
scene synthesis. It addresses the challenges of lengthy training times and slow
rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid,
differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time
rendering and accelerated training. They, however, demand substantial memory
resources for both training and storage, as they require millions of Gaussians
in their point cloud representation for each scene. We present a technique
utilizing quantized embeddings to significantly reduce memory storage
requirements and a coarse-to-fine training strategy for a faster and more
stable optimization of the Gaussian point clouds. Our approach results in scene
representations with fewer Gaussians and quantized representations, leading to
faster training times and rendering speeds for real-time rendering of high
resolution scenes. We reduce memory by more than an order of magnitude all
while maintaining the reconstruction quality. We validate the effectiveness of
our approach on a variety of datasets and scenes preserving the visual quality
while consuming 10-20x less memory and faster training/inference speed. Project
page and code is available https://efficientgaussian.github.io";Sharath Girish<author:sep>Kamal Gupta<author:sep>Abhinav Shrivastava;http://arxiv.org/pdf/2312.04564v1;cs.CV;"Website: https://efficientgaussian.github.io Code:
  https://github.com/Sharath-girish/efficientgaussian";gaussian splatting<tag:sep>nerf
2312.04527v1;http://arxiv.org/abs/2312.04527v1;2023-12-07;Correspondences of the Third Kind: Camera Pose Estimation from Object  Reflection;"Computer vision has long relied on two kinds of correspondences: pixel
correspondences in images and 3D correspondences on object surfaces. Is there
another kind, and if there is, what can they do for us? In this paper, we
introduce correspondences of the third kind we call reflection correspondences
and show that they can help estimate camera pose by just looking at objects
without relying on the background. Reflection correspondences are point
correspondences in the reflected world, i.e., the scene reflected by the object
surface. The object geometry and reflectance alters the scene geometrically and
radiometrically, respectively, causing incorrect pixel correspondences.
Geometry recovered from each image is also hampered by distortions, namely
generalized bas-relief ambiguity, leading to erroneous 3D correspondences. We
show that reflection correspondences can resolve the ambiguities arising from
these distortions. We introduce a neural correspondence estimator and a RANSAC
algorithm that fully leverages all three kinds of correspondences for robust
and accurate joint camera pose and object shape estimation just from the object
appearance. The method expands the horizon of numerous downstream tasks,
including camera pose estimation for appearance modeling (e.g., NeRF) and
motion estimation of reflective objects (e.g., cars on the road), to name a
few, as it relieves the requirement of overlapping background.";Kohei Yamashita<author:sep>Vincent Lepetit<author:sep>Ko Nishino;http://arxiv.org/pdf/2312.04527v1;cs.CV;;nerf
2312.04651v1;http://arxiv.org/abs/2312.04651v1;2023-12-07;VOODOO 3D: Volumetric Portrait Disentanglement for One-Shot 3D Head  Reenactment;"We present a 3D-aware one-shot head reenactment method based on a fully
volumetric neural disentanglement framework for source appearance and driver
expressions. Our method is real-time and produces high-fidelity and
view-consistent output, suitable for 3D teleconferencing systems based on
holographic displays. Existing cutting-edge 3D-aware reenactment methods often
use neural radiance fields or 3D meshes to produce view-consistent appearance
encoding, but, at the same time, they rely on linear face models, such as 3DMM,
to achieve its disentanglement with facial expressions. As a result, their
reenactment results often exhibit identity leakage from the driver or have
unnatural expressions. To address these problems, we propose a neural
self-supervised disentanglement approach that lifts both the source image and
driver video frame into a shared 3D volumetric representation based on
tri-planes. This representation can then be freely manipulated with expression
tri-planes extracted from the driving images and rendered from an arbitrary
view using neural radiance fields. We achieve this disentanglement via
self-supervised learning on a large in-the-wild video dataset. We further
introduce a highly effective fine-tuning approach to improve the
generalizability of the 3D lifting using the same real-world data. We
demonstrate state-of-the-art performance on a wide range of datasets, and also
showcase high-quality 3D-aware head reenactment on highly challenging and
diverse subjects, including non-frontal head poses and complex expressions for
both source and driver.";Phong Tran<author:sep>Egor Zakharov<author:sep>Long-Nhat Ho<author:sep>Anh Tuan Tran<author:sep>Liwen Hu<author:sep>Hao Li;http://arxiv.org/pdf/2312.04651v1;cs.CV;;
2312.04143v1;http://arxiv.org/abs/2312.04143v1;2023-12-07;Towards 4D Human Video Stylization;"We present a first step towards 4D (3D and time) human video stylization,
which addresses style transfer, novel view synthesis and human animation within
a unified framework. While numerous video stylization methods have been
developed, they are often restricted to rendering images in specific viewpoints
of the input video, lacking the capability to generalize to novel views and
novel poses in dynamic scenes. To overcome these limitations, we leverage
Neural Radiance Fields (NeRFs) to represent videos, conducting stylization in
the rendered feature space. Our innovative approach involves the simultaneous
representation of both the human subject and the surrounding scene using two
NeRFs. This dual representation facilitates the animation of human subjects
across various poses and novel viewpoints. Specifically, we introduce a novel
geometry-guided tri-plane representation, significantly enhancing feature
representation robustness compared to direct tri-plane optimization. Following
the video reconstruction, stylization is performed within the NeRFs' rendered
feature space. Extensive experiments demonstrate that the proposed method
strikes a superior balance between stylized textures and temporal coherence,
surpassing existing approaches. Furthermore, our framework uniquely extends its
capabilities to accommodate novel poses and viewpoints, making it a versatile
tool for creative human video stylization.";Tiantian Wang<author:sep>Xinxin Zuo<author:sep>Fangzhou Mu<author:sep>Jian Wang<author:sep>Ming-Hsuan Yang;http://arxiv.org/pdf/2312.04143v1;cs.CV;Under Review;nerf
2312.03357v1;http://arxiv.org/abs/2312.03357v1;2023-12-06;RING-NeRF: A Versatile Architecture based on Residual Implicit Neural  Grids;"Since their introduction, Neural Fields have become very popular for 3D
reconstruction and new view synthesis. Recent researches focused on
accelerating the process, as well as improving the robustness to variation of
the observation distance and limited number of supervised viewpoints. However,
those approaches often led to dedicated solutions that cannot be easily
combined. To tackle this issue, we introduce a new simple but efficient
architecture named RING-NeRF, based on Residual Implicit Neural Grids, that
provides a control on the level of detail of the mapping function between the
scene and the latent spaces. Associated with a distance-aware forward mapping
mechanism and a continuous coarse-to-fine reconstruction process, our versatile
architecture demonstrates both fast training and state-of-the-art performances
in terms of: (1) anti-aliased rendering, (2) reconstruction quality from few
supervised viewpoints, and (3) robustness in the absence of appropriate
scene-specific initialization for SDF-based NeRFs. We also demonstrate that our
architecture can dynamically add grids to increase the details of the
reconstruction, opening the way to adaptive reconstruction.";Doriand Petit<author:sep>Steve Bourgeois<author:sep>Dumitru Pavel<author:sep>Vincent Gay-Bellile<author:sep>Florian Chabot<author:sep>Loic Barthe;http://arxiv.org/pdf/2312.03357v1;cs.CV;;nerf
2312.03431v1;http://arxiv.org/abs/2312.03431v1;2023-12-06;Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle;"We introduce Gaussian-Flow, a novel point-based approach for fast dynamic
scene reconstruction and real-time rendering from both multi-view and monocular
videos. In contrast to the prevalent NeRF-based approaches hampered by slow
training and rendering speeds, our approach harnesses recent advancements in
point-based 3D Gaussian Splatting (3DGS). Specifically, a novel Dual-Domain
Deformation Model (DDDM) is proposed to explicitly model attribute deformations
of each Gaussian point, where the time-dependent residual of each attribute is
captured by a polynomial fitting in the time domain, and a Fourier series
fitting in the frequency domain. The proposed DDDM is capable of modeling
complex scene deformations across long video footage, eliminating the need for
training separate 3DGS for each frame or introducing an additional implicit
neural field to model 3D dynamics. Moreover, the explicit deformation modeling
for discretized Gaussian points ensures ultra-fast training and rendering of a
4D scene, which is comparable to the original 3DGS designed for static 3D
reconstruction. Our proposed approach showcases a substantial efficiency
improvement, achieving a $5\times$ faster training speed compared to the
per-frame 3DGS modeling. In addition, quantitative results demonstrate that the
proposed Gaussian-Flow significantly outperforms previous leading methods in
novel view rendering quality. Project page:
https://nju-3dv.github.io/projects/Gaussian-Flow";Youtian Lin<author:sep>Zuozhuo Dai<author:sep>Siyu Zhu<author:sep>Yao Yao;http://arxiv.org/pdf/2312.03431v1;cs.CV;;gaussian splatting<tag:sep>nerf
2312.03266v1;http://arxiv.org/abs/2312.03266v1;2023-12-06;SO-NeRF: Active View Planning for NeRF using Surrogate Objectives;"Despite the great success of Neural Radiance Fields (NeRF), its
data-gathering process remains vague with only a general rule of thumb of
sampling as densely as possible. The lack of understanding of what actually
constitutes good views for NeRF makes it difficult to actively plan a sequence
of views that yield the maximal reconstruction quality. We propose Surrogate
Objectives for Active Radiance Fields (SOAR), which is a set of interpretable
functions that evaluates the goodness of views using geometric and photometric
visual cues - surface coverage, geometric complexity, textural complexity, and
ray diversity. Moreover, by learning to infer the SOAR scores from a deep
network, SOARNet, we are able to effectively select views in mere seconds
instead of hours, without the need for prior visits to all the candidate views
or training any radiance field during such planning. Our experiments show
SOARNet outperforms the baselines with $\sim$80x speed-up while achieving
better or comparable reconstruction qualities. We finally show that SOAR is
model-agnostic, thus it generalizes across fully neural-implicit to fully
explicit approaches.";Keifer Lee<author:sep>Shubham Gupta<author:sep>Sunglyoung Kim<author:sep>Bhargav Makwana<author:sep>Chao Chen<author:sep>Chen Feng;http://arxiv.org/pdf/2312.03266v1;cs.CV;13 pages;nerf
2312.03461v2;http://arxiv.org/abs/2312.03461v2;2023-12-06;HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian  Splatting;"We have recently seen tremendous progress in photo-real human modeling and
rendering. Yet, efficiently rendering realistic human performance and
integrating it into the rasterization pipeline remains challenging. In this
paper, we present HiFi4G, an explicit and compact Gaussian-based approach for
high-fidelity human performance rendering from dense footage. Our core
intuition is to marry the 3D Gaussian representation with non-rigid tracking,
achieving a compact and compression-friendly representation. We first propose a
dual-graph mechanism to obtain motion priors, with a coarse deformation graph
for effective initialization and a fine-grained Gaussian graph to enforce
subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with
adaptive spatial-temporal regularizers to effectively balance the non-rigid
prior and Gaussian updating. We also present a companion compression scheme
with residual compensation for immersive experiences on various platforms. It
achieves a substantial compression rate of approximately 25 times, with less
than 2MB of storage per frame. Extensive experiments demonstrate the
effectiveness of our approach, which significantly outperforms existing
approaches in terms of optimization speed, rendering quality, and storage
overhead.";Yuheng Jiang<author:sep>Zhehao Shen<author:sep>Penghao Wang<author:sep>Zhuo Su<author:sep>Yu Hong<author:sep>Yingliang Zhang<author:sep>Jingyi Yu<author:sep>Lan Xu;http://arxiv.org/pdf/2312.03461v2;cs.CV;;
2312.10070v1;http://arxiv.org/abs/2312.10070v1;2023-12-06;Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting;"We present a new dense simultaneous localization and mapping (SLAM) method
that uses Gaussian splats as a scene representation. The new representation
enables interactive-time reconstruction and photo-realistic rendering of
real-world and synthetic scenes. We propose novel strategies for seeding and
optimizing Gaussian splats to extend their use from multiview offline scenarios
to sequential monocular RGBD input data setups. In addition, we extend Gaussian
splats to encode geometry and experiment with tracking against this scene
representation. Our method achieves state-of-the-art rendering quality on both
real-world and synthetic datasets while being competitive in reconstruction
performance and runtime.";Vladimir Yugay<author:sep>Yue Li<author:sep>Theo Gevers<author:sep>Martin R. Oswald;http://arxiv.org/pdf/2312.10070v1;cs.CV;;gaussian splatting
2312.03372v1;http://arxiv.org/abs/2312.03372v1;2023-12-06;Evaluating the point cloud of individual trees generated from images  based on Neural Radiance fields (NeRF) method;"Three-dimensional (3D) reconstruction of trees has always been a key task in
precision forestry management and research. Due to the complex branch
morphological structure of trees themselves and the occlusions from tree stems,
branches and foliage, it is difficult to recreate a complete three-dimensional
tree model from a two-dimensional image by conventional photogrammetric
methods. In this study, based on tree images collected by various cameras in
different ways, the Neural Radiance Fields (NeRF) method was used for
individual tree reconstruction and the exported point cloud models are compared
with point cloud derived from photogrammetric reconstruction and laser scanning
methods. The results show that the NeRF method performs well in individual tree
3D reconstruction, as it has higher successful reconstruction rate, better
reconstruction in the canopy area, it requires less amount of images as input.
Compared with photogrammetric reconstruction method, NeRF has significant
advantages in reconstruction efficiency and is adaptable to complex scenes, but
the generated point cloud tends to be noisy and low resolution. The accuracy of
tree structural parameters (tree height and diameter at breast height)
extracted from the photogrammetric point cloud is still higher than those of
derived from the NeRF point cloud. The results of this study illustrate the
great potential of NeRF method for individual tree reconstruction, and it
provides new ideas and research directions for 3D reconstruction and
visualization of complex forest scenes.";Hongyu Huang<author:sep>Guoji Tian<author:sep>Chongcheng Chen;http://arxiv.org/pdf/2312.03372v1;cs.CV;"25 pages; 6 figures";nerf
2312.03203v1;http://arxiv.org/abs/2312.03203v1;2023-12-06;Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled  Feature Fields;"3D scene representations have gained immense popularity in recent years.
Methods that use Neural Radiance fields are versatile for traditional tasks
such as novel view synthesis. In recent times, some work has emerged that aims
to extend the functionality of NeRF beyond view synthesis, for semantically
aware tasks such as editing and segmentation using 3D feature field
distillation from 2D foundation models. However, these methods have two major
limitations: (a) they are limited by the rendering speed of NeRF pipelines, and
(b) implicitly represented feature fields suffer from continuity artifacts
reducing feature quality. Recently, 3D Gaussian Splatting has shown
state-of-the-art performance on real-time radiance field rendering. In this
work, we go one step further: in addition to radiance field rendering, we
enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D
foundation model distillation. This translation is not straightforward: naively
incorporating feature fields in the 3DGS framework leads to warp-level
divergence. We propose architectural and training changes to efficiently avert
this problem. Our proposed method is general, and our experiments showcase
novel view semantic segmentation, language-guided editing and segment anything
through learning feature fields from state-of-the-art 2D foundation models such
as SAM and CLIP-LSeg. Across experiments, our distillation method is able to
provide comparable or better results, while being significantly faster to both
train and render. Additionally, to the best of our knowledge, we are the first
method to enable point and bounding-box prompting for radiance field
manipulation, by leveraging the SAM model. Project website at:
https://feature-3dgs.github.io/";Shijie Zhou<author:sep>Haoran Chang<author:sep>Sicheng Jiang<author:sep>Zhiwen Fan<author:sep>Zehao Zhu<author:sep>Dejia Xu<author:sep>Pradyumna Chari<author:sep>Suya You<author:sep>Zhangyang Wang<author:sep>Achuta Kadambi;http://arxiv.org/pdf/2312.03203v1;cs.CV;;gaussian splatting<tag:sep>nerf
2312.03869v1;http://arxiv.org/abs/2312.03869v1;2023-12-06;Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion;"This paper presents a novel approach to inpainting 3D regions of a scene,
given masked multi-view images, by distilling a 2D diffusion model into a
learned 3D scene representation (e.g. a NeRF). Unlike 3D generative methods
that explicitly condition the diffusion model on camera pose or multi-view
information, our diffusion model is conditioned only on a single masked 2D
image. Nevertheless, we show that this 2D diffusion model can still serve as a
generative prior in a 3D multi-view reconstruction problem where we optimize a
NeRF using a combination of score distillation sampling and NeRF reconstruction
losses. Predicted depth is used as additional supervision to encourage accurate
geometry. We compare our approach to 3D inpainting methods that focus on object
removal. Because our method can generate content to fill any 3D masked region,
we additionally demonstrate 3D object completion, 3D object replacement, and 3D
scene completion.";Kira Prabhu<author:sep>Jane Wu<author:sep>Lynn Tsai<author:sep>Peter Hedman<author:sep>Dan B Goldman<author:sep>Ben Poole<author:sep>Michael Broxton;http://arxiv.org/pdf/2312.03869v1;cs.CV;;nerf
2312.03420v1;http://arxiv.org/abs/2312.03420v1;2023-12-06;Artist-Friendly Relightable and Animatable Neural Heads;"An increasingly common approach for creating photo-realistic digital avatars
is through the use of volumetric neural fields. The original neural radiance
field (NeRF) allowed for impressive novel view synthesis of static heads when
trained on a set of multi-view images, and follow up methods showed that these
neural representations can be extended to dynamic avatars. Recently, new
variants also surpassed the usual drawback of baked-in illumination in neural
representations, showing that static neural avatars can be relit in any
environment. In this work we simultaneously tackle both the motion and
illumination problem, proposing a new method for relightable and animatable
neural heads. Our method builds on a proven dynamic avatar approach based on a
mixture of volumetric primitives, combined with a recently-proposed lightweight
hardware setup for relightable neural fields, and includes a novel architecture
that allows relighting dynamic neural avatars performing unseen expressions in
any environment, even with nearfield illumination and viewpoints.";Yingyan Xu<author:sep>Prashanth Chandran<author:sep>Sebastian Weiss<author:sep>Markus Gross<author:sep>Gaspard Zoss<author:sep>Derek Bradley;http://arxiv.org/pdf/2312.03420v1;cs.CV;;nerf
2312.02751v2;http://arxiv.org/abs/2312.02751v2;2023-12-05;C-NERF: Representing Scene Changes as Directional Consistency  Difference-based NeRF;"In this work, we aim to detect the changes caused by object variations in a
scene represented by the neural radiance fields (NeRFs). Given an arbitrary
view and two sets of scene images captured at different timestamps, we can
predict the scene changes in that view, which has significant potential
applications in scene monitoring and measuring. We conducted preliminary
studies and found that such an exciting task cannot be easily achieved by
utilizing existing NeRFs and 2D change detection methods with many false or
missing detections. The main reason is that the 2D change detection is based on
the pixel appearance difference between spatial-aligned image pairs and
neglects the stereo information in the NeRF. To address the limitations, we
propose the C-NERF to represent scene changes as directional consistency
difference-based NeRF, which mainly contains three modules. We first perform
the spatial alignment of two NeRFs captured before and after changes. Then, we
identify the change points based on the direction-consistent constraint; that
is, real change points have similar change representations across view
directions, but fake change points do not. Finally, we design the change map
rendering process based on the built NeRFs and can generate the change map of
an arbitrarily specified view direction. To validate the effectiveness, we
build a new dataset containing ten scenes covering diverse scenarios with
different changing objects. Our approach surpasses state-of-the-art 2D change
detection and NeRF-based methods by a significant margin.";Rui Huang<author:sep>Binbin Jiang<author:sep>Qingyi Zhao<author:sep>William Wang<author:sep>Yuxiang Zhang<author:sep>Qing Guo;http://arxiv.org/pdf/2312.02751v2;cs.CV;;nerf
2312.02981v1;http://arxiv.org/abs/2312.02981v1;2023-12-05;ReconFusion: 3D Reconstruction with Diffusion Priors;"3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at
rendering photorealistic novel views of complex scenes. However, recovering a
high-quality NeRF typically requires tens to hundreds of input images,
resulting in a time-consuming capture process. We present ReconFusion to
reconstruct real-world scenes using only a few photos. Our approach leverages a
diffusion prior for novel view synthesis, trained on synthetic and multiview
datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel
camera poses beyond those captured by the set of input images. Our method
synthesizes realistic geometry and texture in underconstrained regions while
preserving the appearance of observed regions. We perform an extensive
evaluation across various real-world datasets, including forward-facing and
360-degree scenes, demonstrating significant performance improvements over
previous few-view NeRF reconstruction approaches.";Rundi Wu<author:sep>Ben Mildenhall<author:sep>Philipp Henzler<author:sep>Keunhong Park<author:sep>Ruiqi Gao<author:sep>Daniel Watson<author:sep>Pratul P. Srinivasan<author:sep>Dor Verbin<author:sep>Jonathan T. Barron<author:sep>Ben Poole<author:sep>Aleksander Holynski;http://arxiv.org/pdf/2312.02981v1;cs.CV;Project page: https://reconfusion.github.io/;nerf
2312.03160v1;http://arxiv.org/abs/2312.03160v1;2023-12-05;HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces;"Neural radiance fields provide state-of-the-art view synthesis quality but
tend to be slow to render. One reason is that they make use of volume
rendering, thus requiring many samples (and model queries) per ray at render
time. Although this representation is flexible and easy to optimize, most
real-world objects can be modeled more efficiently with surfaces instead of
volumes, requiring far fewer samples per ray. This observation has spurred
considerable progress in surface representations such as signed distance
functions, but these may struggle to model semi-opaque and thin structures. We
propose a method, HybridNeRF, that leverages the strengths of both
representations by rendering most objects as surfaces while modeling the
(typically) small fraction of challenging regions volumetrically. We evaluate
HybridNeRF against the challenging Eyeful Tower dataset along with other
commonly used view synthesis datasets. When comparing to state-of-the-art
baselines, including recent rasterization-based approaches, we improve error
rates by 15-30% while achieving real-time framerates (at least 36 FPS) for
virtual-reality resolutions (2Kx2K).";Haithem Turki<author:sep>Vasu Agrawal<author:sep>Samuel Rota BulÃ²<author:sep>Lorenzo Porzi<author:sep>Peter Kontschieder<author:sep>Deva Ramanan<author:sep>Michael ZollhÃ¶fer<author:sep>Christian Richardt;http://arxiv.org/pdf/2312.03160v1;cs.CV;Project page: https://haithemturki.com/hybrid-nerf/;nerf
2312.02963v1;http://arxiv.org/abs/2312.02963v1;2023-12-05;MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human  Captures;"In this era, the success of large language models and text-to-image models
can be attributed to the driving force of large-scale datasets. However, in the
realm of 3D vision, while remarkable progress has been made with models trained
on large-scale synthetic and real-captured object data like Objaverse and
MVImgNet, a similar level of progress has not been observed in the domain of
human-centric tasks partially due to the lack of a large-scale human dataset.
Existing datasets of high-fidelity 3D human capture continue to be mid-sized
due to the significant challenges in acquiring large-scale high-quality 3D
human data. To bridge this gap, we present MVHumanNet, a dataset that comprises
multi-view human action sequences of 4,500 human identities. The primary focus
of our work is on collecting human data that features a large number of diverse
identities and everyday clothing using a multi-view human capture system, which
facilitates easily scalable data collection. Our dataset contains 9,000 daily
outfits, 60,000 motion sequences and 645 million frames with extensive
annotations, including human masks, camera parameters, 2D and 3D keypoints,
SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the
potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot
studies on view-consistent action recognition, human NeRF reconstruction,
text-driven view-unconstrained human image generation, as well as 2D
view-unconstrained human image and 3D avatar generation. Extensive experiments
demonstrate the performance improvements and effective applications enabled by
the scale provided by MVHumanNet. As the current largest-scale 3D human
dataset, we hope that the release of MVHumanNet data with annotations will
foster further innovations in the domain of 3D human-centric tasks at scale.";Zhangyang Xiong<author:sep>Chenghong Li<author:sep>Kenkun Liu<author:sep>Hongjie Liao<author:sep>Jianqiao Hu<author:sep>Junyi Zhu<author:sep>Shuliang Ning<author:sep>Lingteng Qiu<author:sep>Chongjie Wang<author:sep>Shijie Wang<author:sep>Shuguang Cui<author:sep>Xiaoguang Han;http://arxiv.org/pdf/2312.02963v1;cs.CV;Project page: https://x-zhangyang.github.io/MVHumanNet/;nerf
2312.02970v1;http://arxiv.org/abs/2312.02970v1;2023-12-05;Alchemist: Parametric Control of Material Properties with Diffusion  Models;"We propose a method to control material attributes of objects like roughness,
metallic, albedo, and transparency in real images. Our method capitalizes on
the generative prior of text-to-image models known for photorealism, employing
a scalar value and instructions to alter low-level material properties.
Addressing the lack of datasets with controlled material attributes, we
generated an object-centric synthetic dataset with physically-based materials.
Fine-tuning a modified pre-trained text-to-image model on this synthetic
dataset enables us to edit material properties in real-world images while
preserving all other attributes. We show the potential application of our model
to material edited NeRFs.";Prafull Sharma<author:sep>Varun Jampani<author:sep>Yuanzhen Li<author:sep>Xuhui Jia<author:sep>Dmitry Lagun<author:sep>Fredo Durand<author:sep>William T. Freeman<author:sep>Mark Matthews;http://arxiv.org/pdf/2312.02970v1;cs.CV;;nerf
2312.02902v1;http://arxiv.org/abs/2312.02902v1;2023-12-05;HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting;"3D head animation has seen major quality and runtime improvements over the
last few years, particularly empowered by the advances in differentiable
rendering and neural radiance fields. Real-time rendering is a highly desirable
goal for real-world applications. We propose HeadGaS, the first model to use 3D
Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper
we introduce a hybrid model that extends the explicit representation from 3DGS
with a base of learnable latent features, which can be linearly blended with
low-dimensional parameters from parametric head models to obtain
expression-dependent final color and opacity values. We demonstrate that
HeadGaS delivers state-of-the-art results in real-time inference frame rates,
which surpasses baselines by up to ~2dB, while accelerating rendering speed by
over x10.";Helisa Dhamo<author:sep>Yinyu Nie<author:sep>Arthur Moreau<author:sep>Jifei Song<author:sep>Richard Shaw<author:sep>Yiren Zhou<author:sep>Eduardo PÃ©rez-Pellitero;http://arxiv.org/pdf/2312.02902v1;cs.CV;;gaussian splatting
2312.02568v1;http://arxiv.org/abs/2312.02568v1;2023-12-05;Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent;"This paper explores promptable NeRF generation (e.g., text prompt or single
image prompt) for direct conditioning and fast generation of NeRF parameters
for the underlying 3D scenes, thus undoing complex intermediate steps while
providing full 3D generation with conditional control. Unlike previous
diffusion-CLIP-based pipelines that involve tedious per-prompt optimizations,
Prompt2NeRF-PIL is capable of generating a variety of 3D objects with a single
forward pass, leveraging a pre-trained implicit latent space of NeRF
parameters. Furthermore, in zero-shot tasks, our experiments demonstrate that
the NeRFs produced by our method serve as semantically informative
initializations, significantly accelerating the inference process of existing
prompt-to-NeRF methods. Specifically, we will show that our approach speeds up
the text-to-NeRF model DreamFusion and the 3D reconstruction speed of the
image-to-NeRF method Zero-1-to-3 by 3 to 5 times.";Jianmeng Liu<author:sep>Yuyao Zhang<author:sep>Zeyuan Meng<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2312.02568v1;cs.CV;;nerf
2312.02434v1;http://arxiv.org/abs/2312.02434v1;2023-12-05;FINER: Flexible spectral-bias tuning in Implicit NEural Representation  by Variable-periodic Activation Functions;"Implicit Neural Representation (INR), which utilizes a neural network to map
coordinate inputs to corresponding attributes, is causing a revolution in the
field of signal processing. However, current INR techniques suffer from a
restricted capability to tune their supported frequency set, resulting in
imperfect performance when representing complex signals with multiple
frequencies. We have identified that this frequency-related problem can be
greatly alleviated by introducing variable-periodic activation functions, for
which we propose FINER. By initializing the bias of the neural network within
different ranges, sub-functions with various frequencies in the
variable-periodic function are selected for activation. Consequently, the
supported frequency set of FINER can be flexibly tuned, leading to improved
performance in signal representation. We demonstrate the capabilities of FINER
in the contexts of 2D image fitting, 3D signed distance field representation,
and 5D neural radiance fields optimization, and we show that it outperforms
existing INRs.";Zhen Liu<author:sep>Hao Zhu<author:sep>Qi Zhang<author:sep>Jingde Fu<author:sep>Weibing Deng<author:sep>Zhan Ma<author:sep>Yanwen Guo<author:sep>Xun Cao;http://arxiv.org/pdf/2312.02434v1;cs.CV;10 pages, 9 figures;
2312.02973v1;http://arxiv.org/abs/2312.02973v1;2023-12-05;GauHuman: Articulated Gaussian Splatting from Monocular Human Videos;"We present, GauHuman, a 3D human model with Gaussian Splatting for both fast
training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with
existing NeRF-based implicit representation modelling frameworks demanding
hours of training and seconds of rendering per frame. Specifically, GauHuman
encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians
from canonical space to posed space with linear blend skinning (LBS), in which
effective pose and LBS refinement modules are designed to learn fine details of
3D humans under negligible computational cost. Moreover, to enable fast
optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human
prior, while splitting/cloning via KL divergence guidance, along with a novel
merge operation for further speeding up. Extensive experiments on ZJU_Mocap and
MonoCap datasets demonstrate that GauHuman achieves state-of-the-art
performance quantitatively and qualitatively with fast training and real-time
rendering speed. Notably, without sacrificing rendering quality, GauHuman can
fast model the 3D human performer with ~13k 3D Gaussians.";Shoukang Hu<author:sep>Ziwei Liu;http://arxiv.org/pdf/2312.02973v1;cs.CV;"project page: https://skhu101.github.io/GauHuman/; code:
  https://github.com/skhu101/GauHuman";gaussian splatting<tag:sep>nerf
2312.14937v2;http://arxiv.org/abs/2312.14937v2;2023-12-04;SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes;"Novel view synthesis for dynamic scenes is still a challenging problem in
computer vision and graphics. Recently, Gaussian splatting has emerged as a
robust technique to represent static scenes and enable high-quality and
real-time novel view synthesis. Building upon this technique, we propose a new
representation that explicitly decomposes the motion and appearance of dynamic
scenes into sparse control points and dense Gaussians, respectively. Our key
idea is to use sparse control points, significantly fewer in number than the
Gaussians, to learn compact 6 DoF transformation bases, which can be locally
interpolated through learned interpolation weights to yield the motion field of
3D Gaussians. We employ a deformation MLP to predict time-varying 6 DoF
transformations for each control point, which reduces learning complexities,
enhances learning abilities, and facilitates obtaining temporal and spatial
coherent motion patterns. Then, we jointly learn the 3D Gaussians, the
canonical space locations of control points, and the deformation MLP to
reconstruct the appearance, geometry, and dynamics of 3D scenes. During
learning, the location and number of control points are adaptively adjusted to
accommodate varying motion complexities in different regions, and an ARAP loss
following the principle of as rigid as possible is developed to enforce spatial
continuity and local rigidity of learned motions. Finally, thanks to the
explicit sparse motion representation and its decomposition from appearance,
our method can enable user-controlled motion editing while retaining
high-fidelity appearances. Extensive experiments demonstrate that our approach
outperforms existing approaches on novel view synthesis with a high rendering
speed and enables novel appearance-preserved motion editing applications.
Project page: https://yihua7.github.io/SC-GS-web/";Yi-Hua Huang<author:sep>Yang-Tian Sun<author:sep>Ziyi Yang<author:sep>Xiaoyang Lyu<author:sep>Yan-Pei Cao<author:sep>Xiaojuan Qi;http://arxiv.org/pdf/2312.14937v2;cs.CV;Code link: https://github.com/yihua7/SC-GS;gaussian splatting
2312.02362v1;http://arxiv.org/abs/2312.02362v1;2023-12-04;PointNeRF++: A multi-scale, point-based Neural Radiance Field;"Point clouds offer an attractive source of information to complement images
in neural scene representations, especially when few images are available.
Neural rendering methods based on point clouds do exist, but they do not
perform well when the point cloud quality is low -- e.g., sparse or incomplete,
which is often the case with real-world data. We overcome these problems with a
simple representation that aggregates point clouds at multiple scale levels
with sparse voxel grids at different resolutions. To deal with point cloud
sparsity, we average across multiple scale levels -- but only among those that
are valid, i.e., that have enough neighboring points in proximity to the ray of
a pixel. To help model areas without points, we add a global voxel at the
coarsest scale, thus unifying ""classical"" and point-based NeRF formulations. We
validate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets,
outperforming the state of the art by a significant margin.";Weiwei Sun<author:sep>Eduard Trulls<author:sep>Yang-Che Tseng<author:sep>Sneha Sambandam<author:sep>Gopal Sharma<author:sep>Andrea Tagliasacchi<author:sep>Kwang Moo Yi;http://arxiv.org/pdf/2312.02362v1;cs.CV;;nerf
2312.02157v1;http://arxiv.org/abs/2312.02157v1;2023-12-04;Mesh-Guided Neural Implicit Field Editing;"Neural implicit fields have emerged as a powerful 3D representation for
reconstructing and rendering photo-realistic views, yet they possess limited
editability. Conversely, explicit 3D representations, such as polygonal meshes,
offer ease of editing but may not be as suitable for rendering high-quality
novel views. To harness the strengths of both representations, we propose a new
approach that employs a mesh as a guiding mechanism in editing the neural
radiance field. We first introduce a differentiable method using marching
tetrahedra for polygonal mesh extraction from the neural implicit field and
then design a differentiable color extractor to assign colors obtained from the
volume renderings to this extracted mesh. This differentiable colored mesh
allows gradient back-propagation from the explicit mesh to the implicit fields,
empowering users to easily manipulate the geometry and color of neural implicit
fields. To enhance user control from coarse-grained to fine-grained levels, we
introduce an octree-based structure into its optimization. This structure
prioritizes the edited regions and the surface part, making our method achieve
fine-grained edits to the neural implicit field and accommodate various user
modifications, including object additions, component removals, specific area
deformations, and adjustments to local and global colors. Through extensive
experiments involving diverse scenes and editing operations, we have
demonstrated the capabilities and effectiveness of our method. Our project page
is: \url{https://cassiepython.github.io/MNeuEdit/}";Can Wang<author:sep>Mingming He<author:sep>Menglei Chai<author:sep>Dongdong Chen<author:sep>Jing Liao;http://arxiv.org/pdf/2312.02157v1;cs.CV;Project page: https://cassiepython.github.io/MNeuEdit/;
2312.02255v1;http://arxiv.org/abs/2312.02255v1;2023-12-04;Re-Nerfing: Enforcing Geometric Constraints on Neural Radiance Fields  through Novel Views Synthesis;"Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis
capabilities even in large-scale, unbounded scenes, albeit requiring hundreds
of views or introducing artifacts in sparser settings. Their optimization
suffers from shape-radiance ambiguities wherever only a small visual overlap is
available. This leads to erroneous scene geometry and artifacts. In this paper,
we propose Re-Nerfing, a simple and general multi-stage approach that leverages
NeRF's own view synthesis to address these limitations. With Re-Nerfing, we
increase the scene's coverage and enhance the geometric consistency of novel
views as follows: First, we train a NeRF with the available views. Then, we use
the optimized NeRF to synthesize pseudo-views next to the original ones to
simulate a stereo or trifocal setup. Finally, we train a second NeRF with both
original and pseudo views while enforcing structural, epipolar constraints via
the newly synthesized images. Extensive experiments on the mip-NeRF 360 dataset
show the effectiveness of Re-Nerfing across denser and sparser input scenarios,
bringing improvements to the state-of-the-art Zip-NeRF, even when trained with
all views.";Felix Tristram<author:sep>Stefano Gasperini<author:sep>Federico Tombari<author:sep>Nassir Navab<author:sep>Benjamin Busam;http://arxiv.org/pdf/2312.02255v1;cs.CV;Code will be released upon acceptance;nerf
2312.02069v1;http://arxiv.org/abs/2312.02069v1;2023-12-04;GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians;"We introduce GaussianAvatars, a new method to create photorealistic head
avatars that are fully controllable in terms of expression, pose, and
viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian
splats that are rigged to a parametric morphable face model. This combination
facilitates photorealistic rendering while allowing for precise animation
control via the underlying parametric model, e.g., through expression transfer
from a driving sequence or by manually changing the morphable model parameters.
We parameterize each splat by a local coordinate frame of a triangle and
optimize for explicit displacement offset to obtain a more accurate geometric
representation. During avatar reconstruction, we jointly optimize for the
morphable model parameters and Gaussian splat parameters in an end-to-end
fashion. We demonstrate the animation capabilities of our photorealistic avatar
in several challenging scenarios. For instance, we show reenactments from a
driving video, where our method outperforms existing works by a significant
margin.";Shenhan Qian<author:sep>Tobias Kirschstein<author:sep>Liam Schoneveld<author:sep>Davide Davoli<author:sep>Simon Giebenhain<author:sep>Matthias NieÃner;http://arxiv.org/pdf/2312.02069v1;cs.CV;Project page: https://shenhanqian.github.io/gaussian-avatars;
2312.02121v1;http://arxiv.org/abs/2312.02121v1;2023-12-04;Mathematical Supplement for the $\texttt{gsplat}$ Library;"This report provides the mathematical details of the gsplat library, a
modular toolbox for efficient differentiable Gaussian splatting, as proposed by
Kerbl et al. It provides a self-contained reference for the computations
involved in the forward and backward passes of differentiable Gaussian
splatting. To facilitate practical usage and development, we provide a user
friendly Python API that exposes each component of the forward and backward
passes in rasterization at github.com/nerfstudio-project/gsplat .";Vickie Ye<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2312.02121v1;cs.MS;Find the library at: https://docs.gsplat.studio/;gaussian splatting<tag:sep>nerf
2312.01689v2;http://arxiv.org/abs/2312.01689v2;2023-12-04;Fast and accurate sparse-view CBCT reconstruction using meta-learned  neural attenuation field and hash-encoding regularization;"Cone beam computed tomography (CBCT) is an emerging medical imaging technique
to visualize the internal anatomical structures of patients. During a CBCT
scan, several projection images of different angles or views are collectively
utilized to reconstruct a tomographic image. However, reducing the number of
projections in a CBCT scan while preserving the quality of a reconstructed
image is challenging due to the nature of an ill-posed inverse problem.
Recently, a neural attenuation field (NAF) method was proposed by adopting a
neural radiance field algorithm as a new way for CBCT reconstruction,
demonstrating fast and promising results using only 50 views. However,
decreasing the number of projections is still preferable to reduce potential
radiation exposure, and a faster reconstruction time is required considering a
typical scan time. In this work, we propose a fast and accurate sparse-view
CBCT reconstruction (FACT) method to provide better reconstruction quality and
faster optimization speed in the minimal number of view acquisitions ($<$ 50
views). In the FACT method, we meta-trained a neural network and a hash-encoder
using a few scans (= 15), and a new regularization technique is utilized to
reconstruct the details of an anatomical structure. In conclusion, we have
shown that the FACT method produced better, and faster reconstruction results
over the other conventional algorithms based on CBCT scans of different body
parts (chest, head, and abdomen) and CT vendors (Siemens, Phillips, and GE).";Heejun Shin<author:sep>Taehee Kim<author:sep>Jongho Lee<author:sep>Se Young Chun<author:sep>Seungryung Cho<author:sep>Dongmyung Shin;http://arxiv.org/pdf/2312.01689v2;eess.IV;;
2312.02350v1;http://arxiv.org/abs/2312.02350v1;2023-12-04;Calibrated Uncertainties for Neural Radiance Fields;"Neural Radiance Fields have achieved remarkable results for novel view
synthesis but still lack a crucial component: precise measurement of
uncertainty in their predictions. Probabilistic NeRF methods have tried to
address this, but their output probabilities are not typically accurately
calibrated, and therefore do not capture the true confidence levels of the
model. Calibration is a particularly challenging problem in the sparse-view
setting, where additional held-out data is unavailable for fitting a calibrator
that generalizes to the test distribution. In this paper, we introduce the
first method for obtaining calibrated uncertainties from NeRF models. Our
method is based on a robust and efficient metric to calculate per-pixel
uncertainties from the predictive posterior distribution. We propose two
techniques that eliminate the need for held-out data. The first, based on patch
sampling, involves training two NeRF models for each scene. The second is a
novel meta-calibrator that only requires the training of one NeRF model. Our
proposed approach for obtaining calibrated uncertainties achieves
state-of-the-art uncertainty in the sparse-view setting while maintaining image
quality. We further demonstrate our method's effectiveness in applications such
as view enhancement and next-best view selection.";Niki Amini-Naieni<author:sep>Tomas Jakab<author:sep>Andrea Vedaldi<author:sep>Ronald Clark;http://arxiv.org/pdf/2312.02350v1;cs.CV;;nerf
2312.02137v1;http://arxiv.org/abs/2312.02137v1;2023-12-04;MANUS: Markerless Hand-Object Grasp Capture using Articulated 3D  Gaussians;"Understanding how we grasp objects with our hands has important applications
in areas like robotics and mixed reality. However, this challenging problem
requires accurate modeling of the contact between hands and objects. To capture
grasps, existing methods use skeletons, meshes, or parametric models that can
cause misalignments resulting in inaccurate contacts. We present MANUS, a
method for Markerless Hand-Object Grasp Capture using Articulated 3D Gaussians.
We build a novel articulated 3D Gaussians representation that extends 3D
Gaussian splatting for high-fidelity representation of articulating hands.
Since our representation uses Gaussian primitives, it enables us to efficiently
and accurately estimate contacts between the hand and the object. For the most
accurate results, our method requires tens of camera views that current
datasets do not provide. We therefore build MANUS-Grasps, a new dataset that
contains hand-object grasps viewed from 53 cameras across 30+ scenes, 3
subjects, and comprising over 7M frames. In addition to extensive qualitative
results, we also show that our method outperforms others on a quantitative
contact evaluation method that uses paint transfer from the object to the hand.";Chandradeep Pokhariya<author:sep>Ishaan N Shah<author:sep>Angela Xing<author:sep>Zekun Li<author:sep>Kefan Chen<author:sep>Avinash Sharma<author:sep>Srinath Sridhar;http://arxiv.org/pdf/2312.02137v1;cs.CV;;gaussian splatting
2312.01663v1;http://arxiv.org/abs/2312.01663v1;2023-12-04;Customize your NeRF: Adaptive Source Driven 3D Scene Editing via  Local-Global Iterative Training;"In this paper, we target the adaptive source driven 3D scene editing task by
proposing a CustomNeRF model that unifies a text description or a reference
image as the editing prompt. However, obtaining desired editing results
conformed with the editing prompt is nontrivial since there exist two
significant challenges, including accurate editing of only foreground regions
and multi-view consistency given a single-view reference image. To tackle the
first challenge, we propose a Local-Global Iterative Editing (LGIE) training
scheme that alternates between foreground region editing and full-image
editing, aimed at foreground-only manipulation while preserving the background.
For the second challenge, we also design a class-guided regularization that
exploits class priors within the generation model to alleviate the
inconsistency problem among different views in image-driven editing. Extensive
experiments show that our CustomNeRF produces precise editing results under
various real scenes for both text- and image-driven settings.";Runze He<author:sep>Shaofei Huang<author:sep>Xuecheng Nie<author:sep>Tianrui Hui<author:sep>Luoqi Liu<author:sep>Jiao Dai<author:sep>Jizhong Han<author:sep>Guanbin Li<author:sep>Si Liu;http://arxiv.org/pdf/2312.01663v1;cs.CV;14 pages, 13 figures, project website: https://customnerf.github.io/;nerf
2312.02135v1;http://arxiv.org/abs/2312.02135v1;2023-12-04;Fast View Synthesis of Casual Videos;"Novel view synthesis from an in-the-wild video is difficult due to challenges
like scene dynamics and lack of parallax. While existing methods have shown
promising results with implicit neural radiance fields, they are slow to train
and render. This paper revisits explicit video representations to synthesize
high-quality novel views from a monocular video efficiently. We treat static
and dynamic video content separately. Specifically, we build a global static
scene model using an extended plane-based scene representation to synthesize
temporally coherent novel video. Our plane-based scene representation is
augmented with spherical harmonics and displacement maps to capture
view-dependent effects and model non-planar complex surface geometry. We opt to
represent the dynamic content as per-frame point clouds for efficiency. While
such representations are inconsistency-prone, minor temporal inconsistencies
are perceptually masked due to motion. We develop a method to quickly estimate
such a hybrid video representation and render novel views in real time. Our
experiments show that our method can render high-quality novel views from an
in-the-wild video with comparable quality to state-of-the-art methods while
being 100x faster in training and enabling real-time rendering.";Yao-Chih Lee<author:sep>Zhoutong Zhang<author:sep>Kevin Blackburn-Matzen<author:sep>Simon Niklaus<author:sep>Jianming Zhang<author:sep>Jia-Bin Huang<author:sep>Feng Liu;http://arxiv.org/pdf/2312.02135v1;cs.CV;Project page: https://casual-fvs.github.io/;
2312.02155v1;http://arxiv.org/abs/2312.02155v1;2023-12-04;GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for  Real-time Human Novel View Synthesis;"We present a new approach, termed GPS-Gaussian, for synthesizing novel views
of a character in a real-time manner. The proposed method enables 2K-resolution
rendering under a sparse-view camera setting. Unlike the original Gaussian
Splatting or neural implicit rendering methods that necessitate per-subject
optimizations, we introduce Gaussian parameter maps defined on the source views
and regress directly Gaussian Splatting properties for instant novel view
synthesis without any fine-tuning or optimization. To this end, we train our
Gaussian parameter regression module on a large amount of human scan data,
jointly with a depth estimation module to lift 2D parameter maps to 3D space.
The proposed framework is fully differentiable and experiments on several
datasets demonstrate that our method outperforms state-of-the-art methods while
achieving an exceeding rendering speed.";Shunyuan Zheng<author:sep>Boyao Zhou<author:sep>Ruizhi Shao<author:sep>Boning Liu<author:sep>Shengping Zhang<author:sep>Liqiang Nie<author:sep>Yebin Liu;http://arxiv.org/pdf/2312.02155v1;cs.CV;The link to our projectpage is https://shunyuanzheng.github.io;gaussian splatting
2312.02015v1;http://arxiv.org/abs/2312.02015v1;2023-12-04;ColonNeRF: Neural Radiance Fields for High-Fidelity Long-Sequence  Colonoscopy Reconstruction;"Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.
However, accurate long-sequence colonoscopy reconstruction faces three major
challenges: (1) dissimilarity among segments of the colon due to its meandering
and convoluted shape; (2) co-existence of simple and intricately folded
geometry structures; (3) sparse viewpoints due to constrained camera
trajectories. To tackle these challenges, we introduce a new reconstruction
framework based on neural radiance field (NeRF), named ColonNeRF, which
leverages neural rendering for novel view synthesis of long-sequence
colonoscopy. Specifically, to reconstruct the entire colon in a piecewise
manner, our ColonNeRF introduces a region division and integration module,
effectively reducing shape dissimilarity and ensuring geometric consistency in
each segment. To learn both the simple and complex geometry in a unified
framework, our ColonNeRF incorporates a multi-level fusion module that
progressively models the colon regions from easy to hard. Additionally, to
overcome the challenges from sparse views, we devise a DensiNet module for
densifying camera poses under the guidance of semantic consistency. We conduct
extensive experiments on both synthetic and real-world datasets to evaluate our
ColonNeRF. Quantitatively, our ColonNeRF outperforms existing methods on two
benchmarks over four evaluation metrics. Notably, our LPIPS-ALEX scores exhibit
a substantial increase of about 67%-85% on the SimCol-to-3D dataset.
Qualitatively, our reconstruction visualizations show much clearer textures and
more accurate geometric details. These sufficiently demonstrate our superior
performance over the state-of-the-art methods.";Yufei Shi<author:sep>Beijia Lu<author:sep>Jia-Wei Liu<author:sep>Ming Li<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2312.02015v1;cs.CV;for Project Page, see https://showlab.github.io/ColonNeRF/;nerf
2312.01407v1;http://arxiv.org/abs/2312.01407v1;2023-12-03;VideoRF: Rendering Dynamic Radiance Fields as 2D Feature Video Streams;"Neural Radiance Fields (NeRFs) excel in photorealistically rendering static
scenes. However, rendering dynamic, long-duration radiance fields on ubiquitous
devices remains challenging, due to data storage and computational constraints.
In this paper, we introduce VideoRF, the first approach to enable real-time
streaming and rendering of dynamic radiance fields on mobile platforms. At the
core is a serialized 2D feature image stream representing the 4D radiance field
all in one. We introduce a tailored training scheme directly applied to this 2D
domain to impose the temporal and spatial redundancy of the feature image
stream. By leveraging the redundancy, we show that the feature image stream can
be efficiently compressed by 2D video codecs, which allows us to exploit video
hardware accelerators to achieve real-time decoding. On the other hand, based
on the feature image stream, we propose a novel rendering pipeline for VideoRF,
which has specialized space mappings to query radiance properties efficiently.
Paired with a deferred shading model, VideoRF has the capability of real-time
rendering on mobile devices thanks to its efficiency. We have developed a
real-time interactive player that enables online streaming and rendering of
dynamic scenes, offering a seamless and immersive free-viewpoint experience
across a range of devices, from desktops to mobile phones.";Liao Wang<author:sep>Kaixin Yao<author:sep>Chengcheng Guo<author:sep>Zhirui Zhang<author:sep>Qiang Hu<author:sep>Jingyi Yu<author:sep>Lan Xu<author:sep>Minye Wu;http://arxiv.org/pdf/2312.01407v1;cs.CV;Project page, see https://aoliao12138.github.io/VideoRF;nerf
2312.01531v1;http://arxiv.org/abs/2312.01531v1;2023-12-03;SANeRF-HQ: Segment Anything for NeRF in High Quality;"Recently, the Segment Anything Model (SAM) has showcased remarkable
capabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has
gained popularity as a method for various 3D problems beyond novel view
synthesis. Though there exist initial attempts to incorporate these two methods
into 3D segmentation, they face the challenge of accurately and consistently
segmenting objects in complex scenarios. In this paper, we introduce the
Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality
3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for
open-world object segmentation guided by user-supplied prompts, while
leveraging NeRF to aggregate information from different viewpoints. To overcome
the aforementioned challenges, we employ density field and RGB similarity to
enhance the accuracy of segmentation boundary during the aggregation.
Emphasizing on segmentation accuracy, we evaluate our method quantitatively on
multiple NeRF datasets where high-quality ground-truths are available or
manually annotated. SANeRF-HQ shows a significant quality improvement over
previous state-of-the-art methods in NeRF object segmentation, provides higher
flexibility for object localization, and enables more consistent object
segmentation across multiple views. Additional information can be found at
https://lyclyc52.github.io/SANeRF-HQ/.";Yichen Liu<author:sep>Benran Hu<author:sep>Chi-Keung Tang<author:sep>Yu-Wing Tai;http://arxiv.org/pdf/2312.01531v1;cs.CV;;nerf
2312.02218v1;http://arxiv.org/abs/2312.02218v1;2023-12-03;WavePlanes: A compact Wavelet representation for Dynamic Neural Radiance  Fields;"Dynamic Neural Radiance Fields (Dynamic NeRF) enhance NeRF technology to
model moving scenes. However, they are resource intensive and challenging to
compress. To address this issue, this paper presents WavePlanes, a fast and
more compact explicit model. We propose a multi-scale space and space-time
feature plane representation using N-level 2-D wavelet coefficients. The
inverse discrete wavelet transform reconstructs N feature signals at varying
detail, which are linearly decoded to approximate the color and density of
volumes in a 4-D grid. Exploiting the sparsity of wavelet coefficients, we
compress a Hash Map containing only non-zero coefficients and their locations
on each plane. This results in a compressed model size of ~12 MB. Compared with
state-of-the-art plane-based models, WavePlanes is up to 15x smaller, less
computationally demanding and achieves comparable results in as little as one
hour of training - without requiring custom CUDA code or high performance
computing resources. Additionally, we propose new feature fusion schemes that
work as well as previously proposed schemes while providing greater
interpretability. Our code is available at:
https://github.com/azzarelli/waveplanes/";Adrian Azzarelli<author:sep>Nantheera Anantrasirichai<author:sep>David R Bull;http://arxiv.org/pdf/2312.02218v1;cs.CV;;nerf
2312.01003v2;http://arxiv.org/abs/2312.01003v2;2023-12-02;Self-Evolving Neural Radiance Fields;"Recently, neural radiance field (NeRF) has shown remarkable performance in
novel view synthesis and 3D reconstruction. However, it still requires abundant
high-quality images, limiting its applicability in real-world scenarios. To
overcome this limitation, recent works have focused on training NeRF only with
sparse viewpoints by giving additional regularizations, often called few-shot
NeRF. We observe that due to the under-constrained nature of the task, solely
using additional regularization is not enough to prevent the model from
overfitting to sparse viewpoints. In this paper, we propose a novel framework,
dubbed Self-Evolving Neural Radiance Fields (SE-NeRF), that applies a
self-training framework to NeRF to address these problems. We formulate
few-shot NeRF into a teacher-student framework to guide the network to learn a
more robust representation of the scene by training the student with additional
pseudo labels generated from the teacher. By distilling ray-level pseudo labels
using distinct distillation schemes for reliable and unreliable rays obtained
with our novel reliability estimation method, we enable NeRF to learn a more
accurate and robust geometry of the 3D scene. We show and evaluate that
applying our self-training framework to existing models improves the quality of
the rendered images and achieves state-of-the-art performance in multiple
settings.";Jaewoo Jung<author:sep>Jisang Han<author:sep>Jiwon Kang<author:sep>Seongchan Kim<author:sep>Min-Seop Kwak<author:sep>Seungryong Kim;http://arxiv.org/pdf/2312.01003v2;cs.CV;"34 pages, 21 figures Our project page can be found at :
  https://ku-cvlab.github.io/SE-NeRF/";nerf
2312.02189v1;http://arxiv.org/abs/2312.02189v1;2023-12-02;StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D;"In the realm of text-to-3D generation, utilizing 2D diffusion models through
score distillation sampling (SDS) frequently leads to issues such as blurred
appearances and multi-faced geometry, primarily due to the intrinsically noisy
nature of the SDS loss. Our analysis identifies the core of these challenges as
the interaction among noise levels in the 2D diffusion process, the
architecture of the diffusion network, and the 3D model representation. To
overcome these limitations, we present StableDreamer, a methodology
incorporating three advances. First, inspired by InstructNeRF2NeRF, we
formalize the equivalence of the SDS generative prior and a simple supervised
L2 reconstruction loss. This finding provides a novel tool to debug SDS, which
we use to show the impact of time-annealing noise levels on reducing
multi-faced geometries. Second, our analysis shows that while image-space
diffusion contributes to geometric precision, latent-space diffusion is crucial
for vivid color rendition. Based on this observation, StableDreamer introduces
a two-stage training strategy that effectively combines these aspects,
resulting in high-fidelity 3D models. Third, we adopt an anisotropic 3D
Gaussians representation, replacing Neural Radiance Fields (NeRFs), to enhance
the overall quality, reduce memory usage during training, and accelerate
rendering speeds, and better capture semi-transparent objects. StableDreamer
reduces multi-face geometries, generates fine details, and converges stably.";Pengsheng Guo<author:sep>Hans Hao<author:sep>Adam Caccavale<author:sep>Zhongzheng Ren<author:sep>Edward Zhang<author:sep>Qi Shan<author:sep>Aditya Sankar<author:sep>Alexander G. Schwing<author:sep>Alex Colburn<author:sep>Fangchang Ma;http://arxiv.org/pdf/2312.02189v1;cs.CV;;nerf
2312.02202v1;http://arxiv.org/abs/2312.02202v1;2023-12-02;Volumetric Rendering with Baked Quadrature Fields;"We propose a novel Neural Radiance Field (NeRF) representation for non-opaque
scenes that allows fast inference by utilizing textured polygons. Despite the
high-quality novel view rendering that NeRF provides, a critical limitation is
that it relies on volume rendering that can be computationally expensive and
does not utilize the advancements in modern graphics hardware. Existing methods
for this problem fall short when it comes to modelling volumetric effects as
they rely purely on surface rendering. We thus propose to model the scene with
polygons, which can then be used to obtain the quadrature points required to
model volumetric effects, and also their opacity and colour from the texture.
To obtain such polygonal mesh, we train a specialized field whose
zero-crossings would correspond to the quadrature points when volume rendering,
and perform marching cubes on this field. We then rasterize the polygons and
utilize the fragment shaders to obtain the final colour image. Our method
allows rendering on various devices and easy integration with existing graphics
frameworks while keeping the benefits of volume rendering alive.";Gopal Sharma<author:sep>Daniel Rebain<author:sep>Kwang Moo Yi<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2312.02202v1;cs.GR;;nerf
2312.00451v1;http://arxiv.org/abs/2312.00451v1;2023-12-01;FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting;"Novel view synthesis from limited observations remains an important and
persistent task. However, high efficiency in existing NeRF-based few-shot view
synthesis is often compromised to obtain an accurate 3D representation. To
address this challenge, we propose a few-shot view synthesis framework based on
3D Gaussian Splatting that enables real-time and photo-realistic view synthesis
with as few as three training views. The proposed method, dubbed FSGS, handles
the extremely sparse initialized SfM points with a thoughtfully designed
Gaussian Unpooling process. Our method iteratively distributes new Gaussians
around the most representative locations, subsequently infilling local details
in vacant areas. We also integrate a large-scale pre-trained monocular depth
estimator within the Gaussians optimization process, leveraging online
augmented views to guide the geometric optimization towards an optimal
solution. Starting from sparse points observed from limited input viewpoints,
our FSGS can accurately grow into unseen regions, comprehensively covering the
scene and boosting the rendering quality of novel views. Overall, FSGS achieves
state-of-the-art performance in both accuracy and rendering efficiency across
diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website:
https://zehaozhu.github.io/FSGS/.";Zehao Zhu<author:sep>Zhiwen Fan<author:sep>Yifan Jiang<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2312.00451v1;cs.CV;Project page: https://zehaozhu.github.io/FSGS/;gaussian splatting<tag:sep>nerf
2312.00732v1;http://arxiv.org/abs/2312.00732v1;2023-12-01;Gaussian Grouping: Segment and Edit Anything in 3D Scenes;"The recent Gaussian Splatting achieves high-quality and real-time novel-view
synthesis of the 3D scenes. However, it is solely concentrated on the
appearance and geometry modeling, while lacking in fine-grained object-level
scene understanding. To address this issue, we propose Gaussian Grouping, which
extends Gaussian Splatting to jointly reconstruct and segment anything in
open-world 3D scenes. We augment each Gaussian with a compact Identity
Encoding, allowing the Gaussians to be grouped according to their object
instance or stuff membership in the 3D scene. Instead of resorting to expensive
3D labels, we supervise the Identity Encodings during the differentiable
rendering by leveraging the 2D mask predictions by SAM, along with introduced
3D spatial consistency regularization. Comparing to the implicit NeRF
representation, we show that the discrete and grouped 3D Gaussians can
reconstruct, segment and edit anything in 3D with high visual quality, fine
granularity and efficiency. Based on Gaussian Grouping, we further propose a
local Gaussian Editing scheme, which shows efficacy in versatile scene editing
applications, including 3D object removal, inpainting, colorization and scene
recomposition. Our code and models will be at
https://github.com/lkeab/gaussian-grouping.";Mingqiao Ye<author:sep>Martin Danelljan<author:sep>Fisher Yu<author:sep>Lei Ke;http://arxiv.org/pdf/2312.00732v1;cs.CV;"We propose Gaussian Grouping, which extends Gaussian Splatting to
  fine-grained open-world 3D scene understanding. Github:
  https://github.com/lkeab/gaussian-grouping";gaussian splatting<tag:sep>nerf
2401.05345v1;http://arxiv.org/abs/2401.05345v1;2023-12-01;DISTWAR: Fast Differentiable Rendering on Raster-based Rendering  Pipelines;"Differentiable rendering is a technique used in an important emerging class
of visual computing applications that involves representing a 3D scene as a
model that is trained from 2D images using gradient descent. Recent works (e.g.
3D Gaussian Splatting) use a rasterization pipeline to enable rendering high
quality photo-realistic imagery at high speeds from these learned 3D models.
These methods have been demonstrated to be very promising, providing
state-of-art quality for many important tasks. However, training a model to
represent a scene is still a time-consuming task even when using powerful GPUs.
In this work, we observe that the gradient computation phase during training is
a significant bottleneck on GPUs due to the large number of atomic operations
that need to be processed. These atomic operations overwhelm atomic units in
the L2 partitions causing stalls. To address this challenge, we leverage the
observations that during the gradient computation: (1) for most warps, all
threads atomically update the same memory locations; and (2) warps generate
varying amounts of atomic traffic (since some threads may be inactive). We
propose DISTWAR, a software-approach to accelerate atomic operations based on
two key ideas: First, we enable warp-level reduction of threads at the SM
sub-cores using registers to leverage the locality in intra-warp atomic
updates. Second, we distribute the atomic computation between the warp-level
reduction at the SM and the L2 atomic units to increase the throughput of
atomic computation. Warps with many threads performing atomic updates to the
same memory locations are scheduled at the SM, and the rest using L2 atomic
units. We implement DISTWAR using existing warp-level primitives. We evaluate
DISTWAR on widely used raster-based differentiable rendering workloads. We
demonstrate significant speedups of 2.44x on average (up to 5.7x).";Sankeerth Durvasula<author:sep>Adrian Zhao<author:sep>Fan Chen<author:sep>Ruofan Liang<author:sep>Pawan Kumar Sanjaya<author:sep>Nandita Vijaykumar;http://arxiv.org/pdf/2401.05345v1;cs.CV;;gaussian splatting
2312.00846v1;http://arxiv.org/abs/2312.00846v1;2023-12-01;NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting  Guidance;"Existing neural implicit surface reconstruction methods have achieved
impressive performance in multi-view 3D reconstruction by leveraging explicit
geometry priors such as depth maps or point clouds as regularization. However,
the reconstruction results still lack fine details because of the over-smoothed
depth map or sparse point cloud. In this work, we propose a neural implicit
surface reconstruction pipeline with guidance from 3D Gaussian Splatting to
recover highly detailed surfaces. The advantage of 3D Gaussian Splatting is
that it can generate dense point clouds with detailed structure. Nonetheless, a
naive adoption of 3D Gaussian Splatting can fail since the generated points are
the centers of 3D Gaussians that do not necessarily lie on the surface. We thus
introduce a scale regularizer to pull the centers close to the surface by
enforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine
the point cloud from 3D Gaussians Splatting with the normal priors from the
surface predicted by neural implicit models instead of using a fixed set of
points as guidance. Consequently, the quality of surface reconstruction
improves from the guidance of the more accurate 3D Gaussian splatting. By
jointly optimizing the 3D Gaussian Splatting and the neural implicit model, our
approach benefits from both representations and generates complete surfaces
with intricate details. Experiments on Tanks and Temples verify the
effectiveness of our proposed method.";Hanlin Chen<author:sep>Chen Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2312.00846v1;cs.CV;;gaussian splatting
2312.00860v1;http://arxiv.org/abs/2312.00860v1;2023-12-01;Segment Any 3D Gaussians;"Interactive 3D segmentation in radiance fields is an appealing task since its
importance in 3D scene understanding and manipulation. However, existing
methods face challenges in either achieving fine-grained, multi-granularity
segmentation or contending with substantial computational overhead, inhibiting
real-time interaction. In this paper, we introduce Segment Any 3D GAussians
(SAGA), a novel 3D interactive segmentation approach that seamlessly blends a
2D segmentation foundation model with 3D Gaussian Splatting (3DGS), a recent
breakthrough of radiance fields. SAGA efficiently embeds multi-granularity 2D
segmentation results generated by the segmentation foundation model into 3D
Gaussian point features through well-designed contrastive training. Evaluation
on existing benchmarks demonstrates that SAGA can achieve competitive
performance with state-of-the-art methods. Moreover, SAGA achieves
multi-granularity segmentation and accommodates various prompts, including
points, scribbles, and 2D masks. Notably, SAGA can finish the 3D segmentation
within milliseconds, achieving nearly 1000x acceleration compared to previous
SOTA. The project page is at https://jumpat.github.io/SAGA.";Jiazhong Cen<author:sep>Jiemin Fang<author:sep>Chen Yang<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wei Shen<author:sep>Qi Tian;http://arxiv.org/pdf/2312.00860v1;cs.CV;Work in progress. Project page: https://jumpat.github.io/SAGA;gaussian splatting
2312.00252v1;http://arxiv.org/abs/2312.00252v1;2023-11-30;PyNeRF: Pyramidal Neural Radiance Fields;"Neural Radiance Fields (NeRFs) can be dramatically accelerated by spatial
grid representations. However, they do not explicitly reason about scale and so
introduce aliasing artifacts when reconstructing scenes captured at different
camera distances. Mip-NeRF and its extensions propose scale-aware renderers
that project volumetric frustums rather than point samples but such approaches
rely on positional encodings that are not readily compatible with grid methods.
We propose a simple modification to grid-based models by training model heads
at different spatial grid resolutions. At render time, we simply use coarser
grids to render samples that cover larger volumes. Our method can be easily
applied to existing accelerated NeRF methods and significantly improves
rendering quality (reducing error rates by 20-90% across synthetic and
unbounded real-world scenes) while incurring minimal performance overhead (as
each model head is quick to evaluate). Compared to Mip-NeRF, we reduce error
rates by 20% while training over 60x faster.";Haithem Turki<author:sep>Michael ZollhÃ¶fer<author:sep>Christian Richardt<author:sep>Deva Ramanan;http://arxiv.org/pdf/2312.00252v1;cs.CV;Neurips 2023 Project page: https://haithemturki.com/pynerf/;nerf
2312.00583v1;http://arxiv.org/abs/2312.00583v1;2023-11-30;MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly  Deformable Scenes;"Accurate 3D tracking in highly deformable scenes with occlusions and shadows
can facilitate new applications in robotics, augmented reality, and generative
AI. However, tracking under these conditions is extremely challenging due to
the ambiguity that arises with large deformations, shadows, and occlusions. We
introduce MD-Splatting, an approach for simultaneous 3D tracking and novel view
synthesis, using video captures of a dynamic scene from various camera poses.
MD-Splatting builds on recent advances in Gaussian splatting, a method that
learns the properties of a large number of Gaussians for state-of-the-art and
fast novel view synthesis. MD-Splatting learns a deformation function to
project a set of Gaussians with non-metric, thus canonical, properties into
metric space. The deformation function uses a neural-voxel encoding and a
multilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow
scalar. We enforce physics-inspired regularization terms based on local
rigidity, conservation of momentum, and isometry, which leads to trajectories
with smaller trajectory errors. MD-Splatting achieves high-quality 3D tracking
on highly deformable scenes with shadows and occlusions. Compared to
state-of-the-art, we improve 3D tracking by an average of 23.9 %, while
simultaneously achieving high-quality novel view synthesis. With sufficient
texture such as in scene 6, MD-Splatting achieves a median tracking error of
3.39 mm on a cloth of 1 x 1 meters in size. Project website:
https://md-splatting.github.io/.";Bardienus P. Duisterhof<author:sep>Zhao Mandi<author:sep>Yunchao Yao<author:sep>Jia-Wei Liu<author:sep>Mike Zheng Shou<author:sep>Shuran Song<author:sep>Jeffrey Ichnowski;http://arxiv.org/pdf/2312.00583v1;cs.CV;;gaussian splatting
2311.18159v1;http://arxiv.org/abs/2311.18159v1;2023-11-30;Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector  Quantization;"3D Gaussian Splatting is a new method for modeling and rendering 3D radiance
fields that achieves much faster learning and rendering time compared to SOTA
NeRF methods. However, it comes with a drawback in the much larger storage
demand compared to NeRF methods since it needs to store the parameters for
several 3D Gaussians. We notice that many Gaussians may share similar
parameters, so we introduce a simple vector quantization method based on
\kmeans algorithm to quantize the Gaussian parameters. Then, we store the small
codebook along with the index of the code for each Gaussian. Moreover, we
compress the indices further by sorting them and using a method similar to
run-length encoding. We do extensive experiments on standard benchmarks as well
as a new benchmark which is an order of magnitude larger than the standard
benchmarks. We show that our simple yet effective method can reduce the storage
cost for the original 3D Gaussian Splatting method by a factor of almost
$20\times$ with a very small drop in the quality of rendered images.";KL Navaneet<author:sep>Kossar Pourahmadi Meibodi<author:sep>Soroush Abbasi Koohpayegani<author:sep>Hamed Pirsiavash;http://arxiv.org/pdf/2311.18159v1;cs.CV;Code is available at https://github.com/UCDvision/compact3d;gaussian splatting<tag:sep>nerf
2311.18311v1;http://arxiv.org/abs/2311.18311v1;2023-11-30;Anisotropic Neural Representation Learning for High-Quality Neural  Rendering;"Neural radiance fields (NeRFs) have achieved impressive view synthesis
results by learning an implicit volumetric representation from multi-view
images. To project the implicit representation into an image, NeRF employs
volume rendering that approximates the continuous integrals of rays as an
accumulation of the colors and densities of the sampled points. Although this
approximation enables efficient rendering, it ignores the direction information
in point intervals, resulting in ambiguous features and limited reconstruction
quality. In this paper, we propose an anisotropic neural representation
learning method that utilizes learnable view-dependent features to improve
scene representation and reconstruction. We model the volumetric function as
spherical harmonic (SH)-guided anisotropic features, parameterized by
multilayer perceptrons, facilitating ambiguity elimination while preserving the
rendering efficiency. To achieve robust scene reconstruction without anisotropy
overfitting, we regularize the energy of the anisotropic features during
training. Our method is flexiable and can be plugged into NeRF-based
frameworks. Extensive experiments show that the proposed representation can
boost the rendering quality of various NeRFs and achieve state-of-the-art
rendering performance on both synthetic and real-world scenes.";Y. Wang<author:sep>J. Xu<author:sep>Y. Zeng<author:sep>Y. Gong;http://arxiv.org/pdf/2311.18311v1;cs.CV;;nerf
2312.00112v1;http://arxiv.org/abs/2312.00112v1;2023-11-30;DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis  with 3D Gaussian Splatting;"Accurately and efficiently modeling dynamic scenes and motions is considered
so challenging a task due to temporal dynamics and motion complexity. To
address these challenges, we propose DynMF, a compact and efficient
representation that decomposes a dynamic scene into a few neural trajectories.
We argue that the per-point motions of a dynamic scene can be decomposed into a
small set of explicit or learned trajectories. Our carefully designed neural
framework consisting of a tiny set of learned basis queried only in time allows
for rendering speed similar to 3D Gaussian Splatting, surpassing 120 FPS, while
at the same time, requiring only double the storage compared to static scenes.
Our neural representation adequately constrains the inherently underconstrained
motion field of a dynamic scene leading to effective and fast optimization.
This is done by biding each point to motion coefficients that enforce the
per-point sharing of basis trajectories. By carefully applying a sparsity loss
to the motion coefficients, we are able to disentangle the motions that
comprise the scene, independently control them, and generate novel motion
combinations that have never been seen before. We can reach state-of-the-art
render quality within just 5 minutes of training and in less than half an hour,
we can synthesize novel views of dynamic scenes with superior photorealistic
quality. Our representation is interpretable, efficient, and expressive enough
to offer real-time view synthesis of complex dynamic scene motions, in
monocular and multi-view scenarios.";Agelos Kratimenos<author:sep>Jiahui Lei<author:sep>Kostas Daniilidis;http://arxiv.org/pdf/2312.00112v1;cs.CV;Project page: https://agelosk.github.io/dynmf/;gaussian splatting
2312.00206v1;http://arxiv.org/abs/2312.00206v1;2023-11-30;SparseGS: Real-Time 360Â° Sparse View Synthesis using Gaussian  Splatting;"The problem of novel view synthesis has grown significantly in popularity
recently with the introduction of Neural Radiance Fields (NeRFs) and other
implicit scene representation methods. A recent advance, 3D Gaussian Splatting
(3DGS), leverages an explicit representation to achieve real-time rendering
with high-quality results. However, 3DGS still requires an abundance of
training views to generate a coherent scene representation. In few shot
settings, similar to NeRF, 3DGS tends to overfit to training views, causing
background collapse and excessive floaters, especially as the number of
training views are reduced. We propose a method to enable training coherent
3DGS-based radiance fields of 360 scenes from sparse training views. We find
that using naive depth priors is not sufficient and integrate depth priors with
generative and explicit constraints to reduce background collapse, remove
floaters, and enhance consistency from unseen viewpoints. Experiments show that
our method outperforms base 3DGS by up to 30.5% and NeRF-based methods by up to
15.6% in LPIPS on the MipNeRF-360 dataset with substantially less training and
inference cost.";Haolin Xiong<author:sep>Sairisheek Muttukuru<author:sep>Rishi Upadhyay<author:sep>Pradyumna Chari<author:sep>Achuta Kadambi;http://arxiv.org/pdf/2312.00206v1;cs.CV;"The main text spans eight pages, followed by two pages of references
  and four pages of supplementary materials";gaussian splatting<tag:sep>nerf
2311.18288v1;http://arxiv.org/abs/2311.18288v1;2023-11-30;CosAvatar: Consistent and Animatable Portrait Video Tuning with Text  Prompt;"Recently, text-guided digital portrait editing has attracted more and more
attentions. However, existing methods still struggle to maintain consistency
across time, expression, and view or require specific data prerequisites. To
solve these challenging problems, we propose CosAvatar, a high-quality and
user-friendly framework for portrait tuning. With only monocular video and text
instructions as input, we can produce animatable portraits with both temporal
and 3D consistency. Different from methods that directly edit in the 2D domain,
we employ a dynamic NeRF-based 3D portrait representation to model both the
head and torso. We alternate between editing the video frames' dataset and
updating the underlying 3D portrait until the edited frames reach 3D
consistency. Additionally, we integrate the semantic portrait priors to enhance
the edited results, allowing precise modifications in specified semantic areas.
Extensive results demonstrate that our proposed method can not only accurately
edit portrait styles or local attributes based on text instructions but also
support expressive animation driven by a source video.";Haiyao Xiao<author:sep>Chenglai Zhong<author:sep>Xuan Gao<author:sep>Yudong Guo<author:sep>Juyong Zhang;http://arxiv.org/pdf/2311.18288v1;cs.CV;Project page: https://ustc3dv.github.io/CosAvatar/;nerf
2311.18608v1;http://arxiv.org/abs/2311.18608v1;2023-11-30;Contrastive Denoising Score for Text-guided Latent Diffusion Image  Editing;"With the remarkable advent of text-to-image diffusion models, image editing
methods have become more diverse and continue to evolve. A promising recent
approach in this realm is Delta Denoising Score (DDS) - an image editing
technique based on Score Distillation Sampling (SDS) framework that leverages
the rich generative prior of text-to-image diffusion models. However, relying
solely on the difference between scoring functions is insufficient for
preserving specific structural elements from the original image, a crucial
aspect of image editing. Inspired by the similarity and importance differences
between DDS and the contrastive learning for unpaired image-to-image
translation (CUT), here we present an embarrassingly simple yet very powerful
modification of DDS, called Contrastive Denoising Score (CDS), for latent
diffusion models (LDM). Specifically, to enforce structural correspondence
between the input and output while maintaining the controllability of contents,
we introduce a straightforward approach to regulate structural consistency
using CUT loss within the DDS framework. To calculate this loss, instead of
employing auxiliary networks, we utilize the intermediate features of LDM, in
particular, those from the self-attention layers, which possesses rich spatial
information. Our approach enables zero-shot image-to-image translation and
neural radiance field (NeRF) editing, achieving a well-balanced interplay
between maintaining the structural details and transforming content.
Qualitative results and comparisons demonstrates the effectiveness of our
proposed method. Project page with code is available at
https://hyelinnam.github.io/CDS/.";Hyelin Nam<author:sep>Gihyun Kwon<author:sep>Geon Yeong Park<author:sep>Jong Chul Ye;http://arxiv.org/pdf/2311.18608v1;cs.CV;Project page: https://hyelinnam.github.io/CDS/;nerf
2311.18561v1;http://arxiv.org/abs/2311.18561v1;2023-11-30;Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and  Real-time Rendering;"Modeling dynamic, large-scale urban scenes is challenging due to their highly
intricate geometric structures and unconstrained dynamics in both space and
time. Prior methods often employ high-level architectural priors, separating
static and dynamic elements, resulting in suboptimal capture of their
synergistic interactions. To address this challenge, we present a unified
representation model, called Periodic Vibration Gaussian (PVG). PVG builds upon
the efficient 3D Gaussian splatting technique, originally designed for static
scene representation, by introducing periodic vibration-based temporal
dynamics. This innovation enables PVG to elegantly and uniformly represent the
characteristics of various objects and elements in dynamic urban scenes. To
enhance temporally coherent representation learning with sparse training data,
we introduce a novel flow-based temporal smoothing mechanism and a
position-aware adaptive control strategy. Extensive experiments on Waymo Open
Dataset and KITTI benchmarks demonstrate that PVG surpasses state-of-the-art
alternatives in both reconstruction and novel view synthesis for both dynamic
and static scenes. Notably, PVG achieves this without relying on manually
labeled object bounding boxes or expensive optical flow estimation. Moreover,
PVG exhibits 50/6000-fold acceleration in training/rendering over the best
alternative.";Yurui Chen<author:sep>Chun Gu<author:sep>Junzhe Jiang<author:sep>Xiatian Zhu<author:sep>Li Zhang;http://arxiv.org/pdf/2311.18561v1;cs.CV;Project page: https://fudan-zvg.github.io/PVG/;gaussian splatting
2311.18491v1;http://arxiv.org/abs/2311.18491v1;2023-11-30;ZeST-NeRF: Using temporal aggregation for Zero-Shot Temporal NeRFs;"In the field of media production, video editing techniques play a pivotal
role. Recent approaches have had great success at performing novel view image
synthesis of static scenes. But adding temporal information adds an extra layer
of complexity. Previous models have focused on implicitly representing static
and dynamic scenes using NeRF. These models achieve impressive results but are
costly at training and inference time. They overfit an MLP to describe the
scene implicitly as a function of position. This paper proposes ZeST-NeRF, a
new approach that can produce temporal NeRFs for new scenes without retraining.
We can accurately reconstruct novel views using multi-view synthesis techniques
and scene flow-field estimation, trained only with unrelated scenes. We
demonstrate how existing state-of-the-art approaches from a range of fields
cannot adequately solve this new task and demonstrate the efficacy of our
solution. The resulting network improves quantitatively by 15% and produces
significantly better visual results.";Violeta MenÃ©ndez GonzÃ¡lez<author:sep>Andrew Gilbert<author:sep>Graeme Phillipson<author:sep>Stephen Jolly<author:sep>Simon Hadfield;http://arxiv.org/pdf/2311.18491v1;cs.CV;VUA BMVC 2023;nerf
2312.00588v1;http://arxiv.org/abs/2312.00588v1;2023-11-30;LucidDreaming: Controllable Object-Centric 3D Generation;"With the recent development of generative models, Text-to-3D generations have
also seen significant growth. Nonetheless, achieving precise control over 3D
generation continues to be an arduous task, as using text to control often
leads to missing objects and imprecise locations. Contemporary strategies for
enhancing controllability in 3D generation often entail the introduction of
additional parameters, such as customized diffusion models. This often induces
hardness in adapting to different diffusion models or creating distinct
objects.
  In this paper, we present LucidDreaming as an effective pipeline capable of
fine-grained control over 3D generation. It requires only minimal input of 3D
bounding boxes, which can be deduced from a simple text prompt using a Large
Language Model. Specifically, we propose clipped ray sampling to separately
render and optimize objects with user specifications. We also introduce
object-centric density blob bias, fostering the separation of generated
objects. With individual rendering and optimizing of objects, our method excels
not only in controlled content generation from scratch but also within the
pre-trained NeRF scenes. In such scenarios, existing generative approaches
often disrupt the integrity of the original scene, and current editing methods
struggle to synthesize new content in empty spaces. We show that our method
exhibits remarkable adaptability across a spectrum of mainstream Score
Distillation Sampling-based 3D generation frameworks, and achieves superior
alignment of 3D content when compared to baseline approaches. We also provide a
dataset of prompts with 3D bounding boxes, benchmarking 3D spatial
controllability.";Zhaoning Wang<author:sep>Ming Li<author:sep>Chen Chen;http://arxiv.org/pdf/2312.00588v1;cs.CV;;nerf
2401.06143v1;http://arxiv.org/abs/2401.06143v1;2023-11-30;Redefining Recon: Bridging Gaps with UAVs, 360 degree Cameras, and  Neural Radiance Fields;"In the realm of digital situational awareness during disaster situations,
accurate digital representations, like 3D models, play an indispensable role.
To ensure the safety of rescue teams, robotic platforms are often deployed to
generate these models. In this paper, we introduce an innovative approach that
synergizes the capabilities of compact Unmaned Arial Vehicles (UAVs), smaller
than 30 cm, equipped with 360 degree cameras and the advances of Neural
Radiance Fields (NeRFs). A NeRF, a specialized neural network, can deduce a 3D
representation of any scene using 2D images and then synthesize it from various
angles upon request. This method is especially tailored for urban environments
which have experienced significant destruction, where the structural integrity
of buildings is compromised to the point of barring entry-commonly observed
post-earthquakes and after severe fires. We have tested our approach through
recent post-fire scenario, underlining the efficacy of NeRFs even in
challenging outdoor environments characterized by water, snow, varying light
conditions, and reflective surfaces.";Hartmut Surmann<author:sep>Niklas Digakis<author:sep>Jan-Nicklas Kremer<author:sep>Julien Meine<author:sep>Max Schulte<author:sep>Niklas Voigt;http://arxiv.org/pdf/2401.06143v1;cs.CV;"6 pages, published at IEEE International Symposium on
  Safety,Security,and Rescue Robotics SSRR2023 in FUKUSHIMA, November 13-15
  2023";nerf
2312.00109v1;http://arxiv.org/abs/2312.00109v1;2023-11-30;Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering;"Neural rendering methods have significantly advanced photo-realistic 3D scene
rendering in various academic and industrial applications. The recent 3D
Gaussian Splatting method has achieved the state-of-the-art rendering quality
and speed combining the benefits of both primitive-based representations and
volumetric representations. However, it often leads to heavily redundant
Gaussians that try to fit every training view, neglecting the underlying scene
geometry. Consequently, the resulting model becomes less robust to significant
view changes, texture-less area and lighting effects. We introduce Scaffold-GS,
which uses anchor points to distribute local 3D Gaussians, and predicts their
attributes on-the-fly based on viewing direction and distance within the view
frustum. Anchor growing and pruning strategies are developed based on the
importance of neural Gaussians to reliably improve the scene coverage. We show
that our method effectively reduces redundant Gaussians while delivering
high-quality rendering. We also demonstrates an enhanced capability to
accommodate scenes with varying levels-of-detail and view-dependent
observations, without sacrificing the rendering speed.";Tao Lu<author:sep>Mulin Yu<author:sep>Linning Xu<author:sep>Yuanbo Xiangli<author:sep>Limin Wang<author:sep>Dahua Lin<author:sep>Bo Dai;http://arxiv.org/pdf/2312.00109v1;cs.CV;Project page: https://city-super.github.io/scaffold-gs/;gaussian splatting
2311.17754v1;http://arxiv.org/abs/2311.17754v1;2023-11-29;Cinematic Behavior Transfer via NeRF-based Differentiable Filming;"In the evolving landscape of digital media and video production, the precise
manipulation and reproduction of visual elements like camera movements and
character actions are highly desired. Existing SLAM methods face limitations in
dynamic scenes and human pose estimation often focuses on 2D projections,
neglecting 3D statuses. To address these issues, we first introduce a reverse
filming behavior estimation technique. It optimizes camera trajectories by
leveraging NeRF as a differentiable renderer and refining SMPL tracks. We then
introduce a cinematic transfer pipeline that is able to transfer various shot
types to a new 2D video or a 3D virtual environment. The incorporation of 3D
engine workflow enables superior rendering and control abilities, which also
achieves a higher rating in the user study.";Xuekun Jiang<author:sep>Anyi Rao<author:sep>Jingbo Wang<author:sep>Dahua Lin<author:sep>Bo Dai;http://arxiv.org/pdf/2311.17754v1;cs.CV;"Project Page:
  https://virtualfilmstudio.github.io/projects/cinetransfer";nerf
2311.17917v1;http://arxiv.org/abs/2311.17917v1;2023-11-29;AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text;"We study the problem of creating high-fidelity and animatable 3D avatars from
only textual descriptions. Existing text-to-avatar methods are either limited
to static avatars which cannot be animated or struggle to generate animatable
avatars with promising quality and precise pose control. To address these
limitations, we propose AvatarStudio, a coarse-to-fine generative model that
generates explicit textured 3D meshes for animatable human avatars.
Specifically, AvatarStudio begins with a low-resolution NeRF-based
representation for coarse generation, followed by incorporating SMPL-guided
articulation into the explicit mesh representation to support avatar animation
and high resolution rendering. To ensure view consistency and pose
controllability of the resulting avatars, we introduce a 2D diffusion model
conditioned on DensePose for Score Distillation Sampling supervision. By
effectively leveraging the synergy between the articulated mesh representation
and the DensePose-conditional diffusion model, AvatarStudio can create
high-quality avatars from text that are ready for animation, significantly
outperforming previous methods. Moreover, it is competent for many
applications, e.g., multimodal avatar animations and style-guided avatar
creation. For more results, please refer to our project page:
http://jeff95.me/projects/avatarstudio.html";Jianfeng Zhang<author:sep>Xuanmeng Zhang<author:sep>Huichao Zhang<author:sep>Jun Hao Liew<author:sep>Chenxu Zhang<author:sep>Yi Yang<author:sep>Jiashi Feng;http://arxiv.org/pdf/2311.17917v1;cs.GR;Project page at http://jeff95.me/projects/avatarstudio.html;nerf
2311.17332v1;http://arxiv.org/abs/2311.17332v1;2023-11-29;NeRFTAP: Enhancing Transferability of Adversarial Patches on Face  Recognition using Neural Radiance Fields;"Face recognition (FR) technology plays a crucial role in various
applications, but its vulnerability to adversarial attacks poses significant
security concerns. Existing research primarily focuses on transferability to
different FR models, overlooking the direct transferability to victim's face
images, which is a practical threat in real-world scenarios. In this study, we
propose a novel adversarial attack method that considers both the
transferability to the FR model and the victim's face image, called NeRFTAP.
Leveraging NeRF-based 3D-GAN, we generate new view face images for the source
and target subjects to enhance transferability of adversarial patches. We
introduce a style consistency loss to ensure the visual similarity between the
adversarial UV map and the target UV map under a 0-1 mask, enhancing the
effectiveness and naturalness of the generated adversarial face images.
Extensive experiments and evaluations on various FR models demonstrate the
superiority of our approach over existing attack techniques. Our work provides
valuable insights for enhancing the robustness of FR systems in practical
adversarial settings.";Xiaoliang Liu<author:sep>Furao Shen<author:sep>Feng Han<author:sep>Jian Zhao<author:sep>Changhai Nie;http://arxiv.org/pdf/2311.17332v1;cs.CV;;nerf
2311.17977v1;http://arxiv.org/abs/2311.17977v1;2023-11-29;GaussianShader: 3D Gaussian Splatting with Shading Functions for  Reflective Surfaces;"The advent of neural 3D Gaussians has recently brought about a revolution in
the field of neural rendering, facilitating the generation of high-quality
renderings at real-time speeds. However, the explicit and discrete
representation encounters challenges when applied to scenes featuring
reflective surfaces. In this paper, we present GaussianShader, a novel method
that applies a simplified shading function on 3D Gaussians to enhance the
neural rendering in scenes with reflective surfaces while preserving the
training and rendering efficiency. The main challenge in applying the shading
function lies in the accurate normal estimation on discrete 3D Gaussians.
Specifically, we proposed a novel normal estimation framework based on the
shortest axis directions of 3D Gaussians with a delicately designed loss to
make the consistency between the normals and the geometries of Gaussian
spheres. Experiments show that GaussianShader strikes a commendable balance
between efficiency and visual quality. Our method surpasses Gaussian Splatting
in PSNR on specular object datasets, exhibiting an improvement of 1.57dB. When
compared to prior works handling reflective surfaces, such as Ref-NeRF, our
optimization time is significantly accelerated (23h vs. 0.58h). Please click on
our project website to see more results.";Yingwenqi Jiang<author:sep>Jiadong Tu<author:sep>Yuan Liu<author:sep>Xifeng Gao<author:sep>Xiaoxiao Long<author:sep>Wenping Wang<author:sep>Yuexin Ma;http://arxiv.org/pdf/2311.17977v1;cs.CV;13 pages, 11 figures, refrences added;gaussian splatting<tag:sep>nerf
2311.17907v1;http://arxiv.org/abs/2311.17907v1;2023-11-29;CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting;"With the onset of diffusion-based generative models and their ability to
generate text-conditioned images, content generation has received a massive
invigoration. Recently, these models have been shown to provide useful guidance
for the generation of 3D graphics assets. However, existing work in
text-conditioned 3D generation faces fundamental constraints: (i) inability to
generate detailed, multi-object scenes, (ii) inability to textually control
multi-object configurations, and (iii) physically realistic scene composition.
In this work, we propose CG3D, a method for compositionally generating scalable
3D assets that resolves these constraints. We find that explicit Gaussian
radiance fields, parameterized to allow for compositions of objects, possess
the capability to enable semantically and physically consistent scenes. By
utilizing a guidance framework built around this explicit representation, we
show state of the art results, capable of even exceeding the guiding diffusion
model in terms of object combinations and physics accuracy.";Alexander Vilesov<author:sep>Pradyumna Chari<author:sep>Achuta Kadambi;http://arxiv.org/pdf/2311.17907v1;cs.CV;;gaussian splatting
2311.17874v1;http://arxiv.org/abs/2311.17874v1;2023-11-29;FisherRF: Active View Selection and Uncertainty Quantification for  Radiance Fields using Fisher Information;"This study addresses the challenging problem of active view selection and
uncertainty quantification within the domain of Radiance Fields. Neural
Radiance Fields (NeRF) have greatly advanced image rendering and
reconstruction, but the limited availability of 2D images poses uncertainties
stemming from occlusions, depth ambiguities, and imaging errors. Efficiently
selecting informative views becomes crucial, and quantifying NeRF model
uncertainty presents intricate challenges. Existing approaches either depend on
model architecture or are based on assumptions regarding density distributions
that are not generally applicable. By leveraging Fisher Information, we
efficiently quantify observed information within Radiance Fields without ground
truth data. This can be used for the next best view selection and pixel-wise
uncertainty quantification. Our method overcomes existing limitations on model
architecture and effectiveness, achieving state-of-the-art results in both view
selection and uncertainty quantification, demonstrating its potential to
advance the field of Radiance Fields. Our method with the 3D Gaussian Splatting
backend could perform view selections at 70 fps.";Wen Jiang<author:sep>Boshu Lei<author:sep>Kostas Daniilidis;http://arxiv.org/pdf/2311.17874v1;cs.CV;Project page: https://jiangwenpl.github.io/FisherRF/;gaussian splatting<tag:sep>nerf
2311.17590v1;http://arxiv.org/abs/2311.17590v1;2023-11-29;SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis;"Achieving high synchronization in the synthesis of realistic, speech-driven
talking head videos presents a significant challenge. Traditional Generative
Adversarial Networks (GAN) struggle to maintain consistent facial identity,
while Neural Radiance Fields (NeRF) methods, although they can address this
issue, often produce mismatched lip movements, inadequate facial expressions,
and unstable head poses. A lifelike talking head requires synchronized
coordination of subject identity, lip movements, facial expressions, and head
poses. The absence of these synchronizations is a fundamental flaw, leading to
unrealistic and artificial outcomes. To address the critical issue of
synchronization, identified as the ""devil"" in creating realistic talking heads,
we introduce SyncTalk. This NeRF-based method effectively maintains subject
identity, enhancing synchronization and realism in talking head synthesis.
SyncTalk employs a Face-Sync Controller to align lip movements with speech and
innovatively uses a 3D facial blendshape model to capture accurate facial
expressions. Our Head-Sync Stabilizer optimizes head poses, achieving more
natural head movements. The Portrait-Sync Generator restores hair details and
blends the generated head with the torso for a seamless visual experience.
Extensive experiments and user studies demonstrate that SyncTalk outperforms
state-of-the-art methods in synchronization and realism. We recommend watching
the supplementary video: https://ziqiaopeng.github.io/synctalk";Ziqiao Peng<author:sep>Wentao Hu<author:sep>Yue Shi<author:sep>Xiangyu Zhu<author:sep>Xiaomei Zhang<author:sep>Hao Zhao<author:sep>Jun He<author:sep>Hongyan Liu<author:sep>Zhaoxin Fan;http://arxiv.org/pdf/2311.17590v1;cs.CV;11 pages, 5 figures;nerf
2311.17910v1;http://arxiv.org/abs/2311.17910v1;2023-11-29;HUGS: Human Gaussian Splats;"Recent advances in neural rendering have improved both training and rendering
times by orders of magnitude. While these methods demonstrate state-of-the-art
quality and speed, they are designed for photogrammetry of static scenes and do
not generalize well to freely moving humans in the environment. In this work,
we introduce Human Gaussian Splats (HUGS) that represents an animatable human
together with the scene using 3D Gaussian Splatting (3DGS). Our method takes
only a monocular video with a small number of (50-100) frames, and it
automatically learns to disentangle the static scene and a fully animatable
human avatar within 30 minutes. We utilize the SMPL body model to initialize
the human Gaussians. To capture details that are not modeled by SMPL (e.g.
cloth, hairs), we allow the 3D Gaussians to deviate from the human body model.
Utilizing 3D Gaussians for animated humans brings new challenges, including the
artifacts created when articulating the Gaussians. We propose to jointly
optimize the linear blend skinning weights to coordinate the movements of
individual Gaussians during animation. Our approach enables novel-pose
synthesis of human and novel view synthesis of both the human and the scene. We
achieve state-of-the-art rendering quality with a rendering speed of 60 FPS
while being ~100x faster to train over previous work. Our code will be
announced here: https://github.com/apple/ml-hugs";Muhammed Kocabas<author:sep>Jen-Hao Rick Chang<author:sep>James Gabriel<author:sep>Oncel Tuzel<author:sep>Anurag Ranjan;http://arxiv.org/pdf/2311.17910v1;cs.CV;;gaussian splatting
2311.17061v1;http://arxiv.org/abs/2311.17061v1;2023-11-28;HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting;"Realistic 3D human generation from text prompts is a desirable yet
challenging task. Existing methods optimize 3D representations like mesh or
neural fields via score distillation sampling (SDS), which suffers from
inadequate fine details or excessive training time. In this paper, we propose
an efficient yet effective framework, HumanGaussian, that generates
high-quality 3D humans with fine-grained geometry and realistic appearance. Our
key insight is that 3D Gaussian Splatting is an efficient renderer with
periodic Gaussian shrinkage or growing, where such adaptive density control can
be naturally guided by intrinsic human structures. Specifically, 1) we first
propose a Structure-Aware SDS that simultaneously optimizes human appearance
and geometry. The multi-modal score function from both RGB and depth space is
leveraged to distill the Gaussian densification and pruning process. 2)
Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS
into a noisier generative score and a cleaner classifier score, which well
addresses the over-saturation issue. The floating artifacts are further
eliminated based on Gaussian size in a prune-only phase to enhance generation
smoothness. Extensive experiments demonstrate the superior efficiency and
competitive quality of our framework, rendering vivid 3D humans under diverse
scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian";Xian Liu<author:sep>Xiaohang Zhan<author:sep>Jiaxiang Tang<author:sep>Ying Shan<author:sep>Gang Zeng<author:sep>Dahua Lin<author:sep>Xihui Liu<author:sep>Ziwei Liu;http://arxiv.org/pdf/2311.17061v1;cs.CV;Project Page: https://alvinliu0.github.io/projects/HumanGaussian;gaussian splatting
2311.16671v1;http://arxiv.org/abs/2311.16671v1;2023-11-28;SplitNeRF: Split Sum Approximation Neural Field for Joint Geometry,  Illumination, and Material Estimation;"We present a novel approach for digitizing real-world objects by estimating
their geometry, material properties, and environmental lighting from a set of
posed images with fixed lighting. Our method incorporates into Neural Radiance
Field (NeRF) pipelines the split sum approximation used with image-based
lighting for real-time physical-based rendering. We propose modeling the
scene's lighting with a single scene-specific MLP representing pre-integrated
image-based lighting at arbitrary resolutions. We achieve accurate modeling of
pre-integrated lighting by exploiting a novel regularizer based on efficient
Monte Carlo sampling. Additionally, we propose a new method of supervising
self-occlusion predictions by exploiting a similar regularizer based on Monte
Carlo sampling. Experimental results demonstrate the efficiency and
effectiveness of our approach in estimating scene geometry, material
properties, and lighting. Our method is capable of attaining state-of-the-art
relighting quality after only ${\sim}1$ hour of training in a single NVIDIA
A100 GPU.";Jesus Zarzar<author:sep>Bernard Ghanem;http://arxiv.org/pdf/2311.16671v1;cs.CV;;nerf
2311.16737v1;http://arxiv.org/abs/2311.16737v1;2023-11-28;Point'n Move: Interactive Scene Object Manipulation on Gaussian  Splatting Radiance Fields;"We propose Point'n Move, a method that achieves interactive scene object
manipulation with exposed region inpainting. Interactivity here further comes
from intuitive object selection and real-time editing. To achieve this, we
adopt Gaussian Splatting Radiance Field as the scene representation and fully
leverage its explicit nature and speed advantage. Its explicit representation
formulation allows us to devise a 2D prompt points to 3D mask dual-stage
self-prompting segmentation algorithm, perform mask refinement and merging,
minimize change as well as provide good initialization for scene inpainting and
perform editing in real-time without per-editing training, all leads to
superior quality and performance. We test our method by performing editing on
both forward-facing and 360 scenes. We also compare our method against existing
scene object removal methods, showing superior quality despite being more
capable and having a speed advantage.";Jiajun Huang<author:sep>Hongchuan Yu;http://arxiv.org/pdf/2311.16737v1;cs.CV;;gaussian splatting
2311.16504v1;http://arxiv.org/abs/2311.16504v1;2023-11-28;Rethinking Directional Integration in Neural Radiance Fields;"Recent works use the Neural radiance field (NeRF) to perform multi-view 3D
reconstruction, providing a significant leap in rendering photorealistic
scenes. However, despite its efficacy, NeRF exhibits limited capability of
learning view-dependent effects compared to light field rendering or
image-based view synthesis. To that end, we introduce a modification to the
NeRF rendering equation which is as simple as a few lines of code change for
any NeRF variations, while greatly improving the rendering quality of
view-dependent effects. By swapping the integration operator and the direction
decoder network, we only integrate the positional features along the ray and
move the directional terms out of the integration, resulting in a
disentanglement of the view-dependent and independent components. The modified
equation is equivalent to the classical volumetric rendering in ideal cases on
object surfaces with Dirac densities. Furthermore, we prove that with the
errors caused by network approximation and numerical integration, our rendering
equation exhibits better convergence properties with lower error accumulations
compared to the classical NeRF. We also show that the modified equation can be
interpreted as light field rendering with learned ray embeddings. Experiments
on different NeRF variations show consistent improvements in the quality of
view-dependent effects with our simple modification.";Congyue Deng<author:sep>Jiawei Yang<author:sep>Leonidas Guibas<author:sep>Yue Wang;http://arxiv.org/pdf/2311.16504v1;cs.CV;;nerf
2311.17116v3;http://arxiv.org/abs/2311.17116v3;2023-11-28;REF$^2$-NeRF: Reflection and Refraction aware Neural Radiance Field;"Recently, significant progress has been made in the study of methods for 3D
reconstruction from multiple images using implicit neural representations,
exemplified by the neural radiance field (NeRF) method. Such methods, which are
based on volume rendering, can model various light phenomena, and various
extended methods have been proposed to accommodate different scenes and
situations. However, when handling scenes with multiple glass objects, e.g.,
objects in a glass showcase, modeling the target scene accurately has been
challenging due to the presence of multiple reflection and refraction effects.
Thus, this paper proposes a NeRF-based modeling method for scenes containing a
glass case. In the proposed method, refraction and reflection are modeled using
elements that are dependent and independent of the viewer's perspective. This
approach allows us to estimate the surfaces where refraction occurs, i.e.,
glass surfaces, and enables the separation and modeling of both direct and
reflected light components. Compared to existing methods, the proposed method
enables more accurate modeling of both glass refraction and the overall scene.";Wooseok Kim<author:sep>Taiki Fukiage<author:sep>Takeshi Oishi;http://arxiv.org/pdf/2311.17116v3;cs.CV;10 pages, 8 figures, 2 tables;nerf
2311.16854v2;http://arxiv.org/abs/2311.16854v2;2023-11-28;A Unified Approach for Text- and Image-guided 4D Scene Generation;"Large-scale diffusion generative models are greatly simplifying image, video
and 3D asset creation from user-provided text prompts and images. However, the
challenging problem of text-to-4D dynamic 3D scene generation with diffusion
guidance remains largely unexplored. We propose Dream-in-4D, which features a
novel two-stage approach for text-to-4D synthesis, leveraging (1) 3D and 2D
diffusion guidance to effectively learn a high-quality static 3D asset in the
first stage; (2) a deformable neural radiance field that explicitly
disentangles the learned static asset from its deformation, preserving quality
during motion learning; and (3) a multi-resolution feature grid for the
deformation field with a displacement total variation loss to effectively learn
motion with video diffusion guidance in the second stage. Through a user
preference study, we demonstrate that our approach significantly advances image
and motion quality, 3D consistency and text fidelity for text-to-4D generation
compared to baseline approaches. Thanks to its motion-disentangled
representation, Dream-in-4D can also be easily adapted for controllable
generation where appearance is defined by one or multiple images, without the
need to modify the motion learning stage. Thus, our method offers, for the
first time, a unified approach for text-to-4D, image-to-4D and personalized 4D
generation tasks.";Yufeng Zheng<author:sep>Xueting Li<author:sep>Koki Nagano<author:sep>Sifei Liu<author:sep>Karsten Kreis<author:sep>Otmar Hilliges<author:sep>Shalini De Mello;http://arxiv.org/pdf/2311.16854v2;cs.CV;Project page: https://research.nvidia.com/labs/nxp/dream-in-4d/;
2311.16592v1;http://arxiv.org/abs/2311.16592v1;2023-11-28;RGBGrasp: Image-based Object Grasping by Capturing Multiple Views during  Robot Arm Movement with Neural Radiance Fields;"Robotic research encounters a significant hurdle when it comes to the
intricate task of grasping objects that come in various shapes, materials, and
textures. Unlike many prior investigations that heavily leaned on specialized
point-cloud cameras or abundant RGB visual data to gather 3D insights for
object-grasping missions, this paper introduces a pioneering approach called
RGBGrasp. This method depends on a limited set of RGB views to perceive the 3D
surroundings containing transparent and specular objects and achieve accurate
grasping. Our method utilizes pre-trained depth prediction models to establish
geometry constraints, enabling precise 3D structure estimation, even under
limited view conditions. Finally, we integrate hash encoding and a proposal
sampler strategy to significantly accelerate the 3D reconstruction process.
These innovations significantly enhance the adaptability and effectiveness of
our algorithm in real-world scenarios. Through comprehensive experimental
validation, we demonstrate that RGBGrasp achieves remarkable success across a
wide spectrum of object-grasping scenarios, establishing it as a promising
solution for real-world robotic manipulation tasks. The demo of our method can
be found on: https://sites.google.com/view/rgbgrasp";Chang Liu<author:sep>Kejian Shi<author:sep>Kaichen Zhou<author:sep>Haoxiao Wang<author:sep>Jiyao Zhang<author:sep>Hao Dong;http://arxiv.org/pdf/2311.16592v1;cs.RO;;
2311.17245v3;http://arxiv.org/abs/2311.17245v3;2023-11-28;LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and  200+ FPS;"Recent advancements in real-time neural rendering using point-based
techniques have paved the way for the widespread adoption of 3D
representations. However, foundational approaches like 3D Gaussian Splatting
come with a substantial storage overhead caused by growing the SfM points to
millions, often demanding gigabyte-level disk space for a single unbounded
scene, posing significant scalability challenges and hindering the splatting
efficiency.
  To address this challenge, we introduce LightGaussian, a novel method
designed to transform 3D Gaussians into a more efficient and compact format.
Drawing inspiration from the concept of Network Pruning, LightGaussian
identifies Gaussians that are insignificant in contributing to the scene
reconstruction and adopts a pruning and recovery process, effectively reducing
redundancy in Gaussian counts while preserving visual effects. Additionally,
LightGaussian employs distillation and pseudo-view augmentation to distill
spherical harmonics to a lower degree, allowing knowledge transfer to more
compact representations while maintaining reflectance. Furthermore, we propose
a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in
lower bitwidth representations with minimal accuracy losses.
  In summary, LightGaussian achieves an averaged compression rate over 15x
while boosting the FPS from 139 to 215, enabling an efficient representation of
complex scenes on Mip-NeRF 360, Tank and Temple datasets.
  Project website: https://lightgaussian.github.io/";Zhiwen Fan<author:sep>Kevin Wang<author:sep>Kairun Wen<author:sep>Zehao Zhu<author:sep>Dejia Xu<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2311.17245v3;cs.CV;16pages, 8figures;gaussian splatting<tag:sep>nerf
2311.17089v1;http://arxiv.org/abs/2311.17089v1;2023-11-28;Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering;"3D Gaussians have recently emerged as a highly efficient representation for
3D reconstruction and rendering. Despite its high rendering quality and speed
at high resolutions, they both deteriorate drastically when rendered at lower
resolutions or from far away camera position. During low resolution or far away
rendering, the pixel size of the image can fall below the Nyquist frequency
compared to the screen size of each splatted 3D Gaussian and leads to aliasing
effect. The rendering is also drastically slowed down by the sequential alpha
blending of more splatted Gaussians per pixel. To address these issues, we
propose a multi-scale 3D Gaussian splatting algorithm, which maintains
Gaussians at different scales to represent the same scene. Higher-resolution
images are rendered with more small Gaussians, and lower-resolution images are
rendered with fewer larger Gaussians. With similar training time, our algorithm
can achieve 13\%-66\% PSNR and 160\%-2400\% rendering speed improvement at
4$\times$-128$\times$ scale rendering on Mip-NeRF360 dataset compared to the
single scale 3D Gaussian splatting.";Zhiwen Yan<author:sep>Weng Fei Low<author:sep>Yu Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2311.17089v1;cs.CV;;gaussian splatting<tag:sep>nerf
2311.16657v1;http://arxiv.org/abs/2311.16657v1;2023-11-28;SCALAR-NeRF: SCAlable LARge-scale Neural Radiance Fields for Scene  Reconstruction;"In this work, we introduce SCALAR-NeRF, a novel framework tailored for
scalable large-scale neural scene reconstruction. We structure the neural
representation as an encoder-decoder architecture, where the encoder processes
3D point coordinates to produce encoded features, and the decoder generates
geometric values that include volume densities of signed distances and colors.
Our approach first trains a coarse global model on the entire image dataset.
Subsequently, we partition the images into smaller blocks using KMeans with
each block being modeled by a dedicated local model. We enhance the overlapping
regions across different blocks by scaling up the bounding boxes of each local
block. Notably, the decoder from the global model is shared across distinct
blocks and therefore promoting alignment in the feature space of local
encoders. We propose an effective and efficient methodology to fuse the outputs
from these local models to attain the final reconstruction. Employing this
refined coarse-to-fine strategy, our method outperforms state-of-the-art NeRF
methods and demonstrates scalability for large-scale scene reconstruction. The
code will be available on our project page at
https://aibluefisher.github.io/SCALAR-NeRF/";Yu Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2311.16657v1;cs.CV;Project Page: https://aibluefisher.github.io/SCALAR-NeRF;nerf
2311.16945v2;http://arxiv.org/abs/2311.16945v2;2023-11-28;UC-NeRF: Neural Radiance Field for Under-Calibrated Multi-view Cameras  in Autonomous Driving;"Multi-camera setups find widespread use across various applications, such as
autonomous driving, as they greatly expand sensing capabilities. Despite the
fast development of Neural radiance field (NeRF) techniques and their wide
applications in both indoor and outdoor scenes, applying NeRF to multi-camera
systems remains very challenging. This is primarily due to the inherent
under-calibration issues in multi-camera setup, including inconsistent imaging
effects stemming from separately calibrated image signal processing units in
diverse cameras, and system errors arising from mechanical vibrations during
driving that affect relative camera poses. In this paper, we present UC-NeRF, a
novel method tailored for novel view synthesis in under-calibrated multi-view
camera systems. Firstly, we propose a layer-based color correction to rectify
the color inconsistency in different image regions. Second, we propose virtual
warping to generate more viewpoint-diverse but color-consistent virtual views
for color correction and 3D recovery. Finally, a spatiotemporally constrained
pose refinement is designed for more robust and accurate pose calibration in
multi-camera systems. Our method not only achieves state-of-the-art performance
of novel view synthesis in multi-camera setups, but also effectively
facilitates depth estimation in large-scale outdoor scenes with the synthesized
novel views.";Kai Cheng<author:sep>Xiaoxiao Long<author:sep>Wei Yin<author:sep>Jin Wang<author:sep>Zhiqiang Wu<author:sep>Yuexin Ma<author:sep>Kaixuan Wang<author:sep>Xiaozhi Chen<author:sep>Xuejin Chen;http://arxiv.org/pdf/2311.16945v2;cs.CV;"See the project page for code, data:
  https://kcheng1021.github.io/ucnerf.github.io";nerf
2311.16664v1;http://arxiv.org/abs/2311.16664v1;2023-11-28;DGNR: Density-Guided Neural Point Rendering of Large Driving Scenes;"Despite the recent success of Neural Radiance Field (NeRF), it is still
challenging to render large-scale driving scenes with long trajectories,
particularly when the rendering quality and efficiency are in high demand.
Existing methods for such scenes usually involve with spatial warping,
geometric supervision from zero-shot normal or depth estimation, or scene
division strategies, where the synthesized views are often blurry or fail to
meet the requirement of efficient rendering. To address the above challenges,
this paper presents a novel framework that learns a density space from the
scenes to guide the construction of a point-based renderer, dubbed as DGNR
(Density-Guided Neural Rendering). In DGNR, geometric priors are no longer
needed, which can be intrinsically learned from the density space through
volumetric rendering. Specifically, we make use of a differentiable renderer to
synthesize images from the neural density features obtained from the learned
density space. A density-based fusion module and geometric regularization are
proposed to optimize the density space. By conducting experiments on a widely
used autonomous driving dataset, we have validated the effectiveness of DGNR in
synthesizing photorealistic driving scenes and achieving real-time capable
rendering.";Zhuopeng Li<author:sep>Chenming Wu<author:sep>Liangjun Zhang<author:sep>Jianke Zhu;http://arxiv.org/pdf/2311.16664v1;cs.CV;;nerf
2311.16937v1;http://arxiv.org/abs/2311.16937v1;2023-11-28;The Sky's the Limit: Re-lightable Outdoor Scenes via a Sky-pixel  Constrained Illumination Prior and Outside-In Visibility;"Inverse rendering of outdoor scenes from unconstrained image collections is a
challenging task, particularly illumination/albedo ambiguities and occlusion of
the illumination environment (shadowing) caused by geometry. However, there are
many cues in an image that can aid in the disentanglement of geometry, albedo
and shadows. We exploit the fact that any sky pixel provides a direct
measurement of distant lighting in the corresponding direction and, via a
neural illumination prior, a statistical cue as to the remaining illumination
environment. We also introduce a novel `outside-in' method for computing
differentiable sky visibility based on a neural directional distance function.
This is efficient and can be trained in parallel with the neural scene
representation, allowing gradients from appearance loss to flow from shadows to
influence estimation of illumination and geometry. Our method estimates
high-quality albedo, geometry, illumination and sky visibility, achieving
state-of-the-art results on the NeRF-OSR relighting benchmark. Our code and
models can be found https://github.com/JADGardner/neusky";James A. D. Gardner<author:sep>Evgenii Kashin<author:sep>Bernhard Egger<author:sep>William A. P. Smith;http://arxiv.org/pdf/2311.16937v1;cs.CV;;nerf
2311.17113v1;http://arxiv.org/abs/2311.17113v1;2023-11-28;Human Gaussian Splatting: Real-time Rendering of Animatable Avatars;"This work addresses the problem of real-time rendering of photorealistic
human body avatars learned from multi-view videos. While the classical
approaches to model and render virtual humans generally use a textured mesh,
recent research has developed neural body representations that achieve
impressive visual quality. However, these models are difficult to render in
real-time and their quality degrades when the character is animated with body
poses different than the training observations. We propose the first animatable
human model based on 3D Gaussian Splatting, that has recently emerged as a very
efficient alternative to neural radiance fields. Our body is represented by a
set of gaussian primitives in a canonical space which are deformed in a coarse
to fine approach that combines forward skinning and local non-rigid refinement.
We describe how to learn our Human Gaussian Splatting (\OURS) model in an
end-to-end fashion from multi-view observations, and evaluate it against the
state-of-the-art approaches for novel pose synthesis of clothed body. Our
method presents a PSNR 1.5dbB better than the state-of-the-art on THuman4
dataset while being able to render at 20fps or more.";Arthur Moreau<author:sep>Jifei Song<author:sep>Helisa Dhamo<author:sep>Richard Shaw<author:sep>Yiren Zhou<author:sep>Eduardo PÃ©rez-Pellitero;http://arxiv.org/pdf/2311.17113v1;cs.CV;;gaussian splatting
2311.17119v1;http://arxiv.org/abs/2311.17119v1;2023-11-28;Continuous Pose for Monocular Cameras in Neural Implicit Representation;"In this paper, we showcase the effectiveness of optimizing monocular camera
poses as a continuous function of time. The camera poses are represented using
an implicit neural function which maps the given time to the corresponding
camera pose. The mapped camera poses are then used for the downstream tasks
where joint camera pose optimization is also required. While doing so, the
network parameters -- that implicitly represent camera poses -- are optimized.
We exploit the proposed method in four diverse experimental settings, namely,
(1) NeRF from noisy poses; (2) NeRF from asynchronous Events; (3) Visual
Simultaneous Localization and Mapping (vSLAM); and (4) vSLAM with IMUs. In all
four settings, the proposed method performs significantly better than the
compared baselines and the state-of-the-art methods. Additionally, using the
assumption of continuous motion, changes in pose may actually live in a
manifold that has lower than 6 degrees of freedom (DOF) is also realized. We
call this low DOF motion representation as the \emph{intrinsic motion} and use
the approach in vSLAM settings, showing impressive camera tracking performance.";Qi Ma<author:sep>Danda Pani Paudel<author:sep>Ajad Chhatkuli<author:sep>Luc Van Gool;http://arxiv.org/pdf/2311.17119v1;cs.CV;;nerf
2311.16043v1;http://arxiv.org/abs/2311.16043v1;2023-11-27;Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF  Decomposition and Ray Tracing;"We present a novel differentiable point-based rendering framework for
material and lighting decomposition from multi-view images, enabling editing,
ray-tracing, and real-time relighting of the 3D point cloud. Specifically, a 3D
scene is represented as a set of relightable 3D Gaussian points, where each
point is additionally associated with a normal direction, BRDF parameters, and
incident lights from different directions. To achieve robust lighting
estimation, we further divide incident lights of each point into global and
local components, as well as view-dependent visibilities. The 3D scene is
optimized through the 3D Gaussian Splatting technique while BRDF and lighting
are decomposed by physically-based differentiable rendering. Moreover, we
introduce an innovative point-based ray-tracing approach based on the bounding
volume hierarchy for efficient visibility baking, enabling real-time rendering
and relighting of 3D Gaussian points with accurate shadow effects. Extensive
experiments demonstrate improved BRDF estimation and novel view rendering
results compared to state-of-the-art material estimation approaches. Our
framework showcases the potential to revolutionize the mesh-based graphics
pipeline with a relightable, traceable, and editable rendering pipeline solely
based on point cloud. Project
page:https://nju-3dv.github.io/projects/Relightable3DGaussian/.";Jian Gao<author:sep>Chun Gu<author:sep>Youtian Lin<author:sep>Hao Zhu<author:sep>Xun Cao<author:sep>Li Zhang<author:sep>Yao Yao;http://arxiv.org/pdf/2311.16043v1;cs.CV;;gaussian splatting
2311.15803v2;http://arxiv.org/abs/2311.15803v2;2023-11-27;SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using  Neural Radiance Fields;"In rapidly-evolving domains such as autonomous driving, the use of multiple
sensors with different modalities is crucial to ensure high operational
precision and stability. To correctly exploit the provided information by each
sensor in a single common frame, it is essential for these sensors to be
accurately calibrated. In this paper, we leverage the ability of Neural
Radiance Fields (NeRF) to represent different sensors modalities in a common
volumetric representation to achieve robust and accurate spatio-temporal sensor
calibration. By designing a partitioning approach based on the visible part of
the scene for each sensor, we formulate the calibration problem using only the
overlapping areas. This strategy results in a more robust and accurate
calibration that is less prone to failure. We demonstrate that our approach
works on outdoor urban scenes by validating it on multiple established driving
datasets. Results show that our method is able to get better accuracy and
robustness compared to existing methods.";Quentin Herau<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Luis RoldÃ£o<author:sep>Dzmitry Tsishkou<author:sep>Cyrille Migniot<author:sep>Pascal Vasseur<author:sep>CÃ©dric Demonceaux;http://arxiv.org/pdf/2311.15803v2;cs.CV;"Paper + Supplementary, under review. Project page:
  https://qherau.github.io/SOAC/";nerf
2311.16499v1;http://arxiv.org/abs/2311.16499v1;2023-11-27;Deceptive-Human: Prompt-to-NeRF 3D Human Generation with 3D-Consistent  Synthetic Images;"This paper presents Deceptive-Human, a novel Prompt-to-NeRF framework
capitalizing state-of-the-art control diffusion models (e.g., ControlNet) to
generate a high-quality controllable 3D human NeRF. Different from direct 3D
generative approaches, e.g., DreamFusion and DreamHuman, Deceptive-Human
employs a progressive refinement technique to elevate the reconstruction
quality. This is achieved by utilizing high-quality synthetic human images
generated through the ControlNet with view-consistent loss. Our method is
versatile and readily extensible, accommodating multimodal inputs, including a
text prompt and additional data such as 3D mesh, poses, and seed images. The
resulting 3D human NeRF model empowers the synthesis of highly photorealistic
novel views from 360-degree perspectives. The key to our Deceptive-Human for
hallucinating multi-view consistent synthetic human images lies in our
progressive finetuning strategy. This strategy involves iteratively enhancing
views using the provided multimodal inputs at each intermediate step to improve
the human NeRF model. Within this iterative refinement process, view-dependent
appearances are systematically eliminated to prevent interference with the
underlying density estimation. Extensive qualitative and quantitative
experimental comparison shows that our deceptive human models achieve
state-of-the-art application quality.";Shiu-hong Kao<author:sep>Xinhang Liu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2311.16499v1;cs.CV;Github project: https://github.com/DanielSHKao/DeceptiveHuman;nerf
2311.15510v1;http://arxiv.org/abs/2311.15510v1;2023-11-27;CaesarNeRF: Calibrated Semantic Representation for Few-shot  Generalizable Neural Rendering;"Generalizability and few-shot learning are key challenges in Neural Radiance
Fields (NeRF), often due to the lack of a holistic understanding in pixel-level
rendering. We introduce CaesarNeRF, an end-to-end approach that leverages
scene-level CAlibratEd SemAntic Representation along with pixel-level
representations to advance few-shot, generalizable neural rendering,
facilitating a holistic understanding without compromising high-quality
details. CaesarNeRF explicitly models pose differences of reference views to
combine scene-level semantic representations, providing a calibrated holistic
understanding. This calibration process aligns various viewpoints with precise
location and is further enhanced by sequential refinement to capture varying
details. Extensive experiments on public datasets, including LLFF, Shiny,
mip-NeRF 360, and MVImgNet, show that CaesarNeRF delivers state-of-the-art
performance across varying numbers of reference views, proving effective even
with a single reference image. The project page of this work can be found at
https://haidongz-usc.github.io/project/caesarnerf.";Haidong Zhu<author:sep>Tianyu Ding<author:sep>Tianyi Chen<author:sep>Ilya Zharkov<author:sep>Ram Nevatia<author:sep>Luming Liang;http://arxiv.org/pdf/2311.15510v1;cs.CV;;nerf
2311.16482v2;http://arxiv.org/abs/2311.16482v2;2023-11-27;Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple  Human Avatars;"Neural radiance fields are capable of reconstructing high-quality drivable
human avatars but are expensive to train and render. To reduce consumption, we
propose Animatable 3D Gaussian, which learns human avatars from input images
and poses. We extend 3D Gaussians to dynamic human scenes by modeling a set of
skinned 3D Gaussians and a corresponding skeleton in canonical space and
deforming 3D Gaussians to posed space according to the input poses. We
introduce hash-encoded shape and appearance to speed up training and propose
time-dependent ambient occlusion to achieve high-quality reconstructions in
scenes containing complex motions and dynamic shadows. On both novel view
synthesis and novel pose synthesis tasks, our method outperforms existing
methods in terms of training time, rendering speed, and reconstruction quality.
Our method can be easily extended to multi-human scenes and achieve comparable
novel view synthesis results on a scene with ten people in only 25 seconds of
training.";Yang Liu<author:sep>Xiang Huang<author:sep>Minghan Qin<author:sep>Qinwei Lin<author:sep>Haoqian Wang;http://arxiv.org/pdf/2311.16482v2;cs.CV;;
2311.15637v1;http://arxiv.org/abs/2311.15637v1;2023-11-27;PaintNeSF: Artistic Creation of Stylized Scenes with Vectorized 3D  Strokes;"We present Paint Neural Stroke Field (PaintNeSF), a novel technique to
generate stylized images of a 3D scene at arbitrary novel views from multi-view
2D images. Different from existing methods which apply stylization to trained
neural radiance fields at the voxel level, our approach draws inspiration from
image-to-painting methods, simulating the progressive painting process of human
artwork with vector strokes. We develop a palette of stylized 3D strokes from
basic primitives and splines, and consider the 3D scene stylization task as a
multi-view reconstruction process based on these 3D stroke primitives. Instead
of directly searching for the parameters of these 3D strokes, which would be
too costly, we introduce a differentiable renderer that allows optimizing
stroke parameters using gradient descent, and propose a training scheme to
alleviate the vanishing gradient issue. The extensive evaluation demonstrates
that our approach effectively synthesizes 3D scenes with significant geometric
and aesthetic stylization while maintaining a consistent appearance across
different views. Our method can be further integrated with style loss and
image-text contrastive models to extend its applications, including color
transfer and text-driven 3D scene drawing.";Hao-Bin Duan<author:sep>Miao Wang<author:sep>Yan-Xun Li<author:sep>Yong-Liang Yang;http://arxiv.org/pdf/2311.15637v1;cs.CV;;
2311.16096v1;http://arxiv.org/abs/2311.16096v1;2023-11-27;Animatable Gaussians: Learning Pose-dependent Gaussian Maps for  High-fidelity Human Avatar Modeling;"Modeling animatable human avatars from RGB videos is a long-standing and
challenging problem. Recent works usually adopt MLP-based neural radiance
fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to
regress pose-dependent garment details. To this end, we introduce Animatable
Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D
Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians
with the animatable avatar, we learn a parametric template from the input
videos, and then parameterize the template on two front \& back canonical
Gaussian maps where each pixel represents a 3D Gaussian. The learned template
is adaptive to the wearing garments for modeling looser clothes like dresses.
Such template-guided 2D parameterization enables us to employ a powerful
StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling
detailed dynamic appearances. Furthermore, we introduce a pose projection
strategy for better generalization given novel poses. Overall, our method can
create lifelike avatars with dynamic, realistic and generalized appearances.
Experiments show that our method outperforms other state-of-the-art approaches.
Code: https://github.com/lizhe00/AnimatableGaussians";Zhe Li<author:sep>Zerong Zheng<author:sep>Lizhen Wang<author:sep>Yebin Liu;http://arxiv.org/pdf/2311.16096v1;cs.CV;"Projectpage: https://animatable-gaussians.github.io/, Code:
  https://github.com/lizhe00/AnimatableGaussians";gaussian splatting<tag:sep>nerf
2311.16493v1;http://arxiv.org/abs/2311.16493v1;2023-11-27;Mip-Splatting: Alias-free 3D Gaussian Splatting;"Recently, 3D Gaussian Splatting has demonstrated impressive novel view
synthesis results, reaching high fidelity and efficiency. However, strong
artifacts can be observed when changing the sampling rate, \eg, by changing
focal length or camera distance. We find that the source for this phenomenon
can be attributed to the lack of 3D frequency constraints and the usage of a 2D
dilation filter. To address this problem, we introduce a 3D smoothing filter
which constrains the size of the 3D Gaussian primitives based on the maximal
sampling frequency induced by the input views, eliminating high-frequency
artifacts when zooming in. Moreover, replacing 2D dilation with a 2D Mip
filter, which simulates a 2D box filter, effectively mitigates aliasing and
dilation issues. Our evaluation, including scenarios such a training on
single-scale images and testing on multiple scales, validates the effectiveness
of our approach.";Zehao Yu<author:sep>Anpei Chen<author:sep>Binbin Huang<author:sep>Torsten Sattler<author:sep>Andreas Geiger;http://arxiv.org/pdf/2311.16493v1;cs.CV;Project page: https://niujinshuchong.github.io/mip-splatting/;gaussian splatting
2311.16037v1;http://arxiv.org/abs/2311.16037v1;2023-11-27;GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions;"Recently, impressive results have been achieved in 3D scene editing with text
instructions based on a 2D diffusion model. However, current diffusion models
primarily generate images by predicting noise in the latent space, and the
editing is usually applied to the whole image, which makes it challenging to
perform delicate, especially localized, editing for 3D scenes. Inspired by
recent 3D Gaussian splatting, we propose a systematic framework, named
GaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text
instructions. Benefiting from the explicit property of 3D Gaussians, we design
a series of techniques to achieve delicate editing. Specifically, we first
extract the region of interest (RoI) corresponding to the text instruction,
aligning it to 3D Gaussians. The Gaussian RoI is further used to control the
editing process. Our framework can achieve more delicate and precise editing of
3D scenes than previous methods while enjoying much faster training speed, i.e.
within 20 minutes on a single V100 GPU, more than twice as fast as
Instruct-NeRF2NeRF (45 minutes -- 2 hours).";Jiemin Fang<author:sep>Junjie Wang<author:sep>Xiaopeng Zhang<author:sep>Lingxi Xie<author:sep>Qi Tian;http://arxiv.org/pdf/2311.16037v1;cs.CV;Project page: https://GaussianEditor.github.io;gaussian splatting<tag:sep>nerf
2311.15291v1;http://arxiv.org/abs/2311.15291v1;2023-11-26;Obj-NeRF: Extract Object NeRFs from Multi-view Images;"Neural Radiance Fields (NeRFs) have demonstrated remarkable effectiveness in
novel view synthesis within 3D environments. However, extracting a radiance
field of one specific object from multi-view images encounters substantial
challenges due to occlusion and background complexity, thereby presenting
difficulties in downstream applications such as NeRF editing and 3D mesh
extraction. To solve this problem, in this paper, we propose Obj-NeRF, a
comprehensive pipeline that recovers the 3D geometry of a specific object from
multi-view images using a single prompt. This method combines the 2D
segmentation capabilities of the Segment Anything Model (SAM) in conjunction
with the 3D reconstruction ability of NeRF. Specifically, we first obtain
multi-view segmentation for the indicated object using SAM with a single
prompt. Then, we use the segmentation images to supervise NeRF construction,
integrating several effective techniques. Additionally, we construct a large
object-level NeRF dataset containing diverse objects, which can be useful in
various downstream tasks. To demonstrate the practicality of our method, we
also apply Obj-NeRF to various applications, including object removal,
rotation, replacement, and recoloring.";Zhiyi Li<author:sep>Lihe Ding<author:sep>Tianfan Xue;http://arxiv.org/pdf/2311.15291v1;cs.CV;;nerf
2311.16473v2;http://arxiv.org/abs/2311.16473v2;2023-11-26;GS-IR: 3D Gaussian Splatting for Inverse Rendering;"We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian
Splatting (GS) that leverages forward mapping volume rendering to achieve
photorealistic novel view synthesis and relighting results. Unlike previous
works that use implicit neural representations and volume rendering (e.g.
NeRF), which suffer from low expressive power and high computational
complexity, we extend GS, a top-performance representation for novel view
synthesis, to estimate scene geometry, surface material, and environment
illumination from multi-view images captured under unknown lighting conditions.
There are two main problems when introducing GS to inverse rendering: 1) GS
does not support producing plausible normal natively; 2) forward mapping (e.g.
rasterization and splatting) cannot trace the occlusion like backward mapping
(e.g. ray tracing). To address these challenges, our GS-IR proposes an
efficient optimization scheme that incorporates a depth-derivation-based
regularization for normal estimation and a baking-based occlusion to model
indirect lighting. The flexible and expressive GS representation allows us to
achieve fast and compact geometry reconstruction, photorealistic novel view
synthesis, and effective physically-based rendering. We demonstrate the
superiority of our method over baseline methods through qualitative and
quantitative evaluations on various challenging scenes.";Zhihao Liang<author:sep>Qi Zhang<author:sep>Ying Feng<author:sep>Ying Shan<author:sep>Kui Jia;http://arxiv.org/pdf/2311.16473v2;cs.CV;;gaussian splatting<tag:sep>nerf
2311.15260v2;http://arxiv.org/abs/2311.15260v2;2023-11-26;NeuRAD: Neural Rendering for Autonomous Driving;"Neural radiance fields (NeRFs) have gained popularity in the autonomous
driving (AD) community. Recent methods show NeRFs' potential for closed-loop
simulation, enabling testing of AD systems, and as an advanced training data
augmentation technique. However, existing methods often require long training
times, dense semantic supervision, or lack generalizability. This, in turn,
hinders the application of NeRFs for AD at scale. In this paper, we propose
NeuRAD, a robust novel view synthesis method tailored to dynamic AD data. Our
method features simple network design, extensive sensor modeling for both
camera and lidar -- including rolling shutter, beam divergence and ray dropping
-- and is applicable to multiple datasets out of the box. We verify its
performance on five popular AD datasets, achieving state-of-the-art performance
across the board. To encourage further development, we will openly release the
NeuRAD source code. See https://github.com/georghess/NeuRAD .";Adam Tonderski<author:sep>Carl LindstrÃ¶m<author:sep>Georg Hess<author:sep>William Ljungbergh<author:sep>Lennart Svensson<author:sep>Christoffer Petersson;http://arxiv.org/pdf/2311.15260v2;cs.CV;;nerf
2311.15439v1;http://arxiv.org/abs/2311.15439v1;2023-11-26;Efficient Encoding of Graphics Primitives with Simplex-based Structures;"Grid-based structures are commonly used to encode explicit features for
graphics primitives such as images, signed distance functions (SDF), and neural
radiance fields (NeRF) due to their simple implementation. However, in
$n$-dimensional space, calculating the value of a sampled point requires
interpolating the values of its $2^n$ neighboring vertices. The exponential
scaling with dimension leads to significant computational overheads. To address
this issue, we propose a simplex-based approach for encoding graphics
primitives. The number of vertices in a simplex-based structure increases
linearly with dimension, making it a more efficient and generalizable
alternative to grid-based representations. Using the non-axis-aligned
simplicial structure property, we derive and prove a coordinate transformation,
simplicial subdivision, and barycentric interpolation scheme for efficient
sampling, which resembles transformation procedures in the simplex noise
algorithm. Finally, we use hash tables to store multiresolution features of all
interest points in the simplicial grid, which are passed into a tiny fully
connected neural network to parameterize graphics primitives. We implemented a
detailed simplex-based structure encoding algorithm in C++ and CUDA using the
methods outlined in our approach. In the 2D image fitting task, the proposed
method is capable of fitting a giga-pixel image with 9.4% less time compared to
the baseline method proposed by instant-ngp, while maintaining the same quality
and compression rate. In the volumetric rendering setup, we observe a maximum
41.2% speedup when the samples are dense enough.";Yibo Wen<author:sep>Yunfan Yang;http://arxiv.org/pdf/2311.15439v1;cs.CV;10 pages, 8 figures;nerf
2311.14603v1;http://arxiv.org/abs/2311.14603v1;2023-11-24;Animate124: Animating One Image to 4D Dynamic Scene;"We introduce Animate124 (Animate-one-image-to-4D), the first work to animate
a single in-the-wild image into 3D video through textual motion descriptions,
an underexplored problem with significant applications. Our 4D generation
leverages an advanced 4D grid dynamic Neural Radiance Field (NeRF) model,
optimized in three distinct stages using multiple diffusion priors. Initially,
a static model is optimized using the reference image, guided by 2D and 3D
diffusion priors, which serves as the initialization for the dynamic NeRF.
Subsequently, a video diffusion model is employed to learn the motion specific
to the subject. However, the object in the 3D videos tends to drift away from
the reference image over time. This drift is mainly due to the misalignment
between the text prompt and the reference image in the video diffusion model.
In the final stage, a personalized diffusion prior is therefore utilized to
address the semantic drift. As the pioneering image-text-to-4D generation
framework, our method demonstrates significant advancements over existing
baselines, evidenced by comprehensive quantitative and qualitative assessments.";Yuyang Zhao<author:sep>Zhiwen Yan<author:sep>Enze Xie<author:sep>Lanqing Hong<author:sep>Zhenguo Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2311.14603v1;cs.CV;Project Page: https://animate124.github.io;nerf
2311.14521v4;http://arxiv.org/abs/2311.14521v4;2023-11-24;GaussianEditor: Swift and Controllable 3D Editing with Gaussian  Splatting;"3D editing plays a crucial role in many areas such as gaming and virtual
reality. Traditional 3D editing methods, which rely on representations like
meshes and point clouds, often fall short in realistically depicting complex
scenes. On the other hand, methods based on implicit 3D representations, like
Neural Radiance Field (NeRF), render complex scenes effectively but suffer from
slow processing speeds and limited control over specific scene areas. In
response to these challenges, our paper presents GaussianEditor, an innovative
and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D
representation. GaussianEditor enhances precision and control in editing
through our proposed Gaussian semantic tracing, which traces the editing target
throughout the training process. Additionally, we propose Hierarchical Gaussian
splatting (HGS) to achieve stabilized and fine results under stochastic
generative guidance from 2D diffusion models. We also develop editing
strategies for efficient object removal and integration, a challenging task for
existing methods. Our comprehensive experiments demonstrate GaussianEditor's
superior control, efficacy, and rapid performance, marking a significant
advancement in 3D editing. Project Page:
https://buaacyw.github.io/gaussian-editor/";Yiwen Chen<author:sep>Zilong Chen<author:sep>Chi Zhang<author:sep>Feng Wang<author:sep>Xiaofeng Yang<author:sep>Yikai Wang<author:sep>Zhongang Cai<author:sep>Lei Yang<author:sep>Huaping Liu<author:sep>Guosheng Lin;http://arxiv.org/pdf/2311.14521v4;cs.CV;"Project Page: https://buaacyw.github.io/gaussian-editor/ Code:
  https://github.com/buaacyw/GaussianEditor";gaussian splatting<tag:sep>nerf
2311.13831v1;http://arxiv.org/abs/2311.13831v1;2023-11-23;Posterior Distillation Sampling;"We introduce Posterior Distillation Sampling (PDS), a novel optimization
method for parametric image editing based on diffusion models. Existing
optimization-based methods, which leverage the powerful 2D prior of diffusion
models to handle various parametric images, have mainly focused on generation.
Unlike generation, editing requires a balance between conforming to the target
attribute and preserving the identity of the source content. Recent 2D image
editing methods have achieved this balance by leveraging the stochastic latent
encoded in the generative process of diffusion models. To extend the editing
capabilities of diffusion models shown in pixel space to parameter space, we
reformulate the 2D image editing method into an optimization form named PDS.
PDS matches the stochastic latents of the source and the target, enabling the
sampling of targets in diverse parameter spaces that align with a desired
attribute while maintaining the source's identity. We demonstrate that this
optimization resembles running a generative process with the target attribute,
but aligning this process with the trajectory of the source's generative
process. Extensive editing results in Neural Radiance Fields and Scalable
Vector Graphics representations demonstrate that PDS is capable of sampling
targets to fulfill the aforementioned balance across various parameter spaces.";Juil Koo<author:sep>Chanho Park<author:sep>Minhyuk Sung;http://arxiv.org/pdf/2311.13831v1;cs.CV;Project page: https://posterior-distillation-sampling.github.io/;
2311.14153v1;http://arxiv.org/abs/2311.14153v1;2023-11-23;Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC  using Tube-Guided Data Augmentation and NeRFs;"Imitation learning (IL) can train computationally-efficient sensorimotor
policies from a resource-intensive Model Predictive Controller (MPC), but it
often requires many samples, leading to long training times or limited
robustness. To address these issues, we combine IL with a variant of robust MPC
that accounts for process and sensing uncertainties, and we design a data
augmentation (DA) strategy that enables efficient learning of vision-based
policies. The proposed DA method, named Tube-NeRF, leverages Neural Radiance
Fields (NeRFs) to generate novel synthetic images, and uses properties of the
robust MPC (the tube) to select relevant views and to efficiently compute the
corresponding actions. We tailor our approach to the task of localization and
trajectory tracking on a multirotor, by learning a visuomotor policy that
generates control actions using images from the onboard camera as only source
of horizontal position. Our evaluations numerically demonstrate learning of a
robust visuomotor policy with an 80-fold increase in demonstration efficiency
and a 50% reduction in training time over current IL methods. Additionally, our
policies successfully transfer to a real multirotor, achieving accurate
localization and low tracking errors despite large disturbances, with an
onboard inference time of only 1.5 ms.";Andrea Tagliabue<author:sep>Jonathan P. How;http://arxiv.org/pdf/2311.14153v1;cs.RO;"Video: https://youtu.be/_W5z33ZK1m4. Evolved paper from our previous
  work: arXiv:2210.10127";nerf
2311.13750v2;http://arxiv.org/abs/2311.13750v2;2023-11-23;Towards Transferable Multi-modal Perception Representation Learning for  Autonomy: NeRF-Supervised Masked AutoEncoder;"This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. We hope
this study can inspire exploration of more general multi-modal representation
learning for autonomous agents.";Xiaohao Xu;http://arxiv.org/pdf/2311.13750v2;cs.CV;;nerf
2311.14208v1;http://arxiv.org/abs/2311.14208v1;2023-11-23;ECRF: Entropy-Constrained Neural Radiance Fields Compression with  Frequency Domain Optimization;"Explicit feature-grid based NeRF models have shown promising results in terms
of rendering quality and significant speed-up in training. However, these
methods often require a significant amount of data to represent a single scene
or object. In this work, we present a compression model that aims to minimize
the entropy in the frequency domain in order to effectively reduce the data
size. First, we propose using the discrete cosine transform (DCT) on the
tensorial radiance fields to compress the feature-grid. This feature-grid is
transformed into coefficients, which are then quantized and entropy encoded,
following a similar approach to the traditional video coding pipeline.
Furthermore, to achieve a higher level of sparsity, we propose using an entropy
parameterization technique for the frequency domain, specifically for DCT
coefficients of the feature-grid. Since the transformed coefficients are
optimized during the training phase, the proposed model does not require any
fine-tuning or additional information. Our model only requires a lightweight
compression pipeline for encoding and decoding, making it easier to apply
volumetric radiance field methods for real-world applications. Experimental
results demonstrate that our proposed frequency domain entropy model can
achieve superior compression performance across various datasets. The source
code will be made publicly available.";Soonbin Lee<author:sep>Fangwen Shu<author:sep>Yago Sanchez<author:sep>Thomas Schierl<author:sep>Cornelius Hellge;http://arxiv.org/pdf/2311.14208v1;cs.CV;10 pages, 6 figures, 4 tables;nerf
2311.13168v1;http://arxiv.org/abs/2311.13168v1;2023-11-22;3D Face Style Transfer with a Hybrid Solution of NeRF and Mesh  Rasterization;"Style transfer for human face has been widely researched in recent years.
Majority of the existing approaches work in 2D image domain and have 3D
inconsistency issue when applied on different viewpoints of the same face. In
this paper, we tackle the problem of 3D face style transfer which aims at
generating stylized novel views of a 3D human face with multi-view consistency.
We propose to use a neural radiance field (NeRF) to represent 3D human face and
combine it with 2D style transfer to stylize the 3D face. We find that directly
training a NeRF on stylized images from 2D style transfer brings in 3D
inconsistency issue and causes blurriness. On the other hand, training a NeRF
jointly with 2D style transfer objectives shows poor convergence due to the
identity and head pose gap between style image and content image. It also poses
challenge in training time and memory due to the need of volume rendering for
full image to apply style transfer loss functions. We therefore propose a
hybrid framework of NeRF and mesh rasterization to combine the benefits of high
fidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our
framework consists of three stages: 1. Training a NeRF model on input face
images to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF
model and optimizing it with style transfer objectives via differentiable
rasterization; 3. Training a new color network in NeRF conditioned on a style
embedding to enable arbitrary style transfer to the 3D face. Experiment results
show that our approach generates high quality face style transfer with great 3D
consistency, while also enabling a flexible style control.";Jianwei Feng<author:sep>Prateek Singhal;http://arxiv.org/pdf/2311.13168v1;cs.CV;;nerf
2311.13384v2;http://arxiv.org/abs/2311.13384v2;2023-11-22;LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes;"With the widespread usage of VR devices and contents, demands for 3D scene
generation techniques become more popular. Existing 3D scene generation models,
however, limit the target scene to specific domain, primarily due to their
training strategies using 3D scan dataset that is far from the real-world. To
address such limitation, we propose LucidDreamer, a domain-free scene
generation pipeline by fully leveraging the power of existing large-scale
diffusion-based generative model. Our LucidDreamer has two alternate steps:
Dreaming and Alignment. First, to generate multi-view consistent images from
inputs, we set the point cloud as a geometrical guideline for each image
generation. Specifically, we project a portion of point cloud to the desired
view and provide the projection as a guidance for inpainting using the
generative model. The inpainted images are lifted to 3D space with estimated
depth maps, composing a new points. Second, to aggregate the new points into
the 3D scene, we propose an aligning algorithm which harmoniously integrates
the portions of newly generated 3D scenes. The finally obtained 3D scene serves
as initial points for optimizing Gaussian splats. LucidDreamer produces
Gaussian splats that are highly-detailed compared to the previous 3D scene
generation methods, with no constraint on domain of the target scene. Project
page: https://luciddreamer-cvlab.github.io/";Jaeyoung Chung<author:sep>Suyoung Lee<author:sep>Hyeongjin Nam<author:sep>Jaerin Lee<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2311.13384v2;cs.CV;Project page: https://luciddreamer-cvlab.github.io/;gaussian splatting
2311.13404v2;http://arxiv.org/abs/2311.13404v2;2023-11-22;Animatable 3D Gaussians for High-fidelity Synthesis of Human Motions;"We present a novel animatable 3D Gaussian model for rendering high-fidelity
free-view human motions in real time. Compared to existing NeRF-based methods,
the model owns better capability in synthesizing high-frequency details without
the jittering problem across video frames. The core of our model is a novel
augmented 3D Gaussian representation, which attaches each Gaussian with a
learnable code. The learnable code serves as a pose-dependent appearance
embedding for refining the erroneous appearance caused by geometric
transformation of Gaussians, based on which an appearance refinement model is
learned to produce residual Gaussian properties to match the appearance in
target pose. To force the Gaussians to learn the foreground human only without
background interference, we further design a novel alpha loss to explicitly
constrain the Gaussians within the human body. We also propose to jointly
optimize the human joint parameters to improve the appearance accuracy. The
animatable 3D Gaussian model can be learned with shallow MLPs, so new human
motions can be synthesized in real time (66 fps on avarage). Experiments show
that our model has superior performance over NeRF-based methods.";Keyang Ye<author:sep>Tianjia Shao<author:sep>Kun Zhou;http://arxiv.org/pdf/2311.13404v2;cs.CV;"Some experiment data is wrong. The expression of the paper in
  introduction and abstract is incorrect. Some graphs have inappropriate
  descriptions";nerf
2311.13617v1;http://arxiv.org/abs/2311.13617v1;2023-11-22;Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to  3D Prior with Progressive Learning;"We present Boosting3D, a multi-stage single image-to-3D generation method
that can robustly generate reasonable 3D objects in different data domains. The
point of this work is to solve the view consistency problem in single
image-guided 3D generation by modeling a reasonable geometric structure. For
this purpose, we propose to utilize better 3D prior to training the NeRF. More
specifically, we train an object-level LoRA for the target object using
original image and the rendering output of NeRF. And then we train the LoRA and
NeRF using a progressive training strategy. The LoRA and NeRF will boost each
other while training. After the progressive training, the LoRA learns the 3D
information of the generated object and eventually turns to an object-level 3D
prior. In the final stage, we extract the mesh from the trained NeRF and use
the trained LoRA to optimize the structure and appearance of the mesh. The
experiments demonstrate the effectiveness of the proposed method. Boosting3D
learns object-specific 3D prior which is beyond the ability of pre-trained
diffusion priors and achieves state-of-the-art performance in the single
image-to-3d generation task.";Kai Yu<author:sep>Jinlin Liu<author:sep>Mengyang Feng<author:sep>Miaomiao Cui<author:sep>Xuansong Xie;http://arxiv.org/pdf/2311.13617v1;cs.CV;8 pages, 7 figures, 1 table;nerf
2311.13297v1;http://arxiv.org/abs/2311.13297v1;2023-11-22;Retargeting Visual Data with Deformation Fields;"Seam carving is an image editing method that enable content-aware resizing,
including operations like removing objects. However, the seam-finding strategy
based on dynamic programming or graph-cut limits its applications to broader
visual data formats and degrees of freedom for editing. Our observation is that
describing the editing and retargeting of images more generally by a
displacement field yields a generalisation of content-aware deformations. We
propose to learn a deformation with a neural network that keeps the output
plausible while trying to deform it only in places with low information
content. This technique applies to different kinds of visual data, including
images, 3D scenes given as neural radiance fields, or even polygon meshes.
Experiments conducted on different visual data show that our method achieves
better content-aware retargeting compared to previous methods.";Tim Elsner<author:sep>Julia Berger<author:sep>Tong Wu<author:sep>Victor Czech<author:sep>Lin Gao<author:sep>Leif Kobbelt;http://arxiv.org/pdf/2311.13297v1;cs.CV;;
2311.13099v1;http://arxiv.org/abs/2311.13099v1;2023-11-22;PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF;"We show that physics-based simulations can be seamlessly integrated with NeRF
to generate high-quality elastodynamics of real-world objects. Unlike existing
methods, we discretize nonlinear hyperelasticity in a meshless way, obviating
the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh
or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed
to capture nonlinear dynamics and large deformation on the implicit model. Such
meshless integration enables versatile simulations of complex and codimensional
shapes. We adaptively place the least-square kernels according to the NeRF
density field to significantly reduce the complexity of the nonlinear
simulation. As a result, physically realistic animations can be conveniently
synthesized using our method for a wide range of hyperelastic materials at an
interactive rate. For more information, please visit our project page at
https://fytalon.github.io/pienerf/.";Yutao Feng<author:sep>Yintong Shang<author:sep>Xuan Li<author:sep>Tianjia Shao<author:sep>Chenfanfu Jiang<author:sep>Yin Yang;http://arxiv.org/pdf/2311.13099v1;cs.CV;;nerf
2311.13681v1;http://arxiv.org/abs/2311.13681v1;2023-11-22;Compact 3D Gaussian Representation for Radiance Field;"Neural Radiance Fields (NeRFs) have demonstrated remarkable potential in
capturing complex 3D scenes with high fidelity. However, one persistent
challenge that hinders the widespread adoption of NeRFs is the computational
bottleneck due to the volumetric rendering. On the other hand, 3D Gaussian
splatting (3DGS) has recently emerged as an alternative representation that
leverages a 3D Gaussisan-based representation and adopts the rasterization
pipeline to render the images rather than volumetric rendering, achieving very
fast rendering speed and promising image quality. However, a significant
drawback arises as 3DGS entails a substantial number of 3D Gaussians to
maintain the high fidelity of the rendered images, which requires a large
amount of memory and storage. To address this critical issue, we place a
specific emphasis on two key objectives: reducing the number of Gaussian points
without sacrificing performance and compressing the Gaussian attributes, such
as view-dependent color and covariance. To this end, we propose a learnable
mask strategy that significantly reduces the number of Gaussians while
preserving high performance. In addition, we propose a compact but effective
representation of view-dependent color by employing a grid-based neural field
rather than relying on spherical harmonics. Finally, we learn codebooks to
compactly represent the geometric attributes of Gaussian by vector
quantization. In our extensive experiments, we consistently show over
10$\times$ reduced storage and enhanced rendering speed, while maintaining the
quality of the scene representation, compared to 3DGS. Our work provides a
comprehensive framework for 3D scene representation, achieving high
performance, fast training, compactness, and real-time rendering. Our project
page is available at https://maincold2.github.io/c3dgs/.";Joo Chan Lee<author:sep>Daniel Rho<author:sep>Xiangyu Sun<author:sep>Jong Hwan Ko<author:sep>Eunbyung Park;http://arxiv.org/pdf/2311.13681v1;cs.CV;Project page: http://maincold2.github.io/c3dgs/;nerf
2311.13398v3;http://arxiv.org/abs/2311.13398v3;2023-11-22;Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot  Images;"In this paper, we present a method to optimize Gaussian splatting with a
limited number of images while avoiding overfitting. Representing a 3D scene by
combining numerous Gaussian splats has yielded outstanding visual quality.
However, it tends to overfit the training views when only a small number of
images are available. To address this issue, we introduce a dense depth map as
a geometry guide to mitigate overfitting. We obtained the depth map using a
pre-trained monocular depth estimation model and aligning the scale and offset
using sparse COLMAP feature points. The adjusted depth aids in the color-based
optimization of 3D Gaussian splatting, mitigating floating artifacts, and
ensuring adherence to geometric constraints. We verify the proposed method on
the NeRF-LLFF dataset with varying numbers of few images. Our approach
demonstrates robust geometry compared to the original method that relies solely
on images. Project page: robot0321.github.io/DepthRegGS";Jaeyoung Chung<author:sep>Jeongtaek Oh<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2311.13398v3;cs.CV;"10 pages, 5 figures; Project page: robot0321.github.io/DepthRegGS";gaussian splatting<tag:sep>nerf
2311.12775v3;http://arxiv.org/abs/2311.12775v3;2023-11-21;SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh  Reconstruction and High-Quality Mesh Rendering;"We propose a method to allow precise and extremely fast mesh extraction from
3D Gaussian Splatting. Gaussian Splatting has recently become very popular as
it yields realistic rendering while being significantly faster to train than
NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D
gaussians as these gaussians tend to be unorganized after optimization and no
method has been proposed so far. Our first key contribution is a regularization
term that encourages the gaussians to align well with the surface of the scene.
We then introduce a method that exploits this alignment to extract a mesh from
the Gaussians using Poisson reconstruction, which is fast, scalable, and
preserves details, in contrast to the Marching Cubes algorithm usually applied
to extract meshes from Neural SDFs. Finally, we introduce an optional
refinement strategy that binds gaussians to the surface of the mesh, and
jointly optimizes these Gaussians and the mesh through Gaussian splatting
rendering. This enables easy editing, sculpting, rigging, animating,
compositing and relighting of the Gaussians using traditional softwares by
manipulating the mesh instead of the gaussians themselves. Retrieving such an
editable mesh for realistic rendering is done within minutes with our method,
compared to hours with the state-of-the-art methods on neural SDFs, while
providing a better rendering quality. Our project page is the following:
https://anttwo.github.io/sugar/";Antoine GuÃ©don<author:sep>Vincent Lepetit;http://arxiv.org/pdf/2311.12775v3;cs.GR;"We identified a minor typographical error in Equation 6; We updated
  the paper accordingly. Project Webpage: https://anttwo.github.io/sugar/";gaussian splatting<tag:sep>nerf
2311.12897v1;http://arxiv.org/abs/2311.12897v1;2023-11-21;An Efficient 3D Gaussian Representation for Monocular/Multi-view Dynamic  Scenes;"In novel view synthesis of scenes from multiple input views, 3D Gaussian
splatting emerges as a viable alternative to existing radiance field
approaches, delivering great visual quality and real-time rendering. While
successful in static scenes, the present advancement of 3D Gaussian
representation, however, faces challenges in dynamic scenes in terms of memory
consumption and the need for numerous observations per time step, due to the
onus of storing 3D Gaussian parameters per time step. In this study, we present
an efficient 3D Gaussian representation tailored for dynamic scenes in which we
define positions and rotations as functions of time while leaving other
time-invariant properties of the static 3D Gaussian unchanged. Notably, our
representation reduces memory usage, which is consistent regardless of the
input sequence length. Additionally, it mitigates the risk of overfitting
observed frames by accounting for temporal changes. The optimization of our
Gaussian representation based on image and flow reconstruction results in a
powerful framework for dynamic scene view synthesis in both monocular and
multi-view cases. We obtain the highest rendering speed of $118$ frames per
second (FPS) at a resolution of $1352 \times 1014$ with a single GPU, showing
the practical usability and effectiveness of our proposed method in dynamic
scene rendering scenarios.";Kai Katsumata<author:sep>Duc Minh Vo<author:sep>Hideki Nakayama;http://arxiv.org/pdf/2311.12897v1;cs.GR;10 pages, 10 figures;
2311.12490v1;http://arxiv.org/abs/2311.12490v1;2023-11-21;Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields;"Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity
scene reconstruction for novel view synthesis. However, NeRF requires hundreds
of network evaluations per pixel to approximate a volume rendering integral,
making it slow to train. Caching NeRFs into explicit data structures can
effectively enhance rendering speed but at the cost of higher memory usage. To
address these issues, we present Hyb-NeRF, a novel neural radiance field with a
multi-resolution hybrid encoding that achieves efficient neural modeling and
fast rendering, which also allows for high-quality novel view synthesis. The
key idea of Hyb-NeRF is to represent the scene using different encoding
strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits
memory-efficiency learnable positional features at coarse resolutions and the
fast optimization speed and local details of hash-based feature grids at fine
resolutions. In addition, to further boost performance, we embed cone
tracing-based features in our learnable positional encoding that eliminates
encoding ambiguity and reduces aliasing artifacts. Extensive experiments on
both synthetic and real-world datasets show that Hyb-NeRF achieves faster
rendering speed with better rending quality and even a lower memory footprint
in comparison to previous state-of-the-art methods.";Yifan Wang<author:sep>Yi Gong<author:sep>Yuan Zeng;http://arxiv.org/pdf/2311.12490v1;cs.CV;WACV2024;nerf
2311.11863v1;http://arxiv.org/abs/2311.11863v1;2023-11-20;GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene  Understanding;"Applying NeRF to downstream perception tasks for scene understanding and
representation is becoming increasingly popular. Most existing methods treat
semantic prediction as an additional rendering task, \textit{i.e.}, the ""label
rendering"" task, to build semantic NeRFs. However, by rendering
semantic/instance labels per pixel without considering the contextual
information of the rendered image, these methods usually suffer from unclear
boundary segmentation and abnormal segmentation of pixels within an object. To
solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel
pipeline that makes the widely used segmentation model and NeRF work compatibly
under a unified framework, for facilitating context-aware 3D scene perception.
To accomplish this goal, we introduce transformers to aggregate radiance as
well as semantic embedding fields jointly for novel views and facilitate the
joint volumetric rendering of both fields. In addition, we propose two
self-distillation mechanisms, i.e., the Semantic Distill Loss and the
Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality
of the semantic field and the maintenance of geometric consistency. In
evaluation, we conduct experimental comparisons under two perception tasks
(\textit{i.e.} semantic and instance segmentation) using both synthetic and
real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\%,
11.76\%, and 8.47\% on generalized semantic segmentation, finetuning semantic
segmentation, and instance segmentation, respectively.";Hao Li<author:sep>Dingwen Zhang<author:sep>Yalun Dai<author:sep>Nian Liu<author:sep>Lechao Cheng<author:sep>Jingfeng Li<author:sep>Jingdong Wang<author:sep>Junwei Han;http://arxiv.org/pdf/2311.11863v1;cs.CV;;nerf
2311.11845v1;http://arxiv.org/abs/2311.11845v1;2023-11-20;Entangled View-Epipolar Information Aggregation for Generalizable Neural  Radiance Fields;"Generalizable NeRF can directly synthesize novel views across new scenes,
eliminating the need for scene-specific retraining in vanilla NeRF. A critical
enabling factor in these approaches is the extraction of a generalizable 3D
representation by aggregating source-view features. In this paper, we propose
an Entangled View-Epipolar Information Aggregation method dubbed EVE-NeRF.
Different from existing methods that consider cross-view and along-epipolar
information independently, EVE-NeRF conducts the view-epipolar feature
aggregation in an entangled manner by injecting the scene-invariant appearance
continuity and geometry consistency priors to the aggregation process. Our
approach effectively mitigates the potential lack of inherent geometric and
appearance constraint resulting from one-dimensional interactions, thus further
boosting the 3D representation generalizablity. EVE-NeRF attains
state-of-the-art performance across various evaluation scenarios. Extensive
experiments demonstate that, compared to prevailing single-dimensional
aggregation, the entangled network excels in the accuracy of 3D scene geometry
and appearance reconstruction.Our project page is
https://github.com/tatakai1/EVENeRF.";Zhiyuan Min<author:sep>Yawei Luo<author:sep>Wei Yang<author:sep>Yuesong Wang<author:sep>Yi Yang;http://arxiv.org/pdf/2311.11845v1;cs.CV;;nerf
2311.11700v3;http://arxiv.org/abs/2311.11700v3;2023-11-20;GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting;"In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D
Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
system. It facilitates a better balance between efficiency and accuracy.
Compared to recent SLAM methods employing neural implicit representations, our
method utilizes a real-time differentiable splatting rendering pipeline that
offers significant speedup to map optimization and RGB-D re-rendering.
Specifically, we propose an adaptive expansion strategy that adds new or
deletes noisy 3D Gaussian in order to efficiently reconstruct new observed
scene geometry and improve the mapping of previously observed areas. This
strategy is essential to extend 3D Gaussian representation to reconstruct the
whole scene rather than synthesize a static object in existing methods.
Moreover, in the pose tracking process, an effective coarse-to-fine technique
is designed to select reliable 3D Gaussian representations to optimize camera
pose, resulting in runtime reduction and robust estimation. Our method achieves
competitive performance compared with existing state-of-the-art real-time
methods on the Replica, TUM-RGBD datasets. The source code will be released
soon.";Chi Yan<author:sep>Delin Qu<author:sep>Dong Wang<author:sep>Dan Xu<author:sep>Zhigang Wang<author:sep>Bin Zhao<author:sep>Xuelong Li;http://arxiv.org/pdf/2311.11700v3;cs.CV;;gaussian splatting
2311.11284v3;http://arxiv.org/abs/2311.11284v3;2023-11-19;LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval  Score Matching;"The recent advancements in text-to-3D generation mark a significant milestone
in generative models, unlocking new possibilities for creating imaginative 3D
assets across various real-world scenarios. While recent advancements in
text-to-3D generation have shown promise, they often fall short in rendering
detailed and high-quality 3D models. This problem is especially prevalent as
many methods base themselves on Score Distillation Sampling (SDS). This paper
identifies a notable deficiency in SDS, that it brings inconsistent and
low-quality updating direction for the 3D model, causing the over-smoothing
effect. To address this, we propose a novel approach called Interval Score
Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes
interval-based score matching to counteract over-smoothing. Furthermore, we
incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline.
Extensive experiments show that our model largely outperforms the
state-of-the-art in quality and training efficiency.";Yixun Liang<author:sep>Xin Yang<author:sep>Jiantao Lin<author:sep>Haodong Li<author:sep>Xiaogang Xu<author:sep>Yingcong Chen;http://arxiv.org/pdf/2311.11284v3;cs.CV;"The first two authors contributed equally to this work. Our code will
  be available at: https://github.com/EnVision-Research/LucidDreamer";gaussian splatting
2311.11221v1;http://arxiv.org/abs/2311.11221v1;2023-11-19;GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion  Probabilistic Models with Structured Noise;"Text-to-3D, known for its efficient generation methods and expansive creative
potential, has garnered significant attention in the AIGC domain. However, the
amalgamation of Nerf and 2D diffusion models frequently yields oversaturated
images, posing severe limitations on downstream industrial applications due to
the constraints of pixelwise rendering method. Gaussian splatting has recently
superseded the traditional pointwise sampling technique prevalent in NeRF-based
methodologies, revolutionizing various aspects of 3D reconstruction. This paper
introduces a novel text to 3D content generation framework based on Gaussian
splatting, enabling fine control over image saturation through individual
Gaussian sphere transparencies, thereby producing more realistic images. The
challenge of achieving multi-view consistency in 3D generation significantly
impedes modeling complexity and accuracy. Taking inspiration from SJC, we
explore employing multi-view noise distributions to perturb images generated by
3D Gaussian splatting, aiming to rectify inconsistencies in multi-view
geometry. We ingeniously devise an efficient method to generate noise that
produces Gaussian noise from diverse viewpoints, all originating from a shared
noise source. Furthermore, vanilla 3D Gaussian-based generation tends to trap
models in local minima, causing artifacts like floaters, burrs, or
proliferative elements. To mitigate these issues, we propose the variational
Gaussian splatting technique to enhance the quality and stability of 3D
appearance. To our knowledge, our approach represents the first comprehensive
utilization of Gaussian splatting across the entire spectrum of 3D content
generation processes.";Xinhai Li<author:sep>Huaibin Wang<author:sep>Kuo-Kun Tseng;http://arxiv.org/pdf/2311.11221v1;cs.CV;;gaussian splatting<tag:sep>nerf
2311.12059v1;http://arxiv.org/abs/2311.12059v1;2023-11-18;Towards Function Space Mesh Watermarking: Protecting the Copyright of  Signed Distance Fields;"The signed distance field (SDF) represents 3D geometries in continuous
function space. Due to its continuous nature, explicit 3D models (e.g., meshes)
can be extracted from it at arbitrary resolution, which means losing the SDF is
equivalent to losing the mesh. Recent research has shown meshes can also be
extracted from SDF-enhanced neural radiance fields (NeRF). Such a signal raises
an alarm that any implicit neural representation with SDF enhancement can
extract the original mesh, which indicates identifying the SDF's intellectual
property becomes an urgent issue. This paper proposes FuncMark, a robust and
invisible watermarking method to protect the copyright of signed distance
fields by leveraging analytic on-surface deformations to embed binary watermark
messages. Such deformation can survive isosurfacing and thus be inherited by
the extracted meshes for further watermark message decoding. Our method can
recover the message with high-resolution meshes extracted from SDFs and detect
the watermark even when mesh vertices are extremely sparse. Furthermore, our
method is robust even when various distortions (including remeshing) are
encountered. Extensive experiments demonstrate that our \tool significantly
outperforms state-of-the-art approaches and the message is still detectable
even when only 50 vertex samples are given.";Xingyu Zhu<author:sep>Guanhui Ye<author:sep>Chengdong Dong<author:sep>Xiapu Luo<author:sep>Xuetao Wei;http://arxiv.org/pdf/2311.12059v1;cs.CV;;nerf
2311.11016v1;http://arxiv.org/abs/2311.11016v1;2023-11-18;SNI-SLAM: Semantic Neural Implicit SLAM;"We propose SNI-SLAM, a semantic SLAM system utilizing neural implicit
representation, that simultaneously performs accurate semantic mapping,
high-quality surface reconstruction, and robust camera tracking. In this
system, we introduce hierarchical semantic representation to allow multi-level
semantic comprehension for top-down structured semantic mapping of the scene.
In addition, to fully utilize the correlation between multiple attributes of
the environment, we integrate appearance, geometry and semantic features
through cross-attention for feature collaboration. This strategy enables a more
multifaceted understanding of the environment, thereby allowing SNI-SLAM to
remain robust even when single attribute is defective. Then, we design an
internal fusion-based decoder to obtain semantic, RGB, Truncated Signed
Distance Field (TSDF) values from multi-level features for accurate decoding.
Furthermore, we propose a feature loss to update the scene representation at
the feature level. Compared with low-level losses such as RGB loss and depth
loss, our feature loss is capable of guiding the network optimization on a
higher-level. Our SNI-SLAM method demonstrates superior performance over all
recent NeRF-based SLAM methods in terms of mapping and tracking accuracy on
Replica and ScanNet datasets, while also showing excellent capabilities in
accurate semantic segmentation and real-time semantic mapping.";Siting Zhu<author:sep>Guangming Wang<author:sep>Hermann Blum<author:sep>Jiuming Liu<author:sep>Liang Song<author:sep>Marc Pollefeys<author:sep>Hesheng Wang;http://arxiv.org/pdf/2311.11016v1;cs.RO;;nerf
2311.10959v1;http://arxiv.org/abs/2311.10959v1;2023-11-18;Structure-Aware Sparse-View X-ray 3D Reconstruction;"X-ray, known for its ability to reveal internal structures of objects, is
expected to provide richer information for 3D reconstruction than visible
light. Yet, existing neural radiance fields (NeRF) algorithms overlook this
important nature of X-ray, leading to their limitations in capturing structural
contents of imaged objects. In this paper, we propose a framework,
Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view
X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer
(Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal
structures of objects in 3D space by modeling the dependencies within each line
segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray
sampling strategy to extract contextual and geometric information in 2D
projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray
applications. Experiments on X3D show that SAX-NeRF surpasses previous
NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT
reconstruction. Code, models, and data will be released at
https://github.com/caiyuanhao1998/SAX-NeRF";Yuanhao Cai<author:sep>Jiahao Wang<author:sep>Alan Yuille<author:sep>Zongwei Zhou<author:sep>Angtian Wang;http://arxiv.org/pdf/2311.10959v1;eess.IV;;nerf
2311.10523v1;http://arxiv.org/abs/2311.10523v1;2023-11-17;Removing Adverse Volumetric Effects From Trained Neural Radiance Fields;"While the use of neural radiance fields (NeRFs) in different challenging
settings has been explored, only very recently have there been any
contributions that focus on the use of NeRF in foggy environments. We argue
that the traditional NeRF models are able to replicate scenes filled with fog
and propose a method to remove the fog when synthesizing novel views. By
calculating the global contrast of a scene, we can estimate a density threshold
that, when applied, removes all visible fog. This makes it possible to use NeRF
as a way of rendering clear views of objects of interest located in fog-filled
environments. Additionally, to benchmark performance on such scenes, we
introduce a new dataset that expands some of the original synthetic NeRF scenes
through the addition of fog and natural environments. The code, dataset, and
video results can be found on our project page: https://vegardskui.com/fognerf/";Andreas L. Teigen<author:sep>Mauhing Yip<author:sep>Victor P. Hamran<author:sep>Vegard Skui<author:sep>Annette Stahl<author:sep>Rudolf Mester;http://arxiv.org/pdf/2311.10523v1;cs.CV;"This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible";nerf
2311.10812v1;http://arxiv.org/abs/2311.10812v1;2023-11-17;SplatArmor: Articulated Gaussian splatting for animatable humans from  monocular RGB videos;"We propose SplatArmor, a novel approach for recovering detailed and
animatable human models by `armoring' a parameterized body model with 3D
Gaussians. Our approach represents the human as a set of 3D Gaussians within a
canonical space, whose articulation is defined by extending the skinning of the
underlying SMPL geometry to arbitrary locations in the canonical space. To
account for pose-dependent effects, we introduce a SE(3) field, which allows us
to capture both the location and anisotropy of the Gaussians. Furthermore, we
propose the use of a neural color field to provide color regularization and 3D
supervision for the precise positioning of these Gaussians. We show that
Gaussian splatting provides an interesting alternative to neural rendering
based methods by leverging a rasterization primitive without facing any of the
non-differentiability and optimization challenges typically faced in such
approaches. The rasterization paradigms allows us to leverage forward skinning,
and does not suffer from the ambiguities associated with inverse skinning and
warping. We show compelling results on the ZJU MoCap and People Snapshot
datasets, which underscore the effectiveness of our method for controllable
human synthesis.";Rohit Jena<author:sep>Ganesh Subramanian Iyer<author:sep>Siddharth Choudhary<author:sep>Brandon Smith<author:sep>Pratik Chaudhari<author:sep>James Gee;http://arxiv.org/pdf/2311.10812v1;cs.CV;;gaussian splatting
2401.02436v1;http://arxiv.org/abs/2401.02436v1;2023-11-17;Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis;"Recently, high-fidelity scene reconstruction with an optimized 3D Gaussian
splat representation has been introduced for novel view synthesis from sparse
image sets. Making such representations suitable for applications like network
streaming and rendering on low-power devices requires significantly reduced
memory consumption as well as improved rendering efficiency. We propose a
compressed 3D Gaussian splat representation that utilizes sensitivity-aware
vector clustering with quantization-aware training to compress directional
colors and Gaussian parameters. The learned codebooks have low bitrates and
achieve a compression rate of up to $31\times$ on real-world scenes with only
minimal degradation of visual quality. We demonstrate that the compressed splat
representation can be efficiently rendered with hardware rasterization on
lightweight GPUs at up to $4\times$ higher framerates than reported via an
optimized GPU compute pipeline. Extensive experiments across multiple datasets
demonstrate the robustness and rendering speed of the proposed approach.";Simon Niedermayr<author:sep>Josef Stumpfegger<author:sep>RÃ¼diger Westermann;http://arxiv.org/pdf/2401.02436v1;cs.CV;;gaussian splatting
2311.10091v1;http://arxiv.org/abs/2311.10091v1;2023-11-16;Adaptive Shells for Efficient Neural Radiance Field Rendering;"Neural radiance fields achieve unprecedented quality for novel view
synthesis, but their volumetric formulation remains expensive, requiring a huge
number of samples to render high-resolution images. Volumetric encodings are
essential to represent fuzzy geometry such as foliage and hair, and they are
well-suited for stochastic optimization. Yet, many scenes ultimately consist
largely of solid surfaces which can be accurately rendered by a single sample
per pixel. Based on this insight, we propose a neural radiance formulation that
smoothly transitions between volumetric- and surface-based rendering, greatly
accelerating rendering speed and even improving visual fidelity. Our method
constructs an explicit mesh envelope which spatially bounds a neural volumetric
representation. In solid regions, the envelope nearly converges to a surface
and can often be rendered with a single sample. To this end, we generalize the
NeuS formulation with a learned spatially-varying kernel size which encodes the
spread of the density, fitting a wide kernel to volume-like regions and a tight
kernel to surface-like regions. We then extract an explicit mesh of a narrow
band around the surface, with width determined by the kernel size, and
fine-tune the radiance field within this band. At inference time, we cast rays
against the mesh and evaluate the radiance field only within the enclosed
region, greatly reducing the number of samples required. Experiments show that
our approach enables efficient rendering at very high fidelity. We also
demonstrate that the extracted envelope enables downstream applications such as
animation and simulation.";Zian Wang<author:sep>Tianchang Shen<author:sep>Merlin Nimier-David<author:sep>Nicholas Sharp<author:sep>Jun Gao<author:sep>Alexander Keller<author:sep>Sanja Fidler<author:sep>Thomas MÃ¼ller<author:sep>Zan Gojcic;http://arxiv.org/pdf/2311.10091v1;cs.CV;"SIGGRAPH Asia 2023. Project page:
  research.nvidia.com/labs/toronto-ai/adaptive-shells/";
2311.09806v2;http://arxiv.org/abs/2311.09806v2;2023-11-16;EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction  on Mobile Devices;"Reconstructing real-world 3D objects has numerous applications in computer
vision, such as virtual reality, video games, and animations. Ideally, 3D
reconstruction methods should generate high-fidelity results with 3D
consistency in real-time. Traditional methods match pixels between images using
photo-consistency constraints or learned features, while differentiable
rendering methods like Neural Radiance Fields (NeRF) use differentiable volume
rendering or surface-based representation to generate high-fidelity scenes.
However, these methods require excessive runtime for rendering, making them
impractical for daily applications. To address these challenges, we present
$\textbf{EvaSurf}$, an $\textbf{E}$fficient $\textbf{V}$iew-$\textbf{A}$ware
implicit textured $\textbf{Surf}$ace reconstruction method on mobile devices.
In our method, we first employ an efficient surface-based model with a
multi-view supervision module to ensure accurate mesh reconstruction. To enable
high-fidelity rendering, we learn an implicit texture embedded with a set of
Gaussian lobes to capture view-dependent information. Furthermore, with the
explicit geometry and the implicit texture, we can employ a lightweight neural
shader to reduce the expense of computation and further support real-time
rendering on common mobile devices. Extensive experiments demonstrate that our
method can reconstruct high-quality appearance and accurate mesh on both
synthetic and real-world datasets. Moreover, our method can be trained in just
1-2 hours using a single GPU and run on mobile devices at over 40 FPS (Frames
Per Second), with a final package required for rendering taking up only 40-50
MB.";Jingnan Gao<author:sep>Zhuo Chen<author:sep>Yichao Yan<author:sep>Bowen Pan<author:sep>Zhe Wang<author:sep>Jiangjing Lyu<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2311.09806v2;cs.CV;Project Page: http://g-1nonly.github.io/EvaSurf-Website/;nerf
2311.09646v1;http://arxiv.org/abs/2311.09646v1;2023-11-16;Reconstructing Continuous Light Field From Single Coded Image;"We propose a method for reconstructing a continuous light field of a target
scene from a single observed image. Our method takes the best of two worlds:
joint aperture-exposure coding for compressive light-field acquisition, and a
neural radiance field (NeRF) for view synthesis. Joint aperture-exposure coding
implemented in a camera enables effective embedding of 3-D scene information
into an observed image, but in previous works, it was used only for
reconstructing discretized light-field views. NeRF-based neural rendering
enables high quality view synthesis of a 3-D scene from continuous viewpoints,
but when only a single image is given as the input, it struggles to achieve
satisfactory quality. Our method integrates these two techniques into an
efficient and end-to-end trainable pipeline. Trained on a wide variety of
scenes, our method can reconstruct continuous light fields accurately and
efficiently without any test time optimization. To our knowledge, this is the
first work to bridge two worlds: camera design for efficiently acquiring 3-D
information and neural rendering.";Yuya Ishikawa<author:sep>Keita Takahashi<author:sep>Chihiro Tsutake<author:sep>Toshiaki Fujii;http://arxiv.org/pdf/2311.09646v1;cs.CV;;nerf
2311.09217v1;http://arxiv.org/abs/2311.09217v1;2023-11-15;DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction  Model;"We propose \textbf{DMV3D}, a novel 3D generation approach that uses a
transformer-based 3D large reconstruction model to denoise multi-view
diffusion. Our reconstruction model incorporates a triplane NeRF representation
and can denoise noisy multi-view images via NeRF reconstruction and rendering,
achieving single-stage 3D generation in $\sim$30s on single A100 GPU. We train
\textbf{DMV3D} on large-scale multi-view image datasets of highly diverse
objects using only image reconstruction losses, without accessing 3D assets. We
demonstrate state-of-the-art results for the single-image reconstruction
problem where probabilistic modeling of unseen object parts is required for
generating diverse reconstructions with sharp textures. We also show
high-quality text-to-3D generation results outperforming previous 3D diffusion
models. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .";Yinghao Xu<author:sep>Hao Tan<author:sep>Fujun Luan<author:sep>Sai Bi<author:sep>Peng Wang<author:sep>Jiahao Li<author:sep>Zifan Shi<author:sep>Kalyan Sunkavalli<author:sep>Gordon Wetzstein<author:sep>Zexiang Xu<author:sep>Kai Zhang;http://arxiv.org/pdf/2311.09217v1;cs.CV;Project Page: https://justimyhxu.github.io/projects/dmv3d/;nerf
2311.09221v1;http://arxiv.org/abs/2311.09221v1;2023-11-15;Single-Image 3D Human Digitization with Shape-Guided Diffusion;"We present an approach to generate a 360-degree view of a person with a
consistent, high-resolution appearance from a single input image. NeRF and its
variants typically require videos or images from different viewpoints. Most
existing approaches taking monocular input either rely on ground-truth 3D scans
for supervision or lack 3D consistency. While recent 3D generative models show
promise of 3D consistent human digitization, these approaches do not generalize
well to diverse clothing appearances, and the results lack photorealism. Unlike
existing work, we utilize high-capacity 2D diffusion models pretrained for
general image synthesis tasks as an appearance prior of clothed humans. To
achieve better 3D consistency while retaining the input identity, we
progressively synthesize multiple views of the human in the input image by
inpainting missing regions with shape-guided diffusion conditioned on
silhouette and surface normal. We then fuse these synthesized multi-view images
via inverse rendering to obtain a fully textured high-resolution 3D mesh of the
given person. Experiments show that our approach outperforms prior methods and
achieves photorealistic 360-degree synthesis of a wide range of clothed humans
with complex textures from a single image.";Badour AlBahar<author:sep>Shunsuke Saito<author:sep>Hung-Yu Tseng<author:sep>Changil Kim<author:sep>Johannes Kopf<author:sep>Jia-Bin Huang;http://arxiv.org/pdf/2311.09221v1;cs.CV;SIGGRAPH Asia 2023. Project website: https://human-sgd.github.io/;nerf
2311.09077v2;http://arxiv.org/abs/2311.09077v2;2023-11-15;Spiking NeRF: Representing the Real-World Geometry by a Discontinuous  Representation;"A crucial reason for the success of existing NeRF-based methods is to build a
neural density field for the geometry representation via multiple perceptron
layers (MLPs). MLPs are continuous functions, however, real geometry or density
field is frequently discontinuous at the interface between the air and the
surface. Such a contrary brings the problem of unfaithful geometry
representation. To this end, this paper proposes spiking NeRF, which leverages
spiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural
Network (SNN) framework to build a discontinuous density field for faithful
geometry representation. Specifically, we first demonstrate the reason why
continuous density fields will bring inaccuracy. Then, we propose to use the
spiking neurons to build a discontinuous density field. We conduct a
comprehensive analysis for the problem of existing spiking neuron models and
then provide the numerical relationship between the parameter of the spiking
neuron and the theoretical accuracy of geometry. Based on this, we propose a
bounded spiking neuron to build the discontinuous density field. Our method
achieves SOTA performance. The source code and the supplementary material are
available at https://github.com/liaozhanfeng/Spiking-NeRF.";Zhanfeng Liao<author:sep>Qian Zheng<author:sep>Yan Liu<author:sep>Gang Pan;http://arxiv.org/pdf/2311.09077v2;cs.CV;;nerf
2311.08581v1;http://arxiv.org/abs/2311.08581v1;2023-11-14;Drivable 3D Gaussian Avatars;"We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable
model for human bodies rendered with Gaussian splats. Current photorealistic
drivable avatars require either accurate 3D registrations during training,
dense input images during testing, or both. The ones based on neural radiance
fields also tend to be prohibitively slow for telepresence applications. This
work uses the recently presented 3D Gaussian Splatting (3DGS) technique to
render realistic humans at real-time framerates, using dense calibrated
multi-view videos as input. To deform those primitives, we depart from the
commonly used point deformation method of linear blend skinning (LBS) and use a
classic volumetric deformation method: cage deformations. Given their smaller
size, we drive these deformations with joint angles and keypoints, which are
more suitable for communication applications. Our experiments on nine subjects
with varied body shapes, clothes, and motions obtain higher-quality results
than state-of-the-art methods when using the same training and test data.";Wojciech Zielonka<author:sep>Timur Bagautdinov<author:sep>Shunsuke Saito<author:sep>Michael ZollhÃ¶fer<author:sep>Justus Thies<author:sep>Javier Romero;http://arxiv.org/pdf/2311.08581v1;cs.CV;Website: https://zielon.github.io/d3ga/;gaussian splatting
2311.07044v1;http://arxiv.org/abs/2311.07044v1;2023-11-13;$L_0$-Sampler: An $L_{0}$ Model Guided Volume Sampling for NeRF;"Since being proposed, Neural Radiance Fields (NeRF) have achieved great
success in related tasks, mainly adopting the hierarchical volume sampling
(HVS) strategy for volume rendering. However, the HVS of NeRF approximates
distributions using piecewise constant functions, which provides a relatively
rough estimation. Based on the observation that a well-trained weight function
$w(t)$ and the $L_0$ distance between points and the surface have very high
similarity, we propose $L_0$-Sampler by incorporating the $L_0$ model into
$w(t)$ to guide the sampling process. Specifically, we propose to use piecewise
exponential functions rather than piecewise constant functions for
interpolation, which can not only approximate quasi-$L_0$ weight distributions
along rays quite well but also can be easily implemented with few lines of code
without additional computational burden. Stable performance improvements can be
achieved by applying $L_0$-Sampler to NeRF and its related tasks like 3D
reconstruction. Code is available at https://ustc3dv.github.io/L0-Sampler/ .";Liangchen Li<author:sep>Juyong Zhang;http://arxiv.org/pdf/2311.07044v1;cs.CV;Project page: https://ustc3dv.github.io/L0-Sampler/;nerf
2311.06455v1;http://arxiv.org/abs/2311.06455v1;2023-11-11;Aria-NeRF: Multimodal Egocentric View Synthesis;"We seek to accelerate research in developing rich, multimodal scene models
trained from egocentric data, based on differentiable volumetric ray-tracing
inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like
model from an egocentric image sequence plays a pivotal role in understanding
human behavior and holds diverse applications within the realms of VR/AR. Such
egocentric NeRF-like models may be used as realistic simulations, contributing
significantly to the advancement of intelligent agents capable of executing
tasks in the real-world. The future of egocentric view synthesis may lead to
novel environment representations going beyond today's NeRFs by augmenting
visual data with multimodal sensors such as IMU for egomotion tracking, audio
sensors to capture surface texture and human language context, and eye-gaze
trackers to infer human attention patterns in the scene. To support and
facilitate the development and evaluation of egocentric multimodal scene
modeling, we present a comprehensive multimodal egocentric video dataset. This
dataset offers a comprehensive collection of sensory data, featuring RGB
images, eye-tracking camera footage, audio recordings from a microphone,
atmospheric pressure readings from a barometer, positional coordinates from
GPS, connectivity details from Wi-Fi and Bluetooth, and information from
dual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The
dataset was collected with the Meta Aria Glasses wearable device platform. The
diverse data modalities and the real-world context captured within this dataset
serve as a robust foundation for furthering our understanding of human behavior
and enabling more immersive and intelligent experiences in the realms of VR,
AR, and robotics.";Jiankai Sun<author:sep>Jianing Qiu<author:sep>Chuanyang Zheng<author:sep>John Tucker<author:sep>Javier Yu<author:sep>Mac Schwager;http://arxiv.org/pdf/2311.06455v1;cs.CV;;nerf
2311.06211v1;http://arxiv.org/abs/2311.06211v1;2023-11-10;ASSIST: Interactive Scene Nodes for Scalable and Realistic Indoor  Simulation;"We present ASSIST, an object-wise neural radiance field as a panoptic
representation for compositional and realistic simulation. Central to our
approach is a novel scene node data structure that stores the information of
each object in a unified fashion, allowing online interaction in both intra-
and cross-scene settings. By incorporating a differentiable neural network
along with the associated bounding box and semantic features, the proposed
structure guarantees user-friendly interaction on independent objects to scale
up novel view simulation. Objects in the scene can be queried, added,
duplicated, deleted, transformed, or swapped simply through mouse/keyboard
controls or language instructions. Experiments demonstrate the efficacy of the
proposed method, where scaled realistic simulation can be achieved through
interactive editing and compositional rendering, with color images, depth
images, and panoptic segmentation masks generated in a 3D consistent manner.";Zhide Zhong<author:sep>Jiakai Cao<author:sep>Songen Gu<author:sep>Sirui Xie<author:sep>Weibo Gao<author:sep>Liyi Luo<author:sep>Zike Yan<author:sep>Hao Zhao<author:sep>Guyue Zhou;http://arxiv.org/pdf/2311.06211v1;cs.CV;;
2311.06214v2;http://arxiv.org/abs/2311.06214v2;2023-11-10;Instant3D: Fast Text-to-3D with Sparse-View Generation and Large  Reconstruction Model;"Text-to-3D with diffusion models has achieved remarkable progress in recent
years. However, existing methods either rely on score distillation-based
optimization which suffer from slow inference, low diversity and Janus
problems, or are feed-forward methods that generate low-quality results due to
the scarcity of 3D training data. In this paper, we propose Instant3D, a novel
method that generates high-quality and diverse 3D assets from text prompts in a
feed-forward manner. We adopt a two-stage paradigm, which first generates a
sparse set of four structured and consistent views from text in one shot with a
fine-tuned 2D text-to-image diffusion model, and then directly regresses the
NeRF from the generated images with a novel transformer-based sparse-view
reconstructor. Through extensive experiments, we demonstrate that our method
can generate diverse 3D assets of high visual quality within 20 seconds, which
is two orders of magnitude faster than previous optimization-based methods that
can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.";Jiahao Li<author:sep>Hao Tan<author:sep>Kai Zhang<author:sep>Zexiang Xu<author:sep>Fujun Luan<author:sep>Yinghao Xu<author:sep>Yicong Hong<author:sep>Kalyan Sunkavalli<author:sep>Greg Shakhnarovich<author:sep>Sai Bi;http://arxiv.org/pdf/2311.06214v2;cs.CV;Project webpage: https://jiahao.ai/instant3d/;nerf
2311.05836v4;http://arxiv.org/abs/2311.05836v4;2023-11-10;UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical  Neural Radiance Fields;"In the field of clinical medicine, computed tomography (CT) is an effective
medical imaging modality for the diagnosis of various pathologies. Compared
with X-ray images, CT images can provide more information, including
multi-planar slices and three-dimensional structures for clinical diagnosis.
However, CT imaging requires patients to be exposed to large doses of ionizing
radiation for a long time, which may cause irreversible physical harm. In this
paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on
generated radiation fields. The network can learn a continuous representation
of CT projections from 2D X-ray images by obtaining the internal structure and
depth information and using adaptive loss weights to ensure the quality of the
generated images. Our model is trained on publicly available knee and chest
datasets, and we show the results of CT projection rendering with a single
X-ray and compare our method with other methods based on generated radiation
fields.";Jing Hu<author:sep>Qinrui Fan<author:sep>Shu Hu<author:sep>Siwei Lyu<author:sep>Xi Wu<author:sep>Xin Wang;http://arxiv.org/pdf/2311.05836v4;eess.IV;;nerf
2311.05958v1;http://arxiv.org/abs/2311.05958v1;2023-11-10;A Neural Height-Map Approach for the Binocular Photometric Stereo  Problem;"In this work we propose a novel, highly practical, binocular photometric
stereo (PS) framework, which has same acquisition speed as single view PS,
however significantly improves the quality of the estimated geometry.
  As in recent neural multi-view shape estimation frameworks such as NeRF,
SIREN and inverse graphics approaches to multi-view photometric stereo (e.g.
PS-NeRF) we formulate shape estimation task as learning of a differentiable
surface and texture representation by minimising surface normal discrepancy for
normals estimated from multiple varying light images for two views as well as
discrepancy between rendered surface intensity and observed images. Our method
differs from typical multi-view shape estimation approaches in two key ways.
First, our surface is represented not as a volume but as a neural heightmap
where heights of points on a surface are computed by a deep neural network.
Second, instead of predicting an average intensity as PS-NeRF or introducing
lambertian material assumptions as Guo et al., we use a learnt BRDF and perform
near-field per point intensity rendering.
  Our method achieves the state-of-the-art performance on the DiLiGenT-MV
dataset adapted to binocular stereo setup as well as a new binocular
photometric stereo dataset - LUCES-ST.";Fotios Logothetis<author:sep>Ignas Budvytis<author:sep>Roberto Cipolla;http://arxiv.org/pdf/2311.05958v1;cs.CV;WACV 2024;nerf
2311.05289v1;http://arxiv.org/abs/2311.05289v1;2023-11-09;VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for  Enhanced Indoor View Synthesis;"Creating high-quality view synthesis is essential for immersive applications
but continues to be problematic, particularly in indoor environments and for
real-time deployment. Current techniques frequently require extensive
computational time for both training and rendering, and often produce
less-than-ideal 3D representations due to inadequate geometric structuring. To
overcome this, we introduce VoxNeRF, a novel approach that leverages volumetric
representations to enhance the quality and efficiency of indoor view synthesis.
Firstly, VoxNeRF constructs a structured scene geometry and converts it into a
voxel-based representation. We employ multi-resolution hash grids to adaptively
capture spatial features, effectively managing occlusions and the intricate
geometry of indoor scenes. Secondly, we propose a unique voxel-guided efficient
sampling technique. This innovation selectively focuses computational resources
on the most relevant portions of ray segments, substantially reducing
optimization time. We validate our approach against three public indoor
datasets and demonstrate that VoxNeRF outperforms state-of-the-art methods.
Remarkably, it achieves these gains while reducing both training and rendering
times, surpassing even Instant-NGP in speed and bringing the technology closer
to real-time.";Sen Wang<author:sep>Wei Zhang<author:sep>Stefano Gasperini<author:sep>Shun-Cheng Wu<author:sep>Nassir Navab;http://arxiv.org/pdf/2311.05289v1;cs.CV;8 pages, 4 figures;nerf
2311.05230v1;http://arxiv.org/abs/2311.05230v1;2023-11-09;ConRad: Image Constrained Radiance Fields for 3D Generation from a  Single Image;"We present a novel method for reconstructing 3D objects from a single RGB
image. Our method leverages the latest image generation models to infer the
hidden 3D structure while remaining faithful to the input image. While existing
methods obtain impressive results in generating 3D models from text prompts,
they do not provide an easy approach for conditioning on input RGB data.
Na\""ive extensions of these methods often lead to improper alignment in
appearance between the input image and the 3D reconstructions. We address these
challenges by introducing Image Constrained Radiance Fields (ConRad), a novel
variant of neural radiance fields. ConRad is an efficient 3D representation
that explicitly captures the appearance of an input image in one viewpoint. We
propose a training algorithm that leverages the single RGB image in conjunction
with pretrained Diffusion Models to optimize the parameters of a ConRad
representation. Extensive experiments show that ConRad representations can
simplify preservation of image details while producing a realistic 3D
reconstruction. Compared to existing state-of-the-art baselines, we show that
our 3D reconstructions remain more faithful to the input and produce more
consistent 3D models while demonstrating significantly improved quantitative
performance on a ShapeNet object benchmark.";Senthil Purushwalkam<author:sep>Nikhil Naik;http://arxiv.org/pdf/2311.05230v1;cs.CV;Advances in Neural Information Processing Systems (NeurIPS 2023);
2311.05461v1;http://arxiv.org/abs/2311.05461v1;2023-11-09;Control3D: Towards Controllable Text-to-3D Generation;"Recent remarkable advances in large-scale text-to-image diffusion models have
inspired a significant breakthrough in text-to-3D generation, pursuing 3D
content creation solely from a given text prompt. However, existing text-to-3D
techniques lack a crucial ability in the creative process: interactively
control and shape the synthetic 3D contents according to users' desired
specifications (e.g., sketch). To alleviate this issue, we present the first
attempt for text-to-3D generation conditioning on the additional hand-drawn
sketch, namely Control3D, which enhances controllability for users. In
particular, a 2D conditioned diffusion model (ControlNet) is remoulded to guide
the learning of 3D scene parameterized as NeRF, encouraging each view of 3D
scene aligned with the given text prompt and hand-drawn sketch. Moreover, we
exploit a pre-trained differentiable photo-to-sketch model to directly estimate
the sketch of the rendered image over synthetic 3D scene. Such estimated sketch
along with each sampled view is further enforced to be geometrically consistent
with the given sketch, pursuing better controllable text-to-3D generation.
Through extensive experiments, we demonstrate that our proposal can generate
accurate and faithful 3D scenes that align closely with the input text prompts
and sketches.";Yang Chen<author:sep>Yingwei Pan<author:sep>Yehao Li<author:sep>Ting Yao<author:sep>Tao Mei;http://arxiv.org/pdf/2311.05461v1;cs.CV;ACM Multimedia 2023;nerf
2311.05521v2;http://arxiv.org/abs/2311.05521v2;2023-11-09;BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis;"Synthesizing photorealistic 4D human head avatars from videos is essential
for VR/AR, telepresence, and video game applications. Although existing Neural
Radiance Fields (NeRF)-based methods achieve high-fidelity results, the
computational expense limits their use in real-time applications. To overcome
this limitation, we introduce BakedAvatar, a novel representation for real-time
neural head avatar synthesis, deployable in a standard polygon rasterization
pipeline. Our approach extracts deformable multi-layer meshes from learned
isosurfaces of the head and computes expression-, pose-, and view-dependent
appearances that can be baked into static textures for efficient rasterization.
We thus propose a three-stage pipeline for neural head avatar synthesis, which
includes learning continuous deformation, manifold, and radiance fields,
extracting layered meshes and textures, and fine-tuning texture details with
differential rasterization. Experimental results demonstrate that our
representation generates synthesis results of comparable quality to other
state-of-the-art methods while significantly reducing the inference time
required. We further showcase various head avatar synthesis results from
monocular videos, including view synthesis, face reenactment, expression
editing, and pose editing, all at interactive frame rates.";Hao-Bin Duan<author:sep>Miao Wang<author:sep>Jin-Chuan Shi<author:sep>Xu-Chuan Chen<author:sep>Yan-Pei Cao;http://arxiv.org/pdf/2311.05521v2;cs.GR;"ACM Transactions on Graphics (SIGGRAPH Asia 2023). Project Page:
  https://buaavrcg.github.io/BakedAvatar";nerf
2311.04400v1;http://arxiv.org/abs/2311.04400v1;2023-11-08;LRM: Large Reconstruction Model for Single Image to 3D;"We propose the first Large Reconstruction Model (LRM) that predicts the 3D
model of an object from a single input image within just 5 seconds. In contrast
to many previous methods that are trained on small-scale datasets such as
ShapeNet in a category-specific fashion, LRM adopts a highly scalable
transformer-based architecture with 500 million learnable parameters to
directly predict a neural radiance field (NeRF) from the input image. We train
our model in an end-to-end manner on massive multi-view data containing around
1 million objects, including both synthetic renderings from Objaverse and real
captures from MVImgNet. This combination of a high-capacity model and
large-scale training data empowers our model to be highly generalizable and
produce high-quality 3D reconstructions from various testing inputs including
real-world in-the-wild captures and images from generative models. Video demos
and interactable 3D meshes can be found on this website:
https://yiconghong.me/LRM/.";Yicong Hong<author:sep>Kai Zhang<author:sep>Jiuxiang Gu<author:sep>Sai Bi<author:sep>Yang Zhou<author:sep>Difan Liu<author:sep>Feng Liu<author:sep>Kalyan Sunkavalli<author:sep>Trung Bui<author:sep>Hao Tan;http://arxiv.org/pdf/2311.04400v1;cs.CV;23 pages;nerf
2311.04521v1;http://arxiv.org/abs/2311.04521v1;2023-11-08;Learning Robust Multi-Scale Representation for Neural Radiance Fields  from Unposed Images;"We introduce an improved solution to the neural image-based rendering problem
in computer vision. Given a set of images taken from a freely moving camera at
train time, the proposed approach could synthesize a realistic image of the
scene from a novel viewpoint at test time. The key ideas presented in this
paper are (i) Recovering accurate camera parameters via a robust pipeline from
unposed day-to-day images is equally crucial in neural novel view synthesis
problem; (ii) It is rather more practical to model object's content at
different resolutions since dramatic camera motion is highly likely in
day-to-day unposed images. To incorporate the key ideas, we leverage the
fundamentals of scene rigidity, multi-scale neural scene representation, and
single-image depth prediction. Concretely, the proposed approach makes the
camera parameters as learnable in a neural fields-based modeling framework. By
assuming per view depth prediction is given up to scale, we constrain the
relative pose between successive frames. From the relative poses, absolute
camera pose estimation is modeled via a graph-neural network-based multiple
motion averaging within the multi-scale neural-fields network, leading to a
single loss function. Optimizing the introduced loss function provides camera
intrinsic, extrinsic, and image rendering from unposed images. We demonstrate,
with examples, that for a unified framework to accurately model multiscale
neural scene representation from day-to-day acquired unposed multi-view images,
it is equally essential to have precise camera-pose estimates within the scene
representation framework. Without considering robustness measures in the camera
pose estimation pipeline, modeling for multi-scale aliasing artifacts can be
counterproductive. We present extensive experiments on several benchmark
datasets to demonstrate the suitability of our approach.";Nishant Jain<author:sep>Suryansh Kumar<author:sep>Luc Van Gool;http://arxiv.org/pdf/2311.04521v1;cs.CV;"Accepted for publication at International Journal of Computer Vision
  (IJCV). Draft info: 22 pages, 12 figures and 14 tables";
2311.03784v2;http://arxiv.org/abs/2311.03784v2;2023-11-07;UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields;"Neural Radiance Field (NeRF) has enabled novel view synthesis with high
fidelity given images and camera poses. Subsequent works even succeeded in
eliminating the necessity of pose priors by jointly optimizing NeRF and camera
pose. However, these works are limited to relatively simple settings such as
photometrically consistent and occluder-free image collections or a sequence of
images from a video. So they have difficulty handling unconstrained images with
varying illumination and transient occluders. In this paper, we propose
$\textbf{UP-NeRF}$ ($\textbf{U}$nconstrained $\textbf{P}$ose-prior-free
$\textbf{Ne}$ural $\textbf{R}$adiance $\textbf{F}$ields) to optimize NeRF with
unconstrained image collections without camera pose prior. We tackle these
challenges with surrogate tasks that optimize color-insensitive feature fields
and a separate module for transient occluders to block their influence on pose
estimation. In addition, we introduce a candidate head to enable more robust
pose estimation and transient-aware depth supervision to minimize the effect of
incorrect prior. Our experiments verify the superior performance of our method
compared to the baselines including BARF and its variants in a challenging
internet photo collection, $\textit{Phototourism}$ dataset.";Injae Kim<author:sep>Minhyuk Choi<author:sep>Hyunwoo J. Kim;http://arxiv.org/pdf/2311.03784v2;cs.CV;"Neural Information Processing Systems (NeurIPS), 2023. The code is
  available at https://github.com/mlvlab/UP-NeRF";nerf
2311.03965v1;http://arxiv.org/abs/2311.03965v1;2023-11-07;Fast Sun-aligned Outdoor Scene Relighting based on TensoRF;"In this work, we introduce our method of outdoor scene relighting for Neural
Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF).
SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun,
thereby achieving a simplified workflow that eliminates the need for
environment maps. Our sun-alignment strategy is motivated by the insight that
shadows, unlike viewpoint-dependent albedo, are determined by light direction.
We directly use the sun direction as an input during shadow generation,
simplifying the requirements of the inference process significantly. Moreover,
SR-TensoRF leverages the training efficiency of TensoRF by incorporating our
proposed cubemap concept, resulting in notable acceleration in both training
and rendering processes compared to existing methods.";Yeonjin Chang<author:sep>Yearim Kim<author:sep>Seunghyeon Seo<author:sep>Jung Yi<author:sep>Nojun Kwak;http://arxiv.org/pdf/2311.03965v1;cs.CV;WACV 2024;nerf
2311.04246v2;http://arxiv.org/abs/2311.04246v2;2023-11-07;ADFactory: An Effective Framework for Generalizing Optical Flow with  Nerf;"A significant challenge facing current optical flow methods is the difficulty
in generalizing them well to the real world. This is mainly due to the high
cost of hand-crafted datasets, and existing self-supervised methods are limited
by indirect loss and occlusions, resulting in fuzzy outcomes. To address this
challenge, we introduce a novel optical flow training framework: automatic data
factory (ADF). ADF only requires RGB images as input to effectively train the
optical flow network on the target data domain. Specifically, we use advanced
Nerf technology to reconstruct scenes from photo groups collected by a
monocular camera, and then calculate optical flow labels between camera pose
pairs based on the rendering results. To eliminate erroneous labels caused by
defects in the scene reconstructed by Nerf, we screened the generated labels
from multiple aspects, such as optical flow matching accuracy, radiation field
confidence, and depth consistency. The filtered labels can be directly used for
network supervision. Experimentally, the generalization ability of ADF on KITTI
surpasses existing self-supervised optical flow and monocular scene flow
algorithms. In addition, ADF achieves impressive results in real-world
zero-point generalization evaluations and surpasses most supervised methods.";Han Ling;http://arxiv.org/pdf/2311.04246v2;cs.CV;8 pages;nerf
2311.04154v1;http://arxiv.org/abs/2311.04154v1;2023-11-07;High-fidelity 3D Reconstruction of Plants using Neural Radiance Field;"Accurate reconstruction of plant phenotypes plays a key role in optimising
sustainable farming practices in the field of Precision Agriculture (PA).
Currently, optical sensor-based approaches dominate the field, but the need for
high-fidelity 3D reconstruction of crops and plants in unstructured
agricultural environments remains challenging. Recently, a promising
development has emerged in the form of Neural Radiance Field (NeRF), a novel
method that utilises neural density fields. This technique has shown impressive
performance in various novel vision synthesis tasks, but has remained
relatively unexplored in the agricultural context. In our study, we focus on
two fundamental tasks within plant phenotyping: (1) the synthesis of 2D
novel-view images and (2) the 3D reconstruction of crop and plant models. We
explore the world of neural radiance fields, in particular two SOTA methods:
Instant-NGP, which excels in generating high-quality images with impressive
training and inference speed, and Instant-NSR, which improves the reconstructed
geometry by incorporating the Signed Distance Function (SDF) during training.
In particular, we present a novel plant phenotype dataset comprising real plant
images from production environments. This dataset is a first-of-its-kind
initiative aimed at comprehensively exploring the advantages and limitations of
NeRF in agricultural contexts. Our experimental results show that NeRF
demonstrates commendable performance in the synthesis of novel-view images and
is able to achieve reconstruction results that are competitive with Reality
Capture, a leading commercial software for 3D Multi-View Stereo (MVS)-based
reconstruction. However, our study also highlights certain drawbacks of NeRF,
including relatively slow training speeds, performance limitations in cases of
insufficient sampling, and challenges in obtaining geometry quality in complex
setups.";Kewei Hu<author:sep>Ying Wei<author:sep>Yaoqiang Pan<author:sep>Hanwen Kang<author:sep>Chao Chen;http://arxiv.org/pdf/2311.04154v1;cs.CV;;nerf
2311.02848v1;http://arxiv.org/abs/2311.02848v1;2023-11-06;Consistent4D: Consistent 360Â° Dynamic Object Generation from  Monocular Video;"In this paper, we present Consistent4D, a novel approach for generating 4D
dynamic objects from uncalibrated monocular videos. Uniquely, we cast the
360-degree dynamic object reconstruction as a 4D generation problem,
eliminating the need for tedious multi-view data collection and camera
calibration. This is achieved by leveraging the object-level 3D-aware image
diffusion model as the primary supervision signal for training Dynamic Neural
Radiance Fields (DyNeRF). Specifically, we propose a Cascade DyNeRF to
facilitate stable convergence and temporal continuity under the supervision
signal which is discrete along the time axis. To achieve spatial and temporal
consistency, we further introduce an Interpolation-driven Consistency Loss. It
is optimized by minimizing the discrepancy between rendered frames from DyNeRF
and interpolated frames from a pre-trained video interpolation model. Extensive
experiments show that our Consistent4D can perform competitively to prior art
alternatives, opening up new possibilities for 4D dynamic object generation
from monocular videos, whilst also demonstrating advantage for conventional
text-to-3D generation tasks. Our project page is
https://consistent4d.github.io/.";Yanqin Jiang<author:sep>Li Zhang<author:sep>Jin Gao<author:sep>Weimin Hu<author:sep>Yao Yao;http://arxiv.org/pdf/2311.02848v1;cs.CV;Technique report. Project page: https://consistent4d.github.io/;nerf
2311.02826v1;http://arxiv.org/abs/2311.02826v1;2023-11-06;InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image;"With the success of Neural Radiance Field (NeRF) in 3D-aware portrait
editing, a variety of works have achieved promising results regarding both
quality and 3D consistency. However, these methods heavily rely on per-prompt
optimization when handling natural language as editing instructions. Due to the
lack of labeled human face 3D datasets and effective architectures, the area of
human-instructed 3D-aware editing for open-world portraits in an end-to-end
manner remains under-explored. To solve this problem, we propose an end-to-end
diffusion-based framework termed InstructPix2NeRF, which enables instructed
3D-aware portrait editing from a single open-world image with human
instructions. At its core lies a conditional latent 3D diffusion process that
lifts 2D editing to 3D space by learning the correlation between the paired
images' difference and the instructions via triplet data. With the help of our
proposed token position randomization strategy, we could even achieve
multi-semantic editing through one single pass with the portrait identity
well-preserved. Besides, we further propose an identity consistency module that
directly modulates the extracted identity signals into our diffusion process,
which increases the multi-view 3D identity consistency. Extensive experiments
verify the effectiveness of our method and show its superiority against strong
baselines quantitatively and qualitatively.";Jianhui Li<author:sep>Shilong Liu<author:sep>Zidong Liu<author:sep>Yikai Wang<author:sep>Kaiwen Zheng<author:sep>Jinghui Xu<author:sep>Jianmin Li<author:sep>Jun Zhu;http://arxiv.org/pdf/2311.02826v1;cs.CV;https://github.com/mybabyyh/InstructPix2NeRF;nerf
2311.03345v1;http://arxiv.org/abs/2311.03345v1;2023-11-06;Long-Term Invariant Local Features via Implicit Cross-Domain  Correspondences;"Modern learning-based visual feature extraction networks perform well in
intra-domain localization, however, their performance significantly declines
when image pairs are captured across long-term visual domain variations, such
as different seasonal and daytime variations. In this paper, our first
contribution is a benchmark to investigate the performance impact of long-term
variations on visual localization. We conduct a thorough analysis of the
performance of current state-of-the-art feature extraction networks under
various domain changes and find a significant performance gap between intra-
and cross-domain localization. We investigate different methods to close this
gap by improving the supervision of modern feature extractor networks. We
propose a novel data-centric method, Implicit Cross-Domain Correspondences
(iCDC). iCDC represents the same environment with multiple Neural Radiance
Fields, each fitting the scene under individual visual domains. It utilizes the
underlying 3D representations to generate accurate correspondences across
different long-term visual conditions. Our proposed method enhances
cross-domain localization performance, significantly reducing the performance
gap. When evaluated on popular long-term localization benchmarks, our trained
networks consistently outperform existing methods. This work serves as a
substantial stride toward more robust visual localization pipelines for
long-term deployments, and opens up research avenues in the development of
long-term invariant descriptors.";Zador Pataki<author:sep>Mohammad Altillawi<author:sep>Menelaos Kanakis<author:sep>RÃ©mi Pautrat<author:sep>Fengyi Shen<author:sep>Ziyuan Liu<author:sep>Luc Van Gool<author:sep>Marc Pollefeys;http://arxiv.org/pdf/2311.03345v1;cs.CV;14 pages + 5 pages appendix, 13 figures;
2311.03484v1;http://arxiv.org/abs/2311.03484v1;2023-11-06;Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM  and Next Best View Planning;"Aerial mapping systems are important for many surveying applications (e.g.,
industrial inspection or agricultural monitoring). Semi-autonomous mapping with
GPS-guided aerial platforms that fly preplanned missions is already widely
available but fully autonomous systems can significantly improve efficiency.
Autonomously mapping complex 3D structures requires a system that performs
online mapping and mission planning. This paper presents Osprey, an autonomous
aerial mapping system with state-of-the-art multi-session mapping capabilities.
It enables a non-expert operator to specify a bounded target area that the
aerial platform can then map autonomously, over multiple flights if necessary.
Field experiments with Osprey demonstrate that this system can achieve greater
map coverage of large industrial sites than manual surveys with a pilot-flown
aerial platform or a terrestrial laser scanner (TLS). Three sites, with a total
ground coverage of $7085$ m$^2$ and a maximum height of $27$ m, were mapped in
separate missions using $112$ minutes of autonomous flight time. True colour
maps were created from images captured by Osprey using pointcloud and NeRF
reconstruction methods. These maps provide useful data for structural
inspection tasks.";Rowan Border<author:sep>Nived Chebrolu<author:sep>Yifu Tao<author:sep>Jonathan D. Gammell<author:sep>Maurice Fallon;http://arxiv.org/pdf/2311.03484v1;cs.RO;"Submitted to Field Robotics, Manuscript #FR-23-0016. 25 pages, 15
  figures, 3 tables. Video available at
  https://www.youtube.com/watch?v=CVIXu2qUQJ8";nerf
2311.03140v1;http://arxiv.org/abs/2311.03140v1;2023-11-06;Animating NeRFs from Texture Space: A Framework for Pose-Dependent  Rendering of Human Performances;"Creating high-quality controllable 3D human models from multi-view RGB videos
poses a significant challenge. Neural radiance fields (NeRFs) have demonstrated
remarkable quality in reconstructing and free-viewpoint rendering of static as
well as dynamic scenes. The extension to a controllable synthesis of dynamic
human performances poses an exciting research question. In this paper, we
introduce a novel NeRF-based framework for pose-dependent rendering of human
performances. In our approach, the radiance field is warped around an SMPL body
mesh, thereby creating a new surface-aligned representation. Our representation
can be animated through skeletal joint parameters that are provided to the NeRF
in addition to the viewpoint for pose dependent appearances. To achieve this,
our representation includes the corresponding 2D UV coordinates on the mesh
texture map and the distance between the query point and the mesh. To enable
efficient learning despite mapping ambiguities and random visual variations, we
introduce a novel remapping process that refines the mapped coordinates.
Experiments demonstrate that our approach results in high-quality renderings
for novel-view and novel-pose synthesis.";Paul Knoll<author:sep>Wieland Morgenstern<author:sep>Anna Hilsmann<author:sep>Peter Eisert;http://arxiv.org/pdf/2311.03140v1;cs.CV;;nerf
2311.02542v1;http://arxiv.org/abs/2311.02542v1;2023-11-05;VR-NeRF: High-Fidelity Virtualized Walkable Spaces;"We present an end-to-end system for the high-fidelity capture, model
reconstruction, and real-time rendering of walkable spaces in virtual reality
using neural radiance fields. To this end, we designed and built a custom
multi-camera rig to densely capture walkable spaces in high fidelity and with
multi-view high dynamic range images in unprecedented quality and density. We
extend instant neural graphics primitives with a novel perceptual color space
for learning accurate HDR appearance, and an efficient mip-mapping mechanism
for level-of-detail rendering with anti-aliasing, while carefully optimizing
the trade-off between quality and speed. Our multi-GPU renderer enables
high-fidelity volume rendering of our neural radiance field model at the full
VR resolution of dual 2K$\times$2K at 36 Hz on our custom demo machine. We
demonstrate the quality of our results on our challenging high-fidelity
datasets, and compare our method and datasets to existing baselines. We release
our dataset on our project website.";Linning Xu<author:sep>Vasu Agrawal<author:sep>William Laney<author:sep>Tony Garcia<author:sep>Aayush Bansal<author:sep>Changil Kim<author:sep>Samuel Rota BulÃ²<author:sep>Lorenzo Porzi<author:sep>Peter Kontschieder<author:sep>AljaÅ¾ BoÅ¾iÄ<author:sep>Dahua Lin<author:sep>Michael ZollhÃ¶fer<author:sep>Christian Richardt;http://arxiv.org/pdf/2311.02542v1;cs.CV;"SIGGRAPH Asia 2023; Project page: https://vr-nerf.github.io";nerf
2311.01815v2;http://arxiv.org/abs/2311.01815v2;2023-11-03;Estimating 3D Uncertainty Field: Quantifying Uncertainty for Neural  Radiance Fields;"Current methods based on Neural Radiance Fields (NeRF) significantly lack the
capacity to quantify uncertainty in their predictions, particularly on the
unseen space including the occluded and outside scene content. This limitation
hinders their extensive applications in robotics, where the reliability of
model predictions has to be considered for tasks such as robotic exploration
and planning in unknown environments. To address this, we propose a novel
approach to estimate a 3D Uncertainty Field based on the learned incomplete
scene geometry, which explicitly identifies these unseen regions. By
considering the accumulated transmittance along each camera ray, our
Uncertainty Field infers 2D pixel-wise uncertainty, exhibiting high values for
rays directly casting towards occluded or outside the scene content. To
quantify the uncertainty on the learned surface, we model a stochastic radiance
field. Our experiments demonstrate that our approach is the only one that can
explicitly reason about high uncertainty both on 3D unseen regions and its
involved 2D rendered pixels, compared with recent methods. Furthermore, we
illustrate that our designed uncertainty field is ideally suited for real-world
robotics tasks, such as next-best-view selection.";Jianxiong Shen<author:sep>Ruijie Ren<author:sep>Adria Ruiz<author:sep>Francesc Moreno-Noguer;http://arxiv.org/pdf/2311.01815v2;cs.CV;;nerf
2311.01773v1;http://arxiv.org/abs/2311.01773v1;2023-11-03;PDF: Point Diffusion Implicit Function for Large-scale Scene Neural  Representation;"Recent advances in implicit neural representations have achieved impressive
results by sampling and fusing individual points along sampling rays in the
sampling space. However, due to the explosively growing sampling space, finely
representing and synthesizing detailed textures remains a challenge for
unbounded large-scale outdoor scenes. To alleviate the dilemma of using
individual points to perceive the entire colossal space, we explore learning
the surface distribution of the scene to provide structural priors and reduce
the samplable space and propose a Point Diffusion implicit Function, PDF, for
large-scale scene neural representation. The core of our method is a
large-scale point cloud super-resolution diffusion module that enhances the
sparse point cloud reconstructed from several training images into a dense
point cloud as an explicit prior. Then in the rendering stage, only sampling
points with prior points within the sampling radius are retained. That is, the
sampling space is reduced from the unbounded space to the scene surface.
Meanwhile, to fill in the background of the scene that cannot be provided by
point clouds, the region sampling based on Mip-NeRF 360 is employed to model
the background representation. Expensive experiments have demonstrated the
effectiveness of our method for large-scale scene novel view synthesis, which
outperforms relevant state-of-the-art baselines.";Yuhan Ding<author:sep>Fukun Yin<author:sep>Jiayuan Fan<author:sep>Hui Li<author:sep>Xin Chen<author:sep>Wen Liu<author:sep>Chongshan Lu<author:sep>Gang YU<author:sep>Tao Chen;http://arxiv.org/pdf/2311.01773v1;cs.CV;Accepted to NeurIPS 2023;nerf
2311.01659v1;http://arxiv.org/abs/2311.01659v1;2023-11-03;Efficient Cloud Pipelines for Neural Radiance Fields;"Since their introduction in 2020, Neural Radiance Fields (NeRFs) have taken
the computer vision community by storm. They provide a multi-view
representation of a scene or object that is ideal for eXtended Reality (XR)
applications and for creative endeavors such as virtual production, as well as
change detection operations in geospatial analytics. The computational cost of
these generative AI models is quite high, however, and the construction of
cloud pipelines to generate NeRFs is neccesary to realize their potential in
client applications. In this paper, we present pipelines on a high performance
academic computing cluster and compare it with a pipeline implemented on
Microsoft Azure. Along the way, we describe some uses of NeRFs in enabling
novel user interaction scenarios.";Derek Jacoby<author:sep>Donglin Xu<author:sep>Weder Ribas<author:sep>Minyi Xu<author:sep>Ting Liu<author:sep>Vishwanath Jayaraman<author:sep>Mengdi Wei<author:sep>Emma De Blois<author:sep>Yvonne Coady;http://arxiv.org/pdf/2311.01659v1;cs.CV;;nerf
2311.01653v1;http://arxiv.org/abs/2311.01653v1;2023-11-03;INeAT: Iterative Neural Adaptive Tomography;"Computed Tomography (CT) with its remarkable capability for three-dimensional
imaging from multiple projections, enjoys a broad range of applications in
clinical diagnosis, scientific observation, and industrial detection. Neural
Adaptive Tomography (NeAT) is a recently proposed 3D rendering method based on
neural radiance field for CT, and it demonstrates superior performance compared
to traditional methods. However, it still faces challenges when dealing with
the substantial perturbations and pose shifts encountered in CT scanning
processes. Here, we propose a neural rendering method for CT reconstruction,
named Iterative Neural Adaptive Tomography (INeAT), which incorporates
iterative posture optimization to effectively counteract the influence of
posture perturbations in data, particularly in cases involving significant
posture variations. Through the implementation of a posture feedback
optimization strategy, INeAT iteratively refines the posture corresponding to
the input images based on the reconstructed 3D volume. We demonstrate that
INeAT achieves artifact-suppressed and resolution-enhanced reconstruction in
scenarios with significant pose disturbances. Furthermore, we show that our
INeAT maintains comparable reconstruction performance to stable-state
acquisitions even using data from unstable-state acquisitions, which
significantly reduces the time required for CT scanning and relaxes the
stringent requirements on imaging hardware systems, underscoring its immense
potential for applications in short-time and low-cost CT technology.";Bo Xiong<author:sep>Changqing Su<author:sep>Zihan Lin<author:sep>You Zhou<author:sep>Zhaofei Yu;http://arxiv.org/pdf/2311.01653v1;eess.IV;;
2311.01842v1;http://arxiv.org/abs/2311.01842v1;2023-11-03;A Neural Radiance Field-Based Architecture for Intelligent Multilayered  View Synthesis;"A mobile ad hoc network is made up of a number of wireless portable nodes
that spontaneously come together en route for establish a transitory network
with no need for any central management. A mobile ad hoc network (MANET) is
made up of a sizable and reasonably dense community of mobile nodes that travel
across any terrain and rely solely on wireless interfaces for communication,
not on any well before centralized management. Furthermore, routing be supposed
to offer a method for instantly delivering data across a network between any
two nodes. Finding the best packet routing from across infrastructure is the
major issue, though. The proposed protocol's major goal is to identify the
least-expensive nominal capacity acquisition that assures the transportation of
realistic transport that ensures its durability in the event of any node
failure. This study suggests the Optimized Route Selection via Red Imported
Fire Ants (RIFA) Strategy as a way to improve on-demand source routing systems.
Predicting Route Failure and energy Utilization is used to pick the path during
the routing phase. Proposed work assess the results of the comparisons based on
performance parameters like as energy usage, packet delivery rate (PDR), and
end-to-end (E2E) delay. The outcome demonstrates that the proposed strategy is
preferable and increases network lifetime while lowering node energy
consumption and typical E2E delay under the majority of network performance
measures and factors.";D. Dhinakaran<author:sep>S. M. Udhaya Sankar<author:sep>G. Elumalai<author:sep>N. Jagadish kumar;http://arxiv.org/pdf/2311.01842v1;cs.NI;;
2311.01065v1;http://arxiv.org/abs/2311.01065v1;2023-11-02;Novel View Synthesis from a Single RGBD Image for Indoor Scenes;"In this paper, we propose an approach for synthesizing novel view images from
a single RGBD (Red Green Blue-Depth) input. Novel view synthesis (NVS) is an
interesting computer vision task with extensive applications. Methods using
multiple images has been well-studied, exemplary ones include training
scene-specific Neural Radiance Fields (NeRF), or leveraging multi-view stereo
(MVS) and 3D rendering pipelines. However, both are either computationally
intensive or non-generalizable across different scenes, limiting their
practical value. Conversely, the depth information embedded in RGBD images
unlocks 3D potential from a singular view, simplifying NVS. The widespread
availability of compact, affordable stereo cameras, and even LiDARs in
contemporary devices like smartphones, makes capturing RGBD images more
accessible than ever. In our method, we convert an RGBD image into a point
cloud and render it from a different viewpoint, then formulate the NVS task
into an image translation problem. We leveraged generative adversarial networks
to style-transfer the rendered image, achieving a result similar to a
photograph taken from the new perspective. We explore both unsupervised
learning using CycleGAN and supervised learning with Pix2Pix, and demonstrate
the qualitative results. Our method circumvents the limitations of traditional
multi-image techniques, holding significant promise for practical, real-time
applications in NVS.";Congrui Hetang<author:sep>Yuping Wang;http://arxiv.org/pdf/2311.01065v1;cs.CV;"2nd International Conference on Image Processing, Computer Vision and
  Machine Learning, November 2023";nerf
2310.20685v1;http://arxiv.org/abs/2310.20685v1;2023-10-31;NeRF Revisited: Fixing Quadrature Instability in Volume Rendering;"Neural radiance fields (NeRF) rely on volume rendering to synthesize novel
views. Volume rendering requires evaluating an integral along each ray, which
is numerically approximated with a finite sum that corresponds to the exact
integral along the ray under piecewise constant volume density. As a
consequence, the rendered result is unstable w.r.t. the choice of samples along
the ray, a phenomenon that we dub quadrature instability. We propose a
mathematically principled solution by reformulating the sample-based rendering
equation so that it corresponds to the exact integral under piecewise linear
volume density. This simultaneously resolves multiple issues: conflicts between
samples along different rays, imprecise hierarchical sampling, and
non-differentiability of quantiles of ray termination distances w.r.t. model
parameters. We demonstrate several benefits over the classical sample-based
rendering equation, such as sharper textures, better geometric reconstruction,
and stronger depth supervision. Our proposed formulation can be also be used as
a drop-in replacement to the volume rendering equation of existing NeRF-based
methods. Our project page can be found at pl-nerf.github.io.";Mikaela Angelina Uy<author:sep>Kiyohiro Nakayama<author:sep>Guandao Yang<author:sep>Rahul Krishna Thomas<author:sep>Leonidas Guibas<author:sep>Ke Li;http://arxiv.org/pdf/2310.20685v1;cs.CV;Neurips 2023;nerf
2310.20710v1;http://arxiv.org/abs/2310.20710v1;2023-10-31;FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance  Fields by Analyzing and Enhancing Fourier PlenOctrees;"Fourier PlenOctrees have shown to be an efficient representation for
real-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many
advantages, this method suffers from artifacts introduced by the involved
compression when combining it with recent state-of-the-art techniques for
training the static per-frame NeRF models. In this paper, we perform an
in-depth analysis of these artifacts and leverage the resulting insights to
propose an improved representation. In particular, we present a novel density
encoding that adapts the Fourier-based compression to the characteristics of
the transfer function used by the underlying volume rendering procedure and
leads to a substantial reduction of artifacts in the dynamic model.
Furthermore, we show an augmentation of the training data that relaxes the
periodicity assumption of the compression. We demonstrate the effectiveness of
our enhanced Fourier PlenOctrees in the scope of quantitative and qualitative
evaluations on synthetic and real-world scenes.";Saskia Rabich<author:sep>Patrick Stotko<author:sep>Reinhard Klein;http://arxiv.org/pdf/2310.20710v1;cs.CV;;nerf
2310.19464v1;http://arxiv.org/abs/2310.19464v1;2023-10-30;Generative Neural Fields by Mixtures of Neural Implicit Functions;"We propose a novel approach to learning the generative neural fields
represented by linear combinations of implicit basis networks. Our algorithm
learns basis networks in the form of implicit neural representations and their
coefficients in a latent space by either conducting meta-learning or adopting
auto-decoding paradigms. The proposed method easily enlarges the capacity of
generative neural fields by increasing the number of basis networks while
maintaining the size of a network for inference to be small through their
weighted model averaging. Consequently, sampling instances using the model is
efficient in terms of latency and memory footprint. Moreover, we customize
denoising diffusion probabilistic model for a target task to sample latent
mixture coefficients, which allows our final model to generate unseen data
effectively. Experiments show that our approach achieves competitive generation
performance on diverse benchmarks for images, voxel data, and NeRF scenes
without sophisticated designs for specific modalities and domains.";Tackgeun You<author:sep>Mijeong Kim<author:sep>Jungtaek Kim<author:sep>Bohyung Han;http://arxiv.org/pdf/2310.19464v1;cs.LG;;nerf
2310.19441v1;http://arxiv.org/abs/2310.19441v1;2023-10-30;Dynamic Gaussian Splatting from Markerless Motion Capture can  Reconstruct Infants Movements;"Easy access to precise 3D tracking of movement could benefit many aspects of
rehabilitation. A challenge to achieving this goal is that while there are many
datasets and pretrained algorithms for able-bodied adults, algorithms trained
on these datasets often fail to generalize to clinical populations including
people with disabilities, infants, and neonates. Reliable movement analysis of
infants and neonates is important as spontaneous movement behavior is an
important indicator of neurological function and neurodevelopmental disability,
which can help guide early interventions. We explored the application of
dynamic Gaussian splatting to sparse markerless motion capture (MMC) data. Our
approach leverages semantic segmentation masks to focus on the infant,
significantly improving the initialization of the scene. Our results
demonstrate the potential of this method in rendering novel views of scenes and
tracking infant movements. This work paves the way for advanced movement
analysis tools that can be applied to diverse clinical populations, with a
particular emphasis on early detection in infants.";R. James Cotton<author:sep>Colleen Peyton;http://arxiv.org/pdf/2310.19441v1;cs.CV;;gaussian splatting
2311.16127v1;http://arxiv.org/abs/2311.16127v1;2023-10-30;SeamlessNeRF: Stitching Part NeRFs with Gradient Propagation;"Neural Radiance Fields (NeRFs) have emerged as promising digital mediums of
3D objects and scenes, sparking a surge in research to extend the editing
capabilities in this domain. The task of seamless editing and merging of
multiple NeRFs, resembling the ``Poisson blending'' in 2D image editing,
remains a critical operation that is under-explored by existing work. To fill
this gap, we propose SeamlessNeRF, a novel approach for seamless appearance
blending of multiple NeRFs. In specific, we aim to optimize the appearance of a
target radiance field in order to harmonize its merge with a source field. We
propose a well-tailored optimization procedure for blending, which is
constrained by 1) pinning the radiance color in the intersecting boundary area
between the source and target fields and 2) maintaining the original gradient
of the target. Extensive experiments validate that our approach can effectively
propagate the source appearance from the boundary area to the entire target
field through the gradients. To the best of our knowledge, SeamlessNeRF is the
first work that introduces gradient-guided appearance editing to radiance
fields, offering solutions for seamless stitching of 3D objects represented in
NeRFs.";Bingchen Gong<author:sep>Yuehao Wang<author:sep>Xiaoguang Han<author:sep>Qi Dou;http://arxiv.org/pdf/2311.16127v1;cs.CV;"To appear in SIGGRAPH Asia 2023. Project website is accessible at
  https://sites.google.com/view/seamlessnerf";nerf
2310.18999v2;http://arxiv.org/abs/2310.18999v2;2023-10-29;DynPoint: Dynamic Neural Point For View Synthesis;"The introduction of neural radiance fields has greatly improved the
effectiveness of view synthesis for monocular videos. However, existing
algorithms face difficulties when dealing with uncontrolled or lengthy
scenarios, and require extensive training time specific to each new scenario.
To tackle these limitations, we propose DynPoint, an algorithm designed to
facilitate the rapid synthesis of novel views for unconstrained monocular
videos. Rather than encoding the entirety of the scenario information into a
latent representation, DynPoint concentrates on predicting the explicit 3D
correspondence between neighboring frames to realize information aggregation.
Specifically, this correspondence prediction is achieved through the estimation
of consistent depth and scene flow information across frames. Subsequently, the
acquired correspondence is utilized to aggregate information from multiple
reference frames to a target frame, by constructing hierarchical neural point
clouds. The resulting framework enables swift and accurate view synthesis for
desired views of target frames. The experimental results obtained demonstrate
the considerable acceleration of training time achieved - typically an order of
magnitude - by our proposed method while yielding comparable outcomes compared
to prior approaches. Furthermore, our method exhibits strong robustness in
handling long-duration videos without learning a canonical representation of
video content.";Kaichen Zhou<author:sep>Jia-Xing Zhong<author:sep>Sangyun Shin<author:sep>Kai Lu<author:sep>Yiyuan Yang<author:sep>Andrew Markham<author:sep>Niki Trigoni;http://arxiv.org/pdf/2310.18999v2;cs.CV;;
2310.18917v2;http://arxiv.org/abs/2310.18917v2;2023-10-29;TiV-NeRF: Tracking and Mapping via Time-Varying Representation with  Dynamic Neural Radiance Fields;"Previous attempts to integrate Neural Radiance Fields (NeRF) into
Simultaneous Localization and Mapping (SLAM) framework either rely on the
assumption of static scenes or treat dynamic objects as outliers. However, most
of real-world scenarios is dynamic. In this paper, we propose a time-varying
representation to track and reconstruct the dynamic scenes. Our system
simultaneously maintains two processes, tracking process and mapping process.
For tracking process, the entire input images are uniformly sampled and
training of the RGB images are self-supervised. For mapping process, we
leverage know masks to differentiate dynamic objects and static backgrounds,
and we apply distinct sampling strategies for two types of areas. The
parameters optimization for both processes are made up by two stages, the first
stage associates time with 3D positions to convert the deformation field to the
canonical field. And the second associates time with 3D positions in canonical
field to obtain colors and Signed Distance Function (SDF). Besides, We propose
a novel keyframe selection strategy based on the overlapping rate. We evaluate
our approach on two publicly available synthetic datasets and validate that our
method is more effective compared to current state-of-the-art dynamic mapping
methods.";Chengyao Duan<author:sep>Zhiliu Yang;http://arxiv.org/pdf/2310.18917v2;cs.CV;;nerf
2310.18846v1;http://arxiv.org/abs/2310.18846v1;2023-10-28;INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings;"Implicit Neural Representations (INRs) have revolutionized signal
representation by leveraging neural networks to provide continuous and smooth
representations of complex data. However, existing INRs face limitations in
capturing fine-grained details, handling noise, and adapting to diverse signal
types. To address these challenges, we introduce INCODE, a novel approach that
enhances the control of the sinusoidal-based activation function in INRs using
deep prior knowledge. INCODE comprises a harmonizer network and a composer
network, where the harmonizer network dynamically adjusts key parameters of the
activation function. Through a task-specific pre-trained model, INCODE adapts
the task-specific parameters to optimize the representation process. Our
approach not only excels in representation, but also extends its prowess to
tackle complex tasks such as audio, image, and 3D shape reconstructions, as
well as intricate challenges such as neural radiance fields (NeRFs), and
inverse problems, including denoising, super-resolution, inpainting, and CT
reconstruction. Through comprehensive experiments, INCODE demonstrates its
superiority in terms of robustness, accuracy, quality, and convergence rate,
broadening the scope of signal representation. Please visit the project's
website for details on the proposed method and access to the code.";Amirhossein Kazerouni<author:sep>Reza Azad<author:sep>Alireza Hosseini<author:sep>Dorit Merhof<author:sep>Ulas Bagci;http://arxiv.org/pdf/2310.18846v1;cs.CV;Accepted at WACV 2024 conference;nerf
2310.17994v1;http://arxiv.org/abs/2310.17994v1;2023-10-27;ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Real Image;"We introduce a 3D-aware diffusion model, ZeroNVS, for single-image novel view
synthesis for in-the-wild scenes. While existing methods are designed for
single objects with masked backgrounds, we propose new techniques to address
challenges introduced by in-the-wild multi-object scenes with complex
backgrounds. Specifically, we train a generative prior on a mixture of data
sources that capture object-centric, indoor, and outdoor scenes. To address
issues from data mixture such as depth-scale ambiguity, we propose a novel
camera conditioning parameterization and normalization scheme. Further, we
observe that Score Distillation Sampling (SDS) tends to truncate the
distribution of complex backgrounds during distillation of 360-degree scenes,
and propose ""SDS anchoring"" to improve the diversity of synthesized novel
views. Our model sets a new state-of-the-art result in LPIPS on the DTU dataset
in the zero-shot setting, even outperforming methods specifically trained on
DTU. We further adapt the challenging Mip-NeRF 360 dataset as a new benchmark
for single-image novel view synthesis, and demonstrate strong performance in
this setting. Our code and data are at http://kylesargent.github.io/zeronvs/";Kyle Sargent<author:sep>Zizhang Li<author:sep>Tanmay Shah<author:sep>Charles Herrmann<author:sep>Hong-Xing Yu<author:sep>Yunzhi Zhang<author:sep>Eric Ryan Chan<author:sep>Dmitry Lagun<author:sep>Li Fei-Fei<author:sep>Deqing Sun<author:sep>Jiajun Wu;http://arxiv.org/pdf/2310.17994v1;cs.CV;17 pages;nerf
2310.17880v1;http://arxiv.org/abs/2310.17880v1;2023-10-27;Reconstructive Latent-Space Neural Radiance Fields for Efficient 3D  Scene Representations;"Neural Radiance Fields (NeRFs) have proven to be powerful 3D representations,
capable of high quality novel view synthesis of complex scenes. While NeRFs
have been applied to graphics, vision, and robotics, problems with slow
rendering speed and characteristic visual artifacts prevent adoption in many
use cases. In this work, we investigate combining an autoencoder (AE) with a
NeRF, in which latent features (instead of colours) are rendered and then
convolutionally decoded. The resulting latent-space NeRF can produce novel
views with higher quality than standard colour-space NeRFs, as the AE can
correct certain visual artifacts, while rendering over three times faster. Our
work is orthogonal to other techniques for improving NeRF efficiency. Further,
we can control the tradeoff between efficiency and image quality by shrinking
the AE architecture, achieving over 13 times faster rendering with only a small
drop in performance. We hope that our approach can form the basis of an
efficient, yet high-fidelity, 3D scene representation for downstream tasks,
especially when retaining differentiability is useful, as in many robotics
scenarios requiring continual learning.";Tristan Aumentado-Armstrong<author:sep>Ashkan Mirzaei<author:sep>Marcus A. Brubaker<author:sep>Jonathan Kelly<author:sep>Alex Levinshtein<author:sep>Konstantinos G. Derpanis<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2310.17880v1;cs.CV;;nerf
2310.17075v2;http://arxiv.org/abs/2310.17075v2;2023-10-26;HyperFields: Towards Zero-Shot Generation of NeRFs from Text;"We introduce HyperFields, a method for generating text-conditioned Neural
Radiance Fields (NeRFs) with a single forward pass and (optionally) some
fine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns
a smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF
distillation training, which distills scenes encoded in individual NeRFs into
one dynamic hypernetwork. These techniques enable a single network to fit over
a hundred unique scenes. We further demonstrate that HyperFields learns a more
general map between text and NeRFs, and consequently is capable of predicting
novel in-distribution and out-of-distribution scenes -- either zero-shot or
with a few finetuning steps. Finetuning HyperFields benefits from accelerated
convergence thanks to the learned general map, and is capable of synthesizing
novel scenes 5 to 10 times faster than existing neural optimization-based
methods. Our ablation experiments show that both the dynamic architecture and
NeRF distillation are critical to the expressivity of HyperFields.";Sudarshan Babu<author:sep>Richard Liu<author:sep>Avery Zhou<author:sep>Michael Maire<author:sep>Greg Shakhnarovich<author:sep>Rana Hanocka;http://arxiv.org/pdf/2310.17075v2;cs.CV;Project page: https://threedle.github.io/hyperfields/;nerf
2310.16383v1;http://arxiv.org/abs/2310.16383v1;2023-10-25;Open-NeRF: Towards Open Vocabulary NeRF Decomposition;"In this paper, we address the challenge of decomposing Neural Radiance Fields
(NeRF) into objects from an open vocabulary, a critical task for object
manipulation in 3D reconstruction and view synthesis. Current techniques for
NeRF decomposition involve a trade-off between the flexibility of processing
open-vocabulary queries and the accuracy of 3D segmentation. We present,
Open-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage
large-scale, off-the-shelf, segmentation models like the Segment Anything Model
(SAM) and introduce an integrate-and-distill paradigm with hierarchical
embeddings to achieve both the flexibility of open-vocabulary querying and 3D
segmentation accuracy. Open-NeRF first utilizes large-scale foundation models
to generate hierarchical 2D mask proposals from varying viewpoints. These
proposals are then aligned via tracking approaches and integrated within the 3D
space and subsequently distilled into the 3D field. This process ensures
consistent recognition and granularity of objects from different viewpoints,
even in challenging scenarios involving occlusion and indistinct features. Our
experimental results show that the proposed Open-NeRF outperforms
state-of-the-art methods such as LERF \cite{lerf} and FFD \cite{ffd} in
open-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF
decomposition, guided by open-vocabulary queries, enabling novel applications
in robotics and vision-language interaction in open-world 3D scenes.";Hao Zhang<author:sep>Fang Li<author:sep>Narendra Ahuja;http://arxiv.org/pdf/2310.16383v1;cs.CV;Accepted by WACV 2024;nerf
2310.16255v1;http://arxiv.org/abs/2310.16255v1;2023-10-25;UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception;"Tremendous variations coupled with large degrees of freedom in UAV-based
imaging conditions lead to a significant lack of data in adequately learning
UAV-based perception models. Using various synthetic renderers in conjunction
with perception models is prevalent to create synthetic data to augment the
learning in the ground-based imaging domain. However, severe challenges in the
austere UAV-based domain require distinctive solutions to image synthesis for
data augmentation. In this work, we leverage recent advancements in neural
rendering to improve static and dynamic novelview UAV-based image synthesis,
especially from high altitudes, capturing salient scene attributes. Finally, we
demonstrate a considerable performance boost is achieved when a state-ofthe-art
detection model is optimized primarily on hybrid sets of real and synthetic
data instead of the real or synthetic data separately.";Christopher Maxey<author:sep>Jaehoon Choi<author:sep>Hyungtae Lee<author:sep>Dinesh Manocha<author:sep>Heesung Kwon;http://arxiv.org/pdf/2310.16255v1;cs.CV;Video Link: https://www.youtube.com/watch?v=ucPzbPLqqpI;nerf
2310.16832v2;http://arxiv.org/abs/2310.16832v2;2023-10-25;LightSpeed: Light and Fast Neural Light Fields on Mobile Devices;"Real-time novel-view image synthesis on mobile devices is prohibitive due to
the limited computational power and storage. Using volumetric rendering
methods, such as NeRF and its derivatives, on mobile devices is not suitable
due to the high computational cost of volumetric rendering. On the other hand,
recent advances in neural light field representations have shown promising
real-time view synthesis results on mobile devices. Neural light field methods
learn a direct mapping from a ray representation to the pixel color. The
current choice of ray representation is either stratified ray sampling or
Plucker coordinates, overlooking the classic light slab (two-plane)
representation, the preferred representation to interpolate between light field
views. In this work, we find that using the light slab representation is an
efficient representation for learning a neural light field. More importantly,
it is a lower-dimensional ray representation enabling us to learn the 4D ray
space using feature grids which are significantly faster to train and render.
Although mostly designed for frontal views, we show that the light-slab
representation can be further extended to non-frontal scenes using a
divide-and-conquer strategy. Our method offers superior rendering quality
compared to previous light field methods and achieves a significantly improved
trade-off between rendering quality and speed.";Aarush Gupta<author:sep>Junli Cao<author:sep>Chaoyang Wang<author:sep>Ju Hu<author:sep>Sergey Tulyakov<author:sep>Jian Ren<author:sep>LÃ¡szlÃ³ A Jeni;http://arxiv.org/pdf/2310.16832v2;cs.CV;"Project Page: http://lightspeed-r2l.github.io/ . Add camera ready
  version";nerf
2310.16858v2;http://arxiv.org/abs/2310.16858v2;2023-10-25;4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance  Fields via Semantic Distillation;"This paper targets interactive object-level editing (e.g., deletion,
recoloring, transformation, composition) in dynamic scenes. Recently, some
methods aiming for flexible editing static scenes represented by neural
radiance field (NeRF) have shown impressive synthesis quality, while similar
capabilities in time-variant dynamic scenes remain limited. To solve this
problem, we propose 4D-Editor, an interactive semantic-driven editing
framework, allowing editing multiple objects in a dynamic NeRF with user
strokes on a single frame. We propose an extension to the original dynamic NeRF
by incorporating a hybrid semantic feature distillation to maintain
spatial-temporal consistency after editing. In addition, we design Recursive
Selection Refinement that significantly boosts object segmentation accuracy
within a dynamic NeRF to aid the editing process. Moreover, we develop
Multi-view Reprojection Inpainting to fill holes caused by incomplete scene
capture after editing. Extensive experiments and editing examples on real-world
demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs.
Project page: https://patrickddj.github.io/4D-Editor";Dadong Jiang<author:sep>Zhihui Ke<author:sep>Xiaobo Zhou<author:sep>Xidong Shi;http://arxiv.org/pdf/2310.16858v2;cs.CV;Project page: https://patrickddj.github.io/4D-Editor;nerf
2310.16831v2;http://arxiv.org/abs/2310.16831v2;2023-10-25;PERF: Panoramic Neural Radiance Field from a Single Panorama;"Neural Radiance Field (NeRF) has achieved substantial progress in novel view
synthesis given multi-view images. Recently, some works have attempted to train
a NeRF from a single image with 3D priors. They mainly focus on a limited field
of view with a few occlusions, which greatly limits their scalability to
real-world 360-degree panoramic scenarios with large-size occlusions. In this
paper, we present PERF, a 360-degree novel view synthesis framework that trains
a panoramic neural radiance field from a single panorama. Notably, PERF allows
3D roaming in a complex scene without expensive and tedious image collection.
To achieve this goal, we propose a novel collaborative RGBD inpainting method
and a progressive inpainting-and-erasing method to lift up a 360-degree 2D
scene to a 3D scene. Specifically, we first predict a panoramic depth map as
initialization given a single panorama and reconstruct visible 3D regions with
volume rendering. Then we introduce a collaborative RGBD inpainting approach
into a NeRF for completing RGB images and depth maps from random views, which
is derived from an RGB Stable Diffusion model and a monocular depth estimator.
Finally, we introduce an inpainting-and-erasing strategy to avoid inconsistent
geometry between a newly-sampled view and reference views. The two components
are integrated into the learning of NeRFs in a unified optimization framework
and achieve promising results. Extensive experiments on Replica and a new
dataset PERF-in-the-wild demonstrate the superiority of our PERF over
state-of-the-art methods. Our PERF can be widely used for real-world
applications, such as panorama-to-3D, text-to-3D, and 3D scene stylization
applications. Project page and code are available at
https://perf-project.github.io/ and https://github.com/perf-project/PeRF.";Guangcong Wang<author:sep>Peng Wang<author:sep>Zhaoxi Chen<author:sep>Wenping Wang<author:sep>Chen Change Loy<author:sep>Ziwei Liu;http://arxiv.org/pdf/2310.16831v2;cs.CV;"Project Page: https://perf-project.github.io/ , Code:
  https://github.com/perf-project/PeRF";nerf
2310.15504v1;http://arxiv.org/abs/2310.15504v1;2023-10-24;Cross-view Self-localization from Synthesized Scene-graphs;"Cross-view self-localization is a challenging scenario of visual place
recognition in which database images are provided from sparse viewpoints.
Recently, an approach for synthesizing database images from unseen viewpoints
using NeRF (Neural Radiance Fields) technology has emerged with impressive
performance. However, synthesized images provided by these techniques are often
of lower quality than the original images, and furthermore they significantly
increase the storage cost of the database. In this study, we explore a new
hybrid scene model that combines the advantages of view-invariant appearance
features computed from raw images and view-dependent spatial-semantic features
computed from synthesized images. These two types of features are then fused
into scene graphs, and compressively learned and recognized by a graph neural
network. The effectiveness of the proposed method was verified using a novel
cross-view self-localization dataset with many unseen views generated using a
photorealistic Habitat simulator.";Ryogo Yamamoto<author:sep>Kanji Tanaka;http://arxiv.org/pdf/2310.15504v1;cs.CV;5 pages, 5 figures, technical report;nerf
2310.14695v1;http://arxiv.org/abs/2310.14695v1;2023-10-23;CAwa-NeRF: Instant Learning of Compression-Aware NeRF Features;"Modeling 3D scenes by volumetric feature grids is one of the promising
directions of neural approximations to improve Neural Radiance Fields (NeRF).
Instant-NGP (INGP) introduced multi-resolution hash encoding from a lookup
table of trainable feature grids which enabled learning high-quality neural
graphics primitives in a matter of seconds. However, this improvement came at
the cost of higher storage size. In this paper, we address this challenge by
introducing instant learning of compression-aware NeRF features (CAwa-NeRF),
that allows exporting the zip compressed feature grids at the end of the model
training with a negligible extra time overhead without changing neither the
storage architecture nor the parameters used in the original INGP paper.
Nonetheless, the proposed method is not limited to INGP but could also be
adapted to any model. By means of extensive simulations, our proposed instant
learning pipeline can achieve impressive results on different kinds of static
scenes such as single object masked background scenes and real-life scenes
captured in our studio. In particular, for single object masked background
scenes CAwa-NeRF compresses the feature grids down to 6% (1.2 MB) of the
original size without any loss in the PSNR (33 dB) or down to 2.4% (0.53 MB)
with a slight virtual loss (32.31 dB).";Omnia Mahmoud<author:sep>ThÃ©o Ladune<author:sep>Matthieu Gendrin;http://arxiv.org/pdf/2310.14695v1;cs.CV;10 pages, 9 figures;nerf
2310.14487v1;http://arxiv.org/abs/2310.14487v1;2023-10-23;VQ-NeRF: Vector Quantization Enhances Implicit Neural Representations;"Recent advancements in implicit neural representations have contributed to
high-fidelity surface reconstruction and photorealistic novel view synthesis.
However, the computational complexity inherent in these methodologies presents
a substantial impediment, constraining the attainable frame rates and
resolutions in practical applications. In response to this predicament, we
propose VQ-NeRF, an effective and efficient pipeline for enhancing implicit
neural representations via vector quantization. The essence of our method
involves reducing the sampling space of NeRF to a lower resolution and
subsequently reinstating it to the original size utilizing a pre-trained VAE
decoder, thereby effectively mitigating the sampling time bottleneck
encountered during rendering. Although the codebook furnishes representative
features, reconstructing fine texture details of the scene remains challenging
due to high compression rates. To overcome this constraint, we design an
innovative multi-scale NeRF sampling scheme that concurrently optimizes the
NeRF model at both compressed and original scales to enhance the network's
ability to preserve fine details. Furthermore, we incorporate a semantic loss
function to improve the geometric fidelity and semantic coherence of our 3D
reconstructions. Extensive experiments demonstrate the effectiveness of our
model in achieving the optimal trade-off between rendering quality and
efficiency. Evaluation on the DTU, BlendMVS, and H3DS datasets confirms the
superior performance of our approach.";Yiying Yang<author:sep>Wen Liu<author:sep>Fukun Yin<author:sep>Xin Chen<author:sep>Gang Yu<author:sep>Jiayuan Fan<author:sep>Tao Chen;http://arxiv.org/pdf/2310.14487v1;cs.CV;"Submitted to the 38th Annual AAAI Conference on Artificial
  Intelligence";nerf
2310.13670v1;http://arxiv.org/abs/2310.13670v1;2023-10-20;ManifoldNeRF: View-dependent Image Feature Supervision for Few-shot  Neural Radiance Fields;"Novel view synthesis has recently made significant progress with the advent
of Neural Radiance Fields (NeRF). DietNeRF is an extension of NeRF that aims to
achieve this task from only a few images by introducing a new loss function for
unknown viewpoints with no input images. The loss function assumes that a
pre-trained feature extractor should output the same feature even if input
images are captured at different viewpoints since the images contain the same
object. However, while that assumption is ideal, in reality, it is known that
as viewpoints continuously change, also feature vectors continuously change.
Thus, the assumption can harm training. To avoid this harmful training, we
propose ManifoldNeRF, a method for supervising feature vectors at unknown
viewpoints using interpolated features from neighboring known viewpoints. Since
the method provides appropriate supervision for each unknown viewpoint by the
interpolated features, the volume representation is learned better than
DietNeRF. Experimental results show that the proposed method performs better
than others in a complex scene. We also experimented with several subsets of
viewpoints from a set of viewpoints and identified an effective set of
viewpoints for real environments. This provided a basic policy of viewpoint
patterns for real-world application. The code is available at
https://github.com/haganelego/ManifoldNeRF_BMVC2023";Daiju Kanaoka<author:sep>Motoharu Sonogashira<author:sep>Hakaru Tamukoh<author:sep>Yasutomo Kawanishi;http://arxiv.org/pdf/2310.13670v1;cs.CV;Accepted by BMVC2023;nerf
2310.13356v2;http://arxiv.org/abs/2310.13356v2;2023-10-20;Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos;"Recent advancements in 4D scene reconstruction using neural radiance fields
(NeRF) have demonstrated the ability to represent dynamic scenes from
multi-view videos. However, they fail to reconstruct the dynamic scenes and
struggle to fit even the training views in unsynchronized settings. It happens
because they employ a single latent embedding for a frame while the multi-view
images at the same frame were actually captured at different moments. To
address this limitation, we introduce time offsets for individual
unsynchronized videos and jointly optimize the offsets with NeRF. By design,
our method is applicable for various baselines and improves them with large
margins. Furthermore, finding the offsets naturally works as synchronizing the
videos without manual effort. Experiments are conducted on the common Plenoptic
Video Dataset and a newly built Unsynchronized Dynamic Blender Dataset to
verify the performance of our method. Project page:
https://seoha-kim.github.io/sync-nerf";Seoha Kim<author:sep>Jeongmin Bae<author:sep>Youngsik Yun<author:sep>Hahyun Lee<author:sep>Gun Bang<author:sep>Youngjung Uh;http://arxiv.org/pdf/2310.13356v2;cs.CV;AAAI 2024, Project page: https://seoha-kim.github.io/sync-nerf;nerf
2310.13263v1;http://arxiv.org/abs/2310.13263v1;2023-10-20;UE4-NeRF:Neural Radiance Field for Real-Time Rendering of Large-Scale  Scene;"Neural Radiance Fields (NeRF) is a novel implicit 3D reconstruction method
that shows immense potential and has been gaining increasing attention. It
enables the reconstruction of 3D scenes solely from a set of photographs.
However, its real-time rendering capability, especially for interactive
real-time rendering of large-scale scenes, still has significant limitations.
To address these challenges, in this paper, we propose a novel neural rendering
system called UE4-NeRF, specifically designed for real-time rendering of
large-scale scenes. We partitioned each large scene into different sub-NeRFs.
In order to represent the partitioned independent scene, we initialize
polygonal meshes by constructing multiple regular octahedra within the scene
and the vertices of the polygonal faces are continuously optimized during the
training process. Drawing inspiration from Level of Detail (LOD) techniques, we
trained meshes of varying levels of detail for different observation levels.
Our approach combines with the rasterization pipeline in Unreal Engine 4 (UE4),
achieving real-time rendering of large-scale scenes at 4K resolution with a
frame rate of up to 43 FPS. Rendering within UE4 also facilitates scene editing
in subsequent stages. Furthermore, through experiments, we have demonstrated
that our method achieves rendering quality comparable to state-of-the-art
approaches. Project page: https://jamchaos.github.io/UE4-NeRF/.";Jiaming Gu<author:sep>Minchao Jiang<author:sep>Hongsheng Li<author:sep>Xiaoyuan Lu<author:sep>Guangming Zhu<author:sep>Syed Afaq Ali Shah<author:sep>Liang Zhang<author:sep>Mohammed Bennamoun;http://arxiv.org/pdf/2310.13263v1;cs.CV;Accepted by NeurIPS2023;nerf
2310.11864v3;http://arxiv.org/abs/2310.11864v3;2023-10-18;VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector  Quantization;"We propose VQ-NeRF, a two-branch neural network model that incorporates
Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes.
Conventional neural reflectance fields use only continuous representations to
model 3D scenes, despite the fact that objects are typically composed of
discrete materials in reality. This lack of discretization can result in noisy
material decomposition and complicated material editing. To address these
limitations, our model consists of a continuous branch and a discrete branch.
The continuous branch follows the conventional pipeline to predict decomposed
materials, while the discrete branch uses the VQ mechanism to quantize
continuous materials into individual ones. By discretizing the materials, our
model can reduce noise in the decomposition process and generate a segmentation
map of discrete materials. Specific materials can be easily selected for
further editing by clicking on the corresponding area of the segmentation
outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy
to predict the number of materials in a scene, which reduces redundancy in the
material segmentation process. To improve usability, we also develop an
interactive interface to further assist material editing. We evaluate our model
on both computer-generated and real-world scenes, demonstrating its superior
performance. To the best of our knowledge, our model is the first to enable
discrete material editing in 3D scenes.";Hongliang Zhong<author:sep>Jingbo Zhang<author:sep>Jing Liao;http://arxiv.org/pdf/2310.11864v3;cs.CV;"Accepted by TVCG. Project Page:
  https://jtbzhl.github.io/VQ-NeRF.github.io/";nerf
2310.11645v1;http://arxiv.org/abs/2310.11645v1;2023-10-18;Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos  using NeRFs;"Given that a conventional laparoscope only provides a two-dimensional (2-D)
view, the detection and diagnosis of medical ailments can be challenging. To
overcome the visual constraints associated with laparoscopy, the use of
laparoscopic images and videos to reconstruct the three-dimensional (3-D)
anatomical structure of the abdomen has proven to be a promising approach.
Neural Radiance Fields (NeRFs) have recently gained attention thanks to their
ability to generate photorealistic images from a 3-D static scene, thus
facilitating a more comprehensive exploration of the abdomen through the
synthesis of new views. This distinguishes NeRFs from alternative methods such
as Simultaneous Localization and Mapping (SLAM) and depth estimation. In this
paper, we present a comprehensive examination of NeRFs in the context of
laparoscopy surgical videos, with the goal of rendering abdominal scenes in
3-D. Although our experimental results are promising, the proposed approach
encounters substantial challenges, which require further exploration in future
research.";Khoa Tuan Nguyen<author:sep>Francesca Tozzi<author:sep>Nikdokht Rashidian<author:sep>Wouter Willaert<author:sep>Joris Vankerschaver<author:sep>Wesley De Neve;http://arxiv.org/pdf/2310.11645v1;cs.CV;"The Version of Record of this contribution is published in MLMI 2023
  Part I, and is available online at
  https://doi.org/10.1007/978-3-031-45673-2_9";nerf
2310.10642v2;http://arxiv.org/abs/2310.10642v2;2023-10-16;Real-time Photorealistic Dynamic Scene Representation and Rendering with  4D Gaussian Splatting;"Reconstructing dynamic 3D scenes from 2D images and generating diverse views
over time is challenging due to scene complexity and temporal dynamics. Despite
advancements in neural implicit models, limitations persist: (i) Inadequate
Scene Structure: Existing methods struggle to reveal the spatial and temporal
structure of dynamic scenes from directly learning the complex 6D plenoptic
function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element
deformation becomes impractical for complex dynamics. To address these issues,
we consider the spacetime as an entirety and propose to approximate the
underlying spatio-temporal 4D volume of a dynamic scene by optimizing a
collection of 4D primitives, with explicit geometry and appearance modeling.
Learning to optimize the 4D primitives enables us to synthesize novel views at
any desired time with our tailored rendering routine. Our model is conceptually
simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that
can rotate arbitrarily in space and time, as well as view-dependent and
time-evolved appearance represented by the coefficient of 4D spherindrical
harmonics. This approach offers simplicity, flexibility for variable-length
video and end-to-end training, and efficient real-time rendering, making it
suitable for capturing complex dynamic scene motions. Experiments across
various benchmarks, including monocular and multi-view scenarios, demonstrate
our 4DGS model's superior visual quality and efficiency.";Zeyu Yang<author:sep>Hongye Yang<author:sep>Zijie Pan<author:sep>Xiatian Zhu<author:sep>Li Zhang;http://arxiv.org/pdf/2310.10642v2;cs.CV;ICLR 2024;gaussian splatting
2310.10624v2;http://arxiv.org/abs/2310.10624v2;2023-10-16;DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and  View-Change Human-Centric Video Editing;"Despite recent progress in diffusion-based video editing, existing methods
are limited to short-length videos due to the contradiction between long-range
consistency and frame-wise editing. Prior attempts to address this challenge by
introducing video-2D representations encounter significant difficulties with
large-scale motion- and view-change videos, especially in human-centric
scenarios. To overcome this, we propose to introduce the dynamic Neural
Radiance Fields (NeRF) as the innovative video representation, where the
editing can be performed in the 3D spaces and propagated to the entire video
via the deformation field. To provide consistent and controllable editing, we
propose the image-based video-NeRF editing pipeline with a set of innovative
designs, including multi-view multi-pose Score Distillation Sampling (SDS) from
both the 2D personalized diffusion prior and 3D diffusion prior, reconstruction
losses, text-guided local parts super-resolution, and style transfer. Extensive
experiments demonstrate that our method, dubbed as DynVideo-E, significantly
outperforms SOTA approaches on two challenging datasets by a large margin of
50% ~ 95% for human preference. Code will be released at
https://showlab.github.io/DynVideo-E/.";Jia-Wei Liu<author:sep>Yan-Pei Cao<author:sep>Jay Zhangjie Wu<author:sep>Weijia Mao<author:sep>Yuchao Gu<author:sep>Rui Zhao<author:sep>Jussi Keppo<author:sep>Ying Shan<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2310.10624v2;cs.CV;Project Page: https://showlab.github.io/DynVideo-E/;nerf
2310.10209v1;http://arxiv.org/abs/2310.10209v1;2023-10-16;Self-supervised Fetal MRI 3D Reconstruction Based on Radiation Diffusion  Generation Model;"Although the use of multiple stacks can handle slice-to-volume motion
correction and artifact removal problems, there are still several problems: 1)
The slice-to-volume method usually uses slices as input, which cannot solve the
problem of uniform intensity distribution and complementarity in regions of
different fetal MRI stacks; 2) The integrity of 3D space is not considered,
which adversely affects the discrimination and generation of globally
consistent information in fetal MRI; 3) Fetal MRI with severe motion artifacts
in the real-world cannot achieve high-quality super-resolution reconstruction.
To address these issues, we propose a novel fetal brain MRI high-quality volume
reconstruction method, called the Radiation Diffusion Generation Model (RDGM).
It is a self-supervised generation method, which incorporates the idea of
Neural Radiation Field (NeRF) based on the coordinate generation and diffusion
model based on super-resolution generation. To solve regional intensity
heterogeneity in different directions, we use a pre-trained transformer model
for slice registration, and then, a new regionally Consistent Implicit Neural
Representation (CINR) network sub-module is proposed. CINR can generate the
initial volume by combining a coordinate association map of two different
coordinate mapping spaces. To enhance volume global consistency and
discrimination, we introduce the Volume Diffusion Super-resolution Generation
(VDSG) mechanism. The global intensity discriminant generation from
volume-to-volume is carried out using the idea of diffusion generation, and
CINR becomes the deviation intensity generation network of the volume-to-volume
diffusion model. Finally, the experimental results on real-world fetal brain
MRI stacks demonstrate the state-of-the-art performance of our method.";Junpeng Tan<author:sep>Xin Zhang<author:sep>Yao Lv<author:sep>Xiangmin Xu<author:sep>Gang Li;http://arxiv.org/pdf/2310.10209v1;eess.IV;;nerf
2310.10650v1;http://arxiv.org/abs/2310.10650v1;2023-10-16;TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through  Neural Radiance Fields;"Implicit representations like Neural Radiance Fields (NeRF) showed impressive
results for photorealistic rendering of complex scenes with fine details.
However, ideal or near-perfectly specular reflecting objects such as mirrors,
which are often encountered in various indoor scenes, impose ambiguities and
inconsistencies in the representation of the reconstructed scene leading to
severe artifacts in the synthesized renderings. In this paper, we present a
novel reflection tracing method tailored for the involved volume rendering
within NeRF that takes these mirror-like objects into account while avoiding
the cost of straightforward but expensive extensions through standard path
tracing. By explicitly modeling the reflection behavior using physically
plausible materials and estimating the reflected radiance with Monte-Carlo
methods within the volume rendering formulation, we derive efficient strategies
for importance sampling and the transmittance computation along rays from only
few samples. We show that our novel method enables the training of consistent
representations of such challenging scenes and achieves superior results in
comparison to previous state-of-the-art approaches.";Leif Van Holland<author:sep>Ruben Bliersbach<author:sep>Jan U. MÃ¼ller<author:sep>Patrick Stotko<author:sep>Reinhard Klein;http://arxiv.org/pdf/2310.10650v1;cs.CV;;nerf
2310.09892v1;http://arxiv.org/abs/2310.09892v1;2023-10-15;Active Perception using Neural Radiance Fields;"We study active perception from first principles to argue that an autonomous
agent performing active perception should maximize the mutual information that
past observations posses about future ones. Doing so requires (a) a
representation of the scene that summarizes past observations and the ability
to update this representation to incorporate new observations (state estimation
and mapping), (b) the ability to synthesize new observations of the scene (a
generative model), and (c) the ability to select control trajectories that
maximize predictive information (planning). This motivates a neural radiance
field (NeRF)-like representation which captures photometric, geometric and
semantic properties of the scene grounded. This representation is well-suited
to synthesizing new observations from different viewpoints. And thereby, a
sampling-based planner can be used to calculate the predictive information from
synthetic observations along dynamically-feasible trajectories. We use active
perception for exploring cluttered indoor environments and employ a notion of
semantic uncertainty to check for the successful completion of an exploration
task. We demonstrate these ideas via simulation in realistic 3D indoor
environments.";Siming He<author:sep>Christopher D. Hsu<author:sep>Dexter Ong<author:sep>Yifei Simon Shao<author:sep>Pratik Chaudhari;http://arxiv.org/pdf/2310.09892v1;cs.RO;;nerf
2310.09965v1;http://arxiv.org/abs/2310.09965v1;2023-10-15;ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context;"Neural Radiance Fields (NeRFs) have recently emerged as a popular option for
photo-realistic object capture due to their ability to faithfully capture
high-fidelity volumetric content even from handheld video input. Although much
research has been devoted to efficient optimization leading to real-time
training and rendering, options for interactive editing NeRFs remain limited.
We present a very simple but effective neural network architecture that is fast
and efficient while maintaining a low memory footprint. This architecture can
be incrementally guided through user-friendly image-based edits. Our
representation allows straightforward object selection via semantic feature
distillation at the training stage. More importantly, we propose a local
3D-aware image context to facilitate view-consistent image editing that can
then be distilled into fine-tuned NeRFs, via geometric and appearance
adjustments. We evaluate our setup on a variety of examples to demonstrate
appearance and geometric edits and report 10-30x speedup over concurrent work
focusing on text-guided NeRF editing. Video results can be seen on our project
webpage at https://proteusnerf.github.io.";Binglun Wang<author:sep>Niladri Shekhar Dutt<author:sep>Niloy J. Mitra;http://arxiv.org/pdf/2310.09965v1;cs.CV;;nerf
2310.09776v1;http://arxiv.org/abs/2310.09776v1;2023-10-15;CBARF: Cascaded Bundle-Adjusting Neural Radiance Fields from Imperfect  Camera Poses;"Existing volumetric neural rendering techniques, such as Neural Radiance
Fields (NeRF), face limitations in synthesizing high-quality novel views when
the camera poses of input images are imperfect. To address this issue, we
propose a novel 3D reconstruction framework that enables simultaneous
optimization of camera poses, dubbed CBARF (Cascaded Bundle-Adjusting NeRF).In
a nutshell, our framework optimizes camera poses in a coarse-to-fine manner and
then reconstructs scenes based on the rectified poses. It is observed that the
initialization of camera poses has a significant impact on the performance of
bundle-adjustment (BA). Therefore, we cascade multiple BA modules at different
scales to progressively improve the camera poses. Meanwhile, we develop a
neighbor-replacement strategy to further optimize the results of BA in each
stage. In this step, we introduce a novel criterion to effectively identify
poorly estimated camera poses. Then we replace them with the poses of
neighboring cameras, thus further eliminating the impact of inaccurate camera
poses. Once camera poses have been optimized, we employ a density voxel grid to
generate high-quality 3D reconstructed scenes and images in novel views.
Experimental results demonstrate that our CBARF model achieves state-of-the-art
performance in both pose optimization and novel view synthesis, especially in
the existence of large camera pose noise.";Hongyu Fu<author:sep>Xin Yu<author:sep>Lincheng Li<author:sep>Li Zhang;http://arxiv.org/pdf/2310.09776v1;cs.CV;;nerf
2310.08529v2;http://arxiv.org/abs/2310.08529v2;2023-10-12;GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging  2D and 3D Diffusion Models;"In recent times, the generation of 3D assets from text prompts has shown
impressive results. Both 2D and 3D diffusion models can help generate decent 3D
objects based on prompts. 3D diffusion models have good 3D consistency, but
their quality and generalization are limited as trainable 3D data is expensive
and hard to obtain. 2D diffusion models enjoy strong abilities of
generalization and fine generation, but 3D consistency is hard to guarantee.
This paper attempts to bridge the power from the two types of diffusion models
via the recent explicit and efficient 3D Gaussian splatting representation. A
fast 3D object generation framework, named as GaussianDreamer, is proposed,
where the 3D diffusion model provides priors for initialization and the 2D
diffusion model enriches the geometry and appearance. Operations of noisy point
growing and color perturbation are introduced to enhance the initialized
Gaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D
avatar within 15 minutes on one GPU, much faster than previous methods, while
the generated instances can be directly rendered in real time. Demos and code
are available at https://taoranyi.com/gaussiandreamer/.";Taoran Yi<author:sep>Jiemin Fang<author:sep>Junjie Wang<author:sep>Guanjun Wu<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wenyu Liu<author:sep>Qi Tian<author:sep>Xinggang Wang;http://arxiv.org/pdf/2310.08529v2;cs.CV;Project page: https://taoranyi.com/gaussiandreamer/;gaussian splatting
2310.08528v2;http://arxiv.org/abs/2310.08528v2;2023-10-12;4D Gaussian Splatting for Real-Time Dynamic Scene Rendering;"Representing and rendering dynamic scenes has been an important but
challenging task. Especially, to accurately model complex motions, high
efficiency is usually hard to guarantee. To achieve real-time dynamic scene
rendering while also enjoying high training and storage efficiency, we propose
4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes
rather than applying 3D-GS for each individual frame. In 4D-GS, a novel
explicit representation containing both 3D Gaussians and 4D neural voxels is
proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is
proposed to efficiently build Gaussian features from 4D neural voxels and then
a lightweight MLP is applied to predict Gaussian deformations at novel
timestamps. Our 4D-GS method achieves real-time rendering under high
resolutions, 82 FPS at an 800$\times$800 resolution on an RTX 3090 GPU while
maintaining comparable or better quality than previous state-of-the-art
methods. More demos and code are available at
https://guanjunwu.github.io/4dgs/.";Guanjun Wu<author:sep>Taoran Yi<author:sep>Jiemin Fang<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wei Wei<author:sep>Wenyu Liu<author:sep>Qi Tian<author:sep>Xinggang Wang;http://arxiv.org/pdf/2310.08528v2;cs.CV;Project page: https://guanjunwu.github.io/4dgs/;gaussian splatting
2310.07179v1;http://arxiv.org/abs/2310.07179v1;2023-10-11;rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera;"Novel view synthesis of satellite images holds a wide range of practical
applications. While recent advances in the Neural Radiance Field have
predominantly targeted pin-hole cameras, and models for satellite cameras often
demand sufficient input views. This paper presents rpcPRF, a Multiplane Images
(MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC).
Unlike coordinate-based neural radiance fields in need of sufficient views of
one scene, our model is applicable to single or few inputs and performs well on
images from unseen scenes. To enable generalization across scenes, we propose
to use reprojection supervision to induce the predicted MPI to learn the
correct geometry between the 3D coordinates and the images. Moreover, we remove
the stringent requirement of dense depth supervision from deep
multiview-stereo-based methods by introducing rendering techniques of radiance
fields. rpcPRF combines the superiority of implicit representations and the
advantages of the RPC model, to capture the continuous altitude space while
learning the 3D structure. Given an RGB image and its corresponding RPC, the
end-to-end model learns to synthesize the novel view with a new RPC and
reconstruct the altitude of the scene. When multiple views are provided as
inputs, rpcPRF exerts extra supervision provided by the extra views. On the TLC
dataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF
outperforms state-of-the-art nerf-based methods by a significant margin in
terms of image fidelity, reconstruction accuracy, and efficiency, for both
single-view and multiview task.";Tongtong Zhang<author:sep>Yuanxiang Li;http://arxiv.org/pdf/2310.07179v1;cs.CV;;nerf
2310.07916v2;http://arxiv.org/abs/2310.07916v2;2023-10-11;Dynamic Appearance Particle Neural Radiance Field;"Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D
scenes. Dynamic NeRFs extend this model by capturing time-varying elements,
typically using deformation fields. The existing dynamic NeRFs employ a similar
Eulerian representation for both light radiance and deformation fields. This
leads to a close coupling of appearance and motion and lacks a physical
interpretation. In this work, we propose Dynamic Appearance Particle Neural
Radiance Field (DAP-NeRF), which introduces particle-based representation to
model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists
of superposition of a static field and a dynamic field. The dynamic field is
quantised as a collection of {\em appearance particles}, which carries the
visual information of a small dynamic element in the scene and is equipped with
a motion model. All components, including the static field, the visual features
and motion models of the particles, are learned from monocular videos without
any prior geometric knowledge of the scene. We develop an efficient
computational framework for the particle-based model. We also construct a new
dataset to evaluate motion modelling. Experimental results show that DAP-NeRF
is an effective technique to capture not only the appearance but also the
physically meaningful motions in a 3D dynamic scene.";Ancheng Lin<author:sep>Jun Li;http://arxiv.org/pdf/2310.07916v2;cs.CV;;nerf
2310.07449v2;http://arxiv.org/abs/2310.07449v2;2023-10-11;PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction;"Neural surface reconstruction is sensitive to the camera pose noise, even if
state-of-the-art pose estimators like COLMAP or ARKit are used. More
importantly, existing Pose-NeRF joint optimisation methods have struggled to
improve pose accuracy in challenging real-world scenarios. To overcome the
challenges, we introduce the pose residual field (\textbf{PoRF}), a novel
implicit representation that uses an MLP for regressing pose updates. This is
more robust than the conventional pose parameter optimisation due to parameter
sharing that leverages global information over the entire sequence.
Furthermore, we propose an epipolar geometry loss to enhance the supervision
that leverages the correspondences exported from COLMAP results without the
extra computational overhead. Our method yields promising results. On the DTU
dataset, we reduce the rotation error by 78\% for COLMAP poses, leading to the
decreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the
MobileBrick dataset that contains casually captured unbounded 360-degree
videos, our method refines ARKit poses and improves the reconstruction F1 score
from 69.18 to 75.67, outperforming that with the dataset provided ground-truth
pose (75.14). These achievements demonstrate the efficacy of our approach in
refining camera poses and improving the accuracy of neural surface
reconstruction in real-world scenarios.";Jia-Wang Bian<author:sep>Wenjing Bian<author:sep>Victor Adrian Prisacariu<author:sep>Philip Torr;http://arxiv.org/pdf/2310.07449v2;cs.CV;Under review;nerf
2310.06275v1;http://arxiv.org/abs/2310.06275v1;2023-10-10;High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying  Expression Conditioned Neural Radiance Field;"One crucial aspect of 3D head avatar reconstruction lies in the details of
facial expressions. Although recent NeRF-based photo-realistic 3D head avatar
methods achieve high-quality avatar rendering, they still encounter challenges
retaining intricate facial expression details because they overlook the
potential of specific expression variations at different spatial positions when
conditioning the radiance field. Motivated by this observation, we introduce a
novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained
by a simple MLP-based generation network, encompassing both spatial positional
features and global expression information. Benefiting from rich and diverse
information of the SVE at different positions, the proposed SVE-conditioned
neural radiance field can deal with intricate facial expressions and achieve
realistic rendering and geometry details of high-fidelity 3D head avatars.
Additionally, to further elevate the geometric and rendering quality, we
introduce a new coarse-to-fine training strategy, including a geometry
initialization strategy at the coarse stage and an adaptive importance sampling
strategy at the fine stage. Extensive experiments indicate that our method
outperforms other state-of-the-art (SOTA) methods in rendering and geometry
quality on mobile phone-collected and public datasets.";Minghan Qin<author:sep>Yifan Liu<author:sep>Yuelang Xu<author:sep>Xiaochen Zhao<author:sep>Yebin Liu<author:sep>Haoqian Wang;http://arxiv.org/pdf/2310.06275v1;cs.CV;9 pages, 5 figures;nerf
2310.06984v1;http://arxiv.org/abs/2310.06984v1;2023-10-10;Leveraging Neural Radiance Fields for Uncertainty-Aware Visual  Localization;"As a promising fashion for visual localization, scene coordinate regression
(SCR) has seen tremendous progress in the past decade. Most recent methods
usually adopt neural networks to learn the mapping from image pixels to 3D
scene coordinates, which requires a vast amount of annotated training data. We
propose to leverage Neural Radiance Fields (NeRF) to generate training samples
for SCR. Despite NeRF's efficiency in rendering, many of the rendered data are
polluted by artifacts or only contain minimal information gain, which can
hinder the regression accuracy or bring unnecessary computational costs with
redundant data. These challenges are addressed in three folds in this paper:
(1) A NeRF is designed to separately predict uncertainties for the rendered
color and depth images, which reveal data reliability at the pixel level. (2)
SCR is formulated as deep evidential learning with epistemic uncertainty, which
is used to evaluate information gain and scene coordinate quality. (3) Based on
the three arts of uncertainties, a novel view selection policy is formed that
significantly improves data efficiency. Experiments on public datasets
demonstrate that our method could select the samples that bring the most
information gain and promote the performance with the highest efficiency.";Le Chen<author:sep>Weirong Chen<author:sep>Rui Wang<author:sep>Marc Pollefeys;http://arxiv.org/pdf/2310.06984v1;cs.CV;8 pages, 5 figures;nerf
2310.05837v1;http://arxiv.org/abs/2310.05837v1;2023-10-09;A Real-time Method for Inserting Virtual Objects into Neural Radiance  Fields;"We present the first real-time method for inserting a rigid virtual object
into a neural radiance field, which produces realistic lighting and shadowing
effects, as well as allows interactive manipulation of the object. By
exploiting the rich information about lighting and geometry in a NeRF, our
method overcomes several challenges of object insertion in augmented reality.
For lighting estimation, we produce accurate, robust and 3D spatially-varying
incident lighting that combines the near-field lighting from NeRF and an
environment lighting to account for sources not covered by the NeRF. For
occlusion, we blend the rendered virtual object with the background scene using
an opacity map integrated from the NeRF. For shadows, with a precomputed field
of spherical signed distance field, we query the visibility term for any point
around the virtual object, and cast soft, detailed shadows onto 3D surfaces.
Compared with state-of-the-art techniques, our approach can insert virtual
object into scenes with superior fidelity, and has a great potential to be
further applied to augmented reality systems.";Keyang Ye<author:sep>Hongzhi Wu<author:sep>Xin Tong<author:sep>Kun Zhou;http://arxiv.org/pdf/2310.05837v1;cs.CV;;nerf
2310.05391v1;http://arxiv.org/abs/2310.05391v1;2023-10-09;Neural Impostor: Editing Neural Radiance Fields with Explicit Shape  Manipulation;"Neural Radiance Fields (NeRF) have significantly advanced the generation of
highly realistic and expressive 3D scenes. However, the task of editing NeRF,
particularly in terms of geometry modification, poses a significant challenge.
This issue has obstructed NeRF's wider adoption across various applications. To
tackle the problem of efficiently editing neural implicit fields, we introduce
Neural Impostor, a hybrid representation incorporating an explicit tetrahedral
mesh alongside a multigrid implicit field designated for each tetrahedron
within the explicit mesh. Our framework bridges the explicit shape manipulation
and the geometric editing of implicit fields by utilizing multigrid barycentric
coordinate encoding, thus offering a pragmatic solution to deform, composite,
and generate neural implicit fields while maintaining a complex volumetric
appearance. Furthermore, we propose a comprehensive pipeline for editing neural
implicit fields based on a set of explicit geometric editing operations. We
show the robustness and adaptability of our system through diverse examples and
experiments, including the editing of both synthetic objects and real captured
data. Finally, we demonstrate the authoring process of a hybrid
synthetic-captured object utilizing a variety of editing operations,
underlining the transformative potential of Neural Impostor in the field of 3D
content creation and manipulation.";Ruiyang Liu<author:sep>Jinxu Xiang<author:sep>Bowen Zhao<author:sep>Ran Zhang<author:sep>Jingyi Yu<author:sep>Changxi Zheng;http://arxiv.org/pdf/2310.05391v1;cs.GR;Accepted at Pacific Graphics 2023 and Computer Graphics Forum;nerf
2310.05134v1;http://arxiv.org/abs/2310.05134v1;2023-10-08;LocoNeRF: A NeRF-based Approach for Local Structure from Motion for  Precise Localization;"Visual localization is a critical task in mobile robotics, and researchers
are continuously developing new approaches to enhance its efficiency. In this
article, we propose a novel approach to improve the accuracy of visual
localization using Structure from Motion (SfM) techniques. We highlight the
limitations of global SfM, which suffers from high latency, and the challenges
of local SfM, which requires large image databases for accurate reconstruction.
To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as
opposed to image databases, to cut down on the space required for storage. We
suggest that sampling reference images around the prior query position can lead
to further improvements. We evaluate the accuracy of our proposed method
against ground truth obtained using LIDAR and Advanced Lidar Odometry and
Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM
with COLMAP in the conducted experiments. Our proposed method achieves an
accuracy of 0.068 meters compared to the ground truth, which is slightly lower
than the most advanced method COLMAP, which has an accuracy of 0.022 meters.
However, the size of the database required for COLMAP is 400 megabytes, whereas
the size of our NeRF model is only 160 megabytes. Finally, we perform an
ablation study to assess the impact of using reference images from the NeRF
reconstruction.";Artem Nenashev<author:sep>Mikhail Kurenkov<author:sep>Andrei Potapov<author:sep>Iana Zhura<author:sep>Maksim Katerishich<author:sep>Dzmitry Tsetserukou;http://arxiv.org/pdf/2310.05134v1;cs.CV;;nerf
2310.05133v1;http://arxiv.org/abs/2310.05133v1;2023-10-08;Geometry Aware Field-to-field Transformations for 3D Semantic  Segmentation;"We present a novel approach to perform 3D semantic segmentation solely from
2D supervision by leveraging Neural Radiance Fields (NeRFs). By extracting
features along a surface point cloud, we achieve a compact representation of
the scene which is sample-efficient and conducive to 3D reasoning. Learning
this feature space in an unsupervised manner via masked autoencoding enables
few-shot segmentation. Our method is agnostic to the scene parameterization,
working on scenes fit with any type of NeRF.";Dominik Hollidt<author:sep>Clinton Wang<author:sep>Polina Golland<author:sep>Marc Pollefeys;http://arxiv.org/pdf/2310.05133v1;cs.CV;8 pages;nerf
2310.04152v1;http://arxiv.org/abs/2310.04152v1;2023-10-06;Improving Neural Radiance Field using Near-Surface Sampling with Point  Cloud Generation;"Neural radiance field (NeRF) is an emerging view synthesis method that
samples points in a three-dimensional (3D) space and estimates their existence
and color probabilities. The disadvantage of NeRF is that it requires a long
training time since it samples many 3D points. In addition, if one samples
points from occluded regions or in the space where an object is unlikely to
exist, the rendering quality of NeRF can be degraded. These issues can be
solved by estimating the geometry of 3D scene. This paper proposes a
near-surface sampling framework to improve the rendering quality of NeRF. To
this end, the proposed method estimates the surface of a 3D object using depth
images of the training set and sampling is performed around there only. To
obtain depth information on a novel view, the paper proposes a 3D point cloud
generation method and a simple refining method for projected depth from a point
cloud. Experimental results show that the proposed near-surface sampling NeRF
framework can significantly improve the rendering quality, compared to the
original NeRF and a state-of-the-art depth-based NeRF method. In addition, one
can significantly accelerate the training time of a NeRF model with the
proposed near-surface sampling framework.";Hye Bin Yoo<author:sep>Hyun Min Han<author:sep>Sung Soo Hwang<author:sep>Il Yong Chun;http://arxiv.org/pdf/2310.04152v1;cs.CV;13 figures, 2 tables;nerf
2310.03578v1;http://arxiv.org/abs/2310.03578v1;2023-10-05;Targeted Adversarial Attacks on Generalizable Neural Radiance Fields;"Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for
3D scene representation and rendering. These data-driven models can learn to
synthesize high-quality images from sparse 2D observations, enabling realistic
and interactive scene reconstructions. However, the growing usage of NeRFs in
critical applications such as augmented reality, robotics, and virtual
environments could be threatened by adversarial attacks.
  In this paper we present how generalizable NeRFs can be attacked by both
low-intensity adversarial attacks and adversarial patches, where the later
could be robust enough to be used in real world applications. We also
demonstrate targeted attacks, where a specific, predefined output scene is
generated by these attack with success.";Andras Horvath<author:sep>Csaba M. Jozsa;http://arxiv.org/pdf/2310.03578v1;cs.LG;;nerf
2310.03563v1;http://arxiv.org/abs/2310.03563v1;2023-10-05;BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance  Fields;"We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which
defines the image pose estimation problem as a NeRF based iterative linear
optimization. NeRFs are novel neural space representation models that can
synthesize photorealistic novel views of real-world scenes or objects. Our
contributions are as follows: we extend the localization optimization objective
with a depth-based loss function, we introduce a multi-image based loss
function where a sequence of images with known relative poses are used without
increasing the computational complexity, we omit hierarchical sampling during
volumetric rendering, meaning only the coarse model is used for pose
estimation, and we how that by extending the sampling interval convergence can
be achieved even or higher initial pose estimate errors. With the proposed
modifications the convergence speed is significantly improved, and the basin of
convergence is substantially extended.";Ãgoston IstvÃ¡n Csehi<author:sep>Csaba MÃ¡tÃ© JÃ³zsa;http://arxiv.org/pdf/2310.03563v1;cs.CV;Accepted to Nerf4ADR workshop of ICCV23 conference;nerf
2310.03375v1;http://arxiv.org/abs/2310.03375v1;2023-10-05;Point-Based Radiance Fields for Controllable Human Motion Synthesis;"This paper proposes a novel controllable human motion synthesis method for
fine-level deformation based on static point-based radiance fields. Although
previous editable neural radiance field methods can generate impressive results
on novel-view synthesis and allow naive deformation, few algorithms can achieve
complex 3D human editing such as forward kinematics. Our method exploits the
explicit point cloud to train the static 3D scene and apply the deformation by
encoding the point cloud translation using a deformation MLP. To make sure the
rendering result is consistent with the canonical space training, we estimate
the local rotation using SVD and interpolate the per-point rotation to the
query view direction of the pre-trained radiance field. Extensive experiments
show that our approach can significantly outperform the state-of-the-art on
fine-level complex deformation which can be generalized to other 3D characters
besides humans.";Haitao Yu<author:sep>Deheng Zhang<author:sep>Peiyuan Xie<author:sep>Tianyi Zhang;http://arxiv.org/pdf/2310.03375v1;cs.CV;;
2310.03015v1;http://arxiv.org/abs/2310.03015v1;2023-10-04;Efficient-3DiM: Learning a Generalizable Single-image Novel-view  Synthesizer in One Day;"The task of novel view synthesis aims to generate unseen perspectives of an
object or scene from a limited set of input images. Nevertheless, synthesizing
novel views from a single image still remains a significant challenge in the
realm of computer vision. Previous approaches tackle this problem by adopting
mesh prediction, multi-plain image construction, or more advanced techniques
such as neural radiance fields. Recently, a pre-trained diffusion model that is
specifically designed for 2D image synthesis has demonstrated its capability in
producing photorealistic novel views, if sufficiently optimized on a 3D
finetuning task. Although the fidelity and generalizability are greatly
improved, training such a powerful diffusion model requires a vast volume of
training data and model parameters, resulting in a notoriously long time and
high computational costs. To tackle this issue, we propose Efficient-3DiM, a
simple but effective framework to learn a single-image novel-view synthesizer.
Motivated by our in-depth analysis of the inference process of diffusion
models, we propose several pragmatic strategies to reduce the training overhead
to a manageable scale, including a crafted timestep sampling strategy, a
superior 3D feature extractor, and an enhanced training scheme. When combined,
our framework is able to reduce the total training time from 10 days to less
than 1 day, significantly accelerating the training process under the same
computational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive
experiments are conducted to demonstrate the efficiency and generalizability of
our proposed method.";Yifan Jiang<author:sep>Hao Tang<author:sep>Jen-Hao Rick Chang<author:sep>Liangchen Song<author:sep>Zhangyang Wang<author:sep>Liangliang Cao;http://arxiv.org/pdf/2310.03015v1;cs.CV;;
2310.03125v1;http://arxiv.org/abs/2310.03125v1;2023-10-04;Shielding the Unseen: Privacy Protection through Poisoning NeRF with  Spatial Deformation;"In this paper, we introduce an innovative method of safeguarding user privacy
against the generative capabilities of Neural Radiance Fields (NeRF) models.
Our novel poisoning attack method induces changes to observed views that are
imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to
accurately reconstruct a 3D scene. To achieve this, we devise a bi-level
optimization algorithm incorporating a Projected Gradient Descent (PGD)-based
spatial deformation. We extensively test our approach on two common NeRF
benchmark datasets consisting of 29 real-world scenes with high-quality images.
Our results compellingly demonstrate that our privacy-preserving method
significantly impairs NeRF's performance across these benchmark datasets.
Additionally, we show that our method is adaptable and versatile, functioning
across various perturbation strengths and NeRF architectures. This work offers
valuable insights into NeRF's vulnerabilities and emphasizes the need to
account for such potential privacy risks when developing robust 3D scene
reconstruction algorithms. Our study contributes to the larger conversation
surrounding responsible AI and generative machine learning, aiming to protect
user privacy and respect creative ownership in the digital age.";Yihan Wu<author:sep>Brandon Y. Feng<author:sep>Heng Huang;http://arxiv.org/pdf/2310.03125v1;cs.CV;;nerf
2310.02977v1;http://arxiv.org/abs/2310.02977v1;2023-10-04;T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation;"Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.";Yuze He<author:sep>Yushi Bai<author:sep>Matthieu Lin<author:sep>Wang Zhao<author:sep>Yubin Hu<author:sep>Jenny Sheng<author:sep>Ran Yi<author:sep>Juanzi Li<author:sep>Yong-Jin Liu;http://arxiv.org/pdf/2310.02977v1;cs.CV;16 pages, 11 figures;nerf
2310.02712v1;http://arxiv.org/abs/2310.02712v1;2023-10-04;ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space  NeRF;"Recently, there has been a significant advancement in text-to-image diffusion
models, leading to groundbreaking performance in 2D image generation. These
advancements have been extended to 3D models, enabling the generation of novel
3D objects from textual descriptions. This has evolved into NeRF editing
methods, which allow the manipulation of existing 3D objects through textual
conditioning. However, existing NeRF editing techniques have faced limitations
in their performance due to slow training speeds and the use of loss functions
that do not adequately consider editing. To address this, here we present a
novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding
real-world scenes into the latent space of the latent diffusion model (LDM)
through a unique refinement layer. This approach enables us to obtain a NeRF
backbone that is not only faster but also more amenable to editing compared to
traditional image space NeRF editing. Furthermore, we propose an improved loss
function tailored for editing by migrating the delta denoising score (DDS)
distillation loss, originally used in 2D image editing to the three-dimensional
domain. This novel loss function surpasses the well-known score distillation
sampling (SDS) loss in terms of suitability for editing purposes. Our
experimental results demonstrate that ED-NeRF achieves faster editing speed
while producing improved output quality compared to state-of-the-art 3D editing
models.";Jangho Park<author:sep>Gihyun Kwon<author:sep>Jong Chul Ye;http://arxiv.org/pdf/2310.02712v1;cs.CV;;nerf
2310.02687v2;http://arxiv.org/abs/2310.02687v2;2023-10-04;USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields;"Neural Radiance Fields (NeRF) has received much attention recently due to its
impressive capability to represent 3D scene and synthesize novel view images.
Existing works usually assume that the input images are captured by a global
shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied
to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter
effect would also affect the accuracy of the camera pose estimation (e.g. via
COLMAP), which further prevents the success of NeRF algorithm with RS images.
In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance
Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and
recover accurate camera motion trajectory simultaneously under the framework of
NeRF, by modeling the physical image formation process of a RS camera.
Experimental results demonstrate that USB-NeRF achieves better performance
compared to prior works, in terms of RS effect removal, novel view image
synthesis as well as camera motion estimation. Furthermore, our algorithm can
also be used to recover high-fidelity high frame-rate global shutter video from
a sequence of RS images.";Moyang Li<author:sep>Peng Wang<author:sep>Lingzhe Zhao<author:sep>Bangyan Liao<author:sep>Peidong Liu;http://arxiv.org/pdf/2310.02687v2;cs.CV;;nerf
2310.01821v1;http://arxiv.org/abs/2310.01821v1;2023-10-03;MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural  Radiance Fields;"Neural radiance fields (NeRFs) have shown impressive results for novel view
synthesis. However, they depend on the repetitive use of a single-input
single-output multilayer perceptron (SISO MLP) that maps 3D coordinates and
view direction to the color and volume density in a sample-wise manner, which
slows the rendering. We propose a multi-input multi-output NeRF (MIMO-NeRF)
that reduces the number of MLPs running by replacing the SISO MLP with a MIMO
MLP and conducting mappings in a group-wise manner. One notable challenge with
this approach is that the color and volume density of each point can differ
according to a choice of input coordinates in a group, which can lead to some
notable ambiguity. We also propose a self-supervised learning method that
regularizes the MIMO MLP with multiple fast reformulated MLPs to alleviate this
ambiguity without using pretrained models. The results of a comprehensive
experimental evaluation including comparative and ablation studies are
presented to show that MIMO-NeRF obtains a good trade-off between speed and
quality with a reasonable training time. We then demonstrate that MIMO-NeRF is
compatible with and complementary to previous advancements in NeRFs by applying
it to two representative fast NeRFs, i.e., a NeRF with sample reduction
(DONeRF) and a NeRF with alternative representations (TensoRF).";Takuhiro Kaneko;http://arxiv.org/pdf/2310.01821v1;cs.CV;"Accepted to ICCV 2023. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/mimo-nerf/";nerf
2310.01881v1;http://arxiv.org/abs/2310.01881v1;2023-10-03;Adaptive Multi-NeRF: Exploit Efficient Parallelism in Adaptive Multiple  Scale Neural Radiance Field Rendering;"Recent advances in Neural Radiance Fields (NeRF) have demonstrated
significant potential for representing 3D scene appearances as implicit neural
networks, enabling the synthesis of high-fidelity novel views. However, the
lengthy training and rendering process hinders the widespread adoption of this
promising technique for real-time rendering applications. To address this
issue, we present an effective adaptive multi-NeRF method designed to
accelerate the neural rendering process for large scenes with unbalanced
workloads due to varying scene complexities.
  Our method adaptively subdivides scenes into axis-aligned bounding boxes
using a tree hierarchy approach, assigning smaller NeRFs to different-sized
subspaces based on the complexity of each scene portion. This ensures the
underlying neural representation is specific to a particular part of the scene.
We optimize scene subdivision by employing a guidance density grid, which
balances representation capability for each Multilayer Perceptron (MLP).
Consequently, samples generated by each ray can be sorted and collected for
parallel inference, achieving a balanced workload suitable for small MLPs with
consistent dimensions for regular and GPU-friendly computations. We aosl
demonstrated an efficient NeRF sampling strategy that intrinsically adapts to
increase parallelism, utilization, and reduce kernel calls, thereby achieving
much higher GPU utilization and accelerating the rendering process.";Tong Wang<author:sep>Shuichi Kurabayashi;http://arxiv.org/pdf/2310.01881v1;cs.CV;;nerf
2310.02437v2;http://arxiv.org/abs/2310.02437v2;2023-10-03;EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields;"We present EvDNeRF, a pipeline for generating event data and training an
event-based dynamic NeRF, for the purpose of faithfully reconstructing
eventstreams on scenes with rigid and non-rigid deformations that may be too
fast to capture with a standard camera. Event cameras register asynchronous
per-pixel brightness changes at MHz rates with high dynamic range, making them
ideal for observing fast motion with almost no motion blur. Neural radiance
fields (NeRFs) offer visual-quality geometric-based learnable rendering, but
prior work with events has only considered reconstruction of static scenes. Our
EvDNeRF can predict eventstreams of dynamic scenes from a static or moving
viewpoint between any desired timestamps, thereby allowing it to be used as an
event-based simulator for a given scene. We show that by training on varied
batch sizes of events, we can improve test-time predictions of events at fine
time resolutions, outperforming baselines that pair standard dynamic NeRFs with
event generators. We release our simulated and real datasets, as well as code
for multi-view event-based data generation and the training and evaluation of
EvDNeRF models (https://github.com/anish-bhattacharya/EvDNeRF).";Anish Bhattacharya<author:sep>Ratnesh Madaan<author:sep>Fernando Cladera<author:sep>Sai Vemprala<author:sep>Rogerio Bonatti<author:sep>Kostas Daniilidis<author:sep>Ashish Kapoor<author:sep>Vijay Kumar<author:sep>Nikolai Matni<author:sep>Jayesh K. Gupta;http://arxiv.org/pdf/2310.02437v2;cs.CV;16 pages, 20 figures, 2 tables;nerf
2310.00874v1;http://arxiv.org/abs/2310.00874v1;2023-10-02;PC-NeRF: Parent-Child Neural Radiance Fields under Partial Sensor Data  Loss in Autonomous Driving Environments;"Reconstructing large-scale 3D scenes is essential for autonomous vehicles,
especially when partial sensor data is lost. Although the recently developed
neural radiance fields (NeRF) have shown compelling results in implicit
representations, the large-scale 3D scene reconstruction using partially lost
LiDAR point cloud data still needs to be explored. To bridge this gap, we
propose a novel 3D scene reconstruction framework called parent-child neural
radiance field (PC-NeRF). The framework comprises two modules, the parent NeRF
and the child NeRF, to simultaneously optimize scene-level, segment-level, and
point-level scene representations. Sensor data can be utilized more efficiently
by leveraging the segment-level representation capabilities of child NeRFs, and
an approximate volumetric representation of the scene can be quickly obtained
even with limited observations. With extensive experiments, our proposed
PC-NeRF is proven to achieve high-precision 3D reconstruction in large-scale
scenes. Moreover, PC-NeRF can effectively tackle situations where partial
sensor data is lost and has high deployment efficiency with limited training
time. Our approach implementation and the pre-trained models will be available
at https://github.com/biter0088/pc-nerf.";Xiuzhong Hu<author:sep>Guangming Xiong<author:sep>Zheng Zang<author:sep>Peng Jia<author:sep>Yuxuan Han<author:sep>Junyi Ma;http://arxiv.org/pdf/2310.00874v1;cs.CV;;nerf
2310.00684v1;http://arxiv.org/abs/2310.00684v1;2023-10-01;How Many Views Are Needed to Reconstruct an Unknown Object Using NeRF?;"Neural Radiance Fields (NeRFs) are gaining significant interest for online
active object reconstruction due to their exceptional memory efficiency and
requirement for only posed RGB inputs. Previous NeRF-based view planning
methods exhibit computational inefficiency since they rely on an iterative
paradigm, consisting of (1) retraining the NeRF when new images arrive; and (2)
planning a path to the next best view only. To address these limitations, we
propose a non-iterative pipeline based on the Prediction of the Required number
of Views (PRV). The key idea behind our approach is that the required number of
views to reconstruct an object depends on its complexity. Therefore, we design
a deep neural network, named PRVNet, to predict the required number of views,
allowing us to tailor the data acquisition based on the object complexity and
plan a globally shortest path. To train our PRVNet, we generate supervision
labels using the ShapeNet dataset. Simulated experiments show that our
PRV-based view planning method outperforms baselines, achieving good
reconstruction quality while significantly reducing movement cost and planning
time. We further justify the generalization ability of our approach in a
real-world experiment.";Sicong Pan<author:sep>Liren Jin<author:sep>Hao Hu<author:sep>Marija PopoviÄ<author:sep>Maren Bennewitz;http://arxiv.org/pdf/2310.00684v1;cs.RO;Submitted to ICRA 2024;nerf
2310.00530v3;http://arxiv.org/abs/2310.00530v3;2023-10-01;Multi-tiling Neural Radiance Field (NeRF) -- Geometric Assessment on  Large-scale Aerial Datasets;"Neural Radiance Fields (NeRF) offer the potential to benefit 3D
reconstruction tasks, including aerial photogrammetry. However, the scalability
and accuracy of the inferred geometry are not well-documented for large-scale
aerial assets,since such datasets usually result in very high memory
consumption and slow convergence.. In this paper, we aim to scale the NeRF on
large-scael aerial datasets and provide a thorough geometry assessment of NeRF.
Specifically, we introduce a location-specific sampling technique as well as a
multi-camera tiling (MCT) strategy to reduce memory consumption during image
loading for RAM, representation training for GPU memory, and increase the
convergence rate within tiles. MCT decomposes a large-frame image into multiple
tiled images with different camera models, allowing these small-frame images to
be fed into the training process as needed for specific locations without a
loss of accuracy. We implement our method on a representative approach,
Mip-NeRF, and compare its geometry performance with threephotgrammetric MVS
pipelines on two typical aerial datasets against LiDAR reference data. Both
qualitative and quantitative results suggest that the proposed NeRF approach
produces better completeness and object details than traditional approaches,
although as of now, it still falls short in terms of accuracy.";Ningli Xu<author:sep>Rongjun Qin<author:sep>Debao Huang<author:sep>Fabio Remondino;http://arxiv.org/pdf/2310.00530v3;cs.CV;9 Figure;nerf
2310.00249v1;http://arxiv.org/abs/2310.00249v1;2023-09-30;MMPI: a Flexible Radiance Field Representation by Multiple Multi-plane  Images Blending;"This paper presents a flexible representation of neural radiance fields based
on multi-plane images (MPI), for high-quality view synthesis of complex scenes.
MPI with Normalized Device Coordinate (NDC) parameterization is widely used in
NeRF learning for its simple definition, easy calculation, and powerful ability
to represent unbounded scenes. However, existing NeRF works that adopt MPI
representation for novel view synthesis can only handle simple forward-facing
unbounded scenes, where the input cameras are all observing in similar
directions with small relative translations. Hence, extending these MPI-based
methods to more complex scenes like large-range or even 360-degree scenes is
very challenging. In this paper, we explore the potential of MPI and show that
MPI can synthesize high-quality novel views of complex scenes with diverse
camera distributions and view directions, which are not only limited to simple
forward-facing scenes. Our key idea is to encode the neural radiance field with
multiple MPIs facing different directions and blend them with an adaptive
blending operation. For each region of the scene, the blending operation gives
larger blending weights to those advantaged MPIs with stronger local
representation abilities while giving lower weights to those with weaker
representation abilities. Such blending operation automatically modulates the
multiple MPIs to appropriately represent the diverse local density and color
information. Experiments on the KITTI dataset and ScanNet dataset demonstrate
that our proposed MMPI synthesizes high-quality images from diverse camera pose
distributions and is fast to train, outperforming the previous fast-training
NeRF methods for novel view synthesis. Moreover, we show that MMPI can encode
extremely long trajectories and produce novel view renderings, demonstrating
its potential in applications like autonomous driving.";Yuze He<author:sep>Peng Wang<author:sep>Yubin Hu<author:sep>Wang Zhao<author:sep>Ran Yi<author:sep>Yong-Jin Liu<author:sep>Wenping Wang;http://arxiv.org/pdf/2310.00249v1;cs.CV;;nerf
2309.17390v1;http://arxiv.org/abs/2309.17390v1;2023-09-29;Forward Flow for Novel View Synthesis of Dynamic Scenes;"This paper proposes a neural radiance field (NeRF) approach for novel view
synthesis of dynamic scenes using forward warping. Existing methods often adopt
a static NeRF to represent the canonical space, and render dynamic images at
other time steps by mapping the sampled 3D points back to the canonical space
with the learned backward flow field. However, this backward flow field is
non-smooth and discontinuous, which is difficult to be fitted by commonly used
smooth motion models. To address this problem, we propose to estimate the
forward flow field and directly warp the canonical radiance field to other time
steps. Such forward flow field is smooth and continuous within the object
region, which benefits the motion model learning. To achieve this goal, we
represent the canonical radiance field with voxel grids to enable efficient
forward warping, and propose a differentiable warping process, including an
average splatting operation and an inpaint network, to resolve the many-to-one
and one-to-many mapping issues. Thorough experiments show that our method
outperforms existing methods in both novel view rendering and motion modeling,
demonstrating the effectiveness of our forward flow motion modeling. Project
page: https://npucvr.github.io/ForwardFlowDNeRF";Xiang Guo<author:sep>Jiadai Sun<author:sep>Yuchao Dai<author:sep>Guanying Chen<author:sep>Xiaoqing Ye<author:sep>Xiao Tan<author:sep>Errui Ding<author:sep>Yumeng Zhang<author:sep>Jingdong Wang;http://arxiv.org/pdf/2309.17390v1;cs.CV;"Accepted by ICCV2023 as oral. Project page:
  https://npucvr.github.io/ForwardFlowDNeRF";nerf
2309.17128v1;http://arxiv.org/abs/2309.17128v1;2023-09-29;HAvatar: High-fidelity Head Avatar via Facial Model Conditioned Neural  Radiance Field;"The problem of modeling an animatable 3D human head avatar under light-weight
setups is of significant importance but has not been well solved. Existing 3D
representations either perform well in the realism of portrait images synthesis
or the accuracy of expression control, but not both. To address the problem, we
introduce a novel hybrid explicit-implicit 3D representation, Facial Model
Conditioned Neural Radiance Field, which integrates the expressiveness of NeRF
and the prior information from the parametric template. At the core of our
representation, a synthetic-renderings-based condition method is proposed to
fuse the prior information from the parametric model into the implicit field
without constraining its topological flexibility. Besides, based on the hybrid
representation, we properly overcome the inconsistent shape issue presented in
existing methods and improve the animation stability. Moreover, by adopting an
overall GAN-based architecture using an image-to-image translation network, we
achieve high-resolution, realistic and view-consistent synthesis of dynamic
head appearance. Experiments demonstrate that our method can achieve
state-of-the-art performance for 3D head avatar animation compared with
previous methods.";Xiaochen Zhao<author:sep>Lizhen Wang<author:sep>Jingxiang Sun<author:sep>Hongwen Zhang<author:sep>Jinli Suo<author:sep>Yebin Liu;http://arxiv.org/pdf/2309.17128v1;cs.CV;;nerf
2309.17450v1;http://arxiv.org/abs/2309.17450v1;2023-09-29;Multi-task View Synthesis with Neural Radiance Fields;"Multi-task visual learning is a critical aspect of computer vision. Current
research, however, predominantly concentrates on the multi-task dense
prediction setting, which overlooks the intrinsic 3D world and its multi-view
consistent structures, and lacks the capability for versatile imagination. In
response to these limitations, we present a novel problem setting -- multi-task
view synthesis (MTVS), which reinterprets multi-task prediction as a set of
novel-view synthesis tasks for multiple scene properties, including RGB. To
tackle the MTVS problem, we propose MuvieNeRF, a framework that incorporates
both multi-task and cross-view knowledge to simultaneously synthesize multiple
scene properties. MuvieNeRF integrates two key modules, the Cross-Task
Attention (CTA) and Cross-View Attention (CVA) modules, enabling the efficient
use of information across multiple views and tasks. Extensive evaluation on
both synthetic and realistic benchmarks demonstrates that MuvieNeRF is capable
of simultaneously synthesizing different scene properties with promising visual
quality, even outperforming conventional discriminative models in various
settings. Notably, we show that MuvieNeRF exhibits universal applicability
across a range of NeRF backbones. Our code is available at
https://github.com/zsh2000/MuvieNeRF.";Shuhong Zheng<author:sep>Zhipeng Bao<author:sep>Martial Hebert<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2309.17450v1;cs.CV;ICCV 2023, Website: https://zsh2000.github.io/mtvs.github.io/;nerf
2310.13700v1;http://arxiv.org/abs/2310.13700v1;2023-09-28;Augmenting Heritage: An Open-Source Multiplatform AR Application;"AI NeRF algorithms, capable of cloud processing, have significantly reduced
hardware requirements and processing efficiency in photogrammetry pipelines.
This accessibility has unlocked the potential for museums, charities, and
cultural heritage sites worldwide to leverage mobile devices for artifact
scanning and processing. However, the adoption of augmented reality platforms
often necessitates the installation of proprietary applications on users'
mobile devices, which adds complexity to development and limits global
availability. This paper presents a case study that demonstrates a
cost-effective pipeline for visualizing scanned museum artifacts using mobile
augmented reality, leveraging an open-source embedded solution on a website.";Corrie Green;http://arxiv.org/pdf/2310.13700v1;cs.HC;;nerf
2309.16110v1;http://arxiv.org/abs/2309.16110v1;2023-09-28;Learning Effective NeRFs and SDFs Representations with 3D Generative  Adversarial Networks for 3D Object Generation: Technical Report for ICCV 2023  OmniObject3D Challenge;"In this technical report, we present a solution for 3D object generation of
ICCV 2023 OmniObject3D Challenge. In recent years, 3D object generation has
made great process and achieved promising results, but it remains a challenging
task due to the difficulty of generating complex, textured and high-fidelity
results. To resolve this problem, we study learning effective NeRFs and SDFs
representations with 3D Generative Adversarial Networks (GANs) for 3D object
generation. Specifically, inspired by recent works, we use the efficient
geometry-aware 3D GANs as the backbone incorporating with label embedding and
color mapping, which enables to train the model on different taxonomies
simultaneously. Then, through a decoder, we aggregate the resulting features to
generate Neural Radiance Fields (NeRFs) based representations for rendering
high-fidelity synthetic images. Meanwhile, we optimize Signed Distance
Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we
observe that this model can be effectively trained with only a few images of
each object from a variety of classes, instead of using a great number of
images per object or training one model per class. With this pipeline, we can
optimize an effective model for 3D object generation. This solution is one of
the final top-3-place solutions in the ICCV 2023 OmniObject3D Challenge.";Zheyuan Yang<author:sep>Yibo Liu<author:sep>Guile Wu<author:sep>Tongtong Cao<author:sep>Yuan Ren<author:sep>Yang Liu<author:sep>Bingbing Liu;http://arxiv.org/pdf/2309.16110v1;cs.CV;;nerf
2309.16653v1;http://arxiv.org/abs/2309.16653v1;2023-09-28;DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content  Creation;"Recent advances in 3D content creation mostly leverage optimization-based 3D
generation via score distillation sampling (SDS). Though promising results have
been exhibited, these methods often suffer from slow per-sample optimization,
limiting their practical usage. In this paper, we propose DreamGaussian, a
novel 3D content generation framework that achieves both efficiency and quality
simultaneously. Our key insight is to design a generative 3D Gaussian Splatting
model with companioned mesh extraction and texture refinement in UV space. In
contrast to the occupancy pruning used in Neural Radiance Fields, we
demonstrate that the progressive densification of 3D Gaussians converges
significantly faster for 3D generative tasks. To further enhance the texture
quality and facilitate downstream applications, we introduce an efficient
algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning
stage to refine the details. Extensive experiments demonstrate the superior
efficiency and competitive generation quality of our proposed approach.
Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes
from a single-view image, achieving approximately 10 times acceleration
compared to existing methods.";Jiaxiang Tang<author:sep>Jiawei Ren<author:sep>Hang Zhou<author:sep>Ziwei Liu<author:sep>Gang Zeng;http://arxiv.org/pdf/2309.16653v1;cs.CV;project page: https://dreamgaussian.github.io/;gaussian splatting
2309.16585v3;http://arxiv.org/abs/2309.16585v3;2023-09-28;Text-to-3D using Gaussian Splatting;"In this paper, we present Gaussian Splatting based text-to-3D generation
(GSGEN), a novel approach for generating high-quality 3D objects. Previous
methods suffer from inaccurate geometry and limited fidelity due to the absence
of 3D prior and proper representation. We leverage 3D Gaussian Splatting, a
recent state-of-the-art representation, to address existing shortcomings by
exploiting the explicit nature that enables the incorporation of 3D prior.
Specifically, our method adopts a progressive optimization strategy, which
includes a geometry optimization stage and an appearance refinement stage. In
geometry optimization, a coarse representation is established under a 3D
geometry prior along with the ordinary 2D SDS loss, ensuring a sensible and
3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an
iterative refinement to enrich details. In this stage, we increase the number
of Gaussians by compactness-based densification to enhance continuity and
improve fidelity. With these designs, our approach can generate 3D content with
delicate details and more accurate geometry. Extensive evaluations demonstrate
the effectiveness of our method, especially for capturing high-frequency
components. Video results are provided at https://gsgen3d.github.io. Our code
is available at https://github.com/gsgen3d/gsgen";Zilong Chen<author:sep>Feng Wang<author:sep>Huaping Liu;http://arxiv.org/pdf/2309.16585v3;cs.CV;"Project page: https://gsgen3d.github.io. Code:
  https://github.com/gsgen3d/gsgen";gaussian splatting
2309.16859v1;http://arxiv.org/abs/2309.16859v1;2023-09-28;Preface: A Data-driven Volumetric Prior for Few-shot Ultra  High-resolution Face Synthesis;"NeRFs have enabled highly realistic synthesis of human faces including
complex appearance and reflectance effects of hair and skin. These methods
typically require a large number of multi-view input images, making the process
hardware intensive and cumbersome, limiting applicability to unconstrained
settings. We propose a novel volumetric human face prior that enables the
synthesis of ultra high-resolution novel views of subjects that are not part of
the prior's training distribution. This prior model consists of an
identity-conditioned NeRF, trained on a dataset of low-resolution multi-view
images of diverse humans with known camera calibration. A simple sparse
landmark-based 3D alignment of the training dataset allows our model to learn a
smooth latent space of geometry and appearance despite a limited number of
training identities. A high-quality volumetric representation of a novel
subject can be obtained by model fitting to 2 or 3 camera views of arbitrary
resolution. Importantly, our method requires as few as two views of casually
captured images as input at inference time.";Marcel C. BÃ¼hler<author:sep>Kripasindhu Sarkar<author:sep>Tanmay Shah<author:sep>Gengyan Li<author:sep>Daoye Wang<author:sep>Leonhard Helminger<author:sep>Sergio Orts-Escolano<author:sep>Dmitry Lagun<author:sep>Otmar Hilliges<author:sep>Thabo Beeler<author:sep>Abhimitra Meka;http://arxiv.org/pdf/2309.16859v1;cs.CV;"In Proceedings of the IEEE/CVF International Conference on Computer
  Vision, 2023";nerf
2309.16364v2;http://arxiv.org/abs/2309.16364v2;2023-09-28;FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for  Independence-Assumption-Free Uncertainty Estimation;"Neural radiance fields with stochasticity have garnered significant interest
by enabling the sampling of plausible radiance fields and quantifying
uncertainty for downstream tasks. Existing works rely on the independence
assumption of points in the radiance field or the pixels in input views to
obtain tractable forms of the probability density function. However, this
assumption inadvertently impacts performance when dealing with intricate
geometry and texture. In this work, we propose an independence-assumption-free
probabilistic neural radiance field based on Flow-GAN. By combining the
generative capability of adversarial learning and the powerful expressivity of
normalizing flow, our method explicitly models the density-radiance
distribution of the whole scene. We represent our probabilistic NeRF as a
mean-shifted probabilistic residual neural model. Our model is trained without
an explicit likelihood function, thereby avoiding the independence assumption.
Specifically, We downsample the training images with different strides and
centers to form fixed-size patches which are used to train the generator with
patch-based adversarial learning. Through extensive experiments, our method
demonstrates state-of-the-art performance by predicting lower rendering errors
and more reliable uncertainty on both synthetic and real-world datasets.";Songlin Wei<author:sep>Jiazhao Zhang<author:sep>Yang Wang<author:sep>Fanbo Xiang<author:sep>Hao Su<author:sep>He Wang;http://arxiv.org/pdf/2309.16364v2;cs.CV;;nerf
2309.16553v1;http://arxiv.org/abs/2309.16553v1;2023-09-28;MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering  and Beyond;"Neural radiance fields (NeRF) and its subsequent variants have led to
remarkable progress in neural rendering. While most of recent neural rendering
works focus on objects and small-scale scenes, developing neural rendering
methods for city-scale scenes is of great potential in many real-world
applications. However, this line of research is impeded by the absence of a
comprehensive and high-quality dataset, yet collecting such a dataset over real
city-scale scenes is costly, sensitive, and technically difficult. To this end,
we build a large-scale, comprehensive, and high-quality synthetic dataset for
city-scale neural rendering researches. Leveraging the Unreal Engine 5 City
Sample project, we develop a pipeline to easily collect aerial and street city
views, accompanied by ground-truth camera poses and a range of additional data
modalities. Flexible controls over environmental factors like light, weather,
human and car crowd are also available in our pipeline, supporting the need of
various tasks covering city-scale neural rendering and beyond. The resulting
pilot dataset, MatrixCity, contains 67k aerial images and 452k street images
from two city maps of total size $28km^2$. On top of MatrixCity, a thorough
benchmark is also conducted, which not only reveals unique challenges of the
task of city-scale neural rendering, but also highlights potential improvements
for future works. The dataset and code will be publicly available at our
project page: https://city-super.github.io/matrixcity/.";Yixuan Li<author:sep>Lihan Jiang<author:sep>Linning Xu<author:sep>Yuanbo Xiangli<author:sep>Zhenzhi Wang<author:sep>Dahua Lin<author:sep>Bo Dai;http://arxiv.org/pdf/2309.16553v1;cs.CV;"Accepted to ICCV 2023. Project page:
  $\href{https://city-super.github.io/matrixcity/}{this\, https\, URL}$";nerf
2309.15526v1;http://arxiv.org/abs/2309.15526v1;2023-09-27;P2I-NET: Mapping Camera Pose to Image via Adversarial Learning for New  View Synthesis in Real Indoor Environments;"Given a new $6DoF$ camera pose in an indoor environment, we study the
challenging problem of predicting the view from that pose based on a set of
reference RGBD views. Existing explicit or implicit 3D geometry construction
methods are computationally expensive while those based on learning have
predominantly focused on isolated views of object categories with regular
geometric structure. Differing from the traditional \textit{render-inpaint}
approach to new view synthesis in the real indoor environment, we propose a
conditional generative adversarial neural network (P2I-NET) to directly predict
the new view from the given pose. P2I-NET learns the conditional distribution
of the images of the environment for establishing the correspondence between
the camera pose and its view of the environment, and achieves this through a
number of innovative designs in its architecture and training lost function.
Two auxiliary discriminator constraints are introduced for enforcing the
consistency between the pose of the generated image and that of the
corresponding real world image in both the latent feature space and the real
world pose space. Additionally a deep convolutional neural network (CNN) is
introduced to further reinforce this consistency in the pixel space. We have
performed extensive new view synthesis experiments on real indoor datasets.
Results show that P2I-NET has superior performance against a number of NeRF
based strong baseline models. In particular, we show that P2I-NET is 40 to 100
times faster than these competitor techniques while synthesising similar
quality images. Furthermore, we contribute a new publicly available indoor
environment dataset containing 22 high resolution RGBD videos where each frame
also has accurate camera pose parameters.";Xujie Kang<author:sep>Kanglin Liu<author:sep>Jiang Duan<author:sep>Yuanhao Gong<author:sep>Guoping Qiu;http://arxiv.org/pdf/2309.15526v1;cs.CV;;nerf
2309.15426v1;http://arxiv.org/abs/2309.15426v1;2023-09-27;NeuRBF: A Neural Fields Representation with Adaptive Radial Basis  Functions;"We present a novel type of neural fields that uses general radial bases for
signal representation. State-of-the-art neural fields typically rely on
grid-based representations for storing local neural features and N-dimensional
linear kernels for interpolating features at continuous query points. The
spatial positions of their neural features are fixed on grid nodes and cannot
well adapt to target signals. Our method instead builds upon general radial
bases with flexible kernel position and shape, which have higher spatial
adaptivity and can more closely fit target signals. To further improve the
channel-wise capacity of radial basis functions, we propose to compose them
with multi-frequency sinusoid functions. This technique extends a radial basis
to multiple Fourier radial bases of different frequency bands without requiring
extra parameters, facilitating the representation of details. Moreover, by
marrying adaptive radial bases with grid-based ones, our hybrid combination
inherits both adaptivity and interpolation smoothness. We carefully designed
weighting schemes to let radial bases adapt to different types of signals
effectively. Our experiments on 2D image and 3D signed distance field
representation demonstrate the higher accuracy and compactness of our method
than prior arts. When applied to neural radiance field reconstruction, our
method achieves state-of-the-art rendering quality, with small model size and
comparable training speed.";Zhang Chen<author:sep>Zhong Li<author:sep>Liangchen Song<author:sep>Lele Chen<author:sep>Jingyi Yu<author:sep>Junsong Yuan<author:sep>Yi Xu;http://arxiv.org/pdf/2309.15426v1;cs.CV;"Accepted to ICCV 2023 Oral. Project page:
  https://oppo-us-research.github.io/NeuRBF-website/";
2309.15329v1;http://arxiv.org/abs/2309.15329v1;2023-09-27;BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction  using Neural Radiance Fields;"Reconstruction of deformable scenes from endoscopic videos is important for
many applications such as intraoperative navigation, surgical visual
perception, and robotic surgery. It is a foundational requirement for realizing
autonomous robotic interventions for minimally invasive surgery. However,
previous approaches in this domain have been limited by their modular nature
and are confined to specific camera and scene settings. Our work adopts the
Neural Radiance Fields (NeRF) approach to learning 3D implicit representations
of scenes that are both dynamic and deformable over time, and furthermore with
unknown camera poses. We demonstrate this approach on endoscopic surgical
scenes from robotic surgery. This work removes the constraints of known camera
poses and overcomes the drawbacks of the state-of-the-art unstructured dynamic
scene reconstruction technique, which relies on the static part of the scene
for accurate reconstruction. Through several experimental datasets, we
demonstrate the versatility of our proposed model to adapt to diverse camera
and scene settings, and show its promise for both current and future robotic
surgical systems.";Shreya Saha<author:sep>Sainan Liu<author:sep>Shan Lin<author:sep>Jingpei Lu<author:sep>Michael Yip;http://arxiv.org/pdf/2309.15329v1;cs.CV;;nerf
2309.14800v1;http://arxiv.org/abs/2309.14800v1;2023-09-26;3D Density-Gradient based Edge Detection on Neural Radiance Fields  (NeRFs) for Geometric Reconstruction;"Generating geometric 3D reconstructions from Neural Radiance Fields (NeRFs)
is of great interest. However, accurate and complete reconstructions based on
the density values are challenging. The network output depends on input data,
NeRF network configuration and hyperparameter. As a result, the direct usage of
density values, e.g. via filtering with global density thresholds, usually
requires empirical investigations. Under the assumption that the density
increases from non-object to object area, the utilization of density gradients
from relative values is evident. As the density represents a position-dependent
parameter it can be handled anisotropically, therefore processing of the
voxelized 3D density field is justified. In this regard, we address geometric
3D reconstructions based on density gradients, whereas the gradients result
from 3D edge detection filters of the first and second derivatives, namely
Sobel, Canny and Laplacian of Gaussian. The gradients rely on relative
neighboring density values in all directions, thus are independent from
absolute magnitudes. Consequently, gradient filters are able to extract edges
along a wide density range, almost independent from assumptions and empirical
investigations. Our approach demonstrates the capability to achieve geometric
3D reconstructions with high geometric accuracy on object surfaces and
remarkable object completeness. Notably, Canny filter effectively eliminates
gaps, delivers a uniform point density, and strikes a favorable balance between
correctness and completeness across the scenes.";Miriam JÃ¤ger<author:sep>Boris Jutzi;http://arxiv.org/pdf/2309.14800v1;cs.CV;"8 pages, 4 figures, 2 tables. Will be published in the ISPRS The
  International Archives of Photogrammetry, Remote Sensing and Spatial
  Information Sciences";nerf
2309.14291v1;http://arxiv.org/abs/2309.14291v1;2023-09-25;Tiled Multiplane Images for Practical 3D Photography;"The task of synthesizing novel views from a single image has useful
applications in virtual reality and mobile computing, and a number of
approaches to the problem have been proposed in recent years. A Multiplane
Image (MPI) estimates the scene as a stack of RGBA layers, and can model
complex appearance effects, anti-alias depth errors and synthesize soft edges
better than methods that use textured meshes or layered depth images. And
unlike neural radiance fields, an MPI can be efficiently rendered on graphics
hardware. However, MPIs are highly redundant and require a large number of
depth layers to achieve plausible results. Based on the observation that the
depth complexity in local image regions is lower than that over the entire
image, we split an MPI into many small, tiled regions, each with only a few
depth planes. We call this representation a Tiled Multiplane Image (TMPI). We
propose a method for generating a TMPI with adaptive depth planes for
single-view 3D photography in the wild. Our synthesized results are comparable
to state-of-the-art single-view MPI methods while having lower computational
overhead.";Numair Khan<author:sep>Douglas Lanman<author:sep>Lei Xiao;http://arxiv.org/pdf/2309.14291v1;cs.CV;ICCV 2023;
2309.14293v3;http://arxiv.org/abs/2309.14293v3;2023-09-25;NAS-NeRF: Generative Neural Architecture Search for Neural Radiance  Fields;"Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their high computational complexity limits deployability. While existing
neural-based solutions strive for efficiency, they use one-size-fits-all
architectures regardless of scene complexity. The same architecture may be
unnecessarily large for simple scenes but insufficient for complex ones. Thus,
there is a need to dynamically optimize the neural network component of NeRFs
to achieve a balance between computational complexity and specific targets for
synthesis quality. We introduce NAS-NeRF, a generative neural architecture
search strategy that generates compact, scene-specialized NeRF architectures by
balancing architecture complexity and target synthesis quality metrics. Our
method incorporates constraints on target metrics and budgets to guide the
search towards architectures tailored for each scene. Experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made
publicly available at https://saeejithnair.github.io/NAS-NeRF.";Saeejith Nair<author:sep>Yuhao Chen<author:sep>Mohammad Javad Shafiee<author:sep>Alexander Wong;http://arxiv.org/pdf/2309.14293v3;cs.CV;8 pages;nerf
2309.14010v1;http://arxiv.org/abs/2309.14010v1;2023-09-25;Variational Inference for Scalable 3D Object-centric Learning;"We tackle the task of scalable unsupervised object-centric representation
learning on 3D scenes. Existing approaches to object-centric representation
learning show limitations in generalizing to larger scenes as their learning
processes rely on a fixed global coordinate system. In contrast, we propose to
learn view-invariant 3D object representations in localized object coordinate
systems. To this end, we estimate the object pose and appearance representation
separately and explicitly map object representations across views while
maintaining object identities. We adopt an amortized variational inference
pipeline that can process sequential input and scalably update object latent
distributions online. To handle large-scale scenes with a varying number of
objects, we further introduce a Cognitive Map that allows the registration and
query of objects on a per-scene global map to achieve scalable representation
learning. We explore the object-centric neural radiance field (NeRF) as our 3D
scene representation, which is jointly modeled within our unsupervised
object-centric learning framework. Experimental results on synthetic and real
datasets show that our proposed method can infer and maintain object-centric
representations of 3D scenes and outperforms previous models.";Tianyu Wang<author:sep>Kee Siong Ng<author:sep>Miaomiao Liu;http://arxiv.org/pdf/2309.14010v1;cs.CV;;nerf
2309.13607v2;http://arxiv.org/abs/2309.13607v2;2023-09-24;MM-NeRF: Multimodal-Guided 3D Multi-Style Transfer of Neural Radiance  Field;"3D style transfer aims to generate stylized views of 3D scenes with specified
styles, which requires high-quality generating and keeping multi-view
consistency. Existing methods still suffer the challenges of high-quality
stylization with texture details and stylization with multimodal guidance. In
this paper, we reveal that the common training method of stylization with NeRF,
which generates stylized multi-view supervision by 2D style transfer models,
causes the same object in supervision to show various states (color tone,
details, etc.) in different views, leading NeRF to tend to smooth the texture
details, further resulting in low-quality rendering for 3D multi-style
transfer. To tackle these problems, we propose a novel Multimodal-guided 3D
Multi-style transfer of NeRF, termed MM-NeRF. First, MM-NeRF projects
multimodal guidance into a unified space to keep the multimodal styles
consistency and extracts multimodal features to guide the 3D stylization.
Second, a novel multi-head learning scheme is proposed to relieve the
difficulty of learning multi-style transfer, and a multi-view style consistent
loss is proposed to track the inconsistency of multi-view supervision data.
Finally, a novel incremental learning mechanism to generalize MM-NeRF to any
new style with small costs. Extensive experiments on several real-world
datasets show that MM-NeRF achieves high-quality 3D multi-style stylization
with multimodal guidance, and keeps multi-view consistency and style
consistency between multimodal guidance. Codes will be released.";Zijiang Yang<author:sep>Zhongwei Qiu<author:sep>Chang Xu<author:sep>Dongmei Fu;http://arxiv.org/pdf/2309.13607v2;cs.CV;;nerf
2309.13240v1;http://arxiv.org/abs/2309.13240v1;2023-09-23;NeRF-Enhanced Outpainting for Faithful Field-of-View Extrapolation;"In various applications, such as robotic navigation and remote visual
assistance, expanding the field of view (FOV) of the camera proves beneficial
for enhancing environmental perception. Unlike image outpainting techniques
aimed solely at generating aesthetically pleasing visuals, these applications
demand an extended view that faithfully represents the scene. To achieve this,
we formulate a new problem of faithful FOV extrapolation that utilizes a set of
pre-captured images as prior knowledge of the scene. To address this problem,
we present a simple yet effective solution called NeRF-Enhanced Outpainting
(NEO) that uses extended-FOV images generated through NeRF to train a
scene-specific image outpainting model. To assess the performance of NEO, we
conduct comprehensive evaluations on three photorealistic datasets and one
real-world dataset. Extensive experiments on the benchmark datasets showcase
the robustness and potential of our method in addressing this challenge. We
believe our work lays a strong foundation for future exploration within the
research community.";Rui Yu<author:sep>Jiachen Liu<author:sep>Zihan Zhou<author:sep>Sharon X. Huang;http://arxiv.org/pdf/2309.13240v1;cs.CV;;nerf
2309.13039v1;http://arxiv.org/abs/2309.13039v1;2023-09-22;NeRRF: 3D Reconstruction and View Synthesis for Transparent and Specular  Objects with Neural Refractive-Reflective Fields;"Neural radiance fields (NeRF) have revolutionized the field of image-based
view synthesis. However, NeRF uses straight rays and fails to deal with
complicated light path changes caused by refraction and reflection. This
prevents NeRF from successfully synthesizing transparent or specular objects,
which are ubiquitous in real-world robotics and A/VR applications. In this
paper, we introduce the refractive-reflective field. Taking the object
silhouette as input, we first utilize marching tetrahedra with a progressive
encoding to reconstruct the geometry of non-Lambertian objects and then model
refraction and reflection effects of the object in a unified framework using
Fresnel terms. Meanwhile, to achieve efficient and effective anti-aliasing, we
propose a virtual cone supersampling technique. We benchmark our method on
different shapes, backgrounds and Fresnel terms on both real-world and
synthetic datasets. We also qualitatively and quantitatively benchmark the
rendering results of various editing applications, including material editing,
object replacement/insertion, and environment illumination estimation. Codes
and data are publicly available at https://github.com/dawning77/NeRRF.";Xiaoxue Chen<author:sep>Junchen Liu<author:sep>Hao Zhao<author:sep>Guyue Zhou<author:sep>Ya-Qin Zhang;http://arxiv.org/pdf/2309.13039v1;cs.CV;;nerf
2309.13101v2;http://arxiv.org/abs/2309.13101v2;2023-09-22;Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene  Reconstruction;"Implicit neural representation has paved the way for new approaches to
dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic
neural rendering methods rely heavily on these implicit representations, which
frequently struggle to capture the intricate details of objects in the scene.
Furthermore, implicit methods have difficulty achieving real-time rendering in
general dynamic scenes, limiting their use in a variety of tasks. To address
the issues, we propose a deformable 3D Gaussians Splatting method that
reconstructs scenes using 3D Gaussians and learns them in canonical space with
a deformation field to model monocular dynamic scenes. We also introduce an
annealing smoothing training mechanism with no extra overhead, which can
mitigate the impact of inaccurate poses on the smoothness of time interpolation
tasks in real-world datasets. Through a differential Gaussian rasterizer, the
deformable 3D Gaussians not only achieve higher rendering quality but also
real-time rendering speed. Experiments show that our method outperforms
existing methods significantly in terms of both rendering quality and speed,
making it well-suited for tasks such as novel-view synthesis, time
interpolation, and real-time rendering.";Ziyi Yang<author:sep>Xinyu Gao<author:sep>Wen Zhou<author:sep>Shaohui Jiao<author:sep>Yuqing Zhang<author:sep>Xiaogang Jin;http://arxiv.org/pdf/2309.13101v2;cs.CV;;
2309.12642v1;http://arxiv.org/abs/2309.12642v1;2023-09-22;RHINO: Regularizing the Hash-based Implicit Neural Representation;"The use of Implicit Neural Representation (INR) through a hash-table has
demonstrated impressive effectiveness and efficiency in characterizing
intricate signals. However, current state-of-the-art methods exhibit
insufficient regularization, often yielding unreliable and noisy results during
interpolations. We find that this issue stems from broken gradient flow between
input coordinates and indexed hash-keys, where the chain rule attempts to model
discrete hash-keys, rather than the continuous coordinates. To tackle this
concern, we introduce RHINO, in which a continuous analytical function is
incorporated to facilitate regularization by connecting the input coordinate
and the network additionally without modifying the architecture of current
hash-based INRs. This connection ensures a seamless backpropagation of
gradients from the network's output back to the input coordinates, thereby
enhancing regularization. Our experimental results not only showcase the
broadened regularization capability across different hash-based INRs like DINER
and Instant NGP, but also across a variety of tasks such as image fitting,
representation of signed distance functions, and optimization of 5D static / 6D
dynamic neural radiance fields. Notably, RHINO outperforms current
state-of-the-art techniques in both quality and speed, affirming its
superiority.";Hao Zhu<author:sep>Fengyi Liu<author:sep>Qi Zhang<author:sep>Xun Cao<author:sep>Zhan Ma;http://arxiv.org/pdf/2309.12642v1;cs.CV;17 pages, 11 figures;
2309.11966v1;http://arxiv.org/abs/2309.11966v1;2023-09-21;NeuralLabeling: A versatile toolset for labeling vision datasets using  Neural Radiance Fields;"We present NeuralLabeling, a labeling approach and toolset for annotating a
scene using either bounding boxes or meshes and generating segmentation masks,
affordance maps, 2D bounding boxes, 3D bounding boxes, 6DOF object poses, depth
maps and object meshes. NeuralLabeling uses Neural Radiance Fields (NeRF) as
renderer, allowing labeling to be performed using 3D spatial tools while
incorporating geometric clues such as occlusions, relying only on images
captured from multiple viewpoints as input. To demonstrate the applicability of
NeuralLabeling to a practical problem in robotics, we added ground truth depth
maps to 30000 frames of transparent object RGB and noisy depth maps of glasses
placed in a dishwasher captured using an RGBD sensor, yielding the
Dishwasher30k dataset. We show that training a simple deep neural network with
supervision using the annotated depth maps yields a higher reconstruction
performance than training with the previously applied weakly supervised
approach.";Floris Erich<author:sep>Naoya Chiba<author:sep>Yusuke Yoshiyasu<author:sep>Noriaki Ando<author:sep>Ryo Hanai<author:sep>Yukiyasu Domae;http://arxiv.org/pdf/2309.11966v1;cs.CV;"8 pages, project website:
  https://florise.github.io/neural_labeling_web/";nerf
2309.12183v1;http://arxiv.org/abs/2309.12183v1;2023-09-21;ORTexME: Occlusion-Robust Human Shape and Pose via Temporal Average  Texture and Mesh Encoding;"In 3D human shape and pose estimation from a monocular video, models trained
with limited labeled data cannot generalize well to videos with occlusion,
which is common in the wild videos. The recent human neural rendering
approaches focusing on novel view synthesis initialized by the off-the-shelf
human shape and pose methods have the potential to correct the initial human
shape. However, the existing methods have some drawbacks such as, erroneous in
handling occlusion, sensitive to inaccurate human segmentation, and ineffective
loss computation due to the non-regularized opacity field. To address these
problems, we introduce ORTexME, an occlusion-robust temporal method that
utilizes temporal information from the input video to better regularize the
occluded body parts. While our ORTexME is based on NeRF, to determine the
reliable regions for the NeRF ray sampling, we utilize our novel average
texture learning approach to learn the average appearance of a person, and to
infer a mask based on the average texture. In addition, to guide the
opacity-field updates in NeRF to suppress blur and noise, we propose the use of
human body mesh. The quantitative evaluation demonstrates that our method
achieves significant improvement on the challenging multi-person 3DPW dataset,
where our method achieves 1.8 P-MPJPE error reduction. The SOTA rendering-based
methods fail and enlarge the error up to 5.6 on the same dataset.";Yu Cheng<author:sep>Bo Wang<author:sep>Robby T. Tan;http://arxiv.org/pdf/2309.12183v1;cs.CV;8 pages, 8 figures;nerf
2309.11747v1;http://arxiv.org/abs/2309.11747v1;2023-09-21;MarkNerf:Watermarking for Neural Radiance Field;"A watermarking algorithm is proposed in this paper to address the copyright
protection issue of implicit 3D models. The algorithm involves embedding
watermarks into the images in the training set through an embedding network,
and subsequently utilizing the NeRF model for 3D modeling. A copyright verifier
is employed to generate a backdoor image by providing a secret perspective as
input to the neural radiation field. Subsequently, a watermark extractor is
devised using the hyperparameterization method of the neural network to extract
the embedded watermark image from that perspective. In a black box scenario, if
there is a suspicion that the 3D model has been used without authorization, the
verifier can extract watermarks from a secret perspective to verify network
copyright. Experimental results demonstrate that the proposed algorithm
effectively safeguards the copyright of 3D models. Furthermore, the extracted
watermarks exhibit favorable visual effects and demonstrate robust resistance
against various types of noise attacks.";Lifeng Chen<author:sep>Jia Liu<author:sep>Yan Ke<author:sep>Wenquan Sun<author:sep>Weina Dong<author:sep>Xiaozhong Pan;http://arxiv.org/pdf/2309.11747v1;cs.CR;;nerf
2309.11698v1;http://arxiv.org/abs/2309.11698v1;2023-09-21;Rendering stable features improves sampling-based localisation with  Neural radiance fields;"Neural radiance fields (NeRFs) are a powerful tool for implicit scene
representations, allowing for differentiable rendering and the ability to make
predictions about previously unseen viewpoints. From a robotics perspective,
there has been growing interest in object and scene-based localisation using
NeRFs, with a number of recent works relying on sampling-based or Monte-Carlo
localisation schemes. Unfortunately, these can be extremely computationally
expensive, requiring multiple network forward passes to infer camera or object
pose. To alleviate this, a variety of sampling strategies have been applied,
many relying on keypoint recognition techniques from classical computer vision.
This work conducts a systematic empirical comparison of these approaches and
shows that in contrast to conventional feature matching approaches for
geometry-based localisation, sampling-based localisation using NeRFs benefits
significantly from stable features. Results show that rendering stable features
can result in a tenfold reduction in the number of forward passes required, a
significant speed improvement.";Boxuan Zhang<author:sep>Lindsay Kleeman<author:sep>Michael Burke;http://arxiv.org/pdf/2309.11698v1;cs.RO;;nerf
2309.11767v1;http://arxiv.org/abs/2309.11767v1;2023-09-21;Fast Satellite Tensorial Radiance Field for Multi-date Satellite Imagery  of Large Size;"Existing NeRF models for satellite images suffer from slow speeds, mandatory
solar information as input, and limitations in handling large satellite images.
In response, we present SatensoRF, which significantly accelerates the entire
process while employing fewer parameters for satellite imagery of large size.
Besides, we observed that the prevalent assumption of Lambertian surfaces in
neural radiance fields falls short for vegetative and aquatic elements. In
contrast to the traditional hierarchical MLP-based scene representation, we
have chosen a multiscale tensor decomposition approach for color, volume
density, and auxiliary variables to model the lightfield with specular color.
Additionally, to rectify inconsistencies in multi-date imagery, we incorporate
total variation loss to restore the density tensor field and treat the problem
as a denosing task.To validate our approach, we conducted assessments of
SatensoRF using subsets from the spacenet multi-view dataset, which includes
both multi-date and single-date multi-view RGB images. Our results clearly
demonstrate that SatensoRF surpasses the state-of-the-art Sat-NeRF series in
terms of novel view synthesis performance. Significantly, SatensoRF requires
fewer parameters for training, resulting in faster training and inference
speeds and reduced computational demands.";Tongtong Zhang<author:sep>Yuanxiang Li;http://arxiv.org/pdf/2309.11767v1;cs.CV;;nerf
2309.11281v2;http://arxiv.org/abs/2309.11281v2;2023-09-20;Language-driven Object Fusion into Neural Radiance Fields with  Pose-Conditioned Dataset Updates;"Neural radiance field is an emerging rendering method that generates
high-quality multi-view consistent images from a neural scene representation
and volume rendering. Although neural radiance field-based techniques are
robust for scene reconstruction, their ability to add or remove objects remains
limited. This paper proposes a new language-driven approach for object
manipulation with neural radiance fields through dataset updates. Specifically,
to insert a new foreground object represented by a set of multi-view images
into a background radiance field, we use a text-to-image diffusion model to
learn and generate combined images that fuse the object of interest into the
given background across views. These combined images are then used for refining
the background radiance field so that we can render view-consistent images
containing both the object and the background. To ensure view consistency, we
propose a dataset updates strategy that prioritizes radiance field training
with camera views close to the already-trained views prior to propagating the
training to remaining views. We show that under the same dataset updates
strategy, we can easily adapt our method for object insertion using data from
text-to-3D models as well as object removal. Experimental results show that our
method generates photorealistic images of the edited scenes, and outperforms
state-of-the-art methods in 3D reconstruction and neural radiance field
blending.";Ka Chun Shum<author:sep>Jaeyeon Kim<author:sep>Binh-Son Hua<author:sep>Duc Thanh Nguyen<author:sep>Sai-Kit Yeung;http://arxiv.org/pdf/2309.11281v2;cs.CV;;
2309.11627v1;http://arxiv.org/abs/2309.11627v1;2023-09-20;GenLayNeRF: Generalizable Layered Representations with 3D Model  Alignment for Multi-Human View Synthesis;"Novel view synthesis (NVS) of multi-human scenes imposes challenges due to
the complex inter-human occlusions. Layered representations handle the
complexities by dividing the scene into multi-layered radiance fields, however,
they are mainly constrained to per-scene optimization making them inefficient.
Generalizable human view synthesis methods combine the pre-fitted 3D human
meshes with image features to reach generalization, yet they are mainly
designed to operate on single-human scenes. Another drawback is the reliance on
multi-step optimization techniques for parametric pre-fitting of the 3D body
models that suffer from misalignment with the images in sparse view settings
causing hallucinations in synthesized views. In this work, we propose,
GenLayNeRF, a generalizable layered scene representation for free-viewpoint
rendering of multiple human subjects which requires no per-scene optimization
and very sparse views as input. We divide the scene into multi-human layers
anchored by the 3D body meshes. We then ensure pixel-level alignment of the
body models with the input views through a novel end-to-end trainable module
that carries out iterative parametric correction coupled with multi-view
feature fusion to produce aligned 3D models. For NVS, we extract point-wise
image-aligned and human-anchored features which are correlated and fused using
self-attention and cross-attention modules. We augment low-level RGB values
into the features with an attention-based RGB fusion module. To evaluate our
approach, we construct two multi-human view synthesis datasets; DeepMultiSyn
and ZJU-MultiHuman. The results indicate that our proposed approach outperforms
generalizable and non-human per-scene NeRF methods while performing at par with
layered per-scene methods without test time optimization.";Youssef Abdelkareem<author:sep>Shady Shehata<author:sep>Fakhri Karray;http://arxiv.org/pdf/2309.11627v1;cs.CV;Accepted to GCPR 2023;nerf
2309.11009v2;http://arxiv.org/abs/2309.11009v2;2023-09-20;Controllable Dynamic Appearance for Neural 3D Portraits;"Recent advances in Neural Radiance Fields (NeRFs) have made it possible to
reconstruct and reanimate dynamic portrait scenes with control over head-pose,
facial expressions and viewing direction. However, training such models assumes
photometric consistency over the deformed region e.g. the face must be evenly
lit as it deforms with changing head-pose and facial expression. Such
photometric consistency across frames of a video is hard to maintain, even in
studio environments, thus making the created reanimatable neural portraits
prone to artifacts during reanimation. In this work, we propose CoDyNeRF, a
system that enables the creation of fully controllable 3D portraits in
real-world capture conditions. CoDyNeRF learns to approximate illumination
dependent effects via a dynamic appearance model in the canonical space that is
conditioned on predicted surface normals and the facial expressions and
head-pose deformations. The surface normals prediction is guided using 3DMM
normals that act as a coarse prior for the normals of the human head, where
direct prediction of normals is hard due to rigid and non-rigid deformations
induced by head-pose and facial expression changes. Using only a
smartphone-captured short video of a subject for training, we demonstrate the
effectiveness of our method on free view synthesis of a portrait scene with
explicit head pose and expression controls, and realistic lighting effects. The
project page can be found here:
http://shahrukhathar.github.io/2023/08/22/CoDyNeRF.html";ShahRukh Athar<author:sep>Zhixin Shu<author:sep>Zexiang Xu<author:sep>Fujun Luan<author:sep>Sai Bi<author:sep>Kalyan Sunkavalli<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2309.11009v2;cs.CV;;nerf
2309.10987v3;http://arxiv.org/abs/2309.10987v3;2023-09-20;SpikingNeRF: Making Bio-inspired Neural Networks See through the Real  World;"Spiking neural networks (SNNs) have been thriving on numerous tasks to
leverage their promising energy efficiency and exploit their potentialities as
biologically plausible intelligence. Meanwhile, the Neural Radiance Fields
(NeRF) render high-quality 3D scenes with massive energy consumption, but few
works delve into the energy-saving solution with a bio-inspired approach. In
this paper, we propose SpikingNeRF, which aligns the radiance ray with the
temporal dimension of SNN, to naturally accommodate the SNN to the
reconstruction of Radiance Fields. Thus, the computation turns into a
spike-based, multiplication-free manner, reducing the energy consumption. In
SpikingNeRF, each sampled point on the ray is matched onto a particular time
step, and represented in a hybrid manner where the voxel grids are maintained
as well. Based on the voxel grids, sampled points are determined whether to be
masked for better training and inference. However, this operation also incurs
irregular temporal length. We propose the temporal padding strategy to tackle
the masked samples to maintain regular temporal length, i.e., regular tensors,
and the temporal condensing strategy to form a denser data structure for
hardware-friendly computation. Extensive experiments on various datasets
demonstrate that our method reduces the 70.79% energy consumption on average
and obtains comparable synthesis quality with the ANN baseline.";Xingting Yao<author:sep>Qinghao Hu<author:sep>Tielong Liu<author:sep>Zitao Mo<author:sep>Zeyu Zhu<author:sep>Zhengyang Zhuge<author:sep>Jian Cheng;http://arxiv.org/pdf/2309.10987v3;cs.NE;;nerf
2309.11525v2;http://arxiv.org/abs/2309.11525v2;2023-09-20;Light Field Diffusion for Single-View Novel View Synthesis;"Single-view novel view synthesis, the task of generating images from new
viewpoints based on a single reference image, is an important but challenging
task in computer vision. Recently, Denoising Diffusion Probabilistic Model
(DDPM) has become popular in this area due to its strong ability to generate
high-fidelity images. However, current diffusion-based methods directly rely on
camera pose matrices as viewing conditions, globally and implicitly introducing
3D constraints. These methods may suffer from inconsistency among generated
images from different perspectives, especially in regions with intricate
textures and structures. In this work, we present Light Field Diffusion (LFD),
a conditional diffusion-based model for single-view novel view synthesis.
Unlike previous methods that employ camera pose matrices, LFD transforms the
camera view information into light field encoding and combines it with the
reference image. This design introduces local pixel-wise constraints within the
diffusion models, thereby encouraging better multi-view consistency.
Experiments on several datasets show that our LFD can efficiently generate
high-fidelity images and maintain better 3D consistency even in intricate
regions. Our method can generate images with higher quality than NeRF-based
models, and we obtain sample quality similar to other diffusion-based models
but with only one-third of the model size.";Yifeng Xiong<author:sep>Haoyu Ma<author:sep>Shanlin Sun<author:sep>Kun Han<author:sep>Xiaohui Xie;http://arxiv.org/pdf/2309.11525v2;cs.CV;;nerf
2309.10503v1;http://arxiv.org/abs/2309.10503v1;2023-09-19;Steganography for Neural Radiance Fields by Backdooring;"The utilization of implicit representation for visual data (such as images,
videos, and 3D models) has recently gained significant attention in computer
vision research. In this letter, we propose a novel model steganography scheme
with implicit neural representation. The message sender leverages Neural
Radiance Fields (NeRF) and its viewpoint synthesis capabilities by introducing
a viewpoint as a key. The NeRF model generates a secret viewpoint image, which
serves as a backdoor. Subsequently, we train a message extractor using
overfitting to establish a one-to-one mapping between the secret message and
the secret viewpoint image. The sender delivers the trained NeRF model and the
message extractor to the receiver over the open channel, and the receiver
utilizes the key shared by both parties to obtain the rendered image in the
secret view from the NeRF model, and then obtains the secret message through
the message extractor. The inherent complexity of the viewpoint information
prevents attackers from stealing the secret message accurately. Experimental
results demonstrate that the message extractor trained in this letter achieves
high-capacity steganography with fast performance, achieving a 100\% accuracy
in message extraction. Furthermore, the extensive viewpoint key space of NeRF
ensures the security of the steganography scheme.";Weina Dong<author:sep>Jia Liu<author:sep>Yan Ke<author:sep>Lifeng Chen<author:sep>Wenquan Sun<author:sep>Xiaozhong Pan;http://arxiv.org/pdf/2309.10503v1;cs.CR;6 pages, 7 figures;nerf
2309.10684v1;http://arxiv.org/abs/2309.10684v1;2023-09-19;Locally Stylized Neural Radiance Fields;"In recent years, there has been increasing interest in applying stylization
on 3D scenes from a reference style image, in particular onto neural radiance
fields (NeRF). While performing stylization directly on NeRF guarantees
appearance consistency over arbitrary novel views, it is a challenging problem
to guide the transfer of patterns from the style image onto different parts of
the NeRF scene. In this work, we propose a stylization framework for NeRF based
on local style transfer. In particular, we use a hash-grid encoding to learn
the embedding of the appearance and geometry components, and show that the
mapping defined by the hash table allows us to control the stylization to a
certain extent. Stylization is then achieved by optimizing the appearance
branch while keeping the geometry branch fixed. To support local style
transfer, we propose a new loss function that utilizes a segmentation network
and bipartite matching to establish region correspondences between the style
image and the content images obtained from volume rendering. Our experiments
show that our method yields plausible stylization results with novel view
synthesis while having flexible controllability via manipulating and
customizing the region correspondences.";Hong-Wing Pang<author:sep>Binh-Son Hua<author:sep>Sai-Kit Yeung;http://arxiv.org/pdf/2309.10684v1;cs.CV;ICCV 2023;nerf
2309.10011v2;http://arxiv.org/abs/2309.10011v2;2023-09-18;Instant Photorealistic Style Transfer: A Lightweight and Adaptive  Approach;"In this paper, we propose an Instant Photorealistic Style Transfer (IPST)
approach, designed to achieve instant photorealistic style transfer on
super-resolution inputs without the need for pre-training on pair-wise datasets
or imposing extra constraints. Our method utilizes a lightweight StyleNet to
enable style transfer from a style image to a content image while preserving
non-color information. To further enhance the style transfer process, we
introduce an instance-adaptive optimization to prioritize the photorealism of
outputs and accelerate the convergence of the style network, leading to a rapid
training completion within seconds. Moreover, IPST is well-suited for
multi-frame style transfer tasks, as it retains temporal and multi-view
consistency of the multi-frame inputs such as video and Neural Radiance Field
(NeRF). Experimental results demonstrate that IPST requires less GPU memory
usage, offers faster multi-frame transfer speed, and generates photorealistic
outputs, making it a promising solution for various photorealistic transfer
applications.";Rong Liu<author:sep>Enyu Zhao<author:sep>Zhiyuan Liu<author:sep>Andrew Feng<author:sep>Scott John Easley;http://arxiv.org/pdf/2309.10011v2;cs.CV;8 pages (reference excluded), 6 figures, 4 tables;nerf
2309.09502v1;http://arxiv.org/abs/2309.09502v1;2023-09-18;RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering  Supervision;"3D occupancy prediction holds significant promise in the fields of robot
perception and autonomous driving, which quantifies 3D scenes into grid cells
with semantic labels. Recent works mainly utilize complete occupancy labels in
3D voxel space for supervision. However, the expensive annotation process and
sometimes ambiguous labels have severely constrained the usability and
scalability of 3D occupancy models. To address this, we present RenderOcc, a
novel paradigm for training 3D occupancy models only using 2D labels.
Specifically, we extract a NeRF-style 3D volume representation from multi-view
images, and employ volume rendering techniques to establish 2D renderings, thus
enabling direct 3D supervision from 2D semantics and depth labels.
Additionally, we introduce an Auxiliary Ray method to tackle the issue of
sparse viewpoints in autonomous driving scenarios, which leverages sequential
frames to construct comprehensive 2D rendering for each object. To our best
knowledge, RenderOcc is the first attempt to train multi-view 3D occupancy
models only using 2D labels, reducing the dependence on costly 3D occupancy
annotations. Extensive experiments demonstrate that RenderOcc achieves
comparable performance to models fully supervised with 3D labels, underscoring
the significance of this approach in real-world applications.";Mingjie Pan<author:sep>Jiaming Liu<author:sep>Renrui Zhang<author:sep>Peixiang Huang<author:sep>Xiaoqi Li<author:sep>Li Liu<author:sep>Shanghang Zhang;http://arxiv.org/pdf/2309.09502v1;cs.CV;;nerf
2309.09295v1;http://arxiv.org/abs/2309.09295v1;2023-09-17;NeRF-VINS: A Real-time Neural Radiance Field Map-based Visual-Inertial  Navigation System;"Achieving accurate, efficient, and consistent localization within an a priori
environment map remains a fundamental challenge in robotics and computer
vision. Conventional map-based keyframe localization often suffers from
sub-optimal viewpoints due to limited field of view (FOV), thus degrading its
performance. To address this issue, in this paper, we design a real-time
tightly-coupled Neural Radiance Fields (NeRF)-aided visual-inertial navigation
system (VINS), termed NeRF-VINS. By effectively leveraging NeRF's potential to
synthesize novel views, essential for addressing limited viewpoints, the
proposed NeRF-VINS optimally fuses IMU and monocular image measurements along
with synthetically rendered images within an efficient filter-based framework.
This tightly coupled integration enables 3D motion tracking with bounded error.
We extensively compare the proposed NeRF-VINS against the state-of-the-art
methods that use prior map information, which is shown to achieve superior
performance. We also demonstrate the proposed method is able to perform
real-time estimation at 15 Hz, on a resource-constrained Jetson AGX Orin
embedded platform with impressive accuracy.";Saimouli Katragadda<author:sep>Woosik Lee<author:sep>Yuxiang Peng<author:sep>Patrick Geneva<author:sep>Chuchu Chen<author:sep>Chao Guo<author:sep>Mingyang Li<author:sep>Guoquan Huang;http://arxiv.org/pdf/2309.09295v1;cs.RO;6 pages, 7 figures;nerf
2309.08927v1;http://arxiv.org/abs/2309.08927v1;2023-09-16;DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic  NeRF;"Dynamic reconstruction with neural radiance fields (NeRF) requires accurate
camera poses. These are often hard to retrieve with existing
structure-from-motion (SfM) pipelines as both camera and scene content can
change. We propose DynaMoN that leverages simultaneous localization and mapping
(SLAM) jointly with motion masking to handle dynamic scene content. Our robust
SLAM-based tracking module significantly accelerates the training process of
the dynamic NeRF while improving the quality of synthesized views at the same
time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and
the DyCheck's iPhone dataset, three real-world datasets, shows the advantages
of DynaMoN both for camera pose estimation and novel view synthesis.";Mert Asim Karaoglu<author:sep>Hannah Schieber<author:sep>Nicolas Schischka<author:sep>Melih GÃ¶rgÃ¼lÃ¼<author:sep>Florian GrÃ¶tzner<author:sep>Alexander Ladikos<author:sep>Daniel Roth<author:sep>Nassir Navab<author:sep>Benjamin Busam;http://arxiv.org/pdf/2309.08927v1;cs.CV;6 pages, 4 figures;nerf
2309.08596v1;http://arxiv.org/abs/2309.08596v1;2023-09-15;Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion;"Event cameras offer many advantages over standard cameras due to their
distinctive principle of operation: low power, low latency, high temporal
resolution and high dynamic range. Nonetheless, the success of many downstream
visual applications also hinges on an efficient and effective scene
representation, where Neural Radiance Field (NeRF) is seen as the leading
candidate. Such promise and potential of event cameras and NeRF inspired recent
works to investigate on the reconstruction of NeRF from moving event cameras.
However, these works are mainly limited in terms of the dependence on dense and
low-noise event streams, as well as generalization to arbitrary contrast
threshold values and camera speed profiles. In this work, we propose Robust
e-NeRF, a novel method to directly and robustly reconstruct NeRFs from moving
event cameras under various real-world conditions, especially from sparse and
noisy events generated under non-uniform motion. It consists of two key
components: a realistic event generation model that accounts for various
intrinsic parameters (e.g. time-independent, asymmetric threshold and
refractory period) and non-idealities (e.g. pixel-to-pixel threshold
variation), as well as a complementary pair of normalized reconstruction losses
that can effectively generalize to arbitrary speed profiles and intrinsic
parameter values without such prior knowledge. Experiments on real and novel
realistically simulated sequences verify our effectiveness. Our code, synthetic
dataset and improved event simulator are public.";Weng Fei Low<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2309.08596v1;cs.CV;"Accepted to ICCV 2023. Project website is accessible at
  https://wengflow.github.io/robust-e-nerf";nerf
2309.08416v2;http://arxiv.org/abs/2309.08416v2;2023-09-15;Deformable Neural Radiance Fields using RGB and Event Cameras;"Modeling Neural Radiance Fields for fast-moving deformable objects from
visual data alone is a challenging problem. A major issue arises due to the
high deformation and low acquisition rates. To address this problem, we propose
to use event cameras that offer very fast acquisition of visual change in an
asynchronous manner. In this work, we develop a novel method to model the
deformable neural radiance fields using RGB and event cameras. The proposed
method uses the asynchronous stream of events and calibrated sparse RGB frames.
In our setup, the camera pose at the individual events required to integrate
them into the radiance fields remains unknown. Our method jointly optimizes
these poses and the radiance field. This happens efficiently by leveraging the
collection of events at once and actively sampling the events during learning.
Experiments conducted on both realistically rendered graphics and real-world
datasets demonstrate a significant benefit of the proposed method over the
state-of-the-art and the compared baseline.
  This shows a promising direction for modeling deformable neural radiance
fields in real-world dynamic scenes.";Qi Ma<author:sep>Danda Pani Paudel<author:sep>Ajad Chhatkuli<author:sep>Luc Van Gool;http://arxiv.org/pdf/2309.08416v2;cs.CV;;
2309.08523v2;http://arxiv.org/abs/2309.08523v2;2023-09-15;Breathing New Life into 3D Assets with Generative Repainting;"Diffusion-based text-to-image models ignited immense attention from the
vision community, artists, and content creators. Broad adoption of these models
is due to significant improvement in the quality of generations and efficient
conditioning on various modalities, not just text. However, lifting the rich
generative priors of these 2D models into 3D is challenging. Recent works have
proposed various pipelines powered by the entanglement of diffusion models and
neural fields. We explore the power of pretrained 2D diffusion models and
standard 3D neural radiance fields as independent, standalone tools and
demonstrate their ability to work together in a non-learned fashion. Such
modularity has the intrinsic advantage of eased partial upgrades, which became
an important property in such a fast-paced domain. Our pipeline accepts any
legacy renderable geometry, such as textured or untextured meshes, orchestrates
the interaction between 2D generative refinement and 3D consistency enforcement
tools, and outputs a painted input geometry in several formats. We conduct a
large-scale study on a wide range of objects and categories from the
ShapeNetSem dataset and demonstrate the advantages of our approach, both
qualitatively and quantitatively. Project page:
https://www.obukhov.ai/repainting_3d_assets";Tianfu Wang<author:sep>Menelaos Kanakis<author:sep>Konrad Schindler<author:sep>Luc Van Gool<author:sep>Anton Obukhov;http://arxiv.org/pdf/2309.08523v2;cs.CV;;
2309.07846v2;http://arxiv.org/abs/2309.07846v2;2023-09-14;MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image  Acquisition Systems;"Neural Radiance Fields (NeRF) employ multi-view images for 3D scene
representation and have shown remarkable performance. As one of the primary
sources of multi-view images, multi-camera systems encounter challenges such as
varying intrinsic parameters and frequent pose changes. Most previous
NeRF-based methods often assume a global unique camera and seldom consider
scenarios with multiple cameras. Besides, some pose-robust methods still remain
susceptible to suboptimal solutions when poses are poor initialized. In this
paper, we propose MC-NeRF, a method can jointly optimize both intrinsic and
extrinsic parameters for bundle-adjusting Neural Radiance Fields. Firstly, we
conduct a theoretical analysis to tackle the degenerate case and coupling issue
that arise from the joint optimization between intrinsic and extrinsic
parameters. Secondly, based on the proposed solutions, we introduce an
efficient calibration image acquisition scheme for multi-camera systems,
including the design of calibration object. Lastly, we present a global
end-to-end network with training sequence that enables the regression of
intrinsic and extrinsic parameters, along with the rendering network. Moreover,
most existing datasets are designed for unique camera, we create a new dataset
that includes four different styles of multi-camera acquisition systems,
allowing readers to generate custom datasets. Experiments confirm the
effectiveness of our method when each image corresponds to different camera
parameters. Specifically, we adopt up to 110 images with 110 different
intrinsic and extrinsic parameters, to achieve 3D scene representation without
providing initial poses. The Code and supplementary materials are available at
https://in2-viaun.github.io/MC-NeRF.";Yu Gao<author:sep>Lutong Su<author:sep>Hao Liang<author:sep>Yufeng Yue<author:sep>Yi Yang<author:sep>Mengyin Fu;http://arxiv.org/pdf/2309.07846v2;cs.CV;This manuscript is currently under review;nerf
2309.07668v1;http://arxiv.org/abs/2309.07668v1;2023-09-14;CoRF : Colorizing Radiance Fields using Knowledge Distillation;"Neural radiance field (NeRF) based methods enable high-quality novel-view
synthesis for multi-view images. This work presents a method for synthesizing
colorized novel views from input grey-scale multi-view images. When we apply
image or video-based colorization methods on the generated grey-scale novel
views, we observe artifacts due to inconsistency across views. Training a
radiance field network on the colorized grey-scale image sequence also does not
solve the 3D consistency issue. We propose a distillation based method to
transfer color knowledge from the colorization networks trained on natural
images to the radiance field network. Specifically, our method uses the
radiance field network as a 3D representation and transfers knowledge from
existing 2D colorization methods. The experimental results demonstrate that the
proposed method produces superior colorized novel views for indoor and outdoor
scenes while maintaining cross-view consistency than baselines. Further, we
show the efficacy of our method on applications like colorization of radiance
field network trained from 1.) Infra-Red (IR) multi-view images and 2.) Old
grey-scale multi-view image sequences.";Ankit Dhiman<author:sep>R Srinath<author:sep>Srinjay Sarkar<author:sep>Lokesh R Boregowda<author:sep>R Venkatesh Babu;http://arxiv.org/pdf/2309.07668v1;cs.CV;AI3DCC @ ICCV 2023;nerf
2310.12987v1;http://arxiv.org/abs/2310.12987v1;2023-09-14;Spec-NeRF: Multi-spectral Neural Radiance Fields;"We propose Multi-spectral Neural Radiance Fields(Spec-NeRF) for jointly
reconstructing a multispectral radiance field and spectral sensitivity
functions(SSFs) of the camera from a set of color images filtered by different
filters. The proposed method focuses on modeling the physical imaging process,
and applies the estimated SSFs and radiance field to synthesize novel views of
multispectral scenes. In this method, the data acquisition requires only a
low-cost trichromatic camera and several off-the-shelf color filters, making it
more practical than using specialized 3D scanning and spectral imaging
equipment. Our experiments on both synthetic and real scenario datasets
demonstrate that utilizing filtered RGB images with learnable NeRF and SSFs can
achieve high fidelity and promising spectral reconstruction while retaining the
inherent capability of NeRF to comprehend geometric structures. Code is
available at https://github.com/CPREgroup/SpecNeRF-v2.";Jiabao Li<author:sep>Yuqi Li<author:sep>Ciliang Sun<author:sep>Chong Wang<author:sep>Jinhui Xiang;http://arxiv.org/pdf/2310.12987v1;eess.IV;;nerf
2309.08040v1;http://arxiv.org/abs/2309.08040v1;2023-09-14;Gradient based Grasp Pose Optimization on a NeRF that Approximates Grasp  Success;"Current robotic grasping methods often rely on estimating the pose of the
target object, explicitly predicting grasp poses, or implicitly estimating
grasp success probabilities. In this work, we propose a novel approach that
directly maps gripper poses to their corresponding grasp success values,
without considering objectness. Specifically, we leverage a Neural Radiance
Field (NeRF) architecture to learn a scene representation and use it to train a
grasp success estimator that maps each pose in the robot's task space to a
grasp success value. We employ this learned estimator to tune its inputs, i.e.,
grasp poses, by gradient-based optimization to obtain successful grasp poses.
Contrary to other NeRF-based methods which enhance existing grasp pose
estimation approaches by relying on NeRF's rendering capabilities or directly
estimate grasp poses in a discretized space using NeRF's scene representation
capabilities, our approach uniquely sidesteps both the need for rendering and
the limitation of discretization. We demonstrate the effectiveness of our
approach on four simulated 3DoF (Degree of Freedom) robotic grasping tasks and
show that it can generalize to novel objects. Our best model achieves an
average translation error of 3mm from valid grasp poses. This work opens the
door for future research to apply our approach to higher DoF grasps and
real-world scenarios.";Gergely SÃ³ti<author:sep>BjÃ¶rn Hein<author:sep>Christian Wurll;http://arxiv.org/pdf/2309.08040v1;cs.RO;;nerf
2309.07640v2;http://arxiv.org/abs/2309.07640v2;2023-09-14;Indoor Scene Reconstruction with Fine-Grained Details Using Hybrid  Representation and Normal Prior Enhancement;"The reconstruction of indoor scenes from multi-view RGB images is challenging
due to the coexistence of flat and texture-less regions alongside delicate and
fine-grained regions. Recent methods leverage neural radiance fields aided by
predicted surface normal priors to recover the scene geometry. These methods
excel in producing complete and smooth results for floor and wall areas.
However, they struggle to capture complex surfaces with high-frequency
structures due to the inadequate neural representation and the inaccurately
predicted normal priors. This work aims to reconstruct high-fidelity surfaces
with fine-grained details by addressing the above limitations. To improve the
capacity of the implicit representation, we propose a hybrid architecture to
represent low-frequency and high-frequency regions separately. To enhance the
normal priors, we introduce a simple yet effective image sharpening and
denoising technique, coupled with a network that estimates the pixel-wise
uncertainty of the predicted surface normal vectors. Identifying such
uncertainty can prevent our model from being misled by unreliable surface
normal supervisions that hinder the accurate reconstruction of intricate
geometries. Experiments on the benchmark datasets show that our method
outperforms existing methods in terms of reconstruction quality. Furthermore,
the proposed method also generalizes well to real-world indoor scenarios
captured by our hand-held mobile phones. Our code is publicly available at:
https://github.com/yec22/Fine-Grained-Indoor-Recon.";Sheng Ye<author:sep>Yubin Hu<author:sep>Matthieu Lin<author:sep>Yu-Hui Wen<author:sep>Wang Zhao<author:sep>Yong-Jin Liu<author:sep>Wenping Wang;http://arxiv.org/pdf/2309.07640v2;cs.CV;;
2309.07752v1;http://arxiv.org/abs/2309.07752v1;2023-09-14;DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for  High-Fidelity Talking Portrait Synthesis;"In this paper, we present the decomposed triplane-hash neural radiance fields
(DT-NeRF), a framework that significantly improves the photorealistic rendering
of talking faces and achieves state-of-the-art results on key evaluation
datasets. Our architecture decomposes the facial region into two specialized
triplanes: one specialized for representing the mouth, and the other for the
broader facial features. We introduce audio features as residual terms and
integrate them as query vectors into our model through an audio-mouth-face
transformer. Additionally, our method leverages the capabilities of Neural
Radiance Fields (NeRF) to enrich the volumetric representation of the entire
face through additive volumetric rendering techniques. Comprehensive
experimental evaluations corroborate the effectiveness and superiority of our
proposed approach.";Yaoyu Su<author:sep>Shaohui Wang<author:sep>Haoqian Wang;http://arxiv.org/pdf/2309.07752v1;cs.CV;5 pages, 5 figures. Submitted to ICASSP 2024;nerf
2309.07125v1;http://arxiv.org/abs/2309.07125v1;2023-09-13;Text-Guided Generation and Editing of Compositional 3D Avatars;"Our goal is to create a realistic 3D facial avatar with hair and accessories
using only a text description. While this challenge has attracted significant
recent interest, existing methods either lack realism, produce unrealistic
shapes, or do not support editing, such as modifications to the hairstyle. We
argue that existing methods are limited because they employ a monolithic
modeling approach, using a single representation for the head, face, hair, and
accessories. Our observation is that the hair and face, for example, have very
different structural qualities that benefit from different representations.
Building on this insight, we generate avatars with a compositional model, in
which the head, face, and upper body are represented with traditional 3D
meshes, and the hair, clothing, and accessories with neural radiance fields
(NeRF). The model-based mesh representation provides a strong geometric prior
for the face region, improving realism while enabling editing of the person's
appearance. By using NeRFs to represent the remaining components, our method is
able to model and synthesize parts with complex geometry and appearance, such
as curly hair and fluffy scarves. Our novel system synthesizes these
high-quality compositional avatars from text descriptions. The experimental
results demonstrate that our method, Text-guided generation and Editing of
Compositional Avatars (TECA), produces avatars that are more realistic than
those of recent methods while being editable because of their compositional
nature. For example, our TECA enables the seamless transfer of compositional
features like hairstyles, scarves, and other accessories between avatars. This
capability supports applications such as virtual try-on.";Hao Zhang<author:sep>Yao Feng<author:sep>Peter Kulits<author:sep>Yandong Wen<author:sep>Justus Thies<author:sep>Michael J. Black;http://arxiv.org/pdf/2309.07125v1;cs.CV;Home page: https://yfeng95.github.io/teca;nerf
2309.06802v1;http://arxiv.org/abs/2309.06802v1;2023-09-13;Dynamic NeRFs for Soccer Scenes;"The long-standing problem of novel view synthesis has many applications,
notably in sports broadcasting. Photorealistic novel view synthesis of soccer
actions, in particular, is of enormous interest to the broadcast industry. Yet
only a few industrial solutions have been proposed, and even fewer that achieve
near-broadcast quality of the synthetic replays. Except for their setup of
multiple static cameras around the playfield, the best proprietary systems
disclose close to no information about their inner workings. Leveraging
multiple static cameras for such a task indeed presents a challenge rarely
tackled in the literature, for a lack of public datasets: the reconstruction of
a large-scale, mostly static environment, with small, fast-moving elements.
Recently, the emergence of neural radiance fields has induced stunning progress
in many novel view synthesis applications, leveraging deep learning principles
to produce photorealistic results in the most challenging settings. In this
work, we investigate the feasibility of basing a solution to the task on
dynamic NeRFs, i.e., neural models purposed to reconstruct general dynamic
content. We compose synthetic soccer environments and conduct multiple
experiments using them, identifying key components that help reconstruct soccer
scenes with dynamic NeRFs. We show that, although this approach cannot fully
meet the quality requirements for the target application, it suggests promising
avenues toward a cost-efficient, automatic solution. We also make our work
dataset and code publicly available, with the goal to encourage further efforts
from the research community on the task of novel view synthesis for dynamic
soccer scenes. For code, data, and video results, please see
https://soccernerfs.isach.be.";Sacha Lewin<author:sep>Maxime Vandegar<author:sep>Thomas Hoyoux<author:sep>Olivier Barnich<author:sep>Gilles Louppe;http://arxiv.org/pdf/2309.06802v1;cs.CV;"Accepted at the 6th International ACM Workshop on Multimedia Content
  Analysis in Sports. 8 pages, 9 figures. Project page:
  https://soccernerfs.isach.be";nerf
2309.06441v1;http://arxiv.org/abs/2309.06441v1;2023-09-12;Learning Disentangled Avatars with Hybrid 3D Representations;"Tremendous efforts have been made to learn animatable and photorealistic
human avatars. Towards this end, both explicit and implicit 3D representations
are heavily studied for a holistic modeling and capture of the whole human
(e.g., body, clothing, face and hair), but neither representation is an optimal
choice in terms of representation efficacy since different parts of the human
avatar have different modeling desiderata. For example, meshes are generally
not suitable for modeling clothing and hair. Motivated by this, we present
Disentangled Avatars~(DELTA), which models humans with hybrid explicit-implicit
3D representations. DELTA takes a monocular RGB video as input, and produces a
human avatar with separate body and clothing/hair layers. Specifically, we
demonstrate two important applications for DELTA. For the first one, we
consider the disentanglement of the human body and clothing and in the second,
we disentangle the face and hair. To do so, DELTA represents the body or face
with an explicit mesh-based parametric 3D model and the clothing or hair with
an implicit neural radiance field. To make this possible, we design an
end-to-end differentiable renderer that integrates meshes into volumetric
rendering, enabling DELTA to learn directly from monocular videos without any
3D supervision. Finally, we show that how these two applications can be easily
combined to model full-body avatars, such that the hair, face, body and
clothing can be fully disentangled yet jointly rendered. Such a disentanglement
enables hair and clothing transfer to arbitrary body shapes. We empirically
validate the effectiveness of DELTA's disentanglement by demonstrating its
promising performance on disentangled reconstruction, virtual clothing try-on
and hairstyle transfer. To facilitate future research, we also release an
open-sourced pipeline for the study of hybrid human avatar modeling.";Yao Feng<author:sep>Weiyang Liu<author:sep>Timo Bolkart<author:sep>Jinlong Yang<author:sep>Marc Pollefeys<author:sep>Michael J. Black;http://arxiv.org/pdf/2309.06441v1;cs.CV;"home page: https://yfeng95.github.io/delta. arXiv admin note: text
  overlap with arXiv:2210.01868";
2309.06030v1;http://arxiv.org/abs/2309.06030v1;2023-09-12;Federated Learning for Large-Scale Scene Modeling with Neural Radiance  Fields;"We envision a system to continuously build and maintain a map based on
earth-scale neural radiance fields (NeRF) using data collected from vehicles
and drones in a lifelong learning manner. However, existing large-scale
modeling by NeRF has problems in terms of scalability and maintainability when
modeling earth-scale environments. Therefore, to address these problems, we
propose a federated learning pipeline for large-scale modeling with NeRF. We
tailor the model aggregation pipeline in federated learning for NeRF, thereby
allowing local updates of NeRF. In the aggregation step, the accuracy of the
clients' global pose is critical. Thus, we also propose global pose alignment
to align the noisy global pose of clients before the aggregation step. In
experiments, we show the effectiveness of the proposed pose alignment and the
federated learning pipeline on the large-scale scene dataset, Mill19.";Teppei Suzuki;http://arxiv.org/pdf/2309.06030v1;cs.CV;;nerf
2309.05339v1;http://arxiv.org/abs/2309.05339v1;2023-09-11;PAg-NeRF: Towards fast and efficient end-to-end panoptic 3D  representations for agricultural robotics;"Precise scene understanding is key for most robot monitoring and intervention
tasks in agriculture. In this work we present PAg-NeRF which is a novel
NeRF-based system that enables 3D panoptic scene understanding. Our
representation is trained using an image sequence with noisy robot odometry
poses and automatic panoptic predictions with inconsistent IDs between frames.
Despite this noisy input, our system is able to output scene geometry,
photo-realistic renders and 3D consistent panoptic representations with
consistent instance IDs. We evaluate this novel system in a very challenging
horticultural scenario and in doing so demonstrate an end-to-end trainable
system that can make use of noisy robot poses rather than precise poses that
have to be pre-calculated. Compared to a baseline approach the peak signal to
noise ratio is improved from 21.34dB to 23.37dB while the panoptic quality
improves from 56.65% to 70.08%. Furthermore, our approach is faster and can be
tuned to improve inference time by more than a factor of 2 while being memory
efficient with approximately 12 times fewer parameters.";Claus Smitt<author:sep>Michael Halstead<author:sep>Patrick Zimmer<author:sep>Thomas LÃ¤be<author:sep>Esra Guclu<author:sep>Cyrill Stachniss<author:sep>Chris McCool;http://arxiv.org/pdf/2309.05339v1;cs.RO;;nerf
2309.04917v3;http://arxiv.org/abs/2309.04917v3;2023-09-10;Editing 3D Scenes via Text Prompts without Retraining;"Numerous diffusion models have recently been applied to image synthesis and
editing. However, editing 3D scenes is still in its early stages. It poses
various challenges, such as the requirement to design specific methods for
different editing types, retraining new models for various 3D scenes, and the
absence of convenient human interaction during editing. To tackle these issues,
we introduce a text-driven editing method, termed DN2N, which allows for the
direct acquisition of a NeRF model with universal editing capabilities,
eliminating the requirement for retraining. Our method employs off-the-shelf
text-based editing models of 2D images to modify the 3D scene images, followed
by a filtering process to discard poorly edited images that disrupt 3D
consistency. We then consider the remaining inconsistency as a problem of
removing noise perturbation, which can be solved by generating training data
with similar perturbation characteristics for training. We further propose
cross-view regularization terms to help the generalized NeRF model mitigate
these perturbations. Our text-driven method allows users to edit a 3D scene
with their desired description, which is more friendly, intuitive, and
practical than prior works. Empirical results show that our method achieves
multiple editing types, including but not limited to appearance editing,
weather transition, material changing, and style transfer. Most importantly,
our method generalizes well with editing abilities shared among a set of model
parameters without requiring a customized editing model for some specific
scenes, thus inferring novel views with editing effects directly from user
input. The project website is available at https://sk-fun.fun/DN2N";Shuangkang Fang<author:sep>Yufeng Wang<author:sep>Yi Yang<author:sep>Yi-Hsuan Tsai<author:sep>Wenrui Ding<author:sep>Shuchang Zhou<author:sep>Ming-Hsuan Yang;http://arxiv.org/pdf/2309.04917v3;cs.CV;Project Website: https://sk-fun.fun/DN2N;nerf
2309.05028v1;http://arxiv.org/abs/2309.05028v1;2023-09-10;SC-NeRF: Self-Correcting Neural Radiance Field with Sparse Views;"In recent studies, the generalization of neural radiance fields for novel
view synthesis task has been widely explored. However, existing methods are
limited to objects and indoor scenes. In this work, we extend the
generalization task to outdoor scenes, trained only on object-level datasets.
This approach presents two challenges. Firstly, the significant distributional
shift between training and testing scenes leads to black artifacts in rendering
results. Secondly, viewpoint changes in outdoor scenes cause ghosting or
missing regions in rendered images. To address these challenges, we propose a
geometric correction module and an appearance correction module based on
multi-head attention mechanisms. We normalize rendered depth and combine it
with light direction as query in the attention mechanism. Our network
effectively corrects varying scene structures and geometric features in outdoor
scenes, generalizing well from object-level to unseen outdoor scenes.
Additionally, we use appearance correction module to correct appearance
features, preventing rendering artifacts like blank borders and ghosting due to
viewpoint changes. By combining these modules, our approach successfully
tackles the challenges of outdoor scene generalization, producing high-quality
rendering results. When evaluated on four datasets (Blender, DTU, LLFF,
Spaces), our network outperforms previous methods. Notably, compared to
MVSNeRF, our network improves average PSNR from 19.369 to 25.989, SSIM from
0.838 to 0.889, and reduces LPIPS from 0.265 to 0.224 on Spaces outdoor scenes.";Liang Song<author:sep>Guangming Wang<author:sep>Jiuming Liu<author:sep>Zhenyang Fu<author:sep>Yanzi Miao<author:sep> Hesheng;http://arxiv.org/pdf/2309.05028v1;cs.CV;;nerf
2309.04750v1;http://arxiv.org/abs/2309.04750v1;2023-09-09;Mirror-Aware Neural Humans;"Human motion capture either requires multi-camera systems or is unreliable
using single-view input due to depth ambiguities. Meanwhile, mirrors are
readily available in urban environments and form an affordable alternative by
recording two views with only a single camera. However, the mirror setting
poses the additional challenge of handling occlusions of real and mirror image.
Going beyond existing mirror approaches for 3D human pose estimation, we
utilize mirrors for learning a complete body model, including shape and dense
appearance. Our main contributions are extending articulated neural radiance
fields to include a notion of a mirror, making it sample-efficient over
potential occlusion regions. Together, our contributions realize a
consumer-level 3D motion capture system that starts from off-the-shelf 2D poses
by automatically calibrating the camera, estimating mirror orientation, and
subsequently lifting 2D keypoint detections to 3D skeleton pose that is used to
condition the mirror-aware NeRF. We empirically demonstrate the benefit of
learning a body model and accounting for occlusion in challenging mirror
scenes.";Daniel Ajisafe<author:sep>James Tang<author:sep>Shih-Yang Su<author:sep>Bastian Wandt<author:sep>Helge Rhodin;http://arxiv.org/pdf/2309.04750v1;cs.CV;"Project website:
  https://danielajisafe.github.io/mirror-aware-neural-humans/";nerf
2309.04410v1;http://arxiv.org/abs/2309.04410v1;2023-09-08;DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields;"In this paper, we address the challenging problem of 3D toonification, which
involves transferring the style of an artistic domain onto a target 3D face
with stylized geometry and texture. Although fine-tuning a pre-trained 3D GAN
on the artistic domain can produce reasonable performance, this strategy has
limitations in the 3D domain. In particular, fine-tuning can deteriorate the
original GAN latent space, which affects subsequent semantic editing, and
requires independent optimization and storage for each new style, limiting
flexibility and efficient deployment. To overcome these challenges, we propose
DeformToon3D, an effective toonification framework tailored for hierarchical 3D
GAN. Our approach decomposes 3D toonification into subproblems of geometry and
texture stylization to better preserve the original latent space. Specifically,
we devise a novel StyleField that predicts conditional 3D deformation to align
a real-space NeRF to the style space for geometry stylization. Thanks to the
StyleField formulation, which already handles geometry stylization well,
texture stylization can be achieved conveniently via adaptive style mixing that
injects information of the artistic domain into the decoder of the pre-trained
3D GAN. Due to the unique design, our method enables flexible style degree
control and shape-texture-specific style swap. Furthermore, we achieve
efficient training without any real-world 2D-3D training pairs but proxy
samples synthesized from off-the-shelf 2D toonification models.";Junzhe Zhang<author:sep>Yushi Lan<author:sep>Shuai Yang<author:sep>Fangzhou Hong<author:sep>Quan Wang<author:sep>Chai Kiat Yeo<author:sep>Ziwei Liu<author:sep>Chen Change Loy;http://arxiv.org/pdf/2309.04410v1;cs.CV;"ICCV 2023. Code: https://github.com/junzhezhang/DeformToon3D Project
  page: https://www.mmlab-ntu.com/project/deformtoon3d/";nerf
2309.04581v1;http://arxiv.org/abs/2309.04581v1;2023-09-08;Dynamic Mesh-Aware Radiance Fields;"Embedding polygonal mesh assets within photorealistic Neural Radience Fields
(NeRF) volumes, such that they can be rendered and their dynamics simulated in
a physically consistent manner with the NeRF, is under-explored from the system
perspective of integrating NeRF into the traditional graphics pipeline. This
paper designs a two-way coupling between mesh and NeRF during rendering and
simulation. We first review the light transport equations for both mesh and
NeRF, then distill them into an efficient algorithm for updating radiance and
throughput along a cast ray with an arbitrary number of bounces. To resolve the
discrepancy between the linear color space that the path tracer assumes and the
sRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range
(HDR) images. We also present a strategy to estimate light sources and cast
shadows on the NeRF. Finally, we consider how the hybrid surface-volumetric
formulation can be efficiently integrated with a high-performance physics
simulator that supports cloth, rigid and soft bodies. The full rendering and
simulation system can be run on a GPU at interactive rates. We show that a
hybrid system approach outperforms alternatives in visual realism for mesh
insertion, because it allows realistic light transport from volumetric NeRF
media onto surfaces, which affects the appearance of reflective/refractive
surfaces and illumination of diffuse surfaces informed by the dynamic scene.";Yi-Ling Qiao<author:sep>Alexander Gao<author:sep>Yiran Xu<author:sep>Yue Feng<author:sep>Jia-Bin Huang<author:sep>Ming C. Lin;http://arxiv.org/pdf/2309.04581v1;cs.GR;ICCV 2023;nerf
2309.03550v1;http://arxiv.org/abs/2309.03550v1;2023-09-07;Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance  Fields using Geometry-Guided Text-to-Image Diffusion Model;"Recent advances in diffusion models such as ControlNet have enabled
geometrically controllable, high-fidelity text-to-image generation. However,
none of them addresses the question of adding such controllability to
text-to-3D generation. In response, we propose Text2Control3D, a controllable
text-to-3D avatar generation method whose facial expression is controllable
given a monocular video casually captured with hand-held camera. Our main
strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF)
optimized with a set of controlled viewpoint-aware images that we generate from
ControlNet, whose condition input is the depth map extracted from the input
video. When generating the viewpoint-aware images, we utilize cross-reference
attention to inject well-controlled, referential facial expression and
appearance via cross attention. We also conduct low-pass filtering of Gaussian
latent of the diffusion model in order to ameliorate the viewpoint-agnostic
texture problem we observed from our empirical analysis, where the
viewpoint-aware images contain identical textures on identical pixel positions
that are incomprehensible in 3D. Finally, to train NeRF with the images that
are viewpoint-aware yet are not strictly consistent in geometry, our approach
considers per-image geometric variation as a view of deformation from a shared
3D canonical space. Consequently, we construct the 3D avatar in a canonical
space of deformable NeRF by learning a set of per-image deformation via
deformation field table. We demonstrate the empirical results and discuss the
effectiveness of our method.";Sungwon Hwang<author:sep>Junha Hyung<author:sep>Jaegul Choo;http://arxiv.org/pdf/2309.03550v1;cs.CV;Project page: https://text2control3d.github.io/;nerf
2309.03933v1;http://arxiv.org/abs/2309.03933v1;2023-09-07;BluNF: Blueprint Neural Field;"Neural Radiance Fields (NeRFs) have revolutionized scene novel view
synthesis, offering visually realistic, precise, and robust implicit
reconstructions. While recent approaches enable NeRF editing, such as object
removal, 3D shape modification, or material property manipulation, the manual
annotation prior to such edits makes the process tedious. Additionally,
traditional 2D interaction tools lack an accurate sense of 3D space, preventing
precise manipulation and editing of scenes. In this paper, we introduce a novel
approach, called Blueprint Neural Field (BluNF), to address these editing
issues. BluNF provides a robust and user-friendly 2D blueprint, enabling
intuitive scene editing. By leveraging implicit neural representation, BluNF
constructs a blueprint of a scene using prior semantic and depth information.
The generated blueprint allows effortless editing and manipulation of NeRF
representations. We demonstrate BluNF's editability through an intuitive
click-and-change mechanism, enabling 3D manipulations, such as masking,
appearance modification, and object removal. Our approach significantly
contributes to visual content creation, paving the way for further research in
this area.";Robin Courant<author:sep>Xi Wang<author:sep>Marc Christie<author:sep>Vicky Kalogeiton;http://arxiv.org/pdf/2309.03933v1;cs.CV;"ICCV-W (AI3DCC) 2023. Project page with videos and code:
  https://www.lix.polytechnique.fr/vista/projects/2023_iccvw_courant/";nerf
2309.03955v2;http://arxiv.org/abs/2309.03955v2;2023-09-07;SimpleNeRF: Regularizing Sparse Input Neural Radiance Fields with  Simpler Solutions;"Neural Radiance Fields (NeRF) show impressive performance for the
photorealistic free-view rendering of scenes. However, NeRFs require dense
sampling of images in the given scene, and their performance degrades
significantly when only a sparse set of views are available. Researchers have
found that supervising the depth estimated by the NeRF helps train it
effectively with fewer views. The depth supervision is obtained either using
classical approaches or neural networks pre-trained on a large dataset. While
the former may provide only sparse supervision, the latter may suffer from
generalization issues. As opposed to the earlier approaches, we seek to learn
the depth supervision by designing augmented models and training them along
with the NeRF. We design augmented models that encourage simpler solutions by
exploring the role of positional encoding and view-dependent radiance in
training the few-shot NeRF. The depth estimated by these simpler models is used
to supervise the NeRF depth estimates. Since the augmented models can be
inaccurate in certain regions, we design a mechanism to choose only reliable
depth estimates for supervision. Finally, we add a consistency loss between the
coarse and fine multi-layer perceptrons of the NeRF to ensure better
utilization of hierarchical sampling. We achieve state-of-the-art
view-synthesis performance on two popular datasets by employing the above
regularizations. The source code for our model can be found on our project
page: https://nagabhushansn95.github.io/publications/2023/SimpleNeRF.html";Nagabhushan Somraj<author:sep>Adithyan Karanayil<author:sep>Rajiv Soundararajan;http://arxiv.org/pdf/2309.03955v2;cs.CV;SIGGRAPH Asia 2023;nerf
2309.03160v2;http://arxiv.org/abs/2309.03160v2;2023-09-06;ResFields: Residual Neural Fields for Spatiotemporal Signals;"Neural fields, a category of neural networks trained to represent
high-frequency signals, have gained significant attention in recent years due
to their impressive performance in modeling complex 3D data, especially large
neural signed distance (SDFs) or radiance fields (NeRFs) via a single
multi-layer perceptron (MLP). However, despite the power and simplicity of
representing signals with an MLP, these methods still face challenges when
modeling large and complex temporal signals due to the limited capacity of
MLPs. In this paper, we propose an effective approach to address this
limitation by incorporating temporal residual layers into neural fields, dubbed
ResFields, a novel class of networks specifically designed to effectively
represent complex temporal signals. We conduct a comprehensive analysis of the
properties of ResFields and propose a matrix factorization technique to reduce
the number of trainable parameters and enhance generalization capabilities.
Importantly, our formulation seamlessly integrates with existing techniques and
consistently improves results across various challenging tasks: 2D video
approximation, dynamic shape modeling via temporal SDFs, and dynamic NeRF
reconstruction. Lastly, we demonstrate the practical utility of ResFields by
showcasing its effectiveness in capturing dynamic 3D scenes from sparse sensory
inputs of a lightweight capture system.";Marko Mihajlovic<author:sep>Sergey Prokudin<author:sep>Marc Pollefeys<author:sep>Siyu Tang;http://arxiv.org/pdf/2309.03160v2;cs.CV;Project page and code at https://markomih.github.io/ResFields/;nerf
2309.03185v1;http://arxiv.org/abs/2309.03185v1;2023-09-06;Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields;"Neural Radiance Fields (NeRFs) have shown promise in applications like view
synthesis and depth estimation, but learning from multiview images faces
inherent uncertainties. Current methods to quantify them are either heuristic
or computationally demanding. We introduce BayesRays, a post-hoc framework to
evaluate uncertainty in any pre-trained NeRF without modifying the training
process. Our method establishes a volumetric uncertainty field using spatial
perturbations and a Bayesian Laplace approximation. We derive our algorithm
statistically and show its superior performance in key metrics and
applications. Additional results available at: https://bayesrays.github.io.";Lily Goli<author:sep>Cody Reading<author:sep>Silvia SellÃ¡n<author:sep>Alec Jacobson<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2309.03185v1;cs.CV;;nerf
2309.01811v2;http://arxiv.org/abs/2309.01811v2;2023-09-04;Instant Continual Learning of Neural Radiance Fields;"Neural radiance fields (NeRFs) have emerged as an effective method for
novel-view synthesis and 3D scene reconstruction. However, conventional
training methods require access to all training views during scene
optimization. This assumption may be prohibitive in continual learning
scenarios, where new data is acquired in a sequential manner and a continuous
update of the NeRF is desired, as in automotive or remote sensing applications.
When naively trained in such a continual setting, traditional scene
representation frameworks suffer from catastrophic forgetting, where previously
learned knowledge is corrupted after training on new data. Prior works in
alleviating forgetting with NeRFs suffer from low reconstruction quality and
high latency, making them impractical for real-world application. We propose a
continual learning framework for training NeRFs that leverages replay-based
methods combined with a hybrid explicit--implicit scene representation. Our
method outperforms previous methods in reconstruction quality when trained in a
continual setting, while having the additional benefit of being an order of
magnitude faster.";Ryan Po<author:sep>Zhengyang Dong<author:sep>Alexander W. Bergman<author:sep>Gordon Wetzstein;http://arxiv.org/pdf/2309.01811v2;cs.CV;For project page please visit https://ryanpo.com/icngp/;nerf
2309.01351v1;http://arxiv.org/abs/2309.01351v1;2023-09-04;Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF;"Deep neural networks (DNNs) have been proven extremely susceptible to
adversarial examples, which raises special safety-critical concerns for
DNN-based autonomous driving stacks (i.e., 3D object detection). Although there
are extensive works on image-level attacks, most are restricted to 2D pixel
spaces, and such attacks are not always physically realistic in our 3D world.
Here we present Adv3D, the first exploration of modeling adversarial examples
as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic
appearances and 3D accurate generation, yielding a more realistic and
realizable adversarial example. We train our adversarial NeRF by minimizing the
surrounding objects' confidence predicted by 3D detectors on the training set.
Then we evaluate Adv3D on the unseen validation set and show that it can cause
a large performance reduction when rendering NeRF in any sampled pose. To
generate physically realizable adversarial examples, we propose primitive-aware
sampling and semantic-guided regularization that enable 3D patch attacks with
camouflage adversarial texture. Experimental results demonstrate that the
trained adversarial NeRF generalizes well to different poses, scenes, and 3D
detectors. Finally, we provide a defense method to our attacks that involves
adversarial training through data augmentation. Project page:
https://len-li.github.io/adv3d-web";Leheng Li<author:sep>Qing Lian<author:sep>Ying-Cong Chen;http://arxiv.org/pdf/2309.01351v1;cs.CV;;nerf
2309.00277v1;http://arxiv.org/abs/2309.00277v1;2023-09-01;SparseSat-NeRF: Dense Depth Supervised Neural Radiance Fields for Sparse  Satellite Images;"Digital surface model generation using traditional multi-view stereo matching
(MVS) performs poorly over non-Lambertian surfaces, with asynchronous
acquisitions, or at discontinuities. Neural radiance fields (NeRF) offer a new
paradigm for reconstructing surface geometries using continuous volumetric
representation. NeRF is self-supervised, does not require ground truth geometry
for training, and provides an elegant way to include in its representation
physical parameters about the scene, thus potentially remedying the challenging
scenarios where MVS fails. However, NeRF and its variants require many views to
produce convincing scene's geometries which in earth observation satellite
imaging is rare. In this paper we present SparseSat-NeRF (SpS-NeRF) - an
extension of Sat-NeRF adapted to sparse satellite views. SpS-NeRF employs dense
depth supervision guided by crosscorrelation similarity metric provided by
traditional semi-global MVS matching. We demonstrate the effectiveness of our
approach on stereo and tri-stereo Pleiades 1B/WorldView-3 images, and compare
against NeRF and Sat-NeRF. The code is available at
https://github.com/LulinZhang/SpS-NeRF";Lulin Zhang<author:sep>Ewelina Rupnik;http://arxiv.org/pdf/2309.00277v1;cs.CV;ISPRS Annals 2023;nerf
2308.16576v3;http://arxiv.org/abs/2308.16576v3;2023-08-31;GHuNeRF: Generalizable Human NeRF from a Monocular Video;"In this paper, we tackle the challenging task of learning a generalizable
human NeRF model from a monocular video. Although existing generalizable human
NeRFs have achieved impressive results, they require muti-view images or videos
which might not be always available. On the other hand, some works on
free-viewpoint rendering of human from monocular videos cannot be generalized
to unseen identities. In view of these limitations, we propose GHuNeRF to learn
a generalizable human NeRF model from a monocular video of the human performer.
We first introduce a visibility-aware aggregation scheme to compute vertex-wise
features, which is used to construct a 3D feature volume. The feature volume
can only represent the overall geometry of the human performer with
insufficient accuracy due to the limited resolution. To solve this, we further
enhance the volume feature with temporally aligned point-wise features using an
attention mechanism. Finally, the enhanced feature is used for predicting
density and color for each sampled point. A surface-guided sampling strategy is
also adopted to improve the efficiency for both training and inference. We
validate our approach on the widely-used ZJU-MoCap dataset, where we achieve
comparable performance with existing multi-view video based approaches. We also
test on the monocular People-Snapshot dataset and achieve better performance
than existing works when only monocular video is used. Our code is available at
the project website.";Chen Li<author:sep>Jiahao Lin<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2308.16576v3;cs.CV;Add in more baseline for comparison;nerf
2308.16041v1;http://arxiv.org/abs/2308.16041v1;2023-08-30;From Pixels to Portraits: A Comprehensive Survey of Talking Head  Generation Techniques and Applications;"Recent advancements in deep learning and computer vision have led to a surge
of interest in generating realistic talking heads. This paper presents a
comprehensive survey of state-of-the-art methods for talking head generation.
We systematically categorises them into four main approaches: image-driven,
audio-driven, video-driven and others (including neural radiance fields (NeRF),
and 3D-based methods). We provide an in-depth analysis of each method,
highlighting their unique contributions, strengths, and limitations.
Furthermore, we thoroughly compare publicly available models, evaluating them
on key aspects such as inference time and human-rated quality of the generated
outputs. Our aim is to provide a clear and concise overview of the current
landscape in talking head generation, elucidating the relationships between
different approaches and identifying promising directions for future research.
This survey will serve as a valuable reference for researchers and
practitioners interested in this rapidly evolving field.";Shreyank N Gowda<author:sep>Dheeraj Pandey<author:sep>Shashank Narayana Gowda;http://arxiv.org/pdf/2308.16041v1;cs.CV;;nerf
2308.15733v1;http://arxiv.org/abs/2308.15733v1;2023-08-30;Drone-NeRF: Efficient NeRF Based 3D Scene Reconstruction for Large-Scale  Drone Survey;"Neural rendering has garnered substantial attention owing to its capacity for
creating realistic 3D scenes. However, its applicability to extensive scenes
remains challenging, with limitations in effectiveness. In this work, we
propose the Drone-NeRF framework to enhance the efficient reconstruction of
unbounded large-scale scenes suited for drone oblique photography using Neural
Radiance Fields (NeRF). Our approach involves dividing the scene into uniform
sub-blocks based on camera position and depth visibility. Sub-scenes are
trained in parallel using NeRF, then merged for a complete scene. We refine the
model by optimizing camera poses and guiding NeRF with a uniform sampler.
Integrating chosen samples enhances accuracy. A hash-coded fusion MLP
accelerates density representation, yielding RGB and Depth outputs. Our
framework accounts for sub-scene constraints, reduces parallel-training noise,
handles shadow occlusion, and merges sub-regions for a polished rendering
result. This Drone-NeRF framework demonstrates promising capabilities in
addressing challenges related to scene complexity, rendering efficiency, and
accuracy in drone-obtained imagery.";Zhihao Jia<author:sep>Bing Wang<author:sep>Changhao Chen;http://arxiv.org/pdf/2308.15733v1;cs.CV;15 pages, 7 figures, in submission;nerf
2308.15547v1;http://arxiv.org/abs/2308.15547v1;2023-08-29;Efficient Ray Sampling for Radiance Fields Reconstruction;"Accelerating neural radiance fields training is of substantial practical
value, as the ray sampling strategy profoundly impacts network convergence.
More efficient ray sampling can thus directly enhance existing NeRF models'
training efficiency. We therefore propose a novel ray sampling approach for
neural radiance fields that improves training efficiency while retaining
photorealistic rendering results. First, we analyze the relationship between
the pixel loss distribution of sampled rays and rendering quality. This reveals
redundancy in the original NeRF's uniform ray sampling. Guided by this finding,
we develop a sampling method leveraging pixel regions and depth boundaries. Our
main idea is to sample fewer rays in training views, yet with each ray more
informative for scene fitting. Sampling probability increases in pixel areas
exhibiting significant color and depth variation, greatly reducing wasteful
rays from other regions without sacrificing precision. Through this method, not
only can the convergence of the network be accelerated, but the spatial
geometry of a scene can also be perceived more accurately. Rendering outputs
are enhanced, especially for texture-complex regions. Experiments demonstrate
that our method significantly outperforms state-of-the-art techniques on public
benchmark datasets.";Shilei Sun<author:sep>Ming Liu<author:sep>Zhongyi Fan<author:sep>Yuxue Liu<author:sep>Chengwei Lv<author:sep>Liquan Dong<author:sep>Lingqin Kong;http://arxiv.org/pdf/2308.15547v1;cs.CV;15 pages;nerf
2308.15049v1;http://arxiv.org/abs/2308.15049v1;2023-08-29;Pose-Free Neural Radiance Fields via Implicit Pose Regularization;"Pose-free neural radiance fields (NeRF) aim to train NeRF with unposed
multi-view images and it has achieved very impressive success in recent years.
Most existing works share the pipeline of training a coarse pose estimator with
rendered images at first, followed by a joint optimization of estimated poses
and neural radiance field. However, as the pose estimator is trained with only
rendered images, the pose estimation is usually biased or inaccurate for real
images due to the domain gap between real images and rendered images, leading
to poor robustness for the pose estimation of real images and further local
minima in joint optimization. We design IR-NeRF, an innovative pose-free NeRF
that introduces implicit pose regularization to refine pose estimator with
unposed real images and improve the robustness of the pose estimation for real
images. With a collection of 2D images of a specific scene, IR-NeRF constructs
a scene codebook that stores scene features and captures the scene-specific
pose distribution implicitly as priors. Thus, the robustness of pose estimation
can be promoted with the scene priors according to the rationale that a 2D real
image can be well reconstructed from the scene codebook only when its estimated
pose lies within the pose distribution. Extensive experiments show that IR-NeRF
achieves superior novel view synthesis and outperforms the state-of-the-art
consistently across multiple synthetic and real datasets.";Jiahui Zhang<author:sep>Fangneng Zhan<author:sep>Yingchen Yu<author:sep>Kunhao Liu<author:sep>Rongliang Wu<author:sep>Xiaoqin Zhang<author:sep>Ling Shao<author:sep>Shijian Lu;http://arxiv.org/pdf/2308.15049v1;cs.CV;Accepted by ICCV2023;nerf
2308.14383v1;http://arxiv.org/abs/2308.14383v1;2023-08-28;Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a  Light-Weight ToF Sensor;"Light-weight time-of-flight (ToF) depth sensors are compact and
cost-efficient, and thus widely used on mobile devices for tasks such as
autofocus and obstacle detection. However, due to the sparse and noisy depth
measurements, these sensors have rarely been considered for dense geometry
reconstruction. In this work, we present the first dense SLAM system with a
monocular camera and a light-weight ToF sensor. Specifically, we propose a
multi-modal implicit scene representation that supports rendering both the
signals from the RGB camera and light-weight ToF sensor which drives the
optimization by comparing with the raw sensor inputs. Moreover, in order to
guarantee successful pose tracking and reconstruction, we exploit a predicted
depth as an intermediate supervision and develop a coarse-to-fine optimization
strategy for efficient learning of the implicit representation. At last, the
temporal information is explicitly exploited to deal with the noisy signals
from light-weight ToF sensors to improve the accuracy and robustness of the
system. Experiments demonstrate that our system well exploits the signals of
light-weight ToF sensors and achieves competitive results both on camera
tracking and dense scene reconstruction. Project page:
\url{https://zju3dv.github.io/tof_slam/}.";Xinyang Liu<author:sep>Yijin Li<author:sep>Yanbin Teng<author:sep>Hujun Bao<author:sep>Guofeng Zhang<author:sep>Yinda Zhang<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2308.14383v1;cs.CV;"Accepted to ICCV 2023 (Oral). Project Page:
  https://zju3dv.github.io/tof_slam/";
2308.14737v1;http://arxiv.org/abs/2308.14737v1;2023-08-28;Flexible Techniques for Differentiable Rendering with 3D Gaussians;"Fast, reliable shape reconstruction is an essential ingredient in many
computer vision applications. Neural Radiance Fields demonstrated that
photorealistic novel view synthesis is within reach, but was gated by
performance requirements for fast reconstruction of real scenes and objects.
Several recent approaches have built on alternative shape representations, in
particular, 3D Gaussians. We develop extensions to these renderers, such as
integrating differentiable optical flow, exporting watertight meshes and
rendering per-ray normals. Additionally, we show how two of the recent methods
are interoperable with each other. These reconstructions are quick, robust, and
easily performed on GPU or CPU. For code and visual examples, see
https://leonidk.github.io/fmb-plus";Leonid Keselman<author:sep>Martial Hebert;http://arxiv.org/pdf/2308.14737v1;cs.CV;;
2308.14816v1;http://arxiv.org/abs/2308.14816v1;2023-08-28;CLNeRF: Continual Learning Meets NeRF;"Novel view synthesis aims to render unseen views given a set of calibrated
images. In practical applications, the coverage, appearance or geometry of the
scene may change over time, with new images continuously being captured.
Efficiently incorporating such continuous change is an open challenge. Standard
NeRF benchmarks only involve scene coverage expansion. To study other practical
scene changes, we propose a new dataset, World Across Time (WAT), consisting of
scenes that change in appearance and geometry over time. We also propose a
simple yet effective method, CLNeRF, which introduces continual learning (CL)
to Neural Radiance Fields (NeRFs). CLNeRF combines generative replay and the
Instant Neural Graphics Primitives (NGP) architecture to effectively prevent
catastrophic forgetting and efficiently update the model when new data arrives.
We also add trainable appearance and geometry embeddings to NGP, allowing a
single compact model to handle complex scene changes. Without the need to store
historical images, CLNeRF trained sequentially over multiple scans of a
changing scene performs on-par with the upper bound model trained on all scans
at once. Compared to other CL baselines CLNeRF performs much better across
standard benchmarks and WAT. The source code, and the WAT dataset are available
at https://github.com/IntelLabs/CLNeRF. Video presentation is available at:
https://youtu.be/nLRt6OoDGq0?si=8yD6k-8MMBJInQPs";Zhipeng Cai<author:sep>Matthias Mueller;http://arxiv.org/pdf/2308.14816v1;cs.CV;Accepted to ICCV 2023;nerf
2308.14152v1;http://arxiv.org/abs/2308.14152v1;2023-08-27;Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code  Diffusion using Transformers;"Generating 3D images of complex objects conditionally from a few 2D views is
a difficult synthesis problem, compounded by issues such as domain gap and
geometric misalignment. For instance, a unified framework such as Generative
Adversarial Networks cannot achieve this unless they explicitly define both a
domain-invariant and geometric-invariant joint latent distribution, whereas
Neural Radiance Fields are generally unable to handle both issues as they
optimize at the pixel level. By contrast, we propose a simple and novel 2D to
3D synthesis approach based on conditional diffusion with vector-quantized
codes. Operating in an information-rich code space enables high-resolution 3D
synthesis via full-coverage attention across the views. Specifically, we
generate the 3D codes (e.g. for CT images) conditional on previously generated
3D codes and the entire codebook of two 2D views (e.g. 2D X-rays). Qualitative
and quantitative results demonstrate state-of-the-art performance over
specialized methods across varied evaluation criteria, including fidelity
metrics such as density, coverage, and distortion metrics for two complex
volumetric imagery datasets from in real-world scenarios.";Abril Corona-Figueroa<author:sep>Sam Bond-Taylor<author:sep>Neelanjan Bhowmik<author:sep>Yona Falinie A. Gaus<author:sep>Toby P. Breckon<author:sep>Hubert P. H. Shum<author:sep>Chris G. Willcocks;http://arxiv.org/pdf/2308.14152v1;cs.CV;Camera-ready version for ICCV 2023;
2308.14078v2;http://arxiv.org/abs/2308.14078v2;2023-08-27;Sparse3D: Distilling Multiview-Consistent Diffusion for Object  Reconstruction from Sparse Views;"Reconstructing 3D objects from extremely sparse views is a long-standing and
challenging problem. While recent techniques employ image diffusion models for
generating plausible images at novel viewpoints or for distilling pre-trained
diffusion priors into 3D representations using score distillation sampling
(SDS), these methods often struggle to simultaneously achieve high-quality,
consistent, and detailed results for both novel-view synthesis (NVS) and
geometry. In this work, we present Sparse3D, a novel 3D reconstruction method
tailored for sparse view inputs. Our approach distills robust priors from a
multiview-consistent diffusion model to refine a neural radiance field.
Specifically, we employ a controller that harnesses epipolar features from
input views, guiding a pre-trained diffusion model, such as Stable Diffusion,
to produce novel-view images that maintain 3D consistency with the input. By
tapping into 2D priors from powerful image diffusion models, our integrated
model consistently delivers high-quality results, even when faced with
open-world objects. To address the blurriness introduced by conventional SDS,
we introduce the category-score distillation sampling (C-SDS) to enhance
detail. We conduct experiments on CO3DV2 which is a multi-view dataset of
real-world objects. Both quantitative and qualitative evaluations demonstrate
that our approach outperforms previous state-of-the-art works on the metrics
regarding NVS and geometry reconstruction.";Zi-Xin Zou<author:sep>Weihao Cheng<author:sep>Yan-Pei Cao<author:sep>Shi-Sheng Huang<author:sep>Ying Shan<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2308.14078v2;cs.CV;;
2308.13897v1;http://arxiv.org/abs/2308.13897v1;2023-08-26;InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules;"Generalizing Neural Radiance Fields (NeRF) to new scenes is a significant
challenge that existing approaches struggle to address without extensive
modifications to vanilla NeRF framework. We introduce InsertNeRF, a method for
INStilling gEneRalizabiliTy into NeRF. By utilizing multiple plug-and-play
HyperNet modules, InsertNeRF dynamically tailors NeRF's weights to specific
reference scenes, transforming multi-scale sampling-aware features into
scene-specific representations. This novel design allows for more accurate and
efficient representations of complex appearances and geometries. Experiments
show that this method not only achieves superior generalization performance but
also provides a flexible pathway for integration with other NeRF-like systems,
even in sparse input settings. Code will be available
https://github.com/bbbbby-99/InsertNeRF.";Yanqi Bao<author:sep>Tianyu Ding<author:sep>Jing Huo<author:sep>Wenbin Li<author:sep>Yuxin Li<author:sep>Yang Gao;http://arxiv.org/pdf/2308.13897v1;cs.CV;;nerf
2308.13404v1;http://arxiv.org/abs/2308.13404v1;2023-08-25;Relighting Neural Radiance Fields with Shadow and Highlight Hints;"This paper presents a novel neural implicit radiance representation for free
viewpoint relighting from a small set of unstructured photographs of an object
lit by a moving point light source different from the view position. We express
the shape as a signed distance function modeled by a multi layer perceptron. In
contrast to prior relightable implicit neural representations, we do not
disentangle the different reflectance components, but model both the local and
global reflectance at each point by a second multi layer perceptron that, in
addition, to density features, the current position, the normal (from the
signed distace function), view direction, and light position, also takes shadow
and highlight hints to aid the network in modeling the corresponding high
frequency light transport effects. These hints are provided as a suggestion,
and we leave it up to the network to decide how to incorporate these in the
final relit result. We demonstrate and validate our neural implicit
representation on synthetic and real scenes exhibiting a wide variety of
shapes, material properties, and global illumination light transport.";Chong Zeng<author:sep>Guojun Chen<author:sep>Yue Dong<author:sep>Pieter Peers<author:sep>Hongzhi Wu<author:sep>Xin Tong;http://arxiv.org/pdf/2308.13404v1;cs.CV;"Accepted to SIGGRAPH 2023. Author's version. Project page:
  https://nrhints.github.io/";
2309.00014v2;http://arxiv.org/abs/2309.00014v2;2023-08-24;Improving NeRF Quality by Progressive Camera Placement for Unrestricted  Navigation in Complex Environments;"Neural Radiance Fields, or NeRFs, have drastically improved novel view
synthesis and 3D reconstruction for rendering. NeRFs achieve impressive results
on object-centric reconstructions, but the quality of novel view synthesis with
free-viewpoint navigation in complex environments (rooms, houses, etc) is often
problematic. While algorithmic improvements play an important role in the
resulting quality of novel view synthesis, in this work, we show that because
optimizing a NeRF is inherently a data-driven process, good quality data play a
fundamental role in the final quality of the reconstruction. As a consequence,
it is critical to choose the data samples -- in this case the cameras -- in a
way that will eventually allow the optimization to converge to a solution that
allows free-viewpoint navigation with good quality. Our main contribution is an
algorithm that efficiently proposes new camera placements that improve visual
quality with minimal assumptions. Our solution can be used with any NeRF model
and outperforms baselines and similar work.";Georgios Kopanas<author:sep>George Drettakis;http://arxiv.org/pdf/2309.00014v2;cs.CV;;nerf
2308.12560v1;http://arxiv.org/abs/2308.12560v1;2023-08-24;NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects;"We propose a novel-view augmentation (NOVA) strategy to train NeRFs for
photo-realistic 3D composition of dynamic objects in a static scene. Compared
to prior work, our framework significantly reduces blending artifacts when
inserting multiple dynamic objects into a 3D scene at novel views and times;
achieves comparable PSNR without the need for additional ground truth
modalities like optical flow; and overall provides ease, flexibility, and
scalability in neural composition. Our codebase is on GitHub.";Dakshit Agrawal<author:sep>Jiajie Xu<author:sep>Siva Karthik Mustikovela<author:sep>Ioannis Gkioulekas<author:sep>Ashish Shrivastava<author:sep>Yuning Chai;http://arxiv.org/pdf/2308.12560v1;cs.CV;"Accepted for publication in ICCV Computer Vision for Metaverse
  Workshop 2023 (code is available at https://github.com/dakshitagrawal/NoVA)";nerf
2308.12452v2;http://arxiv.org/abs/2308.12452v2;2023-08-23;ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for  3D Scene Stylization;"The radiance fields style transfer is an emerging field that has recently
gained popularity as a means of 3D scene stylization, thanks to the outstanding
performance of neural radiance fields in 3D reconstruction and view synthesis.
We highlight a research gap in radiance fields style transfer, the lack of
sufficient perceptual controllability, motivated by the existing concept in the
2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style
transfer framework offering manageable control over perceptual factors, to
systematically explore the perceptual controllability in 3D scene stylization.
Four distinct types of controls - color preservation control, (style pattern)
scale control, spatial (selective stylization area) control, and depth
enhancement control - are proposed and integrated into this framework. Results
from real-world datasets, both quantitative and qualitative, show that the four
types of controls in our ARF-Plus framework successfully accomplish their
corresponding perceptual controls when stylizing 3D scenes. These techniques
work well for individual style inputs as well as for the simultaneous
application of multiple styles within a scene. This unlocks a realm of
limitless possibilities, allowing customized modifications of stylization
effects and flexible merging of the strengths of different styles, ultimately
enabling the creation of novel and eye-catching stylistic effects on 3D scenes.";Wenzhao Li<author:sep>Tianhao Wu<author:sep>Fangcheng Zhong<author:sep>Cengiz Oztireli;http://arxiv.org/pdf/2308.12452v2;cs.CV;;
2308.11951v3;http://arxiv.org/abs/2308.11951v3;2023-08-23;Pose Modulated Avatars from Video;"It is now possible to reconstruct dynamic human motion and shape from a
sparse set of cameras using Neural Radiance Fields (NeRF) driven by an
underlying skeleton. However, a challenge remains to model the deformation of
cloth and skin in relation to skeleton pose. Unlike existing avatar models that
are learned implicitly or rely on a proxy surface, our approach is motivated by
the observation that different poses necessitate unique frequency assignments.
Neglecting this distinction yields noisy artifacts in smooth areas or blurs
fine-grained texture and shape details in sharp regions. We develop a
two-branch neural network that is adaptive and explicit in the frequency
domain. The first branch is a graph neural network that models correlations
among body parts locally, taking skeleton pose as input. The second branch
combines these correlation features to a set of global frequencies and then
modulates the feature encoding. Our experiments demonstrate that our network
outperforms state-of-the-art methods in terms of preserving details and
generalization capabilities.";Chunjin Song<author:sep>Bastian Wandt<author:sep>Helge Rhodin;http://arxiv.org/pdf/2308.11951v3;cs.CV;;nerf
2308.11974v2;http://arxiv.org/abs/2308.11974v2;2023-08-23;Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields;"Text-driven localized editing of 3D objects is particularly difficult as
locally mixing the original 3D object with the intended new object and style
effects without distorting the object's form is not a straightforward process.
To address this issue, we propose a novel NeRF-based model, Blending-NeRF,
which consists of two NeRF networks: pretrained NeRF and editable NeRF.
Additionally, we introduce new blending operations that allow Blending-NeRF to
properly edit target regions which are localized by text. By using a pretrained
vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects
with varying colors and densities, modify textures, and remove parts of the
original object. Our extensive experiments demonstrate that Blending-NeRF
produces naturally and locally edited 3D objects from various text prompts. Our
project page is available at https://seokhunchoi.github.io/Blending-NeRF/";Hyeonseop Song<author:sep>Seokhun Choi<author:sep>Hoseok Do<author:sep>Chul Lee<author:sep>Taehyeong Kim;http://arxiv.org/pdf/2308.11974v2;cs.CV;"Accepted to ICCV 2023. The first two authors contributed equally to
  this work";nerf
2308.11793v1;http://arxiv.org/abs/2308.11793v1;2023-08-22;Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer  with Mixture-of-View-Experts;"Cross-scene generalizable NeRF models, which can directly synthesize novel
views of unseen scenes, have become a new spotlight of the NeRF field. Several
existing attempts rely on increasingly end-to-end ""neuralized"" architectures,
i.e., replacing scene representation and/or rendering modules with performant
neural networks such as transformers, and turning novel view synthesis into a
feed-forward inference pipeline. While those feedforward ""neuralized""
architectures still do not fit diverse scenes well out of the box, we propose
to bridge them with the powerful Mixture-of-Experts (MoE) idea from large
language models (LLMs), which has demonstrated superior generalization ability
by balancing between larger overall model capacity and flexible per-instance
specialization. Starting from a recent generalizable NeRF architecture called
GNT, we first demonstrate that MoE can be neatly plugged in to enhance the
model. We further customize a shared permanent expert and a geometry-aware
consistency loss to enforce cross-scene consistency and spatial smoothness
respectively, which are essential for generalizable view synthesis. Our
proposed model, dubbed GNT with Mixture-of-View-Experts (GNT-MOVE), has
experimentally shown state-of-the-art results when transferring to unseen
scenes, indicating remarkably better cross-scene generalization in both
zero-shot and few-shot settings. Our codes are available at
https://github.com/VITA-Group/GNT-MOVE.";Wenyan Cong<author:sep>Hanxue Liang<author:sep>Peihao Wang<author:sep>Zhiwen Fan<author:sep>Tianlong Chen<author:sep>Mukund Varma<author:sep>Yi Wang<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2308.11793v1;cs.CV;Accepted by ICCV2023;nerf
2308.11774v1;http://arxiv.org/abs/2308.11774v1;2023-08-22;SAMSNeRF: Segment Anything Model (SAM) Guides Dynamic Surgical Scene  Reconstruction by Neural Radiance Field (NeRF);"The accurate reconstruction of surgical scenes from surgical videos is
critical for various applications, including intraoperative navigation and
image-guided robotic surgery automation. However, previous approaches, mainly
relying on depth estimation, have limited effectiveness in reconstructing
surgical scenes with moving surgical tools. To address this limitation and
provide accurate 3D position prediction for surgical tools in all frames, we
propose a novel approach called SAMSNeRF that combines Segment Anything Model
(SAM) and Neural Radiance Field (NeRF) techniques. Our approach generates
accurate segmentation masks of surgical tools using SAM, which guides the
refinement of the dynamic surgical scene reconstruction by NeRF. Our
experimental results on public endoscopy surgical videos demonstrate that our
approach successfully reconstructs high-fidelity dynamic surgical scenes and
accurately reflects the spatial information of surgical tools. Our proposed
approach can significantly enhance surgical navigation and automation by
providing surgeons with accurate 3D position information of surgical tools
during surgery.The source code will be released soon.";Ange Lou<author:sep>Yamin Li<author:sep>Xing Yao<author:sep>Yike Zhang<author:sep>Jack Noble;http://arxiv.org/pdf/2308.11774v1;cs.CV;;nerf
2308.11198v1;http://arxiv.org/abs/2308.11198v1;2023-08-22;Novel-view Synthesis and Pose Estimation for Hand-Object Interaction  from Sparse Views;"Hand-object interaction understanding and the barely addressed novel view
synthesis are highly desired in the immersive communication, whereas it is
challenging due to the high deformation of hand and heavy occlusions between
hand and object. In this paper, we propose a neural rendering and pose
estimation system for hand-object interaction from sparse views, which can also
enable 3D hand-object interaction editing. We share the inspiration from recent
scene understanding work that shows a scene specific model built beforehand can
significantly improve and unblock vision tasks especially when inputs are
sparse, and extend it to the dynamic hand-object interaction scenario and
propose to solve the problem in two stages. We first learn the shape and
appearance prior knowledge of hands and objects separately with the neural
representation at the offline stage. During the online stage, we design a
rendering-based joint model fitting framework to understand the dynamic
hand-object interaction with the pre-built hand and object models as well as
interaction priors, which thereby overcomes penetration and separation issues
between hand and object and also enables novel view synthesis. In order to get
stable contact during the hand-object interaction process in a sequence, we
propose a stable contact loss to make the contact region to be consistent.
Experiments demonstrate that our method outperforms the state-of-the-art
methods. Code and dataset are available in project webpage
https://iscas3dv.github.io/HO-NeRF.";Wentian Qu<author:sep>Zhaopeng Cui<author:sep>Yinda Zhang<author:sep>Chenyu Meng<author:sep>Cuixia Ma<author:sep>Xiaoming Deng<author:sep>Hongan Wang;http://arxiv.org/pdf/2308.11198v1;cs.CV;;nerf
2308.11130v1;http://arxiv.org/abs/2308.11130v1;2023-08-22;Efficient View Synthesis with Neural Radiance Distribution Field;"Recent work on Neural Radiance Fields (NeRF) has demonstrated significant
advances in high-quality view synthesis. A major limitation of NeRF is its low
rendering efficiency due to the need for multiple network forwardings to render
a single pixel. Existing methods to improve NeRF either reduce the number of
required samples or optimize the implementation to accelerate the network
forwarding. Despite these efforts, the problem of multiple sampling persists
due to the intrinsic representation of radiance fields. In contrast, Neural
Light Fields (NeLF) reduce the computation cost of NeRF by querying only one
single network forwarding per pixel. To achieve a close visual quality to NeRF,
existing NeLF methods require significantly larger network capacities which
limits their rendering efficiency in practice. In this work, we propose a new
representation called Neural Radiance Distribution Field (NeRDF) that targets
efficient view synthesis in real-time. Specifically, we use a small network
similar to NeRF while preserving the rendering speed with a single network
forwarding per pixel as in NeLF. The key is to model the radiance distribution
along each ray with frequency basis and predict frequency weights using the
network. Pixel values are then computed via volume rendering on radiance
distributions. Experiments show that our proposed method offers a better
trade-off among speed, quality, and network size than existing methods: we
achieve a ~254x speed-up over NeRF with similar network size, with only a
marginal performance decline. Our project page is at
yushuang-wu.github.io/NeRDF.";Yushuang Wu<author:sep>Xiao Li<author:sep>Jinglu Wang<author:sep>Xiaoguang Han<author:sep>Shuguang Cui<author:sep>Yan Lu;http://arxiv.org/pdf/2308.11130v1;cs.CV;Accepted by ICCV2023;nerf
2308.10902v2;http://arxiv.org/abs/2308.10902v2;2023-08-21;CamP: Camera Preconditioning for Neural Radiance Fields;"Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D
scene reconstructions of objects and large-scale scenes. However, NeRFs require
accurate camera parameters as input -- inaccurate camera parameters result in
blurry renderings. Extrinsic and intrinsic camera parameters are usually
estimated using Structure-from-Motion (SfM) methods as a pre-processing step to
NeRF, but these techniques rarely yield perfect estimates. Thus, prior works
have proposed jointly optimizing camera parameters alongside a NeRF, but these
methods are prone to local minima in challenging settings. In this work, we
analyze how different camera parameterizations affect this joint optimization
problem, and observe that standard parameterizations exhibit large differences
in magnitude with respect to small perturbations, which can lead to an
ill-conditioned optimization problem. We propose using a proxy problem to
compute a whitening transform that eliminates the correlation between camera
parameters and normalizes their effects, and we propose to use this transform
as a preconditioner for the camera parameters during joint optimization. Our
preconditioned camera optimization significantly improves reconstruction
quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE)
by 67% compared to state-of-the-art NeRF approaches that do not optimize for
cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint
optimization approaches using the camera parameterization of SCNeRF. Our
approach is easy to implement, does not significantly increase runtime, can be
applied to a wide variety of camera parameterizations, and can
straightforwardly be incorporated into other NeRF-like models.";Keunhong Park<author:sep>Philipp Henzler<author:sep>Ben Mildenhall<author:sep>Jonathan T. Barron<author:sep>Ricardo Martin-Brualla;http://arxiv.org/pdf/2308.10902v2;cs.CV;SIGGRAPH Asia 2023, Project page: https://camp-nerf.github.io;nerf
2308.10337v1;http://arxiv.org/abs/2308.10337v1;2023-08-20;Strata-NeRF : Neural Radiance Fields for Stratified Scenes;"Neural Radiance Field (NeRF) approaches learn the underlying 3D
representation of a scene and generate photo-realistic novel views with high
fidelity. However, most proposed settings concentrate on modelling a single
object or a single level of a scene. However, in the real world, we may capture
a scene at multiple levels, resulting in a layered capture. For example,
tourists usually capture a monument's exterior structure before capturing the
inner structure. Modelling such scenes in 3D with seamless switching between
levels can drastically improve immersive experiences. However, most existing
techniques struggle in modelling such scenes. We propose Strata-NeRF, a single
neural radiance field that implicitly captures a scene with multiple levels.
Strata-NeRF achieves this by conditioning the NeRFs on Vector Quantized (VQ)
latent representations which allow sudden changes in scene structure. We
evaluate the effectiveness of our approach in multi-layered synthetic dataset
comprising diverse scenes and then further validate its generalization on the
real-world RealEstate10K dataset. We find that Strata-NeRF effectively captures
stratified scenes, minimizes artifacts, and synthesizes high-fidelity views
compared to existing approaches.";Ankit Dhiman<author:sep>Srinath R<author:sep>Harsh Rangwani<author:sep>Rishubh Parihar<author:sep>Lokesh R Boregowda<author:sep>Srinath Sridhar<author:sep>R Venkatesh Babu;http://arxiv.org/pdf/2308.10337v1;cs.CV;ICCV 2023, Project Page: https://ankitatiisc.github.io/Strata-NeRF/;nerf
2308.09894v1;http://arxiv.org/abs/2308.09894v1;2023-08-19;Semantic-Human: Neural Rendering of Humans from Monocular Video with  Human Parsing;"The neural rendering of humans is a topic of great research significance.
However, previous works mostly focus on achieving photorealistic details,
neglecting the exploration of human parsing. Additionally, classical semantic
work are all limited in their ability to efficiently represent fine results in
complex motions. Human parsing is inherently related to radiance
reconstruction, as similar appearance and geometry often correspond to similar
semantic part. Furthermore, previous works often design a motion field that
maps from the observation space to the canonical space, while it tends to
exhibit either underfitting or overfitting, resulting in limited
generalization. In this paper, we present Semantic-Human, a novel method that
achieves both photorealistic details and viewpoint-consistent human parsing for
the neural rendering of humans. Specifically, we extend neural radiance fields
(NeRF) to jointly encode semantics, appearance and geometry to achieve accurate
2D semantic labels using noisy pseudo-label supervision. Leveraging the
inherent consistency and smoothness properties of NeRF, Semantic-Human achieves
consistent human parsing in both continuous and novel views. We also introduce
constraints derived from the SMPL surface for the motion field and
regularization for the recovered volumetric geometry. We have evaluated the
model using the ZJU-MoCap dataset, and the obtained highly competitive results
demonstrate the effectiveness of our proposed Semantic-Human. We also showcase
various compelling applications, including label denoising, label synthesis and
image editing, and empirically validate its advantageous properties.";Jie Zhang<author:sep>Pengcheng Shi<author:sep>Zaiwang Gu<author:sep>Yiyang Zhou<author:sep>Zhi Wang;http://arxiv.org/pdf/2308.09894v1;cs.CV;;nerf
2308.10122v1;http://arxiv.org/abs/2308.10122v1;2023-08-19;HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision  Mitigation;"Neural radiance fields (NeRF) have garnered significant attention, with
recent works such as Instant-NGP accelerating NeRF training and evaluation
through a combination of hashgrid-based positional encoding and neural
networks. However, effectively leveraging the spatial sparsity of 3D scenes
remains a challenge. To cull away unnecessary regions of the feature grid,
existing solutions rely on prior knowledge of object shape or periodically
estimate object shape during training by repeated model evaluations, which are
costly and wasteful.
  To address this issue, we propose HollowNeRF, a novel compression solution
for hashgrid-based NeRF which automatically sparsifies the feature grid during
the training phase. Instead of directly compressing dense features, HollowNeRF
trains a coarse 3D saliency mask that guides efficient feature pruning, and
employs an alternating direction method of multipliers (ADMM) pruner to
sparsify the 3D saliency mask during training. By exploiting the sparsity in
the 3D scene to redistribute hash collisions, HollowNeRF improves rendering
quality while using a fraction of the parameters of comparable state-of-the-art
solutions, leading to a better cost-accuracy trade-off. Our method delivers
comparable rendering quality to Instant-NGP, while utilizing just 31% of the
parameters. In addition, our solution can achieve a PSNR accuracy gain of up to
1dB using only 56% of the parameters.";Xiufeng Xie<author:sep>Riccardo Gherardi<author:sep>Zhihong Pan<author:sep>Stephen Huang;http://arxiv.org/pdf/2308.10122v1;cs.CV;Accepted to ICCV 2023;nerf
2308.10001v1;http://arxiv.org/abs/2308.10001v1;2023-08-19;AltNeRF: Learning Robust Neural Radiance Field via Alternating  Depth-Pose Optimization;"Neural Radiance Fields (NeRF) have shown promise in generating realistic
novel views from sparse scene images. However, existing NeRF approaches often
encounter challenges due to the lack of explicit 3D supervision and imprecise
camera poses, resulting in suboptimal outcomes. To tackle these issues, we
propose AltNeRF -- a novel framework designed to create resilient NeRF
representations using self-supervised monocular depth estimation (SMDE) from
monocular videos, without relying on known camera poses. SMDE in AltNeRF
masterfully learns depth and pose priors to regulate NeRF training. The depth
prior enriches NeRF's capacity for precise scene geometry depiction, while the
pose prior provides a robust starting point for subsequent pose refinement.
Moreover, we introduce an alternating algorithm that harmoniously melds NeRF
outputs into SMDE through a consistence-driven mechanism, thus enhancing the
integrity of depth priors. This alternation empowers AltNeRF to progressively
refine NeRF representations, yielding the synthesis of realistic novel views.
Additionally, we curate a distinctive dataset comprising indoor videos captured
via mobile devices. Extensive experiments showcase the compelling capabilities
of AltNeRF in generating high-fidelity and robust novel views that closely
resemble reality.";Kun Wang<author:sep>Zhiqiang Yan<author:sep>Huang Tian<author:sep>Zhenyu Zhang<author:sep>Xiang Li<author:sep>Jun Li<author:sep>Jian Yang;http://arxiv.org/pdf/2308.10001v1;cs.CV;;nerf
2308.09386v1;http://arxiv.org/abs/2308.09386v1;2023-08-18;DReg-NeRF: Deep Registration for Neural Radiance Fields;"Although Neural Radiance Fields (NeRF) is popular in the computer vision
community recently, registering multiple NeRFs has yet to gain much attention.
Unlike the existing work, NeRF2NeRF, which is based on traditional optimization
methods and needs human annotated keypoints, we propose DReg-NeRF to solve the
NeRF registration problem on object-centric scenes without human intervention.
After training NeRF models, our DReg-NeRF first extracts features from the
occupancy grid in NeRF. Subsequently, our DReg-NeRF utilizes a transformer
architecture with self-attention and cross-attention layers to learn the
relations between pairwise NeRF blocks. In contrast to state-of-the-art (SOTA)
point cloud registration methods, the decoupled correspondences are supervised
by surface fields without any ground truth overlapping labels. We construct a
novel view synthesis dataset with 1,700+ 3D objects obtained from Objaverse to
train our network. When evaluated on the test set, our proposed method beats
the SOTA point cloud registration methods by a large margin, with a mean
$\text{RPE}=9.67^{\circ}$ and a mean $\text{RTE}=0.038$.
  Our code is available at https://github.com/AIBluefisher/DReg-NeRF.";Yu Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2308.09386v1;cs.CV;Accepted at ICCV 2023;nerf
2308.09421v2;http://arxiv.org/abs/2308.09421v2;2023-08-18;MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection;"In the field of monocular 3D detection, it is common practice to utilize
scene geometric clues to enhance the detector's performance. However, many
existing works adopt these clues explicitly such as estimating a depth map and
back-projecting it into 3D space. This explicit methodology induces sparsity in
3D representations due to the increased dimensionality from 2D to 3D, and leads
to substantial information loss, especially for distant and occluded objects.
To alleviate this issue, we propose MonoNeRD, a novel detection framework that
can infer dense 3D geometry and occupancy. Specifically, we model scenes with
Signed Distance Functions (SDF), facilitating the production of dense 3D
representations. We treat these representations as Neural Radiance Fields
(NeRF) and then employ volume rendering to recover RGB images and depth maps.
To the best of our knowledge, this work is the first to introduce volume
rendering for M3D, and demonstrates the potential of implicit reconstruction
for image-based 3D perception. Extensive experiments conducted on the KITTI-3D
benchmark and Waymo Open Dataset demonstrate the effectiveness of MonoNeRD.
Codes are available at https://github.com/cskkxjk/MonoNeRD.";Junkai Xu<author:sep>Liang Peng<author:sep>Haoran Cheng<author:sep>Hao Li<author:sep>Wei Qian<author:sep>Ke Li<author:sep>Wenxiao Wang<author:sep>Deng Cai;http://arxiv.org/pdf/2308.09421v2;cs.CV;Accepted by ICCV 2023;nerf
2308.08854v1;http://arxiv.org/abs/2308.08854v1;2023-08-17;Language-enhanced RNR-Map: Querying Renderable Neural Radiance Field  maps with natural language;"We present Le-RNR-Map, a Language-enhanced Renderable Neural Radiance map for
Visual Navigation with natural language query prompts. The recently proposed
RNR-Map employs a grid structure comprising latent codes positioned at each
pixel. These latent codes, which are derived from image observation, enable: i)
image rendering given a camera pose, since they are converted to Neural
Radiance Field; ii) image navigation and localization with astonishing
accuracy. On top of this, we enhance RNR-Map with CLIP-based embedding latent
codes, allowing natural language search without additional label data. We
evaluate the effectiveness of this map in single and multi-object searches. We
also investigate its compatibility with a Large Language Model as an
""affordance query resolver"". Code and videos are available at
https://intelligolabs.github.io/Le-RNR-Map/";Francesco Taioli<author:sep>Federico Cunico<author:sep>Federico Girella<author:sep>Riccardo Bologna<author:sep>Alessandro Farinelli<author:sep>Marco Cristani;http://arxiv.org/pdf/2308.08854v1;cs.CV;Accepted at ICCVW23 VLAR;
2308.08947v1;http://arxiv.org/abs/2308.08947v1;2023-08-17;Watch Your Steps: Local Image and Scene Editing by Text Instructions;"Denoising diffusion models have enabled high-quality image generation and
editing. We present a method to localize the desired edit region implicit in a
text instruction. We leverage InstructPix2Pix (IP2P) and identify the
discrepancy between IP2P predictions with and without the instruction. This
discrepancy is referred to as the relevance map. The relevance map conveys the
importance of changing each pixel to achieve the edits, and is used to to guide
the modifications. This guidance ensures that the irrelevant pixels remain
unchanged. Relevance maps are further used to enhance the quality of
text-guided editing of 3D scenes in the form of neural radiance fields. A field
is trained on relevance maps of training views, denoted as the relevance field,
defining the 3D region within which modifications should be made. We perform
iterative updates on the training views guided by rendered relevance maps from
the relevance field. Our method achieves state-of-the-art performance on both
image and NeRF editing tasks. Project page:
https://ashmrz.github.io/WatchYourSteps/";Ashkan Mirzaei<author:sep>Tristan Aumentado-Armstrong<author:sep>Marcus A. Brubaker<author:sep>Jonathan Kelly<author:sep>Alex Levinshtein<author:sep>Konstantinos G. Derpanis<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2308.08947v1;cs.CV;Project page: https://ashmrz.github.io/WatchYourSteps/;nerf
2308.08258v1;http://arxiv.org/abs/2308.08258v1;2023-08-16;SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes;"Existing methods for the 4D reconstruction of general, non-rigidly deforming
objects focus on novel-view synthesis and neglect correspondences. However,
time consistency enables advanced downstream tasks like 3D editing, motion
analysis, or virtual-asset creation. We propose SceNeRFlow to reconstruct a
general, non-rigid scene in a time-consistent manner. Our dynamic-NeRF method
takes multi-view RGB videos and background images from static cameras with
known camera parameters as input. It then reconstructs the deformations of an
estimated canonical model of the geometry and appearance in an online fashion.
Since this canonical model is time-invariant, we obtain correspondences even
for long-term, long-range motions. We employ neural scene representations to
parametrize the components of our method. Like prior dynamic-NeRF methods, we
use a backwards deformation model. We find non-trivial adaptations of this
model necessary to handle larger motions: We decompose the deformations into a
strongly regularized coarse component and a weakly regularized fine component,
where the coarse component also extends the deformation field into the space
surrounding the object, which enables tracking over time. We show
experimentally that, unlike prior work that only handles small motion, our
method enables the reconstruction of studio-scale motions.";Edith Tretschk<author:sep>Vladislav Golyanik<author:sep>Michael Zollhoefer<author:sep>Aljaz Bozic<author:sep>Christoph Lassner<author:sep>Christian Theobalt;http://arxiv.org/pdf/2308.08258v1;cs.CV;Project page: https://vcai.mpi-inf.mpg.de/projects/scenerflow/;nerf
2308.08530v3;http://arxiv.org/abs/2308.08530v3;2023-08-16;Ref-DVGO: Reflection-Aware Direct Voxel Grid Optimization for an  Improved Quality-Efficiency Trade-Off in Reflective Scene Reconstruction;"Neural Radiance Fields (NeRFs) have revolutionized the field of novel view
synthesis, demonstrating remarkable performance. However, the modeling and
rendering of reflective objects remain challenging problems. Recent methods
have shown significant improvements over the baselines in handling reflective
scenes, albeit at the expense of efficiency. In this work, we aim to strike a
balance between efficiency and quality. To this end, we investigate an
implicit-explicit approach based on conventional volume rendering to enhance
the reconstruction quality and accelerate the training and rendering processes.
We adopt an efficient density-based grid representation and reparameterize the
reflected radiance in our pipeline. Our proposed reflection-aware approach
achieves a competitive quality efficiency trade-off compared to competing
methods. Based on our experimental results, we propose and discuss hypotheses
regarding the factors influencing the results of density-based methods for
reconstructing reflective objects. The source code is available at
https://github.com/gkouros/ref-dvgo.";Georgios Kouros<author:sep>Minye Wu<author:sep>Shubham Shrivastava<author:sep>Sushruth Nagesh<author:sep>Punarjay Chakravarty<author:sep>Tinne Tuytelaars;http://arxiv.org/pdf/2308.08530v3;cs.CV;5 pages, 4 figures, 3 tables, ICCV TRICKY 2023 Workshop;nerf
2308.07118v2;http://arxiv.org/abs/2308.07118v2;2023-08-14;Neural radiance fields in the industrial and robotics domain:  applications, research opportunities and use cases;"The proliferation of technologies, such as extended reality (XR), has
increased the demand for high-quality three-dimensional (3D) graphical
representations. Industrial 3D applications encompass computer-aided design
(CAD), finite element analysis (FEA), scanning, and robotics. However, current
methods employed for industrial 3D representations suffer from high
implementation costs and reliance on manual human input for accurate 3D
modeling. To address these challenges, neural radiance fields (NeRFs) have
emerged as a promising approach for learning 3D scene representations based on
provided training 2D images. Despite a growing interest in NeRFs, their
potential applications in various industrial subdomains are still unexplored.
In this paper, we deliver a comprehensive examination of NeRF industrial
applications while also providing direction for future research endeavors. We
also present a series of proof-of-concept experiments that demonstrate the
potential of NeRFs in the industrial domain. These experiments include
NeRF-based video compression techniques and using NeRFs for 3D motion
estimation in the context of collision avoidance. In the video compression
experiment, our results show compression savings up to 48\% and 74\% for
resolutions of 1920x1080 and 300x168, respectively. The motion estimation
experiment used a 3D animation of a robotic arm to train Dynamic-NeRF (D-NeRF)
and achieved an average peak signal-to-noise ratio (PSNR) of disparity map with
the value of 23 dB and an structural similarity index measure (SSIM) 0.97.";Eugen Å lapak<author:sep>Enric Pardo<author:sep>MatÃºÅ¡ Dopiriak<author:sep>Taras Maksymyuk<author:sep>Juraj Gazda;http://arxiv.org/pdf/2308.07118v2;cs.RO;;nerf
2308.07032v1;http://arxiv.org/abs/2308.07032v1;2023-08-14;S3IM: Stochastic Structural SIMilarity and Its Unreasonable  Effectiveness for Neural Fields;"Recently, Neural Radiance Field (NeRF) has shown great success in rendering
novel-view images of a given scene by learning an implicit representation with
only posed RGB images. NeRF and relevant neural field methods (e.g., neural
surface representation) typically optimize a point-wise loss and make
point-wise predictions, where one data point corresponds to one pixel.
Unfortunately, this line of research failed to use the collective supervision
of distant pixels, although it is known that pixels in an image or scene can
provide rich structural information. To the best of our knowledge, we are the
first to design a nonlocal multiplex training paradigm for NeRF and relevant
neural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss
that processes multiple data points as a whole set instead of process multiple
inputs independently. Our extensive experiments demonstrate the unreasonable
effectiveness of S3IM in improving NeRF and neural surface representation for
nearly free. The improvements of quality metrics can be particularly
significant for those relatively difficult tasks: e.g., the test MSE loss
unexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view
synthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance
reduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is
consistently robust even with sparse inputs, corrupted images, and dynamic
scenes.";Zeke Xie<author:sep>Xindi Yang<author:sep>Yujie Yang<author:sep>Qi Sun<author:sep>Yixiang Jiang<author:sep>Haoran Wang<author:sep>Yunfeng Cai<author:sep>Mingming Sun;http://arxiv.org/pdf/2308.07032v1;cs.CV;"ICCV 2023 main conference. Code: https://github.com/Madaoer/S3IM. 14
  pages, 5 figures, 17 tables";nerf
2308.05970v1;http://arxiv.org/abs/2308.05970v1;2023-08-11;Focused Specific Objects NeRF;"Most NeRF-based models are designed for learning the entire scene, and
complex scenes can lead to longer learning times and poorer rendering effects.
This paper utilizes scene semantic priors to make improvements in fast
training, allowing the network to focus on the specific targets and not be
affected by complex backgrounds. The training speed can be increased by 7.78
times with better rendering effect, and small to medium sized targets can be
rendered faster. In addition, this improvement applies to all NeRF-based
models. Considering the inherent multi-view consistency and smoothness of NeRF,
this paper also studies weak supervision by sparsely sampling negative ray
samples. With this method, training can be further accelerated and rendering
quality can be maintained. Finally, this paper extends pixel semantic and color
rendering formulas and proposes a new scene editing technique that can achieve
unique displays of the specific semantic targets or masking them in rendering.
To address the problem of unsupervised regions incorrect inferences in the
scene, we also designed a self-supervised loop that combines morphological
operations and clustering.";Yuesong Li<author:sep>Feng Pan<author:sep>Helong Yan<author:sep>Xiuli Xin<author:sep>Xiaoxue Feng;http://arxiv.org/pdf/2308.05970v1;cs.CV;17 pages,32 figures;nerf
2308.05939v1;http://arxiv.org/abs/2308.05939v1;2023-08-11;VERF: Runtime Monitoring of Pose Estimation with Neural Radiance Fields;"We present VERF, a collection of two methods (VERF-PnP and VERF-Light) for
providing runtime assurance on the correctness of a camera pose estimate of a
monocular camera without relying on direct depth measurements. We leverage the
ability of NeRF (Neural Radiance Fields) to render novel RGB perspectives of a
scene. We only require as input the camera image whose pose is being estimated,
an estimate of the camera pose we want to monitor, and a NeRF model containing
the scene pictured by the camera. We can then predict if the pose estimate is
within a desired distance from the ground truth and justify our prediction with
a level of confidence. VERF-Light does this by rendering a viewpoint with NeRF
at the estimated pose and estimating its relative offset to the sensor image up
to scale. Since scene scale is unknown, the approach renders another auxiliary
image and reasons over the consistency of the optical flows across the three
images. VERF-PnP takes a different approach by rendering a stereo pair of
images with NeRF and utilizing the Perspective-n-Point (PnP) algorithm. We
evaluate both methods on the LLFF dataset, on data from a Unitree A1 quadruped
robot, and on data collected from Blue Origin's sub-orbital New Shepard rocket
to demonstrate the effectiveness of the proposed pose monitoring method across
a range of scene scales. We also show monitoring can be completed in under half
a second on a 3090 GPU.";Dominic Maggio<author:sep>Courtney Mario<author:sep>Luca Carlone;http://arxiv.org/pdf/2308.05939v1;cs.RO;;nerf
2308.04669v4;http://arxiv.org/abs/2308.04669v4;2023-08-09;A General Implicit Framework for Fast NeRF Composition and Rendering;"A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.";Xinyu Gao<author:sep>Ziyi Yang<author:sep>Yunlu Zhao<author:sep>Yuxiang Sun<author:sep>Xiaogang Jin<author:sep>Changqing Zou;http://arxiv.org/pdf/2308.04669v4;cs.CV;AAAI 2024;nerf
2308.04826v2;http://arxiv.org/abs/2308.04826v2;2023-08-09;WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields;"Neural Radiance Field (NeRF) has shown impressive performance in novel view
synthesis via implicit scene representation. However, it usually suffers from
poor scalability as requiring densely sampled images for each new scene.
Several studies have attempted to mitigate this problem by integrating
Multi-View Stereo (MVS) technique into NeRF while they still entail a
cumbersome fine-tuning process for new scenes. Notably, the rendering quality
will drop severely without this fine-tuning process and the errors mainly
appear around the high-frequency features. In the light of this observation, we
design WaveNeRF, which integrates wavelet frequency decomposition into MVS and
NeRF to achieve generalizable yet high-quality synthesis without any per-scene
optimization. To preserve high-frequency information when generating 3D feature
volumes, WaveNeRF builds Multi-View Stereo in the Wavelet domain by integrating
the discrete wavelet transform into the classical cascade MVS, which
disentangles high-frequency information explicitly. With that, disentangled
frequency features can be injected into classic NeRF via a novel hybrid neural
renderer to yield faithful high-frequency details, and an intuitive
frequency-guided sampling strategy can be designed to suppress artifacts around
high-frequency regions. Extensive experiments over three widely studied
benchmarks show that WaveNeRF achieves superior generalizable radiance field
modeling when only given three images as input.";Muyu Xu<author:sep>Fangneng Zhan<author:sep>Jiahui Zhang<author:sep>Yingchen Yu<author:sep>Xiaoqin Zhang<author:sep>Christian Theobalt<author:sep>Ling Shao<author:sep>Shijian Lu;http://arxiv.org/pdf/2308.04826v2;cs.CV;"Accepted to ICCV 2023. Project website:
  https://mxuai.github.io/WaveNeRF/";nerf
2308.04413v1;http://arxiv.org/abs/2308.04413v1;2023-08-08;Digging into Depth Priors for Outdoor Neural Radiance Fields;"Neural Radiance Fields (NeRF) have demonstrated impressive performance in
vision and graphics tasks, such as novel view synthesis and immersive reality.
However, the shape-radiance ambiguity of radiance fields remains a challenge,
especially in the sparse viewpoints setting. Recent work resorts to integrating
depth priors into outdoor NeRF training to alleviate the issue. However, the
criteria for selecting depth priors and the relative merits of different priors
have not been thoroughly investigated. Moreover, the relative merits of
selecting different approaches to use the depth priors is also an unexplored
problem. In this paper, we provide a comprehensive study and evaluation of
employing depth priors to outdoor neural radiance fields, covering common depth
sensing technologies and most application ways. Specifically, we conduct
extensive experiments with two representative NeRF methods equipped with four
commonly-used depth priors and different depth usages on two widely used
outdoor datasets. Our experimental results reveal several interesting findings
that can potentially benefit practitioners and researchers in training their
NeRF models with depth priors. Project Page:
https://cwchenwang.github.io/outdoor-nerf-depth";Chen Wang<author:sep>Jiadai Sun<author:sep>Lina Liu<author:sep>Chenming Wu<author:sep>Zhelun Shen<author:sep>Dayan Wu<author:sep>Yuchao Dai<author:sep>Liangjun Zhang;http://arxiv.org/pdf/2308.04413v1;cs.CV;"Accepted to ACM MM 2023. Project Page:
  https://cwchenwang.github.io/outdoor-nerf-depth";nerf
2308.04079v1;http://arxiv.org/abs/2308.04079v1;2023-08-08;3D Gaussian Splatting for Real-Time Radiance Field Rendering;"Radiance Field methods have recently revolutionized novel-view synthesis of
scenes captured with multiple photos or videos. However, achieving high visual
quality still requires neural networks that are costly to train and render,
while recent faster methods inevitably trade off speed for quality. For
unbounded and complete scenes (rather than isolated objects) and 1080p
resolution rendering, no current method can achieve real-time display rates. We
introduce three key elements that allow us to achieve state-of-the-art visual
quality while maintaining competitive training times and importantly allow
high-quality real-time (>= 30 fps) novel-view synthesis at 1080p resolution.
First, starting from sparse points produced during camera calibration, we
represent the scene with 3D Gaussians that preserve desirable properties of
continuous volumetric radiance fields for scene optimization while avoiding
unnecessary computation in empty space; Second, we perform interleaved
optimization/density control of the 3D Gaussians, notably optimizing
anisotropic covariance to achieve an accurate representation of the scene;
Third, we develop a fast visibility-aware rendering algorithm that supports
anisotropic splatting and both accelerates training and allows realtime
rendering. We demonstrate state-of-the-art visual quality and real-time
rendering on several established datasets.";Bernhard Kerbl<author:sep>Georgios Kopanas<author:sep>Thomas LeimkÃ¼hler<author:sep>George Drettakis;http://arxiv.org/pdf/2308.04079v1;cs.GR;https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/;gaussian splatting
2308.03280v1;http://arxiv.org/abs/2308.03280v1;2023-08-07;Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with  Whitted-Style Ray Tracing;"Recently, Neural Radiance Fields (NeRF) has exhibited significant success in
novel view synthesis, surface reconstruction, etc. However, since no physical
reflection is considered in its rendering pipeline, NeRF mistakes the
reflection in the mirror as a separate virtual scene, leading to the inaccurate
reconstruction of the mirror and multi-view inconsistent reflections in the
mirror. In this paper, we present a novel neural rendering framework, named
Mirror-NeRF, which is able to learn accurate geometry and reflection of the
mirror and support various scene manipulation applications with mirrors, such
as adding new objects or mirrors into the scene and synthesizing the
reflections of these new objects in mirrors, controlling mirror roughness, etc.
To achieve this goal, we propose a unified radiance field by introducing the
reflection probability and tracing rays following the light transport model of
Whitted Ray Tracing, and also develop several techniques to facilitate the
learning process. Experiments and comparisons on both synthetic and real
datasets demonstrate the superiority of our method. The code and supplementary
material are available on the project webpage:
https://zju3dv.github.io/Mirror-NeRF/.";Junyi Zeng<author:sep>Chong Bao<author:sep>Rui Chen<author:sep>Zilong Dong<author:sep>Guofeng Zhang<author:sep>Hujun Bao<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2308.03280v1;cs.CV;"Accepted to ACM Multimedia 2023. Project Page:
  https://zju3dv.github.io/Mirror-NeRF/";nerf
2308.02908v1;http://arxiv.org/abs/2308.02908v1;2023-08-05;Where and How: Mitigating Confusion in Neural Radiance Fields from  Sparse Inputs;"Neural Radiance Fields from Sparse input} (NeRF-S) have shown great potential
in synthesizing novel views with a limited number of observed viewpoints.
However, due to the inherent limitations of sparse inputs and the gap between
non-adjacent views, rendering results often suffer from over-fitting and foggy
surfaces, a phenomenon we refer to as ""CONFUSION"" during volume rendering. In
this paper, we analyze the root cause of this confusion and attribute it to two
fundamental questions: ""WHERE"" and ""HOW"". To this end, we present a novel
learning framework, WaH-NeRF, which effectively mitigates confusion by tackling
the following challenges: (i)""WHERE"" to Sample? in NeRF-S -- we introduce a
Deformable Sampling strategy and a Weight-based Mutual Information Loss to
address sample-position confusion arising from the limited number of
viewpoints; and (ii) ""HOW"" to Predict? in NeRF-S -- we propose a
Semi-Supervised NeRF learning Paradigm based on pose perturbation and a
Pixel-Patch Correspondence Loss to alleviate prediction confusion caused by the
disparity between training and testing viewpoints. By integrating our proposed
modules and loss functions, WaH-NeRF outperforms previous methods under the
NeRF-S setting. Code is available https://github.com/bbbbby-99/WaH-NeRF.";Yanqi Bao<author:sep>Yuxin Li<author:sep>Jing Huo<author:sep>Tianyu Ding<author:sep>Xinyue Liang<author:sep>Wenbin Li<author:sep>Yang Gao;http://arxiv.org/pdf/2308.02908v1;cs.CV;"Accepted In Proceedings of the 31st ACM International Conference on
  Multimedia (MM' 23)";nerf
2308.02840v1;http://arxiv.org/abs/2308.02840v1;2023-08-05;Learning Unified Decompositional and Compositional NeRF for Editable  Novel View Synthesis;"Implicit neural representations have shown powerful capacity in modeling
real-world 3D scenes, offering superior performance in novel view synthesis. In
this paper, we target a more challenging scenario, i.e., joint scene novel view
synthesis and editing based on implicit neural scene representations.
State-of-the-art methods in this direction typically consider building separate
networks for these two tasks (i.e., view synthesis and editing). Thus, the
modeling of interactions and correlations between these two tasks is very
limited, which, however, is critical for learning high-quality scene
representations. To tackle this problem, in this paper, we propose a unified
Neural Radiance Field (NeRF) framework to effectively perform joint scene
decomposition and composition for modeling real-world scenes. The decomposition
aims at learning disentangled 3D representations of different objects and the
background, allowing for scene editing, while scene composition models an
entire scene representation for novel view synthesis. Specifically, with a
two-stage NeRF framework, we learn a coarse stage for predicting a global
radiance field as guidance for point sampling, and in the second fine-grained
stage, we perform scene decomposition by a novel one-hot object radiance field
regularization module and a pseudo supervision via inpainting to handle
ambiguous background regions occluded by objects. The decomposed object-level
radiance fields are further composed by using activations from the
decomposition module. Extensive quantitative and qualitative results show the
effectiveness of our method for scene decomposition and composition,
outperforming state-of-the-art methods for both novel-view synthesis and
editing tasks.";Yuxin Wang<author:sep>Wayne Wu<author:sep>Dan Xu;http://arxiv.org/pdf/2308.02840v1;cs.CV;ICCV2023, Project Page: https://w-ted.github.io/publications/udc-nerf;nerf
2308.02751v2;http://arxiv.org/abs/2308.02751v2;2023-08-05;NeRFs: The Search for the Best 3D Representation;"Neural Radiance Fields or NeRFs have become the representation of choice for
problems in view synthesis or image-based rendering, as well as in many other
applications across computer graphics and vision, and beyond. At their core,
NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of
meshes, disparity maps, multiplane images or even voxel grids, they represent
the scene as a continuous volume, with volumetric parameters like
view-dependent radiance and volume density obtained by querying a neural
network. The NeRF representation has now been widely used, with thousands of
papers extending or building on it every year, multiple authors and websites
providing overviews and surveys, and numerous industrial applications and
startup companies. In this article, we briefly review the NeRF representation,
and describe the three decades-long quest to find the best 3D representation
for view synthesis and related problems, culminating in the NeRF papers. We
then describe new developments in terms of NeRF representations and make some
observations and insights regarding the future of 3D representations.";Ravi Ramamoorthi;http://arxiv.org/pdf/2308.02751v2;cs.CV;"Updated based on feedback in-person and via e-mail at SIGGRAPH 2023.
  In particular, I have added references and discussion of seminal SIGGRAPH
  image-based rendering papers, and better put the recent Kerbl et al. work in
  context, with more references";nerf
2308.02191v1;http://arxiv.org/abs/2308.02191v1;2023-08-04;ES-MVSNet: Efficient Framework for End-to-end Self-supervised Multi-View  Stereo;"Compared to the multi-stage self-supervised multi-view stereo (MVS) method,
the end-to-end (E2E) approach has received more attention due to its concise
and efficient training pipeline. Recent E2E self-supervised MVS approaches have
integrated third-party models (such as optical flow models, semantic
segmentation models, NeRF models, etc.) to provide additional consistency
constraints, which grows GPU memory consumption and complicates the model's
structure and training pipeline. In this work, we propose an efficient
framework for end-to-end self-supervised MVS, dubbed ES-MVSNet. To alleviate
the high memory consumption of current E2E self-supervised MVS frameworks, we
present a memory-efficient architecture that reduces memory usage by 43%
without compromising model performance. Furthermore, with the novel design of
asymmetric view selection policy and region-aware depth consistency, we achieve
state-of-the-art performance among E2E self-supervised MVS methods, without
relying on third-party models for additional consistency signals. Extensive
experiments on DTU and Tanks&Temples benchmarks demonstrate that the proposed
ES-MVSNet approach achieves state-of-the-art performance among E2E
self-supervised MVS methods and competitive performance to many supervised and
multi-stage self-supervised methods.";Qiang Zhou<author:sep>Chaohui Yu<author:sep>Jingliang Li<author:sep>Yuang Liu<author:sep>Jing Wang<author:sep>Zhibin Wang;http://arxiv.org/pdf/2308.02191v1;cs.CV;arXiv admin note: text overlap with arXiv:2203.03949 by other authors;nerf
2308.01262v2;http://arxiv.org/abs/2308.01262v2;2023-08-02;Incorporating Season and Solar Specificity into Renderings made by a  NeRF Architecture using Satellite Images;"As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar
angle into account in a NeRF-based framework for rendering a scene from a novel
viewpoint using satellite images for training. Our work extends those
contributions and shows how one can make the renderings season-specific. Our
main challenge was creating a Neural Radiance Field (NeRF) that could render
seasonal features independently of viewing angle and solar angle while still
being able to render shadows. We teach our network to render seasonal features
by introducing one more input variable -- time of the year. However, the small
training datasets typical of satellite imagery can introduce ambiguities in
cases where shadows are present in the same location for every image of a
particular season. We add additional terms to the loss function to discourage
the network from using seasonal features for accounting for shadows. We show
the performance of our network on eight Areas of Interest containing images
captured by the Maxar WorldView-3 satellite. This evaluation includes tests
measuring the ability of our framework to accurately render novel views,
generate height maps, predict shadows, and specify seasonal features
independently from shadows. Our ablation studies justify the choices made for
network design parameters.";Michael Gableman<author:sep>Avinash Kak;http://arxiv.org/pdf/2308.01262v2;cs.CV;18 pages, 17 figures, 10 tables;nerf
2308.00214v2;http://arxiv.org/abs/2308.00214v2;2023-08-01;Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned  Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF);"Many tasks performed in image-guided, mini-invasive, medical procedures can
be cast as pose estimation problems, where an X-ray projection is utilized to
reach a target in 3D space. Expanding on recent advances in the differentiable
rendering of optically reflective materials, we introduce new methods for pose
estimation of radiolucent objects using X-ray projections, and we demonstrate
the critical role of optimal view synthesis in performing this task. We first
develop an algorithm (DiffDRR) that efficiently computes Digitally
Reconstructed Radiographs (DRRs) and leverages automatic differentiation within
TensorFlow. Pose estimation is performed by iterative gradient descent using a
loss function that quantifies the similarity of the DRR synthesized from a
randomly initialized pose and the true fluoroscopic image at the target pose.
We propose two novel methods for high-fidelity view synthesis, Neural Tuned
Tomography (NeTT) and masked Neural Radiance Fields (mNeRF). Both methods rely
on classic Cone-Beam Computerized Tomography (CBCT); NeTT directly optimizes
the CBCT densities, while the non-zero values of mNeRF are constrained by a 3D
mask of the anatomic region segmented from CBCT. We demonstrate that both NeTT
and mNeRF distinctly improve pose estimation within our framework. By defining
a successful pose estimate to be a 3D angle error of less than 3 deg, we find
that NeTT and mNeRF can achieve similar results, both with overall success
rates more than 93%. However, the computational cost of NeTT is significantly
lower than mNeRF in both training and pose estimation. Furthermore, we show
that a NeTT trained for a single subject can generalize to synthesize
high-fidelity DRRs and ensure robust pose estimations for all other subjects.
Therefore, we suggest that NeTT is an attractive option for robust pose
estimation using fluoroscopic projections.";Chaochao Zhou<author:sep>Syed Hasib Akhter Faruqui<author:sep>Abhinav Patel<author:sep>Ramez N. Abdalla<author:sep>Michael C. Hurley<author:sep>Ali Shaibani<author:sep>Matthew B. Potts<author:sep>Babak S. Jahromi<author:sep>Leon Cho<author:sep>Sameer A. Ansari<author:sep>Donald R. Cantrell;http://arxiv.org/pdf/2308.00214v2;cs.CV;;nerf
2308.00462v3;http://arxiv.org/abs/2308.00462v3;2023-08-01;Context-Aware Talking-Head Video Editing;"Talking-head video editing aims to efficiently insert, delete, and substitute
the word of a pre-recorded video through a text transcript editor. The key
challenge for this task is obtaining an editing model that generates new
talking-head video clips which simultaneously have accurate lip synchronization
and motion smoothness. Previous approaches, including 3DMM-based (3D Morphable
Model) methods and NeRF-based (Neural Radiance Field) methods, are sub-optimal
in that they either require minutes of source videos and days of training time
or lack the disentangled control of verbal (e.g., lip motion) and non-verbal
(e.g., head pose and expression) representations for video clip insertion. In
this work, we fully utilize the video context to design a novel framework for
talking-head video editing, which achieves efficiency, disentangled motion
control, and sequential smoothness. Specifically, we decompose this framework
to motion prediction and motion-conditioned rendering: (1) We first design an
animation prediction module that efficiently obtains smooth and lip-sync motion
sequences conditioned on the driven speech. This module adopts a
non-autoregressive network to obtain context prior and improve the prediction
efficiency, and it learns a speech-animation mapping prior with better
generalization to novel speech from a multi-identity video dataset. (2) We then
introduce a neural rendering module to synthesize the photo-realistic and
full-head video frames given the predicted motion sequence. This module adopts
a pre-trained head topology and uses only few frames for efficient fine-tuning
to obtain a person-specific rendering model. Extensive experiments demonstrate
that our method efficiently achieves smoother editing results with higher image
quality and lip accuracy using less data than previous methods.";Songlin Yang<author:sep>Wei Wang<author:sep>Jun Ling<author:sep>Bo Peng<author:sep>Xu Tan<author:sep>Jing Dong;http://arxiv.org/pdf/2308.00462v3;cs.MM;The version of this paper needs to be further improved;nerf
2308.00773v3;http://arxiv.org/abs/2308.00773v3;2023-08-01;High-Fidelity Eye Animatable Neural Radiance Fields for Human Face;"Face rendering using neural radiance fields (NeRF) is a rapidly developing
research area in computer vision. While recent methods primarily focus on
controlling facial attributes such as identity and expression, they often
overlook the crucial aspect of modeling eyeball rotation, which holds
importance for various downstream tasks. In this paper, we aim to learn a face
NeRF model that is sensitive to eye movements from multi-view images. We
address two key challenges in eye-aware face NeRF learning: how to effectively
capture eyeball rotation for training and how to construct a manifold for
representing eyeball rotation. To accomplish this, we first fit FLAME, a
well-established parametric face model, to the multi-view images considering
multi-view consistency. Subsequently, we introduce a new Dynamic Eye-aware NeRF
(DeNeRF). DeNeRF transforms 3D points from different views into a canonical
space to learn a unified face NeRF model. We design an eye deformation field
for the transformation, including rigid transformation, e.g., eyeball rotation,
and non-rigid transformation. Through experiments conducted on the ETH-XGaze
dataset, we demonstrate that our model is capable of generating high-fidelity
images with accurate eyeball rotation and non-rigid periocular deformation,
even under novel viewing angles. Furthermore, we show that utilizing the
rendered images can effectively enhance gaze estimation performance.";Hengfei Wang<author:sep>Zhongqun Zhang<author:sep>Yihua Cheng<author:sep>Hyung Jin Chang;http://arxiv.org/pdf/2308.00773v3;cs.CV;BMVC2023 Oral;nerf
2307.15333v1;http://arxiv.org/abs/2307.15333v1;2023-07-28;Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF;"The explicit neural radiance field (NeRF) has gained considerable interest
for its efficient training and fast inference capabilities, making it a
promising direction such as virtual reality and gaming. In particular,
PlenOctree (POT)[1], an explicit hierarchical multi-scale octree
representation, has emerged as a structural and influential framework. However,
POT's fixed structure for direct optimization is sub-optimal as the scene
complexity evolves continuously with updates to cached color and density,
necessitating refining the sampling distribution to capture signal complexity
accordingly. To address this issue, we propose the dynamic PlenOctree DOT,
which adaptively refines the sample distribution to adjust to changing scene
complexity. Specifically, DOT proposes a concise yet novel hierarchical feature
fusion strategy during the iterative rendering process. Firstly, it identifies
the regions of interest through training signals to ensure adaptive and
efficient refinement. Next, rather than directly filtering out valueless nodes,
DOT introduces the sampling and pruning operations for octrees to aggregate
features, enabling rapid parameter learning. Compared with POT, our DOT
outperforms it by enhancing visual quality, reducing over $55.15$/$68.84\%$
parameters, and providing 1.7/1.9 times FPS for NeRF-synthetic and Tanks $\&$
Temples, respectively. Project homepage:https://vlislab22.github.io/DOT.
  [1] Yu, Alex, et al. ""Plenoctrees for real-time rendering of neural radiance
fields."" Proceedings of the IEEE/CVF International Conference on Computer
Vision. 2021.";Haotian Bai<author:sep>Yiqi Lin<author:sep>Yize Chen<author:sep>Lin Wang;http://arxiv.org/pdf/2307.15333v1;cs.CV;Accepted by ICCV2023;nerf
2307.15131v2;http://arxiv.org/abs/2307.15131v2;2023-07-27;Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields;"With the popularity of implicit neural representations, or neural radiance
fields (NeRF), there is a pressing need for editing methods to interact with
the implicit 3D models for tasks like post-processing reconstructed scenes and
3D content creation. While previous works have explored NeRF editing from
various perspectives, they are restricted in editing flexibility, quality, and
speed, failing to offer direct editing response and instant preview. The key
challenge is to conceive a locally editable neural representation that can
directly reflect the editing instructions and update instantly. To bridge the
gap, we propose a new interactive editing method and system for implicit
representations, called Seal-3D, which allows users to edit NeRF models in a
pixel-level and free manner with a wide range of NeRF-like backbone and preview
the editing effects instantly. To achieve the effects, the challenges are
addressed by our proposed proxy function mapping the editing instructions to
the original space of NeRF models in the teacher model and a two-stage training
strategy for the student model with local pretraining and global finetuning. A
NeRF editing system is built to showcase various editing types. Our system can
achieve compelling editing effects with an interactive speed of about 1 second.";Xiangyu Wang<author:sep>Jingsen Zhu<author:sep>Qi Ye<author:sep>Yuchi Huo<author:sep>Yunlong Ran<author:sep>Zhihua Zhong<author:sep>Jiming Chen;http://arxiv.org/pdf/2307.15131v2;cs.CV;"Accepted by ICCV2023. Project Page:
  https://windingwind.github.io/seal-3d/ Code:
  https://github.com/windingwind/seal-3d/";nerf
2307.15058v1;http://arxiv.org/abs/2307.15058v1;2023-07-27;MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous  Driving;"Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is
widely recognized that realistic sensor simulation will play a critical role in
solving remaining corner cases by simulating them. To this end, we propose an
autonomous driving simulator based upon neural radiance fields (NeRFs).
Compared with existing works, ours has three notable features: (1)
Instance-aware. Our simulator models the foreground instances and background
environments separately with independent networks so that the static (e.g.,
size and appearance) and dynamic (e.g., trajectory) properties of instances can
be controlled separately. (2) Modular. Our simulator allows flexible switching
between different modern NeRF-related backbones, sampling strategies, input
modalities, etc. We expect this modular design to boost academic progress and
industrial deployment of NeRF-based autonomous driving simulation. (3)
Realistic. Our simulator set new state-of-the-art photo-realism results given
the best module selection. Our simulator will be open-sourced while most of our
counterparts are not. Project page: https://open-air-sun.github.io/mars/.";Zirui Wu<author:sep>Tianyu Liu<author:sep>Liyi Luo<author:sep>Zhide Zhong<author:sep>Jianteng Chen<author:sep>Hongmin Xiao<author:sep>Chao Hou<author:sep>Haozhe Lou<author:sep>Yuantao Chen<author:sep>Runyi Yang<author:sep>Yuxin Huang<author:sep>Xiaoyu Ye<author:sep>Zike Yan<author:sep>Yongliang Shi<author:sep>Yiyi Liao<author:sep>Hao Zhao;http://arxiv.org/pdf/2307.15058v1;cs.CV;"CICAI 2023, project page with code:
  https://open-air-sun.github.io/mars/";nerf
2307.14620v1;http://arxiv.org/abs/2307.14620v1;2023-07-27;NeRF-Det: Learning Geometry-Aware Volumetric Representation for  Multi-View 3D Object Detection;"We present NeRF-Det, a novel method for indoor 3D detection with posed RGB
images as input. Unlike existing indoor 3D detection methods that struggle to
model scene geometry, our method makes novel use of NeRF in an end-to-end
manner to explicitly estimate 3D geometry, thereby improving 3D detection
performance. Specifically, to avoid the significant extra latency associated
with per-scene optimization of NeRF, we introduce sufficient geometry priors to
enhance the generalizability of NeRF-MLP. Furthermore, we subtly connect the
detection and NeRF branches through a shared MLP, enabling an efficient
adaptation of NeRF to detection and yielding geometry-aware volumetric
representations for 3D detection. Our method outperforms state-of-the-arts by
3.9 mAP and 3.1 mAP on the ScanNet and ARKITScenes benchmarks, respectively. We
provide extensive analysis to shed light on how NeRF-Det works. As a result of
our joint-training design, NeRF-Det is able to generalize well to unseen scenes
for object detection, view synthesis, and depth estimation tasks without
requiring per-scene optimization. Code is available at
\url{https://github.com/facebookresearch/NeRF-Det}.";Chenfeng Xu<author:sep>Bichen Wu<author:sep>Ji Hou<author:sep>Sam Tsai<author:sep>Ruilong Li<author:sep>Jialiang Wang<author:sep>Wei Zhan<author:sep>Zijian He<author:sep>Peter Vajda<author:sep>Kurt Keutzer<author:sep>Masayoshi Tomizuka;http://arxiv.org/pdf/2307.14620v1;cs.CV;Accepted by ICCV 2023;nerf
2307.14981v2;http://arxiv.org/abs/2307.14981v2;2023-07-27;MapNeRF: Incorporating Map Priors into Neural Radiance Fields for  Driving View Simulation;"Simulating camera sensors is a crucial task in autonomous driving. Although
neural radiance fields are exceptional at synthesizing photorealistic views in
driving simulations, they still fail to generate extrapolated views. This paper
proposes to incorporate map priors into neural radiance fields to synthesize
out-of-trajectory driving views with semantic road consistency. The key insight
is that map information can be utilized as a prior to guiding the training of
the radiance fields with uncertainty. Specifically, we utilize the coarse
ground surface as uncertain information to supervise the density field and warp
depth with uncertainty from unknown camera poses to ensure multi-view
consistency. Experimental results demonstrate that our approach can produce
semantic consistency in deviated views for vehicle camera simulation. The
supplementary video can be viewed at https://youtu.be/jEQWr-Rfh3A.";Chenming Wu<author:sep>Jiadai Sun<author:sep>Zhelun Shen<author:sep>Liangjun Zhang;http://arxiv.org/pdf/2307.14981v2;cs.CV;"Accepted by IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2023";nerf
2308.03772v1;http://arxiv.org/abs/2308.03772v1;2023-07-27;Improved Neural Radiance Fields Using Pseudo-depth and Fusion;"Since the advent of Neural Radiance Fields, novel view synthesis has received
tremendous attention. The existing approach for the generalization of radiance
field reconstruction primarily constructs an encoding volume from nearby source
images as additional inputs. However, these approaches cannot efficiently
encode the geometric information of real scenes with various scale
objects/structures. In this work, we propose constructing multi-scale encoding
volumes and providing multi-scale geometry information to NeRF models. To make
the constructed volumes as close as possible to the surfaces of objects in the
scene and the rendered depth more accurate, we propose to perform depth
prediction and radiance field reconstruction simultaneously. The predicted
depth map will be used to supervise the rendered depth, narrow the depth range,
and guide points sampling. Finally, the geometric information contained in
point volume features may be inaccurate due to occlusion, lighting, etc. To
this end, we propose enhancing the point volume feature from depth-guided
neighbor feature fusion. Experiments demonstrate the superior performance of
our method in both novel view synthesis and dense geometry modeling without
per-scene optimization.";Jingliang Li<author:sep>Qiang Zhou<author:sep>Chaohui Yu<author:sep>Zhengda Lu<author:sep>Jun Xiao<author:sep>Zhibin Wang<author:sep>Fan Wang;http://arxiv.org/pdf/2308.03772v1;cs.CV;;nerf
2307.13908v1;http://arxiv.org/abs/2307.13908v1;2023-07-26;Points-to-3D: Bridging the Gap between Sparse Points and  Shape-Controllable Text-to-3D Generation;"Text-to-3D generation has recently garnered significant attention, fueled by
2D diffusion models trained on billions of image-text pairs. Existing methods
primarily rely on score distillation to leverage the 2D diffusion priors to
supervise the generation of 3D models, e.g., NeRF. However, score distillation
is prone to suffer the view inconsistency problem, and implicit NeRF modeling
can also lead to an arbitrary shape, thus leading to less realistic and
uncontrollable 3D generation. In this work, we propose a flexible framework of
Points-to-3D to bridge the gap between sparse yet freely available 3D points
and realistic shape-controllable 3D generation by distilling the knowledge from
both 2D and 3D diffusion models. The core idea of Points-to-3D is to introduce
controllable sparse 3D points to guide the text-to-3D generation. Specifically,
we use the sparse point cloud generated from the 3D diffusion model, Point-E,
as the geometric prior, conditioned on a single reference image. To better
utilize the sparse 3D points, we propose an efficient point cloud guidance loss
to adaptively drive the NeRF's geometry to align with the shape of the sparse
3D points. In addition to controlling the geometry, we propose to optimize the
NeRF for a more view-consistent appearance. To be specific, we perform score
distillation to the publicly available 2D image diffusion model ControlNet,
conditioned on text as well as depth map of the learned compact geometry.
Qualitative and quantitative comparisons demonstrate that Points-to-3D improves
view consistency and achieves good shape controllability for text-to-3D
generation. Points-to-3D provides users with a new way to improve and control
text-to-3D generation.";Chaohui Yu<author:sep>Qiang Zhou<author:sep>Jingliang Li<author:sep>Zhe Zhang<author:sep>Zhibin Wang<author:sep>Fan Wang;http://arxiv.org/pdf/2307.13908v1;cs.CV;Accepted by ACMMM 2023;nerf
2307.12909v1;http://arxiv.org/abs/2307.12909v1;2023-07-24;Dyn-E: Local Appearance Editing of Dynamic Neural Radiance Fields;"Recently, the editing of neural radiance fields (NeRFs) has gained
considerable attention, but most prior works focus on static scenes while
research on the appearance editing of dynamic scenes is relatively lacking. In
this paper, we propose a novel framework to edit the local appearance of
dynamic NeRFs by manipulating pixels in a single frame of training video.
Specifically, to locally edit the appearance of dynamic NeRFs while preserving
unedited regions, we introduce a local surface representation of the edited
region, which can be inserted into and rendered along with the original NeRF
and warped to arbitrary other frames through a learned invertible motion
representation network. By employing our method, users without professional
expertise can easily add desired content to the appearance of a dynamic scene.
We extensively evaluate our approach on various scenes and show that our
approach achieves spatially and temporally consistent editing results. Notably,
our approach is versatile and applicable to different variants of dynamic NeRF
representations.";Shangzhan Zhang<author:sep>Sida Peng<author:sep>Yinji ShenTu<author:sep>Qing Shuai<author:sep>Tianrun Chen<author:sep>Kaicheng Yu<author:sep>Hujun Bao<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2307.12909v1;cs.CV;project page: https://dyn-e.github.io/;nerf
2307.12718v1;http://arxiv.org/abs/2307.12718v1;2023-07-24;CarPatch: A Synthetic Benchmark for Radiance Field Evaluation on Vehicle  Components;"Neural Radiance Fields (NeRFs) have gained widespread recognition as a highly
effective technique for representing 3D reconstructions of objects and scenes
derived from sets of images. Despite their efficiency, NeRF models can pose
challenges in certain scenarios such as vehicle inspection, where the lack of
sufficient data or the presence of challenging elements (e.g. reflections)
strongly impact the accuracy of the reconstruction. To this aim, we introduce
CarPatch, a novel synthetic benchmark of vehicles. In addition to a set of
images annotated with their intrinsic and extrinsic camera parameters, the
corresponding depth maps and semantic segmentation masks have been generated
for each view. Global and part-based metrics have been defined and used to
evaluate, compare, and better characterize some state-of-the-art techniques.
The dataset is publicly released at
https://aimagelab.ing.unimore.it/go/carpatch and can be used as an evaluation
guide and as a baseline for future work on this challenging topic.";Davide Di Nucci<author:sep>Alessandro Simoni<author:sep>Matteo Tomei<author:sep>Luca Ciuffreda<author:sep>Roberto Vezzani<author:sep>Rita Cucchiara;http://arxiv.org/pdf/2307.12718v1;cs.CV;Accepted at ICIAP2023;nerf
2307.12291v1;http://arxiv.org/abs/2307.12291v1;2023-07-23;TransHuman: A Transformer-based Human Representation for Generalizable  Neural Human Rendering;"In this paper, we focus on the task of generalizable neural human rendering
which trains conditional Neural Radiance Fields (NeRF) from multi-view videos
of different characters. To handle the dynamic human motion, previous methods
have primarily used a SparseConvNet (SPC)-based human representation to process
the painted SMPL. However, such SPC-based representation i) optimizes under the
volatile observation space which leads to the pose-misalignment between
training and inference stages, and ii) lacks the global relationships among
human parts that is critical for handling the incomplete painted SMPL. Tackling
these issues, we present a brand-new framework named TransHuman, which learns
the painted SMPL under the canonical space and captures the global
relationships between human parts with transformers. Specifically, TransHuman
is mainly composed of Transformer-based Human Encoding (TransHE), Deformable
Partial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI).
TransHE first processes the painted SMPL under the canonical space via
transformers for capturing the global relationships between human parts. Then,
DPaRF binds each output token with a deformable radiance field for encoding the
query point under the observation space. Finally, the FDI is employed to
further integrate fine-grained information from reference images. Extensive
experiments on ZJU-MoCap and H36M show that our TransHuman achieves a
significantly new state-of-the-art performance with high efficiency. Project
page: https://pansanity666.github.io/TransHuman/";Xiao Pan<author:sep>Zongxin Yang<author:sep>Jianxin Ma<author:sep>Chang Zhou<author:sep>Yi Yang;http://arxiv.org/pdf/2307.12291v1;cs.CV;Accepted by ICCV 2023;nerf
2307.11418v3;http://arxiv.org/abs/2307.11418v3;2023-07-21;FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural  Radiance Fields;"As recent advances in Neural Radiance Fields (NeRF) have enabled
high-fidelity 3D face reconstruction and novel view synthesis, its manipulation
also became an essential task in 3D vision. However, existing manipulation
methods require extensive human labor, such as a user-provided semantic mask
and manual attribute search unsuitable for non-expert users. Instead, our
approach is designed to require a single text to manipulate a face
reconstructed with NeRF. To do so, we first train a scene manipulator, a latent
code-conditional deformable NeRF, over a dynamic scene to control a face
deformation using the latent code. However, representing a scene deformation
with a single latent code is unfavorable for compositing local deformations
observed in different instances. As so, our proposed Position-conditional
Anchor Compositor (PAC) learns to represent a manipulated scene with spatially
varying latent codes. Their renderings with the scene manipulator are then
optimized to yield high cosine similarity to a target text in CLIP embedding
space for text-driven manipulation. To the best of our knowledge, our approach
is the first to address the text-driven manipulation of a face reconstructed
with NeRF. Extensive results, comparisons, and ablation studies demonstrate the
effectiveness of our approach.";Sungwon Hwang<author:sep>Junha Hyung<author:sep>Daejin Kim<author:sep>Min-Jung Kim<author:sep>Jaegul Choo;http://arxiv.org/pdf/2307.11418v3;cs.CV;ICCV 2023 project page at https://faceclipnerf.github.io;nerf
2307.11526v2;http://arxiv.org/abs/2307.11526v2;2023-07-21;CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields;"Neural Radiance Fields (NeRF) have the potential to be a major representation
of media. Since training a NeRF has never been an easy task, the protection of
its model copyright should be a priority. In this paper, by analyzing the pros
and cons of possible copyright protection solutions, we propose to protect the
copyright of NeRF models by replacing the original color representation in NeRF
with a watermarked color representation. Then, a distortion-resistant rendering
scheme is designed to guarantee robust message extraction in 2D renderings of
NeRF. Our proposed method can directly protect the copyright of NeRF models
while maintaining high rendering quality and bit accuracy when compared among
optional solutions.";Ziyuan Luo<author:sep>Qing Guo<author:sep>Ka Chun Cheung<author:sep>Simon See<author:sep>Renjie Wan;http://arxiv.org/pdf/2307.11526v2;cs.CV;11 pages, 6 figures, accepted by ICCV 2023 non-camera-ready version;nerf
2307.11335v1;http://arxiv.org/abs/2307.11335v1;2023-07-21;Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural  Radiance Fields;"Despite the tremendous progress in neural radiance fields (NeRF), we still
face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF
presents fine-detailed and anti-aliased renderings but takes days for training,
while Instant-ngp can accomplish the reconstruction in a few minutes but
suffers from blurring or aliasing when rendering at various distances or
resolutions due to ignoring the sampling area. To this end, we propose a novel
Tri-Mip encoding that enables both instant reconstruction and anti-aliased
high-fidelity rendering for neural radiance fields. The key is to factorize the
pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can
efficiently perform 3D area sampling by taking advantage of 2D pre-filtered
feature maps, which significantly elevates the rendering quality without
sacrificing efficiency. To cope with the novel Tri-Mip representation, we
propose a cone-casting rendering technique to efficiently sample anti-aliased
3D features with the Tri-Mip encoding considering both pixel imaging and
observing distance. Extensive experiments on both synthetic and real-world
datasets demonstrate our method achieves state-of-the-art rendering quality and
reconstruction speed while maintaining a compact representation that reduces
25% model size compared against Instant-ngp.";Wenbo Hu<author:sep>Yuling Wang<author:sep>Lin Ma<author:sep>Bangbang Yang<author:sep>Lin Gao<author:sep>Xiao Liu<author:sep>Yuewen Ma;http://arxiv.org/pdf/2307.11335v1;cs.CV;"Accepted to ICCV 2023 Project page:
  https://wbhu.github.io/projects/Tri-MipRF";nerf
2307.10664v1;http://arxiv.org/abs/2307.10664v1;2023-07-20;Lighting up NeRF via Unsupervised Decomposition and Enhancement;"Neural Radiance Field (NeRF) is a promising approach for synthesizing novel
views, given a set of images and the corresponding camera poses of a scene.
However, images photographed from a low-light scene can hardly be used to train
a NeRF model to produce high-quality results, due to their low pixel
intensities, heavy noise, and color distortion. Combining existing low-light
image enhancement methods with NeRF methods also does not work well due to the
view inconsistency caused by the individual 2D enhancement process. In this
paper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), to
enhance the scene representation and synthesize normal-light novel views
directly from sRGB low-light images in an unsupervised manner. The core of our
approach is a decomposition of radiance field learning, which allows us to
enhance the illumination, reduce noise and correct the distorted colors jointly
with the NeRF optimization process. Our method is able to produce novel view
images with proper lighting and vivid colors and details, given a collection of
camera-finished low dynamic range (8-bits/channel) images from a low-light
scene. Experiments demonstrate that our method outperforms existing low-light
enhancement methods and NeRF methods.";Haoyuan Wang<author:sep>Xiaogang Xu<author:sep>Ke Xu<author:sep>Rynson WH. Lau;http://arxiv.org/pdf/2307.10664v1;cs.CV;ICCV 2023. Project website: https://whyy.site/paper/llnerf;nerf
2307.10776v1;http://arxiv.org/abs/2307.10776v1;2023-07-20;Urban Radiance Field Representation with Deformable Neural Mesh  Primitives;"Neural Radiance Fields (NeRFs) have achieved great success in the past few
years. However, most current methods still require intensive resources due to
ray marching-based rendering. To construct urban-level radiance fields
efficiently, we design Deformable Neural Mesh Primitive~(DNMP), and propose to
parameterize the entire scene with such primitives. The DNMP is a flexible and
compact neural variant of classic mesh representation, which enjoys both the
efficiency of rasterization-based rendering and the powerful neural
representation capability for photo-realistic image synthesis. Specifically, a
DNMP consists of a set of connected deformable mesh vertices with paired vertex
features to parameterize the geometry and radiance information of a local area.
To constrain the degree of freedom for optimization and lower the storage
budgets, we enforce the shape of each primitive to be decoded from a relatively
low-dimensional latent space. The rendering colors are decoded from the vertex
features (interpolated with rasterization) by a view-dependent MLP. The DNMP
provides a new paradigm for urban-level scene representation with appealing
properties: $(1)$ High-quality rendering. Our method achieves leading
performance for novel view synthesis in urban scenarios. $(2)$ Low
computational costs. Our representation enables fast rendering (2.07ms/1k
pixels) and low peak memory usage (110MB/1k pixels). We also present a
lightweight version that can run 33$\times$ faster than vanilla NeRFs, and
comparable to the highly-optimized Instant-NGP (0.61 vs 0.71ms/1k pixels).
Project page: \href{https://dnmp.github.io/}{https://dnmp.github.io/}.";Fan Lu<author:sep>Yan Xu<author:sep>Guang Chen<author:sep>Hongsheng Li<author:sep>Kwan-Yee Lin<author:sep>Changjun Jiang;http://arxiv.org/pdf/2307.10776v1;cs.CV;Accepted to ICCV2023;nerf
2307.09860v1;http://arxiv.org/abs/2307.09860v1;2023-07-19;Magic NeRF Lens: Interactive Fusion of Neural Radiance Fields for  Virtual Facility Inspection;"Large industrial facilities such as particle accelerators and nuclear power
plants are critical infrastructures for scientific research and industrial
processes. These facilities are complex systems that not only require regular
maintenance and upgrades but are often inaccessible to humans due to various
safety hazards. Therefore, a virtual reality (VR) system that can quickly
replicate real-world remote environments to provide users with a high level of
spatial and situational awareness is crucial for facility maintenance planning.
However, the exact 3D shapes of these facilities are often too complex to be
accurately modeled with geometric primitives through the traditional
rasterization pipeline.
  In this work, we develop Magic NeRF Lens, an interactive framework to support
facility inspection in immersive VR using neural radiance fields (NeRF) and
volumetric rendering. We introduce a novel data fusion approach that combines
the complementary strengths of volumetric rendering and geometric
rasterization, allowing a NeRF model to be merged with other conventional 3D
data, such as a computer-aided design model. We develop two novel 3D magic lens
effects to optimize NeRF rendering by exploiting the properties of human vision
and context-aware visualization. We demonstrate the high usability of our
framework and methods through a technical benchmark, a visual search user
study, and expert reviews. In addition, the source code of our VR NeRF
framework is made publicly available for future research and development.";Ke Li<author:sep>Susanne Schmidt<author:sep>Tim Rolff<author:sep>Reinhard Bacher<author:sep>Wim Leemans<author:sep>Frank Steinicke;http://arxiv.org/pdf/2307.09860v1;cs.GR;"This work has been submitted to the IEEE TVCG for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible";nerf
2307.09153v2;http://arxiv.org/abs/2307.09153v2;2023-07-18;OPHAvatars: One-shot Photo-realistic Head Avatars;"We propose a method for synthesizing photo-realistic digital avatars from
only one portrait as the reference. Given a portrait, our method synthesizes a
coarse talking head video using driving keypoints features. And with the coarse
video, our method synthesizes a coarse talking head avatar with a deforming
neural radiance field. With rendered images of the coarse avatar, our method
updates the low-quality images with a blind face restoration model. With
updated images, we retrain the avatar for higher quality. After several
iterations, our method can synthesize a photo-realistic animatable 3D neural
head avatar. The motivation of our method is deformable neural radiance field
can eliminate the unnatural distortion caused by the image2video method. Our
method outperforms state-of-the-art methods in quantitative and qualitative
studies on various subjects.";Shaoxu Li;http://arxiv.org/pdf/2307.09153v2;cs.CV;code: https://github.com/lsx0101/OPHAvatars;
2307.09323v2;http://arxiv.org/abs/2307.09323v2;2023-07-18;Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking  Portrait Synthesis;"This paper presents ER-NeRF, a novel conditional Neural Radiance Fields
(NeRF) based architecture for talking portrait synthesis that can concurrently
achieve fast convergence, real-time rendering, and state-of-the-art performance
with small model size. Our idea is to explicitly exploit the unequal
contribution of spatial regions to guide talking portrait modeling.
Specifically, to improve the accuracy of dynamic head reconstruction, a compact
and expressive NeRF-based Tri-Plane Hash Representation is introduced by
pruning empty spatial regions with three planar hash encoders. For speech
audio, we propose a Region Attention Module to generate region-aware condition
feature via an attention mechanism. Different from existing methods that
utilize an MLP-based encoder to learn the cross-modal relation implicitly, the
attention mechanism builds an explicit connection between audio features and
spatial regions to capture the priors of local motions. Moreover, a direct and
fast Adaptive Pose Encoding is introduced to optimize the head-torso separation
problem by mapping the complex transformation of the head pose into spatial
coordinates. Extensive experiments demonstrate that our method renders better
high-fidelity and audio-lips synchronized talking portrait videos, with
realistic details and high efficiency compared to previous methods.";Jiahe Li<author:sep>Jiawei Zhang<author:sep>Xiao Bai<author:sep>Jun Zhou<author:sep>Lin Gu;http://arxiv.org/pdf/2307.09323v2;cs.CV;"Accepted by ICCV 2023. Project page:
  https://fictionarry.github.io/ER-NeRF/";nerf
2307.09070v1;http://arxiv.org/abs/2307.09070v1;2023-07-18;PixelHuman: Animatable Neural Radiance Fields from Few Images;"In this paper, we propose PixelHuman, a novel human rendering model that
generates animatable human scenes from a few images of a person with unseen
identity, views, and poses. Previous work have demonstrated reasonable
performance in novel view and pose synthesis, but they rely on a large number
of images to train and are trained per scene from videos, which requires
significant amount of time to produce animatable scenes from unseen human
images. Our method differs from existing methods in that it can generalize to
any input image for animatable human synthesis. Given a random pose sequence,
our method synthesizes each target scene using a neural radiance field that is
conditioned on a canonical representation and pose-aware pixel-aligned
features, both of which can be obtained through deformation fields learned in a
data-driven manner. Our experiments show that our method achieves
state-of-the-art performance in multiview and novel pose synthesis from
few-shot images.";Gyumin Shim<author:sep>Jaeseong Lee<author:sep>Junha Hyung<author:sep>Jaegul Choo;http://arxiv.org/pdf/2307.09070v1;cs.CV;8 pages;
2307.08093v2;http://arxiv.org/abs/2307.08093v2;2023-07-16;Cross-Ray Neural Radiance Fields for Novel-view Synthesis from  Unconstrained Image Collections;"Neural Radiance Fields (NeRF) is a revolutionary approach for rendering
scenes by sampling a single ray per pixel and it has demonstrated impressive
capabilities in novel-view synthesis from static scene images. However, in
practice, we usually need to recover NeRF from unconstrained image collections,
which poses two challenges: 1) the images often have dynamic changes in
appearance because of different capturing time and camera settings; 2) the
images may contain transient objects such as humans and cars, leading to
occlusion and ghosting artifacts. Conventional approaches seek to address these
challenges by locally utilizing a single ray to synthesize a color of a pixel.
In contrast, humans typically perceive appearance and objects by globally
utilizing information across multiple pixels. To mimic the perception process
of humans, in this paper, we propose Cross-Ray NeRF (CR-NeRF) that leverages
interactive information across multiple rays to synthesize occlusion-free novel
views with the same appearances as the images. Specifically, to model varying
appearances, we first propose to represent multiple rays with a novel cross-ray
feature and then recover the appearance by fusing global statistics, i.e.,
feature covariance of the rays and the image appearance. Moreover, to avoid
occlusion introduced by transient objects, we propose a transient objects
handler and introduce a grid sampling strategy for masking out the transient
objects. We theoretically find that leveraging correlation across multiple rays
promotes capturing more global information. Moreover, extensive experimental
results on large real-world datasets verify the effectiveness of CR-NeRF.";Yifan Yang<author:sep>Shuhai Zhang<author:sep>Zixiong Huang<author:sep>Yubing Zhang<author:sep>Mingkui Tan;http://arxiv.org/pdf/2307.08093v2;cs.CV;ICCV 2023 Oral;nerf
2307.07729v1;http://arxiv.org/abs/2307.07729v1;2023-07-15;Improving NeRF with Height Data for Utilization of GIS Data;"Neural Radiance Fields (NeRF) has been applied to various tasks related to
representations of 3D scenes. Most studies based on NeRF have focused on a
small object, while a few studies have tried to reconstruct large-scale scenes
although these methods tend to require large computational cost. For the
application of NeRF to large-scale scenes, a method based on NeRF is proposed
in this paper to effectively use height data which can be obtained from GIS
(Geographic Information System). For this purpose, the scene space was divided
into multiple objects and a background using the height data to represent them
with separate neural networks. In addition, an adaptive sampling method is also
proposed by using the height data. As a result, the accuracy of image rendering
was improved with faster training speed.";Hinata Aoki<author:sep>Takao Yamanaka;http://arxiv.org/pdf/2307.07729v1;cs.CV;ICIP2023;nerf
2307.09555v1;http://arxiv.org/abs/2307.09555v1;2023-07-14;Transient Neural Radiance Fields for Lidar View Synthesis and 3D  Reconstruction;"Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling
scene appearance and geometry from multiview imagery. Recent work has also
begun to explore how to use additional supervision from lidar or depth sensor
measurements in the NeRF framework. However, previous lidar-supervised NeRFs
focus on rendering conventional camera imagery and use lidar-derived point
cloud data as auxiliary supervision; thus, they fail to incorporate the
underlying image formation model of the lidar. Here, we propose a novel method
for rendering transient NeRFs that take as input the raw, time-resolved photon
count histograms measured by a single-photon lidar system, and we seek to
render such histograms from novel views. Different from conventional NeRFs, the
approach relies on a time-resolved version of the volume rendering equation to
render the lidar measurements and capture transient light transport phenomena
at picosecond timescales. We evaluate our method on a first-of-its-kind dataset
of simulated and captured transient multiview scans from a prototype
single-photon lidar. Overall, our work brings NeRFs to a new dimension of
imaging at transient timescales, newly enabling rendering of transient imagery
from novel views. Additionally, we show that our approach recovers improved
geometry and conventional appearance compared to point cloud-based supervision
when training on few input viewpoints. Transient NeRFs may be especially useful
for applications which seek to simulate raw lidar measurements for downstream
tasks in autonomous driving, robotics, and remote sensing.";Anagh Malik<author:sep>Parsa Mirdehghan<author:sep>Sotiris Nousias<author:sep>Kiriakos N. Kutulakos<author:sep>David B. Lindell;http://arxiv.org/pdf/2307.09555v1;cs.CV;;nerf
2307.07125v2;http://arxiv.org/abs/2307.07125v2;2023-07-14;CeRF: Convolutional Neural Radiance Fields for New View Synthesis with  Derivatives of Ray Modeling;"In recent years, novel view synthesis has gained popularity in generating
high-fidelity images. While demonstrating superior performance in the task of
synthesizing novel views, the majority of these methods are still based on the
conventional multi-layer perceptron for scene embedding. Furthermore, light
field models suffer from geometric blurring during pixel rendering, while
radiance field-based volume rendering methods have multiple solutions for a
certain target of density distribution integration. To address these issues, we
introduce the Convolutional Neural Radiance Fields to model the derivatives of
radiance along rays. Based on 1D convolutional operations, our proposed method
effectively extracts potential ray representations through a structured neural
network architecture. Besides, with the proposed ray modeling, a proposed
recurrent module is employed to solve geometric ambiguity in the fully neural
rendering process. Extensive experiments demonstrate the promising results of
our proposed model compared with existing state-of-the-art methods.";Xiaoyan Yang<author:sep>Dingbo Lu<author:sep>Yang Li<author:sep>Chenhui Li<author:sep>Changbo Wang;http://arxiv.org/pdf/2307.07125v2;cs.CV;16 pages, 11 figures;
2307.05087v1;http://arxiv.org/abs/2307.05087v1;2023-07-11;SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View  Representation;"SAR images are highly sensitive to observation configurations, and they
exhibit significant variations across different viewing angles, making it
challenging to represent and learn their anisotropic features. As a result,
deep learning methods often generalize poorly across different view angles.
Inspired by the concept of neural radiance fields (NeRF), this study combines
SAR imaging mechanisms with neural networks to propose a novel NeRF model for
SAR image generation. Following the mapping and projection pinciples, a set of
SAR images is modeled implicitly as a function of attenuation coefficients and
scattering intensities in the 3D imaging space through a differentiable
rendering equation. SAR-NeRF is then constructed to learn the distribution of
attenuation coefficients and scattering intensities of voxels, where the
vectorized form of 3D voxel SAR rendering equation and the sampling
relationship between the 3D space voxels and the 2D view ray grids are
analytically derived. Through quantitative experiments on various datasets, we
thoroughly assess the multi-view representation and generalization capabilities
of SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset can
significantly improve SAR target classification performance under few-shot
learning setup, where a 10-type classification accuracy of 91.6\% can be
achieved by using only 12 images per class.";Zhengxin Lei<author:sep>Feng Xu<author:sep>Jiangtao Wei<author:sep>Feng Cai<author:sep>Feng Wang<author:sep>Ya-Qiu Jin;http://arxiv.org/pdf/2307.05087v1;cs.CV;;nerf
2307.03441v1;http://arxiv.org/abs/2307.03441v1;2023-07-07;NOFA: NeRF-based One-shot Facial Avatar Reconstruction;"3D facial avatar reconstruction has been a significant research topic in
computer graphics and computer vision, where photo-realistic rendering and
flexible controls over poses and expressions are necessary for many related
applications. Recently, its performance has been greatly improved with the
development of neural radiance fields (NeRF). However, most existing NeRF-based
facial avatars focus on subject-specific reconstruction and reenactment,
requiring multi-shot images containing different views of the specific subject
for training, and the learned model cannot generalize to new identities,
limiting its further applications. In this work, we propose a one-shot 3D
facial avatar reconstruction framework that only requires a single source image
to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking
generalization ability and missing multi-view information, we leverage the
generative prior of 3D GAN and develop an efficient encoder-decoder network to
reconstruct the canonical neural volume of the source image, and further
propose a compensation network to complement facial details. To enable
fine-grained control over facial dynamics, we propose a deformation field to
warp the canonical volume into driven expressions. Through extensive
experimental comparisons, we achieve superior synthesis results compared to
several state-of-the-art methods.";Wangbo Yu<author:sep>Yanbo Fan<author:sep>Yong Zhang<author:sep>Xuan Wang<author:sep>Fei Yin<author:sep>Yunpeng Bai<author:sep>Yan-Pei Cao<author:sep>Ying Shan<author:sep>Yang Wu<author:sep>Zhongqian Sun<author:sep>Baoyuan Wu;http://arxiv.org/pdf/2307.03441v1;cs.CV;;nerf
2307.03404v2;http://arxiv.org/abs/2307.03404v2;2023-07-07;RGB-D Mapping and Tracking in a Plenoxel Radiance Field;"The widespread adoption of Neural Radiance Fields (NeRFs) have ensured
significant advances in the domain of novel view synthesis in recent years.
These models capture a volumetric radiance field of a scene, creating highly
convincing, dense, photorealistic models through the use of simple,
differentiable rendering equations. Despite their popularity, these algorithms
suffer from severe ambiguities in visual data inherent to the RGB sensor, which
means that although images generated with view synthesis can visually appear
very believable, the underlying 3D model will often be wrong. This considerably
limits the usefulness of these models in practical applications like Robotics
and Extended Reality (XR), where an accurate dense 3D reconstruction otherwise
would be of significant value. In this paper, we present the vital differences
between view synthesis models and 3D reconstruction models. We also comment on
why a depth sensor is essential for modeling accurate geometry in general
outward-facing scenes using the current paradigm of novel view synthesis
methods. Focusing on the structure-from-motion task, we practically demonstrate
this need by extending the Plenoxel radiance field model: Presenting an
analytical differential approach for dense mapping and tracking with radiance
fields based on RGB-D data without a neural network. Our method achieves
state-of-the-art results in both mapping and tracking tasks, while also being
faster than competing neural network-based approaches. The code is available
at: https://github.com/ysus33/RGB-D_Plenoxel_Mapping_Tracking.git";Andreas L. Teigen<author:sep>Yeonsoo Park<author:sep>Annette Stahl<author:sep>Rudolf Mester;http://arxiv.org/pdf/2307.03404v2;cs.CV;"2024 IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV) *The first two authors contributed equally to this paper";nerf
2306.17723v4;http://arxiv.org/abs/2306.17723v4;2023-06-30;FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis;"Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis
with its remarkable quality of rendered images and simple architecture.
Although NeRF has been developed in various directions improving continuously
its performance, the necessity of a dense set of multi-view images still exists
as a stumbling block to progress for practical application. In this work, we
propose FlipNeRF, a novel regularization method for few-shot novel view
synthesis by utilizing our proposed flipped reflection rays. The flipped
reflection rays are explicitly derived from the input ray directions and
estimated normal vectors, and play a role of effective additional training rays
while enabling to estimate more accurate surface normals and learn the 3D
geometry effectively. Since the surface normal and the scene depth are both
derived from the estimated densities along a ray, the accurate surface normal
leads to more exact depth estimation, which is a key factor for few-shot novel
view synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Loss
and Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate more
reliable outputs with reducing floating artifacts effectively across the
different scene structures, and enhance the feature-level consistency between
the pair of the rays cast toward the photo-consistent pixels without any
additional feature extractor, respectively. Our FlipNeRF achieves the SOTA
performance on the multiple benchmarks across all the scenarios.";Seunghyeon Seo<author:sep>Yeonjin Chang<author:sep>Nojun Kwak;http://arxiv.org/pdf/2306.17723v4;cs.CV;ICCV 2023. Project Page: https://shawn615.github.io/flipnerf/;nerf
2306.17624v2;http://arxiv.org/abs/2306.17624v2;2023-06-30;Sphere2Vec: A General-Purpose Location Representation Learning over a  Spherical Surface for Large-Scale Geospatial Predictions;"Generating learning-friendly representations for points in space is a
fundamental and long-standing problem in ML. Recently, multi-scale encoding
schemes (such as Space2Vec and NeRF) were proposed to directly encode any point
in 2D/3D Euclidean space as a high-dimensional vector, and has been
successfully applied to various geospatial prediction and generative tasks.
However, all current 2D and 3D location encoders are designed to model point
distances in Euclidean space. So when applied to large-scale real-world GPS
coordinate datasets, which require distance metric learning on the spherical
surface, both types of models can fail due to the map projection distortion
problem (2D) and the spherical-to-Euclidean distance approximation error (3D).
To solve these problems, we propose a multi-scale location encoder called
Sphere2Vec which can preserve spherical distances when encoding point
coordinates on a spherical surface. We developed a unified view of
distance-reserving encoding on spheres based on the DFS. We also provide
theoretical proof that the Sphere2Vec preserves the spherical surface distance
between any two points, while existing encoding schemes do not. Experiments on
20 synthetic datasets show that Sphere2Vec can outperform all baseline models
on all these datasets with up to 30.8% error rate reduction. We then apply
Sphere2Vec to three geo-aware image classification tasks - fine-grained species
recognition, Flickr image recognition, and remote sensing image classification.
Results on 7 real-world datasets show the superiority of Sphere2Vec over
multiple location encoders on all three tasks. Further analysis shows that
Sphere2Vec outperforms other location encoder models, especially in the polar
regions and data-sparse areas because of its nature for spherical surface
distance preservation. Code and data are available at
https://gengchenmai.github.io/sphere2vec-website/.";Gengchen Mai<author:sep>Yao Xuan<author:sep>Wenyun Zuo<author:sep>Yutong He<author:sep>Jiaming Song<author:sep>Stefano Ermon<author:sep>Krzysztof Janowicz<author:sep>Ni Lao;http://arxiv.org/pdf/2306.17624v2;cs.CV;"30 Pages, 16 figures. Accepted to ISPRS Journal of Photogrammetry and
  Remote Sensing";nerf
2306.17843v2;http://arxiv.org/abs/2306.17843v2;2023-06-30;Magic123: One Image to High-Quality 3D Object Generation Using Both 2D  and 3D Diffusion Priors;"We present Magic123, a two-stage coarse-to-fine approach for high-quality,
textured 3D meshes generation from a single unposed image in the wild using
both2D and 3D priors. In the first stage, we optimize a neural radiance field
to produce a coarse geometry. In the second stage, we adopt a memory-efficient
differentiable mesh representation to yield a high-resolution mesh with a
visually appealing texture. In both stages, the 3D content is learned through
reference view supervision and novel views guided by a combination of 2D and 3D
diffusion priors. We introduce a single trade-off parameter between the 2D and
3D priors to control exploration (more imaginative) and exploitation (more
precise) of the generated geometry. Additionally, we employ textual inversion
and monocular depth regularization to encourage consistent appearances across
views and to prevent degenerate solutions, respectively. Magic123 demonstrates
a significant improvement over previous image-to-3D techniques, as validated
through extensive experiments on synthetic benchmarks and diverse real-world
images. Our code, models, and generated 3D assets are available at
https://github.com/guochengqian/Magic123.";Guocheng Qian<author:sep>Jinjie Mai<author:sep>Abdullah Hamdi<author:sep>Jian Ren<author:sep>Aliaksandr Siarohin<author:sep>Bing Li<author:sep>Hsin-Ying Lee<author:sep>Ivan Skorokhodov<author:sep>Peter Wonka<author:sep>Sergey Tulyakov<author:sep>Bernard Ghanem;http://arxiv.org/pdf/2306.17843v2;cs.CV;webpage: https://guochengqian.github.io/project/magic123/;
2306.16928v1;http://arxiv.org/abs/2306.16928v1;2023-06-29;One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape  Optimization;"Single image 3D reconstruction is an important but challenging task that
requires extensive knowledge of our natural world. Many existing methods solve
this problem by optimizing a neural radiance field under the guidance of 2D
diffusion models but suffer from lengthy optimization time, 3D inconsistency
results, and poor geometry. In this work, we propose a novel method that takes
a single image of any object as input and generates a full 360-degree 3D
textured mesh in a single feed-forward pass. Given a single image, we first use
a view-conditioned 2D diffusion model, Zero123, to generate multi-view images
for the input view, and then aim to lift them up to 3D space. Since traditional
reconstruction methods struggle with inconsistent multi-view predictions, we
build our 3D reconstruction module upon an SDF-based generalizable neural
surface reconstruction method and propose several critical training strategies
to enable the reconstruction of 360-degree meshes. Without costly
optimizations, our method reconstructs 3D shapes in significantly less time
than existing methods. Moreover, our method favors better geometry, generates
more 3D consistent results, and adheres more closely to the input image. We
evaluate our approach on both synthetic data and in-the-wild images and
demonstrate its superiority in terms of both mesh quality and runtime. In
addition, our approach can seamlessly support the text-to-3D task by
integrating with off-the-shelf text-to-image diffusion models.";Minghua Liu<author:sep>Chao Xu<author:sep>Haian Jin<author:sep>Linghao Chen<author:sep>Mukund Varma T<author:sep>Zexiang Xu<author:sep>Hao Su;http://arxiv.org/pdf/2306.16928v1;cs.CV;project website: one-2-3-45.com;
2306.16541v1;http://arxiv.org/abs/2306.16541v1;2023-06-28;Envisioning a Next Generation Extended Reality Conferencing System with  Efficient Photorealistic Human Rendering;"Meeting online is becoming the new normal. Creating an immersive experience
for online meetings is a necessity towards more diverse and seamless
environments. Efficient photorealistic rendering of human 3D dynamics is the
core of immersive meetings. Current popular applications achieve real-time
conferencing but fall short in delivering photorealistic human dynamics, either
due to limited 2D space or the use of avatars that lack realistic interactions
between participants. Recent advances in neural rendering, such as the Neural
Radiance Field (NeRF), offer the potential for greater realism in metaverse
meetings. However, the slow rendering speed of NeRF poses challenges for
real-time conferencing. We envision a pipeline for a future extended reality
metaverse conferencing system that leverages monocular video acquisition and
free-viewpoint synthesis to enhance data and hardware efficiency. Towards an
immersive conferencing experience, we explore an accelerated NeRF-based
free-viewpoint synthesis algorithm for rendering photorealistic human dynamics
more efficiently. We show that our algorithm achieves comparable rendering
quality while performing training and inference 44.5% and 213% faster than
state-of-the-art methods, respectively. Our exploration provides a design basis
for constructing metaverse conferencing systems that can handle complex
application scenarios, including dynamic scene relighting with customized
themes and multi-user conferencing that harmonizes real-world people into an
extended world.";Chuanyue Shen<author:sep>Letian Zhang<author:sep>Zhangsihao Yang<author:sep>Masood Mortazavi<author:sep>Xiyun Song<author:sep>Liang Peng<author:sep>Heather Yu;http://arxiv.org/pdf/2306.16541v1;cs.CV;Accepted to CVPR 2023 ECV Workshop;nerf
2306.15703v1;http://arxiv.org/abs/2306.15703v1;2023-06-27;Toward a Spectral Foundation Model: An Attention-Based Approach with  Domain-Inspired Fine-Tuning and Wavelength Parameterization;"Astrophysical explorations are underpinned by large-scale stellar
spectroscopy surveys, necessitating a paradigm shift in spectral fitting
techniques. Our study proposes three enhancements to transcend the limitations
of the current spectral emulation models. We implement an attention-based
emulator, adept at unveiling long-range information between wavelength pixels.
We leverage a domain-specific fine-tuning strategy where the model is
pre-trained on spectra with fixed stellar parameters and variable elemental
abundances, followed by fine-tuning on the entire domain. Moreover, by treating
wavelength as an autonomous model parameter, akin to neural radiance fields,
the model can generate spectra on any wavelength grid. In the case with a
training set of O(1000), our approach exceeds current leading methods by a
factor of 5-10 across all metrics.";Tomasz RÃ³Å¼aÅski<author:sep>Yuan-Sen Ting<author:sep>Maja JabÅoÅska;http://arxiv.org/pdf/2306.15703v1;astro-ph.IM;"7 pages, 3 figures, accepted to ICML 2023 Workshop on Machine
  Learning for Astrophysics";
2306.15203v2;http://arxiv.org/abs/2306.15203v2;2023-06-27;Unsupervised Polychromatic Neural Representation for CT Metal Artifact  Reduction;"Emerging neural reconstruction techniques based on tomography (e.g., NeRF,
NeAT, and NeRP) have started showing unique capabilities in medical imaging. In
this work, we present a novel Polychromatic neural representation (Polyner) to
tackle the challenging problem of CT imaging when metallic implants exist
within the human body. CT metal artifacts arise from the drastic variation of
metal's attenuation coefficients at various energy levels of the X-ray
spectrum, leading to a nonlinear metal effect in CT measurements. Recovering CT
images from metal-affected measurements hence poses a complicated nonlinear
inverse problem where empirical models adopted in previous metal artifact
reduction (MAR) approaches lead to signal loss and strongly aliased
reconstructions. Polyner instead models the MAR problem from a nonlinear
inverse problem perspective. Specifically, we first derive a polychromatic
forward model to accurately simulate the nonlinear CT acquisition process.
Then, we incorporate our forward model into the implicit neural representation
to accomplish reconstruction. Lastly, we adopt a regularizer to preserve the
physical properties of the CT images across different energy levels while
effectively constraining the solution space. Our Polyner is an unsupervised
method and does not require any external training data. Experimenting with
multiple datasets shows that our Polyner achieves comparable or better
performance than supervised methods on in-domain datasets while demonstrating
significant performance improvements on out-of-domain datasets. To the best of
our knowledge, our Polyner is the first unsupervised MAR method that
outperforms its supervised counterparts. The code for this work is available
at: https://github.com/iwuqing/Polyner.";Qing Wu<author:sep>Lixuan Chen<author:sep>Ce Wang<author:sep>Hongjiang Wei<author:sep>S. Kevin Zhou<author:sep>Jingyi Yu<author:sep>Yuyao Zhang;http://arxiv.org/pdf/2306.15203v2;eess.IV;Accepted by NeurIPS 2023;nerf
2306.14490v1;http://arxiv.org/abs/2306.14490v1;2023-06-26;TaiChi Action Capture and Performance Analysis with Multi-view RGB  Cameras;"Recent advances in computer vision and deep learning have influenced the
field of sports performance analysis for researchers to track and reconstruct
freely moving humans without any marker attachment. However, there are few
works for vision-based motion capture and intelligent analysis for professional
TaiChi movement. In this paper, we propose a framework for TaiChi performance
capture and analysis with multi-view geometry and artificial intelligence
technology. The main innovative work is as follows: 1) A multi-camera system
suitable for TaiChi motion capture is built and the multi-view TaiChi data is
collected and processed; 2) A combination of traditional visual method and
implicit neural radiance field is proposed to achieve sparse 3D skeleton fusion
and dense 3D surface reconstruction. 3) The normalization modeling of movement
sequences is carried out based on motion transfer, so as to realize TaiChi
performance analysis for different groups. We have carried out evaluation
experiments, and the experimental results have shown the efficiency of our
method.";Jianwei Li<author:sep>Siyu Mo<author:sep>Yanfei Shen;http://arxiv.org/pdf/2306.14490v1;cs.CV;;
2306.12760v2;http://arxiv.org/abs/2306.12760v2;2023-06-22;Blended-NeRF: Zero-Shot Object Generation and Blending in Existing  Neural Radiance Fields;"Editing a local region or a specific object in a 3D scene represented by a
NeRF or consistently blending a new realistic object into the scene is
challenging, mainly due to the implicit nature of the scene representation. We
present Blended-NeRF, a robust and flexible framework for editing a specific
region of interest in an existing NeRF scene, based on text prompts, along with
a 3D ROI box. Our method leverages a pretrained language-image model to steer
the synthesis towards a user-provided text prompt, along with a 3D MLP model
initialized on an existing NeRF scene to generate the object and blend it into
a specified region in the original scene. We allow local editing by localizing
a 3D ROI box in the input scene, and blend the content synthesized inside the
ROI with the existing scene using a novel volumetric blending technique. To
obtain natural looking and view-consistent results, we leverage existing and
new geometric priors and 3D augmentations for improving the visual fidelity of
the final result. We test our framework both qualitatively and quantitatively
on a variety of real 3D scenes and text prompts, demonstrating realistic
multi-view consistent results with much flexibility and diversity compared to
the baselines. Finally, we show the applicability of our framework for several
3D editing applications, including adding new objects to a scene,
removing/replacing/altering existing objects, and texture conversion.";Ori Gordon<author:sep>Omri Avrahami<author:sep>Dani Lischinski;http://arxiv.org/pdf/2306.12760v2;cs.CV;"16 pages, 14 figures. Project page:
  https://www.vision.huji.ac.il/blended-nerf/";nerf
2306.12570v1;http://arxiv.org/abs/2306.12570v1;2023-06-21;Local 3D Editing via 3D Distillation of CLIP Knowledge;"3D content manipulation is an important computer vision task with many
real-world applications (e.g., product design, cartoon generation, and 3D
Avatar editing). Recently proposed 3D GANs can generate diverse photorealistic
3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of
NeRF still remains a challenging problem since the visual quality tends to
degrade after manipulation and suboptimal control handles such as 2D semantic
maps are used for manipulations. While text-guided manipulations have shown
potential in 3D editing, such approaches often lack locality. To overcome these
problems, we propose Local Editing NeRF (LENeRF), which only requires text
inputs for fine-grained and localized manipulation. Specifically, we present
three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field
Network, and the Deformation Network, which are jointly used for local
manipulations of 3D features by estimating a 3D attention field. The 3D
attention field is learned in an unsupervised way, by distilling the zero-shot
mask generation capability of CLIP to the 3D space with multi-view guidance. We
conduct diverse experiments and thorough evaluations both quantitatively and
qualitatively.";Junha Hyung<author:sep>Sungwon Hwang<author:sep>Daejin Kim<author:sep>Hyunji Lee<author:sep>Jaegul Choo;http://arxiv.org/pdf/2306.12570v1;cs.CV;conference: CVPR 2023;nerf
2306.12422v1;http://arxiv.org/abs/2306.12422v1;2023-06-21;DreamTime: An Improved Optimization Strategy for Text-to-3D Content  Creation;"Text-to-image diffusion models pre-trained on billions of image-text pairs
have recently enabled text-to-3D content creation by optimizing a randomly
initialized Neural Radiance Fields (NeRF) with score distillation. However, the
resultant 3D models exhibit two limitations: (a) quality concerns such as
saturated color and the Janus problem; (b) extremely low diversity comparing to
text-guided image synthesis. In this paper, we show that the conflict between
NeRF optimization process and uniform timestep sampling in score distillation
is the main reason for these limitations. To resolve this conflict, we propose
to prioritize timestep sampling with monotonically non-increasing functions,
which aligns NeRF optimization with the sampling process of diffusion model.
Extensive experiments show that our simple redesign significantly improves
text-to-3D content creation with higher quality and diversity.";Yukun Huang<author:sep>Jianan Wang<author:sep>Yukai Shi<author:sep>Xianbiao Qi<author:sep>Zheng-Jun Zha<author:sep>Lei Zhang;http://arxiv.org/pdf/2306.12422v1;cs.CV;;nerf
2306.12423v1;http://arxiv.org/abs/2306.12423v1;2023-06-21;Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized  Codebase;"Despite the rapid advance of 3D-aware image synthesis, existing studies
usually adopt a mixture of techniques and tricks, leaving it unclear how each
part contributes to the final performance in terms of generality. Following the
most popular and effective paradigm in this field, which incorporates a neural
radiance field (NeRF) into the generator of a generative adversarial network
(GAN), we build a well-structured codebase, dubbed Carver, through modularizing
the generation process. Such a design allows researchers to develop and replace
each module independently, and hence offers an opportunity to fairly compare
various approaches and recognize their contributions from the module
perspective. The reproduction of a range of cutting-edge algorithms
demonstrates the availability of our modularized codebase. We also perform a
variety of in-depth analyses, such as the comparison across different types of
point feature, the necessity of the tailing upsampler in the generator, the
reliance on the camera pose prior, etc., which deepen our understanding of
existing methods and point out some further directions of the research work. We
release code and models at https://github.com/qiuyu96/Carver to facilitate the
development and evaluation of this field.";Qiuyu Wang<author:sep>Zifan Shi<author:sep>Kecheng Zheng<author:sep>Yinghao Xu<author:sep>Sida Peng<author:sep>Yujun Shen;http://arxiv.org/pdf/2306.12423v1;cs.CV;Code: https://github.com/qiuyu96/Carver;nerf
2306.11556v1;http://arxiv.org/abs/2306.11556v1;2023-06-20;NeRF synthesis with shading guidance;"The emerging Neural Radiance Field (NeRF) shows great potential in
representing 3D scenes, which can render photo-realistic images from novel view
with only sparse views given. However, utilizing NeRF to reconstruct real-world
scenes requires images from different viewpoints, which limits its practical
application. This problem can be even more pronounced for large scenes. In this
paper, we introduce a new task called NeRF synthesis that utilizes the
structural content of a NeRF patch exemplar to construct a new radiance field
of large size. We propose a two-phase method for synthesizing new scenes that
are continuous in geometry and appearance. We also propose a boundary
constraint method to synthesize scenes of arbitrary size without artifacts.
Specifically, we control the lighting effects of synthesized scenes using
shading guidance instead of decoupling the scene. We have demonstrated that our
method can generate high-quality results with consistent geometry and
appearance, even for scenes with complex lighting. We can also synthesize new
scenes on curved surface with arbitrary lighting effects, which enhances the
practicality of our proposed NeRF synthesis approach.";Chenbin Li<author:sep>Yu Xin<author:sep>Gaoyi Liu<author:sep>Xiang Zeng<author:sep>Ligang Liu;http://arxiv.org/pdf/2306.11556v1;cs.CV;16 pages, 16 figures, accepted by CAD/Graphics 2023(poster);nerf
2306.10350v2;http://arxiv.org/abs/2306.10350v2;2023-06-17;MA-NeRF: Motion-Assisted Neural Radiance Fields for Face Synthesis from  Sparse Images;"We address the problem of photorealistic 3D face avatar synthesis from sparse
images. Existing Parametric models for face avatar reconstruction struggle to
generate details that originate from inputs. Meanwhile, although current
NeRF-based avatar methods provide promising results for novel view synthesis,
they fail to generalize well for unseen expressions. We improve from NeRF and
propose a novel framework that, by leveraging the parametric 3DMM models, can
reconstruct a high-fidelity drivable face avatar and successfully handle the
unseen expressions. At the core of our implementation are structured
displacement feature and semantic-aware learning module. Our structured
displacement feature will introduce the motion prior as an additional
constraints and help perform better for unseen expressions, by constructing
displacement volume. Besides, the semantic-aware learning incorporates
multi-level prior, e.g., semantic embedding, learnable latent code, to lift the
performance to a higher level. Thorough experiments have been doen both
quantitatively and qualitatively to demonstrate the design of our framework,
and our method achieves much better results than the current state-of-the-arts.";Weichen Zhang<author:sep>Xiang Zhou<author:sep>Yukang Cao<author:sep>Wensen Feng<author:sep>Chun Yuan;http://arxiv.org/pdf/2306.10350v2;cs.CV;;nerf
2306.09329v1;http://arxiv.org/abs/2306.09329v1;2023-06-15;DreamHuman: Animatable 3D Avatars from Text;"We present DreamHuman, a method to generate realistic animatable 3D human
avatar models solely from textual descriptions. Recent text-to-3D methods have
made considerable strides in generation, but are still lacking in important
aspects. Control and often spatial resolution remain limited, existing methods
produce fixed rather than animated 3D human models, and anthropometric
consistency for complex structures like people remains a challenge. DreamHuman
connects large text-to-image synthesis models, neural radiance fields, and
statistical human body models in a novel modeling and optimization framework.
This makes it possible to generate dynamic 3D human avatars with high-quality
textures and learned, instance-specific, surface deformations. We demonstrate
that our method is capable to generate a wide variety of animatable, realistic
3D human models from text. Our 3D models have diverse appearance, clothing,
skin tones and body shapes, and significantly outperform both generic
text-to-3D approaches and previous text-based 3D avatar generators in visual
fidelity. For more results and animations please check our website at
https://dream-human.github.io.";Nikos Kolotouros<author:sep>Thiemo Alldieck<author:sep>Andrei Zanfir<author:sep>Eduard Gabriel Bazavan<author:sep>Mihai Fieraru<author:sep>Cristian Sminchisescu;http://arxiv.org/pdf/2306.09329v1;cs.CV;Project website at https://dream-human.github.io/;
2306.09551v1;http://arxiv.org/abs/2306.09551v1;2023-06-15;Edit-DiffNeRF: Editing 3D Neural Radiance Fields using 2D Diffusion  Model;"Recent research has demonstrated that the combination of pretrained diffusion
models with neural radiance fields (NeRFs) has emerged as a promising approach
for text-to-3D generation. Simply coupling NeRF with diffusion models will
result in cross-view inconsistency and degradation of stylized view syntheses.
To address this challenge, we propose the Edit-DiffNeRF framework, which is
composed of a frozen diffusion model, a proposed delta module to edit the
latent semantic space of the diffusion model, and a NeRF. Instead of training
the entire diffusion for each scene, our method focuses on editing the latent
semantic space in frozen pretrained diffusion models by the delta module. This
fundamental change to the standard diffusion framework enables us to make
fine-grained modifications to the rendered views and effectively consolidate
these instructions in a 3D scene via NeRF training. As a result, we are able to
produce an edited 3D scene that faithfully aligns to input text instructions.
Furthermore, to ensure semantic consistency across different viewpoints, we
propose a novel multi-view semantic consistency loss that extracts a latent
semantic embedding from the input view as a prior, and aim to reconstruct it in
different views. Our proposed method has been shown to effectively edit
real-world 3D scenes, resulting in 25% improvement in the alignment of the
performed 3D edits with text instructions compared to prior work.";Lu Yu<author:sep>Wei Xiang<author:sep>Kang Han;http://arxiv.org/pdf/2306.09551v1;cs.CV;;nerf
2306.09349v2;http://arxiv.org/abs/2306.09349v2;2023-06-15;UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video;"We show how to build a model that allows realistic, free-viewpoint renderings
of a scene under novel lighting conditions from video. Our method -- UrbanIR:
Urban Scene Inverse Rendering -- computes an inverse graphics representation
from the video. UrbanIR jointly infers shape, albedo, visibility, and sun and
sky illumination from a single video of unbounded outdoor scenes with unknown
lighting. UrbanIR uses videos from cameras mounted on cars (in contrast to many
views of the same points in typical NeRF-style estimation). As a result,
standard methods produce poor geometry estimates (for example, roofs), and
there are numerous ''floaters''. Errors in inverse graphics inference can
result in strong rendering artifacts. UrbanIR uses novel losses to control
these and other sources of error. UrbanIR uses a novel loss to make very good
estimates of shadow volumes in the original scene. The resulting
representations facilitate controllable editing, delivering photorealistic
free-viewpoint renderings of relit scenes and inserted objects. Qualitative
evaluation demonstrates strong improvements over the state-of-the-art.";Zhi-Hao Lin<author:sep>Bohan Liu<author:sep>Yi-Ting Chen<author:sep>David Forsyth<author:sep>Jia-Bin Huang<author:sep>Anand Bhattad<author:sep>Shenlong Wang;http://arxiv.org/pdf/2306.09349v2;cs.CV;https://urbaninverserendering.github.io/;nerf
2306.07579v1;http://arxiv.org/abs/2306.07579v1;2023-06-13;Parametric Implicit Face Representation for Audio-Driven Facial  Reenactment;"Audio-driven facial reenactment is a crucial technique that has a range of
applications in film-making, virtual avatars and video conferences. Existing
works either employ explicit intermediate face representations (e.g., 2D facial
landmarks or 3D face models) or implicit ones (e.g., Neural Radiance Fields),
thus suffering from the trade-offs between interpretability and expressive
power, hence between controllability and quality of the results. In this work,
we break these trade-offs with our novel parametric implicit face
representation and propose a novel audio-driven facial reenactment framework
that is both controllable and can generate high-quality talking heads.
Specifically, our parametric implicit representation parameterizes the implicit
representation with interpretable parameters of 3D face models, thereby taking
the best of both explicit and implicit methods. In addition, we propose several
new techniques to improve the three components of our framework, including i)
incorporating contextual information into the audio-to-expression parameters
encoding; ii) using conditional image synthesis to parameterize the implicit
representation and implementing it with an innovative tri-plane structure for
efficient learning; iii) formulating facial reenactment as a conditional image
inpainting problem and proposing a novel data augmentation technique to improve
model generalizability. Extensive experiments demonstrate that our method can
generate more realistic results than previous methods with greater fidelity to
the identities and talking styles of speakers.";Ricong Huang<author:sep>Peiwen Lai<author:sep>Yipeng Qin<author:sep>Guanbin Li;http://arxiv.org/pdf/2306.07579v1;cs.CV;CVPR 2023;
2306.07581v2;http://arxiv.org/abs/2306.07581v2;2023-06-13;Binary Radiance Fields;"In this paper, we propose \textit{binary radiance fields} (BiRF), a
storage-efficient radiance field representation employing binary feature
encoding that encodes local features using binary encoding parameters in a
format of either $+1$ or $-1$. This binarization strategy lets us represent the
feature grid with highly compact feature encoding and a dramatic reduction in
storage size. Furthermore, our 2D-3D hybrid feature grid design enhances the
compactness of feature encoding as the 3D grid includes main components while
2D grids capture details. In our experiments, binary radiance field
representation successfully outperforms the reconstruction performance of
state-of-the-art (SOTA) efficient radiance field models with lower storage
allocation. In particular, our model achieves impressive results in static
scene reconstruction, with a PSNR of 32.03 dB for Synthetic-NeRF scenes, 34.48
dB for Synthetic-NSVF scenes, 28.20 dB for Tanks and Temples scenes while only
utilizing 0.5 MB of storage space, respectively. We hope the proposed binary
radiance field representation will make radiance fields more accessible without
a storage bottleneck.";Seungjoo Shin<author:sep>Jaesik Park;http://arxiv.org/pdf/2306.07581v2;cs.CV;"Accepted to NeurIPS 2023. Project page:
  https://seungjooshin.github.io/BiRF";nerf
2306.08068v2;http://arxiv.org/abs/2306.08068v2;2023-06-13;DORSal: Diffusion for Object-centric Representations of Scenes et al;"Recent progress in 3D scene understanding enables scalable learning of
representations across large datasets of diverse scenes. As a consequence,
generalization to unseen scenes and objects, rendering novel views from just a
single or a handful of input images, and controllable scene generation that
supports editing, is now possible. However, training jointly on a large number
of scenes typically compromises rendering quality when compared to single-scene
optimized models such as NeRFs. In this paper, we leverage recent progress in
diffusion models to equip 3D scene representation learning models with the
ability to render high-fidelity novel views, while retaining benefits such as
object-level scene editing to a large degree. In particular, we propose DORSal,
which adapts a video diffusion architecture for 3D scene generation conditioned
on frozen object-centric slot-based representations of scenes. On both complex
synthetic multi-object scenes and on the real-world large-scale Street View
dataset, we show that DORSal enables scalable neural rendering of 3D scenes
with object-level editing and improves upon existing approaches.";Allan Jabri<author:sep>Sjoerd van Steenkiste<author:sep>Emiel Hoogeboom<author:sep>Mehdi S. M. Sajjadi<author:sep>Thomas Kipf;http://arxiv.org/pdf/2306.08068v2;cs.CV;Project page: https://www.sjoerdvansteenkiste.com/dorsal;nerf
2306.06388v3;http://arxiv.org/abs/2306.06388v3;2023-06-10;From NeRFLiX to NeRFLiX++: A General NeRF-Agnostic Restorer Paradigm;"Neural radiance fields (NeRF) have shown great success in novel view
synthesis. However, recovering high-quality details from real-world scenes is
still challenging for the existing NeRF-based approaches, due to the potential
imperfect calibration information and scene representation inaccuracy. Even
with high-quality training frames, the synthetic novel views produced by NeRF
models still suffer from notable rendering artifacts, such as noise and blur.
To address this, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm
that learns a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for deep neural networks. Moreover, beyond the degradation removal,
we propose an inter-viewpoint aggregation framework that fuses highly related
high-quality training images, pushing the performance of cutting-edge NeRF
models to entirely new levels and producing highly photo-realistic synthetic
views. Based on this paradigm, we further present NeRFLiX++ with a stronger
two-stage NeRF degradation simulator and a faster inter-viewpoint mixer,
achieving superior performance with significantly improved computational
efficiency. Notably, NeRFLiX++ is capable of restoring photo-realistic
ultra-high-resolution outputs from noisy low-resolution NeRF-rendered views.
Extensive experiments demonstrate the excellent restoration ability of
NeRFLiX++ on various novel view synthesis benchmarks.";Kun Zhou<author:sep>Wenbo Li<author:sep>Nianjuan Jiang<author:sep>Xiaoguang Han<author:sep>Jiangbo Lu;http://arxiv.org/pdf/2306.06388v3;cs.CV;"17 pages, 17 figures. To appear in TPAMI2023. Project Page:
  https://redrock303.github.io/nerflix_plus/. arXiv admin note: text overlap
  with arXiv:2303.06919";nerf
2306.06359v1;http://arxiv.org/abs/2306.06359v1;2023-06-10;NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance  Fields against Adversarial Perturbations;"Generalizable Neural Radiance Fields (GNeRF) are one of the most promising
real-world solutions for novel view synthesis, thanks to their cross-scene
generalization capability and thus the possibility of instant rendering on new
scenes. While adversarial robustness is essential for real-world applications,
little study has been devoted to understanding its implication on GNeRF. We
hypothesize that because GNeRF is implemented by conditioning on the source
views from new scenes, which are often acquired from the Internet or
third-party providers, there are potential new security concerns regarding its
real-world applications. Meanwhile, existing understanding and solutions for
neural networks' adversarial robustness may not be applicable to GNeRF, due to
its 3D nature and uniquely diverse operations. To this end, we present NeRFool,
which to the best of our knowledge is the first work that sets out to
understand the adversarial robustness of GNeRF. Specifically, NeRFool unveils
the vulnerability patterns and important insights regarding GNeRF's adversarial
robustness. Built upon the above insights gained from NeRFool, we further
develop NeRFool+, which integrates two techniques capable of effectively
attacking GNeRF across a wide range of target views, and provide guidelines for
defending against our proposed attacks. We believe that our NeRFool/NeRFool+
lays the initial foundation for future innovations in developing robust
real-world GNeRF solutions. Our codes are available at:
https://github.com/GATECH-EIC/NeRFool.";Yonggan Fu<author:sep>Ye Yuan<author:sep>Souvik Kundu<author:sep>Shang Wu<author:sep>Shunyao Zhang<author:sep>Yingyan Lin;http://arxiv.org/pdf/2306.06359v1;cs.CV;Accepted by ICML 2023;nerf
2306.06044v2;http://arxiv.org/abs/2306.06044v2;2023-06-09;GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields;"Neural Radiance Fields (NeRF) have shown impressive novel view synthesis
results; nonetheless, even thorough recordings yield imperfections in
reconstructions, for instance due to poorly observed areas or minor lighting
changes. Our goal is to mitigate these imperfections from various sources with
a joint solution: we take advantage of the ability of generative adversarial
networks (GANs) to produce realistic images and use them to enhance realism in
3D scene reconstruction with NeRFs. To this end, we learn the patch
distribution of a scene using an adversarial discriminator, which provides
feedback to the radiance field reconstruction, thus improving realism in a
3D-consistent fashion. Thereby, rendering artifacts are repaired directly in
the underlying 3D representation by imposing multi-view path rendering
constraints. In addition, we condition a generator with multi-resolution NeRF
renderings which is adversarially trained to further improve rendering quality.
We demonstrate that our approach significantly improves rendering quality,
e.g., nearly halving LPIPS scores compared to Nerfacto while at the same time
improving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples.";Barbara Roessle<author:sep>Norman MÃ¼ller<author:sep>Lorenzo Porzi<author:sep>Samuel Rota BulÃ²<author:sep>Peter Kontschieder<author:sep>Matthias NieÃner;http://arxiv.org/pdf/2306.06044v2;cs.CV;"SIGGRAPH Asia 2023, project page:
  https://barbararoessle.github.io/ganerf , video: https://youtu.be/352ccXWxQVE";nerf
2306.05668v2;http://arxiv.org/abs/2306.05668v2;2023-06-09;RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models;"The emergence of Neural Radiance Fields (NeRF) has promoted the development
of synthesized high-fidelity views of the intricate real world. However, it is
still a very demanding task to repaint the content in NeRF. In this paper, we
propose a novel framework that can take RGB images as input and alter the 3D
content in neural scenes. Our work leverages existing diffusion models to guide
changes in the designated 3D content. Specifically, we semantically select the
target object and a pre-trained diffusion model will guide the NeRF model to
generate new 3D objects, which can improve the editability, diversity, and
application range of NeRF. Experiment results show that our algorithm is
effective for editing 3D objects in NeRF under different text prompts,
including editing appearance, shape, and more. We validate our method on both
real-world datasets and synthetic-world datasets for these editing tasks.
Please visit https://starstesla.github.io/repaintnerf for a better view of our
results.";Xingchen Zhou<author:sep>Ying He<author:sep>F. Richard Yu<author:sep>Jianqiang Li<author:sep>You Li;http://arxiv.org/pdf/2306.05668v2;cs.CV;IJCAI 2023 Accepted (Main Track);nerf
2306.06093v3;http://arxiv.org/abs/2306.06093v3;2023-06-09;HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork;"Neural Radiance Fields (NeRF) have become an increasingly popular
representation to capture high-quality appearance and shape of scenes and
objects. However, learning generalizable NeRF priors over categories of scenes
or objects has been challenging due to the high dimensionality of network
weight space. To address the limitations of existing work on generalization,
multi-view consistency and to improve quality, we propose HyP-NeRF, a latent
conditioning method for learning generalizable category-level NeRF priors using
hypernetworks. Rather than using hypernetworks to estimate only the weights of
a NeRF, we estimate both the weights and the multi-resolution hash encodings
resulting in significant quality gains. To improve quality even further, we
incorporate a denoise and finetune strategy that denoises images rendered from
NeRFs estimated by the hypernetwork and finetunes it while retaining multiview
consistency. These improvements enable us to use HyP-NeRF as a generalizable
prior for multiple downstream tasks including NeRF reconstruction from
single-view or cluttered scenes and text-to-NeRF. We provide qualitative
comparisons and evaluate HyP-NeRF on three tasks: generalization, compression,
and retrieval, demonstrating our state-of-the-art results.";Bipasha Sen<author:sep>Gaurav Singh<author:sep>Aditya Agarwal<author:sep>Rohith Agaram<author:sep>K Madhava Krishna<author:sep>Srinath Sridhar;http://arxiv.org/pdf/2306.06093v3;cs.CV;Project Page: https://hyp-nerf.github.io;nerf
2306.06300v2;http://arxiv.org/abs/2306.06300v2;2023-06-09;NERFBK: A High-Quality Benchmark for NERF-Based 3D Reconstruction;"This paper introduces a new real and synthetic dataset called NeRFBK
specifically designed for testing and comparing NeRF-based 3D reconstruction
algorithms. High-quality 3D reconstruction has significant potential in various
fields, and advancements in image-based algorithms make it essential to
evaluate new advanced techniques. However, gathering diverse data with precise
ground truth is challenging and may not encompass all relevant applications.
The NeRFBK dataset addresses this issue by providing multi-scale, indoor and
outdoor datasets with high-resolution images and videos and camera parameters
for testing and comparing NeRF-based algorithms. This paper presents the design
and creation of the NeRFBK benchmark, various examples and application
scenarios, and highlights its potential for advancing the field of 3D
reconstruction.";Ali Karami<author:sep>Simone Rigon<author:sep>Gabriele Mazzacca<author:sep>Ziyang Yan<author:sep>Fabio Remondino;http://arxiv.org/pdf/2306.06300v2;cs.CV;paper result has problem;nerf
2306.05410v1;http://arxiv.org/abs/2306.05410v1;2023-06-08;LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs;"A critical obstacle preventing NeRF models from being deployed broadly in the
wild is their reliance on accurate camera poses. Consequently, there is growing
interest in extending NeRF models to jointly optimize camera poses and scene
representation, which offers an alternative to off-the-shelf SfM pipelines
which have well-understood failure modes. Existing approaches for unposed NeRF
operate under limited assumptions, such as a prior pose distribution or coarse
pose initialization, making them less effective in a general setting. In this
work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses
and neural radiance fields with relaxed assumptions on pose configuration. Our
approach operates in a local-to-global manner, where we first optimize over
local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and
geometry for this challenging few-shot task. The mini-scene poses are brought
into a global reference frame through a robust pose synchronization step, where
a final global optimization of pose and scene can be performed. We show our
LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making
restrictive assumptions on the pose prior. This allows us to operate in the
general SE(3) pose setting, unlike the baselines. Our results also indicate our
model can be complementary to feature-based SfM pipelines as it compares
favorably to COLMAP on low-texture and low-resolution images.";Zezhou Cheng<author:sep>Carlos Esteves<author:sep>Varun Jampani<author:sep>Abhishek Kar<author:sep>Subhransu Maji<author:sep>Ameesh Makadia;http://arxiv.org/pdf/2306.05410v1;cs.CV;Project website: https://people.cs.umass.edu/~zezhoucheng/lu-nerf/;nerf
2306.05145v1;http://arxiv.org/abs/2306.05145v1;2023-06-08;Variable Radiance Field for Real-Life Category-Specifc Reconstruction  from Single Image;"Reconstructing category-specific objects from a single image is a challenging
task that requires inferring the geometry and appearance of an object from a
limited viewpoint. Existing methods typically rely on local feature retrieval
based on re-projection with known camera intrinsic, which are slow and prone to
distortion at viewpoints distant from the input image. In this paper, we
present Variable Radiance Field (VRF), a novel framework that can efficiently
reconstruct category-specific objects from a single image without known camera
parameters. Our key contributions are: (1) We parameterize the geometry and
appearance of the object using a multi-scale global feature extractor, which
avoids frequent point-wise feature retrieval and camera dependency. We also
propose a contrastive learning-based pretraining strategy to improve the
feature extractor. (2) We reduce the geometric complexity of the object by
learning a category template, and use hypernetworks to generate a small neural
radiance field for fast and instance-specific rendering. (3) We align each
training instance to the template space using a learned similarity
transformation, which enables semantic-consistent learning across different
objects. We evaluate our method on the CO3D dataset and show that it
outperforms existing methods in terms of quality and speed. We also demonstrate
its applicability to shape interpolation and object placement tasks.";Kun Wang<author:sep>Zhiqiang Yan<author:sep>Zhenyu Zhang<author:sep>Xiang Li<author:sep>Jun Li<author:sep>Jian Yang;http://arxiv.org/pdf/2306.05145v1;cs.CV;;
2306.05303v1;http://arxiv.org/abs/2306.05303v1;2023-06-08;Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields;"The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored ""fog"" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF";Qianqiu Tan<author:sep>Tao Liu<author:sep>Yinling Xie<author:sep>Shuwan Yu<author:sep>Baohua Zhang;http://arxiv.org/pdf/2306.05303v1;cs.CV;;nerf
2306.04166v3;http://arxiv.org/abs/2306.04166v3;2023-06-07;BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives;"Implicit neural representation has emerged as a powerful method for
reconstructing 3D scenes from 2D images. Given a set of camera poses and
associated images, the models can be trained to synthesize novel, unseen views.
In order to expand the use cases for implicit neural representations, we need
to incorporate camera pose estimation capabilities as part of the
representation learning, as this is necessary for reconstructing scenes from
real-world video sequences where cameras are generally not being tracked.
Existing approaches like COLMAP and, most recently, bundle-adjusting neural
radiance field methods often suffer from lengthy processing times. These delays
ranging from hours to days, arise from laborious feature matching, hardware
limitations, dense point sampling, and long training times required by a
multi-layer perceptron structure with a large number of parameters. To address
these challenges, we propose a framework called bundle-adjusting accelerated
neural graphics primitives (BAA-NGP). Our approach leverages accelerated
sampling and hash encoding to expedite both pose refinement/estimation and 3D
scene reconstruction. Experimental results demonstrate that our method achieves
a more than 10 to 20 $\times$ speed improvement in novel view synthesis
compared to other bundle-adjusting neural radiance field methods without
sacrificing the quality of pose estimation. The github repository can be found
here https://github.com/IntelLabs/baa-ngp.";Sainan Liu<author:sep>Shan Lin<author:sep>Jingpei Lu<author:sep>Shreya Saha<author:sep>Alexey Supikov<author:sep>Michael Yip;http://arxiv.org/pdf/2306.04166v3;cs.CV;;
2306.03576v1;http://arxiv.org/abs/2306.03576v1;2023-06-06;Human 3D Avatar Modeling with Implicit Neural Representation: A Brief  Survey;"A human 3D avatar is one of the important elements in the metaverse, and the
modeling effect directly affects people's visual experience. However, the human
body has a complex topology and diverse details, so it is often expensive,
time-consuming, and laborious to build a satisfactory model. Recent studies
have proposed a novel method, implicit neural representation, which is a
continuous representation method and can describe objects with arbitrary
topology at arbitrary resolution. Researchers have applied implicit neural
representation to human 3D avatar modeling and obtained more excellent results
than traditional methods. This paper comprehensively reviews the application of
implicit neural representation in human body modeling. First, we introduce
three implicit representations of occupancy field, SDF, and NeRF, and make a
classification of the literature investigated in this paper. Then the
application of implicit modeling methods in the body, hand, and head are
compared and analyzed respectively. Finally, we point out the shortcomings of
current work and provide available suggestions for researchers.";Mingyang Sun<author:sep>Dingkang Yang<author:sep>Dongliang Kou<author:sep>Yang Jiang<author:sep>Weihua Shan<author:sep>Zhe Yan<author:sep>Lihua Zhang;http://arxiv.org/pdf/2306.03576v1;cs.CV;A Brief Survey;nerf
2306.03727v1;http://arxiv.org/abs/2306.03727v1;2023-06-06;Towards Visual Foundational Models of Physical Scenes;"We describe a first step towards learning general-purpose visual
representations of physical scenes using only image prediction as a training
criterion. To do so, we first define ""physical scene"" and show that, even
though different agents may maintain different representations of the same
scene, the underlying physical scene that can be inferred is unique. Then, we
show that NeRFs cannot represent the physical scene, as they lack extrapolation
mechanisms. Those, however, could be provided by Diffusion Models, at least in
theory. To test this hypothesis empirically, NeRFs can be combined with
Diffusion Models, a process we refer to as NeRF Diffusion, used as unsupervised
representations of the physical scene. Our analysis is limited to visual data,
without external grounding mechanisms that can be provided by independent
sensory modalities.";Chethan Parameshwara<author:sep>Alessandro Achille<author:sep>Matthew Trager<author:sep>Xiaolong Li<author:sep>Jiawei Mo<author:sep>Matthew Trager<author:sep>Ashwin Swaminathan<author:sep>CJ Taylor<author:sep>Dheera Venkatraman<author:sep>Xiaohan Fei<author:sep>Stefano Soatto;http://arxiv.org/pdf/2306.03727v1;cs.CV;"TLDR: Physical scenes are equivalence classes of sufficient
  statistics, and can be inferred uniquely by any agent measuring the same
  finite data; We formalize and implement an approach to representation
  learning that overturns ""naive realism"" in favor of an analytical approach of
  Russell and Koenderink. NeRFs cannot capture the physical scenes, but
  combined with Diffusion Models they can";nerf
2306.07349v1;http://arxiv.org/abs/2306.07349v1;2023-06-06;ATT3D: Amortized Text-to-3D Object Synthesis;"Text-to-3D modelling has seen exciting progress by combining generative
text-to-image models with image-to-3D methods like Neural Radiance Fields.
DreamFusion recently achieved high-quality results but requires a lengthy,
per-prompt optimization to create 3D objects. To address this, we amortize
optimization over text prompts by training on many prompts simultaneously with
a unified model, instead of separately. With this, we share computation across
a prompt set, training in less time than per-prompt optimization. Our framework
- Amortized text-to-3D (ATT3D) - enables knowledge-sharing between prompts to
generalize to unseen setups and smooth interpolations between text for novel
assets and simple animations.";Jonathan Lorraine<author:sep>Kevin Xie<author:sep>Xiaohui Zeng<author:sep>Chen-Hsuan Lin<author:sep>Towaki Takikawa<author:sep>Nicholas Sharp<author:sep>Tsung-Yi Lin<author:sep>Ming-Yu Liu<author:sep>Sanja Fidler<author:sep>James Lucas;http://arxiv.org/pdf/2306.07349v1;cs.LG;22 pages, 20 figures;
2306.02741v1;http://arxiv.org/abs/2306.02741v1;2023-06-05;ZIGNeRF: Zero-shot 3D Scene Representation with Invertible Generative  Neural Radiance Fields;"Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable
proficiency in synthesizing multi-view images by learning the distribution of a
set of unposed images. Despite the aptitude of existing generative NeRFs in
generating 3D-consistent high-quality random samples within data distribution,
the creation of a 3D representation of a singular input image remains a
formidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative
model that executes zero-shot Generative Adversarial Network (GAN) inversion
for the generation of multi-view images from a single out-of-domain image. The
model is underpinned by a novel inverter that maps out-of-domain images into
the latent code of the generator manifold. Notably, ZIGNeRF is capable of
disentangling the object from the background and executing 3D operations such
as 360-degree rotation or depth and horizontal translation. The efficacy of our
model is validated using multiple real-image datasets: Cats, AFHQ, CelebA,
CelebA-HQ, and CompCars.";Kanghyeok Ko<author:sep>Minhyeok Lee;http://arxiv.org/pdf/2306.02741v1;cs.CV;;nerf
2306.03000v2;http://arxiv.org/abs/2306.03000v2;2023-06-05;BeyondPixels: A Comprehensive Review of the Evolution of Neural Radiance  Fields;"Neural rendering combines ideas from classical computer graphics and machine
learning to synthesize images from real-world observations. NeRF, short for
Neural Radiance Fields, is a recent innovation that uses AI algorithms to
create 3D objects from 2D images. By leveraging an interpolation approach, NeRF
can produce new 3D reconstructed views of complicated scenes. Rather than
directly restoring the whole 3D scene geometry, NeRF generates a volumetric
representation called a ``radiance field,'' which is capable of creating color
and density for every point within the relevant 3D space. The broad appeal and
notoriety of NeRF make it imperative to examine the existing research on the
topic comprehensively. While previous surveys on 3D rendering have primarily
focused on traditional computer vision-based or deep learning-based approaches,
only a handful of them discuss the potential of NeRF. However, such surveys
have predominantly focused on NeRF's early contributions and have not explored
its full potential. NeRF is a relatively new technique continuously being
investigated for its capabilities and limitations. This survey reviews recent
advances in NeRF and categorizes them according to their architectural designs,
especially in the field of novel view synthesis.";AKM Shahariar Azad Rabby<author:sep>Chengcui Zhang;http://arxiv.org/pdf/2306.03000v2;cs.CV;22 page, 1 figure, 5 table;nerf
2306.02903v1;http://arxiv.org/abs/2306.02903v1;2023-06-05;Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions;"We propose a method for synthesizing edited photo-realistic digital avatars
with text instructions. Given a short monocular RGB video and text
instructions, our method uses an image-conditioned diffusion model to edit one
head image and uses the video stylization method to accomplish the editing of
other head images. Through iterative training and update (three times or more),
our method synthesizes edited photo-realistic animatable 3D neural head avatars
with a deformable neural radiance field head synthesis method. In quantitative
and qualitative studies on various subjects, our method outperforms
state-of-the-art methods.";Shaoxu Li;http://arxiv.org/pdf/2306.02903v1;cs.CV;https://github.com/lsx0101/Instruct-Video2Avatar;
2306.03207v2;http://arxiv.org/abs/2306.03207v2;2023-06-05;H2-Mapping: Real-time Dense Mapping Using Hierarchical Hybrid  Representation;"Constructing a high-quality dense map in real-time is essential for robotics,
AR/VR, and digital twins applications. As Neural Radiance Field (NeRF) greatly
improves the mapping performance, in this paper, we propose a NeRF-based
mapping method that enables higher-quality reconstruction and real-time
capability even on edge computers. Specifically, we propose a novel
hierarchical hybrid representation that leverages implicit multiresolution hash
encoding aided by explicit octree SDF priors, describing the scene at different
levels of detail. This representation allows for fast scene geometry
initialization and makes scene geometry easier to learn. Besides, we present a
coverage-maximizing keyframe selection strategy to address the forgetting issue
and enhance mapping quality, particularly in marginal areas. To the best of our
knowledge, our method is the first to achieve high-quality NeRF-based mapping
on edge computers of handheld devices and quadrotors in real-time. Experiments
demonstrate that our method outperforms existing NeRF-based mapping methods in
geometry accuracy, texture realism, and time consumption. The code will be
released at: https://github.com/SYSU-STAR/H2-Mapping";Chenxing Jiang<author:sep>Hanwen Zhang<author:sep>Peize Liu<author:sep>Zehuan Yu<author:sep>Hui Cheng<author:sep>Boyu Zhou<author:sep>Shaojie Shen;http://arxiv.org/pdf/2306.03207v2;cs.RO;Accepted by IEEE Robotics and Automation Letters;nerf
2306.01531v2;http://arxiv.org/abs/2306.01531v2;2023-06-02;PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline  Panoramas;"Achieving an immersive experience enabling users to explore virtual
environments with six degrees of freedom (6DoF) is essential for various
applications such as virtual reality (VR). Wide-baseline panoramas are commonly
used in these applications to reduce network bandwidth and storage
requirements. However, synthesizing novel views from these panoramas remains a
key challenge. Although existing neural radiance field methods can produce
photorealistic views under narrow-baseline and dense image captures, they tend
to overfit the training views when dealing with \emph{wide-baseline} panoramas
due to the difficulty in learning accurate geometry from sparse $360^{\circ}$
views. To address this problem, we propose PanoGRF, Generalizable Spherical
Radiance Fields for Wide-baseline Panoramas, which construct spherical radiance
fields incorporating $360^{\circ}$ scene priors. Unlike generalizable radiance
fields trained on perspective images, PanoGRF avoids the information loss from
panorama-to-perspective conversion and directly aggregates geometry and
appearance features of 3D sample points from each panoramic view based on
spherical projection. Moreover, as some regions of the panorama are only
visible from one view while invisible from others under wide baseline settings,
PanoGRF incorporates $360^{\circ}$ monocular depth priors into spherical depth
estimation to improve the geometry features. Experimental results on multiple
panoramic datasets demonstrate that PanoGRF significantly outperforms
state-of-the-art generalizable view synthesis methods for wide-baseline
panoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay).";Zheng Chen<author:sep>Yan-Pei Cao<author:sep>Yuan-Chen Guo<author:sep>Chen Wang<author:sep>Ying Shan<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2306.01531v2;cs.CV;"accepted to NeurIPS2023; Project Page:
  https://thucz.github.io/PanoGRF/";
2306.00547v2;http://arxiv.org/abs/2306.00547v2;2023-06-01;AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars;"Capturing and editing full head performances enables the creation of virtual
characters with various applications such as extended reality and media
production. The past few years witnessed a steep rise in the photorealism of
human head avatars. Such avatars can be controlled through different input data
modalities, including RGB, audio, depth, IMUs and others. While these data
modalities provide effective means of control, they mostly focus on editing the
head movements such as the facial expressions, head pose and/or camera
viewpoint. In this paper, we propose AvatarStudio, a text-based method for
editing the appearance of a dynamic full head avatar. Our approach builds on
existing work to capture dynamic performances of human heads using neural
radiance field (NeRF) and edits this representation with a text-to-image
diffusion model. Specifically, we introduce an optimization strategy for
incorporating multiple keyframes representing different camera viewpoints and
time stamps of a video performance into a single diffusion model. Using this
personalized diffusion model, we edit the dynamic NeRF by introducing
view-and-time-aware Score Distillation Sampling (VT-SDS) following a
model-based guidance approach. Our method edits the full head in a canonical
space, and then propagates these edits to remaining time steps via a pretrained
deformation network. We evaluate our method visually and numerically via a user
study, and results show that our method outperforms existing approaches. Our
experiments validate the design choices of our method and highlight that our
edits are genuine, personalized, as well as 3D- and time-consistent.";Mohit Mendiratta<author:sep>Xingang Pan<author:sep>Mohamed Elgharib<author:sep>Kartik Teotia<author:sep>Mallikarjun B R<author:sep>Ayush Tewari<author:sep>Vladislav Golyanik<author:sep>Adam Kortylewski<author:sep>Christian Theobalt;http://arxiv.org/pdf/2306.00547v2;cs.CV;"17 pages, 17 figures. Project page:
  https://vcai.mpi-inf.mpg.de/projects/AvatarStudio/";nerf
2306.00696v1;http://arxiv.org/abs/2306.00696v1;2023-06-01;Analyzing the Internals of Neural Radiance Fields;"Modern Neural Radiance Fields (NeRFs) learn a mapping from position to
volumetric density via proposal network samplers. In contrast to the
coarse-to-fine sampling approach with two NeRFs, this offers significant
potential for speedups using lower network capacity as the task of mapping
spatial coordinates to volumetric density involves no view-dependent effects
and is thus much easier to learn. Given that most of the network capacity is
utilized to estimate radiance, NeRFs could store valuable density information
in their parameters or their deep features. To this end, we take one step back
and analyze large, trained ReLU-MLPs used in coarse-to-fine sampling. We find
that trained NeRFs, Mip-NeRFs and proposal network samplers map samples with
high density to local minima along a ray in activation feature space. We show
how these large MLPs can be accelerated by transforming the intermediate
activations to a weight estimate, without any modifications to the parameters
post-optimization. With our approach, we can reduce the computational
requirements of trained NeRFs by up to 50% with only a slight hit in rendering
quality and no changes to the training protocol or architecture. We evaluate
our approach on a variety of architectures and datasets, showing that our
proposition holds in various settings.";Lukas Radl<author:sep>Andreas Kurz<author:sep>Markus Steinberger;http://arxiv.org/pdf/2306.00696v1;cs.CV;project page: nerfinternals.github.io;nerf
2306.00783v2;http://arxiv.org/abs/2306.00783v2;2023-06-01;FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and  Relighting with Diffusion Models;"The ability to create high-quality 3D faces from a single image has become
increasingly important with wide applications in video conferencing, AR/VR, and
advanced video editing in movie industries. In this paper, we propose Face
Diffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality
Face NeRFs from single images, complete with semantic editing and relighting
capabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly
trained 2D latent-diffusion model, allowing users to manipulate and construct
Face NeRFs in zero-shot learning without the need for explicit 3D data. With
carefully designed illumination and identity preserving loss, as well as
multi-modal pre-training, FaceDNeRF offers users unparalleled control over the
editing process enabling them to create and edit face NeRFs using just
single-view images, text prompts, and explicit target lighting. The advanced
features of FaceDNeRF have been designed to produce more impressive results
than existing 2D editing approaches that rely on 2D segmentation maps for
editable attributes. Experiments show that our FaceDNeRF achieves exceptionally
realistic results and unprecedented flexibility in editing compared with
state-of-the-art 3D face reconstruction and editing methods. Our code will be
available at https://github.com/BillyXYB/FaceDNeRF.";Hao Zhang<author:sep>Yanbo Xu<author:sep>Tianyuan Dai<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2306.00783v2;cs.CV;;nerf
2305.20082v2;http://arxiv.org/abs/2305.20082v2;2023-05-31;Control4D: Efficient 4D Portrait Editing with Text;"We introduce Control4D, an innovative framework for editing dynamic 4D
portraits using text instructions. Our method addresses the prevalent
challenges in 4D editing, notably the inefficiencies of existing 4D
representations and the inconsistent editing effect caused by diffusion-based
editors. We first propose GaussianPlanes, a novel 4D representation that makes
Gaussian Splatting more structured by applying plane-based decomposition in 3D
space and time. This enhances both efficiency and robustness in 4D editing.
Furthermore, we propose to leverage a 4D generator to learn a more continuous
generation space from inconsistent edited images produced by the
diffusion-based editor, which effectively improves the consistency and quality
of 4D editing. Comprehensive evaluation demonstrates the superiority of
Control4D, including significantly reduced training time, high-quality
rendering, and spatial-temporal consistency in 4D portrait editing. The link to
our project website is https://control4darxiv.github.io.";Ruizhi Shao<author:sep>Jingxiang Sun<author:sep>Cheng Peng<author:sep>Zerong Zheng<author:sep>Boyao Zhou<author:sep>Hongwen Zhang<author:sep>Yebin Liu;http://arxiv.org/pdf/2305.20082v2;cs.CV;The link to our project website is https://control4darxiv.github.io;gaussian splatting
2305.19201v2;http://arxiv.org/abs/2305.19201v2;2023-05-30;DaRF: Boosting Radiance Fields from Sparse Inputs with Monocular Depth  Adaptation;"Neural radiance fields (NeRF) shows powerful performance in novel view
synthesis and 3D geometry reconstruction, but it suffers from critical
performance degradation when the number of known viewpoints is drastically
reduced. Existing works attempt to overcome this problem by employing external
priors, but their success is limited to certain types of scenes or datasets.
Employing monocular depth estimation (MDE) networks, pretrained on large-scale
RGB-D datasets, with powerful generalization capability would be a key to
solving this problem: however, using MDE in conjunction with NeRF comes with a
new set of challenges due to various ambiguity problems exhibited by monocular
depths. In this light, we propose a novel framework, dubbed D\""aRF, that
achieves robust NeRF reconstruction with a handful of real-world images by
combining the strengths of NeRF and monocular depth estimation through online
complementary training. Our framework imposes the MDE network's powerful
geometry prior to NeRF representation at both seen and unseen viewpoints to
enhance its robustness and coherence. In addition, we overcome the ambiguity
problems of monocular depths through patch-wise scale-shift fitting and
geometry distillation, which adapts the MDE network to produce depths aligned
accurately with NeRF geometry. Experiments show our framework achieves
state-of-the-art results both quantitatively and qualitatively, demonstrating
consistent and reliable performance in both indoor and outdoor real-world
datasets. Project page is available at https://ku-cvlab.github.io/DaRF/.";Jiuhn Song<author:sep>Seonghoon Park<author:sep>Honggyu An<author:sep>Seokju Cho<author:sep>Min-Seop Kwak<author:sep>Sungjin Cho<author:sep>Seungryong Kim;http://arxiv.org/pdf/2305.19201v2;cs.CV;"To appear at NeurIPS 2023. Project Page:
  https://ku-cvlab.github.io/DaRF/";nerf
2305.19065v2;http://arxiv.org/abs/2305.19065v2;2023-05-30;Template-free Articulated Neural Point Clouds for Reposable View  Synthesis;"Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when
synthesizing novel views of time-evolving 3D scenes. However, the common
reliance on backward deformation fields makes reanimation of the captured
object poses challenging. Moreover, the state of the art dynamic models are
often limited by low visual fidelity, long reconstruction time or specificity
to narrow application domains. In this paper, we present a novel method
utilizing a point-based representation and Linear Blend Skinning (LBS) to
jointly learn a Dynamic NeRF and an associated skeletal model from even sparse
multi-view video. Our forward-warping approach achieves state-of-the-art visual
fidelity when synthesizing novel views and poses while significantly reducing
the necessary learning time when compared to existing work. We demonstrate the
versatility of our representation on a variety of articulated objects from
common datasets and obtain reposable 3D reconstructions without the need of
object-specific skeletal templates. Code will be made available at
https://github.com/lukasuz/Articulated-Point-NeRF.";Lukas Uzolas<author:sep>Elmar Eisemann<author:sep>Petr Kellnhofer;http://arxiv.org/pdf/2305.19065v2;cs.CV;;nerf
2305.18766v3;http://arxiv.org/abs/2305.18766v3;2023-05-30;HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion  Guidance;"The advancements in automatic text-to-3D generation have been remarkable.
Most existing methods use pre-trained text-to-image diffusion models to
optimize 3D representations like Neural Radiance Fields (NeRFs) via
latent-space denoising score matching. Yet, these methods often result in
artifacts and inconsistencies across different views due to their suboptimal
optimization approaches and limited understanding of 3D geometry. Moreover, the
inherent constraints of NeRFs in rendering crisp geometry and stable textures
usually lead to a two-stage optimization to attain high-resolution details.
This work proposes holistic sampling and smoothing approaches to achieve
high-quality text-to-3D generation, all in a single-stage optimization. We
compute denoising scores in the text-to-image diffusion model's latent and
image spaces. Instead of randomly sampling timesteps (also referred to as noise
levels in denoising score matching), we introduce a novel timestep annealing
approach that progressively reduces the sampled timestep throughout
optimization. To generate high-quality renderings in a single-stage
optimization, we propose regularization for the variance of z-coordinates along
NeRF rays. To address texture flickering issues in NeRFs, we introduce a kernel
smoothing technique that refines importance sampling weights coarse-to-fine,
ensuring accurate and thorough sampling in high-density regions. Extensive
experiments demonstrate the superiority of our method over previous approaches,
enabling the generation of highly detailed and view-consistent 3D assets
through a single-stage training process.";Junzhe Zhu<author:sep>Peiye Zhuang;http://arxiv.org/pdf/2305.18766v3;cs.CV;Project page: https://hifa-team.github.io/HiFA-site/;nerf
2305.18079v3;http://arxiv.org/abs/2305.18079v3;2023-05-29;Towards a Robust Framework for NeRF Evaluation;"Neural Radiance Field (NeRF) research has attracted significant attention
recently, with 3D modelling, virtual/augmented reality, and visual effects
driving its application. While current NeRF implementations can produce high
quality visual results, there is a conspicuous lack of reliable methods for
evaluating them. Conventional image quality assessment methods and analytical
metrics (e.g. PSNR, SSIM, LPIPS etc.) only provide approximate indicators of
performance since they generalise the ability of the entire NeRF pipeline.
Hence, in this paper, we propose a new test framework which isolates the neural
rendering network from the NeRF pipeline and then performs a parametric
evaluation by training and evaluating the NeRF on an explicit radiance field
representation. We also introduce a configurable approach for generating
representations specifically for evaluation purposes. This employs ray-casting
to transform mesh models into explicit NeRF samples, as well as to ""shade""
these representations. Combining these two approaches, we demonstrate how
different ""tasks"" (scenes with different visual effects or learning strategies)
and types of networks (NeRFs and depth-wise implicit neural representations
(INRs)) can be evaluated within this framework. Additionally, we propose a
novel metric to measure task complexity of the framework which accounts for the
visual parameters and the distribution of the spatial data. Our approach offers
the potential to create a comparative objective evaluation framework for NeRF
methods.";Adrian Azzarelli<author:sep>Nantheera Anantrasirichai<author:sep>David R Bull;http://arxiv.org/pdf/2305.18079v3;cs.CV;9 pages, 2 main experiments, 2 additional experiments;nerf
2305.17916v2;http://arxiv.org/abs/2305.17916v2;2023-05-29;Volume Feature Rendering for Fast Neural Radiance Field Reconstruction;"Neural radiance fields (NeRFs) are able to synthesize realistic novel views
from multi-view images captured from distinct positions and perspectives. In
NeRF's rendering pipeline, neural networks are used to represent a scene
independently or transform queried learnable feature vector of a point to the
expected color or density. With the aid of geometry guides either in occupancy
grids or proposal networks, the number of neural network evaluations can be
reduced from hundreds to dozens in the standard volume rendering framework.
Instead of rendering yielded color after neural network evaluation, we propose
to render the queried feature vectors of a ray first and then transform the
rendered feature vector to the final pixel color by a neural network. This
fundamental change to the standard volume rendering framework requires only one
single neural network evaluation to render a pixel, which substantially lowers
the high computational complexity of the rendering framework attributed to a
large number of neural network evaluations. Consequently, we can use a
comparably larger neural network to achieve a better rendering quality while
maintaining the same training and rendering time costs. Our model achieves the
state-of-the-art rendering quality on both synthetic and real-world datasets
while requiring a training time of several minutes.";Kang Han<author:sep>Wei Xiang<author:sep>Lu Yu;http://arxiv.org/pdf/2305.17916v2;cs.CV;;nerf
2305.18163v1;http://arxiv.org/abs/2305.18163v1;2023-05-29;Compact Real-time Radiance Fields with Neural Codebook;"Reconstructing neural radiance fields with explicit volumetric
representations, demonstrated by Plenoxels, has shown remarkable advantages on
training and rendering efficiency, while grid-based representations typically
induce considerable overhead for storage and transmission. In this work, we
present a simple and effective framework for pursuing compact radiance fields
from the perspective of compression methodology. By exploiting intrinsic
properties exhibiting in grid models, a non-uniform compression stem is
developed to significantly reduce model complexity and a novel parameterized
module, named Neural Codebook, is introduced for better encoding high-frequency
details specific to per-scene models via a fast optimization. Our approach can
achieve over 40 $\times$ reduction on grid model storage with competitive
rendering quality. In addition, the method can achieve real-time rendering
speed with 180 fps, realizing significant advantage on storage cost compared to
real-time rendering methods.";Lingzhi Li<author:sep>Zhongshu Wang<author:sep>Zhen Shen<author:sep>Li Shen<author:sep>Ping Tan;http://arxiv.org/pdf/2305.18163v1;cs.CV;Accepted by ICME 2023;
2305.16914v4;http://arxiv.org/abs/2305.16914v4;2023-05-26;PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale  Scene Reconstruction;"Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images
and camera poses for Novel View Synthesis (NVS). Although NeRF can produce
photorealistic results, it often suffers from overfitting to training views,
leading to poor geometry reconstruction, especially in low-texture areas. This
limitation restricts many important applications which require accurate
geometry, such as extrapolated NVS, HD mapping and scene editing. To address
this limitation, we propose a new method to improve NeRF's 3D structure using
only RGB images and semantic maps. Our approach introduces a novel plane
regularization based on Singular Value Decomposition (SVD), that does not rely
on any geometric prior. In addition, we leverage the Structural Similarity
Index Measure (SSIM) in our loss design to properly initialize the volumetric
representation of NeRF. Quantitative and qualitative results show that our
method outperforms popular regularization approaches in accurate geometry
reconstruction for large-scale outdoor scenes and achieves SoTA rendering
quality on the KITTI-360 NVS benchmark.";Fusang Wang<author:sep>Arnaud Louys<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Luis RoldÃ£o<author:sep>Dzmitry Tsishkou;http://arxiv.org/pdf/2305.16914v4;cs.CV;Accepted to 3DV 2023;nerf
2305.16411v1;http://arxiv.org/abs/2305.16411v1;2023-05-25;ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image;"Recent advancements in text-to-image generation have enabled significant
progress in zero-shot 3D shape generation. This is achieved by score
distillation, a methodology that uses pre-trained text-to-image diffusion
models to optimize the parameters of a 3D neural presentation, e.g. Neural
Radiance Field (NeRF). While showing promising results, existing methods are
often not able to preserve the geometry of complex shapes, such as human
bodies. To address this challenge, we present ZeroAvatar, a method that
introduces the explicit 3D human body prior to the optimization process.
Specifically, we first estimate and refine the parameters of a parametric human
body from a single image. Then during optimization, we use the posed parametric
body as additional geometry constraint to regularize the diffusion model as
well as the underlying density field. Lastly, we propose a UV-guided texture
regularization term to further guide the completion of texture on invisible
body parts. We show that ZeroAvatar significantly enhances the robustness and
3D consistency of optimization-based image-to-3D avatar generation,
outperforming existing zero-shot image-to-3D methods.";Zhenzhen Weng<author:sep>Zeyu Wang<author:sep>Serena Yeung;http://arxiv.org/pdf/2305.16411v1;cs.CV;;nerf
2305.16233v1;http://arxiv.org/abs/2305.16233v1;2023-05-25;Interactive Segment Anything NeRF with Feature Imitation;"This paper investigates the potential of enhancing Neural Radiance Fields
(NeRF) with semantics to expand their applications. Although NeRF has been
proven useful in real-world applications like VR and digital creation, the lack
of semantics hinders interaction with objects in complex scenes. We propose to
imitate the backbone feature of off-the-shelf perception models to achieve
zero-shot semantic segmentation with NeRF. Our framework reformulates the
segmentation process by directly rendering semantic features and only applying
the decoder from perception models. This eliminates the need for expensive
backbones and benefits 3D consistency. Furthermore, we can project the learned
semantics onto extracted mesh surfaces for real-time interaction. With the
state-of-the-art Segment Anything Model (SAM), our framework accelerates
segmentation by 16 times with comparable mask quality. The experimental results
demonstrate the efficacy and computational advantages of our approach. Project
page: \url{https://me.kiui.moe/san/}.";Xiaokang Chen<author:sep>Jiaxiang Tang<author:sep>Diwen Wan<author:sep>Jingbo Wang<author:sep>Gang Zeng;http://arxiv.org/pdf/2305.16233v1;cs.CV;Technical Report;nerf
2305.16213v2;http://arxiv.org/abs/2305.16213v2;2023-05-25;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with  Variational Score Distillation;"Score distillation sampling (SDS) has shown great promise in text-to-3D
generation by distilling pretrained large-scale text-to-image diffusion models,
but suffers from over-saturation, over-smoothing, and low-diversity problems.
In this work, we propose to model the 3D parameter as a random variable instead
of a constant as in SDS and present variational score distillation (VSD), a
principled particle-based variational framework to explain and address the
aforementioned issues in text-to-3D generation. We show that SDS is a special
case of VSD and leads to poor samples with both small and large CFG weights. In
comparison, VSD works well with various CFG weights as ancestral sampling from
diffusion models and simultaneously improves the diversity and sample quality
with a common CFG weight (i.e., $7.5$). We further present various improvements
in the design space for text-to-3D such as distillation time schedule and
density initialization, which are orthogonal to the distillation algorithm yet
not well explored. Our overall approach, dubbed ProlificDreamer, can generate
high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with
rich structure and complex effects (e.g., smoke and drops). Further,
initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and
photo-realistic. Project page and codes:
https://ml.cs.tsinghua.edu.cn/prolificdreamer/";Zhengyi Wang<author:sep>Cheng Lu<author:sep>Yikai Wang<author:sep>Fan Bao<author:sep>Chongxuan Li<author:sep>Hang Su<author:sep>Jun Zhu;http://arxiv.org/pdf/2305.16213v2;cs.LG;NeurIPS 2023 (Spotlight);nerf
2305.14831v1;http://arxiv.org/abs/2305.14831v1;2023-05-24;OD-NeRF: Efficient Training of On-the-Fly Dynamic Neural Radiance Fields;"Dynamic neural radiance fields (dynamic NeRFs) have demonstrated impressive
results in novel view synthesis on 3D dynamic scenes. However, they often
require complete video sequences for training followed by novel view synthesis,
which is similar to playing back the recording of a dynamic 3D scene. In
contrast, we propose OD-NeRF to efficiently train and render dynamic NeRFs
on-the-fly which instead is capable of streaming the dynamic scene. When
training on-the-fly, the training frames become available sequentially and the
model is trained and rendered frame-by-frame. The key challenge of efficient
on-the-fly training is how to utilize the radiance field estimated from the
previous frames effectively. To tackle this challenge, we propose: 1) a NeRF
model conditioned on the multi-view projected colors to implicitly track
correspondence between the current and previous frames, and 2) a transition and
update algorithm that leverages the occupancy grid from the last frame to
sample efficiently at the current frame. Our algorithm can achieve an
interactive speed of 6FPS training and rendering on synthetic dynamic scenes
on-the-fly, and a significant speed-up compared to the state-of-the-art on
real-world dynamic scenes.";Zhiwen Yan<author:sep>Chen Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2305.14831v1;cs.CV;;nerf
2305.15094v1;http://arxiv.org/abs/2305.15094v1;2023-05-24;InpaintNeRF360: Text-Guided 3D Inpainting on Unbounded Neural Radiance  Fields;"Neural Radiance Fields (NeRF) can generate highly realistic novel views.
However, editing 3D scenes represented by NeRF across 360-degree views,
particularly removing objects while preserving geometric and photometric
consistency, remains a challenging problem due to NeRF's implicit scene
representation. In this paper, we propose InpaintNeRF360, a unified framework
that utilizes natural language instructions as guidance for inpainting
NeRF-based 3D scenes.Our approach employs a promptable segmentation model by
generating multi-modal prompts from the encoded text for multiview
segmentation. We apply depth-space warping to enforce viewing consistency in
the segmentations, and further refine the inpainted NeRF model using perceptual
priors to ensure visual plausibility. InpaintNeRF360 is capable of
simultaneously removing multiple objects or modifying object appearance based
on text instructions while synthesizing 3D viewing-consistent and
photo-realistic inpainting. Through extensive experiments on both unbounded and
frontal-facing scenes trained through NeRF, we demonstrate the effectiveness of
our approach and showcase its potential to enhance the editability of implicit
radiance fields.";Dongqing Wang<author:sep>Tong Zhang<author:sep>Alaa Abboud<author:sep>Sabine SÃ¼sstrunk;http://arxiv.org/pdf/2305.15094v1;cs.CV;;nerf
2305.15171v3;http://arxiv.org/abs/2305.15171v3;2023-05-24;Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations  from Diffusion Models;"We introduce Deceptive-NeRF, a novel methodology for few-shot NeRF
reconstruction, which leverages diffusion models to synthesize plausible
pseudo-observations to improve the reconstruction. This approach unfolds
through three key steps: 1) reconstructing a coarse NeRF from sparse input
data; 2) utilizing the coarse NeRF to render images and subsequently generating
pseudo-observations based on them; 3) training a refined NeRF model utilizing
input images augmented with pseudo-observations. We develop a deceptive
diffusion model that adeptly transitions RGB images and depth maps from coarse
NeRFs into photo-realistic pseudo-observations, all while preserving scene
semantics for reconstruction. Furthermore, we propose a progressive strategy
for training the Deceptive-NeRF, using the current NeRF renderings to create
pseudo-observations that enhance the next iteration's NeRF. Extensive
experiments demonstrate that our approach is capable of synthesizing
photo-realistic novel views, even for highly complex scenes with very sparse
inputs. Codes will be released.";Xinhang Liu<author:sep>Jiaben Chen<author:sep>Shiu-hong Kao<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2305.15171v3;cs.CV;;nerf
2305.14093v4;http://arxiv.org/abs/2305.14093v4;2023-05-23;Weakly Supervised 3D Open-vocabulary Segmentation;"Open-vocabulary segmentation of 3D scenes is a fundamental function of human
perception and thus a crucial objective in computer vision research. However,
this task is heavily impeded by the lack of large-scale and diverse 3D
open-vocabulary segmentation datasets for training robust and generalizable
models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation
models helps but it compromises the open-vocabulary feature as the 2D models
are mostly finetuned with close-vocabulary datasets. We tackle the challenges
in 3D open-vocabulary segmentation by exploiting pre-trained foundation models
CLIP and DINO in a weakly supervised manner. Specifically, given only the
open-vocabulary text descriptions of the objects in a scene, we distill the
open-vocabulary multimodal knowledge and object reasoning capability of CLIP
and DINO into a neural radiance field (NeRF), which effectively lifts 2D
features into view-consistent 3D segmentation. A notable aspect of our approach
is that it does not require any manual segmentation annotations for either the
foundation models or the distillation process. Extensive experiments show that
our method even outperforms fully supervised models trained with segmentation
annotations in certain scenes, suggesting that 3D open-vocabulary segmentation
can be effectively learned from 2D images and text-image pairs. Code is
available at \url{https://github.com/Kunhao-Liu/3D-OVS}.";Kunhao Liu<author:sep>Fangneng Zhan<author:sep>Jiahui Zhang<author:sep>Muyu Xu<author:sep>Yingchen Yu<author:sep>Abdulmotaleb El Saddik<author:sep>Christian Theobalt<author:sep>Eric Xing<author:sep>Shijian Lu;http://arxiv.org/pdf/2305.14093v4;cs.CV;Accepted to NeurIPS 2023;nerf
2305.12843v1;http://arxiv.org/abs/2305.12843v1;2023-05-22;Registering Neural Radiance Fields as 3D Density Images;"No significant work has been done to directly merge two partially overlapping
scenes using NeRF representations. Given pre-trained NeRF models of a 3D scene
with partial overlapping, this paper aligns them with a rigid transform, by
generalizing the traditional registration pipeline, that is, key point
detection and point set registration, to operate on 3D density fields. To
describe corner points as key points in 3D, we propose to use universal
pre-trained descriptor-generating neural networks that can be trained and
tested on different scenes. We perform experiments to demonstrate that the
descriptor networks can be conveniently trained using a contrastive learning
strategy. We demonstrate that our method, as a global approach, can effectively
register NeRF models, thus making possible future large-scale NeRF construction
by registering its smaller and overlapping NeRFs captured individually.";Han Jiang<author:sep>Ruoxuan Li<author:sep>Haosen Sun<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2305.12843v1;cs.CV;;nerf
2305.13307v1;http://arxiv.org/abs/2305.13307v1;2023-05-22;NeRFuser: Large-Scale Scene Representation by NeRF Fusion;"A practical benefit of implicit visual representations like Neural Radiance
Fields (NeRFs) is their memory efficiency: large scenes can be efficiently
stored and shared as small neural nets instead of collections of images.
However, operating on these implicit visual data structures requires extending
classical image-based vision techniques (e.g., registration, blending) from
image sets to neural fields. Towards this goal, we propose NeRFuser, a novel
architecture for NeRF registration and blending that assumes only access to
pre-generated NeRFs, and not the potentially large sets of images used to
generate them. We propose registration from re-rendering, a technique to infer
the transformation between NeRFs based on images synthesized from individual
NeRFs. For blending, we propose sample-based inverse distance weighting to
blend visual information at the ray-sample level. We evaluate NeRFuser on
public benchmarks and a self-collected object-centric indoor dataset, showing
the robustness of our method, including to views that are challenging to render
from the individual source NeRFs.";Jiading Fang<author:sep>Shengjie Lin<author:sep>Igor Vasiljevic<author:sep>Vitor Guizilini<author:sep>Rares Ambrus<author:sep>Adrien Gaidon<author:sep>Gregory Shakhnarovich<author:sep>Matthew R. Walter;http://arxiv.org/pdf/2305.13307v1;cs.CV;Code available at https://github.com/ripl/nerfuser;nerf
2305.11588v1;http://arxiv.org/abs/2305.11588v1;2023-05-19;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields;"Text-driven 3D scene generation is widely applicable to video gaming, film
industry, and metaverse applications that have a large demand for 3D scenes.
However, existing text-to-3D generation methods are limited to producing 3D
objects with simple geometries and dreamlike styles that lack realism. In this
work, we present Text2NeRF, which is able to generate a wide range of 3D scenes
with complicated geometric structures and high-fidelity textures purely from a
text prompt. To this end, we adopt NeRF as the 3D representation and leverage a
pre-trained text-to-image diffusion model to constrain the 3D reconstruction of
the NeRF to reflect the scene description. Specifically, we employ the
diffusion model to infer the text-related image as the content prior and use a
monocular depth estimation method to offer the geometric prior. Both content
and geometric priors are utilized to update the NeRF model. To guarantee
textured and geometric consistency between different views, we introduce a
progressive scene inpainting and updating strategy for novel view synthesis of
the scene. Our method requires no additional training data but only a natural
language description of the scene as the input. Extensive experiments
demonstrate that our Text2NeRF outperforms existing methods in producing
photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of
natural language prompts.";Jingbo Zhang<author:sep>Xiaoyu Li<author:sep>Ziyu Wan<author:sep>Can Wang<author:sep>Jing Liao;http://arxiv.org/pdf/2305.11588v1;cs.CV;Homepage: https://eckertzhang.github.io/Text2NeRF.github.io/;nerf
2305.11167v1;http://arxiv.org/abs/2305.11167v1;2023-05-18;MVPSNet: Fast Generalizable Multi-view Photometric Stereo;"We propose a fast and generalizable solution to Multi-view Photometric Stereo
(MVPS), called MVPSNet. The key to our approach is a feature extraction network
that effectively combines images from the same view captured under multiple
lighting conditions to extract geometric features from shading cues for stereo
matching. We demonstrate these features, termed `Light Aggregated Feature Maps'
(LAFM), are effective for feature matching even in textureless regions, where
traditional multi-view stereo methods fail. Our method produces similar
reconstruction results to PS-NeRF, a state-of-the-art MVPS method that
optimizes a neural network per-scene, while being 411$\times$ faster (105
seconds vs. 12 hours) in inference. Additionally, we introduce a new synthetic
dataset for MVPS, sMVPS, which is shown to be effective to train a
generalizable MVPS method.";Dongxu Zhao<author:sep>Daniel Lichy<author:sep>Pierre-Nicolas Perrin<author:sep>Jan-Michael Frahm<author:sep>Soumyadip Sengupta;http://arxiv.org/pdf/2305.11167v1;cs.CV;;nerf
2305.11031v1;http://arxiv.org/abs/2305.11031v1;2023-05-18;ConsistentNeRF: Enhancing Neural Radiance Fields with 3D Consistency for  Sparse View Synthesis;"Neural Radiance Fields (NeRF) has demonstrated remarkable 3D reconstruction
capabilities with dense view images. However, its performance significantly
deteriorates under sparse view settings. We observe that learning the 3D
consistency of pixels among different views is crucial for improving
reconstruction quality in such cases. In this paper, we propose ConsistentNeRF,
a method that leverages depth information to regularize both multi-view and
single-view 3D consistency among pixels. Specifically, ConsistentNeRF employs
depth-derived geometry information and a depth-invariant loss to concentrate on
pixels that exhibit 3D correspondence and maintain consistent depth
relationships. Extensive experiments on recent representative works reveal that
our approach can considerably enhance model performance in sparse view
conditions, achieving improvements of up to 94% in PSNR, 76% in SSIM, and 31%
in LPIPS compared to the vanilla baselines across various benchmarks, including
DTU, NeRF Synthetic, and LLFF.";Shoukang Hu<author:sep>Kaichen Zhou<author:sep>Kaiyu Li<author:sep>Longhui Yu<author:sep>Lanqing Hong<author:sep>Tianyang Hu<author:sep>Zhenguo Li<author:sep>Gim Hee Lee<author:sep>Ziwei Liu;http://arxiv.org/pdf/2305.11031v1;cs.CV;https://github.com/skhu101/ConsistentNeRF;nerf
2305.10579v2;http://arxiv.org/abs/2305.10579v2;2023-05-17;MultiPlaneNeRF: Neural Radiance Field with Non-Trainable Representation;"NeRF is a popular model that efficiently represents 3D objects from 2D
images. However, vanilla NeRF has some important limitations. NeRF must be
trained on each object separately. The training time is long since we encode
the object's shape and color in neural network weights. Moreover, NeRF does not
generalize well to unseen data. In this paper, we present MultiPlaneNeRF -- a
model that simultaneously solves the above problems. Our model works directly
on 2D images. We project 3D points on 2D images to produce non-trainable
representations. The projection step is not parametrized and a very shallow
decoder can efficiently process the representation. Furthermore, we can train
MultiPlaneNeRF on a large data set and force our implicit decoder to generalize
across many objects. Consequently, we can only replace the 2D images (without
additional training) to produce a NeRF representation of the new object. In the
experimental section, we demonstrate that MultiPlaneNeRF achieves results
comparable to state-of-the-art models for synthesizing new views and has
generalization properties. Additionally, MultiPlane decoder can be used as a
component in large generative models like GANs.";Dominik Zimny<author:sep>Artur Kasymov<author:sep>Adam Kania<author:sep>Jacek Tabor<author:sep>Maciej ZiÄba<author:sep>PrzemysÅaw Spurek;http://arxiv.org/pdf/2305.10579v2;cs.CV;;nerf
2305.10503v3;http://arxiv.org/abs/2305.10503v3;2023-05-17;OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation  with Neural Radiance Fields;"The emergence of Neural Radiance Fields (NeRF) for novel view synthesis has
increased interest in 3D scene editing. An essential task in editing is
removing objects from a scene while ensuring visual reasonability and multiview
consistency. However, current methods face challenges such as time-consuming
object labeling, limited capability to remove specific targets, and compromised
rendering quality after removal. This paper proposes a novel object-removing
pipeline, named OR-NeRF, that can remove objects from 3D scenes with user-given
points or text prompts on a single view, achieving better performance in less
time than previous works. Our method spreads user annotations to all views
through 3D geometry and sparse correspondence, ensuring 3D consistency with
less processing burden. Then recent 2D segmentation model Segment-Anything
(SAM) is applied to predict masks, and a 2D inpainting model is used to
generate color supervision. Finally, our algorithm applies depth supervision
and perceptual loss to maintain consistency in geometry and appearance after
object removal. Experimental results demonstrate that our method achieves
better editing quality with less time than previous works, considering both
quality and quantity.";Youtan Yin<author:sep>Zhoujie Fu<author:sep>Fan Yang<author:sep>Guosheng Lin;http://arxiv.org/pdf/2305.10503v3;cs.CV;project site: https://ornerf.github.io/ (codes available);nerf
2305.09761v1;http://arxiv.org/abs/2305.09761v1;2023-05-16;NerfBridge: Bringing Real-time, Online Neural Radiance Field Training to  Robotics;"This work was presented at the IEEE International Conference on Robotics and
Automation 2023 Workshop on Unconventional Spatial Representations.
  Neural radiance fields (NeRFs) are a class of implicit scene representations
that model 3D environments from color images. NeRFs are expressive, and can
model the complex and multi-scale geometry of real world environments, which
potentially makes them a powerful tool for robotics applications. Modern NeRF
training libraries can generate a photo-realistic NeRF from a static data set
in just a few seconds, but are designed for offline use and require a slow pose
optimization pre-computation step.
  In this work we propose NerfBridge, an open-source bridge between the Robot
Operating System (ROS) and the popular Nerfstudio library for real-time, online
training of NeRFs from a stream of images. NerfBridge enables rapid development
of research on applications of NeRFs in robotics by providing an extensible
interface to the efficient training pipelines and model libraries provided by
Nerfstudio. As an example use case we outline a hardware setup that can be used
NerfBridge to train a NeRF from images captured by a camera mounted to a
quadrotor in both indoor and outdoor environments.
  For accompanying video https://youtu.be/EH0SLn-RcDg and code
https://github.com/javieryu/nerf_bridge.";Javier Yu<author:sep>Jun En Low<author:sep>Keiko Nagami<author:sep>Mac Schwager;http://arxiv.org/pdf/2305.09761v1;cs.RO;;nerf
2305.08851v3;http://arxiv.org/abs/2305.08851v3;2023-05-15;MV-Map: Offboard HD-Map Generation with Multi-view Consistency;"While bird's-eye-view (BEV) perception models can be useful for building
high-definition maps (HD-Maps) with less human labor, their results are often
unreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps
from different viewpoints. This is because BEV perception is typically set up
in an 'onboard' manner, which restricts the computation and consequently
prevents algorithms from reasoning multiple views simultaneously. This paper
overcomes these limitations and advocates a more practical 'offboard' HD-Map
generation setup that removes the computation constraints, based on the fact
that HD-Maps are commonly reusable infrastructures built offline in data
centers. To this end, we propose a novel offboard pipeline called MV-Map that
capitalizes multi-view consistency and can handle an arbitrary number of frames
with the key design of a 'region-centric' framework. In MV-Map, the target
HD-Maps are created by aggregating all the frames of onboard predictions,
weighted by the confidence scores assigned by an 'uncertainty network'. To
further enhance multi-view consistency, we augment the uncertainty network with
the global 3D structure optimized by a voxelized neural radiance field
(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map
significantly improves the quality of HD-Maps, further highlighting the
importance of offboard methods for HD-Map generation.";Ziyang Xie<author:sep>Ziqi Pang<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2305.08851v3;cs.CV;ICCV 2023;nerf
2305.08552v1;http://arxiv.org/abs/2305.08552v1;2023-05-15;Curvature-Aware Training for Coordinate Networks;"Coordinate networks are widely used in computer vision due to their ability
to represent signals as compressed, continuous entities. However, training
these networks with first-order optimizers can be slow, hindering their use in
real-time applications. Recent works have opted for shallow voxel-based
representations to achieve faster training, but this sacrifices memory
efficiency. This work proposes a solution that leverages second-order
optimization methods to significantly reduce training times for coordinate
networks while maintaining their compressibility. Experiments demonstrate the
effectiveness of this approach on various signal modalities, such as audio,
images, videos, shape reconstruction, and neural radiance fields.";Hemanth Saratchandran<author:sep>Shin-Fang Chng<author:sep>Sameera Ramasinghe<author:sep>Lachlan MacDonald<author:sep>Simon Lucey;http://arxiv.org/pdf/2305.08552v1;cs.CV;;
2305.07342v1;http://arxiv.org/abs/2305.07342v1;2023-05-12;BundleRecon: Ray Bundle-Based 3D Neural Reconstruction;"With the growing popularity of neural rendering, there has been an increasing
number of neural implicit multi-view reconstruction methods. While many models
have been enhanced in terms of positional encoding, sampling, rendering, and
other aspects to improve the reconstruction quality, current methods do not
fully leverage the information among neighboring pixels during the
reconstruction process. To address this issue, we propose an enhanced model
called BundleRecon. In the existing approaches, sampling is performed by a
single ray that corresponds to a single pixel. In contrast, our model samples a
patch of pixels using a bundle of rays, which incorporates information from
neighboring pixels. Furthermore, we design bundle-based constraints to further
improve the reconstruction quality. Experimental results demonstrate that
BundleRecon is compatible with the existing neural implicit multi-view
reconstruction methods and can improve their reconstruction quality.";Weikun Zhang<author:sep>Jianke Zhu;http://arxiv.org/pdf/2305.07342v1;cs.CV;CVPR 2023 workshop XRNeRF: Advances in NeRF for the Metaverse;
2305.07024v1;http://arxiv.org/abs/2305.07024v1;2023-05-11;SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input  Views;"We study to generate novel views of indoor scenes given sparse input views.
The challenge is to achieve both photorealism and view consistency. We present
SparseGNV: a learning framework that incorporates 3D structures and image
generative models to generate novel views with three modules. The first module
builds a neural point cloud as underlying geometry, providing contextual
information and guidance for the target novel view. The second module utilizes
a transformer-based network to map the scene context and the guidance into a
shared latent space and autoregressively decodes the target view in the form of
discrete image tokens. The third module reconstructs the tokens into the image
of the target view. SparseGNV is trained across a large indoor scene dataset to
learn generalizable priors. Once trained, it can efficiently generate novel
views of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNV
on both real-world and synthetic indoor scenes and demonstrate that it
outperforms state-of-the-art methods based on either neural radiance fields or
conditional image generation.";Weihao Cheng<author:sep>Yan-Pei Cao<author:sep>Ying Shan;http://arxiv.org/pdf/2305.07024v1;cs.CV;10 pages, 6 figures;
2305.06118v2;http://arxiv.org/abs/2305.06118v2;2023-05-10;NeRF2: Neural Radio-Frequency Radiance Fields;"Although Maxwell discovered the physical laws of electromagnetic waves 160
years ago, how to precisely model the propagation of an RF signal in an
electrically large and complex environment remains a long-standing problem. The
difficulty is in the complex interactions between the RF signal and the
obstacles (e.g., reflection, diffraction, etc.). Inspired by the great success
of using a neural network to describe the optical field in computer vision, we
propose a neural radio-frequency radiance field, NeRF$^\textbf{2}$, which
represents a continuous volumetric scene function that makes sense of an RF
signal's propagation. Particularly, after training with a few signal
measurements, NeRF$^\textbf{2}$ can tell how/what signal is received at any
position when it knows the position of a transmitter. As a physical-layer
neural network, NeRF$^\textbf{2}$ can take advantage of the learned statistic
model plus the physical model of ray tracing to generate a synthetic dataset
that meets the training demands of application-layer artificial neural networks
(ANNs). Thus, we can boost the performance of ANNs by the proposed
turbo-learning, which mixes the true and synthetic datasets to intensify the
training. Our experiment results show that turbo-learning can enhance
performance with an approximate 50% increase. We also demonstrate the power of
NeRF$^\textbf{2}$ in the field of indoor localization and 5G MIMO.";Xiaopeng Zhao<author:sep>Zhenlin An<author:sep>Qingrui Pan<author:sep>Lei Yang;http://arxiv.org/pdf/2305.06118v2;cs.NI;;nerf
2305.06131v2;http://arxiv.org/abs/2305.06131v2;2023-05-10;Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era;"Generative AI (AIGC, a.k.a. AI generated content) has made remarkable
progress in the past few years, among which text-guided content generation is
the most practical one since it enables the interaction between human
instruction and AIGC. Due to the development in text-to-image as well 3D
modeling technologies (like NeRF), text-to-3D has become a newly emerging yet
highly active research field. Our work conducts the first yet comprehensive
survey on text-to-3D to help readers interested in this direction quickly catch
up with its fast development. First, we introduce 3D data representations,
including both Euclidean data and non-Euclidean data. On top of that, we
introduce various foundation technologies as well as summarize how recent works
combine those foundation technologies to realize satisfactory text-to-3D.
Moreover, we summarize how text-to-3D technology is used in various
applications, including avatar generation, texture generation, shape
transformation, and scene generation.";Chenghao Li<author:sep>Chaoning Zhang<author:sep>Atish Waghwase<author:sep>Lik-Hang Lee<author:sep>Francois Rameau<author:sep>Yang Yang<author:sep>Sung-Ho Bae<author:sep>Choong Seon Hong;http://arxiv.org/pdf/2305.06131v2;cs.CV;;nerf
2305.06356v2;http://arxiv.org/abs/2305.06356v2;2023-05-10;HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion;"Representing human performance at high-fidelity is an essential building
block in diverse applications, such as film production, computer games or
videoconferencing. To close the gap to production-level quality, we introduce
HumanRF, a 4D dynamic neural scene representation that captures full-body
appearance in motion from multi-view video input, and enables playback from
novel, unseen viewpoints. Our novel representation acts as a dynamic video
encoding that captures fine details at high compression rates by factorizing
space-time into a temporal matrix-vector decomposition. This allows us to
obtain temporally coherent reconstructions of human actors for long sequences,
while representing high-resolution details even in the context of challenging
motion. While most research focuses on synthesizing at resolutions of 4MP or
lower, we address the challenge of operating at 12MP. To this end, we introduce
ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160
cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions. We
demonstrate challenges that emerge from using such high-resolution data and
show that our newly introduced HumanRF effectively leverages this data, making
a significant step towards production-level quality novel view synthesis.";Mustafa IÅÄ±k<author:sep>Martin RÃ¼nz<author:sep>Markos Georgopoulos<author:sep>Taras Khakhulin<author:sep>Jonathan Starck<author:sep>Lourdes Agapito<author:sep>Matthias NieÃner;http://arxiv.org/pdf/2305.06356v2;cs.CV;"Project webpage: https://synthesiaresearch.github.io/humanrf Dataset
  webpage: https://www.actors-hq.com/ Video:
  https://www.youtube.com/watch?v=OTnhiLLE7io Code:
  https://github.com/synthesiaresearch/humanrf";
2305.05594v1;http://arxiv.org/abs/2305.05594v1;2023-05-09;PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces;"A signed distance function (SDF) parametrized by an MLP is a common
ingredient of neural surface reconstruction. We build on the successful recent
method NeuS to extend it by three new components. The first component is to
borrow the tri-plane representation from EG3D and represent signed distance
fields as a mixture of tri-planes and MLPs instead of representing it with MLPs
only. Using tri-planes leads to a more expressive data structure but will also
introduce noise in the reconstructed surface. The second component is to use a
new type of positional encoding with learnable weights to combat noise in the
reconstruction process. We divide the features in the tri-plane into multiple
frequency scales and modulate them with sin and cos functions of different
frequencies. The third component is to use learnable convolution operations on
the tri-plane features using self-attention convolution to produce features
with different frequency bands. The experiments show that PET-NeuS achieves
high-fidelity surface reconstruction on standard datasets. Following previous
work and using the Chamfer metric as the most important way to measure surface
reconstruction quality, we are able to improve upon the NeuS baseline by 57% on
Nerf-synthetic (0.84 compared to 1.97) and by 15.5% on DTU (0.71 compared to
0.84). The qualitative evaluation reveals how our method can better control the
interference of high-frequency noise. Code available at
\url{https://github.com/yiqun-wang/PET-NeuS}.";Yiqun Wang<author:sep>Ivan Skorokhodov<author:sep>Peter Wonka;http://arxiv.org/pdf/2305.05594v1;cs.CV;"CVPR 2023; 20 Pages; Project page:
  \url{https://github.com/yiqun-wang/PET-NeuS}";nerf
2305.05766v1;http://arxiv.org/abs/2305.05766v1;2023-05-09;Instant-NeRF: Instant On-Device Neural Radiance Field Training via  Algorithm-Accelerator Co-Designed Near-Memory Processing;"Instant on-device Neural Radiance Fields (NeRFs) are in growing demand for
unleashing the promise of immersive AR/VR experiences, but are still limited by
their prohibitive training time. Our profiling analysis reveals a memory-bound
inefficiency in NeRF training. To tackle this inefficiency, near-memory
processing (NMP) promises to be an effective solution, but also faces
challenges due to the unique workloads of NeRFs, including the random hash
table lookup, random point processing sequence, and heterogeneous bottleneck
steps. Therefore, we propose the first NMP framework, Instant-NeRF, dedicated
to enabling instant on-device NeRF training. Experiments on eight datasets
consistently validate the effectiveness of Instant-NeRF.";Yang Zhao<author:sep>Shang Wu<author:sep>Jingqun Zhang<author:sep>Sixu Li<author:sep>Chaojian Li<author:sep>Yingyan Lin;http://arxiv.org/pdf/2305.05766v1;cs.CV;Accepted by DAC 2023;nerf
2305.04789v1;http://arxiv.org/abs/2305.04789v1;2023-05-08;AvatarReX: Real-time Expressive Full-body Avatars;"We present AvatarReX, a new method for learning NeRF-based full-body avatars
from video data. The learnt avatar not only provides expressive control of the
body, hands and the face together, but also supports real-time animation and
rendering. To this end, we propose a compositional avatar representation, where
the body, hands and the face are separately modeled in a way that the
structural prior from parametric mesh templates is properly utilized without
compromising representation flexibility. Furthermore, we disentangle the
geometry and appearance for each part. With these technical designs, we propose
a dedicated deferred rendering pipeline, which can be executed in real-time
framerate to synthesize high-quality free-view images. The disentanglement of
geometry and appearance also allows us to design a two-pass training strategy
that combines volume rendering and surface rendering for network training. In
this way, patch-level supervision can be applied to force the network to learn
sharp appearance details on the basis of geometry estimation. Overall, our
method enables automatic construction of expressive full-body avatars with
real-time rendering capability, and can generate photo-realistic images with
dynamic details for novel body motions and facial expressions.";Zerong Zheng<author:sep>Xiaochen Zhao<author:sep>Hongwen Zhang<author:sep>Boning Liu<author:sep>Yebin Liu;http://arxiv.org/pdf/2305.04789v1;cs.CV;"To appear in SIGGRAPH 2023 Journal Track. Project page at
  https://liuyebin.com/AvatarRex/";nerf
2305.04966v2;http://arxiv.org/abs/2305.04966v2;2023-05-08;NerfAcc: Efficient Sampling Accelerates NeRFs;"Optimizing and rendering Neural Radiance Fields is computationally expensive
due to the vast number of samples required by volume rendering. Recent works
have included alternative sampling approaches to help accelerate their methods,
however, they are often not the focus of the work. In this paper, we
investigate and compare multiple sampling approaches and demonstrate that
improved sampling is generally applicable across NeRF variants under an unified
concept of transmittance estimator. To facilitate future experiments, we
develop NerfAcc, a Python toolbox that provides flexible APIs for incorporating
advanced sampling methods into NeRF related methods. We demonstrate its
flexibility by showing that it can reduce the training time of several recent
NeRF methods by 1.5x to 20x with minimal modifications to the existing
codebase. Additionally, highly customized NeRFs, such as Instant-NGP, can be
implemented in native PyTorch using NerfAcc.";Ruilong Li<author:sep>Hang Gao<author:sep>Matthew Tancik<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2305.04966v2;cs.CV;Website: https://www.nerfacc.com;nerf
2305.04296v1;http://arxiv.org/abs/2305.04296v1;2023-05-07;HashCC: Lightweight Method to Improve the Quality of the Camera-less  NeRF Scene Generation;"Neural Radiance Fields has become a prominent method of scene generation via
view synthesis. A critical requirement for the original algorithm to learn
meaningful scene representation is camera pose information for each image in a
data set. Current approaches try to circumnavigate this assumption with
moderate success, by learning approximate camera positions alongside learning
neural representations of a scene. This requires complicated camera models,
causing a long and complicated training process, or results in a lack of
texture and sharp details in rendered scenes. In this work we introduce Hash
Color Correction (HashCC) -- a lightweight method for improving Neural Radiance
Fields rendered image quality, applicable also in situations where camera
positions for a given set of images are unknown.";Jan Olszewski;http://arxiv.org/pdf/2305.04296v1;cs.CV;;nerf
2305.04268v1;http://arxiv.org/abs/2305.04268v1;2023-05-07;Multi-Space Neural Radiance Fields;"Existing Neural Radiance Fields (NeRF) methods suffer from the existence of
reflective objects, often resulting in blurry or distorted rendering. Instead
of calculating a single radiance field, we propose a multi-space neural
radiance field (MS-NeRF) that represents the scene using a group of feature
fields in parallel sub-spaces, which leads to a better understanding of the
neural network toward the existence of reflective and refractive objects. Our
multi-space scheme works as an enhancement to existing NeRF methods, with only
small computational overheads needed for training and inferring the extra-space
outputs. We demonstrate the superiority and compatibility of our approach using
three representative NeRF-based models, i.e., NeRF, Mip-NeRF, and Mip-NeRF 360.
Comparisons are performed on a novelly constructed dataset consisting of 25
synthetic scenes and 7 real captured scenes with complex reflection and
refraction, all having 360-degree viewpoints. Extensive experiments show that
our approach significantly outperforms the existing single-space NeRF methods
for rendering high-quality scenes concerned with complex light paths through
mirror-like objects. Our code and dataset will be publicly available at
https://zx-yin.github.io/msnerf.";Ze-Xin Yin<author:sep>Jiaxiong Qiu<author:sep>Ming-Ming Cheng<author:sep>Bo Ren;http://arxiv.org/pdf/2305.04268v1;cs.CV;CVPR 2023, 10 pages, 12 figures;nerf
2305.03462v2;http://arxiv.org/abs/2305.03462v2;2023-05-05;General Neural Gauge Fields;"The recent advance of neural fields, such as neural radiance fields, has
significantly pushed the boundary of scene representation learning. Aiming to
boost the computation efficiency and rendering quality of 3D scenes, a popular
line of research maps the 3D coordinate system to another measuring system,
e.g., 2D manifolds and hash tables, for modeling neural fields. The conversion
of coordinate systems can be typically dubbed as \emph{gauge transformation},
which is usually a pre-defined mapping function, e.g., orthogonal projection or
spatial hash function. This begs a question: can we directly learn a desired
gauge transformation along with the neural field in an end-to-end manner? In
this work, we extend this problem to a general paradigm with a taxonomy of
discrete \& continuous cases, and develop a learning framework to jointly
optimize gauge transformations and neural fields. To counter the problem that
the learning of gauge transformations can collapse easily, we derive a general
regularization mechanism from the principle of information conservation during
the gauge transformation. To circumvent the high computation cost in gauge
learning with regularization, we directly derive an information-invariant gauge
transformation which allows to preserve scene information inherently and yield
superior performance. Project: https://fnzhan.com/Neural-Gauge-Fields";Fangneng Zhan<author:sep>Lingjie Liu<author:sep>Adam Kortylewski<author:sep>Christian Theobalt;http://arxiv.org/pdf/2305.03462v2;cs.CV;ICLR 2023;
2305.03043v1;http://arxiv.org/abs/2305.03043v1;2023-05-04;Single-Shot Implicit Morphable Faces with Consistent Texture  Parameterization;"There is a growing demand for the accessible creation of high-quality 3D
avatars that are animatable and customizable. Although 3D morphable models
provide intuitive control for editing and animation, and robustness for
single-view face reconstruction, they cannot easily capture geometric and
appearance details. Methods based on neural implicit representations, such as
signed distance functions (SDF) or neural radiance fields, approach
photo-realism, but are difficult to animate and do not generalize well to
unseen data. To tackle this problem, we propose a novel method for constructing
implicit 3D morphable face models that are both generalizable and intuitive for
editing. Trained from a collection of high-quality 3D scans, our face model is
parameterized by geometry, expression, and texture latent codes with a learned
SDF and explicit UV texture parameterization. Once trained, we can reconstruct
an avatar from a single in-the-wild image by leveraging the learned prior to
project the image into the latent space of our model. Our implicit morphable
face models can be used to render an avatar from novel views, animate facial
expressions by modifying expression codes, and edit textures by directly
painting on the learned UV-texture maps. We demonstrate quantitatively and
qualitatively that our method improves upon photo-realism, geometry, and
expression accuracy compared to state-of-the-art methods.";Connor Z. Lin<author:sep>Koki Nagano<author:sep>Jan Kautz<author:sep>Eric R. Chan<author:sep>Umar Iqbal<author:sep>Leonidas Guibas<author:sep>Gordon Wetzstein<author:sep>Sameh Khamis;http://arxiv.org/pdf/2305.03043v1;cs.CV;"SIGGRAPH 2023, Project Page:
  https://research.nvidia.com/labs/toronto-ai/ssif";
2305.02756v2;http://arxiv.org/abs/2305.02756v2;2023-05-04;Floaters No More: Radiance Field Gradient Scaling for Improved  Near-Camera Training;"NeRF acquisition typically requires careful choice of near planes for the
different cameras or suffers from background collapse, creating floating
artifacts on the edges of the captured scene. The key insight of this work is
that background collapse is caused by a higher density of samples in regions
near cameras. As a result of this sampling imbalance, near-camera volumes
receive significantly more gradients, leading to incorrect density buildup. We
propose a gradient scaling approach to counter-balance this sampling imbalance,
removing the need for near planes, while preventing background collapse. Our
method can be implemented in a few lines, does not induce any significant
overhead, and is compatible with most NeRF implementations.";Julien Philip<author:sep>Valentin Deschaintre;http://arxiv.org/pdf/2305.02756v2;cs.CV;EGSR 2023;nerf
2305.02618v1;http://arxiv.org/abs/2305.02618v1;2023-05-04;Semantic-aware Generation of Multi-view Portrait Drawings;"Neural radiance fields (NeRF) based methods have shown amazing performance in
synthesizing 3D-consistent photographic images, but fail to generate multi-view
portrait drawings. The key is that the basic assumption of these methods -- a
surface point is consistent when rendered from different views -- doesn't hold
for drawings. In a portrait drawing, the appearance of a facial point may
changes when viewed from different angles. Besides, portrait drawings usually
present little 3D information and suffer from insufficient training data. To
combat this challenge, in this paper, we propose a Semantic-Aware GEnerator
(SAGE) for synthesizing multi-view portrait drawings. Our motivation is that
facial semantic labels are view-consistent and correlate with drawing
techniques. We therefore propose to collaboratively synthesize multi-view
semantic maps and the corresponding portrait drawings. To facilitate training,
we design a semantic-aware domain translator, which generates portrait drawings
based on features of photographic faces. In addition, use data augmentation via
synthesis to mitigate collapsed results. We apply SAGE to synthesize multi-view
portrait drawings in diverse artistic styles. Experimental results show that
SAGE achieves significantly superior or highly competitive performance,
compared to existing 3D-aware image synthesis methods. The codes are available
at https://github.com/AiArt-HDU/SAGE.";Biao Ma<author:sep>Fei Gao<author:sep>Chang Jiang<author:sep>Nannan Wang<author:sep>Gang Xu;http://arxiv.org/pdf/2305.02618v1;cs.CV;Accepted by IJCAI 2023;nerf
2305.03049v1;http://arxiv.org/abs/2305.03049v1;2023-05-04;NeuralEditor: Editing Neural Radiance Fields via Manipulating Point  Clouds;"This paper proposes NeuralEditor that enables neural radiance fields (NeRFs)
natively editable for general shape editing tasks. Despite their impressive
results on novel-view synthesis, it remains a fundamental challenge for NeRFs
to edit the shape of the scene. Our key insight is to exploit the explicit
point cloud representation as the underlying structure to construct NeRFs,
inspired by the intuitive interpretation of NeRF rendering as a process that
projects or ""plots"" the associated 3D point cloud to a 2D image plane. To this
end, NeuralEditor introduces a novel rendering scheme based on deterministic
integration within K-D tree-guided density-adaptive voxels, which produces both
high-quality rendering results and precise point clouds through optimization.
NeuralEditor then performs shape editing via mapping associated points between
point clouds. Extensive evaluation shows that NeuralEditor achieves
state-of-the-art performance in both shape deformation and scene morphing
tasks. Notably, NeuralEditor supports both zero-shot inference and further
fine-tuning over the edited scene. Our code, benchmark, and demo video are
available at https://immortalco.github.io/NeuralEditor.";Jun-Kun Chen<author:sep>Jipeng Lyu<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2305.03049v1;cs.CV;CVPR 2023;nerf
2305.03027v1;http://arxiv.org/abs/2305.03027v1;2023-05-04;NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads;"We focus on reconstructing high-fidelity radiance fields of human heads,
capturing their animations over time, and synthesizing re-renderings from novel
viewpoints at arbitrary time steps. To this end, we propose a new multi-view
capture setup composed of 16 calibrated machine vision cameras that record
time-synchronized images at 7.1 MP resolution and 73 frames per second. With
our setup, we collect a new dataset of over 4700 high-resolution,
high-framerate sequences of more than 220 human heads, from which we introduce
a new human head reconstruction benchmark. The recorded sequences cover a wide
range of facial dynamics, including head motions, natural expressions,
emotions, and spoken language. In order to reconstruct high-fidelity human
heads, we propose Dynamic Neural Radiance Fields using Hash Ensembles
(NeRSemble). We represent scene dynamics by combining a deformation field and
an ensemble of 3D multi-resolution hash encodings. The deformation field allows
for precise modeling of simple scene movements, while the ensemble of hash
encodings helps to represent complex dynamics. As a result, we obtain radiance
field representations of human heads that capture motion over time and
facilitate re-rendering of arbitrary novel viewpoints. In a series of
experiments, we explore the design choices of our method and demonstrate that
our approach outperforms state-of-the-art dynamic radiance field approaches by
a significant margin.";Tobias Kirschstein<author:sep>Shenhan Qian<author:sep>Simon Giebenhain<author:sep>Tim Walter<author:sep>Matthias NieÃner;http://arxiv.org/pdf/2305.03027v1;cs.CV;"Siggraph 2023, Project Page:
  https://tobias-kirschstein.github.io/nersemble/ , Video:
  https://youtu.be/a-OAWqBzldU";
2305.03176v1;http://arxiv.org/abs/2305.03176v1;2023-05-04;NeRF-QA: Neural Radiance Fields Quality Assessment Database;"This short paper proposes a new database - NeRF-QA - containing 48 videos
synthesized with seven NeRF based methods, along with their perceived quality
scores, resulting from subjective assessment tests; for the videos selection,
both real and synthetic, 360 degrees scenes were considered. This database will
allow to evaluate the suitability, to NeRF based synthesized views, of existing
objective quality metrics and also the development of new quality metrics,
specific for this case.";Pedro Martin<author:sep>AntÃ³nio Rodrigues<author:sep>JoÃ£o Ascenso<author:sep>Maria Paula Queluz;http://arxiv.org/pdf/2305.03176v1;cs.MM;;nerf
2305.02463v1;http://arxiv.org/abs/2305.02463v1;2023-05-03;Shap-E: Generating Conditional 3D Implicit Functions;"We present Shap-E, a conditional generative model for 3D assets. Unlike
recent work on 3D generative models which produce a single output
representation, Shap-E directly generates the parameters of implicit functions
that can be rendered as both textured meshes and neural radiance fields. We
train Shap-E in two stages: first, we train an encoder that deterministically
maps 3D assets into the parameters of an implicit function; second, we train a
conditional diffusion model on outputs of the encoder. When trained on a large
dataset of paired 3D and text data, our resulting models are capable of
generating complex and diverse 3D assets in a matter of seconds. When compared
to Point-E, an explicit generative model over point clouds, Shap-E converges
faster and reaches comparable or better sample quality despite modeling a
higher-dimensional, multi-representation output space. We release model
weights, inference code, and samples at https://github.com/openai/shap-e.";Heewoo Jun<author:sep>Alex Nichol;http://arxiv.org/pdf/2305.02463v1;cs.CV;23 pages, 13 figures;
2305.02310v1;http://arxiv.org/abs/2305.02310v1;2023-05-03;Real-Time Radiance Fields for Single-Image Portrait View Synthesis;"We present a one-shot method to infer and render a photorealistic 3D
representation from a single unposed image (e.g., face portrait) in real-time.
Given a single RGB input, our image encoder directly predicts a canonical
triplane representation of a neural radiance field for 3D-aware novel view
synthesis via volume rendering. Our method is fast (24 fps) on consumer
hardware, and produces higher quality results than strong GAN-inversion
baselines that require test-time optimization. To train our triplane encoder
pipeline, we use only synthetic data, showing how to distill the knowledge from
a pretrained 3D GAN into a feedforward encoder. Technical contributions include
a Vision Transformer-based triplane encoder, a camera data augmentation
strategy, and a well-designed loss function for synthetic data training. We
benchmark against the state-of-the-art methods, demonstrating significant
improvements in robustness and image quality in challenging real-world
settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ),
but our algorithm can also be applied in the future to other categories with a
3D-aware image generator.";Alex Trevithick<author:sep>Matthew Chan<author:sep>Michael Stengel<author:sep>Eric R. Chan<author:sep>Chao Liu<author:sep>Zhiding Yu<author:sep>Sameh Khamis<author:sep>Manmohan Chandraker<author:sep>Ravi Ramamoorthi<author:sep>Koki Nagano;http://arxiv.org/pdf/2305.02310v1;cs.CV;Project page: https://research.nvidia.com/labs/nxp/lp3d/;
2305.01163v1;http://arxiv.org/abs/2305.01163v1;2023-05-02;Federated Neural Radiance Fields;"The ability of neural radiance fields or NeRFs to conduct accurate 3D
modelling has motivated application of the technique to scene representation.
Previous approaches have mainly followed a centralised learning paradigm, which
assumes that all training images are available on one compute node for
training. In this paper, we consider training NeRFs in a federated manner,
whereby multiple compute nodes, each having acquired a distinct set of
observations of the overall scene, learn a common NeRF in parallel. This
supports the scenario of cooperatively modelling a scene using multiple agents.
Our contribution is the first federated learning algorithm for NeRF, which
splits the training effort across multiple compute nodes and obviates the need
to pool the images at a central node. A technique based on low-rank
decomposition of NeRF layers is introduced to reduce bandwidth consumption to
transmit the model parameters for aggregation. Transferring compressed models
instead of the raw data also contributes to the privacy of the data collecting
agents.";Lachlan Holden<author:sep>Feras Dayoub<author:sep>David Harvey<author:sep>Tat-Jun Chin;http://arxiv.org/pdf/2305.01163v1;cs.CV;10 pages, 7 figures;nerf
2305.01643v2;http://arxiv.org/abs/2305.01643v2;2023-05-02;Neural LiDAR Fields for Novel View Synthesis;"We present Neural Fields for LiDAR (NFL), a method to optimise a neural field
scene representation from LiDAR measurements, with the goal of synthesizing
realistic LiDAR scans from novel viewpoints. NFL combines the rendering power
of neural fields with a detailed, physically motivated model of the LiDAR
sensing process, thus enabling it to accurately reproduce key sensor behaviors
like beam divergence, secondary returns, and ray dropping. We evaluate NFL on
synthetic and real LiDAR scans and show that it outperforms explicit
reconstruct-then-simulate methods as well as other NeRF-style methods on LiDAR
novel view synthesis task. Moreover, we show that the improved realism of the
synthesized views narrows the domain gap to real scans and translates to better
registration and semantic segmentation performance.";Shengyu Huang<author:sep>Zan Gojcic<author:sep>Zian Wang<author:sep>Francis Williams<author:sep>Yoni Kasten<author:sep>Sanja Fidler<author:sep>Konrad Schindler<author:sep>Or Litany;http://arxiv.org/pdf/2305.01643v2;cs.CV;"ICCV 2023 - camera ready. Project page:
  https://research.nvidia.com/labs/toronto-ai/nfl/";nerf
2305.01190v2;http://arxiv.org/abs/2305.01190v2;2023-05-02;LatentAvatar: Learning Latent Expression Code for Expressive Neural Head  Avatar;"Existing approaches to animatable NeRF-based head avatars are either built
upon face templates or use the expression coefficients of templates as the
driving signal. Despite the promising progress, their performances are heavily
bound by the expression power and the tracking accuracy of the templates. In
this work, we present LatentAvatar, an expressive neural head avatar driven by
latent expression codes. Such latent expression codes are learned in an
end-to-end and self-supervised manner without templates, enabling our method to
get rid of expression and tracking issues. To achieve this, we leverage a
latent head NeRF to learn the person-specific latent expression codes from a
monocular portrait video, and further design a Y-shaped network to learn the
shared latent expression codes of different subjects for cross-identity
reenactment. By optimizing the photometric reconstruction objectives in NeRF,
the latent expression codes are learned to be 3D-aware while faithfully
capturing the high-frequency detailed expressions. Moreover, by learning a
mapping between the latent expression code learned in shared and
person-specific settings, LatentAvatar is able to perform expressive
reenactment between different subjects. Experimental results show that our
LatentAvatar is able to capture challenging expressions and the subtle movement
of teeth and even eyeballs, which outperforms previous state-of-the-art
solutions in both quantitative and qualitative comparisons. Project page:
https://www.liuyebin.com/latentavatar.";Yuelang Xu<author:sep>Hongwen Zhang<author:sep>Lizhen Wang<author:sep>Xiaochen Zhao<author:sep>Han Huang<author:sep>Guojun Qi<author:sep>Yebin Liu;http://arxiv.org/pdf/2305.01190v2;cs.CV;Accepted by SIGGRAPH 2023;nerf
2305.00787v1;http://arxiv.org/abs/2305.00787v1;2023-05-01;GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking  Face Generation;"Generating talking person portraits with arbitrary speech audio is a crucial
problem in the field of digital human and metaverse. A modern talking face
generation method is expected to achieve the goals of generalized audio-lip
synchronization, good video quality, and high system efficiency. Recently,
neural radiance field (NeRF) has become a popular rendering technique in this
field since it could achieve high-fidelity and 3D-consistent talking face
generation with a few-minute-long training video. However, there still exist
several challenges for NeRF-based methods: 1) as for the lip synchronization,
it is hard to generate a long facial motion sequence of high temporal
consistency and audio-lip accuracy; 2) as for the video quality, due to the
limited data used to train the renderer, it is vulnerable to out-of-domain
input condition and produce bad rendering results occasionally; 3) as for the
system efficiency, the slow training and inference speed of the vanilla NeRF
severely obstruct its usage in real-world applications. In this paper, we
propose GeneFace++ to handle these challenges by 1) utilizing the pitch contour
as an auxiliary feature and introducing a temporal loss in the facial motion
prediction process; 2) proposing a landmark locally linear embedding method to
regulate the outliers in the predicted motion sequence to avoid robustness
issues; 3) designing a computationally efficient NeRF-based motion-to-video
renderer to achieves fast training and real-time inference. With these
settings, GeneFace++ becomes the first NeRF-based method that achieves stable
and real-time talking face generation with generalized audio-lip
synchronization. Extensive experiments show that our method outperforms
state-of-the-art baselines in terms of subjective and objective evaluation.
Video samples are available at https://genefaceplusplus.github.io .";Zhenhui Ye<author:sep>Jinzheng He<author:sep>Ziyue Jiang<author:sep>Rongjie Huang<author:sep>Jiawei Huang<author:sep>Jinglin Liu<author:sep>Yi Ren<author:sep>Xiang Yin<author:sep>Zejun Ma<author:sep>Zhou Zhao;http://arxiv.org/pdf/2305.00787v1;cs.CV;18 Pages, 7 figures;nerf
2305.00375v1;http://arxiv.org/abs/2305.00375v1;2023-04-30;Neural Radiance Fields (NeRFs): A Review and Some Recent Developments;"Neural Radiance Field (NeRF) is a framework that represents a 3D scene in the
weights of a fully connected neural network, known as the Multi-Layer
Perception(MLP). The method was introduced for the task of novel view synthesis
and is able to achieve state-of-the-art photorealistic image renderings from a
given continuous viewpoint. NeRFs have become a popular field of research as
recent developments have been made that expand the performance and capabilities
of the base framework. Recent developments include methods that require less
images to train the model for view synthesis as well as methods that are able
to generate views from unconstrained and dynamic scene representations.";Mohamed Debbagh;http://arxiv.org/pdf/2305.00375v1;cs.CV;volume rendering, view synthesis, scene representation, deep learning;nerf
2305.00393v3;http://arxiv.org/abs/2305.00393v3;2023-04-30;Unsupervised Object-Centric Voxelization for Dynamic Scene Understanding;"Understanding the compositional dynamics of multiple objects in unsupervised
visual environments is challenging, and existing object-centric representation
learning methods often ignore 3D consistency in scene decomposition. We propose
DynaVol, an inverse graphics approach that learns object-centric volumetric
representations in a neural rendering framework. DynaVol maintains time-varying
3D voxel grids that explicitly represent the probability of each spatial
location belonging to different objects, and decouple temporal dynamics and
spatial information by learning a canonical-space deformation field. To
optimize the volumetric features, we embed them into a fully differentiable
neural network, binding them to object-centric global features and then driving
a compositional NeRF for scene reconstruction. DynaVol outperforms existing
methods in novel view synthesis and unsupervised scene decomposition and allows
for the editing of dynamic scenes, such as adding, deleting, replacing objects,
and modifying their trajectories.";Siyu Gao<author:sep>Yanpeng Zhao<author:sep>Yunbo Wang<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2305.00393v3;cs.CV;;nerf
2304.14811v2;http://arxiv.org/abs/2304.14811v2;2023-04-28;NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance  Fields;"Labeling LiDAR point clouds for training autonomous driving is extremely
expensive and difficult. LiDAR simulation aims at generating realistic LiDAR
data with labels for training and verifying self-driving algorithms more
efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for
novel view synthesis using implicit reconstruction of 3D scenes. Inspired by
this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages
real-world information to generate realistic LIDAR point clouds. Different from
existing LiDAR simulators, we use real images and point cloud data collected by
self-driving cars to learn the 3D scene representation, point cloud generation
and label rendering. We verify the effectiveness of our NeRF-LiDAR by training
different 3D segmentation models on the generated LiDAR point clouds. It
reveals that the trained models are able to achieve similar accuracy when
compared with the same model trained on the real LiDAR data. Besides, the
generated data is capable of boosting the accuracy through pre-training which
helps reduce the requirements of the real labeled data.";Junge Zhang<author:sep>Feihu Zhang<author:sep>Shaochen Kuang<author:sep>Li Zhang;http://arxiv.org/pdf/2304.14811v2;cs.CV;;nerf
2305.00041v1;http://arxiv.org/abs/2305.00041v1;2023-04-28;ViP-NeRF: Visibility Prior for Sparse Input Neural Radiance Fields;"Neural radiance fields (NeRF) have achieved impressive performances in view
synthesis by encoding neural representations of a scene. However, NeRFs require
hundreds of images per scene to synthesize photo-realistic novel views.
Training them on sparse input views leads to overfitting and incorrect scene
depth estimation resulting in artifacts in the rendered novel views. Sparse
input NeRFs were recently regularized by providing dense depth estimated from
pre-trained networks as supervision, to achieve improved performance over
sparse depth constraints. However, we find that such depth priors may be
inaccurate due to generalization issues. Instead, we hypothesize that the
visibility of pixels in different input views can be more reliably estimated to
provide dense supervision. In this regard, we compute a visibility prior
through the use of plane sweep volumes, which does not require any
pre-training. By regularizing the NeRF training with the visibility prior, we
successfully train the NeRF with few input views. We reformulate the NeRF to
also directly output the visibility of a 3D point from a given viewpoint to
reduce the training time with the visibility constraint. On multiple datasets,
our model outperforms the competing sparse input NeRF models including those
that use learned priors. The source code for our model can be found on our
project page:
https://nagabhushansn95.github.io/publications/2023/ViP-NeRF.html.";Nagabhushan Somraj<author:sep>Rajiv Soundararajan;http://arxiv.org/pdf/2305.00041v1;cs.CV;SIGGRAPH 2023;nerf
2304.14401v1;http://arxiv.org/abs/2304.14401v1;2023-04-27;ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs;"While NeRF-based human representations have shown impressive novel view
synthesis results, most methods still rely on a large number of images / views
for training. In this work, we propose a novel animatable NeRF called
ActorsNeRF. It is first pre-trained on diverse human subjects, and then adapted
with few-shot monocular video frames for a new actor with unseen poses.
Building on previous generalizable NeRFs with parameter sharing using a ConvNet
encoder, ActorsNeRF further adopts two human priors to capture the large human
appearance, shape, and pose variations. Specifically, in the encoded feature
space, we will first align different human subjects in a category-level
canonical space, and then align the same human from different frames in an
instance-level canonical space for rendering. We quantitatively and
qualitatively demonstrate that ActorsNeRF significantly outperforms the
existing state-of-the-art on few-shot generalization to new people and poses on
multiple datasets. Project Page: https://jitengmu.github.io/ActorsNeRF/";Jiteng Mu<author:sep>Shen Sang<author:sep>Nuno Vasconcelos<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2304.14401v1;cs.CV;Project Page : https://jitengmu.github.io/ActorsNeRF/;nerf
2304.14070v1;http://arxiv.org/abs/2304.14070v1;2023-04-27;Compositional 3D Human-Object Neural Animation;"Human-object interactions (HOIs) are crucial for human-centric scene
understanding applications such as human-centric visual generation, AR/VR, and
robotics. Since existing methods mainly explore capturing HOIs, rendering HOI
remains less investigated. In this paper, we address this challenge in HOI
animation from a compositional perspective, i.e., animating novel HOIs
including novel interaction, novel human and/or novel object driven by a novel
pose sequence. Specifically, we adopt neural human-object deformation to model
and render HOI dynamics based on implicit neural representations. To enable the
interaction pose transferring among different persons and objects, we then
devise a new compositional conditional neural radiance field (or CC-NeRF),
which decomposes the interdependence between human and object using latent
codes to enable compositionally animation control of novel HOIs. Experiments
show that the proposed method can generalize well to various novel HOI
animation settings. Our project page is https://zhihou7.github.io/CHONA/";Zhi Hou<author:sep>Baosheng Yu<author:sep>Dacheng Tao;http://arxiv.org/pdf/2304.14070v1;cs.CV;14 pages, 6 figures;nerf
2304.14005v2;http://arxiv.org/abs/2304.14005v2;2023-04-27;ContraNeRF: 3D-Aware Generative Model via Contrastive Learning with  Unsupervised Implicit Pose Embedding;"Although 3D-aware GANs based on neural radiance fields have achieved
competitive performance, their applicability is still limited to objects or
scenes with the ground-truths or prediction models for clearly defined
canonical camera poses. To extend the scope of applicable datasets, we propose
a novel 3D-aware GAN optimization technique through contrastive learning with
implicit pose embeddings. To this end, we first revise the discriminator design
and remove dependency on ground-truth camera poses. Then, to capture complex
and challenging 3D scene structures more effectively, we make the discriminator
estimate a high-dimensional implicit pose embedding from a given image and
perform contrastive learning on the pose embedding. The proposed approach can
be employed for the dataset, where the canonical camera pose is ill-defined
because it does not look up or estimate camera poses. Experimental results show
that our algorithm outperforms existing methods by large margins on the
datasets with multiple object categories and inconsistent canonical camera
poses.";Mijeong Kim<author:sep>Hyunjoon Lee<author:sep>Bohyung Han;http://arxiv.org/pdf/2304.14005v2;cs.CV;"20 pages. For the project page, see
  https://cv.snu.ac.kr/research/ContraNeRF/";nerf
2304.14301v2;http://arxiv.org/abs/2304.14301v2;2023-04-27;Combining HoloLens with Instant-NeRFs: Advanced Real-Time 3D Mobile  Mapping;"This work represents a large step into modern ways of fast 3D reconstruction
based on RGB camera images. Utilizing a Microsoft HoloLens 2 as a multisensor
platform that includes an RGB camera and an inertial measurement unit for
SLAM-based camera-pose determination, we train a Neural Radiance Field (NeRF)
as a neural scene representation in real-time with the acquired data from the
HoloLens. The HoloLens is connected via Wifi to a high-performance PC that is
responsible for the training and 3D reconstruction. After the data stream ends,
the training is stopped and the 3D reconstruction is initiated, which extracts
a point cloud of the scene. With our specialized inference algorithm, five
million scene points can be extracted within 1 second. In addition, the point
cloud also includes radiometry per point. Our method of 3D reconstruction
outperforms grid point sampling with NeRFs by multiple orders of magnitude and
can be regarded as a complete real-time 3D reconstruction method in a mobile
mapping setup.";Dennis Haitz<author:sep>Boris Jutzi<author:sep>Markus Ulrich<author:sep>Miriam Jaeger<author:sep>Patrick Huebner;http://arxiv.org/pdf/2304.14301v2;cs.CV;8 pages, 6 figures;nerf
2304.14473v1;http://arxiv.org/abs/2304.14473v1;2023-04-27;Learning a Diffusion Prior for NeRFs;"Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D
representation for objects and scenes derived from 2D data. Generating NeRFs,
however, remains difficult in many scenarios. For instance, training a NeRF
with only a small number of views as supervision remains challenging since it
is an under-constrained problem. In such settings, it calls for some inductive
prior to filter out bad local minima. One way to introduce such inductive
priors is to learn a generative model for NeRFs modeling a certain class of
scenes. In this paper, we propose to use a diffusion model to generate NeRFs
encoded on a regularized grid. We show that our model can sample realistic
NeRFs, while at the same time allowing conditional generations, given a certain
observation as guidance.";Guandao Yang<author:sep>Abhijit Kundu<author:sep>Leonidas J. Guibas<author:sep>Jonathan T. Barron<author:sep>Ben Poole;http://arxiv.org/pdf/2304.14473v1;cs.CV;;nerf
2304.13518v1;http://arxiv.org/abs/2304.13518v1;2023-04-26;Super-NeRF: View-consistent Detail Generation for NeRF super-resolution;"The neural radiance field (NeRF) achieved remarkable success in modeling 3D
scenes and synthesizing high-fidelity novel views. However, existing NeRF-based
methods focus more on the make full use of the image resolution to generate
novel views, but less considering the generation of details under the limited
input resolution. In analogy to the extensive usage of image super-resolution,
NeRF super-resolution is an effective way to generate the high-resolution
implicit representation of 3D scenes and holds great potential applications. Up
to now, such an important topic is still under-explored. In this paper, we
propose a NeRF super-resolution method, named Super-NeRF, to generate
high-resolution NeRF from only low-resolution inputs. Given multi-view
low-resolution images, Super-NeRF constructs a consistency-controlling
super-resolution module to generate view-consistent high-resolution details for
NeRF. Specifically, an optimizable latent code is introduced for each
low-resolution input image to control the 2D super-resolution images to
converge to the view-consistent output. The latent codes of each low-resolution
image are optimized synergistically with the target Super-NeRF representation
to fully utilize the view consistency constraint inherent in NeRF construction.
We verify the effectiveness of Super-NeRF on synthetic, real-world, and
AI-generated NeRF datasets. Super-NeRF achieves state-of-the-art NeRF
super-resolution performance on high-resolution detail generation and
cross-view consistency.";Yuqi Han<author:sep>Tao Yu<author:sep>Xiaohang Yu<author:sep>Yuwang Wang<author:sep>Qionghai Dai;http://arxiv.org/pdf/2304.13518v1;cs.CV;;nerf
2304.13386v2;http://arxiv.org/abs/2304.13386v2;2023-04-26;VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs;"Neural Radiance Fields (NeRF) has shown great success in novel view synthesis
due to its state-of-the-art quality and flexibility. However, NeRF requires
dense input views (tens to hundreds) and a long training time (hours to days)
for a single scene to generate high-fidelity images. Although using the voxel
grids to represent the radiance field can significantly accelerate the
optimization process, we observe that for sparse inputs, the voxel grids are
more prone to overfitting to the training views and will have holes and
floaters, which leads to artifacts. In this paper, we propose VGOS, an approach
for fast (3-5 minutes) radiance field reconstruction from sparse inputs (3-10
views) to address these issues. To improve the performance of voxel-based
radiance field in sparse input scenarios, we propose two methods: (a) We
introduce an incremental voxel training strategy, which prevents overfitting by
suppressing the optimization of peripheral voxels in the early stage of
reconstruction. (b) We use several regularization techniques to smooth the
voxels, which avoids degenerate solutions. Experiments demonstrate that VGOS
achieves state-of-the-art performance for sparse inputs with super-fast
convergence. Code will be available at https://github.com/SJoJoK/VGOS.";Jiakai Sun<author:sep>Zhanjie Zhang<author:sep>Jiafu Chen<author:sep>Guangyuan Li<author:sep>Boyan Ji<author:sep>Lei Zhao<author:sep>Wei Xing<author:sep>Huaizhong Lin;http://arxiv.org/pdf/2304.13386v2;cs.CV;IJCAI 2023 Accepted (Main Track);nerf
2304.12587v4;http://arxiv.org/abs/2304.12587v4;2023-04-25;MF-NeRF: Memory Efficient NeRF with Mixed-Feature Hash Table;"Neural radiance field (NeRF) has shown remarkable performance in generating
photo-realistic novel views. Among recent NeRF related research, the approaches
that involve the utilization of explicit structures like grids to manage
features achieve exceptionally fast training by reducing the complexity of
multilayer perceptron (MLP) networks. However, storing features in dense grids
demands a substantial amount of memory space, resulting in a notable memory
bottleneck within computer system. Consequently, it leads to a significant
increase in training times without prior hyper-parameter tuning. To address
this issue, in this work, we are the first to propose MF-NeRF, a
memory-efficient NeRF framework that employs a Mixed-Feature hash table to
improve memory efficiency and reduce training time while maintaining
reconstruction quality. Specifically, we first design a mixed-feature hash
encoding to adaptively mix part of multi-level feature grids and map it to a
single hash table. Following that, in order to obtain the correct index of a
grid point, we further develop an index transformation method that transforms
indices of an arbitrary level grid to those of a canonical grid. Extensive
experiments benchmarking with state-of-the-art Instant-NGP, TensoRF, and DVGO,
indicate our MF-NeRF could achieve the fastest training time on the same GPU
hardware with similar or even higher reconstruction quality.";Yongjae Lee<author:sep>Li Yang<author:sep>Deliang Fan;http://arxiv.org/pdf/2304.12587v4;cs.CV;;nerf
2304.12746v1;http://arxiv.org/abs/2304.12746v1;2023-04-25;Local Implicit Ray Function for Generalizable Radiance Field  Representation;"We propose LIRF (Local Implicit Ray Function), a generalizable neural
rendering approach for novel view rendering. Current generalizable neural
radiance fields (NeRF) methods sample a scene with a single ray per pixel and
may therefore render blurred or aliased views when the input views and rendered
views capture scene content with different resolutions. To solve this problem,
we propose LIRF to aggregate the information from conical frustums to construct
a ray. Given 3D positions within conical frustums, LIRF takes 3D coordinates
and the features of conical frustums as inputs and predicts a local volumetric
radiance field. Since the coordinates are continuous, LIRF renders high-quality
novel views at a continuously-valued scale via volume rendering. Besides, we
predict the visible weights for each input view via transformer-based feature
matching to improve the performance in occluded areas. Experimental results on
real-world scenes validate that our method outperforms state-of-the-art methods
on novel view rendering of unseen scenes at arbitrary scales.";Xin Huang<author:sep>Qi Zhang<author:sep>Ying Feng<author:sep>Xiaoyu Li<author:sep>Xuan Wang<author:sep>Qing Wang;http://arxiv.org/pdf/2304.12746v1;cs.CV;Accepted to CVPR 2023. Project page: https://xhuangcv.github.io/lirf/;nerf
2304.12467v2;http://arxiv.org/abs/2304.12467v2;2023-04-24;Instant-3D: Instant Neural Radiance Field Training Towards On-Device  AR/VR 3D Reconstruction;"Neural Radiance Field (NeRF) based 3D reconstruction is highly desirable for
immersive Augmented and Virtual Reality (AR/VR) applications, but achieving
instant (i.e., < 5 seconds) on-device NeRF training remains a challenge. In
this work, we first identify the inefficiency bottleneck: the need to
interpolate NeRF embeddings up to 200,000 times from a 3D embedding grid during
each training iteration. To alleviate this, we propose Instant-3D, an
algorithm-hardware co-design acceleration framework that achieves instant
on-device NeRF training. Our algorithm decomposes the embedding grid
representation in terms of color and density, enabling computational redundancy
to be squeezed out by adopting different (1) grid sizes and (2) update
frequencies for the color and density branches. Our hardware accelerator
further reduces the dominant memory accesses for embedding grid interpolation
by (1) mapping multiple nearby points' memory read requests into one during the
feed-forward process, (2) merging embedding grid updates from the same sliding
time window during back-propagation, and (3) fusing different computation cores
to support the different grid sizes needed by the color and density branches of
Instant-3D algorithm. Extensive experiments validate the effectiveness of
Instant-3D, achieving a large training time reduction of 41x - 248x while
maintaining the same reconstruction quality. Excitingly, Instant-3D has enabled
instant 3D reconstruction for AR/VR, requiring a reconstruction time of only
1.6 seconds per scene and meeting the AR/VR power consumption constraint of 1.9
W.";Sixu Li<author:sep>Chaojian Li<author:sep>Wenbo Zhu<author:sep> Boyang<author:sep> Yu<author:sep> Yang<author:sep> Zhao<author:sep>Cheng Wan<author:sep>Haoran You<author:sep>Huihong Shi<author:sep> Yingyan<author:sep> Lin;http://arxiv.org/pdf/2304.12467v2;cs.AR;Accepted by ISCA'23;nerf
2304.12294v1;http://arxiv.org/abs/2304.12294v1;2023-04-24;Explicit Correspondence Matching for Generalizable Neural Radiance  Fields;"We present a new generalizable NeRF method that is able to directly
generalize to new unseen scenarios and perform novel view synthesis with as few
as two source views. The key to our approach lies in the explicitly modeled
correspondence matching information, so as to provide the geometry prior to the
prediction of NeRF color and density for volume rendering. The explicit
correspondence matching is quantified with the cosine similarity between image
features sampled at the 2D projections of a 3D point on different views, which
is able to provide reliable cues about the surface geometry. Unlike previous
methods where image features are extracted independently for each view, we
consider modeling the cross-view interactions via Transformer cross-attention,
which greatly improves the feature matching quality. Our method achieves
state-of-the-art results on different evaluation settings, with the experiments
showing a strong correlation between our learned cosine feature similarity and
volume density, demonstrating the effectiveness and superiority of our proposed
method. Code is at https://github.com/donydchen/matchnerf";Yuedong Chen<author:sep>Haofei Xu<author:sep>Qianyi Wu<author:sep>Chuanxia Zheng<author:sep>Tat-Jen Cham<author:sep>Jianfei Cai;http://arxiv.org/pdf/2304.12294v1;cs.CV;"Code and pre-trained models: https://github.com/donydchen/matchnerf
  Project Page: https://donydchen.github.io/matchnerf/";nerf
2304.12308v4;http://arxiv.org/abs/2304.12308v4;2023-04-24;Segment Anything in 3D with NeRFs;"Recently, the Segment Anything Model (SAM) emerged as a powerful vision
foundation model which is capable to segment anything in 2D images. This paper
aims to generalize SAM to segment 3D objects. Rather than replicating the data
acquisition and annotation procedure which is costly in 3D, we design an
efficient solution, leveraging the Neural Radiance Field (NeRF) as a cheap and
off-the-shelf prior that connects multi-view 2D images to the 3D space. We
refer to the proposed solution as SA3D, for Segment Anything in 3D. It is only
required to provide a manual segmentation prompt (e.g., rough points) for the
target object in a single view, which is used to generate its 2D mask in this
view with SAM. Next, SA3D alternately performs mask inverse rendering and
cross-view self-prompting across various views to iteratively complete the 3D
mask of the target object constructed with voxel grids. The former projects the
2D mask obtained by SAM in the current view onto 3D mask with guidance of the
density distribution learned by the NeRF; The latter extracts reliable prompts
automatically as the input to SAM from the NeRF-rendered 2D mask in another
view. We show in experiments that SA3D adapts to various scenes and achieves 3D
segmentation within minutes. Our research reveals a potential methodology to
lift the ability of a 2D vision foundation model to 3D, as long as the 2D model
can steadily address promptable segmentation across multiple views. Our code is
available at https://github.com/Jumpat/SegmentAnythingin3D.";Jiazhong Cen<author:sep>Zanwei Zhou<author:sep>Jiemin Fang<author:sep>Chen Yang<author:sep>Wei Shen<author:sep>Lingxi Xie<author:sep>Dongsheng Jiang<author:sep>Xiaopeng Zhang<author:sep>Qi Tian;http://arxiv.org/pdf/2304.12308v4;cs.CV;NeurIPS 2023. Project page: https://jumpat.github.io/SA3D/;nerf
2304.12281v1;http://arxiv.org/abs/2304.12281v1;2023-04-24;HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single  Video;"We introduce HOSNeRF, a novel 360{\deg} free-viewpoint rendering method that
reconstructs neural radiance fields for dynamic human-object-scene from a
single monocular in-the-wild video. Our method enables pausing the video at any
frame and rendering all scene details (dynamic humans, objects, and
backgrounds) from arbitrary viewpoints. The first challenge in this task is the
complex object motions in human-object interactions, which we tackle by
introducing the new object bones into the conventional human skeleton hierarchy
to effectively estimate large object deformations in our dynamic human-object
model. The second challenge is that humans interact with different objects at
different times, for which we introduce two new learnable object state
embeddings that can be used as conditions for learning our human-object
representation and scene representation, respectively. Extensive experiments
show that HOSNeRF significantly outperforms SOTA approaches on two challenging
datasets by a large margin of 40% ~ 50% in terms of LPIPS. The code, data, and
compelling examples of 360{\deg} free-viewpoint renderings from single videos
will be released in https://showlab.github.io/HOSNeRF.";Jia-Wei Liu<author:sep>Yan-Pei Cao<author:sep>Tianyuan Yang<author:sep>Eric Zhongcong Xu<author:sep>Jussi Keppo<author:sep>Ying Shan<author:sep>Xiaohu Qie<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2304.12281v1;cs.CV;Project page: https://showlab.github.io/HOSNeRF;nerf
2304.11842v3;http://arxiv.org/abs/2304.11842v3;2023-04-24;Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via  Algorithm-Hardware Co-Design;"Novel view synthesis is an essential functionality for enabling immersive
experiences in various Augmented- and Virtual-Reality (AR/VR) applications, for
which generalizable Neural Radiance Fields (NeRFs) have gained increasing
popularity thanks to their cross-scene generalization capability. Despite their
promise, the real-device deployment of generalizable NeRFs is bottlenecked by
their prohibitive complexity due to the required massive memory accesses to
acquire scene features, causing their ray marching process to be
memory-bounded. To this end, we propose Gen-NeRF, an algorithm-hardware
co-design framework dedicated to generalizable NeRF acceleration, which for the
first time enables real-time generalizable NeRFs. On the algorithm side,
Gen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact
that different regions of a 3D scene contribute differently to the rendered
pixel, to enable sparse yet effective sampling. On the hardware side, Gen-NeRF
highlights an accelerator micro-architecture to maximize the data reuse
opportunities among different rays by making use of their epipolar geometric
relationship. Furthermore, our Gen-NeRF accelerator features a customized
dataflow to enhance data locality during point-to-hardware mapping and an
optimized scene feature storage strategy to minimize memory bank conflicts.
Extensive experiments validate the effectiveness of our proposed Gen-NeRF
framework in enabling real-time and generalizable novel view synthesis.";Yonggan Fu<author:sep>Zhifan Ye<author:sep>Jiayi Yuan<author:sep>Shunyao Zhang<author:sep>Sixu Li<author:sep>Haoran You<author:sep>Yingyan Lin;http://arxiv.org/pdf/2304.11842v3;cs.CV;Accepted by ISCA 2023;nerf
2304.12439v1;http://arxiv.org/abs/2304.12439v1;2023-04-24;TextMesh: Generation of Realistic 3D Meshes From Text Prompts;"The ability to generate highly realistic 2D images from mere text prompts has
recently made huge progress in terms of speed and quality, thanks to the advent
of image diffusion models. Naturally, the question arises if this can be also
achieved in the generation of 3D content from such text prompts. To this end, a
new line of methods recently emerged trying to harness diffusion models,
trained on 2D images, for supervision of 3D model generation using view
dependent prompts. While achieving impressive results, these methods, however,
have two major drawbacks. First, rather than commonly used 3D meshes, they
instead generate neural radiance fields (NeRFs), making them impractical for
most real applications. Second, these approaches tend to produce over-saturated
models, giving the output a cartoonish looking effect. Therefore, in this work
we propose a novel method for generation of highly realistic-looking 3D meshes.
To this end, we extend NeRF to employ an SDF backbone, leading to improved 3D
mesh extraction. In addition, we propose a novel way to finetune the mesh
texture, removing the effect of high saturation and improving the details of
the output 3D mesh.";Christina Tsalicoglou<author:sep>Fabian Manhardt<author:sep>Alessio Tonioni<author:sep>Michael Niemeyer<author:sep>Federico Tombari;http://arxiv.org/pdf/2304.12439v1;cs.CV;Project Website: https://fabi92.github.io/textmesh/;nerf
2304.11448v1;http://arxiv.org/abs/2304.11448v1;2023-04-22;Dehazing-NeRF: Neural Radiance Fields from Hazy Images;"Neural Radiance Field (NeRF) has received much attention in recent years due
to the impressively high quality in 3D scene reconstruction and novel view
synthesis. However, image degradation caused by the scattering of atmospheric
light and object light by particles in the atmosphere can significantly
decrease the reconstruction quality when shooting scenes in hazy conditions. To
address this issue, we propose Dehazing-NeRF, a method that can recover clear
NeRF from hazy image inputs. Our method simulates the physical imaging process
of hazy images using an atmospheric scattering model, and jointly learns the
atmospheric scattering model and a clean NeRF model for both image dehazing and
novel view synthesis. Different from previous approaches, Dehazing-NeRF is an
unsupervised method with only hazy images as the input, and also does not rely
on hand-designed dehazing priors. By jointly combining the depth estimated from
the NeRF 3D scene with the atmospheric scattering model, our proposed model
breaks through the ill-posed problem of single-image dehazing while maintaining
geometric consistency. Besides, to alleviate the degradation of image quality
caused by information loss, soft margin consistency regularization, as well as
atmospheric consistency and contrast discriminative loss, are addressed during
the model training process. Extensive experiments demonstrate that our method
outperforms the simple combination of single-image dehazing and NeRF on both
image dehazing and novel view image synthesis.";Tian Li<author:sep>LU Li<author:sep>Wei Wang<author:sep>Zhangchi Feng;http://arxiv.org/pdf/2304.11448v1;cs.CV;;nerf
2304.11342v1;http://arxiv.org/abs/2304.11342v1;2023-04-22;NaviNeRF: NeRF-based 3D Representation Disentanglement by Latent  Semantic Navigation;"3D representation disentanglement aims to identify, decompose, and manipulate
the underlying explanatory factors of 3D data, which helps AI fundamentally
understand our 3D world. This task is currently under-explored and poses great
challenges: (i) the 3D representations are complex and in general contains much
more information than 2D image; (ii) many 3D representations are not well
suited for gradient-based optimization, let alone disentanglement. To address
these challenges, we use NeRF as a differentiable 3D representation, and
introduce a self-supervised Navigation to identify interpretable semantic
directions in the latent space. To our best knowledge, this novel method,
dubbed NaviNeRF, is the first work to achieve fine-grained 3D disentanglement
without any priors or supervisions. Specifically, NaviNeRF is built upon the
generative NeRF pipeline, and equipped with an Outer Navigation Branch and an
Inner Refinement Branch. They are complementary -- the outer navigation is to
identify global-view semantic directions, and the inner refinement dedicates to
fine-grained attributes. A synergistic loss is further devised to coordinate
two branches. Extensive experiments demonstrate that NaviNeRF has a superior
fine-grained 3D disentanglement ability than the previous 3D-aware models. Its
performance is also comparable to editing-oriented models relying on semantic
or geometry priors.";Baao Xie<author:sep>Bohan Li<author:sep>Zequn Zhang<author:sep>Junting Dong<author:sep>Xin Jin<author:sep>Jingyu Yang<author:sep>Wenjun Zeng;http://arxiv.org/pdf/2304.11342v1;cs.CV;;nerf
2304.11470v1;http://arxiv.org/abs/2304.11470v1;2023-04-22;3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive  Physics under Challenging Scenes;"Given a visual scene, humans have strong intuitions about how a scene can
evolve over time under given actions. The intuition, often termed visual
intuitive physics, is a critical ability that allows us to make effective plans
to manipulate the scene to achieve desired outcomes without relying on
extensive trial and error. In this paper, we present a framework capable of
learning 3D-grounded visual intuitive physics models from videos of complex
scenes with fluids. Our method is composed of a conditional Neural Radiance
Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction
backend, using which we can impose strong relational and structural inductive
bias to capture the structure of the underlying environment. Unlike existing
intuitive point-based dynamics works that rely on the supervision of dense
point trajectory from simulators, we relax the requirements and only assume
access to multi-view RGB images and (imperfect) instance masks acquired using
color prior. This enables the proposed model to handle scenarios where accurate
point estimation and tracking are hard or impossible. We generate datasets
including three challenging scenarios involving fluid, granular materials, and
rigid objects in the simulation. The datasets do not include any dense particle
information so most previous 3D-based intuitive physics pipelines can barely
deal with that. We show our model can make long-horizon future predictions by
learning from raw images and significantly outperforms models that do not
employ an explicit 3D representation space. We also show that once trained, our
model can achieve strong generalization in complex scenarios under extrapolate
settings.";Haotian Xue<author:sep>Antonio Torralba<author:sep>Joshua B. Tenenbaum<author:sep>Daniel LK Yamins<author:sep>Yunzhu Li<author:sep>Hsiao-Yu Tung;http://arxiv.org/pdf/2304.11470v1;cs.CV;;nerf
2304.10780v1;http://arxiv.org/abs/2304.10780v1;2023-04-21;Omni-Line-of-Sight Imaging for Holistic Shape Reconstruction;"We introduce Omni-LOS, a neural computational imaging method for conducting
holistic shape reconstruction (HSR) of complex objects utilizing a
Single-Photon Avalanche Diode (SPAD)-based time-of-flight sensor. As
illustrated in Fig. 1, our method enables new capabilities to reconstruct
near-$360^\circ$ surrounding geometry of an object from a single scan spot. In
such a scenario, traditional line-of-sight (LOS) imaging methods only see the
front part of the object and typically fail to recover the occluded back
regions. Inspired by recent advances of non-line-of-sight (NLOS) imaging
techniques which have demonstrated great power to reconstruct occluded objects,
Omni-LOS marries LOS and NLOS together, leveraging their complementary
advantages to jointly recover the holistic shape of the object from a single
scan position. The core of our method is to put the object nearby diffuse walls
and augment the LOS scan in the front view with the NLOS scans from the
surrounding walls, which serve as virtual ``mirrors'' to trap lights toward the
object. Instead of separately recovering the LOS and NLOS signals, we adopt an
implicit neural network to represent the object, analogous to NeRF and NeTF.
While transients are measured along straight rays in LOS but over the spherical
wavefronts in NLOS, we derive differentiable ray propagation models to
simultaneously model both types of transient measurements so that the NLOS
reconstruction also takes into account the direct LOS measurements and vice
versa. We further develop a proof-of-concept Omni-LOS hardware prototype for
real-world validation. Comprehensive experiments on various wall settings
demonstrate that Omni-LOS successfully resolves shape ambiguities caused by
occlusions, achieves high-fidelity 3D scan quality, and manages to recover
objects of various scales and complexity.";Binbin Huang<author:sep>Xingyue Peng<author:sep>Siyuan Shen<author:sep>Suan Xia<author:sep>Ruiqian Li<author:sep>Yanhua Yu<author:sep>Yuehan Wang<author:sep>Shenghua Gao<author:sep>Wenzheng Chen<author:sep>Shiying Li<author:sep>Jingyi Yu;http://arxiv.org/pdf/2304.10780v1;cs.CV;;nerf
2304.11241v2;http://arxiv.org/abs/2304.11241v2;2023-04-21;AutoNeRF: Training Implicit Scene Representations with Autonomous Agents;"Implicit representations such as Neural Radiance Fields (NeRF) have been
shown to be very effective at novel view synthesis. However, these models
typically require manual and careful human data collection for training. In
this paper, we present AutoNeRF, a method to collect data required to train
NeRFs using autonomous embodied agents. Our method allows an agent to explore
an unseen environment efficiently and use the experience to build an implicit
map representation autonomously. We compare the impact of different exploration
strategies including handcrafted frontier-based exploration, end-to-end and
modular approaches composed of trained high-level planners and classical
low-level path followers. We train these models with different reward functions
tailored to this problem and evaluate the quality of the learned
representations on four different downstream tasks: classical viewpoint
rendering, map reconstruction, planning, and pose refinement. Empirical results
show that NeRFs can be trained on actively collected data using just a single
episode of experience in an unseen environment, and can be used for several
downstream robotic tasks, and that modular trained exploration models
outperform other classical and end-to-end baselines. Finally, we show that
AutoNeRF can reconstruct large-scale scenes, and is thus a useful tool to
perform scene-specific adaptation as the produced 3D environment models can be
loaded into a simulator to fine-tune a policy of interest.";Pierre Marza<author:sep>Laetitia Matignon<author:sep>Olivier Simonin<author:sep>Dhruv Batra<author:sep>Christian Wolf<author:sep>Devendra Singh Chaplot;http://arxiv.org/pdf/2304.11241v2;cs.CV;;nerf
2304.10448v1;http://arxiv.org/abs/2304.10448v1;2023-04-20;ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of  Real World Objects;"In this paper, we focus on the problem of rendering novel views from a Neural
Radiance Field (NeRF) under unobserved light conditions. To this end, we
introduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real world
objects under one-light-at-time (OLAT) conditions, annotated with accurate
ground-truth camera and light poses. Our acquisition pipeline leverages two
robotic arms holding, respectively, a camera and an omni-directional point-wise
light source. We release a total of 20 scenes depicting a variety of objects
with complex geometry and challenging materials. Each scene includes 2000
images, acquired from 50 different points of views under 40 different OLAT
conditions. By leveraging the dataset, we perform an ablation study on the
relighting capability of variants of the vanilla NeRF architecture and identify
a lightweight architecture that can render novel views of an object under novel
light conditions, which we use to establish a non-trivial baseline for the
dataset. Dataset and benchmark are available at
https://eyecan-ai.github.io/rene.";Marco Toschi<author:sep>Riccardo De Matteo<author:sep>Riccardo Spezialetti<author:sep>Daniele De Gregorio<author:sep>Luigi Di Stefano<author:sep>Samuele Salti;http://arxiv.org/pdf/2304.10448v1;cs.CV;Accepted at CVPR 2023 as a highlight;nerf
2304.10406v2;http://arxiv.org/abs/2304.10406v2;2023-04-20;LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields;"We introduce a new task, novel view synthesis for LiDAR sensors. While
traditional model-based LiDAR simulators with style-transfer neural networks
can be applied to render novel views, they fall short of producing accurate and
realistic LiDAR patterns because the renderers rely on explicit 3D
reconstruction and exploit game engines, that ignore important attributes of
LiDAR points. We address this challenge by formulating, to the best of our
knowledge, the first differentiable end-to-end LiDAR rendering framework,
LiDAR-NeRF, leveraging a neural radiance field (NeRF) to facilitate the joint
learning of geometry and the attributes of 3D points. However, simply employing
NeRF cannot achieve satisfactory results, as it only focuses on learning
individual pixels while ignoring local information, especially at low texture
areas, resulting in poor geometry. To this end, we have taken steps to address
this issue by introducing a structural regularization method to preserve local
structural details. To evaluate the effectiveness of our approach, we establish
an object-centric multi-view LiDAR dataset, dubbed NeRF-MVL. It contains
observations of objects from 9 categories seen from 360-degree viewpoints
captured with multiple LiDAR sensors. Our extensive experiments on the
scene-level KITTI-360 dataset, and on our object-level NeRF-MVL show that our
LiDAR-NeRF surpasses the model-based algorithms significantly.";Tang Tao<author:sep>Longfei Gao<author:sep>Guangrun Wang<author:sep>Yixing Lao<author:sep>Peng Chen<author:sep>Hengshuang Zhao<author:sep>Dayang Hao<author:sep>Xiaodan Liang<author:sep>Mathieu Salzmann<author:sep>Kaicheng Yu;http://arxiv.org/pdf/2304.10406v2;cs.CV;"This paper introduces a new task of novel LiDAR view synthesis, and
  proposes a differentiable framework called LiDAR-NeRF with a structural
  regularization, as well as an object-centric multi-view LiDAR dataset called
  NeRF-MVL";nerf
2304.10532v3;http://arxiv.org/abs/2304.10532v3;2023-04-20;Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs;"Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such
as floaters or flawed geometry when rendered outside the camera trajectory.
Existing evaluation protocols often do not capture these effects, since they
usually only assess image quality at every 8th frame of the training capture.
To push forward progress in novel-view synthesis, we propose a new dataset and
evaluation procedure, where two camera trajectories are recorded of the scene:
one used for training, and the other for evaluation. In this more challenging
in-the-wild setting, we find that existing hand-crafted regularizers do not
remove floaters nor improve scene geometry. Thus, we propose a 3D
diffusion-based method that leverages local 3D priors and a novel density-based
score distillation sampling loss to discourage artifacts during NeRF
optimization. We show that this data-driven prior removes floaters and improves
scene geometry for casual captures.";Frederik Warburg<author:sep>Ethan Weber<author:sep>Matthew Tancik<author:sep>Aleksander Holynski<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2304.10532v3;cs.CV;ICCV 2023, project page: https://ethanweber.me/nerfbusters;nerf
2304.10075v2;http://arxiv.org/abs/2304.10075v2;2023-04-20;Multiscale Representation for Real-Time Anti-Aliasing Neural Rendering;"The rendering scheme in neural radiance field (NeRF) is effective in
rendering a pixel by casting a ray into the scene. However, NeRF yields blurred
rendering results when the training images are captured at non-uniform scales,
and produces aliasing artifacts if the test images are taken in distant views.
To address this issue, Mip-NeRF proposes a multiscale representation as a
conical frustum to encode scale information. Nevertheless, this approach is
only suitable for offline rendering since it relies on integrated positional
encoding (IPE) to query a multilayer perceptron (MLP). To overcome this
limitation, we propose mip voxel grids (Mip-VoG), an explicit multiscale
representation with a deferred architecture for real-time anti-aliasing
rendering. Our approach includes a density Mip-VoG for scene geometry and a
feature Mip-VoG with a small MLP for view-dependent color. Mip-VoG encodes
scene scale using the level of detail (LOD) derived from ray differentials and
uses quadrilinear interpolation to map a queried 3D location to its features
and density from two neighboring downsampled voxel grids. To our knowledge, our
approach is the first to offer multiscale training and real-time anti-aliasing
rendering simultaneously. We conducted experiments on multiscale datasets, and
the results show that our approach outperforms state-of-the-art real-time
rendering baselines.";Dongting Hu<author:sep>Zhenkai Zhang<author:sep>Tingbo Hou<author:sep>Tongliang Liu<author:sep>Huan Fu<author:sep>Mingming Gong;http://arxiv.org/pdf/2304.10075v2;cs.CV;;nerf
2304.10537v1;http://arxiv.org/abs/2304.10537v1;2023-04-20;Learning Neural Duplex Radiance Fields for Real-Time View Synthesis;"Neural radiance fields (NeRFs) enable novel view synthesis with unprecedented
visual quality. However, to render photorealistic images, NeRFs require
hundreds of deep multilayer perceptron (MLP) evaluations - for each pixel. This
is prohibitively expensive and makes real-time rendering infeasible, even on
powerful modern GPUs. In this paper, we propose a novel approach to distill and
bake NeRFs into highly efficient mesh-based neural representations that are
fully compatible with the massively parallel graphics rendering pipeline. We
represent scenes as neural radiance features encoded on a two-layer duplex
mesh, which effectively overcomes the inherent inaccuracies in 3D surface
reconstruction by learning the aggregated radiance information from a reliable
interval of ray-surface intersections. To exploit local geometric relationships
of nearby pixels, we leverage screen-space convolutions instead of the MLPs
used in NeRFs to achieve high-quality appearance. Finally, the performance of
the whole framework is further boosted by a novel multi-view distillation
optimization strategy. We demonstrate the effectiveness and superiority of our
approach via extensive experiments on a range of standard datasets.";Ziyu Wan<author:sep>Christian Richardt<author:sep>AljaÅ¾ BoÅ¾iÄ<author:sep>Chao Li<author:sep>Vijay Rengarajan<author:sep>Seonghyeon Nam<author:sep>Xiaoyu Xiang<author:sep>Tuotuo Li<author:sep>Bo Zhu<author:sep>Rakesh Ranjan<author:sep>Jing Liao;http://arxiv.org/pdf/2304.10537v1;cs.CV;CVPR 2023. Project page: http://raywzy.com/NDRF;nerf
2304.10250v1;http://arxiv.org/abs/2304.10250v1;2023-04-20;Revisiting Implicit Neural Representations in Low-Level Vision;"Implicit Neural Representation (INR) has been emerging in computer vision in
recent years. It has been shown to be effective in parameterising continuous
signals such as dense 3D models from discrete image data, e.g. the neural
radius field (NeRF). However, INR is under-explored in 2D image processing
tasks. Considering the basic definition and the structure of INR, we are
interested in its effectiveness in low-level vision problems such as image
restoration. In this work, we revisit INR and investigate its application in
low-level image restoration tasks including image denoising, super-resolution,
inpainting, and deblurring. Extensive experimental evaluations suggest the
superior performance of INR in several low-level vision tasks with limited
resources, outperforming its counterparts by over 2dB. Code and models are
available at https://github.com/WenTXuL/LINR";Wentian Xu<author:sep>Jianbo Jiao;http://arxiv.org/pdf/2304.10250v1;cs.CV;"Published at the ICLR 2023 Neural Fields workshop. Project Webpage:
  https://wentxul.github.io/LINR-projectpage";nerf
2304.10050v2;http://arxiv.org/abs/2304.10050v2;2023-04-20;Neural Radiance Fields: Past, Present, and Future;"The various aspects like modeling and interpreting 3D environments and
surroundings have enticed humans to progress their research in 3D Computer
Vision, Computer Graphics, and Machine Learning. An attempt made by Mildenhall
et al in their paper about NeRFs (Neural Radiance Fields) led to a boom in
Computer Graphics, Robotics, Computer Vision, and the possible scope of
High-Resolution Low Storage Augmented Reality and Virtual Reality-based 3D
models have gained traction from res with more than 1000 preprints related to
NeRFs published. This paper serves as a bridge for people starting to study
these fields by building on the basics of Mathematics, Geometry, Computer
Vision, and Computer Graphics to the difficulties encountered in Implicit
Representations at the intersection of all these disciplines. This survey
provides the history of rendering, Implicit Learning, and NeRFs, the
progression of research on NeRFs, and the potential applications and
implications of NeRFs in today's world. In doing so, this survey categorizes
all the NeRF-related research in terms of the datasets used, objective
functions, applications solved, and evaluation criteria for these applications.";Ansh Mittal;http://arxiv.org/pdf/2304.10050v2;cs.CV;413 pages, 9 figures, 277 citations;nerf
2304.10664v1;http://arxiv.org/abs/2304.10664v1;2023-04-20;A Comparative Neural Radiance Field (NeRF) 3D Analysis of Camera Poses  from HoloLens Trajectories and Structure from Motion;"Neural Radiance Fields (NeRFs) are trained using a set of camera poses and
associated images as input to estimate density and color values for each
position. The position-dependent density learning is of particular interest for
photogrammetry, enabling 3D reconstruction by querying and filtering the NeRF
coordinate system based on the object density. While traditional methods like
Structure from Motion are commonly used for camera pose calculation in
pre-processing for NeRFs, the HoloLens offers an interesting interface for
extracting the required input data directly. We present a workflow for
high-resolution 3D reconstructions almost directly from HoloLens data using
NeRFs. Thereby, different investigations are considered: Internal camera poses
from the HoloLens trajectory via a server application, and external camera
poses from Structure from Motion, both with an enhanced variant applied through
pose refinement. Results show that the internal camera poses lead to NeRF
convergence with a PSNR of 25\,dB with a simple rotation around the x-axis and
enable a 3D reconstruction. Pose refinement enables comparable quality compared
to external camera poses, resulting in improved training process with a PSNR of
27\,dB and a better 3D reconstruction. Overall, NeRF reconstructions outperform
the conventional photogrammetric dense reconstruction using Multi-View Stereo
in terms of completeness and level of detail.";Miriam JÃ¤ger<author:sep>Patrick HÃ¼bner<author:sep>Dennis Haitz<author:sep>Boris Jutzi;http://arxiv.org/pdf/2304.10664v1;cs.CV;"7 pages, 5 figures. Will be published in the ISPRS The International
  Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences";nerf
2304.09677v2;http://arxiv.org/abs/2304.09677v2;2023-04-19;Reference-guided Controllable Inpainting of Neural Radiance Fields;"The popularity of Neural Radiance Fields (NeRFs) for view synthesis has led
to a desire for NeRF editing tools. Here, we focus on inpainting regions in a
view-consistent and controllable manner. In addition to the typical NeRF inputs
and masks delineating the unwanted region in each view, we require only a
single inpainted view of the scene, i.e., a reference view. We use monocular
depth estimators to back-project the inpainted view to the correct 3D
positions. Then, via a novel rendering technique, a bilateral solver can
construct view-dependent effects in non-reference views, making the inpainted
region appear consistent from any view. For non-reference disoccluded regions,
which cannot be supervised by the single reference view, we devise a method
based on image inpainters to guide both the geometry and appearance. Our
approach shows superior performance to NeRF inpainting baselines, with the
additional advantage that a user can control the generated scene via a single
inpainted image. Project page: https://ashmrz.github.io/reference-guided-3d";Ashkan Mirzaei<author:sep>Tristan Aumentado-Armstrong<author:sep>Marcus A. Brubaker<author:sep>Jonathan Kelly<author:sep>Alex Levinshtein<author:sep>Konstantinos G. Derpanis<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2304.09677v2;cs.CV;Project Page: https://ashmrz.github.io/reference-guided-3d;nerf
2304.09987v3;http://arxiv.org/abs/2304.09987v3;2023-04-19;Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra;"Neural Radiance Fields (NeRFs) are a very recent and very popular approach
for the problems of novel view synthesis and 3D reconstruction. A popular scene
representation used by NeRFs is to combine a uniform, voxel-based subdivision
of the scene with an MLP. Based on the observation that a (sparse) point cloud
of the scene is often available, this paper proposes to use an adaptive
representation based on tetrahedra obtained by Delaunay triangulation instead
of uniform subdivision or point-based representations. We show that such a
representation enables efficient training and leads to state-of-the-art
results. Our approach elegantly combines concepts from 3D geometry processing,
triangle-based rendering, and modern neural radiance fields. Compared to
voxel-based representations, ours provides more detail around parts of the
scene likely to be close to the surface. Compared to point-based
representations, our approach achieves better performance. The source code is
publicly available at: https://jkulhanek.com/tetra-nerf.";Jonas Kulhanek<author:sep>Torsten Sattler;http://arxiv.org/pdf/2304.09987v3;cs.CV;ICCV 2023, Web: https://jkulhanek.com/tetra-nerf;nerf
2304.10261v1;http://arxiv.org/abs/2304.10261v1;2023-04-19;Anything-3D: Towards Single-view Anything Reconstruction in the Wild;"3D reconstruction from a single-RGB image in unconstrained real-world
scenarios presents numerous challenges due to the inherent diversity and
complexity of objects and environments. In this paper, we introduce
Anything-3D, a methodical framework that ingeniously combines a series of
visual-language models and the Segment-Anything object segmentation model to
elevate objects to 3D, yielding a reliable and versatile system for single-view
conditioned 3D reconstruction task. Our approach employs a BLIP model to
generate textural descriptions, utilizes the Segment-Anything model for the
effective extraction of objects of interest, and leverages a text-to-image
diffusion model to lift object into a neural radiance field. Demonstrating its
ability to produce accurate and detailed 3D reconstructions for a wide array of
objects, \emph{Anything-3D\footnotemark[2]} shows promise in addressing the
limitations of existing methodologies. Through comprehensive experiments and
evaluations on various datasets, we showcase the merits of our approach,
underscoring its potential to contribute meaningfully to the field of 3D
reconstruction. Demos and code will be available at
\href{https://github.com/Anything-of-anything/Anything-3D}{https://github.com/Anything-of-anything/Anything-3D}.";Qiuhong Shen<author:sep>Xingyi Yang<author:sep>Xinchao Wang;http://arxiv.org/pdf/2304.10261v1;cs.CV;;
2304.08757v1;http://arxiv.org/abs/2304.08757v1;2023-04-18;NeAI: A Pre-convoluted Representation for Plug-and-Play Neural Ambient  Illumination;"Recent advances in implicit neural representation have demonstrated the
ability to recover detailed geometry and material from multi-view images.
However, the use of simplified lighting models such as environment maps to
represent non-distant illumination, or using a network to fit indirect light
modeling without a solid basis, can lead to an undesirable decomposition
between lighting and material. To address this, we propose a fully
differentiable framework named neural ambient illumination (NeAI) that uses
Neural Radiance Fields (NeRF) as a lighting model to handle complex lighting in
a physically based way. Together with integral lobe encoding for
roughness-adaptive specular lobe and leveraging the pre-convoluted background
for accurate decomposition, the proposed method represents a significant step
towards integrating physically based rendering into the NeRF representation.
The experiments demonstrate the superior performance of novel-view rendering
compared to previous works, and the capability to re-render objects under
arbitrary NeRF-style environments opens up exciting possibilities for bridging
the gap between virtual and real-world scenes. The project and supplementary
materials are available at https://yiyuzhuang.github.io/NeAI/.";Yiyu Zhuang<author:sep>Qi Zhang<author:sep>Xuan Wang<author:sep>Hao Zhu<author:sep>Ying Feng<author:sep>Xiaoyu Li<author:sep>Ying Shan<author:sep>Xun Cao;http://arxiv.org/pdf/2304.08757v1;cs.CV;"Project page: <a class=""link-external link-https""
  href=""https://yiyuzhuang.github.io/NeAI/"" rel=""external noopener
  nofollow"">https://yiyuzhuang.github.io/NeAI/</a>";nerf
2304.08971v1;http://arxiv.org/abs/2304.08971v1;2023-04-18;SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic  Reconstruction of Indoor Scenes;"Online reconstructing and rendering of large-scale indoor scenes is a
long-standing challenge. SLAM-based methods can reconstruct 3D scene geometry
progressively in real time but can not render photorealistic results. While
NeRF-based methods produce promising novel view synthesis results, their long
offline optimization time and lack of geometric constraints pose challenges to
efficiently handling online input. Inspired by the complementary advantages of
classical 3D reconstruction and NeRF, we thus investigate marrying explicit
geometric representation with NeRF rendering to achieve efficient online
reconstruction and high-quality rendering. We introduce SurfelNeRF, a variant
of neural radiance field which employs a flexible and scalable neural surfel
representation to store geometric attributes and extracted appearance features
from input images. We further extend the conventional surfel-based fusion
scheme to progressively integrate incoming input frames into the reconstructed
global neural scene representation. In addition, we propose a highly-efficient
differentiable rasterization scheme for rendering neural surfel radiance
fields, which helps SurfelNeRF achieve $10\times$ speedups in training and
inference time, respectively. Experimental results show that our method
achieves the state-of-the-art 23.82 PSNR and 29.58 PSNR on ScanNet in
feedforward inference and per-scene optimization settings, respectively.";Yiming Gao<author:sep>Yan-Pei Cao<author:sep>Ying Shan;http://arxiv.org/pdf/2304.08971v1;cs.CV;To appear in CVPR 2023;nerf
2304.07979v1;http://arxiv.org/abs/2304.07979v1;2023-04-17;NeRF-Loc: Visual Localization with Conditional Neural Radiance Field;"We propose a novel visual re-localization method based on direct matching
between the implicit 3D descriptors and the 2D image with transformer. A
conditional neural radiance field(NeRF) is chosen as the 3D scene
representation in our pipeline, which supports continuous 3D descriptors
generation and neural rendering. By unifying the feature matching and the scene
coordinate regression to the same framework, our model learns both
generalizable knowledge and scene prior respectively during two training
stages. Furthermore, to improve the localization robustness when domain gap
exists between training and testing phases, we propose an appearance adaptation
layer to explicitly align styles between the 3D model and the query image.
Experiments show that our method achieves higher localization accuracy than
other learning-based approaches on multiple benchmarks. Code is available at
\url{https://github.com/JenningsL/nerf-loc}.";Jianlin Liu<author:sep>Qiang Nie<author:sep>Yong Liu<author:sep>Chengjie Wang;http://arxiv.org/pdf/2304.07979v1;cs.CV;accepted by ICRA 2023;nerf
2304.08279v2;http://arxiv.org/abs/2304.08279v2;2023-04-17;MoDA: Modeling Deformable 3D Objects from Casual Videos;"In this paper, we focus on the challenges of modeling deformable 3D objects
from casual videos. With the popularity of neural radiance fields (NeRF), many
works extend it to dynamic scenes with a canonical NeRF and a deformation model
that achieves 3D point transformation between the observation space and the
canonical space. Recent works rely on linear blend skinning (LBS) to achieve
the canonical-observation transformation. However, the linearly weighted
combination of rigid transformation matrices is not guaranteed to be rigid. As
a matter of fact, unexpected scale and shear factors often appear. In practice,
using LBS as the deformation model can always lead to skin-collapsing artifacts
for bending or twisting motions. To solve this problem, we propose neural dual
quaternion blend skinning (NeuDBS) to achieve 3D point deformation, which can
perform rigid transformation without skin-collapsing artifacts. In the endeavor
to register 2D pixels across different frames, we establish a correspondence
between canonical feature embeddings that encodes 3D points within the
canonical space, and 2D image features by solving an optimal transport problem.
Besides, we introduce a texture filtering approach for texture rendering that
effectively minimizes the impact of noisy colors outside target deformable
objects. Extensive experiments on real and synthetic datasets show that our
approach can reconstruct 3D models for humans and animals with better
qualitative and quantitative performance than state-of-the-art methods.";Chaoyue Song<author:sep>Tianyi Chen<author:sep>Yiwen Chen<author:sep>Jiacheng Wei<author:sep>Chuan Sheng Foo<author:sep>Fayao Liu<author:sep>Guosheng Lin;http://arxiv.org/pdf/2304.08279v2;cs.CV;;nerf
2304.07915v1;http://arxiv.org/abs/2304.07915v1;2023-04-16;CAT-NeRF: Constancy-Aware Tx$^2$Former for Dynamic Body Modeling;"This paper addresses the problem of human rendering in the video with
temporal appearance constancy. Reconstructing dynamic body shapes with
volumetric neural rendering methods, such as NeRF, requires finding the
correspondence of the points in the canonical and observation space, which
demands understanding human body shape and motion. Some methods use rigid
transformation, such as SE(3), which cannot precisely model each frame's unique
motion and muscle movements. Others generate the transformation for each frame
with a trainable network, such as neural blend weight field or translation
vector field, which does not consider the appearance constancy of general body
shape. In this paper, we propose CAT-NeRF for self-awareness of appearance
constancy with Tx$^2$Former, a novel way to combine two Transformer layers, to
separate appearance constancy and uniqueness. Appearance constancy models the
general shape across the video, and uniqueness models the unique patterns for
each frame. We further introduce a novel Covariance Loss to limit the
correlation between each pair of appearance uniquenesses to ensure the
frame-unique pattern is maximally captured in appearance uniqueness. We assess
our method on H36M and ZJU-MoCap and show state-of-the-art performance.";Haidong Zhu<author:sep>Zhaoheng Zheng<author:sep>Wanrong Zheng<author:sep>Ram Nevatia;http://arxiv.org/pdf/2304.07915v1;cs.CV;;nerf
2304.07918v1;http://arxiv.org/abs/2304.07918v1;2023-04-16;Likelihood-Based Generative Radiance Field with Latent Space  Energy-Based Model for 3D-Aware Disentangled Image Representation;"We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image
generative model that incorporates 3D representation via Neural Radiance Fields
(NeRF) and 2D imaging process via differentiable volume rendering. The model
represents an image as a rendering process from 3D object to 2D image and is
conditioned on some latent variables that account for object characteristics
and are assumed to follow informative trainable energy-based prior models. We
propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i)
maximum likelihood estimation with Markov chain Monte Carlo-based inference and
(ii) variational inference with the reparameterization trick. We study our
models in the scenarios with both known and unknown camera poses. Experiments
on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D
object structures from 2D images, generate 2D images with novel views and
objects, learn from incomplete 2D images, and learn from 2D images with known
or unknown camera poses.";Yaxuan Zhu<author:sep>Jianwen Xie<author:sep>Ping Li;http://arxiv.org/pdf/2304.07918v1;cs.CV;;nerf
2304.07743v1;http://arxiv.org/abs/2304.07743v1;2023-04-16;SeaThru-NeRF: Neural Radiance Fields in Scattering Media;"Research on neural radiance fields (NeRFs) for novel view generation is
exploding with new models and extensions. However, a question that remains
unanswered is what happens in underwater or foggy scenes where the medium
strongly influences the appearance of objects. Thus far, NeRF and its variants
have ignored these cases. However, since the NeRF framework is based on
volumetric rendering, it has inherent capability to account for the medium's
effects, once modeled appropriately. We develop a new rendering model for NeRFs
in scattering media, which is based on the SeaThru image formation model, and
suggest a suitable architecture for learning both scene information and medium
parameters. We demonstrate the strength of our method using simulated and
real-world scenes, correctly rendering novel photorealistic views underwater.
Even more excitingly, we can render clear views of these scenes, removing the
medium between the camera and the scene and reconstructing the appearance and
depth of far objects, which are severely occluded by the medium. Our code and
unique datasets are available on the project's website.";Deborah Levy<author:sep>Amit Peleg<author:sep>Naama Pearl<author:sep>Dan Rosenbaum<author:sep>Derya Akkaynak<author:sep>Simon Korman<author:sep>Tali Treibitz;http://arxiv.org/pdf/2304.07743v1;cs.CV;;nerf
2304.06969v1;http://arxiv.org/abs/2304.06969v1;2023-04-14;UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose  rendering, Geometry and Texture Editing;"Neural radiance field (NeRF) has become a popular 3D representation method
for human avatar reconstruction due to its high-quality rendering capabilities,
e.g., regarding novel views and poses. However, previous methods for editing
the geometry and appearance of the avatar only allow for global editing through
body shape parameters and 2D texture maps. In this paper, we propose a new
approach named \textbf{U}nified \textbf{V}olumetric \textbf{A}vatar
(\textbf{UVA}) that enables local and independent editing of both geometry and
texture, while retaining the ability to render novel views and poses. UVA
transforms each observation point to a canonical space using a skinning motion
field and represents geometry and texture in separate neural fields. Each field
is composed of a set of structured latent codes that are attached to anchor
nodes on a deformable mesh in canonical space and diffused into the entire
space via interpolation, allowing for local editing. To address spatial
ambiguity in code interpolation, we use a local signed height indicator. We
also replace the view-dependent radiance color with a pose-dependent shading
factor to better represent surface illumination in different poses. Experiments
on multiple human avatars demonstrate that our UVA achieves competitive results
in novel view synthesis and novel pose rendering while enabling local and
independent editing of geometry and appearance. The source code will be
released.";Jinlong Fan<author:sep>Jing Zhang<author:sep>Dacheng Tao;http://arxiv.org/pdf/2304.06969v1;cs.CV;;nerf
2304.06287v2;http://arxiv.org/abs/2304.06287v2;2023-04-13;NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry  Scaffolds;"We present NeRFVS, a novel neural radiance fields (NeRF) based method to
enable free navigation in a room. NeRF achieves impressive performance in
rendering images for novel views similar to the input views while suffering for
novel views that are significantly different from the training views. To
address this issue, we utilize the holistic priors, including pseudo depth maps
and view coverage information, from neural reconstruction to guide the learning
of implicit neural representations of 3D indoor scenes. Concretely, an
off-the-shelf neural reconstruction method is leveraged to generate a geometry
scaffold. Then, two loss functions based on the holistic priors are proposed to
improve the learning of NeRF: 1) A robust depth loss that can tolerate the
error of the pseudo depth map to guide the geometry learning of NeRF; 2) A
variance loss to regularize the variance of implicit neural representations to
reduce the geometry and color ambiguity in the learning procedure. These two
loss functions are modulated during NeRF optimization according to the view
coverage information to reduce the negative influence brought by the view
coverage imbalance. Extensive results demonstrate that our NeRFVS outperforms
state-of-the-art view synthesis methods quantitatively and qualitatively on
indoor scenes, achieving high-fidelity free navigation results.";Chen Yang<author:sep>Peihao Li<author:sep>Zanwei Zhou<author:sep>Shanxin Yuan<author:sep>Bingbing Liu<author:sep>Xiaokang Yang<author:sep>Weichao Qiu<author:sep>Wei Shen;http://arxiv.org/pdf/2304.06287v2;cs.CV;10 pages, 7 figures;nerf
2304.06706v3;http://arxiv.org/abs/2304.06706v3;2023-04-13;Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields;"Neural Radiance Field training can be accelerated through the use of
grid-based representations in NeRF's learned mapping from spatial coordinates
to colors and volumetric density. However, these grid-based approaches lack an
explicit understanding of scale and therefore often introduce aliasing, usually
in the form of jaggies or missing scene content. Anti-aliasing has previously
been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone
rather than points along a ray, but this approach is not natively compatible
with current grid-based techniques. We show how ideas from rendering and signal
processing can be used to construct a technique that combines mip-NeRF 360 and
grid-based models such as Instant NGP to yield error rates that are 8% - 77%
lower than either prior technique, and that trains 24x faster than mip-NeRF
360.";Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Dor Verbin<author:sep>Pratul P. Srinivasan<author:sep>Peter Hedman;http://arxiv.org/pdf/2304.06706v3;cs.CV;Project page: https://jonbarron.info/zipnerf/;nerf
2304.06714v4;http://arxiv.org/abs/2304.06714v4;2023-04-13;Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and  Reconstruction;"3D-aware image synthesis encompasses a variety of tasks, such as scene
generation and novel view synthesis from images. Despite numerous task-specific
methods, developing a comprehensive model remains challenging. In this paper,
we present SSDNeRF, a unified approach that employs an expressive diffusion
model to learn a generalizable prior of neural radiance fields (NeRF) from
multi-view images of diverse objects. Previous studies have used two-stage
approaches that rely on pretrained NeRFs as real data to train diffusion
models. In contrast, we propose a new single-stage training paradigm with an
end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent
diffusion model, enabling simultaneous 3D reconstruction and prior learning,
even from sparsely available views. At test time, we can directly sample the
diffusion prior for unconditional generation, or combine it with arbitrary
observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates
robust results comparable to or better than leading task-specific methods in
unconditional generation and single/sparse-view 3D reconstruction.";Hansheng Chen<author:sep>Jiatao Gu<author:sep>Anpei Chen<author:sep>Wei Tian<author:sep>Zhuowen Tu<author:sep>Lingjie Liu<author:sep>Hao Su;http://arxiv.org/pdf/2304.06714v4;cs.CV;"ICCV 2023 final version. Project page:
  https://lakonik.github.io/ssdnerf";nerf
2304.05735v2;http://arxiv.org/abs/2304.05735v2;2023-04-12;RO-MAP: Real-Time Multi-Object Mapping with Neural Radiance Fields;"Accurate perception of objects in the environment is important for improving
the scene understanding capability of SLAM systems. In robotic and augmented
reality applications, object maps with semantic and metric information show
attractive advantages. In this paper, we present RO-MAP, a novel multi-object
mapping pipeline that does not rely on 3D priors. Given only monocular input,
we use neural radiance fields to represent objects and couple them with a
lightweight object SLAM based on multi-view geometry, to simultaneously
localize objects and implicitly learn their dense geometry. We create separate
implicit models for each detected object and train them dynamically and in
parallel as new observations are added. Experiments on synthetic and real-world
datasets demonstrate that our method can generate semantic object map with
shape reconstruction, and be competitive with offline methods while achieving
real-time performance (25Hz). The code and dataset will be available at:
https://github.com/XiaoHan-Git/RO-MAP";Xiao Han<author:sep>Houxuan Liu<author:sep>Yunchao Ding<author:sep>Lu Yang;http://arxiv.org/pdf/2304.05735v2;cs.RO;"The code and dataset are available at:
  https://github.com/XiaoHan-Git/RO-MAP";
2304.05620v1;http://arxiv.org/abs/2304.05620v1;2023-04-12;NutritionVerse-Thin: An Optimized Strategy for Enabling Improved  Rendering of 3D Thin Food Models;"With the growth in capabilities of generative models, there has been growing
interest in using photo-realistic renders of common 3D food items to improve
downstream tasks such as food printing, nutrition prediction, or management of
food wastage. Despite 3D modelling capabilities being more accessible than ever
due to the success of NeRF based view-synthesis, such rendering methods still
struggle to correctly capture thin food objects, often generating meshes with
significant holes. In this study, we present an optimized strategy for enabling
improved rendering of thin 3D food models, and demonstrate qualitative
improvements in rendering quality. Our method generates the 3D model mesh via a
proposed thin-object-optimized differentiable reconstruction method and tailors
the strategy at both the data collection and training stages to better handle
thin objects. While simple, we find that this technique can be employed for
quick and highly consistent capturing of thin 3D objects.";Chi-en Amy Tai<author:sep>Jason Li<author:sep>Sriram Kumar<author:sep>Saeejith Nair<author:sep>Yuhao Chen<author:sep>Pengcheng Xi<author:sep>Alexander Wong;http://arxiv.org/pdf/2304.05620v1;cs.CV;;nerf
2304.05097v1;http://arxiv.org/abs/2304.05097v1;2023-04-11;One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural  Radiance Field;"Talking head generation aims to generate faces that maintain the identity
information of the source image and imitate the motion of the driving image.
Most pioneering methods rely primarily on 2D representations and thus will
inevitably suffer from face distortion when large head rotations are
encountered. Recent works instead employ explicit 3D structural representations
or implicit neural rendering to improve performance under large pose changes.
Nevertheless, the fidelity of identity and expression is not so desirable,
especially for novel-view synthesis. In this paper, we propose HiDe-NeRF, which
achieves high-fidelity and free-view talking-head synthesis. Drawing on the
recently proposed Deformable Neural Radiance Fields, HiDe-NeRF represents the
3D dynamic scene into a canonical appearance field and an implicit deformation
field, where the former comprises the canonical source face and the latter
models the driving pose and expression. In particular, we improve fidelity from
two aspects: (i) to enhance identity expressiveness, we design a generalized
appearance module that leverages multi-scale volume features to preserve face
shape and details; (ii) to improve expression preciseness, we propose a
lightweight deformation module that explicitly decouples the pose and
expression to enable precise expression modeling. Extensive experiments
demonstrate that our proposed approach can generate better results than
previous works. Project page: https://www.waytron.net/hidenerf/";Weichuang Li<author:sep>Longhao Zhang<author:sep>Dong Wang<author:sep>Bin Zhao<author:sep>Zhigang Wang<author:sep>Mulin Chen<author:sep>Bang Zhang<author:sep>Zhongjian Wang<author:sep>Liefeng Bo<author:sep>Xuelong Li;http://arxiv.org/pdf/2304.05097v1;cs.CV;Accepted by CVPR 2023;nerf
2304.04962v1;http://arxiv.org/abs/2304.04962v1;2023-04-11;MRVM-NeRF: Mask-Based Pretraining for Neural Radiance Fields;"Most Neural Radiance Fields (NeRFs) have poor generalization ability,
limiting their application when representing multiple scenes by a single model.
To ameliorate this problem, existing methods simply condition NeRF models on
image features, lacking the global understanding and modeling of the entire 3D
scene. Inspired by the significant success of mask-based modeling in other
research fields, we propose a masked ray and view modeling method for
generalizable NeRF (MRVM-NeRF), the first attempt to incorporate mask-based
pretraining into 3D implicit representations. Specifically, considering that
the core of NeRFs lies in modeling 3D representations along the rays and across
the views, we randomly mask a proportion of sampled points along the ray at
fine stage by discarding partial information obtained from multi-viewpoints,
targeting at predicting the corresponding features produced in the coarse
branch. In this way, the learned prior knowledge of 3D scenes during
pretraining helps the model generalize better to novel scenarios after
finetuning. Extensive experiments demonstrate the superiority of our proposed
MRVM-NeRF under various synthetic and real-world settings, both qualitatively
and quantitatively. Our empirical studies reveal the effectiveness of our
proposed innovative MRVM which is specifically designed for NeRF models.";Ganlin Yang<author:sep>Guoqiang Wei<author:sep>Zhizheng Zhang<author:sep>Yan Lu<author:sep>Dong Liu;http://arxiv.org/pdf/2304.04962v1;cs.CV;;nerf
2304.05218v1;http://arxiv.org/abs/2304.05218v1;2023-04-11;Improving Neural Radiance Fields with Depth-aware Optimization for Novel  View Synthesis;"With dense inputs, Neural Radiance Fields (NeRF) is able to render
photo-realistic novel views under static conditions. Although the synthesis
quality is excellent, existing NeRF-based methods fail to obtain moderate
three-dimensional (3D) structures. The novel view synthesis quality drops
dramatically given sparse input due to the implicitly reconstructed inaccurate
3D-scene structure. We propose SfMNeRF, a method to better synthesize novel
views as well as reconstruct the 3D-scene geometry. SfMNeRF leverages the
knowledge from the self-supervised depth estimation methods to constrain the
3D-scene geometry during view synthesis training. Specifically, SfMNeRF employs
the epipolar, photometric consistency, depth smoothness, and
position-of-matches constraints to explicitly reconstruct the 3D-scene
structure. Through these explicit constraints and the implicit constraint from
NeRF, our method improves the view synthesis as well as the 3D-scene geometry
performance of NeRF at the same time. In addition, SfMNeRF synthesizes novel
sub-pixels in which the ground truth is obtained by image interpolation. This
strategy enables SfMNeRF to include more samples to improve generalization
performance. Experiments on two public datasets demonstrate that SfMNeRF
surpasses state-of-the-art approaches. Code is available at
https://github.com/XTU-PR-LAB/SfMNeRF";Shu Chen<author:sep>Junyao Li<author:sep>Yang Zhang<author:sep>Beiji Zou;http://arxiv.org/pdf/2304.05218v1;cs.CV;;nerf
2304.04446v1;http://arxiv.org/abs/2304.04446v1;2023-04-10;Inferring Fluid Dynamics via Inverse Rendering;"Humans have a strong intuitive understanding of physical processes such as
fluid falling by just a glimpse of such a scene picture, i.e., quickly derived
from our immersive visual experiences in memory. This work achieves such a
photo-to-fluid-dynamics reconstruction functionality learned from unannotated
videos, without any supervision of ground-truth fluid dynamics. In a nutshell,
a differentiable Euler simulator modeled with a ConvNet-based pressure
projection solver, is integrated with a volumetric renderer, supporting
end-to-end/coherent differentiable dynamic simulation and rendering. By
endowing each sampled point with a fluid volume value, we derive a NeRF-like
differentiable renderer dedicated from fluid data; and thanks to this
volume-augmented representation, fluid dynamics could be inversely inferred
from the error signal between the rendered result and ground-truth video frame
(i.e., inverse rendering). Experiments on our generated Fluid Fall datasets and
DPI Dam Break dataset are conducted to demonstrate both effectiveness and
generalization ability of our method.";Jinxian Liu<author:sep>Ye Chen<author:sep>Bingbing Ni<author:sep>Jiyao Mao<author:sep>Zhenbo Yu;http://arxiv.org/pdf/2304.04446v1;cs.CV;;nerf
2304.04897v1;http://arxiv.org/abs/2304.04897v1;2023-04-10;Neural Image-based Avatars: Generalizable Radiance Fields for Human  Avatar Modeling;"We present a method that enables synthesizing novel views and novel poses of
arbitrary human performers from sparse multi-view images. A key ingredient of
our method is a hybrid appearance blending module that combines the advantages
of the implicit body NeRF representation and image-based rendering. Existing
generalizable human NeRF methods that are conditioned on the body model have
shown robustness against the geometric variation of arbitrary human performers.
Yet they often exhibit blurry results when generalized onto unseen identities.
Meanwhile, image-based rendering shows high-quality results when sufficient
observations are available, whereas it suffers artifacts in sparse-view
settings. We propose Neural Image-based Avatars (NIA) that exploits the best of
those two methods: to maintain robustness under new articulations and
self-occlusions while directly leveraging the available (sparse) source view
colors to preserve appearance details of new subject identities. Our hybrid
design outperforms recent methods on both in-domain identity generalization as
well as challenging cross-dataset generalization settings. Also, in terms of
the pose generalization, our method outperforms even the per-subject optimized
animatable NeRF methods. The video results are available at
https://youngjoongunc.github.io/nia";Youngjoong Kwon<author:sep>Dahun Kim<author:sep>Duygu Ceylan<author:sep>Henry Fuchs;http://arxiv.org/pdf/2304.04897v1;cs.CV;;nerf
2304.04452v2;http://arxiv.org/abs/2304.04452v2;2023-04-10;Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos;"The success of the Neural Radiance Fields (NeRFs) for modeling and free-view
rendering static objects has inspired numerous attempts on dynamic scenes.
Current techniques that utilize neural rendering for facilitating free-view
videos (FVVs) are restricted to either offline rendering or are capable of
processing only brief sequences with minimal motion. In this paper, we present
a novel technique, Residual Radiance Field or ReRF, as a highly compact neural
representation to achieve real-time FVV rendering on long-duration dynamic
scenes. ReRF explicitly models the residual information between adjacent
timestamps in the spatial-temporal feature space, with a global
coordinate-based tiny MLP as the feature decoder. Specifically, ReRF employs a
compact motion grid along with a residual feature grid to exploit inter-frame
feature similarities. We show such a strategy can handle large motions without
sacrificing quality. We further present a sequential training scheme to
maintain the smoothness and the sparsity of the motion/residual grids. Based on
ReRF, we design a special FVV codec that achieves three orders of magnitudes
compression rate and provides a companion ReRF player to support online
streaming of long-duration FVVs of dynamic scenes. Extensive experiments
demonstrate the effectiveness of ReRF for compactly representing dynamic
radiance fields, enabling an unprecedented free-viewpoint viewing experience in
speed and quality.";Liao Wang<author:sep>Qiang Hu<author:sep>Qihan He<author:sep>Ziyu Wang<author:sep>Jingyi Yu<author:sep>Tinne Tuytelaars<author:sep>Lan Xu<author:sep>Minye Wu;http://arxiv.org/pdf/2304.04452v2;cs.CV;"Accepted by CVPR 2023. Project page, see
  https://aoliao12138.github.io/ReRF/";nerf
2304.04395v3;http://arxiv.org/abs/2304.04395v3;2023-04-10;Instance Neural Radiance Field;"This paper presents one of the first learning-based NeRF 3D instance
segmentation pipelines, dubbed as Instance Neural Radiance Field, or Instance
NeRF. Taking a NeRF pretrained from multi-view RGB images as input, Instance
NeRF can learn 3D instance segmentation of a given scene, represented as an
instance field component of the NeRF model. To this end, we adopt a 3D
proposal-based mask prediction network on the sampled volumetric features from
NeRF, which generates discrete 3D instance masks. The coarse 3D mask prediction
is then projected to image space to match 2D segmentation masks from different
views generated by existing panoptic segmentation models, which are used to
supervise the training of the instance field. Notably, beyond generating
consistent 2D segmentation maps from novel views, Instance NeRF can query
instance information at any 3D point, which greatly enhances NeRF object
segmentation and manipulation. Our method is also one of the first to achieve
such results in pure inference. Experimented on synthetic and real-world NeRF
datasets with complex indoor scenes, Instance NeRF surpasses previous NeRF
segmentation works and competitive 2D segmentation methods in segmentation
performance on unseen views. Watch the demo video at
https://youtu.be/wW9Bme73coI. Code and data are available at
https://github.com/lyclyc52/Instance_NeRF.";Yichen Liu<author:sep>Benran Hu<author:sep>Junkai Huang<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2304.04395v3;cs.CV;International Conference on Computer Vision (ICCV) 2023;nerf
2304.04133v4;http://arxiv.org/abs/2304.04133v4;2023-04-09;NeRF applied to satellite imagery for surface reconstruction;"We present Surf-NeRF, a modified implementation of the recently introduced
Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize
novel views from a sparse set of satellite images of a scene, while accounting
for the variation in lighting present in the pictures. The trained model can
also be used to accurately estimate the surface elevation of the scene, which
is often a desirable quantity for satellite observation applications. S-NeRF
improves on the standard Neural Radiance Field (NeRF) method by considering the
radiance as a function of the albedo and the irradiance. Both these quantities
are output by fully connected neural network branches of the model, and the
latter is considered as a function of the direct light from the sun and the
diffuse color from the sky. The implementations were run on a dataset of
satellite images, augmented using a zoom-and-crop technique. A hyperparameter
study for NeRF was carried out, leading to intriguing observations on the
model's convergence. Finally, both NeRF and S-NeRF were run until 100k epochs
in order to fully fit the data and produce their best possible predictions. The
code related to this article can be found at
https://github.com/fsemerar/surfnerf.";Federico Semeraro<author:sep>Yi Zhang<author:sep>Wenying Wu<author:sep>Patrick Carroll;http://arxiv.org/pdf/2304.04133v4;cs.CV;;nerf
2304.04012v1;http://arxiv.org/abs/2304.04012v1;2023-04-08;PVD-AL: Progressive Volume Distillation with Active Learning for  Efficient Conversion Between Different NeRF Architectures;"Neural Radiance Fields (NeRF) have been widely adopted as practical and
versatile representations for 3D scenes, facilitating various downstream tasks.
However, different architectures, including plain Multi-Layer Perceptron (MLP),
Tensors, low-rank Tensors, Hashtables, and their compositions, have their
trade-offs. For instance, Hashtables-based representations allow for faster
rendering but lack clear geometric meaning, making spatial-relation-aware
editing challenging. To address this limitation and maximize the potential of
each architecture, we propose Progressive Volume Distillation with Active
Learning (PVD-AL), a systematic distillation method that enables any-to-any
conversions between different architectures. PVD-AL decomposes each structure
into two parts and progressively performs distillation from shallower to deeper
volume representation, leveraging effective information retrieved from the
rendering process. Additionally, a Three-Levels of active learning technique
provides continuous feedback during the distillation process, resulting in
high-performance results. Empirical evidence is presented to validate our
method on multiple benchmark datasets. For example, PVD-AL can distill an
MLP-based model from a Hashtables-based model at a 10~20X faster speed and
0.8dB~2dB higher PSNR than training the NeRF model from scratch. Moreover,
PVD-AL permits the fusion of diverse features among distinct structures,
enabling models with multiple editing properties and providing a more efficient
model to meet real-time requirements. Project website:http://sk-fun.fun/PVD-AL.";Shuangkang Fang<author:sep>Yufeng Wang<author:sep>Yi Yang<author:sep>Weixin Xu<author:sep>Heng Wang<author:sep>Wenrui Ding<author:sep>Shuchang Zhou;http://arxiv.org/pdf/2304.04012v1;cs.CV;"Project website: http://sk-fun.fun/PVD-AL. arXiv admin note:
  substantial text overlap with arXiv:2211.15977";nerf
2304.03526v1;http://arxiv.org/abs/2304.03526v1;2023-04-07;Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative  Radiance Field;"This work explores the use of 3D generative models to synthesize training
data for 3D vision tasks. The key requirements of the generative models are
that the generated data should be photorealistic to match the real-world
scenarios, and the corresponding 3D attributes should be aligned with given
sampling labels. However, we find that the recent NeRF-based 3D GANs hardly
meet the above requirements due to their designed generation pipeline and the
lack of explicit 3D supervision. In this work, we propose Lift3D, an inverted
2D-to-3D generation framework to achieve the data generation objectives. Lift3D
has several merits compared to prior methods: (1) Unlike previous 3D GANs that
the output resolution is fixed after training, Lift3D can generalize to any
camera intrinsic with higher resolution and photorealistic output. (2) By
lifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3D
information of generated objects, thus offering accurate 3D annotations for
downstream tasks. We evaluate the effectiveness of our framework by augmenting
autonomous driving datasets. Experimental results demonstrate that our data
generation framework can effectively improve the performance of 3D object
detectors. Project page: https://len-li.github.io/lift3d-web.";Leheng Li<author:sep>Qing Lian<author:sep>Luozhou Wang<author:sep>Ningning Ma<author:sep>Ying-Cong Chen;http://arxiv.org/pdf/2304.03526v1;cs.CV;CVPR 2023;nerf
2304.04559v1;http://arxiv.org/abs/2304.04559v1;2023-04-07;Event-based Camera Tracker by $\nabla$t NeRF;"When a camera travels across a 3D world, only a fraction of pixel value
changes; an event-based camera observes the change as sparse events. How can we
utilize sparse events for efficient recovery of the camera pose? We show that
we can recover the camera pose by minimizing the error between sparse events
and the temporal gradient of the scene represented as a neural radiance field
(NeRF). To enable the computation of the temporal gradient of the scene, we
augment NeRF's camera pose as a time function. When the input pose to the NeRF
coincides with the actual pose, the output of the temporal gradient of NeRF
equals the observed intensity changes on the event's points. Using this
principle, we propose an event-based camera pose tracking framework called
TeGRA which realizes the pose update by using the sparse event's observation.
To the best of our knowledge, this is the first camera pose estimation
algorithm using the scene's implicit representation and the sparse intensity
change from events.";Mana Masuda<author:sep>Yusuke Sekikawa<author:sep>Hideo Saito;http://arxiv.org/pdf/2304.04559v1;cs.CV;;nerf
2304.03266v1;http://arxiv.org/abs/2304.03266v1;2023-04-06;Neural Fields meet Explicit Geometric Representation for Inverse  Rendering of Urban Scenes;"Reconstruction and intrinsic decomposition of scenes from captured imagery
would enable many applications such as relighting and virtual object insertion.
Recent NeRF based methods achieve impressive fidelity of 3D reconstruction, but
bake the lighting and shadows into the radiance field, while mesh-based methods
that facilitate intrinsic decomposition through differentiable rendering have
not yet scaled to the complexity and scale of outdoor scenes. We present a
novel inverse rendering framework for large urban scenes capable of jointly
reconstructing the scene geometry, spatially-varying materials, and HDR
lighting from a set of posed RGB images with optional depth. Specifically, we
use a neural field to account for the primary rays, and use an explicit mesh
(reconstructed from the underlying neural field) for modeling secondary rays
that produce higher-order lighting effects such as cast shadows. By faithfully
disentangling complex geometry and materials from lighting effects, our method
enables photorealistic relighting with specular and shadow effects on several
outdoor datasets. Moreover, it supports physics-based scene manipulations such
as virtual object insertion with ray-traced shadow casting.";Zian Wang<author:sep>Tianchang Shen<author:sep>Jun Gao<author:sep>Shengyu Huang<author:sep>Jacob Munkberg<author:sep>Jon Hasselgren<author:sep>Zan Gojcic<author:sep>Wenzheng Chen<author:sep>Sanja Fidler;http://arxiv.org/pdf/2304.03266v1;cs.CV;CVPR 2023. Project page: https://nv-tlabs.github.io/fegr/;nerf
2304.03384v2;http://arxiv.org/abs/2304.03384v2;2023-04-06;Beyond NeRF Underwater: Learning Neural Reflectance Fields for True  Color Correction of Marine Imagery;"Underwater imagery often exhibits distorted coloration as a result of
light-water interactions, which complicates the study of benthic environments
in marine biology and geography. In this research, we propose an algorithm to
restore the true color (albedo) in underwater imagery by jointly learning the
effects of the medium and neural scene representations. Our approach models
water effects as a combination of light attenuation with distance and
backscattered light. The proposed neural scene representation is based on a
neural reflectance field model, which learns albedos, normals, and volume
densities of the underwater environment. We introduce a logistic regression
model to separate water from the scene and apply distinct light physics during
training. Our method avoids the need to estimate complex backscatter effects in
water by employing several approximations, enhancing sampling efficiency and
numerical stability during training. The proposed technique integrates
underwater light effects into a volume rendering framework with end-to-end
differentiability. Experimental results on both synthetic and real-world data
demonstrate that our method effectively restores true color from underwater
imagery, outperforming existing approaches in terms of color consistency.";Tianyi Zhang<author:sep>Matthew Johnson-Roberson;http://arxiv.org/pdf/2304.03384v2;cs.CV;Robotics and Automation Letters (RA-L) VOL. 8, NO. 10, OCTOBER 2023;nerf
2304.02827v1;http://arxiv.org/abs/2304.02827v1;2023-04-06;DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model;"The increasing demand for high-quality 3D content creation has motivated the
development of automated methods for creating 3D object models from a single
image and/or from a text prompt. However, the reconstructed 3D objects using
state-of-the-art image-to-3D methods still exhibit low correspondence to the
given image and low multi-view consistency. Recent state-of-the-art text-to-3D
methods are also limited, yielding 3D samples with low diversity per prompt
with long synthesis time. To address these challenges, we propose DITTO-NeRF, a
novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a
single image. Our DITTO-NeRF consists of constructing high-quality partial 3D
object for limited in-boundary (IB) angles using the given or text-generated 2D
image from the frontal view and then iteratively reconstructing the remaining
3D NeRF using inpainting latent diffusion model. We propose progressive 3D
object reconstruction schemes in terms of scales (low to high resolution),
angles (IB angles initially to outer-boundary (OB) later), and masks (object to
background boundary) in our DITTO-NeRF so that high-quality information on IB
can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods
in terms of fidelity and diversity qualitatively and quantitatively with much
faster training times than prior arts on image/text-to-3D such as DreamFusion,
and NeuralLift-360.";Hoigi Seo<author:sep>Hayeon Kim<author:sep>Gwanghyun Kim<author:sep>Se Young Chun;http://arxiv.org/pdf/2304.02827v1;cs.CV;Project page: https://janeyeon.github.io/ditto-nerf/;nerf
2304.03280v1;http://arxiv.org/abs/2304.03280v1;2023-04-06;LANe: Lighting-Aware Neural Fields for Compositional Scene Synthesis;"Neural fields have recently enjoyed great success in representing and
rendering 3D scenes. However, most state-of-the-art implicit representations
model static or dynamic scenes as a whole, with minor variations. Existing work
on learning disentangled world and object neural fields do not consider the
problem of composing objects into different world neural fields in a
lighting-aware manner. We present Lighting-Aware Neural Field (LANe) for the
compositional synthesis of driving scenes in a physically consistent manner.
Specifically, we learn a scene representation that disentangles the static
background and transient elements into a world-NeRF and class-specific
object-NeRFs to allow compositional synthesis of multiple objects in the scene.
Furthermore, we explicitly designed both the world and object models to handle
lighting variation, which allows us to compose objects into scenes with
spatially varying lighting. This is achieved by constructing a light field of
the scene and using it in conjunction with a learned shader to modulate the
appearance of the object NeRFs. We demonstrate the performance of our model on
a synthetic dataset of diverse lighting conditions rendered with the CARLA
simulator, as well as a novel real-world dataset of cars collected at different
times of the day. Our approach shows that it outperforms state-of-the-art
compositional scene synthesis on the challenging dataset setup, via composing
object-NeRFs learned from one scene into an entirely different scene whilst
still respecting the lighting variations in the novel scene. For more results,
please visit our project website https://lane-composition.github.io/.";Akshay Krishnan<author:sep>Amit Raj<author:sep>Xianling Zhang<author:sep>Alexandra Carlson<author:sep>Nathan Tseng<author:sep>Sandhya Sridhar<author:sep>Nikita Jaipuria<author:sep>James Hays;http://arxiv.org/pdf/2304.03280v1;cs.CV;Project website: https://lane-composition.github.io;nerf
2304.02736v1;http://arxiv.org/abs/2304.02736v1;2023-04-05;Image Stabilization for Hololens Camera in Remote Collaboration;"With the advent of new technologies, Augmented Reality (AR) has become an
effective tool in remote collaboration. Narrow field-of-view (FoV) and motion
blur can offer an unpleasant experience with limited cognition for remote
viewers of AR headsets. In this article, we propose a two-stage pipeline to
tackle this issue and ensure a stable viewing experience with a larger FoV. The
solution involves an offline 3D reconstruction of the indoor environment,
followed by enhanced rendering using only the live poses of AR device. We
experiment with and evaluate the two different 3D reconstruction methods, RGB-D
geometric approach and Neural Radiance Fields (NeRF), based on their data
requirements, reconstruction quality, rendering, and training times. The
generated sequences from these methods had smoother transitions and provided a
better perspective of the environment. The geometry-based enhanced FoV method
had better renderings as it lacked blurry outputs making it better than the
other attempted approaches. Structural Similarity Index (SSIM) and Peak Signal
to Noise Ratio (PSNR) metrics were used to quantitatively show that the
rendering quality using the geometry-based enhanced FoV method is better. Link
to the code repository -
https://github.com/MixedRealityETHZ/ImageStabilization.";Gowtham Senthil<author:sep>Siva Vignesh Krishnan<author:sep>Annamalai Lakshmanan<author:sep>Florence Kissling;http://arxiv.org/pdf/2304.02736v1;cs.CV;;nerf
2304.01436v1;http://arxiv.org/abs/2304.01436v1;2023-04-04;Learning Personalized High Quality Volumetric Head Avatars from  Monocular RGB Videos;"We propose a method to learn a high-quality implicit 3D head avatar from a
monocular RGB video captured in the wild. The learnt avatar is driven by a
parametric face model to achieve user-controlled facial expressions and head
poses. Our hybrid pipeline combines the geometry prior and dynamic tracking of
a 3DMM with a neural radiance field to achieve fine-grained control and
photorealism. To reduce over-smoothing and improve out-of-model expressions
synthesis, we propose to predict local features anchored on the 3DMM geometry.
These learnt features are driven by 3DMM deformation and interpolated in 3D
space to yield the volumetric radiance at a designated query point. We further
show that using a Convolutional Neural Network in the UV space is critical in
incorporating spatial context and producing representative local features.
Extensive experiments show that we are able to reconstruct high-quality
avatars, with more accurate expression-dependent details, good generalization
to out-of-training expressions, and quantitatively superior renderings compared
to other state-of-the-art approaches.";Ziqian Bai<author:sep>Feitong Tan<author:sep>Zeng Huang<author:sep>Kripasindhu Sarkar<author:sep>Danhang Tang<author:sep>Di Qiu<author:sep>Abhimitra Meka<author:sep>Ruofei Du<author:sep>Mingsong Dou<author:sep>Sergio Orts-Escolano<author:sep>Rohit Pandey<author:sep>Ping Tan<author:sep>Thabo Beeler<author:sep>Sean Fanello<author:sep>Yinda Zhang;http://arxiv.org/pdf/2304.01436v1;cs.CV;"In CVPR2023. Project page:
  https://augmentedperception.github.io/monoavatar/";
2304.02061v3;http://arxiv.org/abs/2304.02061v3;2023-04-04;Generating Continual Human Motion in Diverse 3D Scenes;"We introduce a method to synthesize animator guided human motion across 3D
scenes. Given a set of sparse (3 or 4) joint locations (such as the location of
a person's hand and two feet) and a seed motion sequence in a 3D scene, our
method generates a plausible motion sequence starting from the seed motion
while satisfying the constraints imposed by the provided keypoints. We
decompose the continual motion synthesis problem into walking along paths and
transitioning in and out of the actions specified by the keypoints, which
enables long generation of motions that satisfy scene constraints without
explicitly incorporating scene information. Our method is trained only using
scene agnostic mocap data. As a result, our approach is deployable across 3D
scenes with various geometries. For achieving plausible continual motion
synthesis without drift, our key contribution is to generate motion in a
goal-centric canonical coordinate frame where the next immediate target is
situated at the origin. Our model can generate long sequences of diverse
actions such as grabbing, sitting and leaning chained together in arbitrary
order, demonstrated on scenes of varying geometry: HPS, Replica, Matterport,
ScanNet and scenes represented using NeRFs. Several experiments demonstrate
that our method outperforms existing methods that navigate paths in 3D scenes.";Aymen Mir<author:sep>Xavier Puig<author:sep>Angjoo Kanazawa<author:sep>Gerard Pons-Moll;http://arxiv.org/pdf/2304.02061v3;cs.CV;;nerf
2304.02001v1;http://arxiv.org/abs/2304.02001v1;2023-04-04;MonoHuman: Animatable Human Neural Field from Monocular Video;"Animating virtual avatars with free-view control is crucial for various
applications like virtual reality and digital entertainment. Previous studies
have attempted to utilize the representation power of the neural radiance field
(NeRF) to reconstruct the human body from monocular videos. Recent works
propose to graft a deformation network into the NeRF to further model the
dynamics of the human neural field for animating vivid human motions. However,
such pipelines either rely on pose-dependent representations or fall short of
motion coherency due to frame-independent optimization, making it difficult to
generalize to unseen pose sequences realistically. In this paper, we propose a
novel framework MonoHuman, which robustly renders view-consistent and
high-fidelity avatars under arbitrary novel poses. Our key insight is to model
the deformation field with bi-directional constraints and explicitly leverage
the off-the-peg keyframe information to reason the feature correlations for
coherent results. Specifically, we first propose a Shared Bidirectional
Deformation module, which creates a pose-independent generalizable deformation
field by disentangling backward and forward deformation correspondences into
shared skeletal motion weight and separate non-rigid motions. Then, we devise a
Forward Correspondence Search module, which queries the correspondence feature
of keyframes to guide the rendering network. The rendered results are thus
multi-view consistent with high fidelity, even under challenging novel pose
settings. Extensive experiments demonstrate the superiority of our proposed
MonoHuman over state-of-the-art methods.";Zhengming Yu<author:sep>Wei Cheng<author:sep>Xian Liu<author:sep>Wayne Wu<author:sep>Kwan-Yee Lin;http://arxiv.org/pdf/2304.02001v1;cs.CV;"15 pages, 14 figures. Accepted to CVPR 2023. Project page:
  https://yzmblog.github.io/projects/MonoHuman/";nerf
2304.00837v1;http://arxiv.org/abs/2304.00837v1;2023-04-03;Disorder-invariant Implicit Neural Representation;"Implicit neural representation (INR) characterizes the attributes of a signal
as a function of corresponding coordinates which emerges as a sharp weapon for
solving inverse problems. However, the expressive power of INR is limited by
the spectral bias in the network training. In this paper, we find that such a
frequency-related problem could be greatly solved by re-arranging the
coordinates of the input signal, for which we propose the disorder-invariant
implicit neural representation (DINER) by augmenting a hash-table to a
traditional INR backbone. Given discrete signals sharing the same histogram of
attributes and different arrangement orders, the hash-table could project the
coordinates into the same distribution for which the mapped signal can be
better modeled using the subsequent INR network, leading to significantly
alleviated spectral bias. Furthermore, the expressive power of the DINER is
determined by the width of the hash-table. Different width corresponds to
different geometrical elements in the attribute space, \textit{e.g.}, 1D curve,
2D curved-plane and 3D curved-volume when the width is set as $1$, $2$ and $3$,
respectively. More covered areas of the geometrical elements result in stronger
expressive power. Experiments not only reveal the generalization of the DINER
for different INR backbones (MLP vs. SIREN) and various tasks (image/video
representation, phase retrieval, refractive index recovery, and neural radiance
field optimization) but also show the superiority over the state-of-the-art
algorithms both in quality and speed. \textit{Project page:}
\url{https://ezio77.github.io/DINER-website/}";Hao Zhu<author:sep>Shaowen Xie<author:sep>Zhen Liu<author:sep>Fengyi Liu<author:sep>Qi Zhang<author:sep>You Zhou<author:sep>Yi Lin<author:sep>Zhan Ma<author:sep>Xun Cao;http://arxiv.org/pdf/2304.00837v1;cs.CV;"Journal extension of the CVPR'23 highlight paper ""DINER:
  Disorder-invariant Implicit Neural Representation"". In the extension, we
  model the expressive power of the DINER using parametric functions in the
  attribute space. As a result, better results are achieved than the conference
  version. arXiv admin note: substantial text overlap with arXiv:2211.07871";
2304.00916v3;http://arxiv.org/abs/2304.00916v3;2023-04-03;DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via  Diffusion Models;"We present DreamAvatar, a text-and-shape guided framework for generating
high-quality 3D human avatars with controllable poses. While encouraging
results have been reported by recent methods on text-guided 3D common object
generation, generating high-quality human avatars remains an open challenge due
to the complexity of the human body's shape, pose, and appearance. We propose
DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for
predicting density and color for 3D points and pretrained text-to-image
diffusion models for providing 2D self-supervision. Specifically, we leverage
the SMPL model to provide shape and pose guidance for the generation. We
introduce a dual-observation-space design that involves the joint optimization
of a canonical space and a posed space that are related by a learnable
deformation field. This facilitates the generation of more complete textures
and geometry faithful to the target pose. We also jointly optimize the losses
computed from the full body and from the zoomed-in 3D head to alleviate the
common multi-face ''Janus'' problem and improve facial details in the generated
avatars. Extensive evaluations demonstrate that DreamAvatar significantly
outperforms existing methods, establishing a new state-of-the-art for
text-and-shape guided 3D human avatar generation.";Yukang Cao<author:sep>Yan-Pei Cao<author:sep>Kai Han<author:sep>Ying Shan<author:sep>Kwan-Yee K. Wong;http://arxiv.org/pdf/2304.00916v3;cs.CV;Project page: https://yukangcao.github.io/DreamAvatar/;nerf
2304.00341v1;http://arxiv.org/abs/2304.00341v1;2023-04-01;JacobiNeRF: NeRF Shaping with Mutual Information Gradients;"We propose a method that trains a neural radiance field (NeRF) to encode not
only the appearance of the scene but also semantic correlations between scene
points, regions, or entities -- aiming to capture their mutual co-variation
patterns. In contrast to the traditional first-order photometric reconstruction
objective, our method explicitly regularizes the learning dynamics to align the
Jacobians of highly-correlated entities, which proves to maximize the mutual
information between them under random scene perturbations. By paying attention
to this second-order information, we can shape a NeRF to express semantically
meaningful synergies when the network weights are changed by a delta along the
gradient of a single entity, region, or even a point. To demonstrate the merit
of this mutual information modeling, we leverage the coordinated behavior of
scene entities that emerges from our shaping to perform label propagation for
semantic and instance segmentation. Our experiments show that a JacobiNeRF is
more efficient in propagating annotations among 2D pixels and 3D points
compared to NeRFs without mutual information shaping, especially in extremely
sparse label regimes -- thus reducing annotation burden. The same machinery can
further be used for entity selection or scene modifications.";Xiaomeng Xu<author:sep>Yanchao Yang<author:sep>Kaichun Mo<author:sep>Boxiao Pan<author:sep>Li Yi<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2304.00341v1;cs.CV;;nerf
2303.17968v1;http://arxiv.org/abs/2303.17968v1;2023-03-31;VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence  Normalization;"We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) for
better geometry under non-Lambertian surface and dynamic lighting conditions
that cause significant variation in the radiance of a point when viewed from
different angles. Instead of explicitly modeling the underlying factors that
result in the view-dependent phenomenon, which could be complex yet not
inclusive, we develop a simple and effective technique that normalizes the
view-dependence by distilling invariant information already encoded in the
learned NeRFs. We then jointly train NeRFs for view synthesis with
view-dependence normalization to attain quality geometry. Our experiments show
that even though shape-radiance ambiguity is inevitable, the proposed
normalization can minimize its effect on geometry, which essentially aligns the
optimal capacity needed for explaining view-dependent variations. Our method
applies to various baselines and significantly improves geometry without
changing the volume rendering pipeline, even if the data is captured under a
moving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF.";Bingfan Zhu<author:sep>Yanchao Yang<author:sep>Xulong Wang<author:sep>Youyi Zheng<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2303.17968v1;cs.CV;;nerf
2303.17368v2;http://arxiv.org/abs/2303.17368v2;2023-03-30;SynBody: Synthetic Dataset with Layered Human Models for 3D Human  Perception and Modeling;"Synthetic data has emerged as a promising source for 3D human research as it
offers low-cost access to large-scale human datasets. To advance the diversity
and annotation quality of human models, we introduce a new synthetic dataset,
SynBody, with three appealing features: 1) a clothed parametric human model
that can generate a diverse range of subjects; 2) the layered human
representation that naturally offers high-quality 3D annotations to support
multiple tasks; 3) a scalable system for producing realistic data to facilitate
real-world tasks. The dataset comprises 1.2M images with corresponding accurate
3D annotations, covering 10,000 human body models, 1,187 actions, and various
viewpoints. The dataset includes two subsets for human pose and shape
estimation as well as human neural rendering. Extensive experiments on SynBody
indicate that it substantially enhances both SMPL and SMPL-X estimation.
Furthermore, the incorporation of layered annotations offers a valuable
training resource for investigating the Human Neural Radiance Fields (NeRF).";Zhitao Yang<author:sep>Zhongang Cai<author:sep>Haiyi Mei<author:sep>Shuai Liu<author:sep>Zhaoxi Chen<author:sep>Weiye Xiao<author:sep>Yukun Wei<author:sep>Zhongfei Qing<author:sep>Chen Wei<author:sep>Bo Dai<author:sep>Wayne Wu<author:sep>Chen Qian<author:sep>Dahua Lin<author:sep>Ziwei Liu<author:sep>Lei Yang;http://arxiv.org/pdf/2303.17368v2;cs.CV;Accepted by ICCV 2023. Project webpage: https://synbody.github.io/;nerf
2303.17147v1;http://arxiv.org/abs/2303.17147v1;2023-03-30;NeILF++: Inter-Reflectable Light Fields for Geometry and Material  Estimation;"We present a novel differentiable rendering framework for joint geometry,
material, and lighting estimation from multi-view images. In contrast to
previous methods which assume a simplified environment map or co-located
flashlights, in this work, we formulate the lighting of a static scene as one
neural incident light field (NeILF) and one outgoing neural radiance field
(NeRF). The key insight of the proposed method is the union of the incident and
outgoing light fields through physically-based rendering and inter-reflections
between surfaces, making it possible to disentangle the scene geometry,
material, and lighting from image observations in a physically-based manner.
The proposed incident light and inter-reflection framework can be easily
applied to other NeRF systems. We show that our method can not only decompose
the outgoing radiance into incident lights and surface materials, but also
serve as a surface refinement module that further improves the reconstruction
detail of the neural surface. We demonstrate on several datasets that the
proposed method is able to achieve state-of-the-art results in terms of
geometry reconstruction quality, material estimation accuracy, and the fidelity
of novel view rendering.";Jingyang Zhang<author:sep>Yao Yao<author:sep>Shiwei Li<author:sep>Jingbo Liu<author:sep>Tian Fang<author:sep>David McKinnon<author:sep>Yanghai Tsin<author:sep>Long Quan;http://arxiv.org/pdf/2303.17147v1;cs.CV;Project page: \url{https://yoyo000.github.io/NeILF_pp};nerf
2303.17094v1;http://arxiv.org/abs/2303.17094v1;2023-03-30;Enhanced Stable View Synthesis;"We introduce an approach to enhance the novel view synthesis from images
taken from a freely moving camera. The introduced approach focuses on outdoor
scenes where recovering accurate geometric scaffold and camera pose is
challenging, leading to inferior results using the state-of-the-art stable view
synthesis (SVS) method. SVS and related methods fail for outdoor scenes
primarily due to (i) over-relying on the multiview stereo (MVS) for geometric
scaffold recovery and (ii) assuming COLMAP computed camera poses as the best
possible estimates, despite it being well-studied that MVS 3D reconstruction
accuracy is limited to scene disparity and camera-pose accuracy is sensitive to
key-point correspondence selection. This work proposes a principled way to
enhance novel view synthesis solutions drawing inspiration from the basics of
multiple view geometry. By leveraging the complementary behavior of MVS and
monocular depth, we arrive at a better scene depth per view for nearby and far
points, respectively. Moreover, our approach jointly refines camera poses with
image-based rendering via multiple rotation averaging graph optimization. The
recovered scene depth and the camera-pose help better view-dependent on-surface
feature aggregation of the entire scene. Extensive evaluation of our approach
on the popular benchmark dataset, such as Tanks and Temples, shows substantial
improvement in view synthesis results compared to the prior art. For instance,
our method shows 1.5 dB of PSNR improvement on the Tank and Temples. Similar
statistics are observed when tested on other benchmark datasets such as FVS,
Mip-NeRF 360, and DTU.";Nishant Jain<author:sep>Suryansh Kumar<author:sep>Luc Van Gool;http://arxiv.org/pdf/2303.17094v1;cs.CV;"Accepted to IEEE/CVF CVPR 2023. Draft info: 13 pages, 6 Figures, 7
  Tables";nerf
2303.17603v1;http://arxiv.org/abs/2303.17603v1;2023-03-30;NeRF-Supervised Deep Stereo;"We introduce a novel framework for training deep stereo networks effortlessly
and without any ground-truth. By leveraging state-of-the-art neural rendering
solutions, we generate stereo training data from image sequences collected with
a single handheld camera. On top of them, a NeRF-supervised training procedure
is carried out, from which we exploit rendered stereo triplets to compensate
for occlusions and depth maps as proxy labels. This results in stereo networks
capable of predicting sharp and detailed disparity maps. Experimental results
show that models trained under this regime yield a 30-40% improvement over
existing self-supervised methods on the challenging Middlebury dataset, filling
the gap to supervised models and, most times, outperforming them at zero-shot
generalization.";Fabio Tosi<author:sep>Alessio Tonioni<author:sep>Daniele De Gregorio<author:sep>Matteo Poggi;http://arxiv.org/pdf/2303.17603v1;cs.CV;"CVPR 2023. Project page: https://nerfstereo.github.io/ Code:
  https://github.com/fabiotosi92/NeRF-Supervised-Deep-Stereo";nerf
2303.16884v1;http://arxiv.org/abs/2303.16884v1;2023-03-29;Instant Neural Radiance Fields Stylization;"We present Instant Neural Radiance Fields Stylization, a novel approach for
multi-view image stylization for the 3D scene. Our approach models a neural
radiance field based on neural graphics primitives, which use a hash
table-based position encoder for position embedding. We split the position
encoder into two parts, the content and style sub-branches, and train the
network for normal novel view image synthesis with the content and style
targets. In the inference stage, we execute AdaIN to the output features of the
position encoder, with content and style voxel grid features as reference. With
the adjusted features, the stylization of novel view images could be obtained.
Our method extends the style target from style images to image sets of scenes
and does not require additional network training for stylization. Given a set
of images of 3D scenes and a style target(a style image or another set of 3D
scenes), our method can generate stylized novel views with a consistent
appearance at various view angles in less than 10 minutes on modern GPU
hardware. Extensive experimental results demonstrate the validity and
superiority of our method.";Shaoxu Li<author:sep>Ye Pan;http://arxiv.org/pdf/2303.16884v1;cs.CV;;
2303.16482v1;http://arxiv.org/abs/2303.16482v1;2023-03-29;Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance  Fields;"Synthesizing photo-realistic images from a point cloud is challenging because
of the sparsity of point cloud representation. Recent Neural Radiance Fields
and extensions are proposed to synthesize realistic images from 2D input. In
this paper, we present Point2Pix as a novel point renderer to link the 3D
sparse point clouds with 2D dense image pixels. Taking advantage of the point
cloud 3D prior and NeRF rendering pipeline, our method can synthesize
high-quality images from colored point clouds, generally for novel indoor
scenes. To improve the efficiency of ray sampling, we propose point-guided
sampling, which focuses on valid samples. Also, we present Point Encoding to
build Multi-scale Radiance Fields that provide discriminative 3D point
features. Finally, we propose Fusion Encoding to efficiently synthesize
high-quality images. Extensive experiments on the ScanNet and ArkitScenes
datasets demonstrate the effectiveness and generalization.";Tao Hu<author:sep>Xiaogang Xu<author:sep>Shu Liu<author:sep>Jiaya Jia;http://arxiv.org/pdf/2303.16482v1;cs.CV;;nerf
2303.16485v1;http://arxiv.org/abs/2303.16485v1;2023-03-29;TriVol: Point Cloud Rendering via Triple Volumes;"Existing learning-based methods for point cloud rendering adopt various 3D
representations and feature querying mechanisms to alleviate the sparsity
problem of point clouds. However, artifacts still appear in rendered images,
due to the challenges in extracting continuous and discriminative 3D features
from point clouds. In this paper, we present a dense while lightweight 3D
representation, named TriVol, that can be combined with NeRF to render
photo-realistic images from point clouds. Our TriVol consists of triple slim
volumes, each of which is encoded from the point cloud. TriVol has two
advantages. First, it fuses respective fields at different scales and thus
extracts local and non-local features for discriminative representation.
Second, since the volume size is greatly reduced, our 3D decoder can be
efficiently inferred, allowing us to increase the resolution of the 3D space to
render more point details. Extensive experiments on different benchmarks with
varying kinds of scenes/objects demonstrate our framework's effectiveness
compared with current approaches. Moreover, our framework has excellent
generalization ability to render a category of scenes/objects without
fine-tuning.";Tao Hu<author:sep>Xiaogang Xu<author:sep>Ruihang Chu<author:sep>Jiaya Jia;http://arxiv.org/pdf/2303.16485v1;cs.CV;;nerf
2303.16184v1;http://arxiv.org/abs/2303.16184v1;2023-03-28;VMesh: Hybrid Volume-Mesh Representation for Efficient View Synthesis;"With the emergence of neural radiance fields (NeRFs), view synthesis quality
has reached an unprecedented level. Compared to traditional mesh-based assets,
this volumetric representation is more powerful in expressing scene geometry
but inevitably suffers from high rendering costs and can hardly be involved in
further processes like editing, posing significant difficulties in combination
with the existing graphics pipeline. In this paper, we present a hybrid
volume-mesh representation, VMesh, which depicts an object with a textured mesh
along with an auxiliary sparse volume. VMesh retains the advantages of
mesh-based assets, such as efficient rendering, compact storage, and easy
editing, while also incorporating the ability to represent subtle geometric
structures provided by the volumetric counterpart. VMesh can be obtained from
multi-view images of an object and renders at 2K 60FPS on common consumer
devices with high fidelity, unleashing new opportunities for real-time
immersive applications.";Yuan-Chen Guo<author:sep>Yan-Pei Cao<author:sep>Chen Wang<author:sep>Yu He<author:sep>Ying Shan<author:sep>Xiaohu Qie<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2303.16184v1;cs.CV;Project page: https://bennyguo.github.io/vmesh/;nerf
2303.15951v1;http://arxiv.org/abs/2303.15951v1;2023-03-28;F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera  Trajectories;"This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF)
for novel view synthesis, which enables arbitrary input camera trajectories and
only costs a few minutes for training. Existing fast grid-based NeRF training
frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed
for bounded scenes and rely on space warping to handle unbounded scenes.
Existing two widely-used space-warping methods are only designed for the
forward-facing trajectory or the 360-degree object-centric trajectory but
cannot process arbitrary trajectories. In this paper, we delve deep into the
mechanism of space warping to handle unbounded scenes. Based on our analysis,
we further propose a novel space-warping method called perspective warping,
which allows us to handle arbitrary trajectories in the grid-based NeRF
framework. Extensive experiments demonstrate that F2-NeRF is able to use the
same perspective warping to render high-quality images on two standard datasets
and a new free trajectory dataset collected by us. Project page:
https://totoro97.github.io/projects/f2-nerf.";Peng Wang<author:sep>Yuan Liu<author:sep>Zhaoxi Chen<author:sep>Lingjie Liu<author:sep>Ziwei Liu<author:sep>Taku Komura<author:sep>Christian Theobalt<author:sep>Wenping Wang;http://arxiv.org/pdf/2303.15951v1;cs.CV;CVPR 2023. Project page: https://totoro97.github.io/projects/f2-nerf;nerf
2303.16333v1;http://arxiv.org/abs/2303.16333v1;2023-03-28;Flow supervision for Deformable NeRF;"In this paper we present a new method for deformable NeRF that can directly
use optical flow as supervision. We overcome the major challenge with respect
to the computationally inefficiency of enforcing the flow constraints to the
backward deformation field, used by deformable NeRFs. Specifically, we show
that inverting the backward deformation function is actually not needed for
computing scene flows between frames. This insight dramatically simplifies the
problem, as one is no longer constrained to deformation functions that can be
analytically inverted. Instead, thanks to the weak assumptions required by our
derivation based on the inverse function theorem, our approach can be extended
to a broad class of commonly used backward deformation field. We present
results on monocular novel view synthesis with rapid object motion, and
demonstrate significant improvements over baselines without flow supervision.";Chaoyang Wang<author:sep>Lachlan Ewen MacDonald<author:sep>Laszlo A. Jeni<author:sep>Simon Lucey;http://arxiv.org/pdf/2303.16333v1;cs.CV;;nerf
2303.16196v2;http://arxiv.org/abs/2303.16196v2;2023-03-28;SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis;"Neural Radiance Field (NeRF) significantly degrades when only a limited
number of views are available. To complement the lack of 3D information,
depth-based models, such as DSNeRF and MonoSDF, explicitly assume the
availability of accurate depth maps of multiple views. They linearly scale the
accurate depth maps as supervision to guide the predicted depth of few-shot
NeRFs. However, accurate depth maps are difficult and expensive to capture due
to wide-range depth distances in the wild.
  In this work, we present a new Sparse-view NeRF (SparseNeRF) framework that
exploits depth priors from real-world inaccurate observations. The inaccurate
depth observations are either from pre-trained depth models or coarse depth
maps of consumer-level depth sensors. Since coarse depth maps are not strictly
scaled to the ground-truth depth maps, we propose a simple yet effective
constraint, a local depth ranking method, on NeRFs such that the expected depth
ranking of the NeRF is consistent with that of the coarse depth maps in local
patches. To preserve the spatial continuity of the estimated depth of NeRF, we
further propose a spatial continuity constraint to encourage the consistency of
the expected depth continuity of NeRF with coarse depth maps. Surprisingly,
with simple depth ranking constraints, SparseNeRF outperforms all
state-of-the-art few-shot NeRF methods (including depth-based models) on
standard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBD
that contains real-world depth maps from Azure Kinect, ZED 2, and iPhone 13
Pro. Extensive experiments on NVS-RGBD dataset also validate the superiority
and generalizability of SparseNeRF. Code and dataset are available at
https://sparsenerf.github.io/.";Guangcong Wang<author:sep>Zhaoxi Chen<author:sep>Chen Change Loy<author:sep>Ziwei Liu;http://arxiv.org/pdf/2303.16196v2;cs.CV;Accepted by ICCV 2023, Project page: https://sparsenerf.github.io/;nerf
2303.16001v2;http://arxiv.org/abs/2303.16001v2;2023-03-28;Adaptive Voronoi NeRFs;"Neural Radiance Fields (NeRFs) learn to represent a 3D scene from just a set
of registered images. Increasing sizes of a scene demands more complex
functions, typically represented by neural networks, to capture all details.
Training and inference then involves querying the neural network millions of
times per image, which becomes impractically slow. Since such complex functions
can be replaced by multiple simpler functions to improve speed, we show that a
hierarchy of Voronoi diagrams is a suitable choice to partition the scene. By
equipping each Voronoi cell with its own NeRF, our approach is able to quickly
learn a scene representation. We propose an intuitive partitioning of the space
that increases quality gains during training by distributing information evenly
among the networks and avoids artifacts through a top-down adaptive refinement.
Our framework is agnostic to the underlying NeRF method and easy to implement,
which allows it to be applied to various NeRF variants for improved learning
and rendering speeds.";Tim Elsner<author:sep>Victor Czech<author:sep>Julia Berger<author:sep>Zain Selman<author:sep>Isaak Lim<author:sep>Leif Kobbelt;http://arxiv.org/pdf/2303.16001v2;cs.CV;;nerf
2303.16242v3;http://arxiv.org/abs/2303.16242v3;2023-03-28;CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image  Arbitrary-Scale Super Resolution;"Medical image arbitrary-scale super-resolution (MIASSR) has recently gained
widespread attention, aiming to super sample medical volumes at arbitrary
scales via a single model. However, existing MIASSR methods face two major
limitations: (i) reliance on high-resolution (HR) volumes and (ii) limited
generalization ability, which restricts their application in various scenarios.
To overcome these limitations, we propose Cube-based Neural Radiance Field
(CuNeRF), a zero-shot MIASSR framework that can yield medical images at
arbitrary scales and viewpoints in a continuous domain. Unlike existing MIASSR
methods that fit the mapping between low-resolution (LR) and HR volumes, CuNeRF
focuses on building a coordinate-intensity continuous representation from LR
volumes without the need for HR references. This is achieved by the proposed
differentiable modules: including cube-based sampling, isotropic volume
rendering, and cube-based hierarchical rendering. Through extensive experiments
on magnetic resource imaging (MRI) and computed tomography (CT) modalities, we
demonstrate that CuNeRF outperforms state-of-the-art MIASSR methods. CuNeRF
yields better visual verisimilitude and reduces aliasing artifacts at various
upsampling factors. Moreover, our CuNeRF does not need any LR-HR training
pairs, which is more flexible and easier to be used than others. Our code will
be publicly available soon.";Zixuan Chen<author:sep>Jian-Huang Lai<author:sep>Lingxiao Yang<author:sep>Xiaohua Xie;http://arxiv.org/pdf/2303.16242v3;eess.IV;"This paper is accepted by the International Conference on Computer
  Vision (ICCV) 2023";nerf
2303.15012v1;http://arxiv.org/abs/2303.15012v1;2023-03-27;3D-Aware Multi-Class Image-to-Image Translation with NeRFs;"Recent advances in 3D-aware generative models (3D-aware GANs) combined with
Neural Radiance Fields (NeRF) have achieved impressive results. However no
prior works investigate 3D-aware GANs for 3D consistent multi-class
image-to-image (3D-aware I2I) translation. Naively using 2D-I2I translation
methods suffers from unrealistic shape/identity change. To perform 3D-aware
multi-class I2I translation, we decouple this learning process into a
multi-class 3D-aware GAN step and a 3D-aware I2I translation step. In the first
step, we propose two novel techniques: a new conditional architecture and an
effective training strategy. In the second step, based on the well-trained
multi-class 3D-aware GAN architecture, that preserves view-consistency, we
construct a 3D-aware I2I translation system. To further reduce the
view-consistency problems, we propose several new techniques, including a
U-net-like adaptor network design, a hierarchical representation constrain and
a relative regularization loss. In extensive experiments on two datasets,
quantitative and qualitative results demonstrate that we successfully perform
3D-aware I2I translation with multi-view consistency.";Senmao Li<author:sep>Joost van de Weijer<author:sep>Yaxing Wang<author:sep>Fahad Shahbaz Khan<author:sep>Meiqin Liu<author:sep>Jian Yang;http://arxiv.org/pdf/2303.15012v1;cs.CV;Accepted by CVPR2023;nerf
2303.15368v1;http://arxiv.org/abs/2303.15368v1;2023-03-27;NeUDF: Learning Unsigned Distance Fields from Multi-view Images for  Reconstructing Non-watertight Models;"Volume rendering-based 3D reconstruction from multi-view images has gained
popularity in recent years, largely due to the success of neural radiance
fields (NeRF). A number of methods have been developed that build upon NeRF and
use neural volume rendering to learn signed distance fields (SDFs) for
reconstructing 3D models. However, SDF-based methods cannot represent
non-watertight models and, therefore, cannot capture open boundaries. This
paper proposes a new algorithm for learning an accurate unsigned distance field
(UDF) from multi-view images, which is specifically designed for reconstructing
non-watertight, textureless models. The proposed method, called NeUDF,
addresses the limitations of existing UDF-based methods by introducing a simple
and approximately unbiased and occlusion-aware density function. In addition, a
smooth and differentiable UDF representation is presented to make the learning
process easier and more efficient. Experiments on both texture-rich and
textureless models demonstrate the robustness and effectiveness of the proposed
approach, making it a promising solution for reconstructing challenging 3D
models from multi-view images.";Fei Hou<author:sep>Jukai Deng<author:sep>Xuhui Chen<author:sep>Wencheng Wang<author:sep>Ying He;http://arxiv.org/pdf/2303.15368v1;cs.CV;;nerf
2303.15427v1;http://arxiv.org/abs/2303.15427v1;2023-03-27;JAWS: Just A Wild Shot for Cinematic Transfer in Neural Radiance Fields;"This paper presents JAWS, an optimization-driven approach that achieves the
robust transfer of visual cinematic features from a reference in-the-wild video
clip to a newly generated clip. To this end, we rely on an
implicit-neural-representation (INR) in a way to compute a clip that shares the
same cinematic features as the reference clip. We propose a general formulation
of a camera optimization problem in an INR that computes extrinsic and
intrinsic camera parameters as well as timing. By leveraging the
differentiability of neural representations, we can back-propagate our designed
cinematic losses measured on proxy estimators through a NeRF network to the
proposed cinematic parameters directly. We also introduce specific enhancements
such as guidance maps to improve the overall quality and efficiency. Results
display the capacity of our system to replicate well known camera sequences
from movies, adapting the framing, camera parameters and timing of the
generated video clip to maximize the similarity with the reference clip.";Xi Wang<author:sep>Robin Courant<author:sep>Jinglei Shi<author:sep>Eric Marchand<author:sep>Marc Christie;http://arxiv.org/pdf/2303.15427v1;cs.CV;"CVPR 2023. Project page with videos and code:
  http://www.lix.polytechnique.fr/vista/projects/2023_cvpr_wang";nerf
2303.15387v1;http://arxiv.org/abs/2303.15387v1;2023-03-27;Generalizable Neural Voxels for Fast Human Radiance Fields;"Rendering moving human bodies at free viewpoints only from a monocular video
is quite a challenging problem. The information is too sparse to model
complicated human body structures and motions from both view and pose
dimensions. Neural radiance fields (NeRF) have shown great power in novel view
synthesis and have been applied to human body rendering. However, most current
NeRF-based methods bear huge costs for both training and rendering, which
impedes the wide applications in real-life scenarios. In this paper, we propose
a rendering framework that can learn moving human body structures extremely
quickly from a monocular video. The framework is built by integrating both
neural fields and neural voxels. Especially, a set of generalizable neural
voxels are constructed. With pretrained on various human bodies, these general
voxels represent a basic skeleton and can provide strong geometric priors. For
the fine-tuning process, individual voxels are constructed for learning
differential textures, complementary to general voxels. Thus learning a novel
body can be further accelerated, taking only a few minutes. Our method shows
significantly higher training efficiency compared with previous methods, while
maintaining similar rendering quality. The project page is at
https://taoranyi.com/gneuvox .";Taoran Yi<author:sep>Jiemin Fang<author:sep>Xinggang Wang<author:sep>Wenyu Liu;http://arxiv.org/pdf/2303.15387v1;cs.CV;Project page: http://taoranyi.com/gneuvox;nerf
2303.14707v1;http://arxiv.org/abs/2303.14707v1;2023-03-26;Clean-NeRF: Reformulating NeRF to account for View-Dependent  Observations;"While Neural Radiance Fields (NeRFs) had achieved unprecedented novel view
synthesis results, they have been struggling in dealing with large-scale
cluttered scenes with sparse input views and highly view-dependent appearances.
Specifically, existing NeRF-based models tend to produce blurry rendering with
the volumetric reconstruction often inaccurate, where a lot of reconstruction
errors are observed in the form of foggy ""floaters"" hovering within the entire
volume of an opaque 3D scene. Such inaccuracies impede NeRF's potential for
accurate 3D NeRF registration, object detection, segmentation, etc., which
possibly accounts for only limited significant research effort so far to
directly address these important 3D fundamental computer vision problems to
date. This paper analyzes the NeRF's struggles in such settings and proposes
Clean-NeRF for accurate 3D reconstruction and novel view rendering in complex
scenes. Our key insights consist of enforcing effective appearance and geometry
constraints, which are absent in the conventional NeRF reconstruction, by 1)
automatically detecting and modeling view-dependent appearances in the training
views to prevent them from interfering with density estimation, which is
complete with 2) a geometric correction procedure performed on each traced ray
during inference. Clean-NeRF can be implemented as a plug-in that can
immediately benefit existing NeRF-based methods without additional input. Codes
will be released.";Xinhang Liu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2303.14707v1;cs.CV;;nerf
2303.14536v1;http://arxiv.org/abs/2303.14536v1;2023-03-25;SUDS: Scalable Urban Dynamic Scenes;"We extend neural radiance fields (NeRFs) to dynamic large-scale urban scenes.
Prior work tends to reconstruct single video clips of short durations (up to 10
seconds). Two reasons are that such methods (a) tend to scale linearly with the
number of moving objects and input videos because a separate model is built for
each and (b) tend to require supervision via 3D bounding boxes and panoptic
labels, obtained manually or via category-specific models. As a step towards
truly open-world reconstructions of dynamic cities, we introduce two key
innovations: (a) we factorize the scene into three separate hash table data
structures to efficiently encode static, dynamic, and far-field radiance
fields, and (b) we make use of unlabeled target signals consisting of RGB
images, sparse LiDAR, off-the-shelf self-supervised 2D descriptors, and most
importantly, 2D optical flow.
  Operationalizing such inputs via photometric, geometric, and feature-metric
reconstruction losses enables SUDS to decompose dynamic scenes into the static
background, individual objects, and their motions. When combined with our
multi-branch table representation, such reconstructions can be scaled to tens
of thousands of objects across 1.2 million frames from 1700 videos spanning
geospatial footprints of hundreds of kilometers, (to our knowledge) the largest
dynamic NeRF built to date.
  We present qualitative initial results on a variety of tasks enabled by our
representations, including novel-view synthesis of dynamic urban scenes,
unsupervised 3D instance segmentation, and unsupervised 3D cuboid detection. To
compare to prior work, we also evaluate on KITTI and Virtual KITTI 2,
surpassing state-of-the-art methods that rely on ground truth 3D bounding box
annotations while being 10x quicker to train.";Haithem Turki<author:sep>Jason Y. Zhang<author:sep>Francesco Ferroni<author:sep>Deva Ramanan;http://arxiv.org/pdf/2303.14536v1;cs.CV;CVPR 2023 Project page: https://haithemturki.com/suds/;nerf
2303.14435v1;http://arxiv.org/abs/2303.14435v1;2023-03-25;NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects;"Dynamic Neural Radiance Field (NeRF) is a powerful algorithm capable of
rendering photo-realistic novel view images from a monocular RGB video of a
dynamic scene. Although it warps moving points across frames from the
observation spaces to a common canonical space for rendering, dynamic NeRF does
not model the change of the reflected color during the warping. As a result,
this approach often fails drastically on challenging specular objects in
motion. We address this limitation by reformulating the neural radiance field
function to be conditioned on surface position and orientation in the
observation space. This allows the specular surface at different poses to keep
the different reflected colors when mapped to the common canonical space.
Additionally, we add the mask of moving objects to guide the deformation field.
As the specular surface changes color during motion, the mask mitigates the
problem of failure to find temporal correspondences with only RGB supervision.
We evaluate our model based on the novel view synthesis quality with a
self-collected dataset of different moving specular objects in realistic
environments. The experimental results demonstrate that our method
significantly improves the reconstruction quality of moving specular objects
from monocular RGB videos compared to the existing NeRF models. Our code and
data are available at the project website https://github.com/JokerYan/NeRF-DS.";Zhiwen Yan<author:sep>Chen Li<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2303.14435v1;cs.CV;CVPR 2023;nerf
2303.14478v1;http://arxiv.org/abs/2303.14478v1;2023-03-25;DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields;"Recent works such as BARF and GARF can bundle adjust camera poses with neural
radiance fields (NeRF) which is based on coordinate-MLPs. Despite the
impressive results, these methods cannot be applied to Generalizable NeRFs
(GeNeRFs) which require image feature extractions that are often based on more
complicated 3D CNN or transformer architectures. In this work, we first analyze
the difficulties of jointly optimizing camera poses with GeNeRFs, and then
further propose our DBARF to tackle these issues. Our DBARF which bundle
adjusts camera poses by taking a cost feature map as an implicit cost function
can be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARF
and its follow-up works, which can only be applied to per-scene optimized NeRFs
and need accurate initial camera poses with the exception of forward-facing
scenes, our method can generalize across scenes and does not require any good
initialization. Experiments show the effectiveness and generalization ability
of our DBARF when evaluated on real-world datasets. Our code is available at
\url{https://aibluefisher.github.io/dbarf}.";Yu Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2303.14478v1;cs.CV;;nerf
2303.15206v3;http://arxiv.org/abs/2303.15206v3;2023-03-24;Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods  for Front-Facing Views;"Neural view synthesis (NVS) is one of the most successful techniques for
synthesizing free viewpoint videos, capable of achieving high fidelity from
only a sparse set of captured images. This success has led to many variants of
the techniques, each evaluated on a set of test views typically using image
quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research
on how NVS methods perform with respect to perceived video quality. We present
the first study on perceptual evaluation of NVS and NeRF variants. For this
study, we collected two datasets of scenes captured in a controlled lab
environment as well as in-the-wild. In contrast to existing datasets, these
scenes come with reference video sequences, allowing us to test for temporal
artifacts and subtle distortions that are easily overlooked when viewing only
static images. We measured the quality of videos synthesized by several NVS
methods in a well-controlled perceptual quality assessment experiment as well
as with many existing state-of-the-art image/video quality metrics. We present
a detailed analysis of the results and recommendations for dataset and metric
selection for NVS evaluation.";Hanxue Liang<author:sep>Tianhao Wu<author:sep>Param Hanji<author:sep>Francesco Banterle<author:sep>Hongyun Gao<author:sep>Rafal Mantiuk<author:sep>Cengiz Oztireli;http://arxiv.org/pdf/2303.15206v3;cs.CV;;nerf
2303.14001v1;http://arxiv.org/abs/2303.14001v1;2023-03-24;Grid-guided Neural Radiance Fields for Large Urban Scenes;"Purely MLP-based neural radiance fields (NeRF-based methods) often suffer
from underfitting with blurred renderings on large-scale scenes due to limited
model capacity. Recent approaches propose to geographically divide the scene
and adopt multiple sub-NeRFs to model each region individually, leading to
linear scale-up in training costs and the number of sub-NeRFs as the scene
expands. An alternative solution is to use a feature grid representation, which
is computationally efficient and can naturally scale to a large scene with
increased grid resolutions. However, the feature grid tends to be less
constrained and often reaches suboptimal solutions, producing noisy artifacts
in renderings, especially in regions with complex geometry and texture. In this
work, we present a new framework that realizes high-fidelity rendering on large
urban scenes while being computationally efficient. We propose to use a compact
multiresolution ground feature plane representation to coarsely capture the
scene, and complement it with positional encoding inputs through another NeRF
branch for rendering in a joint learning fashion. We show that such an
integration can utilize the advantages of two alternative solutions: a
light-weighted NeRF is sufficient, under the guidance of the feature grid
representation, to render photorealistic novel views with fine details; and the
jointly optimized ground feature planes, can meanwhile gain further
refinements, forming a more accurate and compact feature space and output much
more natural rendering results.";Linning Xu<author:sep>Yuanbo Xiangli<author:sep>Sida Peng<author:sep>Xingang Pan<author:sep>Nanxuan Zhao<author:sep>Christian Theobalt<author:sep>Bo Dai<author:sep>Dahua Lin;http://arxiv.org/pdf/2303.14001v1;cs.CV;CVPR2023, Project page at https://city-super.github.io/gridnerf/;nerf
2303.13777v1;http://arxiv.org/abs/2303.13777v1;2023-03-24;GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from  Multi-view Images;"In this work, we focus on synthesizing high-fidelity novel view images for
arbitrary human performers, given a set of sparse multi-view images. It is a
challenging task due to the large variation among articulated body poses and
heavy self-occlusions. To alleviate this, we introduce an effective
generalizable framework Generalizable Model-based Neural Radiance Fields
(GM-NeRF) to synthesize free-viewpoint images. Specifically, we propose a
geometry-guided attention mechanism to register the appearance code from
multi-view 2D images to a geometry proxy which can alleviate the misalignment
between inaccurate geometry prior and pixel space. On top of that, we further
conduct neural rendering and partial gradient backpropagation for efficient
perceptual supervision and improvement of the perceptual quality of synthesis.
To evaluate our method, we conduct experiments on synthesized datasets
THuman2.0 and Multi-garment, and real-world datasets Genebody and ZJUMocap. The
results demonstrate that our approach outperforms state-of-the-art methods in
terms of novel view synthesis and geometric reconstruction.";Jianchuan Chen<author:sep>Wentao Yi<author:sep>Liqian Ma<author:sep>Xu Jia<author:sep>Huchuan Lu;http://arxiv.org/pdf/2303.13777v1;cs.CV;Accepted at CVPR 2023;nerf
2303.13743v1;http://arxiv.org/abs/2303.13743v1;2023-03-24;TEGLO: High Fidelity Canonical Texture Mapping from Single-View Images;"Recent work in Neural Fields (NFs) learn 3D representations from
class-specific single view image collections. However, they are unable to
reconstruct the input data preserving high-frequency details. Further, these
methods do not disentangle appearance from geometry and hence are not suitable
for tasks such as texture transfer and editing. In this work, we propose TEGLO
(Textured EG3D-GLO) for learning 3D representations from single view
in-the-wild image collections for a given class of objects. We accomplish this
by training a conditional Neural Radiance Field (NeRF) without any explicit 3D
supervision. We equip our method with editing capabilities by creating a dense
correspondence mapping to a 2D canonical space. We demonstrate that such
mapping enables texture transfer and texture editing without requiring meshes
with shared topology. Our key insight is that by mapping the input image pixels
onto the texture space we can achieve near perfect reconstruction (>= 74 dB
PSNR at 1024^2 resolution). Our formulation allows for high quality 3D
consistent novel view synthesis with high-frequency details at megapixel image
resolution.";Vishal Vinod<author:sep>Tanmay Shah<author:sep>Dmitry Lagun;http://arxiv.org/pdf/2303.13743v1;cs.CV;;nerf
2303.13825v1;http://arxiv.org/abs/2303.13825v1;2023-03-24;HandNeRF: Neural Radiance Fields for Animatable Interacting Hands;"We propose a novel framework to reconstruct accurate appearance and geometry
with neural radiance fields (NeRF) for interacting hands, enabling the
rendering of photo-realistic images and videos for gesture animation from
arbitrary views. Given multi-view images of a single hand or interacting hands,
an off-the-shelf skeleton estimator is first employed to parameterize the hand
poses. Then we design a pose-driven deformation field to establish
correspondence from those different poses to a shared canonical space, where a
pose-disentangled NeRF for one hand is optimized. Such unified modeling
efficiently complements the geometry and texture cues in rarely-observed areas
for both hands. Meanwhile, we further leverage the pose priors to generate
pseudo depth maps as guidance for occlusion-aware density learning. Moreover, a
neural feature distillation method is proposed to achieve cross-domain
alignment for color optimization. We conduct extensive experiments to verify
the merits of our proposed HandNeRF and report a series of state-of-the-art
results both qualitatively and quantitatively on the large-scale InterHand2.6M
dataset.";Zhiyang Guo<author:sep>Wengang Zhou<author:sep>Min Wang<author:sep>Li Li<author:sep>Houqiang Li;http://arxiv.org/pdf/2303.13825v1;cs.CV;CVPR 2023;nerf
2303.14184v2;http://arxiv.org/abs/2303.14184v2;2023-03-24;Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion  Prior;"In this work, we investigate the problem of creating high-fidelity 3D content
from only a single image. This is inherently challenging: it essentially
involves estimating the underlying 3D geometry while simultaneously
hallucinating unseen textures. To address this challenge, we leverage prior
knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision
for 3D creation. Our approach, Make-It-3D, employs a two-stage optimization
pipeline: the first stage optimizes a neural radiance field by incorporating
constraints from the reference image at the frontal view and diffusion prior at
novel views; the second stage transforms the coarse model into textured point
clouds and further elevates the realism with diffusion prior while leveraging
the high-quality textures from the reference image. Extensive experiments
demonstrate that our method outperforms prior works by a large margin,
resulting in faithful reconstructions and impressive visual quality. Our method
presents the first attempt to achieve high-quality 3D creation from a single
image for general objects and enables various applications such as text-to-3D
creation and texture editing.";Junshu Tang<author:sep>Tengfei Wang<author:sep>Bo Zhang<author:sep>Ting Zhang<author:sep>Ran Yi<author:sep>Lizhuang Ma<author:sep>Dong Chen;http://arxiv.org/pdf/2303.14184v2;cs.CV;17 pages, 18 figures, Project page: https://make-it-3d.github.io/;
2303.13843v3;http://arxiv.org/abs/2303.13843v3;2023-03-24;CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D  Scene Layout;"Recent advances have shown promise in merging neural radiance fields (NeRFs)
with pre-trained diffusion models for text-to-3D object generation. However,
one enduring challenge is their inadequate capability to accurately parse and
regenerate consistent multi-object environments. Specifically, these models
encounter difficulties in accurately representing quantity and style prompted
by multi-object texts, often resulting in a collapse of the rendering fidelity
that fails to match the semantic intricacies. Moreover, amalgamating these
elements into a coherent 3D scene is a substantial challenge, stemming from
generic distribution inherent in diffusion models. To tackle the issue of
'guidance collapse' and enhance consistency, we propose a novel framework,
dubbed CompoNeRF, by integrating an editable 3D scene layout with object
specific and scene-wide guidance mechanisms. It initiates by interpreting a
complex text into an editable 3D layout populated with multiple NeRFs, each
paired with a corresponding subtext prompt for precise object depiction. Next,
a tailored composition module seamlessly blends these NeRFs, promoting
consistency, while the dual-level text guidance reduces ambiguity and boosts
accuracy. Noticeably, the unique modularity of CompoNeRF permits NeRF
decomposition. This enables flexible scene editing and recomposition into new
scenes based on the edited layout or text prompts. Utilizing the open source
Stable Diffusion model, CompoNeRF not only generates scenes with high fidelity
but also paves the way for innovative multi-object composition using editable
3D layouts. Remarkably, our framework achieves up to a 54\% improvement in
performance, as measured by the multi-view CLIP score metric. Code is available
at https://github.com/hbai98/Componerf.";Haotian Bai<author:sep>Yuanhuiyi Lyu<author:sep>Lutao Jiang<author:sep>Sijia Li<author:sep>Haonan Lu<author:sep>Xiaodong Lin<author:sep>Lin Wang;http://arxiv.org/pdf/2303.13843v3;cs.CV;;nerf
2303.13817v1;http://arxiv.org/abs/2303.13817v1;2023-03-24;ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for  Neural Radiance Field;"Neural Radiance Field (NeRF) is a popular method in representing 3D scenes by
optimising a continuous volumetric scene function. Its large success which lies
in applying volumetric rendering (VR) is also its Achilles' heel in producing
view-dependent effects. As a consequence, glossy and transparent surfaces often
appear murky. A remedy to reduce these artefacts is to constrain this VR
equation by excluding volumes with back-facing normal. While this approach has
some success in rendering glossy surfaces, translucent objects are still poorly
represented. In this paper, we present an alternative to the physics-based VR
approach by introducing a self-attention-based framework on volumes along a
ray. In addition, inspired by modern game engines which utilise Light Probes to
store local lighting passing through the scene, we incorporate Learnable
Embeddings to capture view dependent effects within the scene. Our method,
which we call ABLE-NeRF, significantly reduces `blurry' glossy surfaces in
rendering and produces realistic translucent surfaces which lack in prior art.
In the Blender dataset, ABLE-NeRF achieves SOTA results and surpasses Ref-NeRF
in all 3 image quality metrics PSNR, SSIM, LPIPS.";Zhe Jun Tang<author:sep>Tat-Jen Cham<author:sep>Haiyu Zhao;http://arxiv.org/pdf/2303.13817v1;cs.CV;"IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)
  2023";nerf
2303.13014v1;http://arxiv.org/abs/2303.13014v1;2023-03-23;Semantic Ray: Learning a Generalizable Semantic Field with  Cross-Reprojection Attention;"In this paper, we aim to learn a semantic radiance field from multiple scenes
that is accurate, efficient and generalizable. While most existing NeRFs target
at the tasks of neural scene rendering, image synthesis and multi-view
reconstruction, there are a few attempts such as Semantic-NeRF that explore to
learn high-level semantic understanding with the NeRF structure. However,
Semantic-NeRF simultaneously learns color and semantic label from a single ray
with multiple heads, where the single ray fails to provide rich semantic
information. As a result, Semantic NeRF relies on positional encoding and needs
to train one specific model for each scene. To address this, we propose
Semantic Ray (S-Ray) to fully exploit semantic information along the ray
direction from its multi-view reprojections. As directly performing dense
attention over multi-view reprojected rays would suffer from heavy
computational cost, we design a Cross-Reprojection Attention module with
consecutive intra-view radial and cross-view sparse attentions, which
decomposes contextual information along reprojected rays and cross multiple
views and then collects dense connections by stacking the modules. Experiments
show that our S-Ray is able to learn from multiple scenes, and it presents
strong generalization ability to adapt to unseen scenes.";Fangfu Liu<author:sep>Chubin Zhang<author:sep>Yu Zheng<author:sep>Yueqi Duan;http://arxiv.org/pdf/2303.13014v1;cs.CV;Accepted by CVPR 2023. Project page: https://liuff19.github.io/S-Ray/;nerf
2303.13232v1;http://arxiv.org/abs/2303.13232v1;2023-03-23;Transforming Radiance Field with Lipschitz Network for Photorealistic 3D  Scene Stylization;"Recent advances in 3D scene representation and novel view synthesis have
witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not
trivial to exploit NeRF for the photorealistic 3D scene stylization task, which
aims to generate visually consistent and photorealistic stylized scenes from
novel views. Simply coupling NeRF with photorealistic style transfer (PST) will
result in cross-view inconsistency and degradation of stylized view syntheses.
Through a thorough analysis, we demonstrate that this non-trivial task can be
simplified in a new light: When transforming the appearance representation of a
pre-trained NeRF with Lipschitz mapping, the consistency and photorealism
across source views will be seamlessly encoded into the syntheses. That
motivates us to build a concise and flexible learning framework namely LipRF,
which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the
3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct
the 3D scene, and then emulates the style on each view by 2D PST as the prior
to learn a Lipschitz network to stylize the pre-trained appearance. In view of
that Lipschitz condition highly impacts the expressivity of the neural network,
we devise an adaptive regularization to balance the reconstruction and
stylization. A gradual gradient aggregation strategy is further introduced to
optimize LipRF in a cost-efficient manner. We conduct extensive experiments to
show the high quality and robust performance of LipRF on both photorealistic 3D
stylization and object appearance editing.";Zicheng Zhang<author:sep>Yinglu Liu<author:sep>Congying Han<author:sep>Yingwei Pan<author:sep>Tiande Guo<author:sep>Ting Yao;http://arxiv.org/pdf/2303.13232v1;cs.CV;CVPR 2023, Highlight;nerf
2303.13508v2;http://arxiv.org/abs/2303.13508v2;2023-03-23;DreamBooth3D: Subject-Driven Text-to-3D Generation;"We present DreamBooth3D, an approach to personalize text-to-3D generative
models from as few as 3-6 casually captured images of a subject. Our approach
combines recent advances in personalizing text-to-image models (DreamBooth)
with text-to-3D generation (DreamFusion). We find that naively combining these
methods fails to yield satisfactory subject-specific 3D assets due to
personalized text-to-image models overfitting to the input viewpoints of the
subject. We overcome this through a 3-stage optimization strategy where we
jointly leverage the 3D consistency of neural radiance fields together with the
personalization capability of text-to-image models. Our method can produce
high-quality, subject-specific 3D assets with text-driven modifications such as
novel poses, colors and attributes that are not seen in any of the input images
of the subject.";Amit Raj<author:sep>Srinivas Kaza<author:sep>Ben Poole<author:sep>Michael Niemeyer<author:sep>Nataniel Ruiz<author:sep>Ben Mildenhall<author:sep>Shiran Zada<author:sep>Kfir Aberman<author:sep>Michael Rubinstein<author:sep>Jonathan Barron<author:sep>Yuanzhen Li<author:sep>Varun Jampani;http://arxiv.org/pdf/2303.13508v2;cs.CV;"Project page at https://dreambooth3d.github.io/ Video Summary at
  https://youtu.be/kKVDrbfvOoA";
2303.13497v2;http://arxiv.org/abs/2303.13497v2;2023-03-23;TriPlaneNet: An Encoder for EG3D Inversion;"Recent progress in NeRF-based GANs has introduced a number of approaches for
high-resolution and high-fidelity generative modeling of human heads with a
possibility for novel view rendering. At the same time, one must solve an
inverse problem to be able to re-render or modify an existing image or video.
Despite the success of universal optimization-based methods for 2D GAN
inversion, those applied to 3D GANs may fail to extrapolate the result onto the
novel view, whereas optimization-based 3D GAN inversion methods are
time-consuming and can require at least several minutes per image. Fast
encoder-based techniques, such as those developed for StyleGAN, may also be
less appealing due to the lack of identity preservation. Our work introduces a
fast technique that bridges the gap between the two approaches by directly
utilizing the tri-plane representation presented for the EG3D generative model.
In particular, we build upon a feed-forward convolutional encoder for the
latent code and extend it with a fully-convolutional predictor of tri-plane
numerical offsets. The renderings are similar in quality to the ones produced
by optimization-based techniques and outperform the ones by encoder-based
methods. As we empirically prove, this is a consequence of directly operating
in the tri-plane space, not in the GAN parameter space, while making use of an
encoder-based trainable approach. Finally, we demonstrate significantly more
correct embedding of a face image in 3D than for all the baselines, further
strengthened by a probably symmetric prior enabled during training.";Ananta R. Bhattarai<author:sep>Matthias NieÃner<author:sep>Artem Sevastopolsky;http://arxiv.org/pdf/2303.13497v2;cs.CV;Project page: https://anantarb.github.io/triplanenet;nerf
2303.13472v2;http://arxiv.org/abs/2303.13472v2;2023-03-23;Plotting Behind the Scenes: Towards Learnable Game Engines;"Neural video game simulators emerged as powerful tools to generate and edit
videos. Their idea is to represent games as the evolution of an environment's
state driven by the actions of its agents. While such a paradigm enables users
to play a game action-by-action, its rigidity precludes more semantic forms of
control. To overcome this limitation, we augment game models with prompts
specified as a set of natural language actions and desired states. The result-a
Promptable Game Model (PGM)-makes it possible for a user to play the game by
prompting it with high- and low-level action sequences. Most captivatingly, our
PGM unlocks the director's mode, where the game is played by specifying goals
for the agents in the form of a prompt. This requires learning ""game AI"",
encapsulated by our animation model, to navigate the scene using high-level
constraints, play against an adversary, and devise a strategy to win a point.
To render the resulting state, we use a compositional NeRF representation
encapsulated in our synthesis model. To foster future research, we present
newly collected, annotated and calibrated Tennis and Minecraft datasets. Our
method significantly outperforms existing neural video game simulators in terms
of rendering quality and unlocks applications beyond the capabilities of the
current state of the art. Our framework, data, and models are available at
https://snap-research.github.io/promptable-game-models/.";Willi Menapace<author:sep>Aliaksandr Siarohin<author:sep>StÃ©phane LathuiliÃ¨re<author:sep>Panos Achlioptas<author:sep>Vladislav Golyanik<author:sep>Sergey Tulyakov<author:sep>Elisa Ricci;http://arxiv.org/pdf/2303.13472v2;cs.CV;"ACM Transactions on Graphics \c{opyright} Copyright is held by the
  owner/author(s) 2023. This is the author's version of the work. It is posted
  here for your personal use. Not for redistribution. The definitive Version of
  Record was published in ACM Transactions on Graphics,
  http://dx.doi.org/10.1145/3635705";nerf
2303.13277v2;http://arxiv.org/abs/2303.13277v2;2023-03-23;SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing  Field;"Despite the great success in 2D editing using user-friendly tools, such as
Photoshop, semantic strokes, or even text prompts, similar capabilities in 3D
areas are still limited, either relying on 3D modeling skills or allowing
editing within only a few categories. In this paper, we present a novel
semantic-driven NeRF editing approach, which enables users to edit a neural
radiance field with a single image, and faithfully delivers edited novel views
with high fidelity and multi-view consistency. To achieve this goal, we propose
a prior-guided editing field to encode fine-grained geometric and texture
editing in 3D space, and develop a series of techniques to aid the editing
process, including cyclic constraints with a proxy mesh to facilitate geometric
supervision, a color compositing mechanism to stabilize semantic-driven texture
editing, and a feature-cluster-based regularization to preserve the irrelevant
content unchanged. Extensive experiments and editing examples on both
real-world and synthetic data demonstrate that our method achieves
photo-realistic 3D editing using only a single edited image, pushing the bound
of semantic-driven editing in 3D real-world scenes. Our project webpage:
https://zju3dv.github.io/sine/.";Chong Bao<author:sep>Yinda Zhang<author:sep>Bangbang Yang<author:sep>Tianxing Fan<author:sep>Zesong Yang<author:sep>Hujun Bao<author:sep>Guofeng Zhang<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2303.13277v2;cs.CV;Accepted to CVPR 2023. Project Page: https://zju3dv.github.io/sine/;nerf
2303.13582v1;http://arxiv.org/abs/2303.13582v1;2023-03-23;SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates;"Neural radiance fields (NeRFs) have enabled high fidelity 3D reconstruction
from multiple 2D input views. However, a well-known drawback of NeRFs is the
less-than-ideal performance under a small number of views, due to insufficient
constraints enforced by volumetric rendering. To address this issue, we
introduce SCADE, a novel technique that improves NeRF reconstruction quality on
sparse, unconstrained input views for in-the-wild indoor scenes. To constrain
NeRF reconstruction, we leverage geometric priors in the form of per-view depth
estimates produced with state-of-the-art monocular depth estimation models,
which can generalize across scenes. A key challenge is that monocular depth
estimation is an ill-posed problem, with inherent ambiguities. To handle this
issue, we propose a new method that learns to predict, for each view, a
continuous, multimodal distribution of depth estimates using conditional
Implicit Maximum Likelihood Estimation (cIMLE). In order to disambiguate
exploiting multiple views, we introduce an original space carving loss that
guides the NeRF representation to fuse multiple hypothesized depth maps from
each view and distill from them a common geometry that is consistent with all
views. Experiments show that our approach enables higher fidelity novel view
synthesis from sparse views. Our project page can be found at
https://scade-spacecarving-nerfs.github.io .";Mikaela Angelina Uy<author:sep>Ricardo Martin-Brualla<author:sep>Leonidas Guibas<author:sep>Ke Li;http://arxiv.org/pdf/2303.13582v1;cs.CV;CVPR 2023;nerf
2303.13450v1;http://arxiv.org/abs/2303.13450v1;2023-03-23;Set-the-Scene: Global-Local Training for Generating Controllable NeRF  Scenes;"Recent breakthroughs in text-guided image generation have led to remarkable
progress in the field of 3D synthesis from text. By optimizing neural radiance
fields (NeRF) directly from text, recent methods are able to produce remarkable
results. Yet, these methods are limited in their control of each object's
placement or appearance, as they represent the scene as a whole. This can be a
major issue in scenarios that require refining or manipulating objects in the
scene. To remedy this deficit, we propose a novel GlobalLocal training
framework for synthesizing a 3D scene using object proxies. A proxy represents
the object's placement in the generated scene and optionally defines its coarse
geometry. The key to our approach is to represent each object as an independent
NeRF. We alternate between optimizing each NeRF on its own and as part of the
full scene. Thus, a complete representation of each object can be learned,
while also creating a harmonious scene with style and lighting match. We show
that using proxies allows a wide variety of editing options, such as adjusting
the placement of each independent object, removing objects from a scene, or
refining an object. Our results show that Set-the-Scene offers a powerful
solution for scene synthesis and manipulation, filling a crucial gap in
controllable text-to-3D synthesis.";Dana Cohen-Bar<author:sep>Elad Richardson<author:sep>Gal Metzer<author:sep>Raja Giryes<author:sep>Daniel Cohen-Or;http://arxiv.org/pdf/2303.13450v1;cs.CV;project page at https://danacohen95.github.io/Set-the-Scene/;nerf
2303.12280v2;http://arxiv.org/abs/2303.12280v2;2023-03-22;NLOS-NeuS: Non-line-of-sight Neural Implicit Surface;"Non-line-of-sight (NLOS) imaging is conducted to infer invisible scenes from
indirect light on visible objects. The neural transient field (NeTF) was
proposed for representing scenes as neural radiance fields in NLOS scenes. We
propose NLOS neural implicit surface (NLOS-NeuS), which extends the NeTF to
neural implicit surfaces with a signed distance function (SDF) for
reconstructing three-dimensional surfaces in NLOS scenes. We introduce two
constraints as loss functions for correctly learning an SDF to avoid non-zero
level-set surfaces. We also introduce a lower bound constraint of an SDF based
on the geometry of the first-returning photons. The experimental results
indicate that these constraints are essential for learning a correct SDF in
NLOS scenes. Compared with previous methods with discretized representation,
NLOS-NeuS with the neural continuous representation enables us to reconstruct
smooth surfaces while preserving fine details in NLOS scenes. To the best of
our knowledge, this is the first study on neural implicit surfaces with volume
rendering in NLOS scenes.";Yuki Fujimura<author:sep>Takahiro Kushida<author:sep>Takuya Funatomi<author:sep>Yasuhiro Mukaigawa;http://arxiv.org/pdf/2303.12280v2;cs.CV;ICCV 2023;
2303.12865v3;http://arxiv.org/abs/2303.12865v3;2023-03-22;NeRF-GAN Distillation for Efficient 3D-Aware Generation with  Convolutions;"Pose-conditioned convolutional generative models struggle with high-quality
3D-consistent image generation from single-view datasets, due to their lack of
sufficient 3D priors. Recently, the integration of Neural Radiance Fields
(NeRFs) and generative models, such as Generative Adversarial Networks (GANs),
has transformed 3D-aware generation from single-view images. NeRF-GANs exploit
the strong inductive bias of neural 3D representations and volumetric rendering
at the cost of higher computational complexity. This study aims at revisiting
pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by
distilling 3D knowledge from pretrained NeRF-GANs. We propose a simple and
effective method, based on re-using the well-disentangled latent space of a
pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly
generate 3D-consistent images corresponding to the underlying 3D
representations. Experiments on several datasets demonstrate that the proposed
method obtains results comparable with volumetric rendering in terms of quality
and 3D consistency while benefiting from the computational advantage of
convolutional networks. The code will be available at:
https://github.com/mshahbazi72/NeRF-GAN-Distillation";Mohamad Shahbazi<author:sep>Evangelos Ntavelis<author:sep>Alessio Tonioni<author:sep>Edo Collins<author:sep>Danda Pani Paudel<author:sep>Martin Danelljan<author:sep>Luc Van Gool;http://arxiv.org/pdf/2303.12865v3;cs.CV;;nerf
2303.12408v2;http://arxiv.org/abs/2303.12408v2;2023-03-22;Balanced Spherical Grid for Egocentric View Synthesis;"We present EgoNeRF, a practical solution to reconstruct large-scale
real-world environments for VR assets. Given a few seconds of casually captured
360 video, EgoNeRF can efficiently build neural radiance fields which enable
high-quality rendering from novel viewpoints. Motivated by the recent
acceleration of NeRF using feature grids, we adopt spherical coordinate instead
of conventional Cartesian coordinate. Cartesian feature grid is inefficient to
represent large-scale unbounded scenes because it has a spatially uniform
resolution, regardless of distance from viewers. The spherical parameterization
better aligns with the rays of egocentric images, and yet enables factorization
for performance enhancement. However, the na\""ive spherical grid suffers from
irregularities at two poles, and also cannot represent unbounded scenes. To
avoid singularities near poles, we combine two balanced grids, which results in
a quasi-uniform angular grid. We also partition the radial grid exponentially
and place an environment map at infinity to represent unbounded scenes.
Furthermore, with our resampling technique for grid-based methods, we can
increase the number of valid samples to train NeRF volume. We extensively
evaluate our method in our newly introduced synthetic and real-world egocentric
360 video datasets, and it consistently achieves state-of-the-art performance.";Changwoon Choi<author:sep>Sang Min Kim<author:sep>Young Min Kim;http://arxiv.org/pdf/2303.12408v2;cs.CV;Accepted to CVPR 2023;nerf
2303.12786v1;http://arxiv.org/abs/2303.12786v1;2023-03-22;FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation  Models;"Recent works on generalizable NeRFs have shown promising results on novel
view synthesis from single or few images. However, such models have rarely been
applied on other downstream tasks beyond synthesis such as semantic
understanding and parsing. In this paper, we propose a novel framework named
FeatureNeRF to learn generalizable NeRFs by distilling pre-trained vision
foundation models (e.g., DINO, Latent Diffusion). FeatureNeRF leverages 2D
pre-trained foundation models to 3D space via neural rendering, and then
extract deep features for 3D query points from NeRF MLPs. Consequently, it
allows to map 2D images to continuous 3D semantic feature volumes, which can be
used for various downstream tasks. We evaluate FeatureNeRF on tasks of 2D/3D
semantic keypoint transfer and 2D/3D object part segmentation. Our extensive
experiments demonstrate the effectiveness of FeatureNeRF as a generalizable 3D
semantic feature extractor. Our project page is available at
https://jianglongye.com/featurenerf/ .";Jianglong Ye<author:sep>Naiyan Wang<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2303.12786v1;cs.CV;Project page: https://jianglongye.com/featurenerf/;nerf
2303.12789v2;http://arxiv.org/abs/2303.12789v2;2023-03-22;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions;"We propose a method for editing NeRF scenes with text-instructions. Given a
NeRF of a scene and the collection of images used to reconstruct it, our method
uses an image-conditioned diffusion model (InstructPix2Pix) to iteratively edit
the input images while optimizing the underlying scene, resulting in an
optimized 3D scene that respects the edit instruction. We demonstrate that our
proposed method is able to edit large-scale, real-world scenes, and is able to
accomplish more realistic, targeted edits than prior work.";Ayaan Haque<author:sep>Matthew Tancik<author:sep>Alexei A. Efros<author:sep>Aleksander Holynski<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2303.12789v2;cs.CV;"Project website: https://instruct-nerf2nerf.github.io; v1. Revisions
  to related work and discussion";nerf
2303.12791v2;http://arxiv.org/abs/2303.12791v2;2023-03-22;SHERF: Generalizable Human NeRF from a Single Image;"Existing Human NeRF methods for reconstructing 3D humans typically rely on
multiple 2D images from multi-view cameras or monocular videos captured from
fixed camera views. However, in real-world scenarios, human images are often
captured from random camera angles, presenting challenges for high-quality 3D
human reconstruction. In this paper, we propose SHERF, the first generalizable
Human NeRF model for recovering animatable 3D humans from a single input image.
SHERF extracts and encodes 3D human representations in canonical space,
enabling rendering and animation from free views and poses. To achieve
high-fidelity novel view and pose synthesis, the encoded 3D human
representations should capture both global appearance and local fine-grained
textures. To this end, we propose a bank of 3D-aware hierarchical features,
including global, point-level, and pixel-aligned features, to facilitate
informative encoding. Global features enhance the information extracted from
the single input image and complement the information missing from the partial
2D observation. Point-level features provide strong clues of 3D human
structure, while pixel-aligned features preserve more fine-grained details. To
effectively integrate the 3D-aware hierarchical feature bank, we design a
feature fusion transformer. Extensive experiments on THuman, RenderPeople,
ZJU_MoCap, and HuMMan datasets demonstrate that SHERF achieves state-of-the-art
performance, with better generalizability for novel view and pose synthesis.";Shoukang Hu<author:sep>Fangzhou Hong<author:sep>Liang Pan<author:sep>Haiyi Mei<author:sep>Lei Yang<author:sep>Ziwei Liu;http://arxiv.org/pdf/2303.12791v2;cs.CV;"Accepted by ICCV2023. Project webpage:
  https://skhu101.github.io/SHERF/";nerf
2303.11938v2;http://arxiv.org/abs/2303.11938v2;2023-03-21;3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion;"We tackle the task of text-to-3D creation with pre-trained latent-based NeRFs
(NeRFs that generate 3D objects given input latent code). Recent works such as
DreamFusion and Magic3D have shown great success in generating 3D content using
NeRFs and text prompts, but the current approach of optimizing a NeRF for every
text prompt is 1) extremely time-consuming and 2) often leads to low-resolution
outputs. To address these challenges, we propose a novel method named
3D-CLFusion which leverages the pre-trained latent-based NeRFs and performs
fast 3D content creation in less than a minute. In particular, we introduce a
latent diffusion prior network for learning the w latent from the input CLIP
text/image embeddings. This pipeline allows us to produce the w latent without
further optimization during inference and the pre-trained NeRF is able to
perform multi-view high-resolution 3D synthesis based on the latent. We note
that the novelty of our model lies in that we introduce contrastive learning
during training the diffusion prior which enables the generation of the valid
view-invariant latent code. We demonstrate through experiments the
effectiveness of our proposed view-invariant diffusion process for fast
text-to-3D creation, e.g., 100 times faster than DreamFusion. We note that our
model is able to serve as the role of a plug-and-play tool for text-to-3D with
pre-trained NeRFs.";Yu-Jhe Li<author:sep>Tao Xu<author:sep>Ji Hou<author:sep>Bichen Wu<author:sep>Xiaoliang Dai<author:sep>Albert Pumarola<author:sep>Peizhao Zhang<author:sep>Peter Vajda<author:sep>Kris Kitani;http://arxiv.org/pdf/2303.11938v2;cs.CV;15 pages;nerf
2303.11728v3;http://arxiv.org/abs/2303.11728v3;2023-03-21;Few-shot Neural Radiance Fields Under Unconstrained Illumination;"In this paper, we introduce a new challenge for synthesizing novel view
images in practical environments with limited input multi-view images and
varying lighting conditions. Neural radiance fields (NeRF), one of the
pioneering works for this task, demand an extensive set of multi-view images
taken under constrained illumination, which is often unattainable in real-world
settings. While some previous works have managed to synthesize novel views
given images with different illumination, their performance still relies on a
substantial number of input multi-view images. To address this problem, we
suggest ExtremeNeRF, which utilizes multi-view albedo consistency, supported by
geometric alignment. Specifically, we extract intrinsic image components that
should be illumination-invariant across different views, enabling direct
appearance comparison between the input and novel view under unconstrained
illumination. We offer thorough experimental results for task evaluation,
employing the newly created NeRF Extreme benchmark-the first in-the-wild
benchmark for novel view synthesis under multiple viewing directions and
varying illuminations.";SeokYeong Lee<author:sep>JunYong Choi<author:sep>Seungryong Kim<author:sep>Ig-Jae Kim<author:sep>Junghyun Cho;http://arxiv.org/pdf/2303.11728v3;cs.CV;Project Page: https://seokyeong94.github.io/ExtremeNeRF/;nerf
2303.12234v1;http://arxiv.org/abs/2303.12234v1;2023-03-21;Pre-NeRF 360: Enriching Unbounded Appearances for Neural Radiance Fields;"Neural radiance fields (NeRF) appeared recently as a powerful tool to
generate realistic views of objects and confined areas. Still, they face
serious challenges with open scenes, where the camera has unrestricted movement
and content can appear at any distance. In such scenarios, current
NeRF-inspired models frequently yield hazy or pixelated outputs, suffer slow
training times, and might display irregularities, because of the challenging
task of reconstructing an extensive scene from a limited number of images. We
propose a new framework to boost the performance of NeRF-based architectures
yielding significantly superior outcomes compared to the prior work. Our
solution overcomes several obstacles that plagued earlier versions of NeRF,
including handling multiple video inputs, selecting keyframes, and extracting
poses from real-world frames that are ambiguous and symmetrical. Furthermore,
we applied our framework, dubbed as ""Pre-NeRF 360"", to enable the use of the
Nutrition5k dataset in NeRF and introduce an updated version of this dataset,
known as the N5k360 dataset.";Ahmad AlMughrabi<author:sep>Umair Haroon<author:sep>Ricardo Marques<author:sep>Petia Radeva;http://arxiv.org/pdf/2303.12234v1;cs.CV;;nerf
2303.11537v2;http://arxiv.org/abs/2303.11537v2;2023-03-21;Interactive Geometry Editing of Neural Radiance Fields;"In this paper, we propose a method that enables interactive geometry editing
for neural radiance fields manipulation. We use two proxy cages(inner cage and
outer cage) to edit a scene. The inner cage defines the operation target, and
the outer cage defines the adjustment space. Various operations apply to the
two cages. After cage selection, operations on the inner cage lead to the
desired transformation of the inner cage and adjustment of the outer cage.
Users can edit the scene with translation, rotation, scaling, or combinations.
The operations on the corners and edges of the cage are also supported. Our
method does not need any explicit 3D geometry representations. The interactive
geometry editing applies directly to the implicit neural radiance fields.
Extensive experimental results demonstrate the effectiveness of our approach.";Shaoxu Li<author:sep>Ye Pan;http://arxiv.org/pdf/2303.11537v2;cs.CV;;
2303.11052v3;http://arxiv.org/abs/2303.11052v3;2023-03-20;ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-real  Novel View Synthesis via Contrastive Learning;"Although many recent works have investigated generalizable NeRF-based novel
view synthesis for unseen scenes, they seldom consider the synthetic-to-real
generalization, which is desired in many practical applications. In this work,
we first investigate the effects of synthetic data in synthetic-to-real novel
view synthesis and surprisingly observe that models trained with synthetic data
tend to produce sharper but less accurate volume densities. For pixels where
the volume densities are correct, fine-grained details will be obtained.
Otherwise, severe artifacts will be produced. To maintain the advantages of
using synthetic data while avoiding its negative effects, we propose to
introduce geometry-aware contrastive learning to learn multi-view consistent
features with geometric constraints. Meanwhile, we adopt cross-view attention
to further enhance the geometry perception of features by querying features
across input views. Experiments demonstrate that under the synthetic-to-real
setting, our method can render images with higher quality and better
fine-grained details, outperforming existing generalizable novel view synthesis
methods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our
method also achieves state-of-the-art results.";Hao Yang<author:sep>Lanqing Hong<author:sep>Aoxue Li<author:sep>Tianyang Hu<author:sep>Zhenguo Li<author:sep>Gim Hee Lee<author:sep>Liwei Wang;http://arxiv.org/pdf/2303.11052v3;cs.CV;;nerf
2303.11364v1;http://arxiv.org/abs/2303.11364v1;2023-03-20;DehazeNeRF: Multiple Image Haze Removal and 3D Shape Reconstruction  using Neural Radiance Fields;"Neural radiance fields (NeRFs) have demonstrated state-of-the-art performance
for 3D computer vision tasks, including novel view synthesis and 3D shape
reconstruction. However, these methods fail in adverse weather conditions. To
address this challenge, we introduce DehazeNeRF as a framework that robustly
operates in hazy conditions. DehazeNeRF extends the volume rendering equation
by adding physically realistic terms that model atmospheric scattering. By
parameterizing these terms using suitable networks that match the physical
properties, we introduce effective inductive biases, which, together with the
proposed regularizations, allow DehazeNeRF to demonstrate successful multi-view
haze removal, novel view synthesis, and 3D shape reconstruction where existing
approaches fail.";Wei-Ting Chen<author:sep>Wang Yifan<author:sep>Sy-Yen Kuo<author:sep>Gordon Wetzstein;http://arxiv.org/pdf/2303.11364v1;cs.CV;"including supplemental material; project page:
  https://www.computationalimaging.org/publications/dehazenerf";nerf
2303.10598v3;http://arxiv.org/abs/2303.10598v3;2023-03-19;StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields;"3D style transfer aims to render stylized novel views of a 3D scene with
multi-view consistency. However, most existing work suffers from a three-way
dilemma over accurate geometry reconstruction, high-quality stylization, and
being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance
Fields), an innovative 3D style transfer technique that resolves the three-way
dilemma by performing style transformation within the feature space of a
radiance field. StyleRF employs an explicit grid of high-level features to
represent 3D scenes, with which high-fidelity geometry can be reliably restored
via volume rendering. In addition, it transforms the grid features according to
the reference style which directly leads to high-quality zero-shot style
transfer. StyleRF consists of two innovative designs. The first is
sampling-invariant content transformation that makes the transformation
invariant to the holistic statistics of the sampled 3D points and accordingly
ensures multi-view consistency. The second is deferred style transformation of
2D feature maps which is equivalent to the transformation of 3D points but
greatly reduces memory footprint without degrading multi-view consistency.
Extensive experiments show that StyleRF achieves superior 3D stylization
quality with precise geometry reconstruction and it can generalize to various
new styles in a zero-shot manner.";Kunhao Liu<author:sep>Fangneng Zhan<author:sep>Yiwen Chen<author:sep>Jiahui Zhang<author:sep>Yingchen Yu<author:sep>Abdulmotaleb El Saddik<author:sep>Shijian Lu<author:sep>Eric Xing;http://arxiv.org/pdf/2303.10598v3;cs.CV;"Accepted to CVPR 2023. Project website:
  https://kunhao-liu.github.io/StyleRF/";
2303.10709v1;http://arxiv.org/abs/2303.10709v1;2023-03-19;NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental  LiDAR Odometry and Mapping;"Simultaneously odometry and mapping using LiDAR data is an important task for
mobile systems to achieve full autonomy in large-scale environments. However,
most existing LiDAR-based methods prioritize tracking quality over
reconstruction quality. Although the recently developed neural radiance fields
(NeRF) have shown promising advances in implicit reconstruction for indoor
environments, the problem of simultaneous odometry and mapping for large-scale
scenarios using incremental LiDAR data remains unexplored. To bridge this gap,
in this paper, we propose a novel NeRF-based LiDAR odometry and mapping
approach, NeRF-LOAM, consisting of three modules neural odometry, neural
mapping, and mesh reconstruction. All these modules utilize our proposed neural
signed distance function, which separates LiDAR points into ground and
non-ground points to reduce Z-axis drift, optimizes odometry and voxel
embeddings concurrently, and in the end generates dense smooth mesh maps of the
environment. Moreover, this joint optimization allows our NeRF-LOAM to be
pre-trained free and exhibit strong generalization abilities when applied to
different environments. Extensive evaluations on three publicly available
datasets demonstrate that our approach achieves state-of-the-art odometry and
mapping performance, as well as a strong generalization in large-scale
environments utilizing LiDAR data. Furthermore, we perform multiple ablation
studies to validate the effectiveness of our network design. The implementation
of our approach will be made available at
https://github.com/JunyuanDeng/NeRF-LOAM.";Junyuan Deng<author:sep>Xieyuanli Chen<author:sep>Songpengcheng Xia<author:sep>Zhen Sun<author:sep>Guoqing Liu<author:sep>Wenxian Yu<author:sep>Ling Pei;http://arxiv.org/pdf/2303.10709v1;cs.CV;;nerf
2303.10735v4;http://arxiv.org/abs/2303.10735v4;2023-03-19;SKED: Sketch-guided Text-based 3D Editing;"Text-to-image diffusion models are gradually introduced into computer
graphics, recently enabling the development of Text-to-3D pipelines in an open
domain. However, for interactive editing purposes, local manipulations of
content through a simplistic textual interface can be arduous. Incorporating
user guided sketches with Text-to-image pipelines offers users more intuitive
control. Still, as state-of-the-art Text-to-3D pipelines rely on optimizing
Neural Radiance Fields (NeRF) through gradients from arbitrary rendering views,
conditioning on sketches is not straightforward. In this paper, we present
SKED, a technique for editing 3D shapes represented by NeRFs. Our technique
utilizes as few as two guiding sketches from different views to alter an
existing neural field. The edited region respects the prompt semantics through
a pre-trained diffusion model. To ensure the generated output adheres to the
provided sketches, we propose novel loss functions to generate the desired
edits while preserving the density and radiance of the base instance. We
demonstrate the effectiveness of our proposed method through several
qualitative and quantitative experiments. https://sked-paper.github.io/";Aryan Mikaeili<author:sep>Or Perel<author:sep>Mehdi Safaee<author:sep>Daniel Cohen-Or<author:sep>Ali Mahdavi-Amiri;http://arxiv.org/pdf/2303.10735v4;cs.CV;;nerf
2303.10340v1;http://arxiv.org/abs/2303.10340v1;2023-03-18;3D Data Augmentation for Driving Scenes on Camera;"Driving scenes are extremely diverse and complicated that it is impossible to
collect all cases with human effort alone. While data augmentation is an
effective technique to enrich the training data, existing methods for camera
data in autonomous driving applications are confined to the 2D image plane,
which may not optimally increase data diversity in 3D real-world scenarios. To
this end, we propose a 3D data augmentation approach termed Drive-3DAug, aiming
at augmenting the driving scenes on camera in the 3D space. We first utilize
Neural Radiance Field (NeRF) to reconstruct the 3D models of background and
foreground objects. Then, augmented driving scenes can be obtained by placing
the 3D objects with adapted location and orientation at the pre-defined valid
region of backgrounds. As such, the training database could be effectively
scaled up. However, the 3D object modeling is constrained to the image quality
and the limited viewpoints. To overcome these problems, we modify the original
NeRF by introducing a geometric rectified loss and a symmetric-aware training
strategy. We evaluate our method for the camera-only monocular 3D detection
task on the Waymo and nuScences datasets. The proposed data augmentation
approach contributes to a gain of 1.7% and 1.4% in terms of detection accuracy,
on Waymo and nuScences respectively. Furthermore, the constructed 3D models
serve as digital driving assets and could be recycled for different detectors
or other 3D perception tasks.";Wenwen Tong<author:sep>Jiangwei Xie<author:sep>Tianyu Li<author:sep>Hanming Deng<author:sep>Xiangwei Geng<author:sep>Ruoyi Zhou<author:sep>Dingchen Yang<author:sep>Bo Dai<author:sep>Lewei Lu<author:sep>Hongyang Li;http://arxiv.org/pdf/2303.10340v1;cs.CV;;nerf
2303.09952v2;http://arxiv.org/abs/2303.09952v2;2023-03-17;Single-view Neural Radiance Fields with Depth Teacher;"Neural Radiance Fields (NeRF) have been proposed for photorealistic novel
view rendering. However, it requires many different views of one scene for
training. Moreover, it has poor generalizations to new scenes and requires
retraining or fine-tuning on each scene. In this paper, we develop a new NeRF
model for novel view synthesis using only a single image as input. We propose
to combine the (coarse) planar rendering and the (fine) volume rendering to
achieve higher rendering quality and better generalizations. We also design a
depth teacher net that predicts dense pseudo depth maps to supervise the joint
rendering mechanism and boost the learning of consistent 3D geometry. We
evaluate our method on three challenging datasets. It outperforms
state-of-the-art single-view NeRFs by achieving 5$\sim$20\% improvements in
PSNR and reducing 20$\sim$50\% of the errors in the depth rendering. It also
shows excellent generalization abilities to unseen data without the need to
fine-tune on each new scene.";Yurui Chen<author:sep>Chun Gu<author:sep>Feihu Zhang<author:sep>Li Zhang;http://arxiv.org/pdf/2303.09952v2;cs.CV;;nerf
2303.10083v1;http://arxiv.org/abs/2303.10083v1;2023-03-17;$Î±$Surf: Implicit Surface Reconstruction for Semi-Transparent and  Thin Objects with Decoupled Geometry and Opacity;"Implicit surface representations such as the signed distance function (SDF)
have emerged as a promising approach for image-based surface reconstruction.
However, existing optimization methods assume solid surfaces and are therefore
unable to properly reconstruct semi-transparent surfaces and thin structures,
which also exhibit low opacity due to the blending effect with the background.
While neural radiance field (NeRF) based methods can model semi-transparency
and achieve photo-realistic quality in synthesized novel views, their
volumetric geometry representation tightly couples geometry and opacity, and
therefore cannot be easily converted into surfaces without introducing
artifacts. We present $\alpha$Surf, a novel surface representation with
decoupled geometry and opacity for the reconstruction of semi-transparent and
thin surfaces where the colors mix. Ray-surface intersections on our
representation can be found in closed-form via analytical solutions of cubic
polynomials, avoiding Monte-Carlo sampling and is fully differentiable by
construction. Our qualitative and quantitative evaluations show that our
approach can accurately reconstruct surfaces with semi-transparent and thin
parts with fewer artifacts, achieving better reconstruction quality than
state-of-the-art SDF and NeRF methods. Website: https://alphasurf.netlify.app/";Tianhao Wu<author:sep>Hanxue Liang<author:sep>Fangcheng Zhong<author:sep>Gernot Riegler<author:sep>Shimon Vainer<author:sep>Cengiz Oztireli;http://arxiv.org/pdf/2303.10083v1;cs.CV;;nerf
2303.09412v4;http://arxiv.org/abs/2303.09412v4;2023-03-16;NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing  Diverse Intrinsic and Extrinsic Camera Parameters;"Novel view synthesis using neural radiance fields (NeRF) is the
state-of-the-art technique for generating high-quality images from novel
viewpoints. Existing methods require a priori knowledge about extrinsic and
intrinsic camera parameters. This limits their applicability to synthetic
scenes, or real-world scenarios with the necessity of a preprocessing step.
Current research on the joint optimization of camera parameters and NeRF
focuses on refining noisy extrinsic camera parameters and often relies on the
preprocessing of intrinsic camera parameters. Further approaches are limited to
cover only one single camera intrinsic. To address these limitations, we
propose a novel end-to-end trainable approach called NeRFtrinsic Four. We
utilize Gaussian Fourier features to estimate extrinsic camera parameters and
dynamically predict varying intrinsic camera parameters through the supervision
of the projection error. Our approach outperforms existing joint optimization
methods on LLFF and BLEFF. In addition to these existing datasets, we introduce
a new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsic
Four is a step forward in joint optimization NeRF-based view synthesis and
enables more realistic and flexible rendering in real-world scenarios with
varying camera parameters.";Hannah Schieber<author:sep>Fabian Deuser<author:sep>Bernhard Egger<author:sep>Norbert Oswald<author:sep>Daniel Roth;http://arxiv.org/pdf/2303.09412v4;cs.CV;;nerf
2303.09153v1;http://arxiv.org/abs/2303.09153v1;2023-03-16;Reliable Image Dehazing by NeRF;"We present an image dehazing algorithm with high quality, wide application,
and no data training or prior needed. We analyze the defects of the original
dehazing model, and propose a new and reliable dehazing reconstruction and
dehazing model based on the combination of optical scattering model and
computer graphics lighting rendering model. Based on the new haze model and the
images obtained by the cameras, we can reconstruct the three-dimensional space,
accurately calculate the objects and haze in the space, and use the
transparency relationship of haze to perform accurate haze removal. To obtain a
3D simulation dataset we used the Unreal 5 computer graphics rendering engine.
In order to obtain real shot data in different scenes, we used fog generators,
array cameras, mobile phones, underwater cameras and drones to obtain haze
data. We use formula derivation, simulation data set and real shot data set
result experimental results to prove the feasibility of the new method.
Compared with various other methods, we are far ahead in terms of calculation
indicators (4 dB higher quality average scene), color remains more natural, and
the algorithm is more robust in different scenarios and best in the subjective
perception.";Zheyan Jin<author:sep>Shiqi Chen<author:sep>Huajun Feng<author:sep>Zhihai Xu<author:sep>Qi Li<author:sep>Yueting Chen;http://arxiv.org/pdf/2303.09153v1;cs.CV;12pages, 8figures;nerf
2303.09431v1;http://arxiv.org/abs/2303.09431v1;2023-03-16;NeRFMeshing: Distilling Neural Radiance Fields into  Geometrically-Accurate 3D Meshes;"With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis
has recently made a big leap forward. At the core, NeRF proposes that each 3D
point can emit radiance, allowing to conduct view synthesis using
differentiable volumetric rendering. While neural radiance fields can
accurately represent 3D scenes for computing the image rendering, 3D meshes are
still the main scene representation supported by most computer graphics and
simulation pipelines, enabling tasks such as real time rendering and
physics-based simulations. Obtaining 3D meshes from neural radiance fields
still remains an open challenge since NeRFs are optimized for view synthesis,
not enforcing an accurate underlying geometry on the radiance field. We thus
propose a novel compact and flexible architecture that enables easy 3D surface
reconstruction from any NeRF-driven approach. Upon having trained the radiance
field, we distill the volumetric 3D representation into a Signed Surface
Approximation Network, allowing easy extraction of the 3D mesh and appearance.
Our final 3D mesh is physically accurate and can be rendered in real time on an
array of devices.";Marie-Julie Rakotosaona<author:sep>Fabian Manhardt<author:sep>Diego Martin Arroyo<author:sep>Michael Niemeyer<author:sep>Abhijit Kundu<author:sep>Federico Tombari;http://arxiv.org/pdf/2303.09431v1;cs.CV;;nerf
2303.09554v3;http://arxiv.org/abs/2303.09554v3;2023-03-16;PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D  Supervision;"Impressive progress in generative models and implicit representations gave
rise to methods that can generate 3D shapes of high quality. However, being
able to locally control and edit shapes is another essential property that can
unlock several content creation applications. Local control can be achieved
with part-aware models, but existing methods require 3D supervision and cannot
produce textures. In this work, we devise PartNeRF, a novel part-aware
generative model for editable 3D shape synthesis that does not require any
explicit 3D supervision. Our model generates objects as a set of locally
defined NeRFs, augmented with an affine transformation. This enables several
editing operations such as applying transformations on parts, mixing parts from
different objects etc. To ensure distinct, manipulable parts we enforce a hard
assignment of rays to parts that makes sure that the color of each ray is only
determined by a single NeRF. As a result, altering one part does not affect the
appearance of the others. Evaluations on various ShapeNet categories
demonstrate the ability of our model to generate editable 3D objects of
improved fidelity, compared to previous part-based generative approaches that
require 3D supervision or models relying on NeRFs.";Konstantinos Tertikas<author:sep>Despoina Paschalidou<author:sep>Boxiao Pan<author:sep>Jeong Joon Park<author:sep>Mikaela Angelina Uy<author:sep>Ioannis Emiris<author:sep>Yannis Avrithis<author:sep>Leonidas Guibas;http://arxiv.org/pdf/2303.09554v3;cs.CV;"To appear in CVPR 2023, Project Page:
  https://ktertikas.github.io/part_nerf";nerf
2303.09553v1;http://arxiv.org/abs/2303.09553v1;2023-03-16;LERF: Language Embedded Radiance Fields;"Humans describe the physical world using natural language to refer to
specific 3D locations based on a vast range of properties: visual appearance,
semantics, abstract associations, or actionable affordances. In this work we
propose Language Embedded Radiance Fields (LERFs), a method for grounding
language embeddings from off-the-shelf models like CLIP into NeRF, which enable
these types of open-ended language queries in 3D. LERF learns a dense,
multi-scale language field inside NeRF by volume rendering CLIP embeddings
along training rays, supervising these embeddings across training views to
provide multi-view consistency and smooth the underlying language field. After
optimization, LERF can extract 3D relevancy maps for a broad range of language
prompts interactively in real-time, which has potential use cases in robotics,
understanding vision-language models, and interacting with 3D scenes. LERF
enables pixel-aligned, zero-shot queries on the distilled 3D CLIP embeddings
without relying on region proposals or masks, supporting long-tail
open-vocabulary queries hierarchically across the volume. The project website
can be found at https://lerf.io .";Justin Kerr<author:sep>Chung Min Kim<author:sep>Ken Goldberg<author:sep>Angjoo Kanazawa<author:sep>Matthew Tancik;http://arxiv.org/pdf/2303.09553v1;cs.CV;Project website can be found at https://lerf.io;nerf
2303.08695v1;http://arxiv.org/abs/2303.08695v1;2023-03-15;RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or  missing camera parameters;"Novel view synthesis (NVS) is a challenging task in computer vision that
involves synthesizing new views of a scene from a limited set of input images.
Neural Radiance Fields (NeRF) have emerged as a powerful approach to address
this problem, but they require accurate knowledge of camera \textit{intrinsic}
and \textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM)
and multi-view stereo (MVS) approaches have been used to extract camera
parameters, but these methods can be unreliable and may fail in certain cases.
In this paper, we propose a novel technique that leverages unposed images from
dynamic datasets, such as the NVIDIA dynamic scenes dataset, to learn camera
parameters directly from data. Our approach is highly extensible and can be
integrated into existing NeRF architectures with minimal modifications. We
demonstrate the effectiveness of our method on a variety of static and dynamic
scenes and show that it outperforms traditional SfM and MVS approaches. The
code for our method is publicly available at
\href{https://github.com/redacted/refinerf}{https://github.com/redacted/refinerf}.
Our approach offers a promising new direction for improving the accuracy and
robustness of NVS using NeRF, and we anticipate that it will be a valuable tool
for a wide range of applications in computer vision and graphics.";Shuja Khalid<author:sep>Frank Rudzicz;http://arxiv.org/pdf/2303.08695v1;cs.CV;;nerf
2303.08370v1;http://arxiv.org/abs/2303.08370v1;2023-03-15;Harnessing Low-Frequency Neural Fields for Few-Shot View Synthesis;"Neural Radiance Fields (NeRF) have led to breakthroughs in the novel view
synthesis problem. Positional Encoding (P.E.) is a critical factor that brings
the impressive performance of NeRF, where low-dimensional coordinates are
mapped to high-dimensional space to better recover scene details. However,
blindly increasing the frequency of P.E. leads to overfitting when the
reconstruction problem is highly underconstrained, \eg, few-shot images for
training. We harness low-frequency neural fields to regularize high-frequency
neural fields from overfitting to better address the problem of few-shot view
synthesis. We propose reconstructing with a low-frequency only field and then
finishing details with a high-frequency equipped field. Unlike most existing
solutions that regularize the output space (\ie, rendered images), our
regularization is conducted in the input space (\ie, signal frequency). We
further propose a simple-yet-effective strategy for tuning the frequency to
avoid overfitting few-shot inputs: enforcing consistency among the frequency
domain of rendered 2D images. Thanks to the input space regularizing scheme,
our method readily applies to inputs beyond spatial locations, such as the time
dimension in dynamic scenes. Comparisons with state-of-the-art on both
synthetic and natural datasets validate the effectiveness of our proposed
solution for few-shot view synthesis. Code is available at
\href{https://github.com/lsongx/halo}{https://github.com/lsongx/halo}.";Liangchen Song<author:sep>Zhong Li<author:sep>Xuan Gong<author:sep>Lele Chen<author:sep>Zhang Chen<author:sep>Yi Xu<author:sep>Junsong Yuan;http://arxiv.org/pdf/2303.08370v1;cs.CV;;nerf
2303.08717v1;http://arxiv.org/abs/2303.08717v1;2023-03-15;Re-ReND: Real-time Rendering of NeRFs across Devices;"This paper proposes a novel approach for rendering a pre-trained Neural
Radiance Field (NeRF) in real-time on resource-constrained devices. We
introduce Re-ReND, a method enabling Real-time Rendering of NeRFs across
Devices. Re-ReND is designed to achieve real-time performance by converting the
NeRF into a representation that can be efficiently processed by standard
graphics pipelines. The proposed method distills the NeRF by extracting the
learned density into a mesh, while the learned color information is factorized
into a set of matrices that represent the scene's light field. Factorization
implies the field is queried via inexpensive MLP-free matrix multiplications,
while using a light field allows rendering a pixel by querying the field a
single time-as opposed to hundreds of queries when employing a radiance field.
Since the proposed representation can be implemented using a fragment shader,
it can be directly integrated with standard rasterization frameworks. Our
flexible implementation can render a NeRF in real-time with low memory
requirements and on a wide range of resource-constrained devices, including
mobiles and AR/VR headsets. Notably, we find that Re-ReND can achieve over a
2.6-fold increase in rendering speed versus the state-of-the-art without
perceptible losses in quality.";Sara Rojas<author:sep>Jesus Zarzar<author:sep>Juan Camilo Perez<author:sep>Artsiom Sanakoyeu<author:sep>Ali Thabet<author:sep>Albert Pumarola<author:sep>Bernard Ghanem;http://arxiv.org/pdf/2303.08717v1;cs.CV;;nerf
2303.08808v1;http://arxiv.org/abs/2303.08808v1;2023-03-15;Mesh Strikes Back: Fast and Efficient Human Reconstruction from RGB  videos;"Human reconstruction and synthesis from monocular RGB videos is a challenging
problem due to clothing, occlusion, texture discontinuities and sharpness, and
framespecific pose changes. Many methods employ deferred rendering, NeRFs and
implicit methods to represent clothed humans, on the premise that mesh-based
representations cannot capture complex clothing and textures from RGB,
silhouettes, and keypoints alone. We provide a counter viewpoint to this
fundamental premise by optimizing a SMPL+D mesh and an efficient,
multi-resolution texture representation using only RGB images, binary
silhouettes and sparse 2D keypoints. Experimental results demonstrate that our
approach is more capable of capturing geometric details compared to visual
hull, mesh-based methods. We show competitive novel view synthesis and
improvements in novel pose synthesis compared to NeRF-based methods, which
introduce noticeable, unwanted artifacts. By restricting the solution space to
the SMPL+D model combined with differentiable rendering, we obtain dramatic
speedups in compute, training times (up to 24x) and inference times (up to
192x). Our method therefore can be used as is or as a fast initialization to
NeRF-based methods.";Rohit Jena<author:sep>Pratik Chaudhari<author:sep>James Gee<author:sep>Ganesh Iyer<author:sep>Siddharth Choudhary<author:sep>Brandon M. Smith;http://arxiv.org/pdf/2303.08808v1;cs.CV;;nerf
2303.08096v2;http://arxiv.org/abs/2303.08096v2;2023-03-14;MELON: NeRF with Unposed Images in SO(3);"Neural radiance fields enable novel-view synthesis and scene reconstruction
with photorealistic quality from a few images, but require known and accurate
camera poses. Conventional pose estimation algorithms fail on smooth or
self-similar scenes, while methods performing inverse rendering from unposed
views require a rough initialization of the camera orientations. The main
difficulty of pose estimation lies in real-life objects being almost invariant
under certain transformations, making the photometric distance between rendered
views non-convex with respect to the camera parameters. Using an equivalence
relation that matches the distribution of local minima in camera space, we
reduce this space to its quotient set, in which pose estimation becomes a more
convex problem. Using a neural-network to regularize pose estimation, we
demonstrate that our method - MELON - can reconstruct a neural radiance field
from unposed images with state-of-the-art accuracy while requiring ten times
fewer views than adversarial approaches.";Axel Levy<author:sep>Mark Matthews<author:sep>Matan Sela<author:sep>Gordon Wetzstein<author:sep>Dmitry Lagun;http://arxiv.org/pdf/2303.08096v2;cs.CV;;nerf
2303.07937v3;http://arxiv.org/abs/2303.07937v3;2023-03-14;Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D  Generation;"Text-to-3D generation has shown rapid progress in recent days with the advent
of score distillation, a methodology of using pretrained text-to-2D diffusion
models to optimize neural radiance field (NeRF) in the zero-shot setting.
However, the lack of 3D awareness in the 2D diffusion models destabilizes score
distillation-based methods from reconstructing a plausible 3D scene. To address
this issue, we propose 3DFuse, a novel framework that incorporates 3D awareness
into pretrained 2D diffusion models, enhancing the robustness and 3D
consistency of score distillation-based methods. We realize this by first
constructing a coarse 3D structure of a given text prompt and then utilizing
projected, view-specific depth map as a condition for the diffusion model.
Additionally, we introduce a training strategy that enables the 2D diffusion
model learns to handle the errors and sparsity within the coarse 3D structure
for robust generation, as well as a method for ensuring semantic consistency
throughout all viewpoints of the scene. Our framework surpasses the limitations
of prior arts, and has significant implications for 3D consistent generation of
2D diffusion models.";Junyoung Seo<author:sep>Wooseok Jang<author:sep>Min-Seop Kwak<author:sep>Jaehoon Ko<author:sep>Hyeonsu Kim<author:sep>Junho Kim<author:sep>Jin-Hwa Kim<author:sep>Jiyoung Lee<author:sep>Seungryong Kim;http://arxiv.org/pdf/2303.07937v3;cs.CV;Project page https://ku-cvlab.github.io/3DFuse/;nerf
2303.07596v2;http://arxiv.org/abs/2303.07596v2;2023-03-14;Frequency-Modulated Point Cloud Rendering with Easy Editing;"We develop an effective point cloud rendering pipeline for novel view
synthesis, which enables high fidelity local detail reconstruction, real-time
rendering and user-friendly editing. In the heart of our pipeline is an
adaptive frequency modulation module called Adaptive Frequency Net (AFNet),
which utilizes a hypernetwork to learn the local texture frequency encoding
that is consecutively injected into adaptive frequency activation layers to
modulate the implicit radiance signal. This mechanism improves the frequency
expressive ability of the network with richer frequency basis support, only at
a small computational budget. To further boost performance, a preprocessing
module is also proposed for point cloud geometry optimization via point opacity
estimation. In contrast to implicit rendering, our pipeline supports
high-fidelity interactive editing based on point cloud manipulation. Extensive
experimental results on NeRF-Synthetic, ScanNet, DTU and Tanks and Temples
datasets demonstrate the superior performances achieved by our method in terms
of PSNR, SSIM and LPIPS, in comparison to the state-of-the-art.";Yi Zhang<author:sep>Xiaoyang Huang<author:sep>Bingbing Ni<author:sep>Teng Li<author:sep>Wenjun Zhang;http://arxiv.org/pdf/2303.07596v2;cs.CV;Accepted by CVPR 2023;nerf
2303.07653v2;http://arxiv.org/abs/2303.07653v2;2023-03-14;NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from  Multi-view Images;"We study the problem of reconstructing 3D feature curves of an object from a
set of calibrated multi-view images. To do so, we learn a neural implicit field
representing the density distribution of 3D edges which we refer to as Neural
Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based
rendering loss where a 2D edge map is rendered at a given view and is compared
to the ground-truth edge map extracted from the image of that view. The
rendering-based differentiable optimization of NEF fully exploits 2D edge
detection, without needing a supervision of 3D edges, a 3D geometric operator
or cross-view edge correspondence. Several technical designs are devised to
ensure learning a range-limited and view-independent NEF for robust edge
extraction. The final parametric 3D curves are extracted from NEF with an
iterative optimization method. On our benchmark with synthetic data, we
demonstrate that NEF outperforms existing state-of-the-art methods on all
metrics. Project page: https://yunfan1202.github.io/NEF/.";Yunfan Ye<author:sep>Renjiao Yi<author:sep>Zhirui Gao<author:sep>Chenyang Zhu<author:sep>Zhiping Cai<author:sep>Kai Xu;http://arxiv.org/pdf/2303.07653v2;cs.CV;CVPR 2023;nerf
2303.07634v2;http://arxiv.org/abs/2303.07634v2;2023-03-14;I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via  Raytracing in Neural SDFs;"In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene
reconstruction and editing using differentiable Monte Carlo raytracing on
neural signed distance fields (SDFs). Our holistic neural SDF-based framework
jointly recovers the underlying shapes, incident radiance and materials from
multi-view images. We introduce a novel bubble loss for fine-grained small
objects and error-guided adaptive sampling scheme to largely improve the
reconstruction quality on large-scale indoor scenes. Further, we propose to
decompose the neural radiance field into spatially-varying material of the
scene as a neural field through surface-based, differentiable Monte Carlo
raytracing and emitter semantic segmentations, which enables physically based
and photorealistic scene relighting and editing applications. Through a number
of qualitative and quantitative experiments, we demonstrate the superior
quality of our method on indoor scene reconstruction, novel view synthesis, and
scene editing compared to state-of-the-art baselines.";Jingsen Zhu<author:sep>Yuchi Huo<author:sep>Qi Ye<author:sep>Fujun Luan<author:sep>Jifan Li<author:sep>Dianbing Xi<author:sep>Lisha Wang<author:sep>Rui Tang<author:sep>Wei Hua<author:sep>Hujun Bao<author:sep>Rui Wang;http://arxiv.org/pdf/2303.07634v2;cs.CV;"Accepted by CVPR 2023, project page:
  https://jingsenzhu.github.io/i2-sdf";
2303.06919v2;http://arxiv.org/abs/2303.06919v2;2023-03-13;NeRFLiX: High-Quality Neural View Synthesis by Learning a  Degradation-Driven Inter-viewpoint MiXer;"Neural radiance fields (NeRF) show great success in novel view synthesis.
However, in real-world scenes, recovering high-quality details from the source
images is still challenging for the existing NeRF-based approaches, due to the
potential imperfect calibration information and scene representation
inaccuracy. Even with high-quality training frames, the synthetic novel views
produced by NeRF models still suffer from notable rendering artifacts, such as
noise, blur, etc. Towards to improve the synthesis quality of NeRF-based
approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by
learning a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for existing deep neural networks. Moreover, beyond the degradation
removal, we propose an inter-viewpoint aggregation framework that is able to
fuse highly related high-quality training images, pushing the performance of
cutting-edge NeRF models to entirely new levels and producing highly
photo-realistic synthetic views.";Kun Zhou<author:sep>Wenbo Li<author:sep>Yi Wang<author:sep>Tao Hu<author:sep>Nianjuan Jiang<author:sep>Xiaoguang Han<author:sep>Jiangbo Lu;http://arxiv.org/pdf/2303.06919v2;cs.CV;"Accepted to CVPR 2023; Project Page: see
  https://redrock303.github.io/nerflix/";nerf
2303.07418v1;http://arxiv.org/abs/2303.07418v1;2023-03-13;FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency  Regularization;"Novel view synthesis with sparse inputs is a challenging problem for neural
radiance fields (NeRF). Recent efforts alleviate this challenge by introducing
external supervision, such as pre-trained models and extra depth signals, and
by non-trivial patch-based rendering. In this paper, we present Frequency
regularized NeRF (FreeNeRF), a surprisingly simple baseline that outperforms
previous methods with minimal modifications to the plain NeRF. We analyze the
key challenges in few-shot neural rendering and find that frequency plays an
important role in NeRF's training. Based on the analysis, we propose two
regularization terms. One is to regularize the frequency range of NeRF's
inputs, while the other is to penalize the near-camera density fields. Both
techniques are ``free lunches'' at no additional computational cost. We
demonstrate that even with one line of code change, the original NeRF can
achieve similar performance as other complicated methods in the few-shot
setting. FreeNeRF achieves state-of-the-art performance across diverse
datasets, including Blender, DTU, and LLFF. We hope this simple baseline will
motivate a rethinking of the fundamental role of frequency in NeRF's training
under the low-data regime and beyond.";Jiawei Yang<author:sep>Marco Pavone<author:sep>Yue Wang;http://arxiv.org/pdf/2303.07418v1;cs.CV;"Project page: https://jiawei-yang.github.io/FreeNeRF/, Code at:
  https://github.com/Jiawei-Yang/FreeNeRF";nerf
2303.06335v3;http://arxiv.org/abs/2303.06335v3;2023-03-11;Just Flip: Flipped Observation Generation and Optimization for Neural  Radiance Fields to Cover Unobserved View;"With the advent of Neural Radiance Field (NeRF), representing 3D scenes
through multiple observations has shown remarkable improvements in performance.
Since this cutting-edge technique is able to obtain high-resolution renderings
by interpolating dense 3D environments, various approaches have been proposed
to apply NeRF for the spatial understanding of robot perception. However,
previous works are challenging to represent unobserved scenes or views on the
unexplored robot trajectory, as these works do not take into account 3D
reconstruction without observation information. To overcome this problem, we
propose a method to generate flipped observation in order to cover unexisting
observation for unexplored robot trajectory. To achieve this, we propose a data
augmentation method for 3D reconstruction using NeRF by flipping observed
images, and estimating flipped camera 6DOF poses. Our technique exploits the
property of objects being geometrically symmetric, making it simple but fast
and powerful, thereby making it suitable for robotic applications where
real-time performance is important. We demonstrate that our method
significantly improves three representative perceptual quality measures on the
NeRF synthetic dataset.";Minjae Lee<author:sep>Kyeongsu Kang<author:sep>Hyeonwoo Yu;http://arxiv.org/pdf/2303.06335v3;cs.RO;;nerf
2303.06226v2;http://arxiv.org/abs/2303.06226v2;2023-03-10;NeRFlame: FLAME-based conditioning of NeRF for 3D face rendering;"Traditional 3D face models are based on mesh representations with texture.
One of the most important models is FLAME (Faces Learned with an Articulated
Model and Expressions), which produces meshes of human faces that are fully
controllable. Unfortunately, such models have problems with capturing geometric
and appearance details. In contrast to mesh representation, the neural radiance
field (NeRF) produces extremely sharp renders. However, implicit methods are
hard to animate and do not generalize well to unseen expressions. It is not
trivial to effectively control NeRF models to obtain face manipulation.
  The present paper proposes a novel approach, named NeRFlame, which combines
the strengths of both NeRF and FLAME methods. Our method enables high-quality
rendering capabilities of NeRF while also offering complete control over the
visual appearance, similar to FLAME. In contrast to traditional NeRF-based
structures that use neural networks for RGB color and volume density modeling,
our approach utilizes the FLAME mesh as a distinct density volume.
Consequently, color values exist only in the vicinity of the FLAME mesh. This
FLAME framework is seamlessly incorporated into the NeRF architecture for
predicting RGB colors, enabling our model to explicitly represent volume
density and implicitly capture RGB colors.";Wojciech ZajÄc<author:sep>Joanna WaczyÅska<author:sep>Piotr Borycki<author:sep>Jacek Tabor<author:sep>Maciej ZiÄba<author:sep>PrzemysÅaw Spurek;http://arxiv.org/pdf/2303.06226v2;cs.CV;;nerf
2303.05703v2;http://arxiv.org/abs/2303.05703v2;2023-03-10;MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field;"We present MovingParts, a NeRF-based method for dynamic scene reconstruction
and part discovery. We consider motion as an important cue for identifying
parts, that all particles on the same part share the common motion pattern.
From the perspective of fluid simulation, existing deformation-based methods
for dynamic NeRF can be seen as parameterizing the scene motion under the
Eulerian view, i.e., focusing on specific locations in space through which the
fluid flows as time passes. However, it is intractable to extract the motion of
constituting objects or parts using the Eulerian view representation. In this
work, we introduce the dual Lagrangian view and enforce representations under
the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian
view, we parameterize the scene motion by tracking the trajectory of particles
on objects. The Lagrangian view makes it convenient to discover parts by
factorizing the scene motion as a composition of part-level rigid motions.
Experimentally, our method can achieve fast and high-quality dynamic scene
reconstruction from even a single moving camera, and the induced part-based
representation allows direct applications of part tracking, animation, 3D scene
editing, etc.";Kaizhi Yang<author:sep>Xiaoshuai Zhang<author:sep>Zhiao Huang<author:sep>Xuejin Chen<author:sep>Zexiang Xu<author:sep>Hao Su;http://arxiv.org/pdf/2303.05703v2;cs.CV;Project Page: https://silenkzyoung.github.io/MovingParts-WebPage/;nerf
2303.05807v2;http://arxiv.org/abs/2303.05807v2;2023-03-10;Aleth-NeRF: Low-light Condition View Synthesis with Concealing Fields;"Common capture low-light scenes are challenging for most computer vision
techniques, including Neural Radiance Fields (NeRF). Vanilla NeRF is
viewer-centred simplifies the rendering process only as light emission from 3D
locations in the viewing direction, thus failing to model the low-illumination
induced darkness. Inspired by the emission theory of ancient Greeks that visual
perception is accomplished by rays casting from eyes, we make slight
modifications on vanilla NeRF to train on multiple views of low-light scenes,
we can thus render out the well-lit scene in an unsupervised manner. We
introduce a surrogate concept, Concealing Fields, that reduces the transport of
light during the volume rendering stage. Specifically, our proposed method,
Aleth-NeRF, directly learns from the dark image to understand volumetric object
representation and concealing field under priors. By simply eliminating
Concealing Fields, we can render a single or multi-view well-lit image(s) and
gain superior performance over other 2D low-light enhancement methods.
Additionally, we collect the first paired LOw-light and normal-light Multi-view
(LOM) datasets for future research. This version is invalid, please refer to
our new AAAI version: arXiv:2312.09093";Ziteng Cui<author:sep>Lin Gu<author:sep>Xiao Sun<author:sep>Xianzheng Ma<author:sep>Yu Qiao<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2303.05807v2;cs.CV;"website page: https://cuiziteng.github.io/Aleth_NeRF_web/, refer to
  new version: arXiv:2312.09093";nerf
2303.06138v4;http://arxiv.org/abs/2303.06138v4;2023-03-10;Learning Object-Centric Neural Scattering Functions for Free-Viewpoint  Relighting and Scene Composition;"Photorealistic object appearance modeling from 2D images is a constant topic
in vision and graphics. While neural implicit methods (such as Neural Radiance
Fields) have shown high-fidelity view synthesis results, they cannot relight
the captured objects. More recent neural inverse rendering approaches have
enabled object relighting, but they represent surface properties as simple
BRDFs, and therefore cannot handle translucent objects. We propose
Object-Centric Neural Scattering Functions (OSFs) for learning to reconstruct
object appearance from only images. OSFs not only support free-viewpoint object
relighting, but also can model both opaque and translucent objects. While
accurately modeling subsurface light transport for translucent objects can be
highly complex and even intractable for neural methods, OSFs learn to
approximate the radiance transfer from a distant light to an outgoing direction
at any spatial location. This approximation avoids explicitly modeling complex
subsurface scattering, making learning a neural implicit model tractable.
Experiments on real and synthetic data show that OSFs accurately reconstruct
appearances for both opaque and translucent objects, allowing faithful
free-viewpoint relighting as well as scene composition.";Hong-Xing Yu<author:sep>Michelle Guo<author:sep>Alireza Fathi<author:sep>Yen-Yu Chang<author:sep>Eric Ryan Chan<author:sep>Ruohan Gao<author:sep>Thomas Funkhouser<author:sep>Jiajun Wu;http://arxiv.org/pdf/2303.06138v4;cs.CV;"Journal extension of arXiv:2012.08503 (TMLR 2023). The first two
  authors contributed equally to this work. Project page:
  https://kovenyu.com/osf/";
2303.05775v1;http://arxiv.org/abs/2303.05775v1;2023-03-10;Self-NeRF: A Self-Training Pipeline for Few-Shot Neural Radiance Fields;"Recently, Neural Radiance Fields (NeRF) have emerged as a potent method for
synthesizing novel views from a dense set of images. Despite its impressive
performance, NeRF is plagued by its necessity for numerous calibrated views and
its accuracy diminishes significantly in a few-shot setting. To address this
challenge, we propose Self-NeRF, a self-evolved NeRF that iteratively refines
the radiance fields with very few number of input views, without incorporating
additional priors. Basically, we train our model under the supervision of
reference and unseen views simultaneously in an iterative procedure. In each
iteration, we label unseen views with the predicted colors or warped pixels
generated by the model from the preceding iteration. However, these expanded
pseudo-views are afflicted by imprecision in color and warping artifacts, which
degrades the performance of NeRF. To alleviate this issue, we construct an
uncertainty-aware NeRF with specialized embeddings. Some techniques such as
cone entropy regularization are further utilized to leverage the pseudo-views
in the most efficient manner. Through experiments under various settings, we
verified that our Self-NeRF is robust to input with uncertainty and surpasses
existing methods when trained on limited training data.";Jiayang Bai<author:sep>Letian Huang<author:sep>Wen Gong<author:sep>Jie Guo<author:sep>Yanwen Guo;http://arxiv.org/pdf/2303.05775v1;cs.CV;11 pages, 11 figures;nerf
2303.05735v6;http://arxiv.org/abs/2303.05735v6;2023-03-10;Hardware Acceleration of Neural Graphics;"Rendering and inverse-rendering algorithms that drive conventional computer
graphics have recently been superseded by neural representations (NR). NRs have
recently been used to learn the geometric and the material properties of the
scenes and use the information to synthesize photorealistic imagery, thereby
promising a replacement for traditional rendering algorithms with scalable
quality and predictable performance. In this work we ask the question: Does
neural graphics (NG) need hardware support? We studied representative NG
applications showing that, if we want to render 4k res. at 60FPS there is a gap
of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications,
there is an even larger gap of 2-4 OOM between the desired performance and the
required system power. We identify that the input encoding and the MLP kernels
are the performance bottlenecks, consuming 72%,60% and 59% of application time
for multi res. hashgrid, multi res. densegrid and low res. densegrid encodings,
respectively. We propose a NG processing cluster, a scalable and flexible
hardware architecture that directly accelerates the input encoding and MLP
kernels through dedicated engines and supports a wide range of NG applications.
We also accelerate the rest of the kernels by fusing them together in Vulkan,
which leads to 9.94X kernel-level performance improvement compared to un-fused
implementation of the pre-processing and the post-processing kernels. Our
results show that, NGPC gives up to 58X end-to-end application-level
performance improvement, for multi res. hashgrid encoding on average across the
four NG applications, the performance benefits are 12X,20X,33X and 39X for the
scaling factor of 8,16,32 and 64, respectively. Our results show that with
multi res. hashgrid encoding, NGPC enables the rendering of 4k res. at 30FPS
for NeRF and 8k res. at 120FPS for all our other NG applications.";Muhammad Husnain Mubarik<author:sep>Ramakrishna Kanungo<author:sep>Tobias Zirr<author:sep>Rakesh Kumar;http://arxiv.org/pdf/2303.05735v6;cs.AR;;nerf
2303.05835v1;http://arxiv.org/abs/2303.05835v1;2023-03-10;You Only Train Once: Multi-Identity Free-Viewpoint Neural Human  Rendering from Monocular Videos;"We introduce You Only Train Once (YOTO), a dynamic human generation
framework, which performs free-viewpoint rendering of different human
identities with distinct motions, via only one-time training from monocular
videos. Most prior works for the task require individualized optimization for
each input video that contains a distinct human identity, leading to a
significant amount of time and resources for the deployment, thereby impeding
the scalability and the overall application potential of the system. In this
paper, we tackle this problem by proposing a set of learnable identity codes to
expand the capability of the framework for multi-identity free-viewpoint
rendering, and an effective pose-conditioned code query mechanism to finely
model the pose-dependent non-rigid motions. YOTO optimizes neural radiance
fields (NeRF) by utilizing designed identity codes to condition the model for
learning various canonical T-pose appearances in a single shared volumetric
representation. Besides, our joint learning of multiple identities within a
unified model incidentally enables flexible motion transfer in high-quality
photo-realistic renderings for all learned appearances. This capability expands
its potential use in important applications, including Virtual Reality. We
present extensive experimental results on ZJU-MoCap and PeopleSnapshot to
clearly demonstrate the effectiveness of our proposed model. YOTO shows
state-of-the-art performance on all evaluation metrics while showing
significant benefits in training and inference efficiency as well as rendering
quality. The code and model will be made publicly available soon.";Jaehyeok Kim<author:sep>Dongyoon Wee<author:sep>Dan Xu;http://arxiv.org/pdf/2303.05835v1;cs.CV;;nerf
2303.05512v1;http://arxiv.org/abs/2303.05512v1;2023-03-09;PAC-NeRF: Physics Augmented Continuum Neural Radiance Fields for  Geometry-Agnostic System Identification;"Existing approaches to system identification (estimating the physical
parameters of an object) from videos assume known object geometries. This
precludes their applicability in a vast majority of scenes where object
geometries are complex or unknown. In this work, we aim to identify parameters
characterizing a physical system from a set of multi-view videos without any
assumption on object geometry or topology. To this end, we propose ""Physics
Augmented Continuum Neural Radiance Fields"" (PAC-NeRF), to estimate both the
unknown geometry and physical parameters of highly dynamic objects from
multi-view videos. We design PAC-NeRF to only ever produce physically plausible
states by enforcing the neural radiance field to follow the conservation laws
of continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian
representation of the neural radiance field, i.e., we use the Eulerian grid
representation for NeRF density and color fields, while advecting the neural
radiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian
representation seamlessly blends efficient neural rendering with the material
point method (MPM) for robust differentiable physics simulation. We validate
the effectiveness of our proposed framework on geometry and physical parameter
estimation over a vast range of materials, including elastic bodies,
plasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate
significant performance gain on most tasks.";Xuan Li<author:sep>Yi-Ling Qiao<author:sep>Peter Yichen Chen<author:sep>Krishna Murthy Jatavallabhula<author:sep>Ming Lin<author:sep>Chenfanfu Jiang<author:sep>Chuang Gan;http://arxiv.org/pdf/2303.05512v1;cs.CV;"ICLR 2023 Spotlight. Project page:
  https://sites.google.com/view/PAC-NeRF";nerf
2303.04508v2;http://arxiv.org/abs/2303.04508v2;2023-03-08;InFusionSurf: Refining Neural RGB-D Surface Reconstruction Using  Per-Frame Intrinsic Refinement and TSDF Fusion Prior Learning;"We introduce InFusionSurf, a novel approach to enhance the fidelity of neural
radiance field (NeRF) frameworks for 3D surface reconstruction using RGB-D
video frames. Building upon previous methods that have employed feature
encoding to improve optimization speed, we further improve the reconstruction
quality with minimal impact on optimization time by refining depth information.
Our per-frame intrinsic refinement scheme addresses frame-specific blurs caused
by camera motion in each depth frame. Furthermore, InFusionSurf utilizes a
classical real-time 3D surface reconstruction method, the truncated signed
distance field (TSDF) Fusion, as prior knowledge to pretrain the feature grid
to support reconstruction details while accelerating the training. The
quantitative and qualitative experiments comparing the performances of
InFusionSurf against prior work indicate that our method is capable of
accurately reconstructing a scene without sacrificing optimization speed. We
also demonstrate the effectiveness of our per-frame intrinsic refinement and
TSDF Fusion prior learning techniques via an ablation study.";Seunghwan Lee<author:sep>Gwanmo Park<author:sep>Hyewon Son<author:sep>Jiwon Ryu<author:sep>Han Joo Chae;http://arxiv.org/pdf/2303.04508v2;cs.CV;;nerf
2303.04322v2;http://arxiv.org/abs/2303.04322v2;2023-03-08;DroNeRF: Real-time Multi-agent Drone Pose Optimization for Computing  Neural Radiance Fields;"We present a novel optimization algorithm called DroNeRF for the autonomous
positioning of monocular camera drones around an object for real-time 3D
reconstruction using only a few images. Neural Radiance Fields or NeRF, is a
novel view synthesis technique used to generate new views of an object or scene
from a set of input images. Using drones in conjunction with NeRF provides a
unique and dynamic way to generate novel views of a scene, especially with
limited scene capabilities of restricted movements. Our approach focuses on
calculating optimized pose for individual drones while solely depending on the
object geometry without using any external localization system. The unique
camera positioning during the data-capturing phase significantly impacts the
quality of the 3D model. To evaluate the quality of our generated novel views,
we compute different perceptual metrics like the Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index Measure(SSIM). Our work demonstrates the
benefit of using an optimal placement of various drones with limited mobility
to generate perceptually better results.";Dipam Patel<author:sep>Phu Pham<author:sep>Aniket Bera;http://arxiv.org/pdf/2303.04322v2;cs.RO;"To appear in 2023 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2023)";nerf
2303.04869v2;http://arxiv.org/abs/2303.04869v2;2023-03-08;CROSSFIRE: Camera Relocalization On Self-Supervised Features from an  Implicit Representation;"Beyond novel view synthesis, Neural Radiance Fields are useful for
applications that interact with the real world. In this paper, we use them as
an implicit map of a given scene and propose a camera relocalization algorithm
tailored for this representation. The proposed method enables to compute in
real-time the precise position of a device using a single RGB camera, during
its navigation. In contrast with previous work, we do not rely on pose
regression or photometric alignment but rather use dense local features
obtained through volumetric rendering which are specialized on the scene with a
self-supervised objective. As a result, our algorithm is more accurate than
competitors, able to operate in dynamic outdoor environments with changing
lightning conditions and can be readily integrated in any volumetric neural
renderer.";Arthur Moreau<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Dzmitry Tsishkou<author:sep>Bogdan Stanciulescu<author:sep>Arnaud de La Fortelle;http://arxiv.org/pdf/2303.04869v2;cs.CV;Accepted to ICCV 2023;
2303.04086v1;http://arxiv.org/abs/2303.04086v1;2023-03-07;NEPHELE: A Neural Platform for Highly Realistic Cloud Radiance Rendering;"We have recently seen tremendous progress in neural rendering (NR) advances,
i.e., NeRF, for photo-real free-view synthesis. Yet, as a local technique based
on a single computer/GPU, even the best-engineered Instant-NGP or i-NGP cannot
reach real-time performance when rendering at a high resolution, and often
requires huge local computing resources. In this paper, we resort to cloud
rendering and present NEPHELE, a neural platform for highly realistic cloud
radiance rendering. In stark contrast with existing NR approaches, our NEPHELE
allows for more powerful rendering capabilities by combining multiple remote
GPUs and facilitates collaboration by allowing multiple people to view the same
NeRF scene simultaneously. We introduce i-NOLF to employ opacity light fields
for ultra-fast neural radiance rendering in a one-query-per-ray manner. We
further resemble the Lumigraph with geometry proxies for fast ray querying and
subsequently employ a small MLP to model the local opacity lumishperes for
high-quality rendering. We also adopt Perfect Spatial Hashing in i-NOLF to
enhance cache coherence. As a result, our i-NOLF achieves an order of magnitude
performance gain in terms of efficiency than i-NGP, especially for the
multi-user multi-viewpoint setting under cloud rendering scenarios. We further
tailor a task scheduler accompanied by our i-NOLF representation and
demonstrate the advance of our methodological design through a comprehensive
cloud platform, consisting of a series of cooperated modules, i.e., render
farms, task assigner, frame composer, and detailed streaming strategies. Using
such a cloud platform compatible with neural rendering, we further showcase the
capabilities of our cloud radiance rendering through a series of applications,
ranging from cloud VR/AR rendering.";Haimin Luo<author:sep>Siyuan Zhang<author:sep>Fuqiang Zhao<author:sep>Haotian Jing<author:sep>Penghao Wang<author:sep>Zhenxiao Yu<author:sep>Dongxue Yan<author:sep>Junran Ding<author:sep>Boyuan Zhang<author:sep>Qiang Hu<author:sep>Shu Yin<author:sep>Lan Xu<author:sep>JIngyi Yu;http://arxiv.org/pdf/2303.04086v1;cs.GR;;nerf
2303.03808v2;http://arxiv.org/abs/2303.03808v2;2023-03-07;Multiscale Tensor Decomposition and Rendering Equation Encoding for View  Synthesis;"Rendering novel views from captured multi-view images has made considerable
progress since the emergence of the neural radiance field. This paper aims to
further advance the quality of view synthesis by proposing a novel approach
dubbed the neural radiance feature field (NRFF). We first propose a multiscale
tensor decomposition scheme to organize learnable features so as to represent
scenes from coarse to fine scales. We demonstrate many benefits of the proposed
multiscale representation, including more accurate scene shape and appearance
reconstruction, and faster convergence compared with the single-scale
representation. Instead of encoding view directions to model view-dependent
effects, we further propose to encode the rendering equation in the feature
space by employing the anisotropic spherical Gaussian mixture predicted from
the proposed multiscale representation. The proposed NRFF improves
state-of-the-art rendering results by over 1 dB in PSNR on both the NeRF and
NSVF synthetic datasets. A significant improvement has also been observed on
the real-world Tanks & Temples dataset. Code can be found at
https://github.com/imkanghan/nrff.";Kang Han<author:sep>Wei Xiang;http://arxiv.org/pdf/2303.03808v2;cs.CV;;nerf
2303.03003v2;http://arxiv.org/abs/2303.03003v2;2023-03-06;Efficient Large-scale Scene Representation with a Hybrid of  High-resolution Grid and Plane Features;"Existing neural radiance fields (NeRF) methods for large-scale scene modeling
require days of training using multiple GPUs, hindering their applications in
scenarios with limited computing resources. Despite fast optimization NeRF
variants have been proposed based on the explicit dense or hash grid features,
their effectivenesses are mainly demonstrated in object-scale scene
representation. In this paper, we point out that the low feature resolution in
explicit representation is the bottleneck for large-scale unbounded scene
representation. To address this problem, we introduce a new and efficient
hybrid feature representation for NeRF that fuses the 3D hash-grids and
high-resolution 2D dense plane features. Compared with the dense-grid
representation, the resolution of a dense 2D plane can be scaled up more
efficiently. Based on this hybrid representation, we propose a fast
optimization NeRF variant, called GP-NeRF, that achieves better rendering
results while maintaining a compact model size. Extensive experiments on
multiple large-scale unbounded scene datasets show that our model can converge
in 1.5 hours using a single GPU while achieving results comparable to or even
better than the existing method that requires about one day's training with 8
GPUs.";Yuqi Zhang<author:sep>Guanying Chen<author:sep>Shuguang Cui;http://arxiv.org/pdf/2303.03003v2;cs.CV;;nerf
2303.03056v3;http://arxiv.org/abs/2303.03056v3;2023-03-06;MOISST: Multimodal Optimization of Implicit Scene for SpatioTemporal  calibration;"With the recent advances in autonomous driving and the decreasing cost of
LiDARs, the use of multimodal sensor systems is on the rise. However, in order
to make use of the information provided by a variety of complimentary sensors,
it is necessary to accurately calibrate them. We take advantage of recent
advances in computer graphics and implicit volumetric scene representation to
tackle the problem of multi-sensor spatial and temporal calibration. Thanks to
a new formulation of the Neural Radiance Field (NeRF) optimization, we are able
to jointly optimize calibration parameters along with scene representation
based on radiometric and geometric measurements. Our method enables accurate
and robust calibration from data captured in uncontrolled and unstructured
urban environments, making our solution more scalable than existing calibration
solutions. We demonstrate the accuracy and robustness of our method in urban
scenes typically encountered in autonomous driving scenarios.";Quentin Herau<author:sep>Nathan Piasco<author:sep>Moussab Bennehar<author:sep>Luis RoldÃ£o<author:sep>Dzmitry Tsishkou<author:sep>Cyrille Migniot<author:sep>Pascal Vasseur<author:sep>CÃ©dric Demonceaux;http://arxiv.org/pdf/2303.03056v3;cs.CV;Accepted at IROS2023 Project site: https://qherau.github.io/MOISST/;nerf
2303.03361v2;http://arxiv.org/abs/2303.03361v2;2023-03-06;Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene  Representation from 2D Supervision;"We address efficient and structure-aware 3D scene representation from images.
Nerflets are our key contribution -- a set of local neural radiance fields that
together represent a scene. Each nerflet maintains its own spatial position,
orientation, and extent, within which it contributes to panoptic, density, and
radiance reconstructions. By leveraging only photometric and inferred panoptic
image supervision, we can directly and jointly optimize the parameters of a set
of nerflets so as to form a decomposed representation of the scene, where each
object instance is represented by a group of nerflets. During experiments with
indoor and outdoor environments, we find that nerflets: (1) fit and approximate
the scene more efficiently than traditional global NeRFs, (2) allow the
extraction of panoptic and photometric renderings from arbitrary views, and (3)
enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive
editing.";Xiaoshuai Zhang<author:sep>Abhijit Kundu<author:sep>Thomas Funkhouser<author:sep>Leonidas Guibas<author:sep>Hao Su<author:sep>Kyle Genova;http://arxiv.org/pdf/2303.03361v2;cs.CV;accepted by CVPR 2023;nerf
2303.03966v1;http://arxiv.org/abs/2303.03966v1;2023-03-05;Semantic-aware Occlusion Filtering Neural Radiance Fields in the Wild;"We present a learning framework for reconstructing neural scene
representations from a small number of unconstrained tourist photos. Since each
image contains transient occluders, decomposing the static and transient
components is necessary to construct radiance fields with such in-the-wild
photographs where existing methods require a lot of training data. We introduce
SF-NeRF, aiming to disentangle those two components with only a few images
given, which exploits semantic information without any supervision. The
proposed method contains an occlusion filtering module that predicts the
transient color and its opacity for each pixel, which enables the NeRF model to
solely learn the static scene representation. This filtering module learns the
transient phenomena guided by pixel-wise semantic features obtained by a
trainable image encoder that can be trained across multiple scenes to learn the
prior of transient objects. Furthermore, we present two techniques to prevent
ambiguous decomposition and noisy results of the filtering module. We
demonstrate that our method outperforms state-of-the-art novel view synthesis
methods on Phototourism dataset in a few-shot setting.";Jaewon Lee<author:sep>Injae Kim<author:sep>Hwan Heo<author:sep>Hyunwoo J. Kim;http://arxiv.org/pdf/2303.03966v1;cs.CV;11 pages, 5 figures;nerf
2303.02091v2;http://arxiv.org/abs/2303.02091v2;2023-03-03;Delicate Textured Mesh Recovery from NeRF via Adaptive Surface  Refinement;"Neural Radiance Fields (NeRF) have constituted a remarkable breakthrough in
image-based 3D reconstruction. However, their implicit volumetric
representations differ significantly from the widely-adopted polygonal meshes
and lack support from common 3D software and hardware, making their rendering
and manipulation inefficient. To overcome this limitation, we present a novel
framework that generates textured surface meshes from images. Our approach
begins by efficiently initializing the geometry and view-dependency decomposed
appearance with a NeRF. Subsequently, a coarse mesh is extracted, and an
iterative surface refining algorithm is developed to adaptively adjust both
vertex positions and face density based on re-projected rendering errors. We
jointly refine the appearance with geometry and bake it into texture images for
real-time rendering. Extensive experiments demonstrate that our method achieves
superior mesh quality and competitive rendering quality.";Jiaxiang Tang<author:sep>Hang Zhou<author:sep>Xiaokang Chen<author:sep>Tianshu Hu<author:sep>Errui Ding<author:sep>Jingdong Wang<author:sep>Gang Zeng;http://arxiv.org/pdf/2303.02091v2;cs.CV;ICCV 2023 camera-ready, Project Page: https://me.kiui.moe/nerf2mesh;nerf
2303.01736v1;http://arxiv.org/abs/2303.01736v1;2023-03-03;Multi-Plane Neural Radiance Fields for Novel View Synthesis;"Novel view synthesis is a long-standing problem that revolves around
rendering frames of scenes from novel camera viewpoints. Volumetric approaches
provide a solution for modeling occlusions through the explicit 3D
representation of the camera frustum. Multi-plane Images (MPI) are volumetric
methods that represent the scene using front-parallel planes at distinct depths
but suffer from depth discretization leading to a 2.D scene representation.
Another line of approach relies on implicit 3D scene representations. Neural
Radiance Fields (NeRF) utilize neural networks for encapsulating the continuous
3D scene structure within the network weights achieving photorealistic
synthesis results, however, methods are constrained to per-scene optimization
settings which are inefficient in practice. Multi-plane Neural Radiance Fields
(MINE) open the door for combining implicit and explicit scene representations.
It enables continuous 3D scene representations, especially in the depth
dimension, while utilizing the input image features to avoid per-scene
optimization. The main drawback of the current literature work in this domain
is being constrained to single-view input, limiting the synthesis ability to
narrow viewpoint ranges. In this work, we thoroughly examine the performance,
generalization, and efficiency of single-view multi-plane neural radiance
fields. In addition, we propose a new multiplane NeRF architecture that accepts
multiple views to improve the synthesis results and expand the viewing range.
Features from the input source frames are effectively fused through a proposed
attention-aware fusion module to highlight important information from different
viewpoints. Experiments show the effectiveness of attention-based fusion and
the promising outcomes of our proposed method when compared to multi-view NeRF
and MPI techniques.";Youssef Abdelkareem<author:sep>Shady Shehata<author:sep>Fakhri Karray;http://arxiv.org/pdf/2303.01736v1;cs.CV;ICDIPV 2023;nerf
2303.00304v4;http://arxiv.org/abs/2303.00304v4;2023-03-01;Renderable Neural Radiance Map for Visual Navigation;"We propose a novel type of map for visual navigation, a renderable neural
radiance map (RNR-Map), which is designed to contain the overall visual
information of a 3D environment. The RNR-Map has a grid form and consists of
latent codes at each pixel. These latent codes are embedded from image
observations, and can be converted to the neural radiance field which enables
image rendering given a camera pose. The recorded latent codes implicitly
contain visual information about the environment, which makes the RNR-Map
visually descriptive. This visual information in RNR-Map can be a useful
guideline for visual localization and navigation. We develop localization and
navigation frameworks that can effectively utilize the RNR-Map. We evaluate the
proposed frameworks on camera tracking, visual localization, and image-goal
navigation. Experimental results show that the RNR-Map-based localization
framework can find the target location based on a single query image with fast
speed and competitive accuracy compared to other baselines. Also, this
localization framework is robust to environmental changes, and even finds the
most visually similar places when a query image from a different environment is
given. The proposed navigation framework outperforms the existing image-goal
navigation methods in difficult scenarios, under odometry and actuation noises.
The navigation framework shows 65.7% success rate in curved scenarios of the
NRNS dataset, which is an improvement of 18.6% over the current
state-of-the-art. Project page: https://rllab-snu.github.io/projects/RNR-Map/";Obin Kwon<author:sep>Jeongho Park<author:sep>Songhwai Oh;http://arxiv.org/pdf/2303.00304v4;cs.CV;"Preprint version. CVPR 2023 accepted, highlight paper. Project page:
  https://rllab-snu.github.io/projects/RNR-Map/";
2303.00749v1;http://arxiv.org/abs/2303.00749v1;2023-03-01;S-NeRF: Neural Radiance Fields for Street Views;"Neural Radiance Fields (NeRFs) aim to synthesize novel views of objects and
scenes, given the object-centric camera views with large overlaps. However, we
conjugate that this paradigm does not fit the nature of the street views that
are collected by many self-driving cars from the large-scale unbounded scenes.
Also, the onboard cameras perceive scenes without much overlapping. Thus,
existing NeRFs often produce blurs, 'floaters' and other artifacts on
street-view synthesis. In this paper, we propose a new street-view NeRF
(S-NeRF) that considers novel view synthesis of both the large-scale background
scenes and the foreground moving vehicles jointly. Specifically, we improve the
scene parameterization function and the camera poses for learning better neural
representations from street views. We also use the the noisy and sparse LiDAR
points to boost the training and learn a robust geometry and reprojection based
confidence to address the depth outliers. Moreover, we extend our S-NeRF for
reconstructing moving vehicles that is impracticable for conventional NeRFs.
Thorough experiments on the large-scale driving datasets (e.g., nuScenes and
Waymo) demonstrate that our method beats the state-of-the-art rivals by
reducing 7% to 40% of the mean-squared error in the street-view synthesis and a
45% PSNR gain for the moving vehicles rendering.";Ziyang Xie<author:sep>Junge Zhang<author:sep>Wenye Li<author:sep>Feihu Zhang<author:sep>Li Zhang;http://arxiv.org/pdf/2303.00749v1;cs.CV;ICLR 2023;nerf
2303.00050v1;http://arxiv.org/abs/2303.00050v1;2023-02-28;Dynamic Multi-View Scene Reconstruction Using Neural Implicit Surface;"Reconstructing general dynamic scenes is important for many computer vision
and graphics applications. Recent works represent the dynamic scene with neural
radiance fields for photorealistic view synthesis, while their surface geometry
is under-constrained and noisy. Other works introduce surface constraints to
the implicit neural representation to disentangle the ambiguity of geometry and
appearance field for static scene reconstruction. To bridge the gap between
rendering dynamic scenes and recovering static surface geometry, we propose a
template-free method to reconstruct surface geometry and appearance using
neural implicit representations from multi-view videos. We leverage
topology-aware deformation and the signed distance field to learn complex
dynamic surfaces via differentiable volume rendering without scene-specific
prior knowledge like template models. Furthermore, we propose a novel
mask-based ray selection strategy to significantly boost the optimization on
challenging time-varying regions. Experiments on different multi-view video
datasets demonstrate that our method achieves high-fidelity surface
reconstruction as well as photorealistic novel view synthesis.";Decai Chen<author:sep>Haofei Lu<author:sep>Ingo Feldmann<author:sep>Oliver Schreer<author:sep>Peter Eisert;http://arxiv.org/pdf/2303.00050v1;cs.CV;5 pages, accepted by ICASSP 2023;
2302.14683v2;http://arxiv.org/abs/2302.14683v2;2023-02-28;IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF;"Recently, many works have been proposed to utilize the neural radiance field
for novel view synthesis of human performers. However, most of these methods
require hours of training, making them difficult for practical use. To address
this challenging problem, we propose IntrinsicNGP, which can train from scratch
and achieve high-fidelity results in few minutes with videos of a human
performer. To achieve this target, we introduce a continuous and optimizable
intrinsic coordinate rather than the original explicit Euclidean coordinate in
the hash encoding module of instant-NGP. With this novel intrinsic coordinate,
IntrinsicNGP can aggregate inter-frame information for dynamic objects with the
help of proxy geometry shapes. Moreover, the results trained with the given
rough geometry shapes can be further refined with an optimizable offset field
based on the intrinsic coordinate.Extensive experimental results on several
datasets demonstrate the effectiveness and efficiency of IntrinsicNGP. We also
illustrate our approach's ability to edit the shape of reconstructed subjects.";Bo Peng<author:sep>Jun Hu<author:sep>Jingtao Zhou<author:sep>Xuan Gao<author:sep>Juyong Zhang;http://arxiv.org/pdf/2302.14683v2;cs.CV;"Project page:https://ustc3dv.github.io/IntrinsicNGP/. arXiv admin
  note: substantial text overlap with arXiv:2210.01651";nerf
2302.13543v3;http://arxiv.org/abs/2302.13543v3;2023-02-27;BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling;"Reasoning the 3D structure of a non-rigid dynamic scene from a single moving
camera is an under-constrained problem. Inspired by the remarkable progress of
neural radiance fields (NeRFs) in photo-realistic novel view synthesis of
static scenes, extensions have been proposed for dynamic settings. These
methods heavily rely on neural priors in order to regularize the problem. In
this work, we take a step back and reinvestigate how current implementations
may entail deleterious effects, including limited expressiveness, entanglement
of light and density fields, and sub-optimal motion localization. As a remedy,
we advocate for a bridge between classic non-rigid-structure-from-motion
(\nrsfm) and NeRF, enabling the well-studied priors of the former to constrain
the latter. To this end, we propose a framework that factorizes time and space
by formulating a scene as a composition of bandlimited, high-dimensional
signals. We demonstrate compelling results across complex dynamic scenes that
involve changes in lighting, texture and long-range dynamics.";Sameera Ramasinghe<author:sep>Violetta Shevchenko<author:sep>Gil Avraham<author:sep>Anton Van Den Hengel;http://arxiv.org/pdf/2302.13543v3;cs.CV;;nerf
2302.13397v1;http://arxiv.org/abs/2302.13397v1;2023-02-26;Efficient physics-informed neural networks using hash encoding;"Physics-informed neural networks (PINNs) have attracted a lot of attention in
scientific computing as their functional representation of partial differential
equation (PDE) solutions offers flexibility and accuracy features. However,
their training cost has limited their practical use as a real alternative to
classic numerical methods. Thus, we propose to incorporate multi-resolution
hash encoding into PINNs to improve the training efficiency, as such encoding
offers a locally-aware (at multi resolution) coordinate inputs to the neural
network. Borrowed from the neural representation field community (NeRF), we
investigate the robustness of calculating the derivatives of such hash encoded
neural networks with respect to the input coordinates, which is often needed by
the PINN loss terms. We propose to replace the automatic differentiation with
finite-difference calculations of the derivatives to address the discontinuous
nature of such derivatives. We also share the appropriate ranges for the hash
encoding hyperparameters to obtain robust derivatives. We test the proposed
method on three problems, including Burgers equation, Helmholtz equation, and
Navier-Stokes equation. The proposed method admits about a 10-fold improvement
in efficiency over the vanilla PINN implementation.";Xinquan Huang<author:sep>Tariq Alkhalifah;http://arxiv.org/pdf/2302.13397v1;cs.LG;;nerf
2302.12931v2;http://arxiv.org/abs/2302.12931v2;2023-02-24;CATNIPS: Collision Avoidance Through Neural Implicit Probabilistic  Scenes;"We introduce a transformation of a Neural Radiance Field (NeRF) to an
equivalent Poisson Point Process (PPP). This PPP transformation allows for
rigorous quantification of uncertainty in NeRFs, in particular, for computing
collision probabilities for a robot navigating through a NeRF environment. The
PPP is a generalization of a probabilistic occupancy grid to the continuous
volume and is fundamental to the volumetric ray-tracing model underlying
radiance fields. Building upon this PPP representation, we present a
chance-constrained trajectory optimization method for safe robot navigation in
NeRFs. Our method relies on a voxel representation called the Probabilistic
Unsafe Robot Region (PURR) that spatially fuses the chance constraint with the
NeRF model to facilitate fast trajectory optimization. We then combine a
graph-based search with a spline-based trajectory optimization to yield robot
trajectories through the NeRF that are guaranteed to satisfy a user-specific
collision probability. We validate our chance constrained planning method
through simulations and hardware experiments, showing superior performance
compared to prior works on trajectory planning in NeRF environments.";Timothy Chen<author:sep>Preston Culbertson<author:sep>Mac Schwager;http://arxiv.org/pdf/2302.12931v2;cs.RO;Under Review in IEEE Transactions on Robotics;nerf
2302.12249v1;http://arxiv.org/abs/2302.12249v1;2023-02-23;MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in  Unbounded Scenes;"Neural radiance fields enable state-of-the-art photorealistic view synthesis.
However, existing radiance field representations are either too
compute-intensive for real-time rendering or require too much memory to scale
to large scenes. We present a Memory-Efficient Radiance Field (MERF)
representation that achieves real-time rendering of large-scale scenes in a
browser. MERF reduces the memory consumption of prior sparse volumetric
radiance fields using a combination of a sparse feature grid and
high-resolution 2D feature planes. To support large-scale unbounded scenes, we
introduce a novel contraction function that maps scene coordinates into a
bounded volume while still allowing for efficient ray-box intersection. We
design a lossless procedure for baking the parameterization used during
training into a model that achieves real-time rendering while still preserving
the photorealistic view synthesis quality of a volumetric radiance field.";Christian Reiser<author:sep>Richard Szeliski<author:sep>Dor Verbin<author:sep>Pratul P. Srinivasan<author:sep>Ben Mildenhall<author:sep>Andreas Geiger<author:sep>Jonathan T. Barron<author:sep>Peter Hedman;http://arxiv.org/pdf/2302.12249v1;cs.CV;Video and interactive web demo available at https://merf42.github.io;
2302.12231v3;http://arxiv.org/abs/2302.12231v3;2023-02-23;DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising  Diffusion Models;"Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive
results on novel view synthesis tasks. NeRFs learn a scene's color and density
fields by minimizing the photometric discrepancy between training views and
differentiable renderings of the scene. Once trained from a sufficient set of
views, NeRFs can generate novel views from arbitrary camera positions. However,
the scene geometry and color fields are severely under-constrained, which can
lead to artifacts, especially when trained with few input views.
  To alleviate this problem we learn a prior over scene geometry and color,
using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of
the synthetic Hypersim dataset and can be used to predict the gradient of the
logarithm of a joint probability distribution of color and depth patches. We
show that, these gradients of logarithms of RGBD patch priors serve to
regularize geometry and color of a scene. During NeRF training, random RGBD
patches are rendered and the estimated gradient of the log-likelihood is
backpropagated to the color and density fields. Evaluations on LLFF, the most
relevant dataset, show that our learned prior achieves improved quality in the
reconstructed geometry and improved generalization to novel views. Evaluations
on DTU show improved reconstruction quality among NeRF methods.";Jamie Wynn<author:sep>Daniyar Turmukhambetov;http://arxiv.org/pdf/2302.12231v3;cs.CV;CVPR 2023. Updated LPIPS scores in Table 1;nerf
2302.12237v2;http://arxiv.org/abs/2302.12237v2;2023-02-23;Learning Neural Volumetric Representations of Dynamic Humans in Minutes;"This paper addresses the challenge of quickly reconstructing free-viewpoint
videos of dynamic humans from sparse multi-view videos. Some recent works
represent the dynamic human as a canonical neural radiance field (NeRF) and a
motion field, which are learned from videos through differentiable rendering.
But the per-scene optimization generally requires hours. Other generalizable
NeRF models leverage learned prior from datasets and reduce the optimization
time by only finetuning on new scenes at the cost of visual fidelity. In this
paper, we propose a novel method for learning neural volumetric videos of
dynamic humans from sparse view videos in minutes with competitive visual
quality. Specifically, we define a novel part-based voxelized human
representation to better distribute the representational power of the network
to different human parts. Furthermore, we propose a novel 2D motion
parameterization scheme to increase the convergence rate of deformation field
learning. Experiments demonstrate that our model can be learned 100 times
faster than prior per-scene optimization methods while being competitive in the
rendering quality. Training our model on a $512 \times 512$ video with 100
frames typically takes about 5 minutes on a single RTX 3090 GPU. The code will
be released on our project page: https://zju3dv.github.io/instant_nvr";Chen Geng<author:sep>Sida Peng<author:sep>Zhen Xu<author:sep>Hujun Bao<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2302.12237v2;cs.CV;Project page: https://zju3dv.github.io/instant_nvr;nerf
2302.10518v3;http://arxiv.org/abs/2302.10518v3;2023-02-21;USR: Unsupervised Separated 3D Garment and Human Reconstruction via  Geometry and Semantic Consistency;"Dressed people reconstruction from images is a popular task with promising
applications in the creative media and game industry. However, most existing
methods reconstruct the human body and garments as a whole with the supervision
of 3D models, which hinders the downstream interaction tasks and requires
hard-to-obtain data. To address these issues, we propose an unsupervised
separated 3D garments and human reconstruction model (USR), which reconstructs
the human body and authentic textured clothes in layers without 3D models. More
specifically, our method proposes a generalized surface-aware neural radiance
field to learn the mapping between sparse multi-view images and geometries of
the dressed people. Based on the full geometry, we introduce a Semantic and
Confidence Guided Separation strategy (SCGS) to detect, segment, and
reconstruct the clothes layer, leveraging the consistency between 2D semantic
and 3D geometry. Moreover, we propose a Geometry Fine-tune Module to smooth
edges. Extensive experiments on our dataset show that comparing with
state-of-the-art methods, USR achieves improvements on both geometry and
appearance reconstruction while supporting generalizing to unseen people in
real time. Besides, we also introduce SMPL-D model to show the benefit of the
separated modeling of clothes and the human body that allows swapping clothes
and virtual try-on.";Yue Shi<author:sep>Yuxuan Xiong<author:sep>Jingyi Chai<author:sep>Bingbing Ni<author:sep>Wenjun Zhang;http://arxiv.org/pdf/2302.10518v3;cs.CV;;
2302.10663v2;http://arxiv.org/abs/2302.10663v2;2023-02-21;RealFusion: 360Â° Reconstruction of Any Object from a Single Image;"We consider the problem of reconstructing a full 360{\deg} photographic model
of an object from a single image of it. We do so by fitting a neural radiance
field to the image, but find this problem to be severely ill-posed. We thus
take an off-the-self conditional image generator based on diffusion and
engineer a prompt that encourages it to ""dream up"" novel views of the object.
Using an approach inspired by DreamFields and DreamFusion, we fuse the given
input view, the conditional prior, and other regularizers in a final,
consistent reconstruction. We demonstrate state-of-the-art reconstruction
results on benchmark images when compared to prior methods for monocular 3D
reconstruction of objects. Qualitatively, our reconstructions provide a
faithful match of the input view and a plausible extrapolation of its
appearance and 3D shape, including to the side of the object not visible in the
image.";Luke Melas-Kyriazi<author:sep>Christian Rupprecht<author:sep>Iro Laina<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2302.10663v2;cs.CV;Project page: https://lukemelas.github.io/realfusion;
2302.10970v2;http://arxiv.org/abs/2302.10970v2;2023-02-21;Differentiable Rendering with Reparameterized Volume Sampling;"In view synthesis, a neural radiance field approximates underlying density
and radiance fields based on a sparse set of scene pictures. To generate a
pixel of a novel view, it marches a ray through the pixel and computes a
weighted sum of radiance emitted from a dense set of ray points. This rendering
algorithm is fully differentiable and facilitates gradient-based optimization
of the fields. However, in practice, only a tiny opaque portion of the ray
contributes most of the radiance to the sum. We propose a simple end-to-end
differentiable sampling algorithm based on inverse transform sampling. It
generates samples according to the probability distribution induced by the
density field and picks non-transparent points on the ray. We utilize the
algorithm in two ways. First, we propose a novel rendering approach based on
Monte Carlo estimates. This approach allows for evaluating and optimizing a
neural radiance field with just a few radiance field calls per ray. Second, we
use the sampling algorithm to modify the hierarchical scheme proposed in the
original NeRF work. We show that our modification improves reconstruction
quality of hierarchical models, at the same time simplifying the training
procedure by removing the need for auxiliary proposal network losses.";Nikita Morozov<author:sep>Denis Rakitin<author:sep>Oleg Desheulin<author:sep>Dmitry Vetrov<author:sep>Kirill Struminsky;http://arxiv.org/pdf/2302.10970v2;cs.CV;Preprint;nerf
2302.10109v1;http://arxiv.org/abs/2302.10109v1;2023-02-20;NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from  3D-aware Diffusion;"Novel view synthesis from a single image requires inferring occluded regions
of objects and scenes whilst simultaneously maintaining semantic and physical
consistency with the input. Existing approaches condition neural radiance
fields (NeRF) on local image features, projecting points to the input image
plane, and aggregating 2D features to perform volume rendering. However, under
severe occlusion, this projection fails to resolve uncertainty, resulting in
blurry renderings that lack details. In this work, we propose NerfDiff, which
addresses this issue by distilling the knowledge of a 3D-aware conditional
diffusion model (CDM) into NeRF through synthesizing and refining a set of
virtual views at test time. We further propose a novel NeRF-guided distillation
algorithm that simultaneously generates 3D consistent virtual views from the
CDM samples, and finetunes the NeRF based on the improved virtual views. Our
approach significantly outperforms existing NeRF-based and geometry-free
approaches on challenging datasets, including ShapeNet, ABO, and Clevr3D.";Jiatao Gu<author:sep>Alex Trevithick<author:sep>Kai-En Lin<author:sep>Josh Susskind<author:sep>Christian Theobalt<author:sep>Lingjie Liu<author:sep>Ravi Ramamoorthi;http://arxiv.org/pdf/2302.10109v1;cs.CV;Project page: https://jiataogu.me/nerfdiff/;nerf
2302.09486v1;http://arxiv.org/abs/2302.09486v1;2023-02-19;LC-NeRF: Local Controllable Face Generation in Neural Randiance Field;"3D face generation has achieved high visual quality and 3D consistency thanks
to the development of neural radiance fields (NeRF). Recently, to generate and
edit 3D faces with NeRF representation, some methods are proposed and achieve
good results in decoupling geometry and texture. The latent codes of these
generative models affect the whole face, and hence modifications to these codes
cause the entire face to change. However, users usually edit a local region
when editing faces and do not want other regions to be affected. Since changes
to the latent code affect global generation results, these methods do not allow
for fine-grained control of local facial regions. To improve local
controllability in NeRF-based face editing, we propose LC-NeRF, which is
composed of a Local Region Generators Module and a Spatial-Aware Fusion Module,
allowing for local geometry and texture control of local facial regions.
Qualitative and quantitative evaluations show that our method provides better
local editing than state-of-the-art face editing methods. Our method also
performs well in downstream tasks, such as text-driven facial image editing.";Wenyang Zhou<author:sep>Lu Yuan<author:sep>Shuyu Chen<author:sep>Lin Gao<author:sep>Shimin Hu;http://arxiv.org/pdf/2302.09486v1;cs.CV;;nerf
2302.09311v2;http://arxiv.org/abs/2302.09311v2;2023-02-18;Temporal Interpolation Is All You Need for Dynamic Neural Radiance  Fields;"Temporal interpolation often plays a crucial role to learn meaningful
representations in dynamic scenes. In this paper, we propose a novel method to
train spatiotemporal neural radiance fields of dynamic scenes based on temporal
interpolation of feature vectors. Two feature interpolation methods are
suggested depending on underlying representations, neural networks or grids. In
the neural representation, we extract features from space-time inputs via
multiple neural network modules and interpolate them based on time frames. The
proposed multi-level feature interpolation network effectively captures
features of both short-term and long-term time ranges. In the grid
representation, space-time features are learned via four-dimensional hash
grids, which remarkably reduces training time. The grid representation shows
more than 100 times faster training speed than the previous neural-net-based
methods while maintaining the rendering quality. Concatenating static and
dynamic features and adding a simple smoothness term further improve the
performance of our proposed models. Despite the simplicity of the model
architectures, our method achieved state-of-the-art performance both in
rendering quality for the neural representation and in training speed for the
grid representation.";Sungheon Park<author:sep>Minjung Son<author:sep>Seokhwan Jang<author:sep>Young Chun Ahn<author:sep>Ji-Yeon Kim<author:sep>Nahyup Kang;http://arxiv.org/pdf/2302.09311v2;cs.CV;"CVPR 2023. Project page:
  https://sungheonpark.github.io/tempinterpnerf";
2302.08788v2;http://arxiv.org/abs/2302.08788v2;2023-02-17;MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis  from Sparse Inputs;"Neural Radiance Field (NeRF) has broken new ground in the novel view
synthesis due to its simple concept and state-of-the-art quality. However, it
suffers from severe performance degradation unless trained with a dense set of
images with different camera poses, which hinders its practical applications.
Although previous methods addressing this problem achieved promising results,
they relied heavily on the additional training resources, which goes against
the philosophy of sparse-input novel-view synthesis pursuing the training
efficiency. In this work, we propose MixNeRF, an effective training strategy
for novel view synthesis from sparse inputs by modeling a ray with a mixture
density model. Our MixNeRF estimates the joint distribution of RGB colors along
the ray samples by modeling it with mixture of distributions. We also propose a
new task of ray depth estimation as a useful training objective, which is
highly correlated with 3D scene geometry. Moreover, we remodel the colors with
regenerated blending weights based on the estimated ray depth and further
improves the robustness for colors and viewpoints. Our MixNeRF outperforms
other state-of-the-art methods in various standard benchmarks with superior
efficiency of training and inference.";Seunghyeon Seo<author:sep>Donghoon Han<author:sep>Yeonjin Chang<author:sep>Nojun Kwak;http://arxiv.org/pdf/2302.08788v2;cs.CV;CVPR 2023. Project Page: https://shawn615.github.io/mixnerf/;nerf
2302.08509v2;http://arxiv.org/abs/2302.08509v2;2023-02-16;3D-aware Conditional Image Synthesis;"We propose pix2pix3D, a 3D-aware conditional generative model for
controllable photorealistic image synthesis. Given a 2D label map, such as a
segmentation or edge map, our model learns to synthesize a corresponding image
from different viewpoints. To enable explicit 3D user control, we extend
conditional generative models with neural radiance fields. Given
widely-available monocular images and label map pairs, our model learns to
assign a label to every 3D point in addition to color and density, which
enables it to render the image and pixel-aligned label map simultaneously.
Finally, we build an interactive system that allows users to edit the label map
from any viewpoint and generate outputs accordingly.";Kangle Deng<author:sep>Gengshan Yang<author:sep>Deva Ramanan<author:sep>Jun-Yan Zhu;http://arxiv.org/pdf/2302.08509v2;cs.CV;Project Page: https://www.cs.cmu.edu/~pix2pix3D/;
2302.07672v3;http://arxiv.org/abs/2302.07672v3;2023-02-15;LiveHand: Real-time and Photorealistic Neural Hand Rendering;"The human hand is the main medium through which we interact with our
surroundings, making its digitization an important problem. While there are
several works modeling the geometry of hands, little attention has been paid to
capturing photo-realistic appearance. Moreover, for applications in extended
reality and gaming, real-time rendering is critical. We present the first
neural-implicit approach to photo-realistically render hands in real-time. This
is a challenging problem as hands are textured and undergo strong articulations
with pose-dependent effects. However, we show that this aim is achievable
through our carefully designed method. This includes training on a
low-resolution rendering of a neural radiance field, together with a
3D-consistent super-resolution module and mesh-guided sampling and space
canonicalization. We demonstrate a novel application of perceptual loss on the
image space, which is critical for learning details accurately. We also show a
live demo where we photo-realistically render the human hand in real-time for
the first time, while also modeling pose- and view-dependent appearance
effects. We ablate all our design choices and show that they optimize for
rendering speed and quality. Video results and our code can be accessed from
https://vcai.mpi-inf.mpg.de/projects/LiveHand/";Akshay Mundra<author:sep>Mallikarjun B R<author:sep>Jiayi Wang<author:sep>Marc Habermann<author:sep>Christian Theobalt<author:sep>Mohamed Elgharib;http://arxiv.org/pdf/2302.07672v3;cs.GR;"Project page: https://vcai.mpi-inf.mpg.de/projects/LiveHand/ |
  Accepted at ICCV '23 | 11 pages, 7 figures";
2302.06833v1;http://arxiv.org/abs/2302.06833v1;2023-02-14;VQ3D: Learning a 3D-Aware Generative Model on ImageNet;"Recent work has shown the possibility of training generative models of 3D
content from 2D image collections on small datasets corresponding to a single
object class, such as human faces, animal faces, or cars. However, these models
struggle on larger, more complex datasets. To model diverse and unconstrained
image collections such as ImageNet, we present VQ3D, which introduces a
NeRF-based decoder into a two-stage vector-quantized autoencoder. Our Stage 1
allows for the reconstruction of an input image and the ability to change the
camera position around the image, and our Stage 2 allows for the generation of
new 3D scenes. VQ3D is capable of generating and reconstructing 3D-aware images
from the 1000-class ImageNet dataset of 1.2 million training images. We achieve
an ImageNet generation FID score of 16.8, compared to 69.8 for the next best
baseline method.";Kyle Sargent<author:sep>Jing Yu Koh<author:sep>Han Zhang<author:sep>Huiwen Chang<author:sep>Charles Herrmann<author:sep>Pratul Srinivasan<author:sep>Jiajun Wu<author:sep>Deqing Sun;http://arxiv.org/pdf/2302.06833v1;cs.CV;"15 pages. For visual results, please visit the project webpage at
  http://kylesargent.github.io/vq3d";nerf
2302.06608v3;http://arxiv.org/abs/2302.06608v3;2023-02-13;3D-aware Blending with Generative NeRFs;"Image blending aims to combine multiple images seamlessly. It remains
challenging for existing 2D-based methods, especially when input images are
misaligned due to differences in 3D camera poses and object shapes. To tackle
these issues, we propose a 3D-aware blending method using generative Neural
Radiance Fields (NeRF), including two key components: 3D-aware alignment and
3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of
the reference image with respect to generative NeRFs and then perform 3D local
alignment for each part. To further leverage 3D information of the generative
NeRF, we propose 3D-aware blending that directly blends images on the NeRF's
latent representation space, rather than raw pixel space. Collectively, our
method outperforms existing 2D baselines, as validated by extensive
quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.";Hyunsu Kim<author:sep>Gayoung Lee<author:sep>Yunjey Choi<author:sep>Jin-Hwa Kim<author:sep>Jun-Yan Zhu;http://arxiv.org/pdf/2302.06608v3;cs.CV;ICCV 2023, Project page: https://blandocs.github.io/blendnerf;nerf
2302.05573v1;http://arxiv.org/abs/2302.05573v1;2023-02-11;3D Colored Shape Reconstruction from a Single RGB Image through  Diffusion;"We propose a novel 3d colored shape reconstruction method from a single RGB
image through diffusion model. Diffusion models have shown great development
potentials for high-quality 3D shape generation. However, most existing work
based on diffusion models only focus on geometric shape generation, they cannot
either accomplish 3D reconstruction from a single image, or produce 3D
geometric shape with color information. In this work, we propose to reconstruct
a 3D colored shape from a single RGB image through a novel conditional
diffusion model. The reverse process of the proposed diffusion model is
consisted of three modules, shape prediction module, color prediction module
and NeRF-like rendering module. In shape prediction module, the reference RGB
image is first encoded into a high-level shape feature and then the shape
feature is utilized as a condition to predict the reverse geometric noise in
diffusion model. Then the color of each 3D point updated in shape prediction
module is predicted by color prediction module. Finally, a NeRF-like rendering
module is designed to render the colored point cloud predicted by the former
two modules to 2D image space to guide the training conditioned only on a
reference image. As far as the authors know, the proposed method is the first
diffusion model for 3D colored shape reconstruction from a single RGB image.
Experimental results demonstrate that the proposed method achieves competitive
performance on colored 3D shape reconstruction, and the ablation study
validates the positive role of the color prediction module in improving the
reconstruction quality of 3D geometric point cloud.";Bo Li<author:sep>Xiaolin Wei<author:sep>Fengwei Chen<author:sep>Bin Liu;http://arxiv.org/pdf/2302.05573v1;cs.CV;9 pages, 8 figures;nerf
2302.04871v3;http://arxiv.org/abs/2302.04871v3;2023-02-09;In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for  Face Editing;"3D-aware GANs offer new capabilities for view synthesis while preserving the
editing functionalities of their 2D counterparts. GAN inversion is a crucial
step that seeks the latent code to reconstruct input images or videos,
subsequently enabling diverse editing tasks through manipulation of this latent
code. However, a model pre-trained on a particular dataset (e.g., FFHQ) often
has difficulty reconstructing images with out-of-distribution (OOD) objects
such as faces with heavy make-up or occluding objects. We address this issue by
explicitly modeling OOD objects from the input in 3D-aware GANs. Our core idea
is to represent the image using two individual neural radiance fields: one for
the in-distribution content and the other for the out-of-distribution object.
The final reconstruction is achieved by optimizing the composition of these two
radiance fields with carefully designed regularization. We demonstrate that our
explicit decomposition alleviates the inherent trade-off between reconstruction
fidelity and editability. We evaluate reconstruction accuracy and editability
of our method on challenging real face images and videos and showcase favorable
results against other baselines.";Yiran Xu<author:sep>Zhixin Shu<author:sep>Cameron Smith<author:sep>Seoung Wug Oh<author:sep>Jia-Bin Huang;http://arxiv.org/pdf/2302.04871v3;cs.CV;Project page: https://in-n-out-3d.github.io/;
2302.04264v4;http://arxiv.org/abs/2302.04264v4;2023-02-08;Nerfstudio: A Modular Framework for Neural Radiance Field Development;"Neural Radiance Fields (NeRF) are a rapidly growing area of research with
wide-ranging applications in computer vision, graphics, robotics, and more. In
order to streamline the development and deployment of NeRF research, we propose
a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play
components for implementing NeRF-based methods, which make it easy for
researchers and practitioners to incorporate NeRF into their projects.
Additionally, the modular design enables support for extensive real-time
visualization tools, streamlined pipelines for importing captured in-the-wild
data, and tools for exporting to video, point cloud and mesh representations.
The modularity of Nerfstudio enables the development of Nerfacto, our method
that combines components from recent papers to achieve a balance between speed
and quality, while also remaining flexible to future modifications. To promote
community-driven development, all associated code and data are made publicly
available with open-source licensing at https://nerf.studio.";Matthew Tancik<author:sep>Ethan Weber<author:sep>Evonne Ng<author:sep>Ruilong Li<author:sep>Brent Yi<author:sep>Justin Kerr<author:sep>Terrance Wang<author:sep>Alexander Kristoffersen<author:sep>Jake Austin<author:sep>Kamyar Salahi<author:sep>Abhik Ahuja<author:sep>David McAllister<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2302.04264v4;cs.CV;Project page at https://nerf.studio;nerf
2302.02088v3;http://arxiv.org/abs/2302.02088v3;2023-02-04;AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene  Synthesis;"Can machines recording an audio-visual scene produce realistic, matching
audio-visual experiences at novel positions and novel view directions? We
answer it by studying a new task -- real-world audio-visual scene synthesis --
and a first-of-its-kind NeRF-based approach for multimodal learning.
Concretely, given a video recording of an audio-visual scene, the task is to
synthesize new videos with spatial audios along arbitrary novel camera
trajectories in that scene. We propose an acoustic-aware audio generation
module that integrates prior knowledge of audio propagation into NeRF, in which
we implicitly associate audio generation with the 3D geometry and material
properties of a visual environment. Furthermore, we present a coordinate
transformation module that expresses a view direction relative to the sound
source, enabling the model to learn sound source-centric acoustic fields. To
facilitate the study of this new task, we collect a high-quality Real-World
Audio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our method
on this real-world dataset and the simulation-based SoundSpaces dataset.";Susan Liang<author:sep>Chao Huang<author:sep>Yapeng Tian<author:sep>Anurag Kumar<author:sep>Chenliang Xu;http://arxiv.org/pdf/2302.02088v3;cs.CV;NeurIPS 2023;nerf
2302.01571v1;http://arxiv.org/abs/2302.01571v1;2023-02-03;Robust Camera Pose Refinement for Multi-Resolution Hash Encoding;"Multi-resolution hash encoding has recently been proposed to reduce the
computational cost of neural renderings, such as NeRF. This method requires
accurate camera poses for the neural renderings of given scenes. However,
contrary to previous methods jointly optimizing camera poses and 3D scenes, the
naive gradient-based camera pose refinement method using multi-resolution hash
encoding severely deteriorates performance. We propose a joint optimization
algorithm to calibrate the camera pose and learn a geometric representation
using efficient multi-resolution hash encoding. Showing that the oscillating
gradient flows of hash encoding interfere with the registration of camera
poses, our method addresses the issue by utilizing smooth interpolation
weighting to stabilize the gradient oscillation for the ray samplings across
hash grids. Moreover, the curriculum training procedure helps to learn the
level-wise hash encoding, further increasing the pose refinement. Experiments
on the novel-view synthesis datasets validate that our learning frameworks
achieve state-of-the-art performance and rapid convergence of neural rendering,
even when initial camera poses are unknown.";Hwan Heo<author:sep>Taekyung Kim<author:sep>Jiyoung Lee<author:sep>Jaewon Lee<author:sep>Soohyun Kim<author:sep>Hyunwoo J. Kim<author:sep>Jin-Hwa Kim;http://arxiv.org/pdf/2302.01571v1;cs.CV;;nerf
2302.01579v2;http://arxiv.org/abs/2302.01579v2;2023-02-03;Semantic 3D-aware Portrait Synthesis and Manipulation Based on  Compositional Neural Radiance Field;"Recently 3D-aware GAN methods with neural radiance field have developed
rapidly. However, current methods model the whole image as an overall neural
radiance field, which limits the partial semantic editability of synthetic
results. Since NeRF renders an image pixel by pixel, it is possible to split
NeRF in the spatial dimension. We propose a Compositional Neural Radiance Field
(CNeRF) for semantic 3D-aware portrait synthesis and manipulation. CNeRF
divides the image by semantic regions and learns an independent neural radiance
field for each region, and finally fuses them and renders the complete image.
Thus we can manipulate the synthesized semantic regions independently, while
fixing the other parts unchanged. Furthermore, CNeRF is also designed to
decouple shape and texture within each semantic region. Compared to
state-of-the-art 3D-aware GAN methods, our approach enables fine-grained
semantic region manipulation, while maintaining high-quality 3D-consistent
synthesis. The ablation studies show the effectiveness of the structure and
loss function used by our method. In addition real image inversion and cartoon
portrait 3D editing experiments demonstrate the application potential of our
method.";Tianxiang Ma<author:sep>Bingchuan Li<author:sep>Qian He<author:sep>Jing Dong<author:sep>Tieniu Tan;http://arxiv.org/pdf/2302.01579v2;cs.CV;Accepted by AAAI2023 Oral;nerf
2302.01532v1;http://arxiv.org/abs/2302.01532v1;2023-02-03;INV: Towards Streaming Incremental Neural Videos;"Recent works in spatiotemporal radiance fields can produce photorealistic
free-viewpoint videos. However, they are inherently unsuitable for interactive
streaming scenarios (e.g. video conferencing, telepresence) because have an
inevitable lag even if the training is instantaneous. This is because these
approaches consume videos and thus have to buffer chunks of frames (often
seconds) before processing. In this work, we take a step towards interactive
streaming via a frame-by-frame approach naturally free of lag. Conventional
wisdom believes that per-frame NeRFs are impractical due to prohibitive
training costs and storage. We break this belief by introducing Incremental
Neural Videos (INV), a per-frame NeRF that is efficiently trained and
streamable. We designed INV based on two insights: (1) Our main finding is that
MLPs naturally partition themselves into Structure and Color Layers, which
store structural and color/texture information respectively. (2) We leverage
this property to retain and improve upon knowledge from previous frames, thus
amortizing training across frames and reducing redundant learning. As a result,
with negligible changes to NeRF, INV can achieve good qualities (>28.6db) in
8min/frame. It can also outperform prior SOTA in 19% less training time.
Additionally, our Temporal Weight Compression reduces the per-frame size to
0.3MB/frame (6.6% of NeRF). More importantly, INV is free from buffer lag and
is naturally fit for streaming. While this work does not achieve real-time
training, it shows that incremental approaches like INV present new
possibilities in interactive 3D streaming. Moreover, our discovery of natural
information partition leads to a better understanding and manipulation of MLPs.
Code and dataset will be released soon.";Shengze Wang<author:sep>Alexey Supikov<author:sep>Joshua Ratcliff<author:sep>Henry Fuchs<author:sep>Ronald Azuma;http://arxiv.org/pdf/2302.01532v1;cs.CV;;nerf
2302.00833v1;http://arxiv.org/abs/2302.00833v1;2023-02-02;RobustNeRF: Ignoring Distractors with Robust Losses;"Neural radiance fields (NeRF) excel at synthesizing new views given
multi-view, calibrated images of a static scene. When scenes include
distractors, which are not persistent during image capture (moving objects,
lighting variations, shadows), artifacts appear as view-dependent effects or
'floaters'. To cope with distractors, we advocate a form of robust estimation
for NeRF training, modeling distractors in training data as outliers of an
optimization problem. Our method successfully removes outliers from a scene and
improves upon our baselines, on synthetic and real-world scenes. Our technique
is simple to incorporate in modern NeRF frameworks, with few hyper-parameters.
It does not assume a priori knowledge of the types of distractors, and is
instead focused on the optimization problem rather than pre-processing or
modeling transient objects. More results on our page
https://robustnerf.github.io/public.";Sara Sabour<author:sep>Suhani Vora<author:sep>Daniel Duckworth<author:sep>Ivan Krasin<author:sep>David J. Fleet<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2302.00833v1;cs.CV;;nerf
2302.01226v3;http://arxiv.org/abs/2302.01226v3;2023-02-02;Factor Fields: A Unified Framework for Neural Fields and Beyond;"We present Factor Fields, a novel framework for modeling and representing
signals. Factor Fields decomposes a signal into a product of factors, each
represented by a classical or neural field representation which operates on
transformed input coordinates. This decomposition results in a unified
framework that accommodates several recent signal representations including
NeRF, Plenoxels, EG3D, Instant-NGP, and TensoRF. Additionally, our framework
allows for the creation of powerful new signal representations, such as the
""Dictionary Field"" (DiF) which is a second contribution of this paper. Our
experiments show that DiF leads to improvements in approximation quality,
compactness, and training time when compared to previous fast reconstruction
methods. Experimentally, our representation achieves better image approximation
quality on 2D image regression tasks, higher geometric quality when
reconstructing 3D signed distance fields, and higher compactness for radiance
field reconstruction tasks. Furthermore, DiF enables generalization to unseen
images/3D scenes by sharing bases across signals during training which greatly
benefits use cases such as image regression from sparse observations and
few-shot radiance field reconstruction.";Anpei Chen<author:sep>Zexiang Xu<author:sep>Xinyue Wei<author:sep>Siyu Tang<author:sep>Hao Su<author:sep>Andreas Geiger;http://arxiv.org/pdf/2302.01226v3;cs.CV;"13 pages, 7 figures; Project Page:
  https://apchenstu.github.io/FactorFields/";nerf
2301.13430v1;http://arxiv.org/abs/2301.13430v1;2023-01-31;GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face  Synthesis;"Generating photo-realistic video portrait with arbitrary speech audio is a
crucial problem in film-making and virtual reality. Recently, several works
explore the usage of neural radiance field in this task to improve 3D realness
and image fidelity. However, the generalizability of previous NeRF-based
methods to out-of-domain audio is limited by the small scale of training data.
In this work, we propose GeneFace, a generalized and high-fidelity NeRF-based
talking face generation method, which can generate natural results
corresponding to various out-of-domain audio. Specifically, we learn a
variaitional motion generator on a large lip-reading corpus, and introduce a
domain adaptative post-net to calibrate the result. Moreover, we learn a
NeRF-based renderer conditioned on the predicted facial motion. A head-aware
torso-NeRF is proposed to eliminate the head-torso separation problem.
Extensive experiments show that our method achieves more generalized and
high-fidelity talking face generation compared to previous methods.";Zhenhui Ye<author:sep>Ziyue Jiang<author:sep>Yi Ren<author:sep>Jinglin Liu<author:sep>JinZheng He<author:sep>Zhou Zhao;http://arxiv.org/pdf/2301.13430v1;cs.CV;Accepted by ICLR2023. Project page: https://geneface.github.io/;nerf
2301.12780v2;http://arxiv.org/abs/2301.12780v2;2023-01-30;Equivariant Architectures for Learning in Deep Weight Spaces;"Designing machine learning architectures for processing neural networks in
their raw weight matrix form is a newly introduced research direction.
Unfortunately, the unique symmetry structure of deep weight spaces makes this
design very challenging. If successful, such architectures would be capable of
performing a wide range of intriguing tasks, from adapting a pre-trained
network to a new domain to editing objects represented as functions (INRs or
NeRFs). As a first step towards this goal, we present here a novel network
architecture for learning in deep weight spaces. It takes as input a
concatenation of weights and biases of a pre-trained MLP and processes it using
a composition of layers that are equivariant to the natural permutation
symmetry of the MLP's weights: Changing the order of neurons in intermediate
layers of the MLP does not affect the function it represents. We provide a full
characterization of all affine equivariant and invariant layers for these
symmetries and show how these layers can be implemented using three basic
operations: pooling, broadcasting, and fully connected layers applied to the
input in an appropriate manner. We demonstrate the effectiveness of our
architecture and its advantages over natural baselines in a variety of learning
tasks.";Aviv Navon<author:sep>Aviv Shamsian<author:sep>Idan Achituve<author:sep>Ethan Fetaya<author:sep>Gal Chechik<author:sep>Haggai Maron;http://arxiv.org/pdf/2301.12780v2;cs.LG;ICML 2023;nerf
2301.11520v3;http://arxiv.org/abs/2301.11520v3;2023-01-27;SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning;"As previous representations for reinforcement learning cannot effectively
incorporate a human-intuitive understanding of the 3D environment, they usually
suffer from sub-optimal performances. In this paper, we present Semantic-aware
Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly
optimizes semantic-aware neural radiance fields (NeRF) with a convolutional
encoder to learn 3D-aware neural implicit representation from multi-view
images. We introduce 3D semantic and distilled feature fields in parallel to
the RGB radiance fields in NeRF to learn semantic and object-centric
representation for reinforcement learning. SNeRL outperforms not only previous
pixel-based representations but also recent 3D-aware representations both in
model-free and model-based reinforcement learning.";Dongseok Shim<author:sep>Seungjae Lee<author:sep>H. Jin Kim;http://arxiv.org/pdf/2301.11520v3;cs.LG;"ICML 2023. First two authors contributed equally. Order was
  determined by coin flip";nerf
2301.11631v1;http://arxiv.org/abs/2301.11631v1;2023-01-27;HyperNeRFGAN: Hypernetwork approach to 3D NeRF GAN;"Recently, generative models for 3D objects are gaining much popularity in VR
and augmented reality applications. Training such models using standard 3D
representations, like voxels or point clouds, is challenging and requires
complex tools for proper color rendering. In order to overcome this limitation,
Neural Radiance Fields (NeRFs) offer a state-of-the-art quality in synthesizing
novel views of complex 3D scenes from a small subset of 2D images.
  In the paper, we propose a generative model called HyperNeRFGAN, which uses
hypernetworks paradigm to produce 3D objects represented by NeRF. Our GAN
architecture leverages a hypernetwork paradigm to transfer gaussian noise into
weights of NeRF model. The model is further used to render 2D novel views, and
a classical 2D discriminator is utilized for training the entire GAN-based
structure. Our architecture produces 2D images, but we use 3D-aware NeRF
representation, which forces the model to produce correct 3D objects. The
advantage of the model over existing approaches is that it produces a dedicated
NeRF representation for the object without sharing some global parameters of
the rendering component. We show the superiority of our approach compared to
reference baselines on three challenging datasets from various domains.";Adam Kania<author:sep>Artur Kasymov<author:sep>Maciej ZiÄba<author:sep>PrzemysÅaw Spurek;http://arxiv.org/pdf/2301.11631v1;cs.CV;;nerf
2301.11522v1;http://arxiv.org/abs/2301.11522v1;2023-01-27;A Comparison of Tiny-nerf versus Spatial Representations for 3d  Reconstruction;"Neural rendering has emerged as a powerful paradigm for synthesizing images,
offering many benefits over classical rendering by using neural networks to
reconstruct surfaces, represent shapes, and synthesize novel views, either for
objects or scenes. In this neural rendering, the environment is encoded into a
neural network. We believe that these new representations can be used to codify
the scene for a mobile robot. Therefore, in this work, we perform a comparison
between a trending neural rendering, called tiny-NeRF, and other volume
representations that are commonly used as maps in robotics, such as voxel maps,
point clouds, and triangular meshes. The target is to know the advantages and
disadvantages of neural representations in the robotics context. The comparison
is made in terms of spatial complexity and processing time to obtain a model.
Experiments show that tiny-NeRF requires three times less memory space compared
to other representations. In terms of processing time, tiny-NeRF takes about
six times more to compute the model.";Saulo Abraham Gante<author:sep>Juan Irving Vasquez<author:sep>Marco Antonio Valencia<author:sep>Mauricio OlguÃ­n Carbajal;http://arxiv.org/pdf/2301.11522v1;cs.AI;;nerf
2301.10941v3;http://arxiv.org/abs/2301.10941v3;2023-01-26;GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency;"We present a novel framework to regularize Neural Radiance Field (NeRF) in a
few-shot setting with a geometry-aware consistency regularization. The proposed
approach leverages a rendered depth map at unobserved viewpoint to warp sparse
input images to the unobserved viewpoint and impose them as pseudo ground
truths to facilitate learning of NeRF. By encouraging such geometry-aware
consistency at a feature-level instead of using pixel-level reconstruction
loss, we regularize the NeRF at semantic and structural levels while allowing
for modeling view dependent radiance to account for color variations across
viewpoints. We also propose an effective method to filter out erroneous warped
solutions, along with training strategies to stabilize training during
optimization. We show that our model achieves competitive results compared to
state-of-the-art few-shot NeRF models. Project page is available at
https://ku-cvlab.github.io/GeCoNeRF/.";Min-seop Kwak<author:sep>Jiuhn Song<author:sep>Seungryong Kim;http://arxiv.org/pdf/2301.10941v3;cs.CV;ICML 2023;nerf
2301.11280v1;http://arxiv.org/abs/2301.11280v1;2023-01-26;Text-To-4D Dynamic Scene Generation;"We present MAV3D (Make-A-Video3D), a method for generating three-dimensional
dynamic scenes from text descriptions. Our approach uses a 4D dynamic Neural
Radiance Field (NeRF), which is optimized for scene appearance, density, and
motion consistency by querying a Text-to-Video (T2V) diffusion-based model. The
dynamic video output generated from the provided text can be viewed from any
camera location and angle, and can be composited into any 3D environment. MAV3D
does not require any 3D or 4D data and the T2V model is trained only on
Text-Image pairs and unlabeled videos. We demonstrate the effectiveness of our
approach using comprehensive quantitative and qualitative experiments and show
an improvement over previously established internal baselines. To the best of
our knowledge, our method is the first to generate 3D dynamic scenes given a
text description.";Uriel Singer<author:sep>Shelly Sheynin<author:sep>Adam Polyak<author:sep>Oron Ashual<author:sep>Iurii Makarov<author:sep>Filippos Kokkinos<author:sep>Naman Goyal<author:sep>Andrea Vedaldi<author:sep>Devi Parikh<author:sep>Justin Johnson<author:sep>Yaniv Taigman;http://arxiv.org/pdf/2301.11280v1;cs.CV;;nerf
2301.10520v2;http://arxiv.org/abs/2301.10520v2;2023-01-25;Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging;"We present a physics-enhanced implicit neural representation (INR) for
ultrasound (US) imaging that learns tissue properties from overlapping US
sweeps. Our proposed method leverages a ray-tracing-based neural rendering for
novel view US synthesis. Recent publications demonstrated that INR models could
encode a representation of a three-dimensional scene from a set of
two-dimensional US frames. However, these models fail to consider the
view-dependent changes in appearance and geometry intrinsic to US imaging. In
our work, we discuss direction-dependent changes in the scene and show that a
physics-inspired rendering improves the fidelity of US image synthesis. In
particular, we demonstrate experimentally that our proposed method generates
geometrically accurate B-mode images for regions with ambiguous representation
owing to view-dependent differences of the US images. We conduct our
experiments using simulated B-mode US sweeps of the liver and acquired US
sweeps of a spine phantom tracked with a robotic arm. The experiments
corroborate that our method generates US frames that enable consistent volume
compounding from previously unseen views. To the best of our knowledge, the
presented work is the first to address view-dependent US image synthesis using
INR.";Magdalena Wysocki<author:sep>Mohammad Farid Azampour<author:sep>Christine Eilers<author:sep>Benjamin Busam<author:sep>Mehrdad Salehi<author:sep>Nassir Navab;http://arxiv.org/pdf/2301.10520v2;eess.IV;"accepted for oral presentation at MIDL 2023
  (https://openreview.net/forum?id=x4McMBwVyi)";nerf
2301.09632v2;http://arxiv.org/abs/2301.09632v2;2023-01-23;HexPlane: A Fast Representation for Dynamic Scenes;"Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D
vision. Prior approaches build on NeRF and rely on implicit representations.
This is slow since it requires many MLP evaluations, constraining real-world
applications. We show that dynamic 3D scenes can be explicitly represented by
six planes of learned features, leading to an elegant solution we call
HexPlane. A HexPlane computes features for points in spacetime by fusing
vectors extracted from each plane, which is highly efficient. Pairing a
HexPlane with a tiny MLP to regress output colors and training via volume
rendering gives impressive results for novel view synthesis on dynamic scenes,
matching the image quality of prior work but reducing training time by more
than $100\times$. Extensive ablations confirm our HexPlane design and show that
it is robust to different feature fusion mechanisms, coordinate systems, and
decoding mechanisms. HexPlane is a simple and effective solution for
representing 4D volumes, and we hope they can broadly contribute to modeling
spacetime for dynamic 3D scenes.";Ang Cao<author:sep>Justin Johnson;http://arxiv.org/pdf/2301.09632v2;cs.CV;"CVPR 2023, Camera Ready Project page:
  https://caoang327.github.io/HexPlane";nerf
2301.09060v3;http://arxiv.org/abs/2301.09060v3;2023-01-22;3D Reconstruction of Non-cooperative Resident Space Objects using  Instant NGP-accelerated NeRF and D-NeRF;"The proliferation of non-cooperative resident space objects (RSOs) in orbit
has spurred the demand for active space debris removal, on-orbit servicing
(OOS), classification, and functionality identification of these RSOs. Recent
advances in computer vision have enabled high-definition 3D modeling of objects
based on a set of 2D images captured from different viewing angles. This work
adapts Instant NeRF and D-NeRF, variations of the neural radiance field (NeRF)
algorithm to the problem of mapping RSOs in orbit for the purposes of
functionality identification and assisting with OOS. The algorithms are
evaluated for 3D reconstruction quality and hardware requirements using
datasets of images of a spacecraft mock-up taken under two different lighting
and motion conditions at the Orbital Robotic Interaction, On-Orbit Servicing
and Navigation (ORION) Laboratory at Florida Institute of Technology. Instant
NeRF is shown to learn high-fidelity 3D models with a computational cost that
could feasibly be trained on on-board computers.";Basilio Caruso<author:sep>Trupti Mahendrakar<author:sep>Van Minh Nguyen<author:sep>Ryan T. White<author:sep>Todd Steffen;http://arxiv.org/pdf/2301.09060v3;cs.CV;"Presented at AAS/AIAA Spaceflight Mechanics Conference 2023, 14
  pages, 10 figures, 2 tables";nerf
2301.07958v3;http://arxiv.org/abs/2301.07958v3;2023-01-19;RecolorNeRF: Layer Decomposed Radiance Fields for Efficient Color  Editing of 3D Scenes;"Radiance fields have gradually become a main representation of media.
Although its appearance editing has been studied, how to achieve
view-consistent recoloring in an efficient manner is still under explored. We
present RecolorNeRF, a novel user-friendly color editing approach for the
neural radiance fields. Our key idea is to decompose the scene into a set of
pure-colored layers, forming a palette. By this means, color manipulation can
be conducted by altering the color components of the palette directly. To
support efficient palette-based editing, the color of each layer needs to be as
representative as possible. In the end, the problem is formulated as an
optimization problem, where the layers and their blending weights are jointly
optimized with the NeRF itself. Extensive experiments show that our
jointly-optimized layer decomposition can be used against multiple backbones
and produce photo-realistic recolored novel-view renderings. We demonstrate
that RecolorNeRF outperforms baseline methods both quantitatively and
qualitatively for color editing even in complex real-world scenes.";Bingchen Gong<author:sep>Yuehao Wang<author:sep>Xiaoguang Han<author:sep>Qi Dou;http://arxiv.org/pdf/2301.07958v3;cs.CV;"To appear in ACM Multimedia 2023. Project website is accessible at
  https://sites.google.com/view/recolornerf";nerf
2301.07668v3;http://arxiv.org/abs/2301.07668v3;2023-01-18;Behind the Scenes: Density Fields for Single View Reconstruction;"Inferring a meaningful geometric scene representation from a single image is
a fundamental problem in computer vision. Approaches based on traditional depth
map prediction can only reason about areas that are visible in the image.
Currently, neural radiance fields (NeRFs) can capture true 3D including color,
but are too complex to be generated from a single image. As an alternative, we
propose to predict implicit density fields. A density field maps every location
in the frustum of the input image to volumetric density. By directly sampling
color from the available views instead of storing color in the density field,
our scene representation becomes significantly less complex compared to NeRFs,
and a neural network can predict it in a single forward pass. The prediction
network is trained through self-supervision from only video data. Our
formulation allows volume rendering to perform both depth prediction and novel
view synthesis. Through experiments, we show that our method is able to predict
meaningful geometry for regions that are occluded in the input image.
Additionally, we demonstrate the potential of our approach on three datasets
for depth prediction and novel-view synthesis.";Felix Wimbauer<author:sep>Nan Yang<author:sep>Christian Rupprecht<author:sep>Daniel Cremers;http://arxiv.org/pdf/2301.07668v3;cs.CV;Project Page: https://fwmb.github.io/bts/;nerf
2301.08556v1;http://arxiv.org/abs/2301.08556v1;2023-01-18;NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via  Novel-View Synthesis;"Expert demonstrations are a rich source of supervision for training visual
robotic manipulation policies, but imitation learning methods often require
either a large number of demonstrations or expensive online expert supervision
to learn reactive closed-loop behaviors. In this work, we introduce SPARTN
(Synthetic Perturbations for Augmenting Robot Trajectories via NeRF): a
fully-offline data augmentation scheme for improving robot policies that use
eye-in-hand cameras. Our approach leverages neural radiance fields (NeRFs) to
synthetically inject corrective noise into visual demonstrations, using NeRFs
to generate perturbed viewpoints while simultaneously calculating the
corrective actions. This requires no additional expert supervision or
environment interaction, and distills the geometric information in NeRFs into a
real-time reactive RGB-only policy. In a simulated 6-DoF visual grasping
benchmark, SPARTN improves success rates by 2.8$\times$ over imitation learning
without the corrective augmentations and even outperforms some methods that use
online supervision. It additionally closes the gap between RGB-only and RGB-D
success rates, eliminating the previous need for depth sensors. In real-world
6-DoF robotic grasping experiments from limited human demonstrations, our
method improves absolute success rates by $22.5\%$ on average, including
objects that are traditionally challenging for depth-based methods. See video
results at \url{https://bland.website/spartn}.";Allan Zhou<author:sep>Moo Jin Kim<author:sep>Lirui Wang<author:sep>Pete Florence<author:sep>Chelsea Finn;http://arxiv.org/pdf/2301.08556v1;cs.LG;;nerf
2301.06782v1;http://arxiv.org/abs/2301.06782v1;2023-01-17;A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View  Synthesis and Implicit Scene Reconstruction;"Neural Radiance Fields (NeRF) has achieved impressive results in single
object scene reconstruction and novel view synthesis, which have been
demonstrated on many single modality and single object focused indoor scene
datasets like DTU, BMVS, and NeRF Synthetic.However, the study of NeRF on
large-scale outdoor scene reconstruction is still limited, as there is no
unified outdoor scene dataset for large-scale NeRF evaluation due to expensive
data acquisition and calibration costs. In this paper, we propose a large-scale
outdoor multi-modal dataset, OMMO dataset, containing complex land objects and
scenes with calibrated images, point clouds and prompt annotations. Meanwhile,
a new benchmark for several outdoor NeRF-based tasks is established, such as
novel view synthesis, surface reconstruction, and multi-modal NeRF. To create
the dataset, we capture and collect a large number of real fly-view videos and
select high-quality and high-resolution clips from them. Then we design a
quality review module to refine images, remove low-quality frames and
fail-to-calibrate scenes through a learning-based automatic evaluation plus
manual review. Finally, a number of volunteers are employed to add the text
descriptions for each scene and key-frame to meet the potential multi-modal
requirements in the future. Compared with existing NeRF datasets, our dataset
contains abundant real-world urban and natural scenes with various scales,
camera trajectories, and lighting conditions. Experiments show that our dataset
can benchmark most state-of-the-art NeRF methods on different tasks. We will
release the dataset and model weights very soon.";Chongshan Lu<author:sep>Fukun Yin<author:sep>Xin Chen<author:sep>Tao Chen<author:sep>Gang YU<author:sep>Jiayuan Fan;http://arxiv.org/pdf/2301.06782v1;cs.CV;;nerf
2301.05747v1;http://arxiv.org/abs/2301.05747v1;2023-01-13;Laser: Latent Set Representations for 3D Generative Modeling;"NeRF provides unparalleled fidelity of novel view synthesis: rendering a 3D
scene from an arbitrary viewpoint. NeRF requires training on a large number of
views that fully cover a scene, which limits its applicability. While these
issues can be addressed by learning a prior over scenes in various forms,
previous approaches have been either applied to overly simple scenes or
struggling to render unobserved parts. We introduce Laser-NV: a generative
model which achieves high modelling capacity, and which is based on a
set-valued latent representation modelled by normalizing flows. Similarly to
previous amortized approaches, Laser-NV learns structure from multiple scenes
and is capable of fast, feed-forward inference from few views. To encourage
higher rendering fidelity and consistency with observed views, Laser-NV further
incorporates a geometry-informed attention mechanism over the observed views.
Laser-NV further produces diverse and plausible completions of occluded parts
of a scene while remaining consistent with observations. Laser-NV shows
state-of-the-art novel-view synthesis quality when evaluated on ShapeNet and on
a novel simulated City dataset, which features high uncertainty in the
unobserved regions of the scene.";Pol Moreno<author:sep>Adam R. Kosiorek<author:sep>Heiko Strathmann<author:sep>Daniel Zoran<author:sep>Rosalia G. Schneider<author:sep>BjÃ¶rn Winckler<author:sep>Larisa Markeeva<author:sep>ThÃ©ophane Weber<author:sep>Danilo J. Rezende;http://arxiv.org/pdf/2301.05747v1;cs.CV;See https://laser-nv-paper.github.io/ for video results;nerf
2301.04075v1;http://arxiv.org/abs/2301.04075v1;2023-01-10;Benchmarking Robustness in Neural Radiance Fields;"Neural Radiance Field (NeRF) has demonstrated excellent quality in novel view
synthesis, thanks to its ability to model 3D object geometries in a concise
formulation. However, current approaches to NeRF-based models rely on clean
images with accurate camera calibration, which can be difficult to obtain in
the real world, where data is often subject to corruption and distortion. In
this work, we provide the first comprehensive analysis of the robustness of
NeRF-based novel view synthesis algorithms in the presence of different types
of corruptions.
  We find that NeRF-based models are significantly degraded in the presence of
corruption, and are more sensitive to a different set of corruptions than image
recognition models. Furthermore, we analyze the robustness of the feature
encoder in generalizable methods, which synthesize images using neural features
extracted via convolutional neural networks or transformers, and find that it
only contributes marginally to robustness. Finally, we reveal that standard
data augmentation techniques, which can significantly improve the robustness of
recognition models, do not help the robustness of NeRF-based models. We hope
that our findings will attract more researchers to study the robustness of
NeRF-based approaches and help to improve their performance in the real world.";Chen Wang<author:sep>Angtian Wang<author:sep>Junbo Li<author:sep>Alan Yuille<author:sep>Cihang Xie;http://arxiv.org/pdf/2301.04075v1;cs.CV;;nerf
2301.04101v2;http://arxiv.org/abs/2301.04101v2;2023-01-10;Neural Radiance Field Codebooks;"Compositional representations of the world are a promising step towards
enabling high-level scene understanding and efficient transfer to downstream
tasks. Learning such representations for complex scenes and tasks remains an
open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks
(NRC), a scalable method for learning object-centric representations through
novel view reconstruction. NRC learns to reconstruct scenes from novel views
using a dictionary of object codes which are decoded through a volumetric
renderer. This enables the discovery of reoccurring visual and geometric
patterns across scenes which are transferable to downstream tasks. We show that
NRC representations transfer well to object navigation in THOR, outperforming
2D and 3D representation learning methods by 3.1% success rate. We demonstrate
that our approach is able to perform unsupervised segmentation for more complex
synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29%
relative improvement). Finally, we show that NRC improves on the task of depth
ordering by 5.5% accuracy in THOR.";Matthew Wallingford<author:sep>Aditya Kusupati<author:sep>Alex Fang<author:sep>Vivek Ramanujan<author:sep>Aniruddha Kembhavi<author:sep>Roozbeh Mottaghi<author:sep>Ali Farhadi;http://arxiv.org/pdf/2301.04101v2;cs.CV;19 pages, 8 figures, 9 tables;
2301.02975v2;http://arxiv.org/abs/2301.02975v2;2023-01-08;Traditional Readability Formulas Compared for English;"Traditional English readability formulas, or equations, were largely
developed in the 20th century. Nonetheless, many researchers still rely on them
for various NLP applications. This phenomenon is presumably due to the
convenience and straightforwardness of readability formulas. In this work, we
contribute to the NLP community by 1. introducing New English Readability
Formula (NERF), 2. recalibrating the coefficients of old readability formulas
(Flesch-Kincaid Grade Level, Fog Index, SMOG Index, Coleman-Liau Index, and
Automated Readability Index), 3. evaluating the readability formulas, for use
in text simplification studies and medical texts, and 4. developing a
Python-based program for the wide application to various NLP projects.";Bruce W. Lee<author:sep>Jason Hyung-Jong Lee;http://arxiv.org/pdf/2301.02975v2;cs.CL;Submitted to EMNLP 2022;nerf
2301.03102v4;http://arxiv.org/abs/2301.03102v4;2023-01-08;Towards Open World NeRF-Based SLAM;"Neural Radiance Fields (NeRFs) offer versatility and robustness in map
representations for Simultaneous Localization and Mapping (SLAM) tasks. This
paper extends NICE-SLAM, a recent state-of-the-art NeRF-based SLAM algorithm
capable of producing high quality NeRF maps. However, depending on the hardware
used, the required number of iterations to produce these maps often makes
NICE-SLAM run at less than real time. Additionally, the estimated trajectories
fail to be competitive with classical SLAM approaches. Finally, NICE-SLAM
requires a grid covering the considered environment to be defined prior to
runtime, making it difficult to extend into previously unseen scenes. This
paper seeks to make NICE-SLAM more open-world-capable by improving the
robustness and tracking accuracy, and generalizing the map representation to
handle unconstrained environments. This is done by improving measurement
uncertainty handling, incorporating motion information, and modelling the map
as having an explicit foreground and background. It is shown that these changes
are able to improve tracking accuracy by 85% to 97% depending on the available
resources, while also improving mapping in environments with visual information
extending outside of the predefined grid.";Daniil Lisus<author:sep>Connor Holmes<author:sep>Steven Waslander;http://arxiv.org/pdf/2301.03102v4;cs.RO;"Presented at Conference on Robots and Vision (CRV) 2023. 8 pages, 2
  figures, 2 tables";nerf
2301.05187v1;http://arxiv.org/abs/2301.05187v1;2023-01-05;WIRE: Wavelet Implicit Neural Representations;"Implicit neural representations (INRs) have recently advanced numerous
vision-related areas. INR performance depends strongly on the choice of the
nonlinear activation function employed in its multilayer perceptron (MLP)
network. A wide range of nonlinearities have been explored, but, unfortunately,
current INRs designed to have high accuracy also suffer from poor robustness
(to signal noise, parameter variation, etc.). Inspired by harmonic analysis, we
develop a new, highly accurate and robust INR that does not exhibit this
tradeoff. Wavelet Implicit neural REpresentation (WIRE) uses a continuous
complex Gabor wavelet activation function that is well-known to be optimally
concentrated in space-frequency and to have excellent biases for representing
images. A wide range of experiments (image denoising, image inpainting,
super-resolution, computed tomography reconstruction, image overfitting, and
novel view synthesis with neural radiance fields) demonstrate that WIRE defines
the new state of the art in INR accuracy, training time, and robustness.";Vishwanath Saragadam<author:sep>Daniel LeJeune<author:sep>Jasper Tan<author:sep>Guha Balakrishnan<author:sep>Ashok Veeraraghavan<author:sep>Richard G. Baraniuk;http://arxiv.org/pdf/2301.05187v1;cs.CV;;
2301.00950v3;http://arxiv.org/abs/2301.00950v3;2023-01-03;Class-Continuous Conditional Generative Neural Radiance Field;"The 3D-aware image synthesis focuses on conserving spatial consistency
besides generating high-resolution images with fine details. Recently, Neural
Radiance Field (NeRF) has been introduced for synthesizing novel views with low
computational cost and superior performance. While several works investigate a
generative NeRF and show remarkable achievement, they cannot handle conditional
and continuous feature manipulation in the generation procedure. In this work,
we introduce a novel model, called Class-Continuous Conditional Generative NeRF
($\text{C}^{3}$G-NeRF), which can synthesize conditionally manipulated
photorealistic 3D-consistent images by projecting conditional features to the
generator and the discriminator. The proposed $\text{C}^{3}$G-NeRF is evaluated
with three image datasets, AFHQ, CelebA, and Cars. As a result, our model shows
strong 3D-consistency with fine details and smooth interpolation in conditional
feature manipulation. For instance, $\text{C}^{3}$G-NeRF exhibits a Fr\'echet
Inception Distance (FID) of 7.64 in 3D-aware face image synthesis with a
$\text{128}^{2}$ resolution. Additionally, we provide FIDs of generated
3D-aware images of each class of the datasets as it is possible to synthesize
class-conditional images with $\text{C}^{3}$G-NeRF.";Jiwook Kim<author:sep>Minhyeok Lee;http://arxiv.org/pdf/2301.00950v3;cs.CV;BMVC 2023 (Accepted);nerf
2301.00411v2;http://arxiv.org/abs/2301.00411v2;2023-01-01;Detachable Novel Views Synthesis of Dynamic Scenes Using  Distribution-Driven Neural Radiance Fields;"Representing and synthesizing novel views in real-world dynamic scenes from
casual monocular videos is a long-standing problem. Existing solutions
typically approach dynamic scenes by applying geometry techniques or utilizing
temporal information between several adjacent frames without considering the
underlying background distribution in the entire scene or the transmittance
over the ray dimension, limiting their performance on static and occlusion
areas. Our approach $\textbf{D}$istribution-$\textbf{D}$riven neural radiance
fields offers high-quality view synthesis and a 3D solution to
$\textbf{D}$etach the background from the entire $\textbf{D}$ynamic scene,
which is called $\text{D}^4$NeRF. Specifically, it employs a neural
representation to capture the scene distribution in the static background and a
6D-input NeRF to represent dynamic objects, respectively. Each ray sample is
given an additional occlusion weight to indicate the transmittance lying in the
static and dynamic components. We evaluate $\text{D}^4$NeRF on public dynamic
scenes and our urban driving scenes acquired from an autonomous-driving
dataset. Extensive experiments demonstrate that our approach outperforms
previous methods in rendering texture details and motion areas while also
producing a clean static background. Our code will be released at
https://github.com/Luciferbobo/D4NeRF.";Boyu Zhang<author:sep>Wenbo Xu<author:sep>Zheng Zhu<author:sep>Guan Huang;http://arxiv.org/pdf/2301.00411v2;cs.CV;;nerf
2212.14710v1;http://arxiv.org/abs/2212.14710v1;2022-12-30;NeRF-Gaze: A Head-Eye Redirection Parametric Model for Gaze Estimation;"Gaze estimation is the fundamental basis for many visual tasks. Yet, the high
cost of acquiring gaze datasets with 3D annotations hinders the optimization
and application of gaze estimation models. In this work, we propose a novel
Head-Eye redirection parametric model based on Neural Radiance Field, which
allows dense gaze data generation with view consistency and accurate gaze
direction. Moreover, our head-eye redirection parametric model can decouple the
face and eyes for separate neural rendering, so it can achieve the purpose of
separately controlling the attributes of the face, identity, illumination, and
eye gaze direction. Thus diverse 3D-aware gaze datasets could be obtained by
manipulating the latent code belonging to different face attributions in an
unsupervised manner. Extensive experiments on several benchmarks demonstrate
the effectiveness of our method in domain generalization and domain adaptation
for gaze estimation tasks.";Pengwei Yin<author:sep>Jiawu Dai<author:sep>Jingjing Wang<author:sep>Di Xie<author:sep>Shiliang Pu;http://arxiv.org/pdf/2212.14710v1;cs.CV;10 pages, 8 figures, submitted to CVPR 2023;nerf
2212.14704v2;http://arxiv.org/abs/2212.14704v2;2022-12-28;Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and  Text-to-Image Diffusion Models;"Recent CLIP-guided 3D optimization methods, such as DreamFields and
PureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D
synthesis. However, due to scratch training and random initialization without
prior knowledge, these methods often fail to generate accurate and faithful 3D
structures that conform to the input text. In this paper, we make the first
attempt to introduce explicit 3D shape priors into the CLIP-guided 3D
optimization process. Specifically, we first generate a high-quality 3D shape
from the input text in the text-to-shape stage as a 3D shape prior. We then use
it as the initialization of a neural radiance field and optimize it with the
full prompt. To address the challenging text-to-shape generation task, we
present a simple yet effective approach that directly bridges the text and
image modalities with a powerful text-to-image diffusion model. To narrow the
style domain gap between the images synthesized by the text-to-image diffusion
model and shape renderings used to train the image-to-shape generator, we
further propose to jointly optimize a learnable text prompt and fine-tune the
text-to-image diffusion model for rendering-style image generation. Our method,
Dream3D, is capable of generating imaginative 3D content with superior visual
quality and shape accuracy compared to state-of-the-art methods.";Jiale Xu<author:sep>Xintao Wang<author:sep>Weihao Cheng<author:sep>Yan-Pei Cao<author:sep>Ying Shan<author:sep>Xiaohu Qie<author:sep>Shenghua Gao;http://arxiv.org/pdf/2212.14704v2;cs.CV;"Accepted by CVPR 2023. Project page:
  https://bluestyle97.github.io/dream3d/";nerf
2212.13056v3;http://arxiv.org/abs/2212.13056v3;2022-12-26;MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular  Videos;"In this paper, we target at the problem of learning a generalizable dynamic
radiance field from monocular videos. Different from most existing NeRF methods
that are based on multiple views, monocular videos only contain one view at
each timestamp, thereby suffering from ambiguity along the view direction in
estimating point features and scene flows. Previous studies such as DynNeRF
disambiguate point features by positional encoding, which is not transferable
and severely limits the generalization ability. As a result, these methods have
to train one independent model for each scene and suffer from heavy
computational costs when applying to increasing monocular videos in real-world
applications. To address this, We propose MonoNeRF to simultaneously learn
point features and scene flows with point trajectory and feature correspondence
constraints across frames. More specifically, we learn an implicit velocity
field to estimate point trajectory from temporal features with Neural ODE,
which is followed by a flow-based feature aggregation module to obtain spatial
features along the point trajectory. We jointly optimize temporal and spatial
features in an end-to-end manner. Experiments show that our MonoNeRF is able to
learn from multiple scenes and support new applications such as scene editing,
unseen frame synthesis, and fast novel scene adaptation. Codes are available at
https://github.com/tianfr/MonoNeRF.";Fengrui Tian<author:sep>Shaoyi Du<author:sep>Yueqi Duan;http://arxiv.org/pdf/2212.13056v3;cs.CV;Accepted by ICCV 2023;nerf
2212.12871v1;http://arxiv.org/abs/2212.12871v1;2022-12-25;PaletteNeRF: Palette-based Color Editing for NeRFs;"Neural Radiance Field (NeRF) is a powerful tool to faithfully generate novel
views for scenes with only sparse captured images. Despite its strong
capability for representing 3D scenes and their appearance, its editing ability
is very limited. In this paper, we propose a simple but effective extension of
vanilla NeRF, named PaletteNeRF, to enable efficient color editing on
NeRF-represented scenes. Motivated by recent palette-based image decomposition
works, we approximate each pixel color as a sum of palette colors modulated by
additive weights. Instead of predicting pixel colors as in vanilla NeRFs, our
method predicts additive weights. The underlying NeRF backbone could also be
replaced with more recent NeRF models such as KiloNeRF to achieve real-time
editing. Experimental results demonstrate that our method achieves efficient,
view-consistent, and artifact-free color editing on a wide range of
NeRF-represented scenes.";Qiling Wu<author:sep>Jianchao Tan<author:sep>Kun Xu;http://arxiv.org/pdf/2212.12871v1;cs.CV;12 pages, 10 figures;nerf
2212.11966v1;http://arxiv.org/abs/2212.11966v1;2022-12-22;Removing Objects From Neural Radiance Fields;"Neural Radiance Fields (NeRFs) are emerging as a ubiquitous scene
representation that allows for novel view synthesis. Increasingly, NeRFs will
be shareable with other people. Before sharing a NeRF, though, it might be
desirable to remove personal information or unsightly objects. Such removal is
not easily achieved with the current NeRF editing frameworks. We propose a
framework to remove objects from a NeRF representation created from an RGB-D
sequence. Our NeRF inpainting method leverages recent work in 2D image
inpainting and is guided by a user-provided mask. Our algorithm is underpinned
by a confidence based view selection procedure. It chooses which of the
individual 2D inpainted images to use in the creation of the NeRF, so that the
resulting inpainted NeRF is 3D consistent. We show that our method for NeRF
editing is effective for synthesizing plausible inpaintings in a multi-view
coherent manner. We validate our approach using a new and still-challenging
dataset for the task of NeRF inpainting.";Silvan Weder<author:sep>Guillermo Garcia-Hernando<author:sep>Aron Monszpart<author:sep>Marc Pollefeys<author:sep>Gabriel Brostow<author:sep>Michael Firman<author:sep>Sara Vicente;http://arxiv.org/pdf/2212.11966v1;cs.CV;;nerf
2212.10699v2;http://arxiv.org/abs/2212.10699v2;2022-12-21;PaletteNeRF: Palette-based Appearance Editing of Neural Radiance Fields;"Recent advances in neural radiance fields have enabled the high-fidelity 3D
reconstruction of complex scenes for novel view synthesis. However, it remains
underexplored how the appearance of such representations can be efficiently
edited while maintaining photorealism.
  In this work, we present PaletteNeRF, a novel method for photorealistic
appearance editing of neural radiance fields (NeRF) based on 3D color
decomposition. Our method decomposes the appearance of each 3D point into a
linear combination of palette-based bases (i.e., 3D segmentations defined by a
group of NeRF-type functions) that are shared across the scene. While our
palette-based bases are view-independent, we also predict a view-dependent
function to capture the color residual (e.g., specular shading). During
training, we jointly optimize the basis functions and the color palettes, and
we also introduce novel regularizers to encourage the spatial coherence of the
decomposition.
  Our method allows users to efficiently edit the appearance of the 3D scene by
modifying the color palettes. We also extend our framework with compressed
semantic features for semantic-aware appearance editing. We demonstrate that
our technique is superior to baseline methods both quantitatively and
qualitatively for appearance editing of complex real-world scenes.";Zhengfei Kuang<author:sep>Fujun Luan<author:sep>Sai Bi<author:sep>Zhixin Shu<author:sep>Gordon Wetzstein<author:sep>Kalyan Sunkavalli;http://arxiv.org/pdf/2212.10699v2;cs.CV;;nerf
2212.10950v2;http://arxiv.org/abs/2212.10950v2;2022-12-21;Incremental Neural Implicit Representation with Uncertainty-Filtered  Knowledge Distillation;"Recent neural implicit representations (NIRs) have achieved great success in
the tasks of 3D reconstruction and novel view synthesis. However, they suffer
from the catastrophic forgetting problem when continuously learning from
streaming data without revisiting the previously seen data. This limitation
prohibits the application of existing NIRs to scenarios where images come in
sequentially. In view of this, we explore the task of incremental learning for
NIRs in this work. We design a student-teacher framework to mitigate the
catastrophic forgetting problem. Specifically, we iterate the process of using
the student as the teacher at the end of each time step and let the teacher
guide the training of the student in the next step. As a result, the student
network is able to learn new information from the streaming data and retain old
knowledge from the teacher network simultaneously. Although intuitive, naively
applying the student-teacher pipeline does not work well in our task. Not all
information from the teacher network is helpful since it is only trained with
the old data. To alleviate this problem, we further introduce a random inquirer
and an uncertainty-based filter to filter useful information. Our proposed
method is general and thus can be adapted to different implicit representations
such as neural radiance field (NeRF) and neural SDF. Extensive experimental
results for both 3D reconstruction and novel view synthesis demonstrate the
effectiveness of our approach compared to different baselines.";Mengqi Guo<author:sep>Chen Li<author:sep>Hanlin Chen<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2212.10950v2;cs.CV;;nerf
2212.09330v1;http://arxiv.org/abs/2212.09330v1;2022-12-19;StyleTRF: Stylizing Tensorial Radiance Fields;"Stylized view generation of scenes captured casually using a camera has
received much attention recently. The geometry and appearance of the scene are
typically captured as neural point sets or neural radiance fields in the
previous work. An image stylization method is used to stylize the captured
appearance by training its network jointly or iteratively with the structure
capture network. The state-of-the-art SNeRF method trains the NeRF and
stylization network in an alternating manner. These methods have high training
time and require joint optimization. In this work, we present StyleTRF, a
compact, quick-to-optimize strategy for stylized view generation using TensoRF.
The appearance part is fine-tuned using sparse stylized priors of a few views
rendered using the TensoRF representation for a few iterations. Our method thus
effectively decouples style-adaption from view capture and is much faster than
the previous methods. We show state-of-the-art results on several scenes used
for this purpose.";Rahul Goel<author:sep>Sirikonda Dhawal<author:sep>Saurabh Saini<author:sep>P. J. Narayanan;http://arxiv.org/pdf/2212.09330v1;cs.CV;Accepted at ICVGIP-2022;nerf
2212.09735v2;http://arxiv.org/abs/2212.09735v2;2022-12-19;Correspondence Distillation from NeRF-based GAN;"The neural radiance field (NeRF) has shown promising results in preserving
the fine details of objects and scenes. However, unlike mesh-based
representations, it remains an open problem to build dense correspondences
across different NeRFs of the same category, which is essential in many
downstream tasks. The main difficulties of this problem lie in the implicit
nature of NeRF and the lack of ground-truth correspondence annotations. In this
paper, we show it is possible to bypass these challenges by leveraging the rich
semantics and structural priors encapsulated in a pre-trained NeRF-based GAN.
Specifically, we exploit such priors from three aspects, namely 1) a dual
deformation field that takes latent codes as global structural indicators, 2) a
learning objective that regards generator features as geometric-aware local
descriptors, and 3) a source of infinite object-specific NeRF samples. Our
experiments demonstrate that such priors lead to 3D dense correspondence that
is accurate, smooth, and robust. We also show that established dense
correspondence across NeRFs can effectively enable many NeRF-based downstream
applications such as texture transfer.";Yushi Lan<author:sep>Chen Change Loy<author:sep>Bo Dai;http://arxiv.org/pdf/2212.09735v2;cs.CV;Project page: https://nirvanalan.github.io/projects/DDF/index.html;nerf
2212.09100v3;http://arxiv.org/abs/2212.09100v3;2022-12-18;SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input  Images;"Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel
view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels
for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage
machine learning and adoption of SRFs as a 3D representation, we present SPARF,
a large-scale ShapeNet-based synthetic dataset for novel view synthesis
consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at
high resolution (400 X 400 pixels). The dataset is orders of magnitude larger
than existing synthetic datasets for novel view synthesis and includes more
than one million 3D-optimized radiance fields with multiple voxel resolutions.
Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate
sparse voxel radiance fields from only few views. This is done by using the
densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs
partial SRFs from few/one images and a specialized SRF loss to learn to
generate high-quality sparse voxel radiance fields that can be rendered from
novel views. Our approach achieves state-of-the-art results in the task of
unconstrained novel view synthesis based on few views on ShapeNet as compared
to recent baselines. The SPARF dataset is made public with the code and models
on the project website https://abdullahamdi.com/sparf/ .";Abdullah Hamdi<author:sep>Bernard Ghanem<author:sep>Matthias NieÃner;http://arxiv.org/pdf/2212.09100v3;cs.CV;published at ICCV 2023 workshop proceedings;nerf
2212.09069v2;http://arxiv.org/abs/2212.09069v2;2022-12-18;Masked Wavelet Representation for Compact Neural Radiance Fields;"Neural radiance fields (NeRF) have demonstrated the potential of
coordinate-based neural representation (neural fields or implicit neural
representation) in neural rendering. However, using a multi-layer perceptron
(MLP) to represent a 3D scene or object requires enormous computational
resources and time. There have been recent studies on how to reduce these
computational inefficiencies by using additional data structures, such as grids
or trees. Despite the promising performance, the explicit data structure
necessitates a substantial amount of memory. In this work, we present a method
to reduce the size without compromising the advantages of having additional
data structures. In detail, we propose using the wavelet transform on
grid-based neural fields. Grid-based neural fields are for fast convergence,
and the wavelet transform, whose efficiency has been demonstrated in
high-performance standard codecs, is to improve the parameter efficiency of
grids. Furthermore, in order to achieve a higher sparsity of grid coefficients
while maintaining reconstruction quality, we present a novel trainable masking
approach. Experimental results demonstrate that non-spatial grid coefficients,
such as wavelet coefficients, are capable of attaining a higher level of
sparsity than spatial grid coefficients, resulting in a more compact
representation. With our proposed mask and compression pipeline, we achieved
state-of-the-art performance within a memory budget of 2 MB. Our code is
available at https://github.com/daniel03c1/masked_wavelet_nerf.";Daniel Rho<author:sep>Byeonghyeon Lee<author:sep>Seungtae Nam<author:sep>Joo Chan Lee<author:sep>Jong Hwan Ko<author:sep>Eunbyung Park;http://arxiv.org/pdf/2212.09069v2;cs.CV;Accepted to CVPR 2023;nerf
2212.08328v2;http://arxiv.org/abs/2212.08328v2;2022-12-16;MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance  Fields;"Hinged on the representation power of neural networks, neural radiance fields
(NeRF) have recently emerged as one of the promising and widely applicable
methods for 3D object and scene representation. However, NeRF faces challenges
in practical applications, such as large-scale scenes and edge devices with a
limited amount of memory, where data needs to be processed sequentially. Under
such incremental learning scenarios, neural networks are known to suffer
catastrophic forgetting: easily forgetting previously seen data after training
with new data. We observe that previous incremental learning algorithms are
limited by either low performance or memory scalability issues. As such, we
develop a Memory-Efficient Incremental Learning algorithm for NeRF (MEIL-NeRF).
MEIL-NeRF takes inspiration from NeRF itself in that a neural network can serve
as a memory that provides the pixel RGB values, given rays as queries. Upon the
motivation, our framework learns which rays to query NeRF to extract previous
pixel values. The extracted pixel values are then used to train NeRF in a
self-distillation manner to prevent catastrophic forgetting. As a result,
MEIL-NeRF demonstrates constant memory consumption and competitive performance.";Jaeyoung Chung<author:sep>Kanggeon Lee<author:sep>Sungyong Baik<author:sep>Kyoung Mu Lee;http://arxiv.org/pdf/2212.08328v2;cs.CV;"18 pages. For the project page, see
  https://robot0321.github.io/meil-nerf/index.html";nerf
2212.08476v1;http://arxiv.org/abs/2212.08476v1;2022-12-15;SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory;"Neural Radiance Fields (NeRF) have demonstrated superior novel view synthesis
performance but are slow at rendering. To speed up the volume rendering
process, many acceleration methods have been proposed at the cost of large
memory consumption. To push the frontier of the efficiency-memory trade-off, we
explore a new perspective to accelerate NeRF rendering, leveraging a key fact
that the viewpoint change is usually smooth and continuous in interactive
viewpoint control. This allows us to leverage the information of preceding
viewpoints to reduce the number of rendered pixels as well as the number of
sampled points along the ray of the remaining pixels. In our pipeline, a
low-resolution feature map is rendered first by volume rendering, then a
lightweight 2D neural renderer is applied to generate the output image at
target resolution leveraging the features of preceding and current frames. We
show that the proposed method can achieve competitive rendering quality while
reducing the rendering time with little memory overhead, enabling 30FPS at
1080P image resolution with a low memory footprint.";Sicheng Li<author:sep>Hao Li<author:sep>Yue Wang<author:sep>Yiyi Liao<author:sep>Lu Yu;http://arxiv.org/pdf/2212.08476v1;cs.CV;;nerf
2212.08067v2;http://arxiv.org/abs/2212.08067v2;2022-12-15;VolRecon: Volume Rendering of Signed Ray Distance Functions for  Generalizable Multi-View Reconstruction;"The success of the Neural Radiance Fields (NeRF) in novel view synthesis has
inspired researchers to propose neural implicit scene reconstruction. However,
most existing neural implicit reconstruction methods optimize per-scene
parameters and therefore lack generalizability to new scenes. We introduce
VolRecon, a novel generalizable implicit reconstruction method with Signed Ray
Distance Function (SRDF). To reconstruct the scene with fine details and little
noise, VolRecon combines projection features aggregated from multi-view
features, and volume features interpolated from a coarse global feature volume.
Using a ray transformer, we compute SRDF values of sampled points on a ray and
then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by
about 30% in sparse view reconstruction and achieves comparable accuracy as
MVSNet in full view reconstruction. Furthermore, our approach exhibits good
generalization performance on the large-scale ETH3D benchmark.";Yufan Ren<author:sep>Fangjinhua Wang<author:sep>Tong Zhang<author:sep>Marc Pollefeys<author:sep>Sabine SÃ¼sstrunk;http://arxiv.org/pdf/2212.08067v2;cs.CV;;nerf
2212.08070v1;http://arxiv.org/abs/2212.08070v1;2022-12-15;NeRF-Art: Text-Driven Neural Radiance Fields Stylization;"As a powerful representation of 3D scenes, the neural radiance field (NeRF)
enables high-quality novel view synthesis from multi-view images. Stylizing
NeRF, however, remains challenging, especially on simulating a text-guided
style with both the appearance and the geometry altered simultaneously. In this
paper, we present NeRF-Art, a text-guided NeRF stylization approach that
manipulates the style of a pre-trained NeRF model with a simple text prompt.
Unlike previous approaches that either lack sufficient geometry deformations
and texture details or require meshes to guide the stylization, our method can
shift a 3D scene to the target style characterized by desired geometry and
appearance variations without any mesh guidance. This is achieved by
introducing a novel global-local contrastive learning strategy, combined with
the directional constraint to simultaneously control both the trajectory and
the strength of the target style. Moreover, we adopt a weight regularization
method to effectively suppress cloudy artifacts and geometry noises which arise
easily when the density field is transformed during geometry stylization.
Through extensive experiments on various styles, we demonstrate that our method
is effective and robust regarding both single-view stylization quality and
cross-view consistency. The code and more results can be found in our project
page: https://cassiepython.github.io/nerfart/.";Can Wang<author:sep>Ruixiang Jiang<author:sep>Menglei Chai<author:sep>Mingming He<author:sep>Dongdong Chen<author:sep>Jing Liao;http://arxiv.org/pdf/2212.08070v1;cs.CV;Project page: https://cassiepython.github.io/nerfart/;nerf
2212.08057v2;http://arxiv.org/abs/2212.08057v2;2022-12-15;Real-Time Neural Light Field on Mobile Devices;"Recent efforts in Neural Rendering Fields (NeRF) have shown impressive
results on novel view synthesis by utilizing implicit neural representation to
represent 3D scenes. Due to the process of volumetric rendering, the inference
speed for NeRF is extremely slow, limiting the application scenarios of
utilizing NeRF on resource-constrained hardware, such as mobile devices. Many
works have been conducted to reduce the latency of running NeRF models.
However, most of them still require high-end GPU for acceleration or extra
storage memory, which is all unavailable on mobile devices. Another emerging
direction utilizes the neural light field (NeLF) for speedup, as only one
forward pass is performed on a ray to predict the pixel color. Nevertheless, to
reach a similar rendering quality as NeRF, the network in NeLF is designed with
intensive computation, which is not mobile-friendly. In this work, we propose
an efficient network that runs in real-time on mobile devices for neural
rendering. We follow the setting of NeLF to train our network. Unlike existing
works, we introduce a novel network architecture that runs efficiently on
mobile devices with low latency and small size, i.e., saving $15\times \sim
24\times$ storage compared with MobileNeRF. Our model achieves high-resolution
generation while maintaining real-time inference for both synthetic and
real-world scenes on mobile devices, e.g., $18.04$ms (iPhone 13) for rendering
one $1008\times756$ image of real 3D scenes. Additionally, we achieve similar
image quality as NeRF and better quality than MobileNeRF (PSNR $26.15$ vs.
$25.91$ on the real-world forward-facing dataset).";Junli Cao<author:sep>Huan Wang<author:sep>Pavlo Chemerys<author:sep>Vladislav Shakhrai<author:sep>Ju Hu<author:sep>Yun Fu<author:sep>Denys Makoviichuk<author:sep>Sergey Tulyakov<author:sep>Jian Ren;http://arxiv.org/pdf/2212.08057v2;cs.CV;"CVPR 2023. Project page: https://snap-research.github.io/MobileR2L/
  Code: https://github.com/snap-research/MobileR2L/";nerf
2212.07388v3;http://arxiv.org/abs/2212.07388v3;2022-12-14;NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior;"Training a Neural Radiance Field (NeRF) without pre-computed camera poses is
challenging. Recent advances in this direction demonstrate the possibility of
jointly optimising a NeRF and camera poses in forward-facing scenes. However,
these methods still face difficulties during dramatic camera movement. We
tackle this challenging problem by incorporating undistorted monocular depth
priors. These priors are generated by correcting scale and shift parameters
during training, with which we are then able to constrain the relative poses
between consecutive frames. This constraint is achieved using our proposed
novel loss functions. Experiments on real-world indoor and outdoor scenes show
that our method can handle challenging camera trajectories and outperforms
existing methods in terms of novel view rendering quality and pose estimation
accuracy. Our project page is https://nope-nerf.active.vision.";Wenjing Bian<author:sep>Zirui Wang<author:sep>Kejie Li<author:sep>Jia-Wang Bian<author:sep>Victor Adrian Prisacariu;http://arxiv.org/pdf/2212.07388v3;cs.CV;;nerf
2212.06135v1;http://arxiv.org/abs/2212.06135v1;2022-12-12;Rodin: A Generative Model for Sculpting 3D Digital Avatars Using  Diffusion;"This paper presents a 3D generative model that uses diffusion models to
automatically generate 3D digital avatars represented as neural radiance
fields. A significant challenge in generating such avatars is that the memory
and processing costs in 3D are prohibitive for producing the rich details
required for high-quality avatars. To tackle this problem we propose the
roll-out diffusion network (Rodin), which represents a neural radiance field as
multiple 2D feature maps and rolls out these maps into a single 2D feature
plane within which we perform 3D-aware diffusion. The Rodin model brings the
much-needed computational efficiency while preserving the integrity of
diffusion in 3D by using 3D-aware convolution that attends to projected
features in the 2D feature plane according to their original relationship in
3D. We also use latent conditioning to orchestrate the feature generation for
global coherence, leading to high-fidelity avatars and enabling their semantic
editing based on text prompts. Finally, we use hierarchical synthesis to
further enhance details. The 3D avatars generated by our model compare
favorably with those produced by existing generative techniques. We can
generate highly detailed avatars with realistic hairstyles and facial hair like
beards. We also demonstrate 3D avatar generation from image or text as well as
text-guided editability.";Tengfei Wang<author:sep>Bo Zhang<author:sep>Ting Zhang<author:sep>Shuyang Gu<author:sep>Jianmin Bao<author:sep>Tadas Baltrusaitis<author:sep>Jingjing Shen<author:sep>Dong Chen<author:sep>Fang Wen<author:sep>Qifeng Chen<author:sep>Baining Guo;http://arxiv.org/pdf/2212.06135v1;cs.CV;Project Webpage: https://3d-avatar-diffusion.microsoft.com/;
2212.04701v2;http://arxiv.org/abs/2212.04701v2;2022-12-09;4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions;"In this paper, we present a novel and effective framework, named 4K-NeRF, to
pursue high fidelity view synthesis on the challenging scenarios of ultra high
resolutions, building on the methodology of neural radiance fields (NeRF). The
rendering procedure of NeRF-based methods typically relies on a pixel-wise
manner in which rays (or pixels) are treated independently on both training and
inference phases, limiting its representational ability on describing subtle
details, especially when lifting to a extremely high resolution. We address the
issue by exploring ray correlation to enhance high-frequency details recovery.
Particularly, we use the 3D-aware encoder to model geometric information
effectively in a lower resolution space and recover fine details through the
3D-aware decoder, conditioned on ray features and depths estimated by the
encoder. Joint training with patch-based sampling further facilitates our
method incorporating the supervision from perception oriented regularization
beyond pixel-wise loss. Benefiting from the use of geometry-aware local
context, our method can significantly boost rendering quality on high-frequency
details compared with modern NeRF methods, and achieve the state-of-the-art
visual quality on 4K ultra-high-resolution scenarios. Code Available at
\url{https://github.com/frozoul/4K-NeRF}";Zhongshu Wang<author:sep>Lingzhi Li<author:sep>Zhen Shen<author:sep>Li Shen<author:sep>Liefeng Bo;http://arxiv.org/pdf/2212.04701v2;cs.CV;;nerf
2212.04823v2;http://arxiv.org/abs/2212.04823v2;2022-12-08;GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields;"We propose GazeNeRF, a 3D-aware method for the task of gaze redirection.
Existing gaze redirection methods operate on 2D images and struggle to generate
3D consistent results. Instead, we build on the intuition that the face region
and eyeballs are separate 3D structures that move in a coordinated yet
independent fashion. Our method leverages recent advancements in conditional
image-based neural radiance fields and proposes a two-stream architecture that
predicts volumetric features for the face and eye regions separately. Rigidly
transforming the eye features via a 3D rotation matrix provides fine-grained
control over the desired gaze angle. The final, redirected image is then
attained via differentiable volume compositing. Our experiments show that this
architecture outperforms naively conditioned NeRF baselines as well as previous
state-of-the-art 2D gaze redirection methods in terms of redirection accuracy
and identity preservation.";Alessandro Ruzzi<author:sep>Xiangwei Shi<author:sep>Xi Wang<author:sep>Gengyan Li<author:sep>Shalini De Mello<author:sep>Hyung Jin Chang<author:sep>Xucong Zhang<author:sep>Otmar Hilliges;http://arxiv.org/pdf/2212.04823v2;cs.CV;"Accepted at CVPR 2023. Github page:
  https://github.com/AlessandroRuzzi/GazeNeRF";nerf
2212.04492v2;http://arxiv.org/abs/2212.04492v2;2022-12-08;Few-View Object Reconstruction with Unknown Categories and Camera Poses;"While object reconstruction has made great strides in recent years, current
methods typically require densely captured images and/or known camera poses,
and generalize poorly to novel object categories. To step toward object
reconstruction in the wild, this work explores reconstructing general
real-world objects from a few images without known camera poses or object
categories. The crux of our work is solving two fundamental 3D vision problems
-- shape reconstruction and pose estimation -- in a unified approach. Our
approach captures the synergies of these two problems: reliable camera pose
estimation gives rise to accurate shape reconstruction, and the accurate
reconstruction, in turn, induces robust correspondence between different views
and facilitates pose estimation. Our method FORGE predicts 3D features from
each view and leverages them in conjunction with the input images to establish
cross-view correspondence for estimating relative camera poses. The 3D features
are then transformed by the estimated poses into a shared space and are fused
into a neural radiance field. The reconstruction results are rendered by volume
rendering techniques, enabling us to train the model without 3D shape
ground-truth. Our experiments show that FORGE reliably reconstructs objects
from five views. Our pose estimation method outperforms existing ones by a
large margin. The reconstruction results under predicted poses are comparable
to the ones using ground-truth poses. The performance on novel testing
categories matches the results on categories seen during training. Project
page: https://ut-austin-rpl.github.io/FORGE/";Hanwen Jiang<author:sep>Zhenyu Jiang<author:sep>Kristen Grauman<author:sep>Yuke Zhu;http://arxiv.org/pdf/2212.04492v2;cs.CV;;
2212.03635v1;http://arxiv.org/abs/2212.03635v1;2022-12-07;Non-uniform Sampling Strategies for NeRF on 360{\textdegree} images;"In recent years, the performance of novel view synthesis using perspective
images has dramatically improved with the advent of neural radiance fields
(NeRF). This study proposes two novel techniques that effectively build NeRF
for 360{\textdegree} omnidirectional images. Due to the characteristics of a
360{\textdegree} image of ERP format that has spatial distortion in their high
latitude regions and a 360{\textdegree} wide viewing angle, NeRF's general ray
sampling strategy is ineffective. Hence, the view synthesis accuracy of NeRF is
limited and learning is not efficient. We propose two non-uniform ray sampling
schemes for NeRF to suit 360{\textdegree} images - distortion-aware ray
sampling and content-aware ray sampling. We created an evaluation dataset
Synth360 using Replica and SceneCity models of indoor and outdoor scenes,
respectively. In experiments, we show that our proposal successfully builds
360{\textdegree} image NeRF in terms of both accuracy and efficiency. The
proposal is widely applicable to advanced variants of NeRF. DietNeRF, AugNeRF,
and NeRF++ combined with the proposed techniques further improve the
performance. Moreover, we show that our proposed method enhances the quality of
real-world scenes in 360{\textdegree} images. Synth360:
https://drive.google.com/drive/folders/1suL9B7DO2no21ggiIHkH3JF3OecasQLb.";Takashi Otonari<author:sep>Satoshi Ikehata<author:sep>Kiyoharu Aizawa;http://arxiv.org/pdf/2212.03635v1;cs.CV;Accepted at the 33rd British Machine Vision Conference (BMVC) 2022;nerf
2212.03848v2;http://arxiv.org/abs/2212.03848v2;2022-12-07;NeRFEditor: Differentiable Style Decomposition for Full 3D Scene Editing;"We present NeRFEditor, an efficient learning framework for 3D scene editing,
which takes a video captured over 360{\deg} as input and outputs a
high-quality, identity-preserving stylized 3D scene. Our method supports
diverse types of editing such as guided by reference images, text prompts, and
user interactions. We achieve this by encouraging a pre-trained StyleGAN model
and a NeRF model to learn from each other mutually. Specifically, we use a NeRF
model to generate numerous image-angle pairs to train an adjustor, which can
adjust the StyleGAN latent code to generate high-fidelity stylized images for
any given angle. To extrapolate editing to GAN out-of-domain views, we devise
another module that is trained in a self-supervised learning manner. This
module maps novel-view images to the hidden space of StyleGAN that allows
StyleGAN to generate stylized images on novel views. These two modules together
produce guided images in 360{\deg}views to finetune a NeRF to make stylization
effects, where a stable fine-tuning strategy is proposed to achieve this.
Experiments show that NeRFEditor outperforms prior work on benchmark and
real-world scenes with better editability, fidelity, and identity preservation.";Chunyi Sun<author:sep>Yanbin Liu<author:sep>Junlin Han<author:sep>Stephen Gould;http://arxiv.org/pdf/2212.03848v2;cs.CV;Project page: https://chuny1.github.io/NeRFEditor/nerfeditor.html;nerf
2212.03406v1;http://arxiv.org/abs/2212.03406v1;2022-12-07;SSDNeRF: Semantic Soft Decomposition of Neural Radiance Fields;"Neural Radiance Fields (NeRFs) encode the radiance in a scene parameterized
by the scene's plenoptic function. This is achieved by using an MLP together
with a mapping to a higher-dimensional space, and has been proven to capture
scenes with a great level of detail. Naturally, the same parameterization can
be used to encode additional properties of the scene, beyond just its radiance.
A particularly interesting property in this regard is the semantic
decomposition of the scene. We introduce a novel technique for semantic soft
decomposition of neural radiance fields (named SSDNeRF) which jointly encodes
semantic signals in combination with radiance signals of a scene. Our approach
provides a soft decomposition of the scene into semantic parts, enabling us to
correctly encode multiple semantic classes blending along the same direction --
an impossible feat for existing methods. Not only does this lead to a detailed,
3D semantic representation of the scene, but we also show that the regularizing
effects of the MLP used for encoding help to improve the semantic
representation. We show state-of-the-art segmentation and reconstruction
results on a dataset of common objects and demonstrate how the proposed
approach can be applied for high quality temporally consistent video editing
and re-compositing on a dataset of casually captured selfie videos.";Siddhant Ranade<author:sep>Christoph Lassner<author:sep>Kai Li<author:sep>Christian Haene<author:sep>Shen-Chi Chen<author:sep>Jean-Charles Bazin<author:sep>Sofien Bouaziz;http://arxiv.org/pdf/2212.03406v1;cs.CV;"Project page:
  https://www.siddhantranade.com/research/2022/12/06/SSDNeRF-Semantic-Soft-Decomposition-of-Neural-Radiance-Fields.html";nerf
2212.04247v2;http://arxiv.org/abs/2212.04247v2;2022-12-07;EditableNeRF: Editing Topologically Varying Neural Radiance Fields by  Key Points;"Neural radiance fields (NeRF) achieve highly photo-realistic novel-view
synthesis, but it's a challenging problem to edit the scenes modeled by
NeRF-based methods, especially for dynamic scenes. We propose editable neural
radiance fields that enable end-users to easily edit dynamic scenes and even
support topological changes. Input with an image sequence from a single camera,
our network is trained fully automatically and models topologically varying
dynamics using our picked-out surface key points. Then end-users can edit the
scene by easily dragging the key points to desired new positions. To achieve
this, we propose a scene analysis method to detect and initialize key points by
considering the dynamics in the scene, and a weighted key points strategy to
model topologically varying dynamics by joint key points and weights
optimization. Our method supports intuitive multi-dimensional (up to 3D)
editing and can generate novel scenes that are unseen in the input sequence.
Experiments demonstrate that our method achieves high-quality editing on
various dynamic scenes and outperforms the state-of-the-art. Our code and
captured data are available at https://chengwei-zheng.github.io/EditableNeRF/.";Chengwei Zheng<author:sep>Wenbin Lin<author:sep>Feng Xu;http://arxiv.org/pdf/2212.04247v2;cs.CV;Accepted by CVPR 2023;nerf
2212.03267v1;http://arxiv.org/abs/2212.03267v1;2022-12-06;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as  General Image Priors;"2D-to-3D reconstruction is an ill-posed problem, yet humans are good at
solving this problem due to their prior knowledge of the 3D world developed
over years. Driven by this observation, we propose NeRDi, a single-view NeRF
synthesis framework with general image priors from 2D diffusion models.
Formulating single-view reconstruction as an image-conditioned 3D generation
problem, we optimize the NeRF representations by minimizing a diffusion loss on
its arbitrary view renderings with a pretrained image diffusion model under the
input-view constraint. We leverage off-the-shelf vision-language models and
introduce a two-section language guidance as conditioning inputs to the
diffusion model. This is essentially helpful for improving multiview content
coherence as it narrows down the general image prior conditioned on the
semantic and visual features of the single-view input image. Additionally, we
introduce a geometric loss based on estimated depth maps to regularize the
underlying 3D geometry of the NeRF. Experimental results on the DTU MVS dataset
show that our method can synthesize novel views with higher quality even
compared to existing methods trained on this dataset. We also demonstrate our
generalizability in zero-shot NeRF synthesis for in-the-wild images.";"Congyue Deng<author:sep>Chiyu ""Max'' Jiang<author:sep>Charles R. Qi<author:sep>Xinchen Yan<author:sep>Yin Zhou<author:sep>Leonidas Guibas<author:sep>Dragomir Anguelov";http://arxiv.org/pdf/2212.03267v1;cs.CV;;nerf
2212.02501v4;http://arxiv.org/abs/2212.02501v4;2022-12-05;SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance  Fields;"3D reconstruction from a single 2D image was extensively covered in the
literature but relies on depth supervision at training time, which limits its
applicability. To relax the dependence to depth we propose SceneRF, a
self-supervised monocular scene reconstruction method using only posed image
sequences for training. Fueled by the recent progress in neural radiance fields
(NeRF) we optimize a radiance field though with explicit depth optimization and
a novel probabilistic sampling strategy to efficiently handle large scenes. At
inference, a single input image suffices to hallucinate novel depth views which
are fused together to obtain 3D scene reconstruction. Thorough experiments
demonstrate that we outperform all baselines for novel depth views synthesis
and scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI.
Code is available at https://astra-vision.github.io/SceneRF .";Anh-Quan Cao<author:sep>Raoul de Charette;http://arxiv.org/pdf/2212.02501v4;cs.CV;ICCV 2023. Project page: https://astra-vision.github.io/SceneRF;nerf
2212.01959v1;http://arxiv.org/abs/2212.01959v1;2022-12-05;INGeo: Accelerating Instant Neural Scene Reconstruction with Noisy  Geometry Priors;"We present a method that accelerates reconstruction of 3D scenes and objects,
aiming to enable instant reconstruction on edge devices such as mobile phones
and AR/VR headsets. While recent works have accelerated scene reconstruction
training to minute/second-level on high-end GPUs, there is still a large gap to
the goal of instant training on edge devices which is yet highly desired in
many emerging applications such as immersive AR/VR. To this end, this work aims
to further accelerate training by leveraging geometry priors of the target
scene. Our method proposes strategies to alleviate the noise of the imperfect
geometry priors to accelerate the training speed on top of the highly optimized
Instant-NGP. On the NeRF Synthetic dataset, our work uses half of the training
iterations to reach an average test PSNR of >30.";Chaojian Li<author:sep>Bichen Wu<author:sep>Albert Pumarola<author:sep>Peizhao Zhang<author:sep>Yingyan Lin<author:sep>Peter Vajda;http://arxiv.org/pdf/2212.01959v1;cs.CV;Accepted by Computer Vision for Metaverse Workshop @ ECCV'22;nerf
2212.02493v3;http://arxiv.org/abs/2212.02493v3;2022-12-05;Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural  Fields;"Coordinate-based implicit neural networks, or neural fields, have emerged as
useful representations of shape and appearance in 3D computer vision. Despite
advances, however, it remains challenging to build neural fields for categories
of objects without datasets like ShapeNet that provide ""canonicalized"" object
instances that are consistently aligned for their 3D position and orientation
(pose). We present Canonical Field Network (CaFi-Net), a self-supervised method
to canonicalize the 3D pose of instances from an object category represented as
neural fields, specifically neural radiance fields (NeRFs). CaFi-Net directly
learns from continuous and noisy radiance fields using a Siamese network
architecture that is designed to extract equivariant field features for
category-level canonicalization. During inference, our method takes pre-trained
neural radiance fields of novel object instances at arbitrary 3D pose and
estimates a canonical field with consistent 3D pose across the entire category.
Extensive experiments on a new dataset of 1300 NeRF models across 13 object
categories show that our method matches or exceeds the performance of 3D point
cloud-based methods.";Rohith Agaram<author:sep>Shaurya Dewan<author:sep>Rahul Sajnani<author:sep>Adrien Poulenard<author:sep>Madhava Krishna<author:sep>Srinath Sridhar;http://arxiv.org/pdf/2212.02493v3;cs.CV;;nerf
2212.02375v2;http://arxiv.org/abs/2212.02375v2;2022-12-05;D-TensoRF: Tensorial Radiance Fields for Dynamic Scenes;"Neural radiance field (NeRF) attracts attention as a promising approach to
reconstructing the 3D scene. As NeRF emerges, subsequent studies have been
conducted to model dynamic scenes, which include motions or topological
changes. However, most of them use an additional deformation network, slowing
down the training and rendering speed. Tensorial radiance field (TensoRF)
recently shows its potential for fast, high-quality reconstruction of static
scenes with compact model size. In this paper, we present D-TensoRF, a
tensorial radiance field for dynamic scenes, enabling novel view synthesis at a
specific time. We consider the radiance field of a dynamic scene as a 5D
tensor. The 5D tensor represents a 4D grid in which each axis corresponds to X,
Y, Z, and time and has 1D multi-channel features per element. Similar to
TensoRF, we decompose the grid either into rank-one vector components (CP
decomposition) or low-rank matrix components (newly proposed MM decomposition).
We also use smoothing regularization to reflect the relationship between
features at different times (temporal dependency). We conduct extensive
evaluations to analyze our models. We show that D-TensoRF with CP decomposition
and MM decomposition both have short training times and significantly low
memory footprints with quantitatively and qualitatively competitive rendering
results in comparison to the state-of-the-art methods in 3D dynamic scene
modeling.";Hankyu Jang<author:sep>Daeyoung Kim;http://arxiv.org/pdf/2212.02375v2;cs.CV;21 pages, 11 figures;nerf
2212.02469v4;http://arxiv.org/abs/2212.02469v4;2022-12-05;One-shot Implicit Animatable Avatars with Model-based Priors;"Existing neural rendering methods for creating human avatars typically either
require dense input signals such as video or multi-view images, or leverage a
learned prior from large-scale specific 3D human datasets such that
reconstruction can be performed with sparse-view inputs. Most of these methods
fail to achieve realistic reconstruction when only a single image is available.
To enable the data-efficient creation of realistic animatable 3D humans, we
propose ELICIT, a novel method for learning human-specific neural radiance
fields from a single image. Inspired by the fact that humans can effortlessly
estimate the body geometry and imagine full-body clothing from a single image,
we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior.
Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned
vertex-based template model (i.e., SMPL) and implements the visual clothing
semantic prior with the CLIP-based pretrained models. Both priors are used to
jointly guide the optimization for creating plausible content in the invisible
areas. Taking advantage of the CLIP models, ELICIT can use text descriptions to
generate text-conditioned unseen regions. In order to further improve visual
details, we propose a segmentation-based sampling strategy that locally refines
different parts of the avatar. Comprehensive evaluations on multiple popular
benchmarks, including ZJU-MoCAP, Human3.6M, and DeepFashion, show that ELICIT
has outperformed strong baseline methods of avatar creation when only a single
image is available. The code is public for research purposes at
https://huangyangyi.github.io/ELICIT/.";Yangyi Huang<author:sep>Hongwei Yi<author:sep>Weiyang Liu<author:sep>Haofan Wang<author:sep>Boxi Wu<author:sep>Wenxiao Wang<author:sep>Binbin Lin<author:sep>Debing Zhang<author:sep>Deng Cai;http://arxiv.org/pdf/2212.02469v4;cs.CV;"To appear at ICCV 2023. Project website:
  https://huangyangyi.github.io/ELICIT/";
2212.02280v2;http://arxiv.org/abs/2212.02280v2;2022-12-05;GARF:Geometry-Aware Generalized Neural Radiance Field;"Neural Radiance Field (NeRF) has revolutionized free viewpoint rendering
tasks and achieved impressive results. However, the efficiency and accuracy
problems hinder its wide applications. To address these issues, we propose
Geometry-Aware Generalized Neural Radiance Field (GARF) with a geometry-aware
dynamic sampling (GADS) strategy to perform real-time novel view rendering and
unsupervised depth estimation on unseen scenes without per-scene optimization.
Distinct from most existing generalized NeRFs, our framework infers the unseen
scenes on both pixel-scale and geometry-scale with only a few input images.
More specifically, our method learns common attributes of novel-view synthesis
by an encoder-decoder structure and a point-level learnable multi-view feature
fusion module which helps avoid occlusion. To preserve scene characteristics in
the generalized model, we introduce an unsupervised depth estimation module to
derive the coarse geometry, narrow down the ray sampling interval to proximity
space of the estimated surface and sample in expectation maximum position,
constituting Geometry-Aware Dynamic Sampling strategy (GADS). Moreover, we
introduce a Multi-level Semantic Consistency loss (MSC) to assist more
informative representation learning. Extensive experiments on indoor and
outdoor datasets show that comparing with state-of-the-art generalized NeRF
methods, GARF reduces samples by more than 25\%, while improving rendering
quality and 3D geometry estimation.";Yue Shi<author:sep>Dingyi Rong<author:sep>Bingbing Ni<author:sep>Chang Chen<author:sep>Wenjun Zhang;http://arxiv.org/pdf/2212.02280v2;cs.CV;;nerf
2212.01735v4;http://arxiv.org/abs/2212.01735v4;2022-12-04;Neural Fourier Filter Bank;"We present a novel method to provide efficient and highly detailed
reconstructions. Inspired by wavelets, we learn a neural field that decompose
the signal both spatially and frequency-wise. We follow the recent grid-based
paradigm for spatial decomposition, but unlike existing work, encourage
specific frequencies to be stored in each grid via Fourier features encodings.
We then apply a multi-layer perceptron with sine activations, taking these
Fourier encoded features in at appropriate layers so that higher-frequency
components are accumulated on top of lower-frequency components sequentially,
which we sum up to form the final output. We demonstrate that our method
outperforms the state of the art regarding model compactness and convergence
speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural
radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.";Zhijie Wu<author:sep>Yuhe Jin<author:sep>Kwang Moo Yi;http://arxiv.org/pdf/2212.01735v4;cs.CV;;
2212.01672v1;http://arxiv.org/abs/2212.01672v1;2022-12-03;MaRF: Representing Mars as Neural Radiance Fields;"The aim of this work is to introduce MaRF, a novel framework able to
synthesize the Martian environment using several collections of images from
rover cameras. The idea is to generate a 3D scene of Mars' surface to address
key challenges in planetary surface exploration such as: planetary geology,
simulated navigation and shape analysis. Although there exist different methods
to enable a 3D reconstruction of Mars' surface, they rely on classical computer
graphics techniques that incur high amounts of computational resources during
the reconstruction process, and have limitations with generalizing
reconstructions to unseen scenes and adapting to new images coming from rover
cameras. The proposed framework solves the aforementioned limitations by
exploiting Neural Radiance Fields (NeRFs), a method that synthesize complex
scenes by optimizing a continuous volumetric scene function using a sparse set
of images. To speed up the learning process, we replaced the sparse set of
rover images with their neural graphics primitives (NGPs), a set of vectors of
fixed length that are learned to preserve the information of the original
images in a significantly smaller size. In the experimental section, we
demonstrate the environments created from actual Mars datasets captured by
Curiosity rover, Perseverance rover and Ingenuity helicopter, all of which are
available on the Planetary Data System (PDS).";Lorenzo Giusti<author:sep>Josue Garcia<author:sep>Steven Cozine<author:sep>Darrick Suen<author:sep>Christina Nguyen<author:sep>Ryan Alimo;http://arxiv.org/pdf/2212.01672v1;cs.CV;ECCV 2022 (oral);nerf
2212.01602v1;http://arxiv.org/abs/2212.01602v1;2022-12-03;StegaNeRF: Embedding Invisible Information within Neural Radiance Fields;"Recent advances in neural rendering imply a future of widespread visual data
distributions through sharing NeRF model weights. However, while common visual
data (images and videos) have standard approaches to embed ownership or
copyright information explicitly or subtly, the problem remains unexplored for
the emerging NeRF format. We present StegaNeRF, a method for steganographic
information embedding in NeRF renderings. We design an optimization framework
allowing accurate hidden information extractions from images rendered by NeRF,
while preserving its original visual quality. We perform experimental
evaluations of our method under several potential deployment scenarios, and we
further discuss the insights discovered through our analysis. StegaNeRF
signifies an initial exploration into the novel problem of instilling
customizable, imperceptible, and recoverable information to NeRF renderings,
with minimal impact to rendered images. Project page:
https://xggnet.github.io/StegaNeRF/.";Chenxin Li<author:sep>Brandon Y. Feng<author:sep>Zhiwen Fan<author:sep>Panwang Pan<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2212.01602v1;cs.CV;Project page: https://xggnet.github.io/StegaNeRF/;nerf
2212.01331v4;http://arxiv.org/abs/2212.01331v4;2022-12-02;Surface Normal Clustering for Implicit Representation of Manhattan  Scenes;"Novel view synthesis and 3D modeling using implicit neural field
representation are shown to be very effective for calibrated multi-view
cameras. Such representations are known to benefit from additional geometric
and semantic supervision. Most existing methods that exploit additional
supervision require dense pixel-wise labels or localized scene priors. These
methods cannot benefit from high-level vague scene priors provided in terms of
scenes' descriptions. In this work, we aim to leverage the geometric prior of
Manhattan scenes to improve the implicit neural radiance field representations.
More precisely, we assume that only the knowledge of the indoor scene (under
investigation) being Manhattan is known -- with no additional information
whatsoever -- with an unknown Manhattan coordinate frame. Such high-level prior
is used to self-supervise the surface normals derived explicitly in the
implicit neural fields. Our modeling allows us to cluster the derived normals
and exploit their orthogonality constraints for self-supervision. Our
exhaustive experiments on datasets of diverse indoor scenes demonstrate the
significant benefit of the proposed method over the established baselines. The
source code is available at
https://github.com/nikola3794/normal-clustering-nerf.";Nikola Popovic<author:sep>Danda Pani Paudel<author:sep>Luc Van Gool;http://arxiv.org/pdf/2212.01331v4;cs.CV;Paper accepted to ICCV23;nerf
2212.01120v1;http://arxiv.org/abs/2212.01120v1;2022-12-02;RT-NeRF: Real-Time On-Device Neural Radiance Fields Towards Immersive  AR/VR Rendering;"Neural Radiance Field (NeRF) based rendering has attracted growing attention
thanks to its state-of-the-art (SOTA) rendering quality and wide applications
in Augmented and Virtual Reality (AR/VR). However, immersive real-time (> 30
FPS) NeRF based rendering enabled interactions are still limited due to the low
achievable throughput on AR/VR devices. To this end, we first profile SOTA
efficient NeRF algorithms on commercial devices and identify two primary causes
of the aforementioned inefficiency: (1) the uniform point sampling and (2) the
dense accesses and computations of the required embeddings in NeRF.
Furthermore, we propose RT-NeRF, which to the best of our knowledge is the
first algorithm-hardware co-design acceleration of NeRF. Specifically, on the
algorithm level, RT-NeRF integrates an efficient rendering pipeline for largely
alleviating the inefficiency due to the commonly adopted uniform point sampling
method in NeRF by directly computing the geometry of pre-existing points.
Additionally, RT-NeRF leverages a coarse-grained view-dependent computing
ordering scheme for eliminating the (unnecessary) processing of invisible
points. On the hardware level, our proposed RT-NeRF accelerator (1) adopts a
hybrid encoding scheme to adaptively switch between a bitmap- or
coordinate-based sparsity encoding format for NeRF's sparse embeddings, aiming
to maximize the storage savings and thus reduce the required DRAM accesses
while supporting efficient NeRF decoding; and (2) integrates both a
dual-purpose bi-direction adder & search tree and a high-density sparse search
unit to coordinate the two aforementioned encoding formats. Extensive
experiments on eight datasets consistently validate the effectiveness of
RT-NeRF, achieving a large throughput improvement (e.g., 9.7x - 3,201x) while
maintaining the rendering quality as compared with SOTA efficient NeRF
solutions.";Chaojian Li<author:sep>Sixu Li<author:sep>Yang Zhao<author:sep>Wenbo Zhu<author:sep>Yingyan Lin;http://arxiv.org/pdf/2212.01120v1;cs.AR;Accepted to ICCAD 2022;nerf
2212.01368v2;http://arxiv.org/abs/2212.01368v2;2022-12-02;Fast Non-Rigid Radiance Fields from Monocularized Data;"The reconstruction and novel view synthesis of dynamic scenes recently gained
increased attention. As reconstruction from large-scale multi-view data
involves immense memory and computational requirements, recent benchmark
datasets provide collections of single monocular views per timestamp sampled
from multiple (virtual) cameras. We refer to this form of inputs as
""monocularized"" data. Existing work shows impressive results for synthetic
setups and forward-facing real-world data, but is often limited in the training
speed and angular range for generating novel views. This paper addresses these
limitations and proposes a new method for full 360{\deg} inward-facing novel
view synthesis of non-rigidly deforming scenes. At the core of our method are:
1) An efficient deformation module that decouples the processing of spatial and
temporal information for accelerated training and inference; and 2) A static
module representing the canonical scene as a fast hash-encoded neural radiance
field. In addition to existing synthetic monocularized data, we systematically
analyze the performance on real-world inward-facing scenes using a newly
recorded challenging dataset sampled from a synchronized large-scale multi-view
rig. In both cases, our method is significantly faster than previous methods,
converging in less than 7 minutes and achieving real-time framerates at 1K
resolution, while obtaining a higher visual accuracy for generated novel views.
Our source code and data is available at our project page
https://graphics.tu-bs.de/publications/kappel2022fast.";Moritz Kappel<author:sep>Vladislav Golyanik<author:sep>Susana Castillo<author:sep>Christian Theobalt<author:sep>Marcus Magnor;http://arxiv.org/pdf/2212.01368v2;cs.CV;"18 pages, 14 figures; project page:
  https://graphics.tu-bs.de/publications/kappel2022fast";
2212.01103v2;http://arxiv.org/abs/2212.01103v2;2022-12-02;3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation;"Text-guided 3D object generation aims to generate 3D objects described by
user-defined captions, which paves a flexible way to visualize what we
imagined. Although some works have been devoted to solving this challenging
task, these works either utilize some explicit 3D representations (e.g., mesh),
which lack texture and require post-processing for rendering photo-realistic
views; or require individual time-consuming optimization for every single case.
Here, we make the first attempt to achieve generic text-guided cross-category
3D object generation via a new 3D-TOGO model, which integrates a text-to-views
generation module and a views-to-3D generation module. The text-to-views
generation module is designed to generate different views of the target 3D
object given an input caption. prior-guidance, caption-guidance and view
contrastive learning are proposed for achieving better view-consistency and
caption similarity. Meanwhile, a pixelNeRF model is adopted for the views-to-3D
generation module to obtain the implicit 3D neural representation from the
previously-generated views. Our 3D-TOGO model generates 3D objects in the form
of the neural radiance field with good texture and requires no time-cost
optimization for every single caption. Besides, 3D-TOGO can control the
category, color and shape of generated 3D objects with the input caption.
Extensive experiments on the largest 3D object dataset (i.e., ABO) are
conducted to verify that 3D-TOGO can better generate high-quality 3D objects
according to the input captions across 98 different categories, in terms of
PSNR, SSIM, LPIPS and CLIP-score, compared with text-NeRF and Dreamfields.";Zutao Jiang<author:sep>Guansong Lu<author:sep>Xiaodan Liang<author:sep>Jihua Zhu<author:sep>Wei Zhang<author:sep>Xiaojun Chang<author:sep>Hang Xu;http://arxiv.org/pdf/2212.01103v2;cs.CV;;nerf
2212.00914v1;http://arxiv.org/abs/2212.00914v1;2022-12-02;QFF: Quantized Fourier Features for Neural Field Representations;"Multilayer perceptrons (MLPs) learn high frequencies slowly. Recent
approaches encode features in spatial bins to improve speed of learning
details, but at the cost of larger model size and loss of continuity. Instead,
we propose to encode features in bins of Fourier features that are commonly
used for positional encoding. We call these Quantized Fourier Features (QFF).
As a naturally multiresolution and periodic representation, our experiments
show that using QFF can result in smaller model size, faster training, and
better quality outputs for several applications, including Neural Image
Representations (NIR), Neural Radiance Field (NeRF) and Signed Distance
Function (SDF) modeling. QFF are easy to code, fast to compute, and serve as a
simple drop-in addition to many neural field representations.";Jae Yong Lee<author:sep>Yuqun Wu<author:sep>Chuhang Zou<author:sep>Shenlong Wang<author:sep>Derek Hoiem;http://arxiv.org/pdf/2212.00914v1;cs.CV;;nerf
2212.00436v1;http://arxiv.org/abs/2212.00436v1;2022-12-01;ViewNeRF: Unsupervised Viewpoint Estimation Using Category-Level Neural  Radiance Fields;"We introduce ViewNeRF, a Neural Radiance Field-based viewpoint estimation
method that learns to predict category-level viewpoints directly from images
during training. While NeRF is usually trained with ground-truth camera poses,
multiple extensions have been proposed to reduce the need for this expensive
supervision. Nonetheless, most of these methods still struggle in complex
settings with large camera movements, and are restricted to single scenes, i.e.
they cannot be trained on a collection of scenes depicting the same object
category. To address these issues, our method uses an analysis by synthesis
approach, combining a conditional NeRF with a viewpoint predictor and a scene
encoder in order to produce self-supervised reconstructions for whole object
categories. Rather than focusing on high fidelity reconstruction, we target
efficient and accurate viewpoint prediction in complex scenarios, e.g.
360{\deg} rotation on real data. Our model shows competitive results on
synthetic and real datasets, both for single scenes and multi-instance
collections.";Octave Mariotti<author:sep>Oisin Mac Aodha<author:sep>Hakan Bilen;http://arxiv.org/pdf/2212.00436v1;cs.CV;;nerf
2212.00190v2;http://arxiv.org/abs/2212.00190v2;2022-12-01;Mixed Neural Voxels for Fast Multi-view Video Synthesis;"Synthesizing high-fidelity videos from real-world multi-view input is
challenging because of the complexities of real-world environments and highly
dynamic motions. Previous works based on neural radiance fields have
demonstrated high-quality reconstructions of dynamic scenes. However, training
such models on real-world scenes is time-consuming, usually taking days or
weeks. In this paper, we present a novel method named MixVoxels to better
represent the dynamic scenes with fast training speed and competitive rendering
qualities. The proposed MixVoxels represents the 4D dynamic scenes as a mixture
of static and dynamic voxels and processes them with different networks. In
this way, the computation of the required modalities for static voxels can be
processed by a lightweight model, which essentially reduces the amount of
computation, especially for many daily dynamic scenes dominated by the static
background. To separate the two kinds of voxels, we propose a novel variation
field to estimate the temporal variance of each voxel. For the dynamic voxels,
we design an inner-product time query method to efficiently query multiple time
steps, which is essential to recover the high-dynamic motions. As a result,
with 15 minutes of training for dynamic scenes with inputs of 300-frame videos,
MixVoxels achieves better PSNR than previous methods. Codes and trained models
are available at https://github.com/fengres/mixvoxels";Feng Wang<author:sep>Sinan Tan<author:sep>Xinghang Li<author:sep>Zeyue Tian<author:sep>Yafei Song<author:sep>Huaping Liu;http://arxiv.org/pdf/2212.00190v2;cs.CV;ICCV 2023 (Oral);
2211.17235v1;http://arxiv.org/abs/2211.17235v1;2022-11-30;NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real  Image Animation;"Nerf-based Generative models have shown impressive capacity in generating
high-quality images with consistent 3D geometry. Despite successful synthesis
of fake identity images randomly sampled from latent space, adopting these
models for generating face images of real subjects is still a challenging task
due to its so-called inversion issue. In this paper, we propose a universal
method to surgically fine-tune these NeRF-GAN models in order to achieve
high-fidelity animation of real subjects only by a single image. Given the
optimized latent code for an out-of-domain real image, we employ 2D loss
functions on the rendered image to reduce the identity gap. Furthermore, our
method leverages explicit and implicit 3D regularizations using the in-domain
neighborhood samples around the optimized latent code to remove geometrical and
visual artifacts. Our experiments confirm the effectiveness of our method in
realistic, high-fidelity, and 3D consistent animation of real faces on multiple
NeRF-GAN models across different datasets.";Yu Yin<author:sep>Kamran Ghasedi<author:sep>HsiangTao Wu<author:sep>Jiaolong Yang<author:sep>Xin Tong<author:sep>Yun Fu;http://arxiv.org/pdf/2211.17235v1;cs.CV;;nerf
2211.16630v2;http://arxiv.org/abs/2211.16630v2;2022-11-29;DINER: Depth-aware Image-based NEural Radiance fields;"We present Depth-aware Image-based NEural Radiance fields (DINER). Given a
sparse set of RGB input views, we predict depth and feature maps to guide the
reconstruction of a volumetric scene representation that allows us to render 3D
objects under novel views. Specifically, we propose novel techniques to
incorporate depth information into feature fusion and efficient scene sampling.
In comparison to the previous state of the art, DINER achieves higher synthesis
quality and can process input views with greater disparity. This allows us to
capture scenes more completely without changing capturing hardware requirements
and ultimately enables larger viewpoint changes during novel view synthesis. We
evaluate our method by synthesizing novel views, both for human heads and for
general objects, and observe significantly improved qualitative results and
increased perceptual metrics compared to the previous state of the art. The
code is publicly available for research purposes.";Malte Prinzler<author:sep>Otmar Hilliges<author:sep>Justus Thies;http://arxiv.org/pdf/2211.16630v2;cs.CV;"Website: https://malteprinzler.github.io/projects/diner/diner.html ;
  Video: https://www.youtube.com/watch?v=iI_fpjY5k8Y&t=1s";
2211.16431v2;http://arxiv.org/abs/2211.16431v2;2022-11-29;NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with  360Â° Views;"Virtual reality and augmented reality (XR) bring increasing demand for 3D
content. However, creating high-quality 3D content requires tedious work that a
human expert must do. In this work, we study the challenging task of lifting a
single image to a 3D object and, for the first time, demonstrate the ability to
generate a plausible 3D object with 360{\deg} views that correspond well with
the given reference image. By conditioning on the reference image, our model
can fulfill the everlasting curiosity for synthesizing novel views of objects
from images. Our technique sheds light on a promising direction of easing the
workflows for 3D artists and XR designers. We propose a novel framework, dubbed
NeuralLift-360, that utilizes a depth-aware neural radiance representation
(NeRF) and learns to craft the scene guided by denoising diffusion models. By
introducing a ranking loss, our NeuralLift-360 can be guided with rough depth
estimation in the wild. We also adopt a CLIP-guided sampling strategy for the
diffusion prior to provide coherent guidance. Extensive experiments demonstrate
that our NeuralLift-360 significantly outperforms existing state-of-the-art
baselines. Project page: https://vita-group.github.io/NeuralLift-360/";Dejia Xu<author:sep>Yifan Jiang<author:sep>Peihao Wang<author:sep>Zhiwen Fan<author:sep>Yi Wang<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2211.16431v2;cs.CV;Project page: https://vita-group.github.io/NeuralLift-360/;nerf
2211.16386v1;http://arxiv.org/abs/2211.16386v1;2022-11-29;Compressing Volumetric Radiance Fields to 1 MB;"Approximating radiance fields with volumetric grids is one of promising
directions for improving NeRF, represented by methods like Plenoxels and DVGO,
which achieve super-fast training convergence and real-time rendering. However,
these methods typically require a tremendous storage overhead, costing up to
hundreds of megabytes of disk space and runtime memory for a single scene. We
address this issue in this paper by introducing a simple yet effective
framework, called vector quantized radiance fields (VQRF), for compressing
these volume-grid-based radiance fields. We first present a robust and adaptive
metric for estimating redundancy in grid models and performing voxel pruning by
better exploring intermediate outputs of volumetric rendering. A trainable
vector quantization is further proposed to improve the compactness of grid
models. In combination with an efficient joint tuning strategy and
post-processing, our method can achieve a compression ratio of 100$\times$ by
reducing the overall model size to 1 MB with negligible loss on visual quality.
Extensive experiments demonstrate that the proposed framework is capable of
achieving unrivaled performance and well generalization across multiple methods
with distinct volumetric structures, facilitating the wide use of volumetric
radiance fields methods in real-world applications. Code Available at
\url{https://github.com/AlgoHunt/VQRF}";Lingzhi Li<author:sep>Zhen Shen<author:sep>Zhongshu Wang<author:sep>Li Shen<author:sep>Liefeng Bo;http://arxiv.org/pdf/2211.16386v1;cs.CV;;nerf
2211.15977v3;http://arxiv.org/abs/2211.15977v3;2022-11-29;One is All: Bridging the Gap Between Neural Radiance Fields  Architectures with Progressive Volume Distillation;"Neural Radiance Fields (NeRF) methods have proved effective as compact,
high-quality and versatile representations for 3D scenes, and enable downstream
tasks such as editing, retrieval, navigation, etc. Various neural architectures
are vying for the core structure of NeRF, including the plain Multi-Layer
Perceptron (MLP), sparse tensors, low-rank tensors, hashtables and their
compositions. Each of these representations has its particular set of
trade-offs. For example, the hashtable-based representations admit faster
training and rendering but their lack of clear geometric meaning hampers
downstream tasks like spatial-relation-aware editing. In this paper, we propose
Progressive Volume Distillation (PVD), a systematic distillation method that
allows any-to-any conversions between different architectures, including MLP,
sparse or low-rank tensors, hashtables and their compositions. PVD consequently
empowers downstream applications to optimally adapt the neural representations
for the task at hand in a post hoc fashion. The conversions are fast, as
distillation is progressively performed on different levels of volume
representations, from shallower to deeper. We also employ special treatment of
density to deal with its specific numerical instability problem. Empirical
evidence is presented to validate our method on the NeRF-Synthetic, LLFF and
TanksAndTemples datasets. For example, with PVD, an MLP-based NeRF model can be
distilled from a hashtable-based Instant-NGP model at a 10X~20X faster speed
than being trained the original NeRF from scratch, while achieving a superior
level of synthesis quality. Code is available at
https://github.com/megvii-research/AAAI2023-PVD.";Shuangkang Fang<author:sep>Weixin Xu<author:sep>Heng Wang<author:sep>Yi Yang<author:sep>Yufeng Wang<author:sep>Shuchang Zhou;http://arxiv.org/pdf/2211.15977v3;cs.CV;Accepted by AAAI2023. Project Page: https://sk-fun.fun/PVD;nerf
2211.16193v2;http://arxiv.org/abs/2211.16193v2;2022-11-28;In-Hand 3D Object Scanning from an RGB Sequence;"We propose a method for in-hand 3D scanning of an unknown object with a
monocular camera. Our method relies on a neural implicit surface representation
that captures both the geometry and the appearance of the object, however, by
contrast with most NeRF-based methods, we do not assume that the camera-object
relative poses are known. Instead, we simultaneously optimize both the object
shape and the pose trajectory. As direct optimization over all shape and pose
parameters is prone to fail without coarse-level initialization, we propose an
incremental approach that starts by splitting the sequence into carefully
selected overlapping segments within which the optimization is likely to
succeed. We reconstruct the object shape and track its poses independently
within each segment, then merge all the segments before performing a global
optimization. We show that our method is able to reconstruct the shape and
color of both textured and challenging texture-less objects, outperforms
classical methods that rely only on appearance features, and that its
performance is close to recent methods that assume known camera poses.";Shreyas Hampali<author:sep>Tomas Hodan<author:sep>Luan Tran<author:sep>Lingni Ma<author:sep>Cem Keskin<author:sep>Vincent Lepetit;http://arxiv.org/pdf/2211.16193v2;cs.CV;CVPR 2023;nerf
2211.15064v2;http://arxiv.org/abs/2211.15064v2;2022-11-28;High-fidelity Facial Avatar Reconstruction from Monocular Video with  Generative Priors;"High-fidelity facial avatar reconstruction from a monocular video is a
significant research problem in computer graphics and computer vision.
Recently, Neural Radiance Field (NeRF) has shown impressive novel view
rendering results and has been considered for facial avatar reconstruction.
However, the complex facial dynamics and missing 3D information in monocular
videos raise significant challenges for faithful facial reconstruction. In this
work, we propose a new method for NeRF-based facial avatar reconstruction that
utilizes 3D-aware generative prior. Different from existing works that depend
on a conditional deformation field for dynamic modeling, we propose to learn a
personalized generative prior, which is formulated as a local and low
dimensional subspace in the latent space of 3D-GAN. We propose an efficient
method to construct the personalized generative prior based on a small set of
facial images of a given individual. After learning, it allows for
photo-realistic rendering with novel views and the face reenactment can be
realized by performing navigation in the latent space. Our proposed method is
applicable for different driven signals, including RGB images, 3DMM
coefficients, and audios. Compared with existing works, we obtain superior
novel view synthesis results and faithfully face reenactment performance.";Yunpeng Bai<author:sep>Yanbo Fan<author:sep>Xuan Wang<author:sep>Yong Zhang<author:sep>Jingxiang Sun<author:sep>Chun Yuan<author:sep>Ying Shan;http://arxiv.org/pdf/2211.15064v2;cs.CV;8 pages, 7 figures;nerf
2211.14879v1;http://arxiv.org/abs/2211.14879v1;2022-11-27;SuNeRF: Validation of a 3D Global Reconstruction of the Solar Corona  Using Simulated EUV Images;"Extreme Ultraviolet (EUV) light emitted by the Sun impacts satellite
operations and communications and affects the habitability of planets.
Currently, EUV-observing instruments are constrained to viewing the Sun from
its equator (i.e., ecliptic), limiting our ability to forecast EUV emission for
other viewpoints (e.g. solar poles), and to generalize our knowledge of the
Sun-Earth system to other host stars. In this work, we adapt Neural Radiance
Fields (NeRFs) to the physical properties of the Sun and demonstrate that
non-ecliptic viewpoints could be reconstructed from observations limited to the
solar ecliptic. To validate our approach, we train on simulations of solar EUV
emission that provide a ground truth for all viewpoints. Our model accurately
reconstructs the simulated 3D structure of the Sun, achieving a peak
signal-to-noise ratio of 43.3 dB and a mean absolute relative error of 0.3\%
for non-ecliptic viewpoints. Our method provides a consistent 3D reconstruction
of the Sun from a limited number of viewpoints, thus highlighting the potential
to create a virtual instrument for satellite observations of the Sun. Its
extension to real observations will provide the missing link to compare the Sun
to other stars and to improve space-weather forecasting.";Kyriaki-Margarita Bintsi<author:sep>Robert Jarolim<author:sep>Benoit Tremblay<author:sep>Miraflor Santos<author:sep>Anna Jungbluth<author:sep>James Paul Mason<author:sep>Sairam Sundaresan<author:sep>Angelos Vourlidas<author:sep>Cooper Downs<author:sep>Ronald M. Caplan<author:sep>AndrÃ©s MuÃ±oz Jaramillo;http://arxiv.org/pdf/2211.14879v1;astro-ph.SR;"Accepted at Machine Learning and the Physical Sciences workshop,
  NeurIPS 2022";nerf
2211.14823v2;http://arxiv.org/abs/2211.14823v2;2022-11-27;3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer  Avenue;"This paper studies how to flexibly integrate reconstructed 3D models into
practical 3D modeling pipelines such as 3D scene creation and rendering. Due to
the technical difficulty, one can only obtain rough 3D models (R3DMs) for most
real objects using existing 3D reconstruction techniques. As a result,
physically-based rendering (PBR) would render low-quality images or videos for
scenes that are constructed by R3DMs. One promising solution would be
representing real-world objects as Neural Fields such as NeRFs, which are able
to generate photo-realistic renderings of an object under desired viewpoints.
However, a drawback is that the synthesized views through Neural Fields
Rendering (NFR) cannot reflect the simulated lighting details on R3DMs in PBR
pipelines, especially when object interactions in the 3D scene creation cause
local shadows. To solve this dilemma, we propose a lighting transfer network
(LighTNet) to bridge NFR and PBR, such that they can benefit from each other.
LighTNet reasons about a simplified image composition model, remedies the
uneven surface issue caused by R3DMs, and is empowered by several
perceptual-motivated constraints and a new Lab angle loss which enhances the
contrast between lighting strength and colors. Comparisons demonstrate that
LighTNet is superior in synthesizing impressive lighting, and is promising in
pushing NFR further in practical 3D modeling workflows. Project page:
https://3d-front-future.github.io/LighTNet .";Yujie Li<author:sep>Bowen Cai<author:sep>Yuqin Liang<author:sep>Rongfei Jia<author:sep>Binqiang Zhao<author:sep>Mingming Gong<author:sep>Huan Fu;http://arxiv.org/pdf/2211.14823v2;cs.CV;;nerf
2211.14799v1;http://arxiv.org/abs/2211.14799v1;2022-11-27;Sampling Neural Radiance Fields for Refractive Objects;"Recently, differentiable volume rendering in neural radiance fields (NeRF)
has gained a lot of popularity, and its variants have attained many impressive
results. However, existing methods usually assume the scene is a homogeneous
volume so that a ray is cast along the straight path. In this work, the scene
is instead a heterogeneous volume with a piecewise-constant refractive index,
where the path will be curved if it intersects the different refractive
indices. For novel view synthesis of refractive objects, our NeRF-based
framework aims to optimize the radiance fields of bounded volume and boundary
from multi-view posed images with refractive object silhouettes. To tackle this
challenging problem, the refractive index of a scene is reconstructed from
silhouettes. Given the refractive index, we extend the stratified and
hierarchical sampling techniques in NeRF to allow drawing samples along a
curved path tracked by the Eikonal equation. The results indicate that our
framework outperforms the state-of-the-art method both quantitatively and
qualitatively, demonstrating better performance on the perceptual similarity
metric and an apparent improvement in the rendering quality on several
synthetic and real scenes.";Jen-I Pan<author:sep>Jheng-Wei Su<author:sep>Kai-Wen Hsiao<author:sep>Ting-Yu Yen<author:sep>Hung-Kuo Chu;http://arxiv.org/pdf/2211.14799v1;cs.CV;"SIGGRAPH Asia 2022 Technical Communications. 4 pages, 4 figures, 1
  table. Project: https://alexkeroro86.github.io/SampleNeRFRO/ Code:
  https://github.com/alexkeroro86/SampleNeRFRO";nerf
2211.16211v3;http://arxiv.org/abs/2211.16211v3;2022-11-26;ResNeRF: Geometry-Guided Residual Neural Radiance Field for Indoor Scene  Novel View Synthesis;"We represent the ResNeRF, a novel geometry-guided two-stage framework for
indoor scene novel view synthesis. Be aware of that a good geometry would
greatly boost the performance of novel view synthesis, and to avoid the
geometry ambiguity issue, we propose to characterize the density distribution
of the scene based on a base density estimated from scene geometry and a
residual density parameterized by the geometry. In the first stage, we focus on
geometry reconstruction based on SDF representation, which would lead to a good
geometry surface of the scene and also a sharp density. In the second stage,
the residual density is learned based on the SDF learned in the first stage for
encoding more details about the appearance. In this way, our method can better
learn the density distribution with the geometry prior for high-fidelity novel
view synthesis while preserving the 3D structures. Experiments on large-scale
indoor scenes with many less-observed and textureless areas show that with the
good 3D surface, our method achieves state-of-the-art performance for novel
view synthesis.";Yuting Xiao<author:sep>Yiqun Zhao<author:sep>Yanyu Xu<author:sep>Shenghua Gao;http://arxiv.org/pdf/2211.16211v3;cs.CV;This is an incomplete paper;nerf
2211.14086v2;http://arxiv.org/abs/2211.14086v2;2022-11-25;ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision;"By supervising camera rays between a scene and multi-view image planes, NeRF
reconstructs a neural scene representation for the task of novel view
synthesis. On the other hand, shadow rays between the light source and the
scene have yet to be considered. Therefore, we propose a novel shadow ray
supervision scheme that optimizes both the samples along the ray and the ray
location. By supervising shadow rays, we successfully reconstruct a neural SDF
of the scene from single-view images under multiple lighting conditions. Given
single-view binary shadows, we train a neural network to reconstruct a complete
scene not limited by the camera's line of sight. By further modeling the
correlation between the image colors and the shadow rays, our technique can
also be effectively extended to RGB inputs. We compare our method with previous
works on challenging tasks of shape reconstruction from single-view binary
shadow or RGB images and observe significant improvements. The code and data
are available at https://github.com/gerwang/ShadowNeuS.";Jingwang Ling<author:sep>Zhibo Wang<author:sep>Feng Xu;http://arxiv.org/pdf/2211.14086v2;cs.CV;CVPR 2023. Project page: https://gerwang.github.io/shadowneus/;nerf
2211.13887v1;http://arxiv.org/abs/2211.13887v1;2022-11-25;TPA-Net: Generate A Dataset for Text to Physics-based Animation;"Recent breakthroughs in Vision-Language (V&L) joint research have achieved
remarkable results in various text-driven tasks. High-quality Text-to-video
(T2V), a task that has been long considered mission-impossible, was proven
feasible with reasonably good results in latest works. However, the resulting
videos often have undesired artifacts largely because the system is purely
data-driven and agnostic to the physical laws. To tackle this issue and further
push T2V towards high-level physical realism, we present an autonomous data
generation technique and a dataset, which intend to narrow the gap with a large
number of multi-modal, 3D Text-to-Video/Simulation (T2V/S) data. In the
dataset, we provide high-resolution 3D physical simulations for both solids and
fluids, along with textual descriptions of the physical phenomena. We take
advantage of state-of-the-art physical simulation methods (i) Incremental
Potential Contact (IPC) and (ii) Material Point Method (MPM) to simulate
diverse scenarios, including elastic deformations, material fractures,
collisions, turbulence, etc. Additionally, high-quality, multi-view rendering
videos are supplied for the benefit of T2V, Neural Radiance Fields (NeRF), and
other communities. This work is the first step towards fully automated
Text-to-Video/Simulation (T2V/S). Live examples and subsequent work are at
https://sites.google.com/view/tpa-net.";Yuxing Qiu<author:sep>Feng Gao<author:sep>Minchen Li<author:sep>Govind Thattai<author:sep>Yin Yang<author:sep>Chenfanfu Jiang;http://arxiv.org/pdf/2211.13887v1;cs.AI;;nerf
2211.14108v3;http://arxiv.org/abs/2211.14108v3;2022-11-25;3DDesigner: Towards Photorealistic 3D Object Generation and Editing with  Text-guided Diffusion Models;"Text-guided diffusion models have shown superior performance in image/video
generation and editing. While few explorations have been performed in 3D
scenarios. In this paper, we discuss three fundamental and interesting problems
on this topic. First, we equip text-guided diffusion models to achieve
3D-consistent generation. Specifically, we integrate a NeRF-like neural field
to generate low-resolution coarse results for a given camera view. Such results
can provide 3D priors as condition information for the following diffusion
process. During denoising diffusion, we further enhance the 3D consistency by
modeling cross-view correspondences with a novel two-stream (corresponding to
two different views) asynchronous diffusion process. Second, we study 3D local
editing and propose a two-step solution that can generate 360-degree
manipulated results by editing an object from a single view. Step 1, we propose
to perform 2D local editing by blending the predicted noises. Step 2, we
conduct a noise-to-text inversion process that maps 2D blended noises into the
view-independent text embedding space. Once the corresponding text embedding is
obtained, 360-degree images can be generated. Last but not least, we extend our
model to perform one-shot novel view synthesis by fine-tuning on a single
image, firstly showing the potential of leveraging text guidance for novel view
synthesis. Extensive experiments and various applications show the prowess of
our 3DDesigner. The project page is available at
https://3ddesigner-diffusion.github.io/.";Gang Li<author:sep>Heliang Zheng<author:sep>Chaoyue Wang<author:sep>Chang Li<author:sep>Changwen Zheng<author:sep>Dacheng Tao;http://arxiv.org/pdf/2211.14108v3;cs.CV;Submitted to IJCV;nerf
2211.13969v2;http://arxiv.org/abs/2211.13969v2;2022-11-25;Unsupervised Continual Semantic Adaptation through Neural Rendering;"An increasing amount of applications rely on data-driven models that are
deployed for perception tasks across a sequence of scenes. Due to the mismatch
between training and deployment data, adapting the model on the new scenes is
often crucial to obtain good performance. In this work, we study continual
multi-scene adaptation for the task of semantic segmentation, assuming that no
ground-truth labels are available during deployment and that performance on the
previous scenes should be maintained. We propose training a Semantic-NeRF
network for each scene by fusing the predictions of a segmentation model and
then using the view-consistent rendered semantic labels as pseudo-labels to
adapt the model. Through joint training with the segmentation model, the
Semantic-NeRF model effectively enables 2D-3D knowledge transfer. Furthermore,
due to its compact size, it can be stored in a long-term memory and
subsequently used to render data from arbitrary viewpoints to reduce
forgetting. We evaluate our approach on ScanNet, where we outperform both a
voxel-based baseline and a state-of-the-art unsupervised domain adaptation
method.";Zhizheng Liu<author:sep>Francesco Milano<author:sep>Jonas Frey<author:sep>Roland Siegwart<author:sep>Hermann Blum<author:sep>Cesar Cadena;http://arxiv.org/pdf/2211.13969v2;cs.CV;"Accepted by the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2023. Zhizheng Liu and Francesco Milano share first
  authorship. Hermann Blum and Cesar Cadena share senior authorship. 18 pages,
  8 figures, 9 tables";nerf
2211.13994v1;http://arxiv.org/abs/2211.13994v1;2022-11-25;Dynamic Neural Portraits;"We present Dynamic Neural Portraits, a novel approach to the problem of
full-head reenactment. Our method generates photo-realistic video portraits by
explicitly controlling head pose, facial expressions and eye gaze. Our proposed
architecture is different from existing methods that rely on GAN-based
image-to-image translation networks for transforming renderings of 3D faces
into photo-realistic images. Instead, we build our system upon a 2D
coordinate-based MLP with controllable dynamics. Our intuition to adopt a
2D-based representation, as opposed to recent 3D NeRF-like systems, stems from
the fact that video portraits are captured by monocular stationary cameras,
therefore, only a single viewpoint of the scene is available. Primarily, we
condition our generative model on expression blendshapes, nonetheless, we show
that our system can be successfully driven by audio features as well. Our
experiments demonstrate that the proposed method is 270 times faster than
recent NeRF-based reenactment methods, with our networks achieving speeds of 24
fps for resolutions up to 1024 x 1024, while outperforming prior works in terms
of visual quality.";Michail Christos Doukas<author:sep>Stylianos Ploumpis<author:sep>Stefanos Zafeiriou;http://arxiv.org/pdf/2211.13994v1;cs.CV;"In IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV) 2023";nerf
2211.13762v2;http://arxiv.org/abs/2211.13762v2;2022-11-24;ScanNeRF: a Scalable Benchmark for Neural Radiance Fields;"In this paper, we propose the first-ever real benchmark thought for
evaluating Neural Radiance Fields (NeRFs) and, in general, Neural Rendering
(NR) frameworks. We design and implement an effective pipeline for scanning
real objects in quantity and effortlessly. Our scan station is built with less
than 500$ hardware budget and can collect roughly 4000 images of a scanned
object in just 5 minutes. Such a platform is used to build ScanNeRF, a dataset
characterized by several train/val/test splits aimed at benchmarking the
performance of modern NeRF methods under different conditions. Accordingly, we
evaluate three cutting-edge NeRF variants on it to highlight their strengths
and weaknesses. The dataset is available on our project page, together with an
online benchmark to foster the development of better and better NeRFs.";Luca De Luigi<author:sep>Damiano Bolognini<author:sep>Federico Domeniconi<author:sep>Daniele De Gregorio<author:sep>Matteo Poggi<author:sep>Luigi Di Stefano;http://arxiv.org/pdf/2211.13762v2;cs.CV;"WACV 2023. The first three authors contributed equally. Project page:
  https://eyecan-ai.github.io/scannerf/";nerf
2211.13494v1;http://arxiv.org/abs/2211.13494v1;2022-11-24;Immersive Neural Graphics Primitives;"Neural radiance field (NeRF), in particular its extension by instant neural
graphics primitives, is a novel rendering method for view synthesis that uses
real-world images to build photo-realistic immersive virtual scenes. Despite
its potential, research on the combination of NeRF and virtual reality (VR)
remains sparse. Currently, there is no integration into typical VR systems
available, and the performance and suitability of NeRF implementations for VR
have not been evaluated, for instance, for different scene complexities or
screen resolutions. In this paper, we present and evaluate a NeRF-based
framework that is capable of rendering scenes in immersive VR allowing users to
freely move their heads to explore complex real-world scenes. We evaluate our
framework by benchmarking three different NeRF scenes concerning their
rendering performance at different scene complexities and resolutions.
Utilizing super-resolution, our approach can yield a frame rate of 30 frames
per second with a resolution of 1280x720 pixels per eye. We discuss potential
applications of our framework and provide an open source implementation online.";Ke Li<author:sep>Tim Rolff<author:sep>Susanne Schmidt<author:sep>Reinhard Bacher<author:sep>Simone Frintrop<author:sep>Wim Leemans<author:sep>Frank Steinicke;http://arxiv.org/pdf/2211.13494v1;cs.CV;Submitted to IEEE VR, currently under review;nerf
2211.13206v3;http://arxiv.org/abs/2211.13206v3;2022-11-23;AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural  Voxels;"With NeRF widely used for facial reenactment, recent methods can recover
photo-realistic 3D head avatar from just a monocular video. Unfortunately, the
training process of the NeRF-based methods is quite time-consuming, as MLP used
in the NeRF-based methods is inefficient and requires too many iterations to
converge. To overcome this problem, we propose AvatarMAV, a fast 3D head avatar
reconstruction method using Motion-Aware Neural Voxels. AvatarMAV is the first
to model both the canonical appearance and the decoupled expression motion by
neural voxels for head avatar. In particular, the motion-aware neural voxels is
generated from the weighted concatenation of multiple 4D tensors. The 4D
tensors semantically correspond one-to-one with 3DMM expression basis and share
the same weights as 3DMM expression coefficients. Benefiting from our novel
representation, the proposed AvatarMAV can recover photo-realistic head avatars
in just 5 minutes (implemented with pure PyTorch), which is significantly
faster than the state-of-the-art facial reenactment methods. Project page:
https://www.liuyebin.com/avatarmav.";Yuelang Xu<author:sep>Lizhen Wang<author:sep>Xiaochen Zhao<author:sep>Hongwen Zhang<author:sep>Yebin Liu;http://arxiv.org/pdf/2211.13206v3;cs.CV;Accepted by SIGGRAPH 2023;nerf
2211.12853v2;http://arxiv.org/abs/2211.12853v2;2022-11-23;BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields;"Neural Radiance Fields (NeRF) have received considerable attention recently,
due to its impressive capability in photo-realistic 3D reconstruction and novel
view synthesis, given a set of posed camera images. Earlier work usually
assumes the input images are of good quality. However, image degradation (e.g.
image motion blur in low-light conditions) can easily happen in real-world
scenarios, which would further affect the rendering quality of NeRF. In this
paper, we present a novel bundle adjusted deblur Neural Radiance Fields
(BAD-NeRF), which can be robust to severe motion blurred images and inaccurate
camera poses. Our approach models the physical image formation process of a
motion blurred image, and jointly learns the parameters of NeRF and recovers
the camera motion trajectories during exposure time. In experiments, we show
that by directly modeling the real physical image formation process, BAD-NeRF
achieves superior performance over prior works on both synthetic and real
datasets. Code and data are available at https://github.com/WU-CVGL/BAD-NeRF.";Peng Wang<author:sep>Lingzhe Zhao<author:sep>Ruijie Ma<author:sep>Peidong Liu;http://arxiv.org/pdf/2211.12853v2;cs.CV;"Accepted to CVPR 2023, Project page:
  https://wangpeng000.github.io/BAD-NeRF/";nerf
2211.12656v1;http://arxiv.org/abs/2211.12656v1;2022-11-23;ActiveRMAP: Radiance Field for Active Mapping And Planning;"A high-quality 3D reconstruction of a scene from a collection of 2D images
can be achieved through offline/online mapping methods. In this paper, we
explore active mapping from the perspective of implicit representations, which
have recently produced compelling results in a variety of applications. One of
the most popular implicit representations - Neural Radiance Field (NeRF), first
demonstrated photorealistic rendering results using multi-layer perceptrons,
with promising offline 3D reconstruction as a by-product of the radiance field.
More recently, researchers also applied this implicit representation for online
reconstruction and localization (i.e. implicit SLAM systems). However, the
study on using implicit representation for active vision tasks is still very
limited. In this paper, we are particularly interested in applying the neural
radiance field for active mapping and planning problems, which are closely
coupled tasks in an active system. We, for the first time, present an RGB-only
active vision framework using radiance field representation for active 3D
reconstruction and planning in an online manner. Specifically, we formulate
this joint task as an iterative dual-stage optimization problem, where we
alternatively optimize for the radiance field representation and path planning.
Experimental results suggest that the proposed method achieves competitive
results compared to other offline methods and outperforms active reconstruction
methods using NeRFs.";Huangying Zhan<author:sep>Jiyang Zheng<author:sep>Yi Xu<author:sep>Ian Reid<author:sep>Hamid Rezatofighi;http://arxiv.org/pdf/2211.12656v1;cs.CV;Under review;nerf
2211.13251v2;http://arxiv.org/abs/2211.13251v2;2022-11-23;CGOF++: Controllable 3D Face Synthesis with Conditional Generative  Occupancy Fields;"Capitalizing on the recent advances in image generation models, existing
controllable face image synthesis methods are able to generate high-fidelity
images with some levels of controllability, e.g., controlling the shapes,
expressions, textures, and poses of the generated face images. However,
previous methods focus on controllable 2D image generative models, which are
prone to producing inconsistent face images under large expression and pose
changes. In this paper, we propose a new NeRF-based conditional 3D face
synthesis framework, which enables 3D controllability over the generated face
images by imposing explicit 3D conditions from 3D face priors. At its core is a
conditional Generative Occupancy Field (cGOF++) that effectively enforces the
shape of the generated face to conform to a given 3D Morphable Model (3DMM)
mesh, built on top of EG3D [1], a recent tri-plane-based generative model. To
achieve accurate control over fine-grained 3D face shapes of the synthesized
images, we additionally incorporate a 3D landmark loss as well as a volume
warping loss into our synthesis framework. Experiments validate the
effectiveness of the proposed method, which is able to generate high-fidelity
face images and shows more precise 3D controllability than state-of-the-art
2D-based controllable face synthesis methods.";Keqiang Sun<author:sep>Shangzhe Wu<author:sep>Ning Zhang<author:sep>Zhaoyang Huang<author:sep>Quan Wang<author:sep>Hongsheng Li;http://arxiv.org/pdf/2211.13251v2;cs.CV;"Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). This article is an extension of the NeurIPS'22 paper
  arXiv:2206.08361";nerf
2211.13226v3;http://arxiv.org/abs/2211.13226v3;2022-11-23;ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field;"Physical simulations produce excellent predictions of weather effects. Neural
radiance fields produce SOTA scene models. We describe a novel NeRF-editing
procedure that can fuse physical simulations with NeRF models of scenes,
producing realistic movies of physical phenomena in those scenes. Our
application -- Climate NeRF -- allows people to visualize what climate change
outcomes will do to them. ClimateNeRF allows us to render realistic weather
effects, including smog, snow, and flood. Results can be controlled with
physically meaningful variables like water level. Qualitative and quantitative
studies show that our simulated results are significantly more realistic than
those from SOTA 2D image editing and SOTA 3D NeRF stylization.";Yuan Li<author:sep>Zhi-Hao Lin<author:sep>David Forsyth<author:sep>Jia-Bin Huang<author:sep>Shenlong Wang;http://arxiv.org/pdf/2211.13226v3;cs.CV;project page: https://climatenerf.github.io/;nerf
2211.12758v1;http://arxiv.org/abs/2211.12758v1;2022-11-23;PANeRF: Pseudo-view Augmentation for Improved Neural Radiance Fields  Based on Few-shot Inputs;"The method of neural radiance fields (NeRF) has been developed in recent
years, and this technology has promising applications for synthesizing novel
views of complex scenes. However, NeRF requires dense input views, typically
numbering in the hundreds, for generating high-quality images. With a decrease
in the number of input views, the rendering quality of NeRF for unseen
viewpoints tends to degenerate drastically. To overcome this challenge, we
propose pseudo-view augmentation of NeRF, a scheme that expands a sufficient
amount of data by considering the geometry of few-shot inputs. We first
initialized the NeRF network by leveraging the expanded pseudo-views, which
efficiently minimizes uncertainty when rendering unseen views. Subsequently, we
fine-tuned the network by utilizing sparse-view inputs containing precise
geometry and color information. Through experiments under various settings, we
verified that our model faithfully synthesizes novel-view images of superior
quality and outperforms existing methods for multi-view datasets.";Young Chun Ahn<author:sep>Seokhwan Jang<author:sep>Sungheon Park<author:sep>Ji-Yeon Kim<author:sep>Nahyup Kang;http://arxiv.org/pdf/2211.12758v1;cs.CV;;nerf
2211.12436v2;http://arxiv.org/abs/2211.12436v2;2022-11-22;Dynamic Depth-Supervised NeRF for Multi-View RGB-D Operating Room Images;"The operating room (OR) is an environment of interest for the development of
sensing systems, enabling the detection of people, objects, and their semantic
relations. Due to frequent occlusions in the OR, these systems often rely on
input from multiple cameras. While increasing the number of cameras generally
increases algorithm performance, there are hard limitations to the number and
locations of cameras in the OR. Neural Radiance Fields (NeRF) can be used to
render synthetic views from arbitrary camera positions, virtually enlarging the
number of cameras in the dataset. In this work, we explore the use of NeRF for
view synthesis of dynamic scenes in the OR, and we show that regularisation
with depth supervision from RGB-D sensor data results in higher image quality.
We optimise a dynamic depth-supervised NeRF with up to six synchronised cameras
that capture the surgical field in five distinct phases before and during a
knee replacement surgery. We qualitatively inspect views rendered by a virtual
camera that moves 180 degrees around the surgical field at differing time
values. Quantitatively, we evaluate view synthesis from an unseen camera
position in terms of PSNR, SSIM and LPIPS for the colour channels and in MAE
and error percentage for the estimated depth. We find that NeRFs can be used to
generate geometrically consistent views, also from interpolated camera
positions and at interpolated time intervals. Views are generated from an
unseen camera pose with an average PSNR of 18.2 and a depth estimation error of
2.0%. Our results show the potential of a dynamic NeRF for view synthesis in
the OR and stress the relevance of depth supervision in a clinical setting.";Beerend G. A. Gerats<author:sep>Jelmer M. Wolterink<author:sep>Ivo A. M. J. Broeders;http://arxiv.org/pdf/2211.12436v2;cs.CV;Accepted to the Workshop on Ambient Intelligence for HealthCare 2023;nerf
2211.12368v1;http://arxiv.org/abs/2211.12368v1;2022-11-22;Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial  Decomposition;"While dynamic Neural Radiance Fields (NeRF) have shown success in
high-fidelity 3D modeling of talking portraits, the slow training and inference
speed severely obstruct their potential usage. In this paper, we propose an
efficient NeRF-based framework that enables real-time synthesizing of talking
portraits and faster convergence by leveraging the recent success of grid-based
NeRF. Our key insight is to decompose the inherently high-dimensional talking
portrait representation into three low-dimensional feature grids. Specifically,
a Decomposed Audio-spatial Encoding Module models the dynamic head with a 3D
spatial grid and a 2D audio grid. The torso is handled with another 2D grid in
a lightweight Pseudo-3D Deformable Module. Both modules focus on efficiency
under the premise of good rendering quality. Extensive experiments demonstrate
that our method can generate realistic and audio-lips synchronized talking
portrait videos, while also being highly efficient compared to previous
methods.";Jiaxiang Tang<author:sep>Kaisiyuan Wang<author:sep>Hang Zhou<author:sep>Xiaokang Chen<author:sep>Dongliang He<author:sep>Tianshu Hu<author:sep>Jingtuo Liu<author:sep>Gang Zeng<author:sep>Jingdong Wang;http://arxiv.org/pdf/2211.12368v1;cs.CV;Project page: https://me.kiui.moe/radnerf/;nerf
2211.12285v2;http://arxiv.org/abs/2211.12285v2;2022-11-22;Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for  Neural Radiance Fields;"Neural Radiance Fields (NeRF) have attracted significant attention due to
their ability to synthesize novel scene views with great accuracy. However,
inherent to their underlying formulation, the sampling of points along a ray
with zero width may result in ambiguous representations that lead to further
rendering artifacts such as aliasing in the final scene. To address this issue,
the recent variant mip-NeRF proposes an Integrated Positional Encoding (IPE)
based on a conical view frustum. Although this is expressed with an integral
formulation, mip-NeRF instead approximates this integral as the expected value
of a multivariate Gaussian distribution. This approximation is reliable for
short frustums but degrades with highly elongated regions, which arises when
dealing with distant scene objects under a larger depth of field. In this
paper, we explore the use of an exact approach for calculating the IPE by using
a pyramid-based integral formulation instead of an approximated conical-based
one. We denote this formulation as Exact-NeRF and contribute the first approach
to offer a precise analytical solution to the IPE within the NeRF domain. Our
exploratory work illustrates that such an exact formulation Exact-NeRF matches
the accuracy of mip-NeRF and furthermore provides a natural extension to more
challenging scenarios without further modification, such as in the case of
unbounded scenes. Our contribution aims to both address the hitherto unexplored
issues of frustum approximation in earlier NeRF work and additionally provide
insight into the potential future consideration of analytical solutions in
future NeRF extensions.";Brian K. S. Isaac-Medina<author:sep>Chris G. Willcocks<author:sep>Toby P. Breckon;http://arxiv.org/pdf/2211.12285v2;cs.CV;15 pages,10 figures;nerf
2211.12046v4;http://arxiv.org/abs/2211.12046v4;2022-11-22;DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors;"Neural Radiance Field (NeRF) has exhibited outstanding three-dimensional (3D)
reconstruction quality via the novel view synthesis from multi-view images and
paired calibrated camera parameters. However, previous NeRF-based systems have
been demonstrated under strictly controlled settings, with little attention
paid to less ideal scenarios, including with the presence of noise such as
exposure, illumination changes, and blur. In particular, though blur frequently
occurs in real situations, NeRF that can handle blurred images has received
little attention. The few studies that have investigated NeRF for blurred
images have not considered geometric and appearance consistency in 3D space,
which is one of the most important factors in 3D reconstruction. This leads to
inconsistency and the degradation of the perceptual quality of the constructed
scene. Hence, this paper proposes a DP-NeRF, a novel clean NeRF framework for
blurred images, which is constrained with two physical priors. These priors are
derived from the actual blurring process during image acquisition by the
camera. DP-NeRF proposes rigid blurring kernel to impose 3D consistency
utilizing the physical priors and adaptive weight proposal to refine the color
composition error in consideration of the relationship between depth and blur.
We present extensive experimental results for synthetic and real scenes with
two types of blur: camera motion blur and defocus blur. The results demonstrate
that DP-NeRF successfully improves the perceptual quality of the constructed
NeRF ensuring 3D geometric and appearance consistency. We further demonstrate
the effectiveness of our model with comprehensive ablation analysis.";Dogyoon Lee<author:sep>Minhyeok Lee<author:sep>Chajin Shin<author:sep>Sangyoun Lee;http://arxiv.org/pdf/2211.12046v4;cs.CV;"Accepted at CVPR 2023, Code: https://github.com/dogyoonlee/DP-NeRF,
  Project page: https://dogyoonlee.github.io/dpnerf/";nerf
2211.12499v2;http://arxiv.org/abs/2211.12499v2;2022-11-22;Instant Volumetric Head Avatars;"We present Instant Volumetric Head Avatars (INSTA), a novel approach for
reconstructing photo-realistic digital avatars instantaneously. INSTA models a
dynamic neural radiance field based on neural graphics primitives embedded
around a parametric face model. Our pipeline is trained on a single monocular
RGB portrait video that observes the subject under different expressions and
views. While state-of-the-art methods take up to several days to train an
avatar, our method can reconstruct a digital avatar in less than 10 minutes on
modern GPU hardware, which is orders of magnitude faster than previous
solutions. In addition, it allows for the interactive rendering of novel poses
and expressions. By leveraging the geometry prior of the underlying parametric
face model, we demonstrate that INSTA extrapolates to unseen poses. In
quantitative and qualitative studies on various subjects, INSTA outperforms
state-of-the-art methods regarding rendering quality and training time.";Wojciech Zielonka<author:sep>Timo Bolkart<author:sep>Justus Thies;http://arxiv.org/pdf/2211.12499v2;cs.CV;"Website: https://zielon.github.io/insta/ Video:
  https://youtu.be/HOgaeWTih7Q Accepted to CVPR2023";
2211.12544v1;http://arxiv.org/abs/2211.12544v1;2022-11-22;Zero NeRF: Registration with Zero Overlap;"We present Zero-NeRF, a projective surface registration method that, to the
best of our knowledge, offers the first general solution capable of alignment
between scene representations with minimal or zero visual correspondence. To do
this, we enforce consistency between visible surfaces of partial and complete
reconstructions, which allows us to constrain occluded geometry. We use a NeRF
as our surface representation and the NeRF rendering pipeline to perform this
alignment. To demonstrate the efficacy of our method, we register real-world
scenes from opposite sides with infinitesimal overlaps that cannot be
accurately registered using prior methods, and we compare these results against
widely used registration methods.";Casey Peat<author:sep>Oliver Batchelor<author:sep>Richard Green<author:sep>James Atlas;http://arxiv.org/pdf/2211.12544v1;cs.CV;;nerf
2211.12038v1;http://arxiv.org/abs/2211.12038v1;2022-11-22;ONeRF: Unsupervised 3D Object Segmentation from Multiple Views;"We present ONeRF, a method that automatically segments and reconstructs
object instances in 3D from multi-view RGB images without any additional manual
annotations. The segmented 3D objects are represented using separate Neural
Radiance Fields (NeRFs) which allow for various 3D scene editing and novel view
rendering. At the core of our method is an unsupervised approach using the
iterative Expectation-Maximization algorithm, which effectively aggregates 2D
visual features and the corresponding 3D cues from multi-views for joint 3D
object segmentation and reconstruction. Unlike existing approaches that can
only handle simple objects, our method produces segmented full 3D NeRFs of
individual objects with complex shapes, topologies and appearance. The
segmented ONeRfs enable a range of 3D scene editing, such as object
transformation, insertion and deletion.";Shengnan Liang<author:sep>Yichen Liu<author:sep>Shangzhe Wu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2211.12038v1;cs.CV;;nerf
2211.12254v2;http://arxiv.org/abs/2211.12254v2;2022-11-22;SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural  Radiance Fields;"Neural Radiance Fields (NeRFs) have emerged as a popular approach for novel
view synthesis. While NeRFs are quickly being adapted for a wider set of
applications, intuitively editing NeRF scenes is still an open challenge. One
important editing task is the removal of unwanted objects from a 3D scene, such
that the replaced region is visually plausible and consistent with its context.
We refer to this task as 3D inpainting. In 3D, solutions must be both
consistent across multiple views and geometrically valid. In this paper, we
propose a novel 3D inpainting method that addresses these challenges. Given a
small set of posed images and sparse annotations in a single input image, our
framework first rapidly obtains a 3D segmentation mask for a target object.
Using the mask, a perceptual optimizationbased approach is then introduced that
leverages learned 2D image inpainters, distilling their information into 3D
space, while ensuring view consistency. We also address the lack of a diverse
benchmark for evaluating 3D scene inpainting methods by introducing a dataset
comprised of challenging real-world scenes. In particular, our dataset contains
views of the same scene with and without a target object, enabling more
principled benchmarking of the 3D inpainting task. We first demonstrate the
superiority of our approach on multiview segmentation, comparing to NeRFbased
methods and 2D segmentation approaches. We then evaluate on the task of 3D
inpainting, establishing state-ofthe-art performance against other NeRF
manipulation algorithms, as well as a strong 2D image inpainter baseline.
Project Page: https://spinnerf3d.github.io";Ashkan Mirzaei<author:sep>Tristan Aumentado-Armstrong<author:sep>Konstantinos G. Derpanis<author:sep>Jonathan Kelly<author:sep>Marcus A. Brubaker<author:sep>Igor Gilitschenski<author:sep>Alex Levinshtein;http://arxiv.org/pdf/2211.12254v2;cs.CV;Project Page: https://spinnerf3d.github.io;nerf
2211.11505v3;http://arxiv.org/abs/2211.11505v3;2022-11-21;Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields;"Neural Radiance Fields (NeRF) have achieved photorealistic novel views
synthesis; however, the requirement of accurate camera poses limits its
application. Despite analysis-by-synthesis extensions for jointly learning
neural 3D representations and registering camera frames exist, they are
susceptible to suboptimal solutions if poorly initialized. We propose L2G-NeRF,
a Local-to-Global registration method for bundle-adjusting Neural Radiance
Fields: first, a pixel-wise flexible alignment, followed by a frame-wise
constrained parametric alignment. Pixel-wise local alignment is learned in an
unsupervised way via a deep network which optimizes photometric reconstruction
errors. Frame-wise global alignment is performed using differentiable parameter
estimation solvers on the pixel-wise correspondences to find a global
transformation. Experiments on synthetic and real-world data show that our
method outperforms the current state-of-the-art in terms of high-fidelity
reconstruction and resolving large camera pose misalignment. Our module is an
easy-to-use plugin that can be applied to NeRF variants and other neural field
applications. The Code and supplementary materials are available at
https://rover-xingyu.github.io/L2G-NeRF/.";Yue Chen<author:sep>Xingyu Chen<author:sep>Xuan Wang<author:sep>Qi Zhang<author:sep>Yu Guo<author:sep>Ying Shan<author:sep>Fei Wang;http://arxiv.org/pdf/2211.11505v3;cs.CV;Accepted to CVPR 2023;nerf
2211.11646v3;http://arxiv.org/abs/2211.11646v3;2022-11-21;NeRF-RPN: A general framework for object detection in NeRFs;"This paper presents the first significant object detection framework,
NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,
NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting
a novel voxel representation that incorporates multi-scale 3D neural volumetric
features, we demonstrate it is possible to regress the 3D bounding boxes of
objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN
is a general framework and can be applied to detect objects without class
labels. We experimented NeRF-RPN with various backbone architectures, RPN head
designs and loss functions. All of them can be trained in an end-to-end manner
to estimate high quality 3D bounding boxes. To facilitate future research in
object detection for NeRF, we built a new benchmark dataset which consists of
both synthetic and real-world data with careful labeling and clean up. Code and
dataset are available at https://github.com/lyclyc52/NeRF_RPN.";Benran Hu<author:sep>Junkai Huang<author:sep>Yichen Liu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2211.11646v3;cs.CV;Accepted by CVPR 2023;nerf
2211.11202v3;http://arxiv.org/abs/2211.11202v3;2022-11-21;FLNeRF: 3D Facial Landmarks Estimation in Neural Radiance Fields;"This paper presents the first significant work on directly predicting 3D face
landmarks on neural radiance fields (NeRFs). Our 3D coarse-to-fine Face
Landmarks NeRF (FLNeRF) model efficiently samples from a given face NeRF with
individual facial features for accurate landmarks detection. Expression
augmentation is applied to facial features in a fine scale to simulate large
emotions range including exaggerated facial expressions (e.g., cheek blowing,
wide opening mouth, eye blinking) for training FLNeRF. Qualitative and
quantitative comparison with related state-of-the-art 3D facial landmark
estimation methods demonstrate the efficacy of FLNeRF, which contributes to
downstream tasks such as high-quality face editing and swapping with direct
control using our NeRF landmarks. Code and data will be available. Github link:
https://github.com/ZHANG1023/FLNeRF.";Hao Zhang<author:sep>Tianyuan Dai<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2211.11202v3;cs.CV;"Hao Zhang and Tianyuan Dai contributed equally. Project website:
  https://github.com/ZHANG1023/FLNeRF";nerf
2211.11836v1;http://arxiv.org/abs/2211.11836v1;2022-11-21;Towards Live 3D Reconstruction from Wearable Video: An Evaluation of  V-SLAM, NeRF, and Videogrammetry Techniques;"Mixed reality (MR) is a key technology which promises to change the future of
warfare. An MR hybrid of physical outdoor environments and virtual military
training will enable engagements with long distance enemies, both real and
simulated. To enable this technology, a large-scale 3D model of a physical
environment must be maintained based on live sensor observations. 3D
reconstruction algorithms should utilize the low cost and pervasiveness of
video camera sensors, from both overhead and soldier-level perspectives.
Mapping speed and 3D quality can be balanced to enable live MR training in
dynamic environments. Given these requirements, we survey several 3D
reconstruction algorithms for large-scale mapping for military applications
given only live video. We measure 3D reconstruction performance from common
structure from motion, visual-SLAM, and photogrammetry techniques. This
includes the open source algorithms COLMAP, ORB-SLAM3, and NeRF using
Instant-NGP. We utilize the autonomous driving academic benchmark KITTI, which
includes both dashboard camera video and lidar produced 3D ground truth. With
the KITTI data, our primary contribution is a quantitative evaluation of 3D
reconstruction computational speed when considering live video.";David Ramirez<author:sep>Suren Jayasuriya<author:sep>Andreas Spanias;http://arxiv.org/pdf/2211.11836v1;eess.IV;"Accepted to 2022 Interservice/Industry Training, Simulation, and
  Education Conference (I/ITSEC), 13 pages";nerf
2211.11738v3;http://arxiv.org/abs/2211.11738v3;2022-11-21;SPARF: Neural Radiance Fields from Sparse and Noisy Poses;"Neural Radiance Field (NeRF) has recently emerged as a powerful
representation to synthesize photorealistic novel views. While showing
impressive performance, it relies on the availability of dense input views with
highly accurate camera poses, thus limiting its application in real-world
scenarios. In this work, we introduce Sparse Pose Adjusting Radiance Field
(SPARF), to address the challenge of novel-view synthesis given only few
wide-baseline input images (as low as 3) with noisy camera poses. Our approach
exploits multi-view geometry constraints in order to jointly learn the NeRF and
refine the camera poses. By relying on pixel matches extracted between the
input views, our multi-view correspondence objective enforces the optimized
scene and camera poses to converge to a global and geometrically accurate
solution. Our depth consistency loss further encourages the reconstructed scene
to be consistent from any viewpoint. Our approach sets a new state of the art
in the sparse-view regime on multiple challenging datasets.";Prune Truong<author:sep>Marie-Julie Rakotosaona<author:sep>Fabian Manhardt<author:sep>Federico Tombari;http://arxiv.org/pdf/2211.11738v3;cs.CV;"Code is released at https://github.com/google-research/sparf.
  Published at CVPR 2023 as a Highlight";nerf
2211.11704v2;http://arxiv.org/abs/2211.11704v2;2022-11-21;ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of  Signed Distance Fields;"We present ESLAM, an efficient implicit neural representation method for
Simultaneous Localization and Mapping (SLAM). ESLAM reads RGB-D frames with
unknown camera poses in a sequential manner and incrementally reconstructs the
scene representation while estimating the current camera position in the scene.
We incorporate the latest advances in Neural Radiance Fields (NeRF) into a SLAM
system, resulting in an efficient and accurate dense visual SLAM method. Our
scene representation consists of multi-scale axis-aligned perpendicular feature
planes and shallow decoders that, for each point in the continuous space,
decode the interpolated features into Truncated Signed Distance Field (TSDF)
and RGB values. Our extensive experiments on three standard datasets, Replica,
ScanNet, and TUM RGB-D show that ESLAM improves the accuracy of 3D
reconstruction and camera localization of state-of-the-art dense visual SLAM
methods by more than 50%, while it runs up to 10 times faster and does not
require any pre-training.";Mohammad Mahdi Johari<author:sep>Camilla Carta<author:sep>FranÃ§ois Fleuret;http://arxiv.org/pdf/2211.11704v2;cs.CV;CVPR 2023 Highlight. Project page: https://www.idiap.ch/paper/eslam/;nerf
2211.11674v2;http://arxiv.org/abs/2211.11674v2;2022-11-21;Shape, Pose, and Appearance from a Single Image via Bootstrapped  Radiance Field Inversion;"Neural Radiance Fields (NeRF) coupled with GANs represent a promising
direction in the area of 3D reconstruction from a single view, owing to their
ability to efficiently model arbitrary topologies. Recent work in this area,
however, has mostly focused on synthetic datasets where exact ground-truth
poses are known, and has overlooked pose estimation, which is important for
certain downstream applications such as augmented reality (AR) and robotics. We
introduce a principled end-to-end reconstruction framework for natural images,
where accurate ground-truth poses are not available. Our approach recovers an
SDF-parameterized 3D shape, pose, and appearance from a single image of an
object, without exploiting multiple views during training. More specifically,
we leverage an unconditional 3D-aware generator, to which we apply a hybrid
inversion scheme where a model produces a first guess of the solution which is
then refined via optimization. Our framework can de-render an image in as few
as 10 steps, enabling its use in practical scenarios. We demonstrate
state-of-the-art results on a variety of real and synthetic benchmarks.";Dario Pavllo<author:sep>David Joseph Tan<author:sep>Marie-Julie Rakotosaona<author:sep>Federico Tombari;http://arxiv.org/pdf/2211.11674v2;cs.CV;"CVPR 2023. Code and models are available at
  https://github.com/google-research/nerf-from-image";nerf
2211.11215v2;http://arxiv.org/abs/2211.11215v2;2022-11-21;SegNeRF: 3D Part Segmentation with Neural Radiance Fields;"Recent advances in Neural Radiance Fields (NeRF) boast impressive
performances for generative tasks such as novel view synthesis and 3D
reconstruction. Methods based on neural radiance fields are able to represent
the 3D world implicitly by relying exclusively on posed images. Yet, they have
seldom been explored in the realm of discriminative tasks such as 3D part
segmentation. In this work, we attempt to bridge that gap by proposing SegNeRF:
a neural field representation that integrates a semantic field along with the
usual radiance field. SegNeRF inherits from previous works the ability to
perform novel view synthesis and 3D reconstruction, and enables 3D part
segmentation from a few images. Our extensive experiments on PartNet show that
SegNeRF is capable of simultaneously predicting geometry, appearance, and
semantic information from posed images, even for unseen objects. The predicted
semantic fields allow SegNeRF to achieve an average mIoU of $\textbf{30.30%}$
for 2D novel view segmentation, and $\textbf{37.46%}$ for 3D part segmentation,
boasting competitive performance against point-based methods by using only a
few posed images. Additionally, SegNeRF is able to generate an explicit 3D
model from a single image of an object taken in the wild, with its
corresponding part segmentation.";Jesus Zarzar<author:sep>Sara Rojas<author:sep>Silvio Giancola<author:sep>Bernard Ghanem;http://arxiv.org/pdf/2211.11215v2;cs.CV;Fixed abstract typo;nerf
2211.11082v3;http://arxiv.org/abs/2211.11082v3;2022-11-20;DynIBaR: Neural Dynamic Image-Based Rendering;"We address the problem of synthesizing novel views from a monocular video
depicting a complex dynamic scene. State-of-the-art methods based on temporally
varying Neural Radiance Fields (aka dynamic NeRFs) have shown impressive
results on this task. However, for long videos with complex object motions and
uncontrolled camera trajectories, these methods can produce blurry or
inaccurate renderings, hampering their use in real-world applications. Instead
of encoding the entire dynamic scene within the weights of MLPs, we present a
new approach that addresses these limitations by adopting a volumetric
image-based rendering framework that synthesizes new viewpoints by aggregating
features from nearby views in a scene-motion-aware manner. Our system retains
the advantages of prior methods in its ability to model complex scenes and
view-dependent effects, but also enables synthesizing photo-realistic novel
views from long videos featuring complex scene dynamics with unconstrained
camera trajectories. We demonstrate significant improvements over
state-of-the-art methods on dynamic scene datasets, and also apply our approach
to in-the-wild videos with challenging camera and object motion, where prior
methods fail to produce high-quality renderings. Our project webpage is at
dynibar.github.io.";Zhengqi Li<author:sep>Qianqian Wang<author:sep>Forrester Cole<author:sep>Richard Tucker<author:sep>Noah Snavely;http://arxiv.org/pdf/2211.11082v3;cs.CV;Award Candidate, CVPR 2023 Project page: dynibar.github.io;nerf
2211.10440v2;http://arxiv.org/abs/2211.10440v2;2022-11-18;Magic3D: High-Resolution Text-to-3D Content Creation;"DreamFusion has recently demonstrated the utility of a pre-trained
text-to-image diffusion model to optimize Neural Radiance Fields (NeRF),
achieving remarkable text-to-3D synthesis results. However, the method has two
inherent limitations: (a) extremely slow optimization of NeRF and (b)
low-resolution image space supervision on NeRF, leading to low-quality 3D
models with a long processing time. In this paper, we address these limitations
by utilizing a two-stage optimization framework. First, we obtain a coarse
model using a low-resolution diffusion prior and accelerate with a sparse 3D
hash grid structure. Using the coarse representation as the initialization, we
further optimize a textured 3D mesh model with an efficient differentiable
renderer interacting with a high-resolution latent diffusion model. Our method,
dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is
2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also
achieving higher resolution. User studies show 61.7% raters to prefer our
approach over DreamFusion. Together with the image-conditioned generation
capabilities, we provide users with new ways to control 3D synthesis, opening
up new avenues to various creative applications.";Chen-Hsuan Lin<author:sep>Jun Gao<author:sep>Luming Tang<author:sep>Towaki Takikawa<author:sep>Xiaohui Zeng<author:sep>Xun Huang<author:sep>Karsten Kreis<author:sep>Sanja Fidler<author:sep>Ming-Yu Liu<author:sep>Tsung-Yi Lin;http://arxiv.org/pdf/2211.10440v2;cs.CV;"Accepted to CVPR 2023 as highlight. Project website:
  https://research.nvidia.com/labs/dir/magic3d";nerf
2211.10444v1;http://arxiv.org/abs/2211.10444v1;2022-11-18;Neural Fields for Fast and Scalable Interpolation of Geophysical Ocean  Variables;"Optimal Interpolation (OI) is a widely used, highly trusted algorithm for
interpolation and reconstruction problems in geosciences. With the influx of
more satellite missions, we have access to more and more observations and it is
becoming more pertinent to take advantage of these observations in applications
such as forecasting and reanalysis. With the increase in the volume of
available data, scalability remains an issue for standard OI and it prevents
many practitioners from effectively and efficiently taking advantage of these
large sums of data to learn the model hyperparameters. In this work, we
leverage recent advances in Neural Fields (NerFs) as an alternative to the OI
framework where we show how they can be easily applied to standard
reconstruction problems in physical oceanography. We illustrate the relevance
of NerFs for gap-filling of sparse measurements of sea surface height (SSH) via
satellite altimetry and demonstrate how NerFs are scalable with comparable
results to the standard OI. We find that NerFs are a practical set of methods
that can be readily applied to geoscience interpolation problems and we
anticipate a wider adoption in the future.";J. Emmanuel Johnson<author:sep>Redouane Lguensat<author:sep>Ronan Fablet<author:sep>Emmanuel Cosme<author:sep>Julien Le Sommer;http://arxiv.org/pdf/2211.10444v1;physics.ao-ph;Machine Learning and the Physical Sciences workshop, NeurIPS 2022;nerf
2211.09682v1;http://arxiv.org/abs/2211.09682v1;2022-11-17;AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware  Training;"Neural Radiance Fields (NeRFs) are a powerful representation for modeling a
3D scene as a continuous function. Though NeRF is able to render complex 3D
scenes with view-dependent effects, few efforts have been devoted to exploring
its limits in a high-resolution setting. Specifically, existing NeRF-based
methods face several limitations when reconstructing high-resolution real
scenes, including a very large number of parameters, misaligned input data, and
overly smooth details. In this work, we conduct the first pilot study on
training NeRF with high-resolution data and propose the corresponding
solutions: 1) marrying the multilayer perceptron (MLP) with convolutional
layers which can encode more neighborhood information while reducing the total
number of parameters; 2) a novel training strategy to address misalignment
caused by moving objects or small camera calibration errors; and 3) a
high-frequency aware loss. Our approach is nearly free without introducing
obvious training/testing costs, while experiments on different datasets
demonstrate that it can recover more high-frequency details compared with the
current state-of-the-art NeRF models. Project page:
\url{https://yifanjiang.net/alignerf.}";Yifan Jiang<author:sep>Peter Hedman<author:sep>Ben Mildenhall<author:sep>Dejia Xu<author:sep>Jonathan T. Barron<author:sep>Zhangyang Wang<author:sep>Tianfan Xue;http://arxiv.org/pdf/2211.09682v1;cs.CV;;nerf
2211.08610v1;http://arxiv.org/abs/2211.08610v1;2022-11-16;CoNFies: Controllable Neural Face Avatars;"Neural Radiance Fields (NeRF) are compelling techniques for modeling dynamic
3D scenes from 2D image collections. These volumetric representations would be
well suited for synthesizing novel facial expressions but for two problems.
First, deformable NeRFs are object agnostic and model holistic movement of the
scene: they can replay how the motion changes over time, but they cannot alter
it in an interpretable way. Second, controllable volumetric representations
typically require either time-consuming manual annotations or 3D supervision to
provide semantic meaning to the scene. We propose a controllable neural
representation for face self-portraits (CoNFies), that solves both of these
problems within a common framework, and it can rely on automated processing. We
use automated facial action recognition (AFAR) to characterize facial
expressions as a combination of action units (AU) and their intensities. AUs
provide both the semantic locations and control labels for the system. CoNFies
outperformed competing methods for novel view and expression synthesis in terms
of visual and anatomic fidelity of expressions.";Heng Yu<author:sep>Koichiro Niinuma<author:sep>Laszlo A. Jeni;http://arxiv.org/pdf/2211.08610v1;cs.CV;accepted by FG2023;nerf
2211.07968v1;http://arxiv.org/abs/2211.07968v1;2022-11-15;NeRFFaceEditing: Disentangled Face Editing in Neural Radiance Fields;"Recent methods for synthesizing 3D-aware face images have achieved rapid
development thanks to neural radiance fields, allowing for high quality and
fast inference speed. However, existing solutions for editing facial geometry
and appearance independently usually require retraining and are not optimized
for the recent work of generation, thus tending to lag behind the generation
process. To address these issues, we introduce NeRFFaceEditing, which enables
editing and decoupling geometry and appearance in the pretrained
tri-plane-based neural radiance field while retaining its high quality and fast
inference speed. Our key idea for disentanglement is to use the statistics of
the tri-plane to represent the high-level appearance of its corresponding
facial volume. Moreover, we leverage a generated 3D-continuous semantic mask as
an intermediary for geometry editing. We devise a geometry decoder (whose
output is unchanged when the appearance changes) and an appearance decoder. The
geometry decoder aligns the original facial volume with the semantic mask
volume. We also enhance the disentanglement by explicitly regularizing rendered
images with the same appearance but different geometry to be similar in terms
of color distribution for each facial component separately. Our method allows
users to edit via semantic masks with decoupled control of geometry and
appearance. Both qualitative and quantitative evaluations show the superior
geometry and appearance control abilities of our method compared to existing
and alternative solutions.";Kaiwen Jiang<author:sep>Shu-Yu Chen<author:sep>Feng-Lin Liu<author:sep>Hongbo Fu<author:sep>Lin Gao;http://arxiv.org/pdf/2211.07968v1;cs.GR;;nerf
2211.07600v1;http://arxiv.org/abs/2211.07600v1;2022-11-14;Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures;"Text-guided image generation has progressed rapidly in recent years,
inspiring major breakthroughs in text-guided shape generation. Recently, it has
been shown that using score distillation, one can successfully text-guide a
NeRF model to generate a 3D object. We adapt the score distillation to the
publicly available, and computationally efficient, Latent Diffusion Models,
which apply the entire diffusion process in a compact latent space of a
pretrained autoencoder. As NeRFs operate in image space, a naive solution for
guiding them with latent score distillation would require encoding to the
latent space at each guidance step. Instead, we propose to bring the NeRF to
the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we
show that while Text-to-3D models can generate impressive results, they are
inherently unconstrained and may lack the ability to guide or enforce a
specific 3D structure. To assist and direct the 3D generation, we propose to
guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines
the coarse structure of the desired object. Then, we present means to integrate
such a constraint directly into a Latent-NeRF. This unique combination of text
and shape guidance allows for increased control over the generation process. We
also show that latent score distillation can be successfully applied directly
on 3D meshes. This allows for generating high-quality textures on a given
geometry. Our experiments validate the power of our different forms of guidance
and the efficiency of using latent rendering. Implementation is available at
https://github.com/eladrich/latent-nerf";Gal Metzer<author:sep>Elad Richardson<author:sep>Or Patashnik<author:sep>Raja Giryes<author:sep>Daniel Cohen-Or;http://arxiv.org/pdf/2211.07600v1;cs.CV;;nerf
2211.06583v1;http://arxiv.org/abs/2211.06583v1;2022-11-12;3D-Aware Encoding for Style-based Neural Radiance Fields;"We tackle the task of NeRF inversion for style-based neural radiance fields,
(e.g., StyleNeRF). In the task, we aim to learn an inversion function to
project an input image to the latent space of a NeRF generator and then
synthesize novel views of the original image based on the latent code. Compared
with GAN inversion for 2D generative models, NeRF inversion not only needs to
1) preserve the identity of the input image, but also 2) ensure 3D consistency
in generated novel views. This requires the latent code obtained from the
single-view image to be invariant across multiple views. To address this new
challenge, we propose a two-stage encoder for style-based NeRF inversion. In
the first stage, we introduce a base encoder that converts the input image to a
latent code. To ensure the latent code is view-invariant and is able to
synthesize 3D consistent novel view images, we utilize identity contrastive
learning to train the base encoder. Second, to better preserve the identity of
the input image, we introduce a refining encoder to refine the latent code and
add finer details to the output image. Importantly note that the novelty of
this model lies in the design of its first-stage encoder which produces the
closest latent code lying on the latent manifold and thus the refinement in the
second stage would be close to the NeRF manifold. Through extensive
experiments, we demonstrate that our proposed two-stage encoder qualitatively
and quantitatively exhibits superiority over the existing encoders for
inversion in both image reconstruction and novel-view rendering.";Yu-Jhe Li<author:sep>Tao Xu<author:sep>Bichen Wu<author:sep>Ningyuan Zheng<author:sep>Xiaoliang Dai<author:sep>Albert Pumarola<author:sep>Peizhao Zhang<author:sep>Peter Vajda<author:sep>Kris Kitani;http://arxiv.org/pdf/2211.06583v1;cs.CV;21 pages (under review);nerf
2211.04041v4;http://arxiv.org/abs/2211.04041v4;2022-11-08;ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance  Fields;"While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline
methods with an emphasis on visual fidelity, our paper addresses the online use
case that prioritises real-time adaptability. We present ParticleNeRF, a new
approach that dynamically adapts to changes in the scene geometry by learning
an up-to-date representation online, every 200ms. ParticleNeRF achieves this
using a novel particle-based parametric encoding. We couple features to
particles in space and backpropagate the photometric reconstruction loss into
the particles' position gradients, which are then interpreted as velocity
vectors. Governed by a lightweight physics system to handle collisions, this
lets the features move freely with the changing scene geometry. We demonstrate
ParticleNeRF on various dynamic scenes containing translating, rotating,
articulated, and deformable objects. ParticleNeRF is the first online dynamic
NeRF and achieves fast adaptability with better visual fidelity than
brute-force online InstantNGP and other baseline approaches on dynamic scenes
with online constraints. Videos of our system can be found at our project
website https://sites.google.com/view/particlenerf.";Jad Abou-Chakra<author:sep>Feras Dayoub<author:sep>Niko SÃ¼nderhauf;http://arxiv.org/pdf/2211.04041v4;cs.CV;;nerf
2211.03889v1;http://arxiv.org/abs/2211.03889v1;2022-11-07;Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable  Categories;"Obtaining photorealistic reconstructions of objects from sparse views is
inherently ambiguous and can only be achieved by learning suitable
reconstruction priors. Earlier works on sparse rigid object reconstruction
successfully learned such priors from large datasets such as CO3D. In this
paper, we extend this approach to dynamic objects. We use cats and dogs as a
representative example and introduce Common Pets in 3D (CoP3D), a collection of
crowd-sourced videos showing around 4,200 distinct pets. CoP3D is one of the
first large-scale datasets for benchmarking non-rigid 3D reconstruction ""in the
wild"". We also propose Tracker-NeRF, a method for learning 4D reconstruction
from our dataset. At test time, given a small number of video frames of an
unseen object, Tracker-NeRF predicts the trajectories of its 3D points and
generates new views, interpolating viewpoint and time. Results on CoP3D reveal
significantly better non-rigid new-view synthesis performance than existing
baselines.";Samarth Sinha<author:sep>Roman Shapovalov<author:sep>Jeremy Reizenstein<author:sep>Ignacio Rocco<author:sep>Natalia Neverova<author:sep>Andrea Vedaldi<author:sep>David Novotny;http://arxiv.org/pdf/2211.03889v1;cs.CV;;nerf
2211.03017v2;http://arxiv.org/abs/2211.03017v2;2022-11-06;Learning-based Inverse Rendering of Complex Indoor Scenes with  Differentiable Monte Carlo Raytracing;"Indoor scenes typically exhibit complex, spatially-varying appearance from
global illumination, making inverse rendering a challenging ill-posed problem.
This work presents an end-to-end, learning-based inverse rendering framework
incorporating differentiable Monte Carlo raytracing with importance sampling.
The framework takes a single image as input to jointly recover the underlying
geometry, spatially-varying lighting, and photorealistic materials.
Specifically, we introduce a physically-based differentiable rendering layer
with screen-space ray tracing, resulting in more realistic specular reflections
that match the input photo. In addition, we create a large-scale,
photorealistic indoor scene dataset with significantly richer details like
complex furniture and dedicated decorations. Further, we design a novel
out-of-view lighting network with uncertainty-aware refinement leveraging
hypernetwork-based neural radiance fields to predict lighting outside the view
of the input photo. Through extensive evaluations on common benchmark datasets,
we demonstrate superior inverse rendering quality of our method compared to
state-of-the-art baselines, enabling various applications such as complex
object insertion and material editing with high fidelity. Code and data will be
made available at \url{https://jingsenzhu.github.io/invrend}.";Jingsen Zhu<author:sep>Fujun Luan<author:sep>Yuchi Huo<author:sep>Zihao Lin<author:sep>Zhihua Zhong<author:sep>Dianbing Xi<author:sep>Jiaxiang Zheng<author:sep>Rui Tang<author:sep>Hujun Bao<author:sep>Rui Wang;http://arxiv.org/pdf/2211.03017v2;cs.CV;;
2211.01600v1;http://arxiv.org/abs/2211.01600v1;2022-11-03;nerf2nerf: Pairwise Registration of Neural Radiance Fields;"We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io";Lily Goli<author:sep>Daniel Rebain<author:sep>Sara Sabour<author:sep>Animesh Garg<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2211.01600v1;cs.CV;;nerf
2211.00597v1;http://arxiv.org/abs/2211.00597v1;2022-10-29;Mixed Reality Interface for Digital Twin of Plant Factory;"An easier and intuitive interface architecture is necessary for digital twin
of plant factory. I suggest an immersive and interactive mixed reality
interface for digital twin models of smart farming, for remote work rather than
simulation of components. The environment is constructed with UI display and a
streaming background scene, which is a real time scene taken from camera device
located in the plant factory, processed with deformable neural radiance fields.
User can monitor and control the remote plant factory facilities with HMD or 2D
display based mixed reality environment. This paper also introduces detailed
concept and describes the system architecture to implement suggested mixed
reality interface.";Byunghyun Ban;http://arxiv.org/pdf/2211.00597v1;cs.HC;5 pages, 7 figures;
2210.15947v2;http://arxiv.org/abs/2210.15947v2;2022-10-28;NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed  Neural Radiance Fields;"Visually exploring in a real-world 4D spatiotemporal space freely in VR has
been a long-term quest. The task is especially appealing when only a few or
even single RGB cameras are used for capturing the dynamic scene. To this end,
we present an efficient framework capable of fast reconstruction, compact
modeling, and streamable rendering. First, we propose to decompose the 4D
spatiotemporal space according to temporal characteristics. Points in the 4D
space are associated with probabilities of belonging to three categories:
static, deforming, and new areas. Each area is represented and regularized by a
separate neural field. Second, we propose a hybrid representations based
feature streaming scheme for efficiently modeling the neural fields. Our
approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single
hand-held cameras and multi-camera arrays, achieving comparable or superior
rendering performance in terms of quality and speed comparable to recent
state-of-the-art methods, achieving reconstruction in 10 seconds per frame and
interactive rendering.";Liangchen Song<author:sep>Anpei Chen<author:sep>Zhong Li<author:sep>Zhang Chen<author:sep>Lele Chen<author:sep>Junsong Yuan<author:sep>Yi Xu<author:sep>Andreas Geiger;http://arxiv.org/pdf/2210.15947v2;cs.CV;Project page: https://lsongx.github.io/projects/nerfplayer.html;nerf
2210.17415v1;http://arxiv.org/abs/2210.17415v1;2022-10-27;ProbNeRF: Uncertainty-Aware Inference of 3D Shapes from 2D Images;"The problem of inferring object shape from a single 2D image is
underconstrained. Prior knowledge about what objects are plausible can help,
but even given such prior knowledge there may still be uncertainty about the
shapes of occluded parts of objects. Recently, conditional neural radiance
field (NeRF) models have been developed that can learn to infer good point
estimates of 3D models from single 2D images. The problem of inferring
uncertainty estimates for these models has received less attention. In this
work, we propose probabilistic NeRF (ProbNeRF), a model and inference strategy
for learning probabilistic generative models of 3D objects' shapes and
appearances, and for doing posterior inference to recover those properties from
2D images. ProbNeRF is trained as a variational autoencoder, but at test time
we use Hamiltonian Monte Carlo (HMC) for inference. Given one or a few 2D
images of an object (which may be partially occluded), ProbNeRF is able not
only to accurately model the parts it sees, but also to propose realistic and
diverse hypotheses about the parts it does not see. We show that key to the
success of ProbNeRF are (i) a deterministic rendering scheme, (ii) an
annealed-HMC strategy, (iii) a hypernetwork-based decoder architecture, and
(iv) doing inference over a full set of NeRF weights, rather than just a
low-dimensional code.";Matthew D. Hoffman<author:sep>Tuan Anh Le<author:sep>Pavel Sountsov<author:sep>Christopher Suter<author:sep>Ben Lee<author:sep>Vikash K. Mansinghka<author:sep>Rif A. Saurous;http://arxiv.org/pdf/2210.17415v1;cs.CV;"18 pages, 18 figures, 1 table; submitted to the 26th International
  Conference on Artificial Intelligence and Statistics (AISTATS 2023)";nerf
2210.15107v2;http://arxiv.org/abs/2210.15107v2;2022-10-27;Boosting Point Clouds Rendering via Radiance Mapping;"Recent years we have witnessed rapid development in NeRF-based image
rendering due to its high quality. However, point clouds rendering is somehow
less explored. Compared to NeRF-based rendering which suffers from dense
spatial sampling, point clouds rendering is naturally less computation
intensive, which enables its deployment in mobile computing device. In this
work, we focus on boosting the image quality of point clouds rendering with a
compact model design. We first analyze the adaption of the volume rendering
formulation on point clouds. Based on the analysis, we simplify the NeRF
representation to a spatial mapping function which only requires single
evaluation per pixel. Further, motivated by ray marching, we rectify the the
noisy raw point clouds to the estimated intersection between rays and surfaces
as queried coordinates, which could avoid \textit{spatial frequency collapse}
and neighbor point disturbance. Composed of rasterization, spatial mapping and
the refinement stages, our method achieves the state-of-the-art performance on
point clouds rendering, outperforming prior works by notable margins, with a
smaller model size. We obtain a PSNR of 31.74 on NeRF-Synthetic, 25.88 on
ScanNet and 30.81 on DTU. Code and data are publicly available at
https://github.com/seanywang0408/RadianceMapping.";Xiaoyang Huang<author:sep>Yi Zhang<author:sep>Bingbing Ni<author:sep>Teng Li<author:sep>Kai Chen<author:sep>Wenjun Zhang;http://arxiv.org/pdf/2210.15107v2;cs.CV;"Accepted by Thirty-Seventh AAAI Conference on Artificial Intelligence
  (AAAI 2023)";nerf
2210.13041v1;http://arxiv.org/abs/2210.13041v1;2022-10-24;Learning Neural Radiance Fields from Multi-View Geometry;"We present a framework, called MVG-NeRF, that combines classical Multi-View
Geometry algorithms and Neural Radiance Fields (NeRF) for image-based 3D
reconstruction. NeRF has revolutionized the field of implicit 3D
representations, mainly due to a differentiable volumetric rendering
formulation that enables high-quality and geometry-aware novel view synthesis.
However, the underlying geometry of the scene is not explicitly constrained
during training, thus leading to noisy and incorrect results when extracting a
mesh with marching cubes. To this end, we propose to leverage pixelwise depths
and normals from a classical 3D reconstruction pipeline as geometric priors to
guide NeRF optimization. Such priors are used as pseudo-ground truth during
training in order to improve the quality of the estimated underlying surface.
Moreover, each pixel is weighted by a confidence value based on the
forward-backward reprojection error for additional robustness. Experimental
results on real-world data demonstrate the effectiveness of this approach in
obtaining clean 3D meshes from images, while maintaining competitive
performances in novel view synthesis.";Marco Orsingher<author:sep>Paolo Zani<author:sep>Paolo Medici<author:sep>Massimo Bertozzi;http://arxiv.org/pdf/2210.13041v1;cs.CV;"ECCV 2022 Workshop on ""Learning to Generate 3D Shapes and Scenes""";nerf
2210.13641v1;http://arxiv.org/abs/2210.13641v1;2022-10-24;NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields;"We propose a novel geometric and photometric 3D mapping pipeline for accurate
and real-time scene reconstruction from monocular images. To achieve this, we
leverage recent advances in dense monocular SLAM and real-time hierarchical
volumetric neural radiance fields. Our insight is that dense monocular SLAM
provides the right information to fit a neural radiance field of the scene in
real-time, by providing accurate pose estimates and depth-maps with associated
uncertainty. With our proposed uncertainty-based depth loss, we achieve not
only good photometric accuracy, but also great geometric accuracy. In fact, our
proposed pipeline achieves better geometric and photometric accuracy than
competing approaches (up to 179% better PSNR and 86% better L1 depth), while
working in real-time and using only monocular images.";Antoni Rosinol<author:sep>John J. Leonard<author:sep>Luca Carlone;http://arxiv.org/pdf/2210.13641v1;cs.CV;10 pages, 6 figures;nerf
2210.12731v2;http://arxiv.org/abs/2210.12731v2;2022-10-23;Joint Rigid Motion Correction and Sparse-View CT via Self-Calibrating  Neural Field;"Neural Radiance Field (NeRF) has widely received attention in Sparse-View
Computed Tomography (SVCT) reconstruction tasks as a self-supervised deep
learning framework. NeRF-based SVCT methods represent the desired CT image as a
continuous function of spatial coordinates and train a Multi-Layer Perceptron
(MLP) to learn the function by minimizing loss on the SV sinogram. Benefiting
from the continuous representation provided by NeRF, the high-quality CT image
can be reconstructed. However, existing NeRF-based SVCT methods strictly
suppose there is completely no relative motion during the CT acquisition
because they require \textit{accurate} projection poses to model the X-rays
that scan the SV sinogram. Therefore, these methods suffer from severe
performance drops for real SVCT imaging with motion. In this work, we propose a
self-calibrating neural field to recover the artifacts-free image from the
rigid motion-corrupted SV sinogram without using any external data.
Specifically, we parametrize the inaccurate projection poses caused by rigid
motion as trainable variables and then jointly optimize these pose variables
and the MLP. We conduct numerical experiments on a public CT image dataset. The
results indicate our model significantly outperforms two representative
NeRF-based methods for SVCT reconstruction tasks with four different levels of
rigid motion.";Qing Wu<author:sep>Xin Li<author:sep>Hongjiang Wei<author:sep>Jingyi Yu<author:sep>Yuyao Zhang;http://arxiv.org/pdf/2210.12731v2;eess.IV;5 pages;nerf
2210.12782v1;http://arxiv.org/abs/2210.12782v1;2022-10-23;Compressing Explicit Voxel Grid Representations: fast NeRFs become also  small;"NeRFs have revolutionized the world of per-scene radiance field
reconstruction because of their intrinsic compactness. One of the main
limitations of NeRFs is their slow rendering speed, both at training and
inference time. Recent research focuses on the optimization of an explicit
voxel grid (EVG) that represents the scene, which can be paired with neural
networks to learn radiance fields. This approach significantly enhances the
speed both at train and inference time, but at the cost of large memory
occupation. In this work we propose Re:NeRF, an approach that specifically
targets EVG-NeRFs compressibility, aiming to reduce memory storage of NeRF
models while maintaining comparable performance. We benchmark our approach with
three different EVG-NeRF architectures on four popular benchmarks, showing
Re:NeRF's broad usability and effectiveness.";Chenxi Lola Deng<author:sep>Enzo Tartaglione;http://arxiv.org/pdf/2210.12782v1;cs.CV;;nerf
2210.12126v3;http://arxiv.org/abs/2210.12126v3;2022-10-21;One-Shot Neural Fields for 3D Object Understanding;"We present a unified and compact scene representation for robotics, where
each object in the scene is depicted by a latent code capturing geometry and
appearance. This representation can be decoded for various tasks such as novel
view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or
voxel maps), collision checking, and stable grasp prediction. We build our
representation from a single RGB input image at test time by leveraging recent
advances in Neural Radiance Fields (NeRF) that learn category-level priors on
large multiview datasets, then fine-tune on novel objects from one or few
views. We expand the NeRF model for additional grasp outputs and explore ways
to leverage this representation for robotics. At test-time, we build the
representation from a single RGB input image observing the scene from only one
viewpoint. We find that the recovered representation allows rendering from
novel views, including of occluded object parts, and also for predicting
successful stable grasps. Grasp poses can be directly decoded from our latent
representation with an implicit grasp decoder. We experimented in both
simulation and real world and demonstrated the capability for robust robotic
grasping using such compact representation. Website:
https://nerfgrasp.github.io";Valts Blukis<author:sep>Taeyeop Lee<author:sep>Jonathan Tremblay<author:sep>Bowen Wen<author:sep>In So Kweon<author:sep>Kuk-Jin Yoon<author:sep>Dieter Fox<author:sep>Stan Birchfield;http://arxiv.org/pdf/2210.12126v3;cs.RO;"IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshop (CVPRW) on XRNeRF: Advances in NeRF for the Metaverse 2023";nerf
2210.12003v2;http://arxiv.org/abs/2210.12003v2;2022-10-21;HDHumans: A Hybrid Approach for High-fidelity Digital Humans;"Photo-real digital human avatars are of enormous importance in graphics, as
they enable immersive communication over the globe, improve gaming and
entertainment experiences, and can be particularly beneficial for AR and VR
settings. However, current avatar generation approaches either fall short in
high-fidelity novel view synthesis, generalization to novel motions,
reproduction of loose clothing, or they cannot render characters at the high
resolution offered by modern displays. To this end, we propose HDHumans, which
is the first method for HD human character synthesis that jointly produces an
accurate and temporally coherent 3D deforming surface and highly
photo-realistic images of arbitrary novel views and of motions not seen at
training time. At the technical core, our method tightly integrates a classical
deforming character template with neural radiance fields (NeRF). Our method is
carefully designed to achieve a synergy between classical surface deformation
and NeRF. First, the template guides the NeRF, which allows synthesizing novel
views of a highly dynamic and articulated character and even enables the
synthesis of novel motions. Second, we also leverage the dense pointclouds
resulting from NeRF to further improve the deforming surface via 3D-to-3D
supervision. We outperform the state of the art quantitatively and
qualitatively in terms of synthesis quality and resolution, as well as the
quality of 3D surface reconstruction.";Marc Habermann<author:sep>Lingjie Liu<author:sep>Weipeng Xu<author:sep>Gerard Pons-Moll<author:sep>Michael Zollhoefer<author:sep>Christian Theobalt;http://arxiv.org/pdf/2210.12003v2;cs.CV;;nerf
2210.12268v1;http://arxiv.org/abs/2210.12268v1;2022-10-21;An Exploration of Neural Radiance Field Scene Reconstruction: Synthetic,  Real-world and Dynamic Scenes;"This project presents an exploration into 3D scene reconstruction of
synthetic and real-world scenes using Neural Radiance Field (NeRF) approaches.
We primarily take advantage of the reduction in training and rendering time of
neural graphic primitives multi-resolution hash encoding, to reconstruct static
video game scenes and real-world scenes, comparing and observing reconstruction
detail and limitations. Additionally, we explore dynamic scene reconstruction
using Neural Radiance Fields for Dynamic Scenes(D-NeRF). Finally, we extend the
implementation of D-NeRF, originally constrained to handle synthetic scenes to
also handle real-world dynamic scenes.";Benedict Quartey<author:sep>Tuluhan Akbulut<author:sep>Wasiwasi Mgonzo<author:sep>Zheng Xin Yong;http://arxiv.org/pdf/2210.12268v1;cs.CV;;nerf
2210.11668v2;http://arxiv.org/abs/2210.11668v2;2022-10-21;RGB-Only Reconstruction of Tabletop Scenes for Collision-Free  Manipulator Control;"We present a system for collision-free control of a robot manipulator that
uses only RGB views of the world. Perceptual input of a tabletop scene is
provided by multiple images of an RGB camera (without depth) that is either
handheld or mounted on the robot end effector. A NeRF-like process is used to
reconstruct the 3D geometry of the scene, from which the Euclidean full signed
distance function (ESDF) is computed. A model predictive control algorithm is
then used to control the manipulator to reach a desired pose while avoiding
obstacles in the ESDF. We show results on a real dataset collected and
annotated in our lab.";Zhenggang Tang<author:sep>Balakumar Sundaralingam<author:sep>Jonathan Tremblay<author:sep>Bowen Wen<author:sep>Ye Yuan<author:sep>Stephen Tyree<author:sep>Charles Loop<author:sep>Alexander Schwing<author:sep>Stan Birchfield;http://arxiv.org/pdf/2210.11668v2;cs.RO;ICRA 2023. Project page at https://ngp-mpc.github.io/;nerf
2210.11170v2;http://arxiv.org/abs/2210.11170v2;2022-10-20;Coordinates Are NOT Lonely -- Codebook Prior Helps Implicit Neural 3D  Representations;"Implicit neural 3D representation has achieved impressive results in surface
or scene reconstruction and novel view synthesis, which typically uses the
coordinate-based multi-layer perceptrons (MLPs) to learn a continuous scene
representation. However, existing approaches, such as Neural Radiance Field
(NeRF) and its variants, usually require dense input views (i.e. 50-150) to
obtain decent results. To relive the over-dependence on massive calibrated
images and enrich the coordinate-based feature representation, we explore
injecting the prior information into the coordinate-based network and introduce
a novel coordinate-based model, CoCo-INR, for implicit neural 3D
representation. The cores of our method are two attention modules: codebook
attention and coordinate attention. The former extracts the useful prototypes
containing rich geometry and appearance information from the prior codebook,
and the latter propagates such prior information into each coordinate and
enriches its feature representation for a scene or object surface. With the
help of the prior information, our method can render 3D views with more
photo-realistic appearance and geometries than the current methods using fewer
calibrated images available. Experiments on various scene reconstruction
datasets, including DTU and BlendedMVS, and the full 3D head reconstruction
dataset, H3DS, demonstrate the robustness under fewer input views and fine
detail-preserving capability of our proposed method.";Fukun Yin<author:sep>Wen Liu<author:sep>Zilong Huang<author:sep>Pei Cheng<author:sep>Tao Chen<author:sep>Gang YU;http://arxiv.org/pdf/2210.11170v2;cs.CV;NeurIPS 2022;nerf
2210.10036v1;http://arxiv.org/abs/2210.10036v1;2022-10-18;ARAH: Animatable Volume Rendering of Articulated Human SDFs;"Combining human body models with differentiable rendering has recently
enabled animatable avatars of clothed humans from sparse sets of multi-view RGB
videos. While state-of-the-art approaches achieve realistic appearance with
neural radiance fields (NeRF), the inferred geometry often lacks detail due to
missing geometric constraints. Further, animating avatars in
out-of-distribution poses is not yet possible because the mapping from
observation space to canonical space does not generalize faithfully to unseen
poses. In this work, we address these shortcomings and propose a model to
create animatable clothed human avatars with detailed geometry that generalize
well to out-of-distribution poses. To achieve detailed geometry, we combine an
articulated implicit surface representation with volume rendering. For
generalization, we propose a novel joint root-finding algorithm for
simultaneous ray-surface intersection search and correspondence search. Our
algorithm enables efficient point sampling and accurate point canonicalization
while generalizing well to unseen poses. We demonstrate that our proposed
pipeline can generate clothed avatars with high-quality pose-dependent geometry
and appearance from a sparse set of multi-view RGB videos. Our method achieves
state-of-the-art performance on geometry and appearance reconstruction while
creating animatable avatars that generalize well to out-of-distribution poses
beyond the small number of training poses.";Shaofei Wang<author:sep>Katja Schwarz<author:sep>Andreas Geiger<author:sep>Siyu Tang;http://arxiv.org/pdf/2210.10036v1;cs.CV;"Accepted to ECCV 2022. Project page:
  https://neuralbodies.github.io/arah/";nerf
2210.10108v2;http://arxiv.org/abs/2210.10108v2;2022-10-18;Parallel Inversion of Neural Radiance Fields for Robust Pose Estimation;"We present a parallelized optimization method based on fast Neural Radiance
Fields (NeRF) for estimating 6-DoF pose of a camera with respect to an object
or scene. Given a single observed RGB image of the target, we can predict the
translation and rotation of the camera by minimizing the residual between
pixels rendered from a fast NeRF model and pixels in the observed image. We
integrate a momentum-based camera extrinsic optimization procedure into Instant
Neural Graphics Primitives, a recent exceptionally fast NeRF implementation. By
introducing parallel Monte Carlo sampling into the pose estimation task, our
method overcomes local minima and improves efficiency in a more extensive
search space. We also show the importance of adopting a more robust pixel-based
loss function to reduce error. Experiments demonstrate that our method can
achieve improved generalization and robustness on both synthetic and real-world
benchmarks.";Yunzhi Lin<author:sep>Thomas MÃ¼ller<author:sep>Jonathan Tremblay<author:sep>Bowen Wen<author:sep>Stephen Tyree<author:sep>Alex Evans<author:sep>Patricio A. Vela<author:sep>Stan Birchfield;http://arxiv.org/pdf/2210.10108v2;cs.CV;ICRA 2023. Project page at https://pnerfp.github.io/;nerf
2210.09420v3;http://arxiv.org/abs/2210.09420v3;2022-10-17;Differentiable Physics Simulation of Dynamics-Augmented Neural Objects;"We present a differentiable pipeline for simulating the motion of objects
that represent their geometry as a continuous density field parameterized as a
deep network. This includes Neural Radiance Fields (NeRFs), and other related
models. From the density field, we estimate the dynamical properties of the
object, including its mass, center of mass, and inertia matrix. We then
introduce a differentiable contact model based on the density field for
computing normal and friction forces resulting from collisions. This allows a
robot to autonomously build object models that are visually and
\emph{dynamically} accurate from still images and videos of objects in motion.
The resulting Dynamics-Augmented Neural Objects (DANOs) are simulated with an
existing differentiable simulation engine, Dojo, interacting with other
standard simulation objects, such as spheres, planes, and robots specified as
URDFs. A robot can use this simulation to optimize grasps and manipulation
trajectories of neural objects, or to improve the neural object models through
gradient-based real-to-simulation transfer. We demonstrate the pipeline to
learn the coefficient of friction of a bar of soap from a real video of the
soap sliding on a table. We also learn the coefficient of friction and mass of
a Stanford bunny through interactions with a Panda robot arm from synthetic
data, and we optimize trajectories in simulation for the Panda arm to push the
bunny to a goal location.";Simon Le Cleac'h<author:sep>Hong-Xing Yu<author:sep>Michelle Guo<author:sep>Taylor A. Howell<author:sep>Ruohan Gao<author:sep>Jiajun Wu<author:sep>Zachary Manchester<author:sep>Mac Schwager;http://arxiv.org/pdf/2210.09420v3;cs.RO;;nerf
2210.08398v3;http://arxiv.org/abs/2210.08398v3;2022-10-15;SPIDR: SDF-based Neural Point Fields for Illumination and Deformation;"Neural radiance fields (NeRFs) have recently emerged as a promising approach
for 3D reconstruction and novel view synthesis. However, NeRF-based methods
encode shape, reflectance, and illumination implicitly and this makes it
challenging for users to manipulate these properties in the rendered images
explicitly. Existing approaches only enable limited editing of the scene and
deformation of the geometry. Furthermore, no existing work enables accurate
scene illumination after object deformation. In this work, we introduce SPIDR,
a new hybrid neural SDF representation. SPIDR combines point cloud and neural
implicit representations to enable the reconstruction of higher quality object
surfaces for geometry deformation and lighting estimation. meshes and surfaces
for object deformation and lighting estimation. To more accurately capture
environment illumination for scene relighting, we propose a novel neural
implicit model to learn environment light. To enable more accurate illumination
updates after deformation, we use the shadow mapping technique to approximate
the light visibility updates caused by geometry editing. We demonstrate the
effectiveness of SPIDR in enabling high quality geometry editing with more
accurate updates to the illumination of the scene.";Ruofan Liang<author:sep>Jiahao Zhang<author:sep>Haoda Li<author:sep>Chen Yang<author:sep>Yushi Guan<author:sep>Nandita Vijaykumar;http://arxiv.org/pdf/2210.08398v3;cs.CV;Project page: https://nexuslrf.github.io/SPIDR_webpage/;nerf
2210.08202v2;http://arxiv.org/abs/2210.08202v2;2022-10-15;IBL-NeRF: Image-Based Lighting Formulation of Neural Radiance Fields;"We propose IBL-NeRF, which decomposes the neural radiance fields (NeRF) of
large-scale indoor scenes into intrinsic components. Recent approaches further
decompose the baked radiance of the implicit volume into intrinsic components
such that one can partially approximate the rendering equation. However, they
are limited to representing isolated objects with a shared environment
lighting, and suffer from computational burden to aggregate rays with Monte
Carlo integration. In contrast, our prefiltered radiance field extends the
original NeRF formulation to capture the spatial variation of lighting within
the scene volume, in addition to surface properties. Specifically, the scenes
of diverse materials are decomposed into intrinsic components for rendering,
namely, albedo, roughness, surface normal, irradiance, and prefiltered
radiance. All of the components are inferred as neural images from MLP, which
can model large-scale general scenes. Especially the prefiltered radiance
effectively models the volumetric light field, and captures spatial variation
beyond a single environment light. The prefiltering aggregates rays in a set of
predefined neighborhood sizes such that we can replace the costly Monte Carlo
integration of global illumination with a simple query from a neural image. By
adopting NeRF, our approach inherits superior visual quality and multi-view
consistency for synthesized images as well as the intrinsic components. We
demonstrate the performance on scenes with complex object layouts and light
configurations, which could not be processed in any of the previous works.";Changwoon Choi<author:sep>Juhyeon Kim<author:sep>Young Min Kim;http://arxiv.org/pdf/2210.08202v2;cs.CV;Computer Graphics Forum (Pacific Graphics 2023);nerf
2210.07181v2;http://arxiv.org/abs/2210.07181v2;2022-10-13;MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without  Camera Pose;"We propose a generalizable neural radiance fields - MonoNeRF, that can be
trained on large-scale monocular videos of moving in static scenes without any
ground-truth annotations of depth and camera poses. MonoNeRF follows an
Autoencoder-based architecture, where the encoder estimates the monocular depth
and the camera pose, and the decoder constructs a Multiplane NeRF
representation based on the depth encoder feature, and renders the input frames
with the estimated camera. The learning is supervised by the reconstruction
error. Once the model is learned, it can be applied to multiple applications
including depth estimation, camera pose estimation, and single-image novel view
synthesis. More qualitative results are available at:
https://oasisyang.github.io/mononerf .";Yang Fu<author:sep>Ishan Misra<author:sep>Xiaolong Wang;http://arxiv.org/pdf/2210.07181v2;cs.CV;"ICML 2023 camera ready version. Project page:
  https://oasisyang.github.io/mononerf";nerf
2210.07301v2;http://arxiv.org/abs/2210.07301v2;2022-10-13;3D GAN Inversion with Pose Optimization;"With the recent advances in NeRF-based 3D aware GANs quality, projecting an
image into the latent space of these 3D-aware GANs has a natural advantage over
2D GAN inversion: not only does it allow multi-view consistent editing of the
projected image, but it also enables 3D reconstruction and novel view synthesis
when given only a single image. However, the explicit viewpoint control acts as
a main hindrance in the 3D GAN inversion process, as both camera pose and
latent code have to be optimized simultaneously to reconstruct the given image.
Most works that explore the latent space of the 3D-aware GANs rely on
ground-truth camera viewpoint or deformable 3D model, thus limiting their
applicability. In this work, we introduce a generalizable 3D GAN inversion
method that infers camera viewpoint and latent code simultaneously to enable
multi-view consistent semantic image editing. The key to our approach is to
leverage pre-trained estimators for better initialization and utilize the
pixel-wise depth calculated from NeRF parameters to better reconstruct the
given image. We conduct extensive experiments on image reconstruction and
editing both quantitatively and qualitatively, and further compare our results
with 2D GAN-based editing to demonstrate the advantages of utilizing the latent
space of 3D GANs. Additional results and visualizations are available at
https://3dgan-inversion.github.io .";Jaehoon Ko<author:sep>Kyusun Cho<author:sep>Daewon Choi<author:sep>Kwangrok Ryoo<author:sep>Seungryong Kim;http://arxiv.org/pdf/2210.07301v2;cs.CV;Project Page: https://3dgan-inversion.github.io;nerf
2210.06575v3;http://arxiv.org/abs/2210.06575v3;2022-10-12;GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and  Specular Objects Using Generalizable NeRF;"In this work, we tackle 6-DoF grasp detection for transparent and specular
objects, which is an important yet challenging problem in vision-based robotic
systems, due to the failure of depth cameras in sensing their geometry. We, for
the first time, propose a multiview RGB-based 6-DoF grasp detection network,
GraspNeRF, that leverages the generalizable neural radiance field (NeRF) to
achieve material-agnostic object grasping in clutter. Compared to the existing
NeRF-based 3-DoF grasp detection methods that rely on densely captured input
images and time-consuming per-scene optimization, our system can perform
zero-shot NeRF construction with sparse RGB inputs and reliably detect 6-DoF
grasps, both in real-time. The proposed framework jointly learns generalizable
NeRF and grasp detection in an end-to-end manner, optimizing the scene
representation construction for the grasping. For training data, we generate a
large-scale photorealistic domain-randomized synthetic dataset of grasping in
cluttered tabletop scenes that enables direct transfer to the real world. Our
extensive experiments in synthetic and real-world environments demonstrate that
our method significantly outperforms all the baselines in all the experiments
while remaining in real-time. Project page can be found at
https://pku-epic.github.io/GraspNeRF";Qiyu Dai<author:sep>Yan Zhu<author:sep>Yiran Geng<author:sep>Ciyu Ruan<author:sep>Jiazhao Zhang<author:sep>He Wang;http://arxiv.org/pdf/2210.06575v3;cs.RO;IEEE International Conference on Robotics and Automation (ICRA), 2023;nerf
2210.06108v1;http://arxiv.org/abs/2210.06108v1;2022-10-12;Reconstructing Personalized Semantic Facial NeRF Models From Monocular  Video;"We present a novel semantic model for human head defined with neural radiance
field. The 3D-consistent head model consist of a set of disentangled and
interpretable bases, and can be driven by low-dimensional expression
coefficients. Thanks to the powerful representation ability of neural radiance
field, the constructed model can represent complex facial attributes including
hair, wearings, which can not be represented by traditional mesh blendshape. To
construct the personalized semantic facial model, we propose to define the
bases as several multi-level voxel fields. With a short monocular RGB video as
input, our method can construct the subject's semantic facial NeRF model with
only ten to twenty minutes, and can render a photo-realistic human head image
in tens of miliseconds with a given expression coefficient and view direction.
With this novel representation, we apply it to many tasks like facial
retargeting and expression editing. Experimental results demonstrate its strong
representation ability and training/inference speed. Demo videos and released
code are provided in our project page:
https://ustc3dv.github.io/NeRFBlendShape/";Xuan Gao<author:sep>Chenglai Zhong<author:sep>Jun Xiang<author:sep>Yang Hong<author:sep>Yudong Guo<author:sep>Juyong Zhang;http://arxiv.org/pdf/2210.06108v1;cs.GR;"Accepted by SIGGRAPH Asia 2022 (Journal Track). Project page:
  https://ustc3dv.github.io/NeRFBlendShape/";nerf
2210.05135v1;http://arxiv.org/abs/2210.05135v1;2022-10-11;X-NeRF: Explicit Neural Radiance Field for Multi-Scene 360$^{\circ} $  Insufficient RGB-D Views;"Neural Radiance Fields (NeRFs), despite their outstanding performance on
novel view synthesis, often need dense input views. Many papers train one model
for each scene respectively and few of them explore incorporating multi-modal
data into this problem. In this paper, we focus on a rarely discussed but
important setting: can we train one model that can represent multiple scenes,
with 360$^\circ $ insufficient views and RGB-D images? We refer insufficient
views to few extremely sparse and almost non-overlapping views. To deal with
it, X-NeRF, a fully explicit approach which learns a general scene completion
process instead of a coordinate-based mapping, is proposed. Given a few
insufficient RGB-D input views, X-NeRF first transforms them to a sparse point
cloud tensor and then applies a 3D sparse generative Convolutional Neural
Network (CNN) to complete it to an explicit radiance field whose volumetric
rendering can be conducted fast without running networks during inference. To
avoid overfitting, besides common rendering loss, we apply perceptual loss as
well as view augmentation through random rotation on point clouds. The proposed
methodology significantly out-performs previous implicit methods in our
setting, indicating the great potential of proposed problem and approach. Codes
and data are available at https://github.com/HaoyiZhu/XNeRF.";Haoyi Zhu<author:sep>Hao-Shu Fang<author:sep>Cewu Lu;http://arxiv.org/pdf/2210.05135v1;cs.CV;;nerf
2210.04932v1;http://arxiv.org/abs/2210.04932v1;2022-10-10;NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills  using Neural Radiance Fields;"We present a system for applying sim2real approaches to ""in the wild"" scenes
with realistic visuals, and to policies which rely on active perception using
RGB cameras. Given a short video of a static scene collected using a generic
phone, we learn the scene's contact geometry and a function for novel view
synthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering
of the static scene by overlaying the rendering of other dynamic objects (e.g.
the robot's own body, a ball). A simulation is then created using the rendering
engine in a physics simulator which computes contact dynamics from the static
scene geometry (estimated from the NeRF volume density) and the dynamic
objects' geometry and physical properties (assumed known). We demonstrate that
we can use this simulation to learn vision-based whole body navigation and ball
pushing policies for a 20 degrees of freedom humanoid robot with an actuated
head-mounted RGB camera, and we successfully transfer these policies to a real
robot. Project video is available at
https://sites.google.com/view/nerf2real/home";Arunkumar Byravan<author:sep>Jan Humplik<author:sep>Leonard Hasenclever<author:sep>Arthur Brussee<author:sep>Francesco Nori<author:sep>Tuomas Haarnoja<author:sep>Ben Moran<author:sep>Steven Bohez<author:sep>Fereshteh Sadeghi<author:sep>Bojan Vujatovic<author:sep>Nicolas Heess;http://arxiv.org/pdf/2210.04932v1;cs.RO;;nerf
2210.04847v3;http://arxiv.org/abs/2210.04847v3;2022-10-10;NerfAcc: A General NeRF Acceleration Toolbox;"We propose NerfAcc, a toolbox for efficient volumetric rendering of radiance
fields. We build on the techniques proposed in Instant-NGP, and extend these
techniques to not only support bounded static scenes, but also for dynamic
scenes and unbounded scenes. NerfAcc comes with a user-friendly Python API, and
is ready for plug-and-play acceleration of most NeRFs. Various examples are
provided to show how to use this toolbox. Code can be found here:
https://github.com/KAIR-BAIR/nerfacc. Note this write-up matches with NerfAcc
v0.3.5. For the latest features in NerfAcc, please check out our more recent
write-up at arXiv:2305.04966";Ruilong Li<author:sep>Matthew Tancik<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2210.04847v3;cs.CV;"Webpage: https://www.nerfacc.com/; Updated Write-up: arXiv:2305.04966";nerf
2210.04888v1;http://arxiv.org/abs/2210.04888v1;2022-10-10;EVA3D: Compositional 3D Human Generation from 2D Image Collections;"Inverse graphics aims to recover 3D models from 2D observations. Utilizing
differentiable rendering, recent 3D-aware generative models have shown
impressive results of rigid object generation using 2D images. However, it
remains challenging to generate articulated objects, like human bodies, due to
their complexity and diversity in poses and appearances. In this work, we
propose, EVA3D, an unconditional 3D human generative model learned from 2D
image collections only. EVA3D can sample 3D humans with detailed geometry and
render high-quality images (up to 512x256) without bells and whistles (e.g.
super resolution). At the core of EVA3D is a compositional human NeRF
representation, which divides the human body into local parts. Each part is
represented by an individual volume. This compositional representation enables
1) inherent human priors, 2) adaptive allocation of network parameters, 3)
efficient training and rendering. Moreover, to accommodate for the
characteristics of sparse 2D human image collections (e.g. imbalanced pose
distribution), we propose a pose-guided sampling strategy for better GAN
learning. Extensive experiments validate that EVA3D achieves state-of-the-art
3D human generation performance regarding both geometry and texture quality.
Notably, EVA3D demonstrates great potential and scalability to
""inverse-graphics"" diverse human bodies with a clean framework.";Fangzhou Hong<author:sep>Zhaoxi Chen<author:sep>Yushi Lan<author:sep>Liang Pan<author:sep>Ziwei Liu;http://arxiv.org/pdf/2210.04888v1;cs.CV;Project Page at https://hongfz16.github.io/projects/EVA3D.html;nerf
2210.04553v1;http://arxiv.org/abs/2210.04553v1;2022-10-10;SiNeRF: Sinusoidal Neural Radiance Fields for Joint Pose Estimation and  Scene Reconstruction;"NeRFmm is the Neural Radiance Fields (NeRF) that deal with Joint Optimization
tasks, i.e., reconstructing real-world scenes and registering camera parameters
simultaneously. Despite NeRFmm producing precise scene synthesis and pose
estimations, it still struggles to outperform the full-annotated baseline on
challenging scenes. In this work, we identify that there exists a systematic
sub-optimality in joint optimization and further identify multiple potential
sources for it. To diminish the impacts of potential sources, we propose
Sinusoidal Neural Radiance Fields (SiNeRF) that leverage sinusoidal activations
for radiance mapping and a novel Mixed Region Sampling (MRS) for selecting ray
batch efficiently. Quantitative and qualitative results show that compared to
NeRFmm, SiNeRF achieves comprehensive significant improvements in image
synthesis quality and pose estimation accuracy. Codes are available at
https://github.com/yitongx/sinerf.";Yitong Xia<author:sep>Hao Tang<author:sep>Radu Timofte<author:sep>Luc Van Gool;http://arxiv.org/pdf/2210.04553v1;cs.CV;Accepted yet not published by BMVC2022;nerf
2210.04217v1;http://arxiv.org/abs/2210.04217v1;2022-10-09;Estimating Neural Reflectance Field from Radiance Field using Tree  Structures;"We present a new method for estimating the Neural Reflectance Field (NReF) of
an object from a set of posed multi-view images under unknown lighting. NReF
represents 3D geometry and appearance of objects in a disentangled manner, and
are hard to be estimated from images only. Our method solves this problem by
exploiting the Neural Radiance Field (NeRF) as a proxy representation, from
which we perform further decomposition. A high-quality NeRF decomposition
relies on good geometry information extraction as well as good prior terms to
properly resolve ambiguities between different components. To extract
high-quality geometry information from radiance fields, we re-design a new
ray-casting based method for surface point extraction. To efficiently compute
and apply prior terms, we convert different prior terms into different type of
filter operations on the surface extracted from radiance field. We then employ
two type of auxiliary data structures, namely Gaussian KD-tree and octree, to
support fast querying of surface points and efficient computation of surface
filters during training. Based on this, we design a multi-stage decomposition
optimization pipeline for estimating neural reflectance field from neural
radiance fields. Extensive experiments show our method outperforms other
state-of-the-art methods on different data, and enable high-quality free-view
relighting as well as material editing tasks.";Xiu Li<author:sep>Xiao Li<author:sep>Yan Lu;http://arxiv.org/pdf/2210.04217v1;cs.CV;;nerf
2210.04233v1;http://arxiv.org/abs/2210.04233v1;2022-10-09;Robustifying the Multi-Scale Representation of Neural Radiance Fields;"Neural Radiance Fields (NeRF) recently emerged as a new paradigm for object
representation from multi-view (MV) images. Yet, it cannot handle multi-scale
(MS) images and camera pose estimation errors, which generally is the case with
multi-view images captured from a day-to-day commodity camera. Although
recently proposed Mip-NeRF could handle multi-scale imaging problems with NeRF,
it cannot handle camera pose estimation error. On the other hand, the newly
proposed BARF can solve the camera pose problem with NeRF but fails if the
images are multi-scale in nature. This paper presents a robust multi-scale
neural radiance fields representation approach to simultaneously overcome both
real-world imaging issues. Our method handles multi-scale imaging effects and
camera-pose estimation problems with NeRF-inspired approaches by leveraging the
fundamentals of scene rigidity. To reduce unpleasant aliasing artifacts due to
multi-scale images in the ray space, we leverage Mip-NeRF multi-scale
representation. For joint estimation of robust camera pose, we propose
graph-neural network-based multiple motion averaging in the neural volume
rendering framework. We demonstrate, with examples, that for an accurate neural
representation of an object from day-to-day acquired multi-view images, it is
crucial to have precise camera-pose estimates. Without considering robustness
measures in the camera pose estimation, modeling for multi-scale aliasing
artifacts via conical frustum can be counterproductive. We present extensive
experiments on the benchmark datasets to demonstrate that our approach provides
better results than the recent NeRF-inspired approaches for such realistic
settings.";Nishant Jain<author:sep>Suryansh Kumar<author:sep>Luc Van Gool;http://arxiv.org/pdf/2210.04233v1;cs.CV;"Accepted for publication at British Machine Vision Conference (BMVC)
  2022. Draft info: 13 pages, 3 Figures, and 4 Tables";nerf
2210.04214v2;http://arxiv.org/abs/2210.04214v2;2022-10-09;VM-NeRF: Tackling Sparsity in NeRF with View Morphing;"NeRF aims to learn a continuous neural scene representation by using a finite
set of input images taken from various viewpoints. A well-known limitation of
NeRF methods is their reliance on data: the fewer the viewpoints, the higher
the likelihood of overfitting. This paper addresses this issue by introducing a
novel method to generate geometrically consistent image transitions between
viewpoints using View Morphing. Our VM-NeRF approach requires no prior
knowledge about the scene structure, as View Morphing is based on the
fundamental principles of projective geometry. VM-NeRF tightly integrates this
geometric view generation process during the training procedure of standard
NeRF approaches. Notably, our method significantly improves novel view
synthesis, particularly when only a few views are available. Experimental
evaluation reveals consistent improvement over current methods that handle
sparse viewpoints in NeRF models. We report an increase in PSNR of up to 1.8dB
and 1.0dB when training uses eight and four views, respectively. Source code:
\url{https://github.com/mbortolon97/VM-NeRF}";Matteo Bortolon<author:sep>Alessio Del Bue<author:sep>Fabio Poiesi;http://arxiv.org/pdf/2210.04214v2;cs.CV;ICIAP 2023;nerf
2210.04127v1;http://arxiv.org/abs/2210.04127v1;2022-10-09;Towards Efficient Neural Scene Graphs by Learning Consistency Fields;"Neural Radiance Fields (NeRF) achieves photo-realistic image rendering from
novel views, and the Neural Scene Graphs (NSG) \cite{ost2021neural} extends it
to dynamic scenes (video) with multiple objects. Nevertheless, computationally
heavy ray marching for every image frame becomes a huge burden. In this paper,
taking advantage of significant redundancy across adjacent frames in videos, we
propose a feature-reusing framework. From the first try of naively reusing the
NSG features, however, we learn that it is crucial to disentangle
object-intrinsic properties consistent across frames from transient ones. Our
proposed method, \textit{Consistency-Field-based NSG (CF-NSG)}, reformulates
neural radiance fields to additionally consider \textit{consistency fields}.
With disentangled representations, CF-NSG takes full advantage of the
feature-reusing scheme and performs an extended degree of scene manipulation in
a more controllable manner. We empirically verify that CF-NSG greatly improves
the inference efficiency by using 85\% less queries than NSG without notable
degradation in rendering quality. Code will be available at:
https://github.com/ldynx/CF-NSG";Yeji Song<author:sep>Chaerin Kong<author:sep>Seoyoung Lee<author:sep>Nojun Kwak<author:sep>Joonseok Lee;http://arxiv.org/pdf/2210.04127v1;cs.CV;BMVC 2022, 22 pages;nerf
2210.03895v1;http://arxiv.org/abs/2210.03895v1;2022-10-08;ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial  Viewpoints;"Recent studies have demonstrated that visual recognition models lack
robustness to distribution shift. However, current work mainly considers model
robustness to 2D image transformations, leaving viewpoint changes in the 3D
world less explored. In general, viewpoint changes are prevalent in various
real-world applications (e.g., autonomous driving), making it imperative to
evaluate viewpoint robustness. In this paper, we propose a novel method called
ViewFool to find adversarial viewpoints that mislead visual recognition models.
By encoding real-world objects as neural radiance fields (NeRF), ViewFool
characterizes a distribution of diverse adversarial viewpoints under an
entropic regularizer, which helps to handle the fluctuations of the real camera
pose and mitigate the reality gap between the real objects and their neural
representations. Experiments validate that the common image classifiers are
extremely vulnerable to the generated adversarial viewpoints, which also
exhibit high cross-model transferability. Based on ViewFool, we introduce
ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint
robustness of image classifiers. Evaluation results on 40 classifiers with
diverse architectures, objective functions, and data augmentations reveal a
significant drop in model performance when tested on ImageNet-V, which provides
a possibility to leverage ViewFool as an effective data augmentation strategy
to improve viewpoint robustness.";Yinpeng Dong<author:sep>Shouwei Ruan<author:sep>Hang Su<author:sep>Caixin Kang<author:sep>Xingxing Wei<author:sep>Jun Zhu;http://arxiv.org/pdf/2210.03895v1;cs.CV;NeurIPS 2022;nerf
2210.01651v1;http://arxiv.org/abs/2210.01651v1;2022-10-04;SelfNeRF: Fast Training NeRF for Human from Monocular Self-rotating  Video;"In this paper, we propose SelfNeRF, an efficient neural radiance field based
novel view synthesis method for human performance. Given monocular
self-rotating videos of human performers, SelfNeRF can train from scratch and
achieve high-fidelity results in about twenty minutes. Some recent works have
utilized the neural radiance field for dynamic human reconstruction. However,
most of these methods need multi-view inputs and require hours of training,
making it still difficult for practical use. To address this challenging
problem, we introduce a surface-relative representation based on
multi-resolution hash encoding that can greatly improve the training speed and
aggregate inter-frame information. Extensive experimental results on several
different datasets demonstrate the effectiveness and efficiency of SelfNeRF to
challenging monocular videos.";Bo Peng<author:sep>Jun Hu<author:sep>Jingtao Zhou<author:sep>Juyong Zhang;http://arxiv.org/pdf/2210.01651v1;cs.CV;Project page: https://ustc3dv.github.io/SelfNeRF;nerf
2210.01868v1;http://arxiv.org/abs/2210.01868v1;2022-10-04;Capturing and Animation of Body and Clothing from Monocular Video;"While recent work has shown progress on extracting clothed 3D human avatars
from a single image, video, or a set of 3D scans, several limitations remain.
Most methods use a holistic representation to jointly model the body and
clothing, which means that the clothing and body cannot be separated for
applications like virtual try-on. Other methods separately model the body and
clothing, but they require training from a large set of 3D clothed human meshes
obtained from 3D/4D scanners or physics simulations. Our insight is that the
body and clothing have different modeling requirements. While the body is well
represented by a mesh-based parametric 3D model, implicit representations and
neural radiance fields are better suited to capturing the large variety in
shape and appearance present in clothing. Building on this insight, we propose
SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a
mesh-based body with a neural radiance field. Integrating the mesh into the
volumetric rendering in combination with a differentiable rasterizer enables us
to optimize SCARF directly from monocular videos, without any 3D supervision.
The hybrid modeling enables SCARF to (i) animate the clothed body avatar by
changing body poses (including hand articulation and facial expressions), (ii)
synthesize novel views of the avatar, and (iii) transfer clothing between
avatars in virtual try-on applications. We demonstrate that SCARF reconstructs
clothing with higher visual quality than existing methods, that the clothing
deforms with changing body pose and body shape, and that clothing can be
successfully transferred between avatars of different subjects. The code and
models are available at https://github.com/YadiraF/SCARF.";Yao Feng<author:sep>Jinlong Yang<author:sep>Marc Pollefeys<author:sep>Michael J. Black<author:sep>Timo Bolkart;http://arxiv.org/pdf/2210.01868v1;cs.CV;7 pages main paper, 2 pages supp. mat;
2210.01166v1;http://arxiv.org/abs/2210.01166v1;2022-10-03;NARF22: Neural Articulated Radiance Fields for Configuration-Aware  Rendering;"Articulated objects pose a unique challenge for robotic perception and
manipulation. Their increased number of degrees-of-freedom makes tasks such as
localization computationally difficult, while also making the process of
real-world dataset collection unscalable. With the aim of addressing these
scalability issues, we propose Neural Articulated Radiance Fields (NARF22), a
pipeline which uses a fully-differentiable, configuration-parameterized Neural
Radiance Field (NeRF) as a means of providing high quality renderings of
articulated objects. NARF22 requires no explicit knowledge of the object
structure at inference time. We propose a two-stage parts-based training
mechanism which allows the object rendering models to generalize well across
the configuration space even if the underlying training data has as few as one
configuration represented. We demonstrate the efficacy of NARF22 by training
configurable renderers on a real-world articulated tool dataset collected via a
Fetch mobile manipulation robot. We show the applicability of the model to
gradient-based inference methods through a configuration estimation and 6
degree-of-freedom pose refinement task. The project webpage is available at:
https://progress.eecs.umich.edu/projects/narf/.";Stanley Lewis<author:sep>Jana Pavlasek<author:sep>Odest Chadwicke Jenkins;http://arxiv.org/pdf/2210.01166v1;cs.RO;"Accepted to the 2022 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS). Contact: Stanley Lewis, stanlew@umich.edu";nerf
2210.00647v3;http://arxiv.org/abs/2210.00647v3;2022-10-02;IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable  Novel View Synthesis;"Existing inverse rendering combined with neural rendering methods can only
perform editable novel view synthesis on object-specific scenes, while we
present intrinsic neural radiance fields, dubbed IntrinsicNeRF, which introduce
intrinsic decomposition into the NeRF-based neural rendering method and can
extend its application to room-scale scenes. Since intrinsic decomposition is a
fundamentally under-constrained inverse problem, we propose a novel
distance-aware point sampling and adaptive reflectance iterative clustering
optimization method, which enables IntrinsicNeRF with traditional intrinsic
decomposition constraints to be trained in an unsupervised manner, resulting in
multi-view consistent intrinsic decomposition results. To cope with the problem
that different adjacent instances of similar reflectance in a scene are
incorrectly clustered together, we further propose a hierarchical clustering
method with coarse-to-fine optimization to obtain a fast hierarchical indexing
representation. It supports compelling real-time augmented applications such as
recoloring and illumination variation. Extensive experiments and editing
samples on both object-specific/room-scale scenes and synthetic/real-word data
demonstrate that we can obtain consistent intrinsic decomposition results and
high-fidelity novel view synthesis even for challenging sequences.";Weicai Ye<author:sep>Shuo Chen<author:sep>Chong Bao<author:sep>Hujun Bao<author:sep>Marc Pollefeys<author:sep>Zhaopeng Cui<author:sep>Guofeng Zhang;http://arxiv.org/pdf/2210.00647v3;cs.CV;"Accepted to ICCV2023, Project webpage:
  https://zju3dv.github.io/intrinsic_nerf/, code:
  https://github.com/zju3dv/IntrinsicNeRF";nerf
2210.01548v1;http://arxiv.org/abs/2210.01548v1;2022-10-02;Neural Implicit Surface Reconstruction from Noisy Camera Observations;"Representing 3D objects and scenes with neural radiance fields has become
very popular over the last years. Recently, surface-based representations have
been proposed, that allow to reconstruct 3D objects from simple photographs.
However, most current techniques require an accurate camera calibration, i.e.
camera parameters corresponding to each image, which is often a difficult task
to do in real-life situations. To this end, we propose a method for learning 3D
surfaces from noisy camera parameters. We show that we can learn camera
parameters together with learning the surface representation, and demonstrate
good quality 3D surface reconstruction even with noisy camera observations.";Sarthak Gupta<author:sep>Patrik Huber;http://arxiv.org/pdf/2210.01548v1;cs.CV;4 pages - 2 for paper, 2 for supplementary;
2210.00489v2;http://arxiv.org/abs/2210.00489v2;2022-10-02;Unsupervised Multi-View Object Segmentation Using Radiance Field  Propagation;"We present radiance field propagation (RFP), a novel approach to segmenting
objects in 3D during reconstruction given only unlabeled multi-view images of a
scene. RFP is derived from emerging neural radiance field-based techniques,
which jointly encodes semantics with appearance and geometry. The core of our
method is a novel propagation strategy for individual objects' radiance fields
with a bidirectional photometric loss, enabling an unsupervised partitioning of
a scene into salient or meaningful regions corresponding to different object
instances. To better handle complex scenes with multiple objects and
occlusions, we further propose an iterative expectation-maximization algorithm
to refine object masks. RFP is one of the first unsupervised approach for
tackling 3D real scene object segmentation for neural radiance field (NeRF)
without any supervision, annotations, or other cues such as 3D bounding boxes
and prior knowledge of object class. Experiments demonstrate that RFP achieves
feasible segmentation results that are more accurate than previous unsupervised
image/scene segmentation approaches, and are comparable to existing supervised
NeRF-based methods. The segmented object representations enable individual 3D
object editing operations.";Xinhang Liu<author:sep>Jiaben Chen<author:sep>Huai Yu<author:sep>Yu-Wing Tai<author:sep>Chi-Keung Tang;http://arxiv.org/pdf/2210.00489v2;cs.CV;23 pages, 14 figures, NeurIPS 2022;nerf
2210.00183v1;http://arxiv.org/abs/2210.00183v1;2022-10-01;Structure-Aware NeRF without Posed Camera via Epipolar Constraint;"The neural radiance field (NeRF) for realistic novel view synthesis requires
camera poses to be pre-acquired by a structure-from-motion (SfM) approach. This
two-stage strategy is not convenient to use and degrades the performance
because the error in the pose extraction can propagate to the view synthesis.
We integrate the pose extraction and view synthesis into a single end-to-end
procedure so they can benefit from each other. For training NeRF models, only
RGB images are given, without pre-known camera poses. The camera poses are
obtained by the epipolar constraint in which the identical feature in different
views has the same world coordinates transformed from the local camera
coordinates according to the extracted poses. The epipolar constraint is
jointly optimized with pixel color constraint. The poses are represented by a
CNN-based deep network, whose input is the related frames. This joint
optimization enables NeRF to be aware of the scene's structure that has an
improved generalization performance. Extensive experiments on a variety of
scenes demonstrate the effectiveness of the proposed approach. Code is
available at https://github.com/XTU-PR-LAB/SaNerf.";Shu Chen<author:sep>Yang Zhang<author:sep>Yaxin Xu<author:sep>Beiji Zou;http://arxiv.org/pdf/2210.00183v1;cs.CV;;nerf
2210.00379v5;http://arxiv.org/abs/2210.00379v5;2022-10-01;NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review;"Neural Radiance Field (NeRF) has recently become a significant development in
the field of Computer Vision, allowing for implicit, neural network-based scene
representation and novel view synthesis. NeRF models have found diverse
applications in robotics, urban mapping, autonomous navigation, virtual
reality/augmented reality, and more. Due to the growing popularity of NeRF and
its expanding research area, we present a comprehensive survey of NeRF papers
from the past two years. Our survey is organized into architecture and
application-based taxonomies and provides an introduction to the theory of NeRF
and its training via differentiable volume rendering. We also present a
benchmark comparison of the performance and speed of key NeRF models. By
creating this survey, we hope to introduce new researchers to NeRF, provide a
helpful reference for influential works in this field, as well as motivate
future research directions with our discussion section.";Kyle Gao<author:sep>Yina Gao<author:sep>Hongjie He<author:sep>Dening Lu<author:sep>Linlin Xu<author:sep>Jonathan Li;http://arxiv.org/pdf/2210.00379v5;cs.CV;Fixed some typos from previous version;nerf
2209.15529v1;http://arxiv.org/abs/2209.15529v1;2022-09-30;TT-NF: Tensor Train Neural Fields;"Learning neural fields has been an active topic in deep learning research,
focusing, among other issues, on finding more compact and easy-to-fit
representations. In this paper, we introduce a novel low-rank representation
termed Tensor Train Neural Fields (TT-NF) for learning neural fields on dense
regular grids and efficient methods for sampling from them. Our representation
is a TT parameterization of the neural field, trained with backpropagation to
minimize a non-convex objective. We analyze the effect of low-rank compression
on the downstream task quality metrics in two settings. First, we demonstrate
the efficiency of our method in a sandbox task of tensor denoising, which
admits comparison with SVD-based schemes designed to minimize reconstruction
error. Furthermore, we apply the proposed approach to Neural Radiance Fields,
where the low-rank structure of the field corresponding to the best quality can
be discovered only through learning.";Anton Obukhov<author:sep>Mikhail Usvyatsov<author:sep>Christos Sakaridis<author:sep>Konrad Schindler<author:sep>Luc Van Gool;http://arxiv.org/pdf/2209.15529v1;cs.LG;Preprint, under review;
2209.15172v1;http://arxiv.org/abs/2209.15172v1;2022-09-30;Understanding Pure CLIP Guidance for Voxel Grid NeRF Models;"We explore the task of text to 3D object generation using CLIP. Specifically,
we use CLIP for guidance without access to any datasets, a setting we refer to
as pure CLIP guidance. While prior work has adopted this setting, there is no
systematic study of mechanics for preventing adversarial generations within
CLIP. We illustrate how different image-based augmentations prevent the
adversarial generation problem, and how the generated results are impacted. We
test different CLIP model architectures and show that ensembling different
models for guidance can prevent adversarial generations within bigger models
and generate sharper results. Furthermore, we implement an implicit voxel grid
model to show how neural networks provide an additional layer of
regularization, resulting in better geometrical structure and coherency of
generated objects. Compared to prior work, we achieve more coherent results
with higher memory efficiency and faster training speeds.";Han-Hung Lee<author:sep>Angel X. Chang;http://arxiv.org/pdf/2209.15172v1;cs.CV;;nerf
2209.15637v1;http://arxiv.org/abs/2209.15637v1;2022-09-30;Improving 3D-aware Image Synthesis with A Geometry-aware Discriminator;"3D-aware image synthesis aims at learning a generative model that can render
photo-realistic 2D images while capturing decent underlying 3D shapes. A
popular solution is to adopt the generative adversarial network (GAN) and
replace the generator with a 3D renderer, where volume rendering with neural
radiance field (NeRF) is commonly used. Despite the advancement of synthesis
quality, existing methods fail to obtain moderate 3D shapes. We argue that,
considering the two-player game in the formulation of GANs, only making the
generator 3D-aware is not enough. In other words, displacing the generative
mechanism only offers the capability, but not the guarantee, of producing
3D-aware images, because the supervision of the generator primarily comes from
the discriminator. To address this issue, we propose GeoD through learning a
geometry-aware discriminator to improve 3D-aware GANs. Concretely, besides
differentiating real and fake samples from the 2D image space, the
discriminator is additionally asked to derive the geometry information from the
inputs, which is then applied as the guidance of the generator. Such a simple
yet effective design facilitates learning substantially more accurate 3D
shapes. Extensive experiments on various generator architectures and training
datasets verify the superiority of GeoD over state-of-the-art alternatives.
Moreover, our approach is registered as a general framework such that a more
capable discriminator (i.e., with a third task of novel view synthesis beyond
domain classification and geometry extraction) can further assist the generator
with a better multi-view consistency.";Zifan Shi<author:sep>Yinghao Xu<author:sep>Yujun Shen<author:sep>Deli Zhao<author:sep>Qifeng Chen<author:sep>Dit-Yan Yeung;http://arxiv.org/pdf/2209.15637v1;cs.CV;"Accepted by NeurIPS 2022. Project page:
  https://vivianszf.github.io/geod";nerf
2209.14988v1;http://arxiv.org/abs/2209.14988v1;2022-09-29;DreamFusion: Text-to-3D using 2D Diffusion;"Recent breakthroughs in text-to-image synthesis have been driven by diffusion
models trained on billions of image-text pairs. Adapting this approach to 3D
synthesis would require large-scale datasets of labeled 3D data and efficient
architectures for denoising 3D data, neither of which currently exist. In this
work, we circumvent these limitations by using a pretrained 2D text-to-image
diffusion model to perform text-to-3D synthesis. We introduce a loss based on
probability density distillation that enables the use of a 2D diffusion model
as a prior for optimization of a parametric image generator. Using this loss in
a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a
Neural Radiance Field, or NeRF) via gradient descent such that its 2D
renderings from random angles achieve a low loss. The resulting 3D model of the
given text can be viewed from any angle, relit by arbitrary illumination, or
composited into any 3D environment. Our approach requires no 3D training data
and no modifications to the image diffusion model, demonstrating the
effectiveness of pretrained image diffusion models as priors.";Ben Poole<author:sep>Ajay Jain<author:sep>Jonathan T. Barron<author:sep>Ben Mildenhall;http://arxiv.org/pdf/2209.14988v1;cs.CV;see project page at https://dreamfusion3d.github.io/;nerf
2209.14819v2;http://arxiv.org/abs/2209.14819v2;2022-09-29;SymmNeRF: Learning to Explore Symmetry Prior for Single-View View  Synthesis;"We study the problem of novel view synthesis of objects from a single image.
Existing methods have demonstrated the potential in single-view view synthesis.
However, they still fail to recover the fine appearance details, especially in
self-occluded areas. This is because a single view only provides limited
information. We observe that manmade objects usually exhibit symmetric
appearances, which introduce additional prior knowledge. Motivated by this, we
investigate the potential performance gains of explicitly embedding symmetry
into the scene representation. In this paper, we propose SymmNeRF, a neural
radiance field (NeRF) based framework that combines local and global
conditioning under the introduction of symmetry priors. In particular, SymmNeRF
takes the pixel-aligned image features and the corresponding symmetric features
as extra inputs to the NeRF, whose parameters are generated by a hypernetwork.
As the parameters are conditioned on the image-encoded latent codes, SymmNeRF
is thus scene-independent and can generalize to new scenes. Experiments on
synthetic and real-world datasets show that SymmNeRF synthesizes novel views
with more details regardless of the pose transformation, and demonstrates good
generalization when applied to unseen objects. Code is available at:
https://github.com/xingyi-li/SymmNeRF.";Xingyi Li<author:sep>Chaoyi Hong<author:sep>Yiran Wang<author:sep>Zhiguo Cao<author:sep>Ke Xian<author:sep>Guosheng Lin;http://arxiv.org/pdf/2209.14819v2;cs.CV;Accepted by ACCV 2022;nerf
2209.14265v2;http://arxiv.org/abs/2209.14265v2;2022-09-28;360FusionNeRF: Panoramic Neural Radiance Fields with Joint Guidance;"We present a method to synthesize novel views from a single $360^\circ$
panorama image based on the neural radiance field (NeRF). Prior studies in a
similar setting rely on the neighborhood interpolation capability of
multi-layer perceptions to complete missing regions caused by occlusion, which
leads to artifacts in their predictions. We propose 360FusionNeRF, a
semi-supervised learning framework where we introduce geometric supervision and
semantic consistency to guide the progressive training process. Firstly, the
input image is re-projected to $360^\circ$ images, and auxiliary depth maps are
extracted at other camera positions. The depth supervision, in addition to the
NeRF color guidance, improves the geometry of the synthesized views.
Additionally, we introduce a semantic consistency loss that encourages
realistic renderings of novel views. We extract these semantic features using a
pre-trained visual encoder such as CLIP, a Vision Transformer trained on
hundreds of millions of diverse 2D photographs mined from the web with natural
language supervision. Experiments indicate that our proposed method can produce
plausible completions of unobserved regions while preserving the features of
the scene. When trained across various scenes, 360FusionNeRF consistently
achieves the state-of-the-art performance when transferring to synthetic
Structured3D dataset (PSNR~5%, SSIM~3% LPIPS~13%), real-world Matterport3D
dataset (PSNR~3%, SSIM~3% LPIPS~9%) and Replica360 dataset (PSNR~8%, SSIM~2%
LPIPS~18%).";Shreyas Kulkarni<author:sep>Peng Yin<author:sep>Sebastian Scherer;http://arxiv.org/pdf/2209.14265v2;cs.CV;"8 pages, Fig 3, Submitted to IEEE RAL. arXiv admin note: text overlap
  with arXiv:2106.10859, arXiv:2104.00677, arXiv:2203.09957, arXiv:2204.00928
  by other authors";nerf
2209.13091v2;http://arxiv.org/abs/2209.13091v2;2022-09-27;WaterNeRF: Neural Radiance Fields for Underwater Scenes;"Underwater imaging is a critical task performed by marine robots for a wide
range of applications including aquaculture, marine infrastructure inspection,
and environmental monitoring. However, water column effects, such as
attenuation and backscattering, drastically change the color and quality of
imagery captured underwater. Due to varying water conditions and
range-dependency of these effects, restoring underwater imagery is a
challenging problem. This impacts downstream perception tasks including depth
estimation and 3D reconstruction. In this paper, we advance state-of-the-art in
neural radiance fields (NeRFs) to enable physics-informed dense depth
estimation and color correction. Our proposed method, WaterNeRF, estimates
parameters of a physics-based model for underwater image formation, leading to
a hybrid data-driven and model-based solution. After determining the scene
structure and radiance field, we can produce novel views of degraded as well as
corrected underwater images, along with dense depth of the scene. We evaluate
the proposed method qualitatively and quantitatively on a real underwater
dataset.";Advaith Venkatramanan Sethuraman<author:sep>Manikandasriram Srinivasan Ramanagopal<author:sep>Katherine A. Skinner;http://arxiv.org/pdf/2209.13091v2;cs.RO;;nerf
2209.13433v1;http://arxiv.org/abs/2209.13433v1;2022-09-27;OmniNeRF: Hybriding Omnidirectional Distance and Radiance fields for  Neural Surface Reconstruction;"3D reconstruction from images has wide applications in Virtual Reality and
Automatic Driving, where the precision requirement is very high.
Ground-breaking research in the neural radiance field (NeRF) by utilizing
Multi-Layer Perceptions has dramatically improved the representation quality of
3D objects. Some later studies improved NeRF by building truncated signed
distance fields (TSDFs) but still suffer from the problem of blurred surfaces
in 3D reconstruction. In this work, this surface ambiguity is addressed by
proposing a novel way of 3D shape representation, OmniNeRF. It is based on
training a hybrid implicit field of Omni-directional Distance Field (ODF) and
neural radiance field, replacing the apparent density in NeRF with
omnidirectional information. Moreover, we introduce additional supervision on
the depth map to further improve reconstruction quality. The proposed method
has been proven to effectively deal with NeRF defects at the edges of the
surface reconstruction, providing higher quality 3D scene reconstruction
results.";Jiaming Shen<author:sep>Bolin Song<author:sep>Zirui Wu<author:sep>Yi Xu;http://arxiv.org/pdf/2209.13433v1;cs.CV;Accepted by CMSDA 2022;nerf
2209.13274v2;http://arxiv.org/abs/2209.13274v2;2022-09-27;Orbeez-SLAM: A Real-time Monocular Visual SLAM with ORB Features and  NeRF-realized Mapping;"A spatial AI that can perform complex tasks through visual signals and
cooperate with humans is highly anticipated. To achieve this, we need a visual
SLAM that easily adapts to new scenes without pre-training and generates dense
maps for downstream tasks in real-time. None of the previous learning-based and
non-learning-based visual SLAMs satisfy all needs due to the intrinsic
limitations of their components. In this work, we develop a visual SLAM named
Orbeez-SLAM, which successfully collaborates with implicit neural
representation and visual odometry to achieve our goals. Moreover, Orbeez-SLAM
can work with the monocular camera since it only needs RGB inputs, making it
widely applicable to the real world. Results show that our SLAM is up to 800x
faster than the strong baseline with superior rendering outcomes. Code link:
https://github.com/MarvinChung/Orbeez-SLAM.";Chi-Ming Chung<author:sep>Yang-Che Tseng<author:sep>Ya-Ching Hsu<author:sep>Xiang-Qian Shi<author:sep>Yun-Hung Hua<author:sep>Jia-Fong Yeh<author:sep>Wen-Chin Chen<author:sep>Yi-Ting Chen<author:sep>Winston H. Hsu;http://arxiv.org/pdf/2209.13274v2;cs.RO;;nerf
2209.12744v1;http://arxiv.org/abs/2209.12744v1;2022-09-26;Baking in the Feature: Accelerating Volumetric Segmentation by Rendering  Feature Maps;"Methods have recently been proposed that densely segment 3D volumes into
classes using only color images and expert supervision in the form of sparse
semantically annotated pixels. While impressive, these methods still require a
relatively large amount of supervision and segmenting an object can take
several minutes in practice. Such systems typically only optimize their
representation on the particular scene they are fitting, without leveraging any
prior information from previously seen images. In this paper, we propose to use
features extracted with models trained on large existing datasets to improve
segmentation performance. We bake this feature representation into a Neural
Radiance Field (NeRF) by volumetrically rendering feature maps and supervising
on features extracted from each input image. We show that by baking this
representation into the NeRF, we make the subsequent classification task much
easier. Our experiments show that our method achieves higher segmentation
accuracy with fewer semantic annotations than existing methods over a wide
range of scenes.";Kenneth Blomqvist<author:sep>Lionel Ott<author:sep>Jen Jen Chung<author:sep>Roland Siegwart;http://arxiv.org/pdf/2209.12744v1;cs.CV;;nerf
2209.12266v3;http://arxiv.org/abs/2209.12266v3;2022-09-25;Enforcing safety for vision-based controllers via Control Barrier  Functions and Neural Radiance Fields;"To navigate complex environments, robots must increasingly use
high-dimensional visual feedback (e.g. images) for control. However, relying on
high-dimensional image data to make control decisions raises important
questions; particularly, how might we prove the safety of a visual-feedback
controller? Control barrier functions (CBFs) are powerful tools for certifying
the safety of feedback controllers in the state-feedback setting, but CBFs have
traditionally been poorly-suited to visual feedback control due to the need to
predict future observations in order to evaluate the barrier function. In this
work, we solve this issue by leveraging recent advances in neural radiance
fields (NeRFs), which learn implicit representations of 3D scenes and can
render images from previously-unseen camera perspectives, to provide
single-step visual foresight for a CBF-based controller. This novel combination
is able to filter out unsafe actions and intervene to preserve safety. We
demonstrate the effect of our controller in real-time simulation experiments
where it successfully prevents the robot from taking dangerous actions.";Mukun Tong<author:sep>Charles Dawson<author:sep>Chuchu Fan;http://arxiv.org/pdf/2209.12266v3;cs.RO;Accepted to ICRA 2023;nerf
2209.12068v2;http://arxiv.org/abs/2209.12068v2;2022-09-24;NeRF-Loc: Transformer-Based Object Localization Within Neural Radiance  Fields;"Neural Radiance Fields (NeRFs) have become a widely-applied scene
representation technique in recent years, showing advantages for robot
navigation and manipulation tasks. To further advance the utility of NeRFs for
robotics, we propose a transformer-based framework, NeRF-Loc, to extract 3D
bounding boxes of objects in NeRF scenes. NeRF-Loc takes a pre-trained NeRF
model and camera view as input and produces labeled, oriented 3D bounding boxes
of objects as output. Using current NeRF training tools, a robot can train a
NeRF environment model in real-time and, using our algorithm, identify 3D
bounding boxes of objects of interest within the NeRF for downstream navigation
or manipulation tasks. Concretely, we design a pair of paralleled transformer
encoder branches, namely the coarse stream and the fine stream, to encode both
the context and details of target objects. The encoded features are then fused
together with attention layers to alleviate ambiguities for accurate object
localization. We have compared our method with conventional RGB(-D) based
methods that take rendered RGB images and depths from NeRFs as inputs. Our
method is better than the baselines.";Jiankai Sun<author:sep>Yan Xu<author:sep>Mingyu Ding<author:sep>Hongwei Yi<author:sep>Chen Wang<author:sep>Jingdong Wang<author:sep>Liangjun Zhang<author:sep>Mac Schwager;http://arxiv.org/pdf/2209.12068v2;cs.CV;;nerf
2209.08776v6;http://arxiv.org/abs/2209.08776v6;2022-09-19;NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes;"Neural volumetric representations have shown the potential that Multi-layer
Perceptrons (MLPs) can be optimized with multi-view calibrated images to
represent scene geometry and appearance, without explicit 3D supervision.
Object segmentation can enrich many downstream applications based on the
learned radiance field. However, introducing hand-crafted segmentation to
define regions of interest in a complex real-world scene is non-trivial and
expensive as it acquires per view annotation. This paper carries out the
exploration of self-supervised learning for object segmentation using NeRF for
complex real-world scenes. Our framework, called NeRF with Self-supervised
Object Segmentation NeRF-SOS, couples object segmentation and neural radiance
field to segment objects in any view within a scene. By proposing a novel
collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS
encourages NeRF models to distill compact geometry-aware segmentation clusters
from their density fields and the self-supervised pre-trained 2D visual
features. The self-supervised object segmentation framework can be applied to
various NeRF models that both lead to photo-realistic rendering results and
convincing segmentation maps for both indoor and outdoor scenarios. Extensive
results on the LLFF, Tank & Temple, and BlendedMVS datasets validate the
effectiveness of NeRF-SOS. It consistently surpasses other 2D-based
self-supervised baselines and predicts finer semantics masks than existing
supervised counterparts. Please refer to the video on our project page for more
details:https://zhiwenfan.github.io/NeRF-SOS.";Zhiwen Fan<author:sep>Peihao Wang<author:sep>Yifan Jiang<author:sep>Xinyu Gong<author:sep>Dejia Xu<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2209.08776v6;cs.CV;;nerf
2209.08718v1;http://arxiv.org/abs/2209.08718v1;2022-09-19;Density-aware NeRF Ensembles: Quantifying Predictive Uncertainty in  Neural Radiance Fields;"We show that ensembling effectively quantifies model uncertainty in Neural
Radiance Fields (NeRFs) if a density-aware epistemic uncertainty term is
considered. The naive ensembles investigated in prior work simply average
rendered RGB images to quantify the model uncertainty caused by conflicting
explanations of the observed scene. In contrast, we additionally consider the
termination probabilities along individual rays to identify epistemic model
uncertainty due to a lack of knowledge about the parts of a scene unobserved
during training. We achieve new state-of-the-art performance across established
uncertainty quantification benchmarks for NeRFs, outperforming methods that
require complex changes to the NeRF architecture and training regime. We
furthermore demonstrate that NeRF uncertainty can be utilised for next-best
view selection and model refinement.";Niko SÃ¼nderhauf<author:sep>Jad Abou-Chakra<author:sep>Dimity Miller;http://arxiv.org/pdf/2209.08718v1;cs.CV;;nerf
2209.09050v1;http://arxiv.org/abs/2209.09050v1;2022-09-19;Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields;"We present Loc-NeRF, a real-time vision-based robot localization approach
that combines Monte Carlo localization and Neural Radiance Fields (NeRF). Our
system uses a pre-trained NeRF model as the map of an environment and can
localize itself in real-time using an RGB camera as the only exteroceptive
sensor onboard the robot. While neural radiance fields have seen significant
applications for visual rendering in computer vision and graphics, they have
found limited use in robotics. Existing approaches for NeRF-based localization
require both a good initial pose guess and significant computation, making them
impractical for real-time robotics applications. By using Monte Carlo
localization as a workhorse to estimate poses using a NeRF map model, Loc-NeRF
is able to perform localization faster than the state of the art and without
relying on an initial pose estimate. In addition to testing on synthetic data,
we also run our system using real data collected by a Clearpath Jackal UGV and
demonstrate for the first time the ability to perform real-time global
localization with neural radiance fields. We make our code publicly available
at https://github.com/MIT-SPARK/Loc-NeRF.";Dominic Maggio<author:sep>Marcus Abate<author:sep>Jingnan Shi<author:sep>Courtney Mario<author:sep>Luca Carlone;http://arxiv.org/pdf/2209.09050v1;cs.RO;;nerf
2209.08498v2;http://arxiv.org/abs/2209.08498v2;2022-09-18;LATITUDE: Robotic Global Localization with Truncated Dynamic Low-pass  Filter in City-scale NeRF;"Neural Radiance Fields (NeRFs) have made great success in representing
complex 3D scenes with high-resolution details and efficient memory.
Nevertheless, current NeRF-based pose estimators have no initial pose
prediction and are prone to local optima during optimization. In this paper, we
present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter,
which introduces a two-stage localization mechanism in city-scale NeRF. In
place recognition stage, we train a regressor through images generated from
trained NeRFs, which provides an initial value for global localization. In pose
optimization stage, we minimize the residual between the observed image and
rendered image by directly optimizing the pose on tangent plane. To avoid
convergence to local optimum, we introduce a Truncated Dynamic Low-pass Filter
(TDLF) for coarse-to-fine pose registration. We evaluate our method on both
synthetic and real-world data and show its potential applications for
high-precision navigation in large-scale city scenes. Codes and data will be
publicly available at https://github.com/jike5/LATITUDE.";Zhenxin Zhu<author:sep>Yuantao Chen<author:sep>Zirui Wu<author:sep>Chao Hou<author:sep>Yongliang Shi<author:sep>Chuxuan Li<author:sep>Pengfei Li<author:sep>Hao Zhao<author:sep>Guyue Zhou;http://arxiv.org/pdf/2209.08498v2;cs.CV;7 pages, 6 figures, ICRA 2023;nerf
2209.08546v1;http://arxiv.org/abs/2209.08546v1;2022-09-18;ActiveNeRF: Learning where to See with Uncertainty Estimation;"Recently, Neural Radiance Fields (NeRF) has shown promising performances on
reconstructing 3D scenes and synthesizing novel views from a sparse set of 2D
images. Albeit effective, the performance of NeRF is highly influenced by the
quality of training samples. With limited posed images from the scene, NeRF
fails to generalize well to novel views and may collapse to trivial solutions
in unobserved regions. This makes NeRF impractical under resource-constrained
scenarios. In this paper, we present a novel learning framework, ActiveNeRF,
aiming to model a 3D scene with a constrained input budget. Specifically, we
first incorporate uncertainty estimation into a NeRF model, which ensures
robustness under few observations and provides an interpretation of how NeRF
understands the scene. On this basis, we propose to supplement the existing
training set with newly captured samples based on an active learning scheme. By
evaluating the reduction of uncertainty given new inputs, we select the samples
that bring the most information gain. In this way, the quality of novel view
synthesis can be improved with minimal additional resources. Extensive
experiments validate the performance of our model on both realistic and
synthetic scenes, especially with scarcer training data. Code will be released
at \url{https://github.com/LeapLabTHU/ActiveNeRF}.";Xuran Pan<author:sep>Zihang Lai<author:sep>Shiji Song<author:sep>Gao Huang;http://arxiv.org/pdf/2209.08546v1;cs.CV;Accepted by ECCV2022;nerf
2209.08409v1;http://arxiv.org/abs/2209.08409v1;2022-09-17;Uncertainty Guided Policy for Active Robotic 3D Reconstruction using  Neural Radiance Fields;"In this paper, we tackle the problem of active robotic 3D reconstruction of
an object. In particular, we study how a mobile robot with an arm-held camera
can select a favorable number of views to recover an object's 3D shape
efficiently. Contrary to the existing solution to this problem, we leverage the
popular neural radiance fields-based object representation, which has recently
shown impressive results for various computer vision tasks. However, it is not
straightforward to directly reason about an object's explicit 3D geometric
details using such a representation, making the next-best-view selection
problem for dense 3D reconstruction challenging. This paper introduces a
ray-based volumetric uncertainty estimator, which computes the entropy of the
weight distribution of the color samples along each ray of the object's
implicit neural representation. We show that it is possible to infer the
uncertainty of the underlying 3D geometry given a novel view with the proposed
estimator. We then present a next-best-view selection policy guided by the
ray-based volumetric uncertainty in neural radiance fields-based
representations. Encouraging experimental results on synthetic and real-world
data suggest that the approach presented in this paper can enable a new
research direction of using an implicit 3D object representation for the
next-best-view problem in robot vision applications, distinguishing our
approach from the existing approaches that rely on explicit 3D geometric
modeling.";Soomin Lee<author:sep>Le Chen<author:sep>Jiahao Wang<author:sep>Alexander Liniger<author:sep>Suryansh Kumar<author:sep>Fisher Yu;http://arxiv.org/pdf/2209.08409v1;cs.CV;"8 pages, 9 figure; Accepted for publication at IEEE Robotics and
  Automation Letters (RA-L) 2022";
2209.07919v1;http://arxiv.org/abs/2209.07919v1;2022-09-16;iDF-SLAM: End-to-End RGB-D SLAM with Neural Implicit Mapping and Deep  Feature Tracking;"We propose a novel end-to-end RGB-D SLAM, iDF-SLAM, which adopts a
feature-based deep neural tracker as the front-end and a NeRF-style neural
implicit mapper as the back-end. The neural implicit mapper is trained
on-the-fly, while though the neural tracker is pretrained on the ScanNet
dataset, it is also finetuned along with the training of the neural implicit
mapper. Under such a design, our iDF-SLAM is capable of learning to use
scene-specific features for camera tracking, thus enabling lifelong learning of
the SLAM system. Both the training for the tracker and the mapper are
self-supervised without introducing ground truth poses. We test the performance
of our iDF-SLAM on the Replica and ScanNet datasets and compare the results to
the two recent NeRF-based neural SLAM systems. The proposed iDF-SLAM
demonstrates state-of-the-art results in terms of scene reconstruction and
competitive performance in camera tracking.";Yuhang Ming<author:sep>Weicai Ye<author:sep>Andrew Calway;http://arxiv.org/pdf/2209.07919v1;cs.RO;7 pages, 6 figures, 3 tables;nerf
2209.07366v1;http://arxiv.org/abs/2209.07366v1;2022-09-15;3DMM-RF: Convolutional Radiance Fields for 3D Face Modeling;"Facial 3D Morphable Models are a main computer vision subject with countless
applications and have been highly optimized in the last two decades. The
tremendous improvements of deep generative networks have created various
possibilities for improving such models and have attracted wide interest.
Moreover, the recent advances in neural radiance fields, are revolutionising
novel-view synthesis of known scenes. In this work, we present a facial 3D
Morphable Model, which exploits both of the above, and can accurately model a
subject's identity, pose and expression and render it in arbitrary
illumination. This is achieved by utilizing a powerful deep style-based
generator to overcome two main weaknesses of neural radiance fields, their
rigidity and rendering speed. We introduce a style-based generative network
that synthesizes in one pass all and only the required rendering samples of a
neural radiance field. We create a vast labelled synthetic dataset of facial
renders, and train the network on these data, so that it can accurately model
and generalize on facial identity, pose and appearance. Finally, we show that
this model can accurately be fit to ""in-the-wild"" facial images of arbitrary
pose and illumination, extract the facial characteristics, and be used to
re-render the face in controllable conditions.";Stathis Galanakis<author:sep>Baris Gecer<author:sep>Alexandros Lattas<author:sep>Stefanos Zafeiriou;http://arxiv.org/pdf/2209.07366v1;cs.CV;;
2209.05277v1;http://arxiv.org/abs/2209.05277v1;2022-09-12;StructNeRF: Neural Radiance Fields for Indoor Scenes with Structural  Hints;"Neural Radiance Fields (NeRF) achieve photo-realistic view synthesis with
densely captured input images. However, the geometry of NeRF is extremely
under-constrained given sparse views, resulting in significant degradation of
novel view synthesis quality. Inspired by self-supervised depth estimation
methods, we propose StructNeRF, a solution to novel view synthesis for indoor
scenes with sparse inputs. StructNeRF leverages the structural hints naturally
embedded in multi-view inputs to handle the unconstrained geometry issue in
NeRF. Specifically, it tackles the texture and non-texture regions
respectively: a patch-based multi-view consistent photometric loss is proposed
to constrain the geometry of textured regions; for non-textured ones, we
explicitly restrict them to be 3D consistent planes. Through the dense
self-supervised depth constraints, our method improves both the geometry and
the view synthesis performance of NeRF without any additional training on
external data. Extensive experiments on several real-world datasets demonstrate
that StructNeRF surpasses state-of-the-art methods for indoor scenes with
sparse inputs both quantitatively and qualitatively.";Zheng Chen<author:sep>Chen Wang<author:sep>Yuan-Chen Guo<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2209.05277v1;cs.CV;;nerf
2209.04183v1;http://arxiv.org/abs/2209.04183v1;2022-09-09;Generative Deformable Radiance Fields for Disentangled Image Synthesis  of Topology-Varying Objects;"3D-aware generative models have demonstrated their superb performance to
generate 3D neural radiance fields (NeRF) from a collection of monocular 2D
images even for topology-varying object categories. However, these methods
still lack the capability to separately control the shape and appearance of the
objects in the generated radiance fields. In this paper, we propose a
generative model for synthesizing radiance fields of topology-varying objects
with disentangled shape and appearance variations. Our method generates
deformable radiance fields, which builds the dense correspondence between the
density fields of the objects and encodes their appearances in a shared
template field. Our disentanglement is achieved in an unsupervised manner
without introducing extra labels to previous 3D-aware GAN training. We also
develop an effective image inversion scheme for reconstructing the radiance
field of an object in a real monocular image and manipulating its shape and
appearance. Experiments show that our method can successfully learn the
generative model from unstructured monocular images and well disentangle the
shape and appearance for objects (e.g., chairs) with large topological
variance. The model trained on synthetic data can faithfully reconstruct the
real object in a given single image and achieve high-quality texture and shape
editing results.";Ziyu Wang<author:sep>Yu Deng<author:sep>Jiaolong Yang<author:sep>Jingyi Yu<author:sep>Xin Tong;http://arxiv.org/pdf/2209.04183v1;cs.CV;"Accepted at Pacific Graphics 2022 & COMPUTER GRAPHICS Forum, Project
  Page: https://ziyuwang98.github.io/GDRF/";nerf
2209.04061v1;http://arxiv.org/abs/2209.04061v1;2022-09-08;im2nerf: Image to Neural Radiance Field in the Wild;"We propose im2nerf, a learning framework that predicts a continuous neural
object representation given a single input image in the wild, supervised by
only segmentation output from off-the-shelf recognition methods. The standard
approach to constructing neural radiance fields takes advantage of multi-view
consistency and requires many calibrated views of a scene, a requirement that
cannot be satisfied when learning on large-scale image data in the wild. We
take a step towards addressing this shortcoming by introducing a model that
encodes the input image into a disentangled object representation that contains
a code for object shape, a code for object appearance, and an estimated camera
pose from which the object image is captured. Our model conditions a NeRF on
the predicted object representation and uses volume rendering to generate
images from novel views. We train the model end-to-end on a large collection of
input images. As the model is only provided with single-view images, the
problem is highly under-constrained. Therefore, in addition to using a
reconstruction loss on the synthesized input view, we use an auxiliary
adversarial loss on the novel rendered views. Furthermore, we leverage object
symmetry and cycle camera pose consistency. We conduct extensive quantitative
and qualitative experiments on the ShapeNet dataset as well as qualitative
experiments on Open Images dataset. We show that in all cases, im2nerf achieves
the state-of-the-art performance for novel view synthesis from a single-view
unposed image in the wild.";Lu Mi<author:sep>Abhijit Kundu<author:sep>David Ross<author:sep>Frank Dellaert<author:sep>Noah Snavely<author:sep>Alireza Fathi;http://arxiv.org/pdf/2209.04061v1;cs.CV;12 pages, 8 figures, 4 tables;nerf
2209.03910v1;http://arxiv.org/abs/2209.03910v1;2022-09-08;PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and  Feature-metric Alignment;"We present PixTrack, a vision based object pose tracking framework using
novel view synthesis and deep feature-metric alignment. Our evaluations
demonstrate that our method produces highly accurate, robust, and jitter-free
6DoF pose estimates of objects in RGB images without the need of any data
annotation or trajectory smoothing. Our method is also computationally
efficient making it easy to have multi-object tracking with no alteration to
our method and just using CPU multiprocessing.";Prajwal Chidananda<author:sep>Saurabh Nair<author:sep>Douglas Lee<author:sep>Adrian Kaehler;http://arxiv.org/pdf/2209.03910v1;cs.CV;;nerf
2209.03494v1;http://arxiv.org/abs/2209.03494v1;2022-09-07;Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D  Image Representations;"We present Neural Feature Fusion Fields (N3F), a method that improves dense
2D image feature extractors when the latter are applied to the analysis of
multiple images reconstructible as a 3D scene. Given an image feature
extractor, for example pre-trained using self-supervision, N3F uses it as a
teacher to learn a student network defined in 3D space. The 3D student network
is similar to a neural radiance field that distills said features and can be
trained with the usual differentiable rendering machinery. As a consequence,
N3F is readily applicable to most neural rendering formulations, including
vanilla NeRF and its extensions to complex dynamic scenes. We show that our
method not only enables semantic understanding in the context of scene-specific
neural fields without the use of manual labels, but also consistently improves
over the self-supervised 2D baselines. This is demonstrated by considering
various tasks, such as 2D object retrieval, 3D segmentation, and scene editing,
in diverse sequences, including long egocentric videos in the EPIC-KITCHENS
benchmark.";Vadim Tschernezki<author:sep>Iro Laina<author:sep>Diane Larlus<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2209.03494v1;cs.CV;3DV2022, Oral. Project page: https://www.robots.ox.ac.uk/~vadim/n3f/;nerf
2209.01194v4;http://arxiv.org/abs/2209.01194v4;2022-09-02;CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural  Representations;"Recent advances in neural radiance fields (NeRFs) achieve state-of-the-art
novel view synthesis and facilitate dense estimation of scene properties.
However, NeRFs often fail for large, unbounded scenes that are captured under
very sparse views with the scene content concentrated far away from the camera,
as is typical for field robotics applications. In particular, NeRF-style
algorithms perform poorly: (1) when there are insufficient views with little
pose diversity, (2) when scenes contain saturation and shadows, and (3) when
finely sampling large unbounded scenes with fine structures becomes
computationally intensive.
  This paper proposes CLONeR, which significantly improves upon NeRF by
allowing it to model large outdoor driving scenes that are observed from sparse
input sensor views. This is achieved by decoupling occupancy and color learning
within the NeRF framework into separate Multi-Layer Perceptrons (MLPs) trained
using LiDAR and camera data, respectively. In addition, this paper proposes a
novel method to build differentiable 3D Occupancy Grid Maps (OGM) alongside the
NeRF model, and leverage this occupancy grid for improved sampling of points
along a ray for volumetric rendering in metric space.
  Through extensive quantitative and qualitative experiments on scenes from the
KITTI dataset, this paper demonstrates that the proposed method outperforms
state-of-the-art NeRF models on both novel view synthesis and dense depth
prediction tasks when trained on sparse input data.";Alexandra Carlson<author:sep>Manikandasriram Srinivasan Ramanagopal<author:sep>Nathan Tseng<author:sep>Matthew Johnson-Roberson<author:sep>Ram Vasudevan<author:sep>Katherine A. Skinner;http://arxiv.org/pdf/2209.01194v4;cs.CV;first two authors equally contributed;nerf
2209.01019v1;http://arxiv.org/abs/2209.01019v1;2022-09-01;On Quantizing Implicit Neural Representations;"The role of quantization within implicit/coordinate neural networks is still
not fully understood. We note that using a canonical fixed quantization scheme
during training produces poor performance at low-rates due to the network
weight distributions changing over the course of training. In this work, we
show that a non-uniform quantization of neural weights can lead to significant
improvements. Specifically, we demonstrate that a clustered quantization
enables improved reconstruction. Finally, by characterising a trade-off between
quantization and network capacity, we demonstrate that it is possible (while
memory inefficient) to reconstruct signals using binary neural networks. We
demonstrate our findings experimentally on 2D image reconstruction and 3D
radiance fields; and show that simple quantization methods and architecture
search can achieve compression of NeRF to less than 16kb with minimal loss in
performance (323x smaller than the original NeRF).";Cameron Gordon<author:sep>Shin-Fang Chng<author:sep>Lachlan MacDonald<author:sep>Simon Lucey;http://arxiv.org/pdf/2209.01019v1;cs.CV;10 pages, 10 figures;nerf
2209.00648v1;http://arxiv.org/abs/2209.00648v1;2022-09-01;Cross-Spectral Neural Radiance Fields;"We propose X-NeRF, a novel method to learn a Cross-Spectral scene
representation given images captured from cameras with different light spectrum
sensitivity, based on the Neural Radiance Fields formulation. X-NeRF optimizes
camera poses across spectra during training and exploits Normalized
Cross-Device Coordinates (NXDC) to render images of different modalities from
arbitrary viewpoints, which are aligned and at the same resolution. Experiments
on 16 forward-facing scenes, featuring color, multi-spectral and infrared
images, confirm the effectiveness of X-NeRF at modeling Cross-Spectral scene
representations.";Matteo Poggi<author:sep>Pierluigi Zama Ramirez<author:sep>Fabio Tosi<author:sep>Samuele Salti<author:sep>Stefano Mattoccia<author:sep>Luigi Di Stefano;http://arxiv.org/pdf/2209.00648v1;cs.CV;3DV 2022. Project page: https://cvlab-unibo.github.io/xnerf-web/;nerf
2208.14851v1;http://arxiv.org/abs/2208.14851v1;2022-08-31;Dual-Space NeRF: Learning Animatable Avatars and Scene Lighting in  Separate Spaces;"Modeling the human body in a canonical space is a common practice for
capturing and animation. But when involving the neural radiance field (NeRF),
learning a static NeRF in the canonical space is not enough because the
lighting of the body changes when the person moves even though the scene
lighting is constant. Previous methods alleviate the inconsistency of lighting
by learning a per-frame embedding, but this operation does not generalize to
unseen poses. Given that the lighting condition is static in the world space
while the human body is consistent in the canonical space, we propose a
dual-space NeRF that models the scene lighting and the human body with two MLPs
in two separate spaces. To bridge these two spaces, previous methods mostly
rely on the linear blend skinning (LBS) algorithm. However, the blending
weights for LBS of a dynamic neural field are intractable and thus are usually
memorized with another MLP, which does not generalize to novel poses. Although
it is possible to borrow the blending weights of a parametric mesh such as
SMPL, the interpolation operation introduces more artifacts. In this paper, we
propose to use the barycentric mapping, which can directly generalize to unseen
poses and surprisingly achieves superior results than LBS with neural blending
weights. Quantitative and qualitative results on the Human3.6M and the
ZJU-MoCap datasets show the effectiveness of our method.";Yihao Zhi<author:sep>Shenhan Qian<author:sep>Xinhao Yan<author:sep>Shenghua Gao;http://arxiv.org/pdf/2208.14851v1;cs.CV;Accepted by 3DV 2022;nerf
2208.14433v1;http://arxiv.org/abs/2208.14433v1;2022-08-30;A Portable Multiscopic Camera for Novel View and Time Synthesis in  Dynamic Scenes;"We present a portable multiscopic camera system with a dedicated model for
novel view and time synthesis in dynamic scenes. Our goal is to render
high-quality images for a dynamic scene from any viewpoint at any time using
our portable multiscopic camera. To achieve such novel view and time synthesis,
we develop a physical multiscopic camera equipped with five cameras to train a
neural radiance field (NeRF) in both time and spatial domains for dynamic
scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal
coordinate, and 2D viewing direction) to view-dependent and time-varying
emitted radiance and volume density. Volume rendering is applied to render a
photo-realistic image at a specified camera pose and time. To improve the
robustness of our physical camera, we propose a camera parameter optimization
module and a temporal frame interpolation module to promote information
propagation across time. We conduct experiments on both real-world and
synthetic datasets to evaluate our system, and the results show that our
approach outperforms alternative solutions qualitatively and quantitatively.
Our code and dataset are available at https://yuenfuilau.github.io.";Tianjia Zhang<author:sep>Yuen-Fui Lau<author:sep>Qifeng Chen;http://arxiv.org/pdf/2208.14433v1;cs.CV;To be presented at IROS2022;nerf
2209.02417v1;http://arxiv.org/abs/2209.02417v1;2022-08-29;Volume Rendering Digest (for NeRF);"Neural Radiance Fields employ simple volume rendering as a way to overcome
the challenges of differentiating through ray-triangle intersections by
leveraging a probabilistic notion of visibility. This is achieved by assuming
the scene is composed by a cloud of light-emitting particles whose density
changes in space. This technical report summarizes the derivations for
differentiable volume rendering. It is a condensed version of previous reports,
but rewritten in the context of NeRF, and adopting its commonly used notation.";Andrea Tagliasacchi<author:sep>Ben Mildenhall;http://arxiv.org/pdf/2209.02417v1;cs.CV;Overleaf: https://www.overleaf.com/read/fkhpkzxhnyws;nerf
2208.12550v2;http://arxiv.org/abs/2208.12550v2;2022-08-26;Training and Tuning Generative Neural Radiance Fields for  Attribute-Conditional 3D-Aware Face Generation;"Generative Neural Radiance Fields (GNeRF) based 3D-aware GANs have
demonstrated remarkable capabilities in generating high-quality images while
maintaining strong 3D consistency. Notably, significant advancements have been
made in the domain of face generation. However, most existing models prioritize
view consistency over disentanglement, resulting in limited semantic/attribute
control during generation. To address this limitation, we propose a conditional
GNeRF model incorporating specific attribute labels as input to enhance the
controllability and disentanglement abilities of 3D-aware generative models.
Our approach builds upon a pre-trained 3D-aware face model, and we introduce a
Training as Init and Optimizing for Tuning (TRIOT) method to train a
conditional normalized flow module to enable the facial attribute editing, then
optimize the latent vector to improve attribute-editing precision further. Our
extensive experiments demonstrate that our model produces high-quality edits
with superior view consistency while preserving non-target regions. Code is
available at https://github.com/zhangqianhui/TT-GNeRF.";Jichao Zhang<author:sep>Aliaksandr Siarohin<author:sep>Yahui Liu<author:sep>Hao Tang<author:sep>Nicu Sebe<author:sep>Wei Wang;http://arxiv.org/pdf/2208.12550v2;cs.CV;13 pages;nerf
2208.11300v2;http://arxiv.org/abs/2208.11300v2;2022-08-24;E-NeRF: Neural Radiance Fields from a Moving Event Camera;"Estimating neural radiance fields (NeRFs) from ""ideal"" images has been
extensively studied in the computer vision community. Most approaches assume
optimal illumination and slow camera motion. These assumptions are often
violated in robotic applications, where images may contain motion blur, and the
scene may not have suitable illumination. This can cause significant problems
for downstream tasks such as navigation, inspection, or visualization of the
scene. To alleviate these problems, we present E-NeRF, the first method which
estimates a volumetric scene representation in the form of a NeRF from a
fast-moving event camera. Our method can recover NeRFs during very fast motion
and in high-dynamic-range conditions where frame-based approaches fail. We show
that rendering high-quality frames is possible by only providing an event
stream as input. Furthermore, by combining events and frames, we can estimate
NeRFs of higher quality than state-of-the-art approaches under severe motion
blur. We also show that combining events and frames can overcome failure cases
of NeRF estimation in scenarios where only a few input views are available
without requiring additional regularization.";Simon Klenk<author:sep>Lukas Koestler<author:sep>Davide Scaramuzza<author:sep>Daniel Cremers;http://arxiv.org/pdf/2208.11300v2;cs.CV;revised RAL version + added suppl. material;nerf
2208.11537v1;http://arxiv.org/abs/2208.11537v1;2022-08-24;PeRFception: Perception using Radiance Fields;"The recent progress in implicit 3D representation, i.e., Neural Radiance
Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible
in a differentiable manner. This new representation can effectively convey the
information of hundreds of high-resolution images in one compact format and
allows photorealistic synthesis of novel views. In this work, using the variant
of NeRF called Plenoxels, we create the first large-scale implicit
representation datasets for perception tasks, called the PeRFception, which
consists of two parts that incorporate both object-centric and scene-centric
scans for classification and segmentation. It shows a significant memory
compression rate (96.4\%) from the original dataset, while containing both 2D
and 3D information in a unified form. We construct the classification and
segmentation models that directly take as input this implicit format and also
propose a novel augmentation technique to avoid overfitting on backgrounds of
images. The code and data are publicly available in
https://postech-cvlab.github.io/PeRFception .";Yoonwoo Jeong<author:sep>Seungjoo Shin<author:sep>Junha Lee<author:sep>Christopher Choy<author:sep>Animashree Anandkumar<author:sep>Minsu Cho<author:sep>Jaesik Park;http://arxiv.org/pdf/2208.11537v1;cs.CV;Project Page: https://postech-cvlab.github.io/PeRFception/;nerf
2208.08728v1;http://arxiv.org/abs/2208.08728v1;2022-08-18;Neural Capture of Animatable 3D Human from Monocular Video;"We present a novel paradigm of building an animatable 3D human representation
from a monocular video input, such that it can be rendered in any unseen poses
and views. Our method is based on a dynamic Neural Radiance Field (NeRF) rigged
by a mesh-based parametric 3D human model serving as a geometry proxy. Previous
methods usually rely on multi-view videos or accurate 3D geometry information
as additional inputs; besides, most methods suffer from degraded quality when
generalized to unseen poses. We identify that the key to generalization is a
good input embedding for querying dynamic NeRF: A good input embedding should
define an injective mapping in the full volumetric space, guided by surface
mesh deformation under pose variation. Based on this observation, we propose to
embed the input query with its relationship to local surface regions spanned by
a set of geodesic nearest neighbors on mesh vertices. By including both
position and relative distance information, our embedding defines a
distance-preserved deformation mapping and generalizes well to unseen poses. To
reduce the dependency on additional inputs, we first initialize per-frame 3D
meshes using off-the-shelf tools and then propose a pipeline to jointly
optimize NeRF and refine the initial mesh. Extensive experiments show our
method can synthesize plausible human rendering results under unseen poses and
views.";Gusi Te<author:sep>Xiu Li<author:sep>Xiao Li<author:sep>Jinglu Wang<author:sep>Wei Hu<author:sep>Yan Lu;http://arxiv.org/pdf/2208.08728v1;cs.CV;ECCV 2022;nerf
2208.07903v2;http://arxiv.org/abs/2208.07903v2;2022-08-16;Casual Indoor HDR Radiance Capture from Omnidirectional Images;"We present PanoHDR-NeRF, a neural representation of the full HDR radiance
field of an indoor scene, and a pipeline to capture it casually, without
elaborate setups or complex capture protocols. First, a user captures a low
dynamic range (LDR) omnidirectional video of the scene by freely waving an
off-the-shelf camera around the scene. Then, an LDR2HDR network uplifts the
captured LDR frames to HDR, which are used to train a tailored NeRF++ model.
The resulting PanoHDR-NeRF can render full HDR images from any location of the
scene. Through experiments on a novel test dataset of real scenes with the
ground truth HDR radiance captured at locations not seen during training, we
show that PanoHDR-NeRF predicts plausible HDR radiance from any scene point. We
also show that the predicted radiance can synthesize correct lighting effects,
enabling the augmentation of indoor scenes with synthetic objects that are lit
correctly. Datasets and code are available at
https://lvsn.github.io/PanoHDR-NeRF/.";Pulkit Gera<author:sep>Mohammad Reza Karimi Dastjerdi<author:sep>Charles Renaud<author:sep>P. J. Narayanan<author:sep>Jean-FranÃ§ois Lalonde;http://arxiv.org/pdf/2208.07903v2;cs.CV;BMVC 2022;nerf
2208.07059v2;http://arxiv.org/abs/2208.07059v2;2022-08-15;UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance  Fields for 3D Scene;"3D scenes photorealistic stylization aims to generate photorealistic images
from arbitrary novel views according to a given style image while ensuring
consistency when rendering from different viewpoints. Some existing stylization
methods with neural radiance fields can effectively predict stylized scenes by
combining the features of the style image with multi-view images to train 3D
scenes. However, these methods generate novel view images that contain
objectionable artifacts. Besides, they cannot achieve universal photorealistic
stylization for a 3D scene. Therefore, a styling image must retrain a 3D scene
representation network based on a neural radiation field. We propose a novel 3D
scene photorealistic style transfer framework to address these issues. It can
realize photorealistic 3D scene style transfer with a 2D style image. We first
pre-trained a 2D photorealistic style transfer network, which can meet the
photorealistic style transfer between any given content image and style image.
Then, we use voxel features to optimize a 3D scene and get the geometric
representation of the scene. Finally, we jointly optimize a hyper network to
realize the scene photorealistic style transfer of arbitrary style images. In
the transfer stage, we use a pre-trained 2D photorealistic network to constrain
the photorealistic style of different views and different style images in the
3D scene. The experimental results show that our method not only realizes the
3D photorealistic style transfer of arbitrary style images but also outperforms
the existing methods in terms of visual quality and consistency. Project
page:https://semchan.github.io/UPST_NeRF.";Yaosen Chen<author:sep>Qi Yuan<author:sep>Zhiqiang Li<author:sep>Yuegen Liu<author:sep>Wei Wang<author:sep>Chaoping Xie<author:sep>Xuming Wen<author:sep>Qien Yu;http://arxiv.org/pdf/2208.07059v2;cs.CV;arXiv admin note: text overlap with arXiv:2205.12183 by other authors;nerf
2208.07227v2;http://arxiv.org/abs/2208.07227v2;2022-08-15;DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images;"In this paper, we study the problem of 3D scene geometry decomposition and
manipulation from 2D views. By leveraging the recent implicit neural
representation techniques, particularly the appealing neural radiance fields,
we introduce an object field component to learn unique codes for all individual
objects in 3D space only from 2D supervision. The key to this component is a
series of carefully designed loss functions to enable every 3D point,
especially in non-occupied space, to be effectively optimized even without 3D
labels. In addition, we introduce an inverse query algorithm to freely
manipulate any specified 3D object shape in the learned scene representation.
Notably, our manipulation algorithm can explicitly tackle key issues such as
object collisions and visual occlusions. Our method, called DM-NeRF, is among
the first to simultaneously reconstruct, decompose, manipulate and render
complex 3D scenes in a single pipeline. Extensive experiments on three datasets
clearly show that our method can accurately decompose all 3D objects from 2D
views, allowing any interested object to be freely manipulated in 3D space such
as translation, rotation, size adjustment, and deformation.";Bing Wang<author:sep>Lu Chen<author:sep>Bo Yang;http://arxiv.org/pdf/2208.07227v2;cs.CV;"ICLR 2023. Our data and code are available at:
  https://github.com/vLAR-group/DM-NeRF";nerf
2208.06335v1;http://arxiv.org/abs/2208.06335v1;2022-08-12;OmniVoxel: A Fast and Precise Reconstruction Method of Omnidirectional  Neural Radiance Field;"This paper proposes a method to reconstruct the neural radiance field with
equirectangular omnidirectional images. Implicit neural scene representation
with a radiance field can reconstruct the 3D shape of a scene continuously
within a limited spatial area. However, training a fully implicit
representation on commercial PC hardware requires a lot of time and computing
resources (15 $\sim$ 20 hours per scene). Therefore, we propose a method to
accelerate this process significantly (20 $\sim$ 40 minutes per scene). Instead
of using a fully implicit representation of rays for radiance field
reconstruction, we adopt feature voxels that contain density and color features
in tensors. Considering omnidirectional equirectangular input and the camera
layout, we use spherical voxelization for representation instead of cubic
representation. Our voxelization method could balance the reconstruction
quality of the inner scene and outer scene. In addition, we adopt the
axis-aligned positional encoding method on the color features to increase the
total image quality. Our method achieves satisfying empirical performance on
synthetic datasets with random camera poses. Moreover, we test our method with
real scenes which contain complex geometries and also achieve state-of-the-art
performance. Our code and complete dataset will be released at the same time as
the paper publication.";Qiaoge Li<author:sep>Itsuki Ueda<author:sep>Chun Xie<author:sep>Hidehiko Shishido<author:sep>Itaru Kitahara;http://arxiv.org/pdf/2208.06335v1;cs.CV;will be appeared in GCCE 2022;
2208.05963v2;http://arxiv.org/abs/2208.05963v2;2022-08-11;RelPose: Predicting Probabilistic Relative Rotation for Single Objects  in the Wild;"We describe a data-driven method for inferring the camera viewpoints given
multiple images of an arbitrary object. This task is a core component of
classic geometric pipelines such as SfM and SLAM, and also serves as a vital
pre-processing requirement for contemporary neural approaches (e.g. NeRF) to
object reconstruction and view synthesis. In contrast to existing
correspondence-driven methods that do not perform well given sparse views, we
propose a top-down prediction based approach for estimating camera viewpoints.
Our key technical insight is the use of an energy-based formulation for
representing distributions over relative camera rotations, thus allowing us to
explicitly represent multiple camera modes arising from object symmetries or
views. Leveraging these relative predictions, we jointly estimate a consistent
set of camera rotations from multiple images. We show that our approach
outperforms state-of-the-art SfM and SLAM methods given sparse images on both
seen and unseen categories. Further, our probabilistic approach significantly
outperforms directly regressing relative poses, suggesting that modeling
multimodality is important for coherent joint reconstruction. We demonstrate
that our system can be a stepping stone toward in-the-wild reconstruction from
multi-view datasets. The project page with code and videos can be found at
https://jasonyzhang.com/relpose.";Jason Y. Zhang<author:sep>Deva Ramanan<author:sep>Shubham Tulsiani;http://arxiv.org/pdf/2208.05963v2;cs.CV;In ECCV 2022. V2: updated references;nerf
2208.05751v2;http://arxiv.org/abs/2208.05751v2;2022-08-11;FDNeRF: Few-shot Dynamic Neural Radiance Fields for Face Reconstruction  and Expression Editing;"We propose a Few-shot Dynamic Neural Radiance Field (FDNeRF), the first
NeRF-based method capable of reconstruction and expression editing of 3D faces
based on a small number of dynamic images. Unlike existing dynamic NeRFs that
require dense images as input and can only be modeled for a single identity,
our method enables face reconstruction across different persons with few-shot
inputs. Compared to state-of-the-art few-shot NeRFs designed for modeling
static scenes, the proposed FDNeRF accepts view-inconsistent dynamic inputs and
supports arbitrary facial expression editing, i.e., producing faces with novel
expressions beyond the input ones. To handle the inconsistencies between
dynamic inputs, we introduce a well-designed conditional feature warping (CFW)
module to perform expression conditioned warping in 2D feature space, which is
also identity adaptive and 3D constrained. As a result, features of different
expressions are transformed into the target ones. We then construct a radiance
field based on these view-consistent features and use volumetric rendering to
synthesize novel views of the modeled faces. Extensive experiments with
quantitative and qualitative evaluation demonstrate that our method outperforms
existing dynamic and few-shot NeRFs on both 3D face reconstruction and
expression editing tasks. Code is available at
https://github.com/FDNeRF/FDNeRF.";Jingbo Zhang<author:sep>Xiaoyu Li<author:sep>Ziyu Wan<author:sep>Can Wang<author:sep>Jing Liao;http://arxiv.org/pdf/2208.05751v2;cs.CV;"Accepted at SIGGRAPH Asia 2022. Project page:
  https://fdnerf.github.io";nerf
2208.04717v2;http://arxiv.org/abs/2208.04717v2;2022-08-09;Cascaded and Generalizable Neural Radiance Fields for Fast View  Synthesis;"We present CG-NeRF, a cascade and generalizable neural radiance fields method
for view synthesis. Recent generalizing view synthesis methods can render
high-quality novel views using a set of nearby input views. However, the
rendering speed is still slow due to the nature of uniformly-point sampling of
neural radiance fields. Existing scene-specific methods can train and render
novel views efficiently but can not generalize to unseen data. Our approach
addresses the problems of fast and generalizing view synthesis by proposing two
novel modules: a coarse radiance fields predictor and a convolutional-based
neural renderer. This architecture infers consistent scene geometry based on
the implicit neural fields and renders new views efficiently using a single
GPU. We first train CG-NeRF on multiple 3D scenes of the DTU dataset, and the
network can produce high-quality and accurate novel views on unseen real and
synthetic data using only photometric losses. Moreover, our method can leverage
a denser set of reference images of a single scene to produce accurate novel
views without relying on additional explicit representations and still
maintains the high-speed rendering of the pre-trained model. Experimental
results show that CG-NeRF outperforms state-of-the-art generalizable neural
rendering methods on various synthetic and real datasets.";Phong Nguyen-Ha<author:sep>Lam Huynh<author:sep>Esa Rahtu<author:sep>Jiri Matas<author:sep>Janne Heikkila;http://arxiv.org/pdf/2208.04717v2;cs.CV;"Accepted at IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)";nerf
2208.02705v2;http://arxiv.org/abs/2208.02705v2;2022-08-04;360Roam: Real-Time Indoor Roaming Using Geometry-Aware 360$^\circ$  Radiance Fields;"Virtual tour among sparse 360$^\circ$ images is widely used while hindering
smooth and immersive roaming experiences. The emergence of Neural Radiance
Field (NeRF) has showcased significant progress in synthesizing novel views,
unlocking the potential for immersive scene exploration. Nevertheless, previous
NeRF works primarily focused on object-centric scenarios, resulting in
noticeable performance degradation when applied to outward-facing and
large-scale scenes due to limitations in scene parameterization. To achieve
seamless and real-time indoor roaming, we propose a novel approach using
geometry-aware radiance fields with adaptively assigned local radiance fields.
Initially, we employ multiple 360$^\circ$ images of an indoor scene to
progressively reconstruct explicit geometry in the form of a probabilistic
occupancy map, derived from a global omnidirectional radiance field.
Subsequently, we assign local radiance fields through an adaptive
divide-and-conquer strategy based on the recovered geometry. By incorporating
geometry-aware sampling and decomposition of the global radiance field, our
system effectively utilizes positional encoding and compact neural networks to
enhance rendering quality and speed. Additionally, the extracted floorplan of
the scene aids in providing visual guidance, contributing to a realistic
roaming experience. To demonstrate the effectiveness of our system, we curated
a diverse dataset of 360$^\circ$ images encompassing various real-life scenes,
on which we conducted extensive experiments. Quantitative and qualitative
comparisons against baseline approaches illustrated the superior performance of
our system in large-scale indoor scene roaming.";Huajian Huang<author:sep>Yingshu Chen<author:sep>Tianjia Zhang<author:sep>Sai-Kit Yeung;http://arxiv.org/pdf/2208.02705v2;cs.CV;;nerf
2208.01421v2;http://arxiv.org/abs/2208.01421v2;2022-08-02;T4DT: Tensorizing Time for Learning Temporal 3D Visual Data;"Unlike 2D raster images, there is no single dominant representation for 3D
visual data processing. Different formats like point clouds, meshes, or
implicit functions each have their strengths and weaknesses. Still, grid
representations such as signed distance functions have attractive properties
also in 3D. In particular, they offer constant-time random access and are
eminently suitable for modern machine learning. Unfortunately, the storage size
of a grid grows exponentially with its dimension. Hence they often exceed
memory limits even at moderate resolution. This work proposes using low-rank
tensor formats, including the Tucker, tensor train, and quantics tensor train
decompositions, to compress time-varying 3D data. Our method iteratively
computes, voxelizes, and compresses each frame's truncated signed distance
function and applies tensor rank truncation to condense all frames into a
single, compressed tensor that represents the entire 4D scene. We show that
low-rank tensor compression is extremely compact to store and query
time-varying signed distance functions. It significantly reduces the memory
footprint of 4D scenes while remarkably preserving their geometric quality.
Unlike existing, iterative learning-based approaches like DeepSDF and NeRF, our
method uses a closed-form algorithm with theoretical guarantees.";Mikhail Usvyatsov<author:sep>Rafael Ballester-Rippoll<author:sep>Lina Bashaeva<author:sep>Konrad Schindler<author:sep>Gonzalo Ferrer<author:sep>Ivan Oseledets;http://arxiv.org/pdf/2208.01421v2;cs.CV;;nerf
2208.00945v1;http://arxiv.org/abs/2208.00945v1;2022-08-01;DoF-NeRF: Depth-of-Field Meets Neural Radiance Fields;"Neural Radiance Field (NeRF) and its variants have exhibited great success on
representing 3D scenes and synthesizing photo-realistic novel views. However,
they are generally based on the pinhole camera model and assume all-in-focus
inputs. This limits their applicability as images captured from the real world
often have finite depth-of-field (DoF). To mitigate this issue, we introduce
DoF-NeRF, a novel neural rendering approach that can deal with shallow DoF
inputs and can simulate DoF effect. In particular, it extends NeRF to simulate
the aperture of lens following the principles of geometric optics. Such a
physical guarantee allows DoF-NeRF to operate views with different focus
configurations. Benefiting from explicit aperture modeling, DoF-NeRF also
enables direct manipulation of DoF effect by adjusting virtual aperture and
focus parameters. It is plug-and-play and can be inserted into NeRF-based
frameworks. Experiments on synthetic and real-world datasets show that,
DoF-NeRF not only performs comparably with NeRF in the all-in-focus setting,
but also can synthesize all-in-focus novel views conditioned on shallow DoF
inputs. An interesting application of DoF-NeRF to DoF rendering is also
demonstrated. The source code will be made available at
https://github.com/zijinwuzijin/DoF-NeRF.";Zijin Wu<author:sep>Xingyi Li<author:sep>Juewen Peng<author:sep>Hao Lu<author:sep>Zhiguo Cao<author:sep>Weicai Zhong;http://arxiv.org/pdf/2208.00945v1;cs.CV;Accepted by ACMMM 2022;nerf
2208.00164v3;http://arxiv.org/abs/2208.00164v3;2022-07-30;Distilled Low Rank Neural Radiance Field with Quantization for Light  Field Compression;"We propose in this paper a Quantized Distilled Low-Rank Neural Radiance Field
(QDLR-NeRF) representation for the task of light field compression. While
existing compression methods encode the set of light field sub-aperture images,
our proposed method learns an implicit scene representation in the form of a
Neural Radiance Field (NeRF), which also enables view synthesis. To reduce its
size, the model is first learned under a Low-Rank (LR) constraint using a
Tensor Train (TT) decomposition within an Alternating Direction Method of
Multipliers (ADMM) optimization framework. To further reduce the model's size,
the components of the tensor train decomposition need to be quantized. However,
simultaneously considering the optimization of the NeRF model with both the
low-rank constraint and rate-constrained weight quantization is challenging. To
address this difficulty, we introduce a network distillation operation that
separates the low-rank approximation and the weight quantization during network
training. The information from the initial LR-constrained NeRF (LR-NeRF) is
distilled into a model of much smaller dimension (DLR-NeRF) based on the TT
decomposition of the LR-NeRF. We then learn an optimized global codebook to
quantize all TT components, producing the final QDLR-NeRF. Experimental results
show that our proposed method yields better compression efficiency compared to
state-of-the-art methods, and it additionally has the advantage of allowing the
synthesis of any light field view with high quality.";Jinglei Shi<author:sep>Christine Guillemot;http://arxiv.org/pdf/2208.00164v3;cs.CV;;nerf
2208.00277v5;http://arxiv.org/abs/2208.00277v5;2022-07-30;MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient  Neural Field Rendering on Mobile Architectures;"Neural Radiance Fields (NeRFs) have demonstrated amazing ability to
synthesize images of 3D scenes from novel views. However, they rely upon
specialized volumetric rendering algorithms based on ray marching that are
mismatched to the capabilities of widely deployed graphics hardware. This paper
introduces a new NeRF representation based on textured polygons that can
synthesize novel images efficiently with standard rendering pipelines. The NeRF
is represented as a set of polygons with textures representing binary opacities
and feature vectors. Traditional rendering of the polygons with a z-buffer
yields an image with features at every pixel, which are interpreted by a small,
view-dependent MLP running in a fragment shader to produce a final pixel color.
This approach enables NeRFs to be rendered with the traditional polygon
rasterization pipeline, which provides massive pixel-level parallelism,
achieving interactive frame rates on a wide range of compute platforms,
including mobile phones.";Zhiqin Chen<author:sep>Thomas Funkhouser<author:sep>Peter Hedman<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2208.00277v5;cs.CV;"CVPR 2023. Project page: https://mobile-nerf.github.io, code:
  https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf";nerf
2207.14455v1;http://arxiv.org/abs/2207.14455v1;2022-07-29;Neural Density-Distance Fields;"The success of neural fields for 3D vision tasks is now indisputable.
Following this trend, several methods aiming for visual localization (e.g.,
SLAM) have been proposed to estimate distance or density fields using neural
fields. However, it is difficult to achieve high localization performance by
only density fields-based methods such as Neural Radiance Field (NeRF) since
they do not provide density gradient in most empty regions. On the other hand,
distance field-based methods such as Neural Implicit Surface (NeuS) have
limitations in objects' surface shapes. This paper proposes Neural
Density-Distance Field (NeDDF), a novel 3D representation that reciprocally
constrains the distance and density fields. We extend distance field
formulation to shapes with no explicit boundary surface, such as fur or smoke,
which enable explicit conversion from distance field to density field.
Consistent distance and density fields realized by explicit conversion enable
both robustness to initial values and high-quality registration. Furthermore,
the consistency between fields allows fast convergence from sparse point
clouds. Experiments show that NeDDF can achieve high localization performance
while providing comparable results to NeRF on novel view synthesis. The code is
available at https://github.com/ueda0319/neddf.";Itsuki Ueda<author:sep>Yoshihiro Fukuhara<author:sep>Hirokatsu Kataoka<author:sep>Hiroaki Aizawa<author:sep>Hidehiko Shishido<author:sep>Itaru Kitahara;http://arxiv.org/pdf/2207.14455v1;cs.CV;ECCV 2022 (poster). project page: https://ueda0319.github.io/neddf/;nerf
2207.14741v3;http://arxiv.org/abs/2207.14741v3;2022-07-29;End-to-end View Synthesis via NeRF Attention;"In this paper, we present a simple seq2seq formulation for view synthesis
where we take a set of ray points as input and output colors corresponding to
the rays. Directly applying a standard transformer on this seq2seq formulation
has two limitations. First, the standard attention cannot successfully fit the
volumetric rendering procedure, and therefore high-frequency components are
missing in the synthesized views. Second, applying global attention to all rays
and pixels is extremely inefficient. Inspired by the neural radiance field
(NeRF), we propose the NeRF attention (NeRFA) to address the above problems. On
the one hand, NeRFA considers the volumetric rendering equation as a soft
feature modulation procedure. In this way, the feature modulation enhances the
transformers with the NeRF-like inductive bias. On the other hand, NeRFA
performs multi-stage attention to reduce the computational overhead.
Furthermore, the NeRFA model adopts the ray and pixel transformers to learn the
interactions between rays and pixels. NeRFA demonstrates superior performance
over NeRF and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D.
Besides, NeRFA establishes a new state-of-the-art under two settings: the
single-scene view synthesis and the category-centric novel view synthesis.";Zelin Zhao<author:sep>Jiaya Jia;http://arxiv.org/pdf/2207.14741v3;cs.CV;Fixed reference formatting issues;nerf
2207.13298v3;http://arxiv.org/abs/2207.13298v3;2022-07-27;Is Attention All That NeRF Needs?;"We present Generalizable NeRF Transformer (GNT), a transformer-based
architecture that reconstructs Neural Radiance Fields (NeRFs) and learns to
renders novel views on the fly from source views. While prior works on NeRFs
optimize a scene representation by inverting a handcrafted rendering equation,
GNT achieves neural representation and rendering that generalizes across scenes
using transformers at two stages. (1) The view transformer leverages multi-view
geometry as an inductive bias for attention-based scene representation, and
predicts coordinate-aligned features by aggregating information from epipolar
lines on the neighboring views. (2) The ray transformer renders novel views
using attention to decode the features from the view transformer along the
sampled points during ray marching. Our experiments demonstrate that when
optimized on a single scene, GNT can successfully reconstruct NeRF without an
explicit rendering formula due to the learned ray renderer. When trained on
multiple scenes, GNT consistently achieves state-of-the-art performance when
transferring to unseen scenes and outperform all other methods by ~10% on
average. Our analysis of the learned attention maps to infer depth and
occlusion indicate that attention enables learning a physically-grounded
rendering. Our results show the promise of transformers as a universal modeling
tool for graphics. Please refer to our project page for video results:
https://vita-group.github.io/GNT/.";Mukund Varma T<author:sep>Peihao Wang<author:sep>Xuxi Chen<author:sep>Tianlong Chen<author:sep>Subhashini Venugopalan<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2207.13298v3;cs.CV;International Conference on Learning Representations (ICLR), 2023;nerf
2207.12298v1;http://arxiv.org/abs/2207.12298v1;2022-07-25;Deforming Radiance Fields with Cages;"Recent advances in radiance fields enable photorealistic rendering of static
or dynamic 3D scenes, but still do not support explicit deformation that is
used for scene manipulation or animation. In this paper, we propose a method
that enables a new type of deformation of the radiance field: free-form
radiance field deformation. We use a triangular mesh that encloses the
foreground object called cage as an interface, and by manipulating the cage
vertices, our approach enables the free-form deformation of the radiance field.
The core of our approach is cage-based deformation which is commonly used in
mesh deformation. We propose a novel formulation to extend it to the radiance
field, which maps the position and the view direction of the sampling points
from the deformed space to the canonical space, thus enabling the rendering of
the deformed scene. The deformation results of the synthetic datasets and the
real-world datasets demonstrate the effectiveness of our approach.";Tianhan Xu<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2207.12298v1;cs.CV;ECCV 2022. Project page: https://xth430.github.io/deforming-nerf/;
2207.11757v1;http://arxiv.org/abs/2207.11757v1;2022-07-24;Learning Generalizable Light Field Networks from Few Images;"We explore a new strategy for few-shot novel view synthesis based on a neural
light field representation. Given a target camera pose, an implicit neural
network maps each ray to its target pixel's color directly. The network is
conditioned on local ray features generated by coarse volumetric rendering from
an explicit 3D feature volume. This volume is built from the input images using
a 3D ConvNet. Our method achieves competitive performances on synthetic and
real MVS data with respect to state-of-the-art neural radiance field based
competition, while offering a 100 times faster rendering.";Qian Li<author:sep>Franck Multon<author:sep>Adnane Boukhayma;http://arxiv.org/pdf/2207.11757v1;cs.CV;;
2207.11770v1;http://arxiv.org/abs/2207.11770v1;2022-07-24;Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head  Synthesis;"Talking head synthesis is an emerging technology with wide applications in
film dubbing, virtual avatars and online education. Recent NeRF-based methods
generate more natural talking videos, as they better capture the 3D structural
information of faces. However, a specific model needs to be trained for each
identity with a large dataset. In this paper, we propose Dynamic Facial
Radiance Fields (DFRF) for few-shot talking head synthesis, which can rapidly
generalize to an unseen identity with few training data. Different from the
existing NeRF-based methods which directly encode the 3D geometry and
appearance of a specific person into the network, our DFRF conditions face
radiance field on 2D appearance images to learn the face prior. Thus the facial
radiance field can be flexibly adjusted to the new identity with few reference
images. Additionally, for better modeling of the facial deformations, we
propose a differentiable face warping module conditioned on audio signals to
deform all reference images to the query space. Extensive experiments show that
with only tens of seconds of training clip available, our proposed DFRF can
synthesize natural and high-quality audio-driven talking head videos for novel
identities with only 40k iterations. We highly recommend readers view our
supplementary video for intuitive comparisons. Code is available in
https://sstzal.github.io/DFRF/.";Shuai Shen<author:sep>Wanhua Li<author:sep>Zheng Zhu<author:sep>Yueqi Duan<author:sep>Jie Zhou<author:sep>Jiwen Lu;http://arxiv.org/pdf/2207.11770v1;cs.CV;Accepted by ECCV 2022. Project page: https://sstzal.github.io/DFRF/;nerf
2207.11406v2;http://arxiv.org/abs/2207.11406v2;2022-07-23;PS-NeRF: Neural Inverse Rendering for Multi-view Photometric Stereo;"Traditional multi-view photometric stereo (MVPS) methods are often composed
of multiple disjoint stages, resulting in noticeable accumulated errors. In
this paper, we present a neural inverse rendering method for MVPS based on
implicit representation. Given multi-view images of a non-Lambertian object
illuminated by multiple unknown directional lights, our method jointly
estimates the geometry, materials, and lights. Our method first employs
multi-light images to estimate per-view surface normal maps, which are used to
regularize the normals derived from the neural radiance field. It then jointly
optimizes the surface normals, spatially-varying BRDFs, and lights based on a
shadow-aware differentiable rendering layer. After optimization, the
reconstructed object can be used for novel-view rendering, relighting, and
material editing. Experiments on both synthetic and real datasets demonstrate
that our method achieves far more accurate shape reconstruction than existing
MVPS and neural rendering methods. Our code and model can be found at
https://ywq.github.io/psnerf.";Wenqi Yang<author:sep>Guanying Chen<author:sep>Chaofeng Chen<author:sep>Zhenfang Chen<author:sep>Kwan-Yee K. Wong;http://arxiv.org/pdf/2207.11406v2;cs.CV;ECCV 2022, Project page: https://ywq.github.io/psnerf;nerf
2207.11368v1;http://arxiv.org/abs/2207.11368v1;2022-07-22;Neural-Sim: Learning to Generate Training Data with NeRF;"Training computer vision models usually requires collecting and labeling vast
amounts of imagery under a diverse set of scene configurations and properties.
This process is incredibly time-consuming, and it is challenging to ensure that
the captured data distribution maps well to the target domain of an application
scenario. Recently, synthetic data has emerged as a way to address both of
these issues. However, existing approaches either require human experts to
manually tune each scene property or use automatic methods that provide little
to no control; this requires rendering large amounts of random data variations,
which is slow and is often suboptimal for the target domain. We present the
first fully differentiable synthetic data pipeline that uses Neural Radiance
Fields (NeRFs) in a closed-loop with a target application's loss function. Our
approach generates data on-demand, with no human labor, to maximize accuracy
for a target task. We illustrate the effectiveness of our method on synthetic
and real-world object detection tasks. We also introduce a new
""YCB-in-the-Wild"" dataset and benchmark that provides a test scenario for
object detection with varied poses in real-world environments.";Yunhao Ge<author:sep>Harkirat Behl<author:sep>Jiashu Xu<author:sep>Suriya Gunasekar<author:sep>Neel Joshi<author:sep>Yale Song<author:sep>Xin Wang<author:sep>Laurent Itti<author:sep>Vibhav Vineet;http://arxiv.org/pdf/2207.11368v1;cs.CV;ECCV 2022;nerf
2207.10662v2;http://arxiv.org/abs/2207.10662v2;2022-07-21;Generalizable Patch-Based Neural Rendering;"Neural rendering has received tremendous attention since the advent of Neural
Radiance Fields (NeRF), and has pushed the state-of-the-art on novel-view
synthesis considerably. The recent focus has been on models that overfit to a
single scene, and the few attempts to learn models that can synthesize novel
views of unseen scenes mostly consist of combining deep convolutional features
with a NeRF-like model. We propose a different paradigm, where no deep features
and no NeRF-like volume rendering are needed. Our method is capable of
predicting the color of a target ray in a novel scene directly, just from a
collection of patches sampled from the scene. We first leverage epipolar
geometry to extract patches along the epipolar lines of each reference view.
Each patch is linearly projected into a 1D feature vector and a sequence of
transformers process the collection. For positional encoding, we parameterize
rays as in a light field representation, with the crucial difference that the
coordinates are canonicalized with respect to the target ray, which makes our
method independent of the reference frame and improves generalization. We show
that our approach outperforms the state-of-the-art on novel view synthesis of
unseen scenes even when being trained with considerably less data than prior
work.";Mohammed Suhail<author:sep>Carlos Esteves<author:sep>Leonid Sigal<author:sep>Ameesh Makadia;http://arxiv.org/pdf/2207.10662v2;cs.CV;"Project Page with code and results at
  https://mohammedsuhail.net/gen_patch_neural_rendering/";nerf
2207.10257v2;http://arxiv.org/abs/2207.10257v2;2022-07-21;Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for  Editable Portrait Image Synthesis;"Over the years, 2D GANs have achieved great successes in photorealistic
portrait generation. However, they lack 3D understanding in the generation
process, thus they suffer from multi-view inconsistency problem. To alleviate
the issue, many 3D-aware GANs have been proposed and shown notable results, but
3D GANs struggle with editing semantic attributes. The controllability and
interpretability of 3D GANs have not been much explored. In this work, we
propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware
GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of
discovering semantic attributes during training and controlling them in an
unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN
to obtain a high-fidelity 3D-controllable generator. Unlike existing
latent-based methods allowing implicit pose control, the proposed
3D-controllable StyleGAN enables explicit pose control over portrait
generation. This distillation allows direct compatibility between 3D control
and many StyleGAN-based techniques (e.g., inversion and stylization), and also
brings an advantage in terms of computational resources. Our codes are
available at https://github.com/jgkwak95/SURF-GAN.";Jeong-gi Kwak<author:sep>Yuanming Li<author:sep>Dongsik Yoon<author:sep>Donghyeon Kim<author:sep>David Han<author:sep>Hanseok Ko;http://arxiv.org/pdf/2207.10257v2;cs.CV;ECCV 2022, project page: https://jgkwak95.github.io/surfgan/;nerf
2207.10312v2;http://arxiv.org/abs/2207.10312v2;2022-07-21;AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance  Fields;"Novel view synthesis has recently been revolutionized by learning neural
radiance fields directly from sparse observations. However, rendering images
with this new paradigm is slow due to the fact that an accurate quadrature of
the volume rendering equation requires a large number of samples for each ray.
Previous work has mainly focused on speeding up the network evaluations that
are associated with each sample point, e.g., via caching of radiance values
into explicit spatial data structures, but this comes at the expense of model
compactness. In this paper, we propose a novel dual-network architecture that
takes an orthogonal direction by learning how to best reduce the number of
required sample points. To this end, we split our network into a sampling and
shading network that are jointly trained. Our training scheme employs fixed
sample positions along each ray, and incrementally introduces sparsity
throughout training to achieve high quality even at low sample counts. After
fine-tuning with the target number of samples, the resulting compact neural
representation can be rendered in real-time. Our experiments demonstrate that
our approach outperforms concurrent compact neural representations in terms of
quality and frame rate and performs on par with highly efficient hybrid
representations. Code and supplementary material is available at
https://thomasneff.github.io/adanerf.";Andreas Kurz<author:sep>Thomas Neff<author:sep>Zhaoyang Lv<author:sep>Michael ZollhÃ¶fer<author:sep>Markus Steinberger;http://arxiv.org/pdf/2207.10312v2;cs.CV;ECCV 2022. Project page: https://thomasneff.github.io/adanerf;nerf
2207.09193v1;http://arxiv.org/abs/2207.09193v1;2022-07-19;NDF: Neural Deformable Fields for Dynamic Human Modelling;"We propose Neural Deformable Fields (NDF), a new representation for dynamic
human digitization from a multi-view video. Recent works proposed to represent
a dynamic human body with shared canonical neural radiance fields which links
to the observation space with deformation fields estimations. However, the
learned canonical representation is static and the current design of the
deformation fields is not able to represent large movements or detailed
geometry changes. In this paper, we propose to learn a neural deformable field
wrapped around a fitted parametric body model to represent the dynamic human.
The NDF is spatially aligned by the underlying reference surface. A neural
network is then learned to map pose to the dynamics of NDF. The proposed NDF
representation can synthesize the digitized performer with novel views and
novel poses with a detailed and reasonable dynamic appearance. Experiments show
that our method significantly outperforms recent human synthesis methods.";Ruiqi Zhang<author:sep>Jie Chen;http://arxiv.org/pdf/2207.09193v1;cs.CV;16 pages, 7 figures. Accepted by ECCV 2022;
2207.06793v1;http://arxiv.org/abs/2207.06793v1;2022-07-14;Neural apparent BRDF fields for multiview photometric stereo;"We propose to tackle the multiview photometric stereo problem using an
extension of Neural Radiance Fields (NeRFs), conditioned on light source
direction. The geometric part of our neural representation predicts surface
normal direction, allowing us to reason about local surface reflectance. The
appearance part of our neural representation is decomposed into a neural
bidirectional reflectance function (BRDF), learnt as part of the fitting
process, and a shadow prediction network (conditioned on light source
direction) allowing us to model the apparent BRDF. This balance of learnt
components with inductive biases based on physical image formation models
allows us to extrapolate far from the light source and viewer directions
observed during training. We demonstrate our approach on a multiview
photometric stereo benchmark and show that competitive performance can be
obtained with the neural density representation of a NeRF.";Meghna Asthana<author:sep>William A. P. Smith<author:sep>Patrik Huber;http://arxiv.org/pdf/2207.06793v1;cs.CV;9 pages, 6 figures, 1 table;nerf
2207.05736v2;http://arxiv.org/abs/2207.05736v2;2022-07-12;Vision Transformer for NeRF-Based View Synthesis from a Single Input  Image;"Although neural radiance fields (NeRF) have shown impressive advances for
novel view synthesis, most methods typically require multiple input images of
the same scene with accurate camera poses. In this work, we seek to
substantially reduce the inputs to a single unposed image. Existing approaches
condition on local image features to reconstruct a 3D object, but often render
blurry predictions at viewpoints that are far away from the source view. To
address this issue, we propose to leverage both the global and local features
to form an expressive 3D representation. The global features are learned from a
vision transformer, while the local features are extracted from a 2D
convolutional network. To synthesize a novel view, we train a multilayer
perceptron (MLP) network conditioned on the learned 3D representation to
perform volume rendering. This novel 3D representation allows the network to
reconstruct unseen regions without enforcing constraints like symmetry or
canonical coordinate systems. Our method can render novel views from only a
single input image and generalize across multiple object categories using a
single model. Quantitative and qualitative evaluations demonstrate that the
proposed method achieves state-of-the-art performance and renders richer
details than existing approaches.";Kai-En Lin<author:sep>Lin Yen-Chen<author:sep>Wei-Sheng Lai<author:sep>Tsung-Yi Lin<author:sep>Yi-Chang Shih<author:sep>Ravi Ramamoorthi;http://arxiv.org/pdf/2207.05736v2;cs.CV;"WACV 2023 Project website:
  https://cseweb.ucsd.edu/~viscomp/projects/VisionNeRF/";nerf
2207.05009v1;http://arxiv.org/abs/2207.05009v1;2022-07-11;A Learned Radiance-Field Representation for Complex Luminaires;"We propose an efficient method for rendering complex luminaires using a
high-quality octree-based representation of the luminaire emission. Complex
luminaires are a particularly challenging problem in rendering, due to their
caustic light paths inside the luminaire. We reduce the geometric complexity of
luminaires by using a simple proxy geometry and encode the visually-complex
emitted light field by using a neural radiance field. We tackle the multiple
challenges of using NeRFs for representing luminaires, including their high
dynamic range, high-frequency content and null-emission areas, by proposing a
specialized loss function. For rendering, we distill our luminaires' NeRF into
a Plenoctree, which we can be easily integrated into traditional rendering
systems. Our approach allows for speed-ups of up to 2 orders of magnitude in
scenes containing complex luminaires introducing minimal error.";Jorge Condor<author:sep>AdriÃ¡n Jarabo;http://arxiv.org/pdf/2207.05009v1;cs.GR;"10 pages, 7 figures. Eurographics Proceedings (EGSR 2022,
  Symposium-only track) (https://diglib.eg.org/handle/10.2312/sr20221155)";nerf
2207.04465v1;http://arxiv.org/abs/2207.04465v1;2022-07-10;Progressively-connected Light Field Network for Efficient View Synthesis;"This paper presents a Progressively-connected Light Field network (ProLiF),
for the novel view synthesis of complex forward-facing scenes. ProLiF encodes a
4D light field, which allows rendering a large batch of rays in one training
step for image- or patch-level losses. Directly learning a neural light field
from images has difficulty in rendering multi-view consistent images due to its
unawareness of the underlying 3D geometry. To address this problem, we propose
a progressive training scheme and regularization losses to infer the underlying
geometry during training, both of which enforce the multi-view consistency and
thus greatly improves the rendering quality. Experiments demonstrate that our
method is able to achieve significantly better rendering quality than the
vanilla neural light fields and comparable results to NeRF-like rendering
methods on the challenging LLFF dataset and Shiny Object dataset. Moreover, we
demonstrate better compatibility with LPIPS loss to achieve robustness to
varying light conditions and CLIP loss to control the rendering style of the
scene. Project page: https://totoro97.github.io/projects/prolif.";Peng Wang<author:sep>Yuan Liu<author:sep>Guying Lin<author:sep>Jiatao Gu<author:sep>Lingjie Liu<author:sep>Taku Komura<author:sep>Wenping Wang;http://arxiv.org/pdf/2207.04465v1;cs.CV;Project page: https://totoro97.github.io/projects/prolif;nerf
2207.02621v2;http://arxiv.org/abs/2207.02621v2;2022-07-06;VMRF: View Matching Neural Radiance Fields;"Neural Radiance Fields (NeRF) have demonstrated very impressive performance
in novel view synthesis via implicitly modelling 3D representations from
multi-view 2D images. However, most existing studies train NeRF models with
either reasonable camera pose initialization or manually-crafted camera pose
distributions which are often unavailable or hard to acquire in various
real-world data. We design VMRF, an innovative view matching NeRF that enables
effective NeRF training without requiring prior knowledge in camera poses or
camera pose distributions. VMRF introduces a view matching scheme, which
exploits unbalanced optimal transport to produce a feature transport plan for
mapping a rendered image with randomly initialized camera pose to the
corresponding real image. With the feature transport plan as the guidance, a
novel pose calibration technique is designed which rectifies the initially
randomized camera poses by predicting relative pose transformations between the
pair of rendered and real images. Extensive experiments over a number of
synthetic and real datasets show that the proposed VMRF outperforms the
state-of-the-art qualitatively and quantitatively by large margins.";Jiahui Zhang<author:sep>Fangneng Zhan<author:sep>Rongliang Wu<author:sep>Yingchen Yu<author:sep>Wenqing Zhang<author:sep>Bai Song<author:sep>Xiaoqin Zhang<author:sep>Shijian Lu;http://arxiv.org/pdf/2207.02621v2;cs.CV;This paper has been accepted to ACM MM 2022;nerf
2207.02363v1;http://arxiv.org/abs/2207.02363v1;2022-07-05;SNeRF: Stylized Neural Implicit Representations for 3D Scenes;"This paper presents a stylized novel view synthesis method. Applying
state-of-the-art stylization methods to novel views frame by frame often causes
jittering artifacts due to the lack of cross-view consistency. Therefore, this
paper investigates 3D scene stylization that provides a strong inductive bias
for consistent novel view synthesis. Specifically, we adopt the emerging neural
radiance fields (NeRF) as our choice of 3D scene representation for their
capability to render high-quality novel views for a variety of scenes. However,
as rendering a novel view from a NeRF requires a large number of samples,
training a stylized NeRF requires a large amount of GPU memory that goes beyond
an off-the-shelf GPU capacity. We introduce a new training method to address
this problem by alternating the NeRF and stylization optimization steps. Such a
method enables us to make full use of our hardware memory capacity to both
generate images at higher resolution and adopt more expressive image style
transfer methods. Our experiments show that our method produces stylized NeRFs
for a wide range of content, including indoor, outdoor and dynamic scenes, and
synthesizes high-quality novel views with cross-view consistency.";Thu Nguyen-Phuoc<author:sep>Feng Liu<author:sep>Lei Xiao;http://arxiv.org/pdf/2207.02363v1;cs.CV;"SIGGRAPH 2022 (Journal track). Project page:
  https://research.facebook.com/publications/snerf-stylized-neural-implicit-representations-for-3d-scenes/";nerf
2207.01583v3;http://arxiv.org/abs/2207.01583v3;2022-07-04;LaTeRF: Label and Text Driven Object Radiance Fields;"Obtaining 3D object representations is important for creating photo-realistic
simulations and for collecting AR and VR assets. Neural fields have shown their
effectiveness in learning a continuous volumetric representation of a scene
from 2D images, but acquiring object representations from these models with
weak supervision remains an open challenge. In this paper we introduce LaTeRF,
a method for extracting an object of interest from a scene given 2D images of
the entire scene, known camera poses, a natural language description of the
object, and a set of point-labels of object and non-object points in the input
images. To faithfully extract the object from the scene, LaTeRF extends the
NeRF formulation with an additional `objectness' probability at each 3D point.
Additionally, we leverage the rich latent space of a pre-trained CLIP model
combined with our differentiable object renderer, to inpaint the occluded parts
of the object. We demonstrate high-fidelity object extraction on both synthetic
and real-world datasets and justify our design choices through an extensive
ablation study.";Ashkan Mirzaei<author:sep>Yash Kant<author:sep>Jonathan Kelly<author:sep>Igor Gilitschenski;http://arxiv.org/pdf/2207.01583v3;cs.CV;;nerf
2207.01164v1;http://arxiv.org/abs/2207.01164v1;2022-07-04;Aug-NeRF: Training Stronger Neural Radiance Fields with Triple-Level  Physically-Grounded Augmentations;"Neural Radiance Field (NeRF) regresses a neural parameterized scene by
differentially rendering multi-view images with ground-truth supervision.
However, when interpolating novel views, NeRF often yields inconsistent and
visually non-smooth geometric results, which we consider as a generalization
gap between seen and unseen views. Recent advances in convolutional neural
networks have demonstrated the promise of advanced robust data augmentations,
either random or learned, in enhancing both in-distribution and
out-of-distribution generalization. Inspired by that, we propose Augmented NeRF
(Aug-NeRF), which for the first time brings the power of robust data
augmentations into regularizing the NeRF training. Particularly, our proposal
learns to seamlessly blend worst-case perturbations into three distinct levels
of the NeRF pipeline with physical grounds, including (1) the input
coordinates, to simulate imprecise camera parameters at image capture; (2)
intermediate features, to smoothen the intrinsic feature manifold; and (3)
pre-rendering output, to account for the potential degradation factors in the
multi-view image supervision. Extensive results demonstrate that Aug-NeRF
effectively boosts NeRF performance in both novel view synthesis (up to 1.5dB
PSNR gain) and underlying geometry reconstruction. Furthermore, thanks to the
implicit smooth prior injected by the triple-level augmentations, Aug-NeRF can
even recover scenes from heavily corrupted images, a highly challenging setting
untackled before. Our codes are available in
https://github.com/VITA-Group/Aug-NeRF.";Tianlong Chen<author:sep>Peihao Wang<author:sep>Zhiwen Fan<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2207.01164v1;cs.CV;"IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2022";nerf
2206.15255v1;http://arxiv.org/abs/2206.15255v1;2022-06-30;Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in  Robotic Surgery;"Reconstruction of the soft tissues in robotic surgery from endoscopic stereo
videos is important for many applications such as intra-operative navigation
and image-guided robotic surgery automation. Previous works on this task mainly
rely on SLAM-based approaches, which struggle to handle complex surgical
scenes. Inspired by recent progress in neural rendering, we present a novel
framework for deformable tissue reconstruction from binocular captures in
robotic surgery under the single-viewpoint setting. Our framework adopts
dynamic neural radiance fields to represent deformable surgical scenes in MLPs
and optimize shapes and deformations in a learning-based manner. In addition to
non-rigid deformations, tool occlusion and poor 3D clues from a single
viewpoint are also particular challenges in soft tissue reconstruction. To
overcome these difficulties, we present a series of strategies of tool
mask-guided ray casting, stereo depth-cueing ray marching and stereo
depth-supervised optimization. With experiments on DaVinci robotic surgery
videos, our method significantly outperforms the current state-of-the-art
reconstruction method for handling various complex non-rigid deformations. To
our best knowledge, this is the first work leveraging neural rendering for
surgical scene 3D reconstruction with remarkable potential demonstrated. Code
is available at: https://github.com/med-air/EndoNeRF.";Yuehao Wang<author:sep>Yonghao Long<author:sep>Siu Hin Fan<author:sep>Qi Dou;http://arxiv.org/pdf/2206.15255v1;cs.CV;11 pages, 4 figures, conference;nerf
2206.14938v2;http://arxiv.org/abs/2206.14938v2;2022-06-29;Regularization of NeRFs using differential geometry;"Neural radiance fields, or NeRF, represent a breakthrough in the field of
novel view synthesis and 3D modeling of complex scenes from multi-view image
collections. Numerous recent works have shown the importance of making NeRF
models more robust, by means of regularization, in order to train with possibly
inconsistent and/or very sparse data. In this work, we explore how differential
geometry can provide elegant regularization tools for robustly training
NeRF-like models, which are modified so as to represent continuous and
infinitely differentiable functions. In particular, we present a generic
framework for regularizing different types of NeRFs observations to improve the
performance in challenging conditions. We also show how the same formalism can
also be used to natively encourage the regularity of surfaces by means of
Gaussian or mean curvatures.";Thibaud Ehret<author:sep>Roger MarÃ­<author:sep>Gabriele Facciolo;http://arxiv.org/pdf/2206.14938v2;cs.CV;;nerf
2206.12455v2;http://arxiv.org/abs/2206.12455v2;2022-06-24;Ev-NeRF: Event Based Neural Radiance Field;"We present Ev-NeRF, a Neural Radiance Field derived from event data. While
event cameras can measure subtle brightness changes in high frame rates, the
measurements in low lighting or extreme motion suffer from significant domain
discrepancy with complex noise. As a result, the performance of event-based
vision tasks does not transfer to challenging environments, where the event
cameras are expected to thrive over normal cameras. We find that the multi-view
consistency of NeRF provides a powerful self-supervision signal for eliminating
the spurious measurements and extracting the consistent underlying structure
despite highly noisy input. Instead of posed images of the original NeRF, the
input to Ev-NeRF is the event measurements accompanied by the movements of the
sensors. Using the loss function that reflects the measurement model of the
sensor, Ev-NeRF creates an integrated neural volume that summarizes the
unstructured and sparse data points captured for about 2-4 seconds. The
generated neural volume can also produce intensity images from novel views with
reasonable depth estimates, which can serve as a high-quality input to various
vision-based tasks. Our results show that Ev-NeRF achieves competitive
performance for intensity image reconstruction under extreme noise conditions
and high-dynamic-range imaging.";Inwoo Hwang<author:sep>Junho Kim<author:sep>Young Min Kim;http://arxiv.org/pdf/2206.12455v2;cs.CV;Accepted to WACV 2023;nerf
2206.11896v3;http://arxiv.org/abs/2206.11896v3;2022-06-23;EventNeRF: Neural Radiance Fields from a Single Colour Event Camera;"Asynchronously operating event cameras find many applications due to their
high dynamic range, vanishingly low motion blur, low latency and low data
bandwidth. The field saw remarkable progress during the last few years, and
existing event-based 3D reconstruction approaches recover sparse point clouds
of the scene. However, such sparsity is a limiting factor in many cases,
especially in computer vision and graphics, that has not been addressed
satisfactorily so far. Accordingly, this paper proposes the first approach for
3D-consistent, dense and photorealistic novel view synthesis using just a
single colour event stream as input. At its core is a neural radiance field
trained entirely in a self-supervised manner from events while preserving the
original resolution of the colour event channels. Next, our ray sampling
strategy is tailored to events and allows for data-efficient training. At test,
our method produces results in the RGB space at unprecedented quality. We
evaluate our method qualitatively and numerically on several challenging
synthetic and real scenes and show that it produces significantly denser and
more visually appealing renderings than the existing methods. We also
demonstrate robustness in challenging scenarios with fast motion and under low
lighting conditions. We release the newly recorded dataset and our source code
to facilitate the research field, see https://4dqv.mpi-inf.mpg.de/EventNeRF.";Viktor Rudnev<author:sep>Mohamed Elgharib<author:sep>Christian Theobalt<author:sep>Vladislav Golyanik;http://arxiv.org/pdf/2206.11896v3;cs.CV;"19 pages, 21 figures, 3 tables; CVPR 2023";nerf
2206.11952v1;http://arxiv.org/abs/2206.11952v1;2022-06-23;UNeRF: Time and Memory Conscious U-Shaped Network for Training Neural  Radiance Fields;"Neural Radiance Fields (NeRFs) increase reconstruction detail for novel view
synthesis and scene reconstruction, with applications ranging from large static
scenes to dynamic human motion. However, the increased resolution and
model-free nature of such neural fields come at the cost of high training times
and excessive memory requirements. Recent advances improve the inference time
by using complementary data structures yet these methods are ill-suited for
dynamic scenes and often increase memory consumption. Little has been done to
reduce the resources required at training time. We propose a method to exploit
the redundancy of NeRF's sample-based computations by partially sharing
evaluations across neighboring sample points. Our UNeRF architecture is
inspired by the UNet, where spatial resolution is reduced in the middle of the
network and information is shared between adjacent samples. Although this
change violates the strict and conscious separation of view-dependent
appearance and view-independent density estimation in the NeRF method, we show
that it improves novel view synthesis. We also introduce an alternative
subsampling strategy which shares computation while minimizing any violation of
view invariance. UNeRF is a plug-in module for the original NeRF network. Our
major contributions include reduction of the memory footprint, improved
accuracy, and reduced amortized processing time both during training and
inference. With only weak assumptions on locality, we achieve improved resource
utilization on a variety of neural radiance fields tasks. We demonstrate
applications to the novel view synthesis of static scenes as well as dynamic
human shape and motion.";Abiramy Kuganesan<author:sep>Shih-yang Su<author:sep>James J. Little<author:sep>Helge Rhodin;http://arxiv.org/pdf/2206.11952v1;cs.CV;;nerf
2206.10885v2;http://arxiv.org/abs/2206.10885v2;2022-06-22;KiloNeuS: A Versatile Neural Implicit Surface Representation for  Real-Time Rendering;"NeRF-based techniques fit wide and deep multi-layer perceptrons (MLPs) to a
continuous radiance field that can be rendered from any unseen viewpoint.
However, the lack of surface and normals definition and high rendering times
limit their usage in typical computer graphics applications. Such limitations
have recently been overcome separately, but solving them together remains an
open problem. We present KiloNeuS, a neural representation reconstructing an
implicit surface represented as a signed distance function (SDF) from
multi-view images and enabling real-time rendering by partitioning the space
into thousands of tiny MLPs fast to inference. As we learn the implicit surface
locally using independent models, resulting in a globally coherent geometry is
non-trivial and needs to be addressed during training. We evaluate rendering
performance on a GPU-accelerated ray-caster with in-shader neural network
inference, resulting in an average of 46 FPS at high resolution, proving a
satisfying tradeoff between storage costs and rendering quality. In fact, our
evaluation for rendering quality and surface recovery shows that KiloNeuS
outperforms its single-MLP counterpart. Finally, to exhibit the versatility of
KiloNeuS, we integrate it into an interactive path-tracer taking full advantage
of its surface normals. We consider our work a crucial first step toward
real-time rendering of implicit neural representations under global
illumination.";Stefano Esposito<author:sep>Daniele Baieri<author:sep>Stefan Zellmann<author:sep>AndrÃ© Hinkenjann<author:sep>Emanuele RodolÃ ;http://arxiv.org/pdf/2206.10885v2;cs.CV;9 pages, 8 figures;nerf
2206.08361v2;http://arxiv.org/abs/2206.08361v2;2022-06-16;Controllable 3D Face Synthesis with Conditional Generative Occupancy  Fields;"Capitalizing on the recent advances in image generation models, existing
controllable face image synthesis methods are able to generate high-fidelity
images with some levels of controllability, e.g., controlling the shapes,
expressions, textures, and poses of the generated face images. However, these
methods focus on 2D image generative models, which are prone to producing
inconsistent face images under large expression and pose changes. In this
paper, we propose a new NeRF-based conditional 3D face synthesis framework,
which enables 3D controllability over the generated face images by imposing
explicit 3D conditions from 3D face priors. At its core is a conditional
Generative Occupancy Field (cGOF) that effectively enforces the shape of the
generated face to commit to a given 3D Morphable Model (3DMM) mesh. To achieve
accurate control over fine-grained 3D face shapes of the synthesized image, we
additionally incorporate a 3D landmark loss as well as a volume warping loss
into our synthesis algorithm. Experiments validate the effectiveness of the
proposed method, which is able to generate high-fidelity face images and shows
more precise 3D controllability than state-of-the-art 2D-based controllable
face synthesis methods. Find code and demo at
https://keqiangsun.github.io/projects/cgof.";Keqiang Sun<author:sep>Shangzhe Wu<author:sep>Zhaoyang Huang<author:sep>Ning Zhang<author:sep>Quan Wang<author:sep>HongSheng Li;http://arxiv.org/pdf/2206.08361v2;cs.CV;;nerf
2206.08355v3;http://arxiv.org/abs/2206.08355v3;2022-06-16;FWD: Real-time Novel View Synthesis with Forward Warping and Depth;"Novel view synthesis (NVS) is a challenging task requiring systems to
generate photorealistic images of scenes from new viewpoints, where both
quality and speed are important for applications. Previous image-based
rendering (IBR) methods are fast, but have poor quality when input views are
sparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give
impressive results but are not real-time. In our paper, we propose a
generalizable NVS method with sparse inputs, called FWD, which gives
high-quality synthesis in real-time. With explicit depth and differentiable
rendering, it achieves competitive results to the SOTA methods with 130-1000x
speedup and better perceptual quality. If available, we can seamlessly
integrate sensor depth during either training or inference to improve image
quality while retaining real-time speed. With the growing prevalence of depths
sensors, we hope that methods making use of depth will become increasingly
useful.";Ang Cao<author:sep>Chris Rockwell<author:sep>Justin Johnson;http://arxiv.org/pdf/2206.08355v3;cs.CV;CVPR 2022. Project website https://caoang327.github.io/FWD/;nerf
2206.07698v2;http://arxiv.org/abs/2206.07698v2;2022-06-15;Neural Deformable Voxel Grid for Fast Optimization of Dynamic View  Synthesis;"Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel
view synthesis (NVS) for its superior performance. In this paper, we propose to
synthesize dynamic scenes. Extending the methods for static scenes to dynamic
scenes is not straightforward as both the scene geometry and appearance change
over time, especially under monocular setup. Also, the existing dynamic NeRF
methods generally require a lengthy per-scene training procedure, where
multi-layer perceptrons (MLP) are fitted to model both motions and radiance. In
this paper, built on top of the recent advances in voxel-grid optimization, we
propose a fast deformable radiance field method to handle dynamic scenes. Our
method consists of two modules. The first module adopts a deformation grid to
store 3D dynamic features, and a light-weight MLP for decoding the deformation
that maps a 3D point in the observation space to the canonical space using the
interpolated features. The second module contains a density and a color grid to
model the geometry and density of the scene. The occlusion is explicitly
modeled to further improve the rendering quality. Experimental results show
that our method achieves comparable performance to D-NeRF using only 20 minutes
for training, which is more than 70x faster than D-NeRF, clearly demonstrating
the efficiency of our proposed method.";Xiang Guo<author:sep>Guanying Chen<author:sep>Yuchao Dai<author:sep>Xiaoqing Ye<author:sep>Jiadai Sun<author:sep>Xiao Tan<author:sep>Errui Ding;http://arxiv.org/pdf/2206.07698v2;cs.CV;"Technical Report: 29 pages; project page:
  https://npucvr.github.io/NDVG";nerf
2206.06577v1;http://arxiv.org/abs/2206.06577v1;2022-06-14;Physics Informed Neural Fields for Smoke Reconstruction with Sparse Data;"High-fidelity reconstruction of fluids from sparse multiview RGB videos
remains a formidable challenge due to the complexity of the underlying physics
as well as complex occlusion and lighting in captures. Existing solutions
either assume knowledge of obstacles and lighting, or only focus on simple
fluid scenes without obstacles or complex lighting, and thus are unsuitable for
real-world scenes with unknown lighting or arbitrary obstacles. We present the
first method to reconstruct dynamic fluid by leveraging the governing physics
(ie, Navier -Stokes equations) in an end-to-end optimization from sparse videos
without taking lighting conditions, geometry information, or boundary
conditions as input. We provide a continuous spatio-temporal scene
representation using neural networks as the ansatz of density and velocity
solution functions for fluids as well as the radiance field for static objects.
With a hybrid architecture that separates static and dynamic contents, fluid
interactions with static obstacles are reconstructed for the first time without
additional geometry input or human labeling. By augmenting time-varying neural
radiance fields with physics-informed deep learning, our method benefits from
the supervision of images and physical priors. To achieve robust optimization
from sparse views, we introduced a layer-by-layer growing strategy to
progressively increase the network capacity. Using progressively growing models
with a new regularization term, we manage to disentangle density-color
ambiguity in radiance fields without overfitting. A pretrained
density-to-velocity fluid model is leveraged in addition as the data prior to
avoid suboptimal velocity which underestimates vorticity but trivially fulfills
physical equations. Our method exhibits high-quality results with relaxed
constraints and strong flexibility on a representative set of synthetic and
real flow captures.";Mengyu Chu<author:sep>Lingjie Liu<author:sep>Quan Zheng<author:sep>Erik Franz<author:sep>Hans-Peter Seidel<author:sep>Christian Theobalt<author:sep>Rhaleb Zayer;http://arxiv.org/pdf/2206.06577v1;cs.GR;"accepted to ACM Transactions On Graphics (SIGGRAPH 2022), further
  info:\url{https://people.mpi-inf.mpg.de/~mchu/projects/PI-NeRF/}";
2206.06340v1;http://arxiv.org/abs/2206.06340v1;2022-06-13;SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data;"We present a method for the accurate 3D reconstruction of partly-symmetric
objects. We build on the strengths of recent advances in neural reconstruction
and rendering such as Neural Radiance Fields (NeRF). A major shortcoming of
such approaches is that they fail to reconstruct any part of the object which
is not clearly visible in the training image, which is often the case for
in-the-wild images and videos. When evidence is lacking, structural priors such
as symmetry can be used to complete the missing information. However,
exploiting such priors in neural rendering is highly non-trivial: while
geometry and non-reflective materials may be symmetric, shadows and reflections
from the ambient scene are not symmetric in general. To address this, we apply
a soft symmetry constraint to the 3D geometry and material properties, having
factored appearance into lighting, albedo colour and reflectivity. We evaluate
our method on the recently introduced CO3D dataset, focusing on the car
category due to the challenge of reconstructing highly-reflective materials. We
show that it can reconstruct unobserved regions with high fidelity and render
high-quality novel view images.";Eldar Insafutdinov<author:sep>Dylan Campbell<author:sep>JoÃ£o F. Henriques<author:sep>Andrea Vedaldi;http://arxiv.org/pdf/2206.06340v1;cs.CV;First two authors contributed equally;nerf
2206.06100v1;http://arxiv.org/abs/2206.06100v1;2022-06-13;AR-NeRF: Unsupervised Learning of Depth and Defocus Effects from Natural  Images with Aperture Rendering Neural Radiance Fields;"Fully unsupervised 3D representation learning has gained attention owing to
its advantages in data collection. A successful approach involves a
viewpoint-aware approach that learns an image distribution based on generative
models (e.g., generative adversarial networks (GANs)) while generating various
view images based on 3D-aware models (e.g., neural radiance fields (NeRFs)).
However, they require images with various views for training, and consequently,
their application to datasets with few or limited viewpoints remains a
challenge. As a complementary approach, an aperture rendering GAN (AR-GAN) that
employs a defocus cue was proposed. However, an AR-GAN is a CNN-based model and
represents a defocus independently from a viewpoint change despite its high
correlation, which is one of the reasons for its performance. As an alternative
to an AR-GAN, we propose an aperture rendering NeRF (AR-NeRF), which can
utilize viewpoint and defocus cues in a unified manner by representing both
factors in a common ray-tracing framework. Moreover, to learn defocus-aware and
defocus-independent representations in a disentangled manner, we propose
aperture randomized training, for which we learn to generate images while
randomizing the aperture size and latent codes independently. During our
experiments, we applied AR-NeRF to various natural image datasets, including
flower, bird, and face images, the results of which demonstrate the utility of
AR-NeRF for unsupervised learning of the depth and defocus effects.";Takuhiro Kaneko;http://arxiv.org/pdf/2206.06100v1;cs.CV;"Accepted to CVPR 2022. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-nerf/";nerf
2206.06481v1;http://arxiv.org/abs/2206.06481v1;2022-06-13;RigNeRF: Fully Controllable Neural 3D Portraits;"Volumetric neural rendering methods, such as neural radiance fields (NeRFs),
have enabled photo-realistic novel view synthesis. However, in their standard
form, NeRFs do not support the editing of objects, such as a human head, within
a scene. In this work, we propose RigNeRF, a system that goes beyond just novel
view synthesis and enables full control of head pose and facial expressions
learned from a single portrait video. We model changes in head pose and facial
expressions using a deformation field that is guided by a 3D morphable face
model (3DMM). The 3DMM effectively acts as a prior for RigNeRF that learns to
predict only residuals to the 3DMM deformations and allows us to render novel
(rigid) poses and (non-rigid) expressions that were not present in the input
sequence. Using only a smartphone-captured short video of a subject for
training, we demonstrate the effectiveness of our method on free view synthesis
of a portrait scene with explicit head pose and expression controls. The
project page can be found here:
http://shahrukhathar.github.io/2022/06/06/RigNeRF.html";ShahRukh Athar<author:sep>Zexiang Xu<author:sep>Kalyan Sunkavalli<author:sep>Eli Shechtman<author:sep>Zhixin Shu;http://arxiv.org/pdf/2206.06481v1;cs.CV;"The project page can be found here:
  http://shahrukhathar.github.io/2022/06/06/RigNeRF.html";nerf
2206.05085v4;http://arxiv.org/abs/2206.05085v4;2022-06-10;Improved Direct Voxel Grid Optimization for Radiance Fields  Reconstruction;"In this technical report, we improve the DVGO framework (called DVGOv2),
which is based on Pytorch and uses the simplest dense grid representation.
First, we re-implement part of the Pytorch operations with cuda, achieving 2-3x
speedup. The cuda extension is automatically compiled just in time. Second, we
extend DVGO to support Forward-facing and Unbounded Inward-facing capturing.
Third, we improve the space time complexity of the distortion loss proposed by
mip-NeRF 360 from O(N^2) to O(N). The distortion loss improves our quality and
training speed. Our efficient implementation could allow more future works to
benefit from the loss.";Cheng Sun<author:sep>Min Sun<author:sep>Hwann-Tzong Chen;http://arxiv.org/pdf/2206.05085v4;cs.GR;"Project page https://sunset1995.github.io/dvgo/ ; Code
  https://github.com/sunset1995/DirectVoxGO ; Results updated";nerf
2206.05375v1;http://arxiv.org/abs/2206.05375v1;2022-06-10;Generalizable Neural Radiance Fields for Novel View Synthesis with  Transformer;"We propose a Transformer-based NeRF (TransNeRF) to learn a generic neural
radiance field conditioned on observed-view images for the novel view synthesis
task. By contrast, existing MLP-based NeRFs are not able to directly receive
observed views with an arbitrary number and require an auxiliary pooling-based
operation to fuse source-view information, resulting in the missing of
complicated relationships between source views and the target rendering view.
Furthermore, current approaches process each 3D point individually and ignore
the local consistency of a radiance field scene representation. These
limitations potentially can reduce their performance in challenging real-world
applications where large differences between source views and a novel rendering
view may exist. To address these challenges, our TransNeRF utilizes the
attention mechanism to naturally decode deep associations of an arbitrary
number of source views into a coordinate-based scene representation. Local
consistency of shape and appearance are considered in the ray-cast space and
the surrounding-view space within a unified Transformer network. Experiments
demonstrate that our TransNeRF, trained on a wide variety of scenes, can
achieve better performance in comparison to state-of-the-art image-based neural
rendering methods in both scene-agnostic and per-scene finetuning scenarios
especially when there is a considerable gap between source views and a
rendering view.";Dan Wang<author:sep>Xinrui Cui<author:sep>Septimiu Salcudean<author:sep>Z. Jane Wang;http://arxiv.org/pdf/2206.05375v1;cs.CV;;nerf
2206.04901v1;http://arxiv.org/abs/2206.04901v1;2022-06-10;NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors;"Though Neural Radiance Field (NeRF) demonstrates compelling novel view
synthesis results, it is still unintuitive to edit a pre-trained NeRF because
the neural network's parameters and the scene geometry/appearance are often not
explicitly associated. In this paper, we introduce the first framework that
enables users to remove unwanted objects or retouch undesired regions in a 3D
scene represented by a pre-trained NeRF without any category-specific data and
training. The user first draws a free-form mask to specify a region containing
unwanted objects over a rendered view from the pre-trained NeRF. Our framework
first transfers the user-provided mask to other rendered views and estimates
guiding color and depth images within these transferred masked regions. Next,
we formulate an optimization problem that jointly inpaints the image content in
all masked regions across multiple views by updating the NeRF model's
parameters. We demonstrate our framework on diverse scenes and show it obtained
visual plausible and structurally consistent results across multiple views
using shorter time and less user manual efforts.";Hao-Kang Liu<author:sep>I-Chao Shen<author:sep>Bing-Yu Chen;http://arxiv.org/pdf/2206.04901v1;cs.CV;"Hao-Kang Liu and I-Chao Shen contributed equally to the paper.
  Project page: https://jdily.github.io/proj_site/nerfin_proj.html";nerf
2206.04669v1;http://arxiv.org/abs/2206.04669v1;2022-06-09;Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields;"Comprehensive 3D scene understanding, both geometrically and semantically, is
important for real-world applications such as robot perception. Most of the
existing work has focused on developing data-driven discriminative models for
scene understanding. This paper provides a new approach to scene understanding,
from a synthesis model perspective, by leveraging the recent progress on
implicit 3D representation and neural rendering. Building upon the great
success of Neural Radiance Fields (NeRFs), we introduce Scene-Property
Synthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic
RGB images from novel viewpoints, but also render various accurate scene
properties (e.g., appearance, geometry, and semantics). By doing so, we
facilitate addressing a variety of scene understanding tasks under a unified
framework, including semantic segmentation, surface normal estimation,
reshading, keypoint detection, and edge detection. Our SS-NeRF framework can be
a powerful tool for bridging generative learning and discriminative learning,
and thus be beneficial to the investigation of a wide range of interesting
problems, such as studying task relationships within a synthesis paradigm,
transferring knowledge to novel tasks, facilitating downstream discriminative
tasks as ways of data augmentation, and serving as auto-labeller for data
creation.";Mingtong Zhang<author:sep>Shuhong Zheng<author:sep>Zhipeng Bao<author:sep>Martial Hebert<author:sep>Yu-Xiong Wang;http://arxiv.org/pdf/2206.04669v1;cs.CV;;nerf
2206.03591v3;http://arxiv.org/abs/2206.03591v3;2022-06-07;ObPose: Leveraging Pose for Object-Centric Scene Inference and  Generation in 3D;"We present ObPose, an unsupervised object-centric inference and generation
model which learns 3D-structured latent representations from RGB-D scenes.
Inspired by prior art in 2D representation learning, ObPose considers a
factorised latent space, separately encoding object location (where) and
appearance (what). ObPose further leverages an object's pose (i.e. location and
orientation), defined via a minimum volume principle, as a novel inductive bias
for learning the where component. To achieve this, we propose an efficient,
voxelised approximation approach to recover the object shape directly from a
neural radiance field (NeRF). As a consequence, ObPose models each scene as a
composition of NeRFs, richly representing individual objects. To evaluate the
quality of the learned representations, ObPose is evaluated quantitatively on
the YCB, MultiShapeNet, and CLEVR datatasets for unsupervised scene
segmentation, outperforming the current state-of-the-art in 3D scene inference
(ObSuRF) by a significant margin. Generative results provide qualitative
demonstration that the same ObPose model can both generate novel scenes and
flexibly edit the objects in them. These capacities again reflect the quality
of the learned latents and the benefits of disentangling the where and what
components of a scene. Key design choices made in the ObPose encoder are
validated with ablations.";Yizhe Wu<author:sep>Oiwi Parker Jones<author:sep>Ingmar Posner;http://arxiv.org/pdf/2206.03591v3;cs.CV;14 pages, 4 figures;nerf
2206.01634v1;http://arxiv.org/abs/2206.01634v1;2022-06-03;Reinforcement Learning with Neural Radiance Fields;"It is a long-standing problem to find effective representations for training
reinforcement learning (RL) agents. This paper demonstrates that learning state
representations with supervision from Neural Radiance Fields (NeRFs) can
improve the performance of RL compared to other learned representations or even
low-dimensional, hand-engineered state information. Specifically, we propose to
train an encoder that maps multiple image observations to a latent space
describing the objects in the scene. The decoder built from a
latent-conditioned NeRF serves as the supervision signal to learn the latent
space. An RL algorithm then operates on the learned latent space as its state
representation. We call this NeRF-RL. Our experiments indicate that NeRF as
supervision leads to a latent space better suited for the downstream RL tasks
involving robotic object manipulations like hanging mugs on hooks, pushing
objects, or opening doors. Video: https://dannydriess.github.io/nerf-rl";Danny Driess<author:sep>Ingmar Schubert<author:sep>Pete Florence<author:sep>Yunzhu Li<author:sep>Marc Toussaint;http://arxiv.org/pdf/2206.01634v1;cs.LG;;nerf
2206.01290v2;http://arxiv.org/abs/2206.01290v2;2022-06-02;Points2NeRF: Generating Neural Radiance Fields from 3D point cloud;"Contemporary registration devices for 3D visual information, such as LIDARs
and various depth cameras, capture data as 3D point clouds. In turn, such
clouds are challenging to be processed due to their size and complexity.
Existing methods address this problem by fitting a mesh to the point cloud and
rendering it instead. This approach, however, leads to the reduced fidelity of
the resulting visualization and misses color information of the objects crucial
in computer graphics applications. In this work, we propose to mitigate this
challenge by representing 3D objects as Neural Radiance Fields (NeRFs). We
leverage a hypernetwork paradigm and train the model to take a 3D point cloud
with the associated color values and return a NeRF network's weights that
reconstruct 3D objects from input 2D images. Our method provides efficient 3D
object representation and offers several advantages over the existing
approaches, including the ability to condition NeRFs and improved
generalization beyond objects seen in training. The latter we also confirmed in
the results of our empirical evaluation.";D. Zimny<author:sep>T. TrzciÅski<author:sep>P. Spurek;http://arxiv.org/pdf/2206.01290v2;cs.CV;arXiv admin note: text overlap with arXiv:2003.08934 by other authors;nerf
2206.00878v1;http://arxiv.org/abs/2206.00878v1;2022-06-02;EfficientNeRF: Efficient Neural Radiance Fields;"Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications.";Tao Hu<author:sep>Shu Liu<author:sep>Yilun Chen<author:sep>Tiancheng Shen<author:sep>Jiaya Jia;http://arxiv.org/pdf/2206.00878v1;cs.CV;;nerf
2205.15768v1;http://arxiv.org/abs/2205.15768v1;2022-05-31;SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary  Image collections;"Inverse rendering of an object under entirely unknown capture conditions is a
fundamental challenge in computer vision and graphics. Neural approaches such
as NeRF have achieved photorealistic results on novel view synthesis, but they
require known camera poses. Solving this problem with unknown camera poses is
highly challenging as it requires joint optimization over shape, radiance, and
pose. This problem is exacerbated when the input images are captured in the
wild with varying backgrounds and illuminations. Standard pose estimation
techniques fail in such image collections in the wild due to very few estimated
correspondences across images. Furthermore, NeRF cannot relight a scene under
any illumination, as it operates on radiance (the product of reflectance and
illumination). We propose a joint optimization framework to estimate the shape,
BRDF, and per-image camera pose and illumination. Our method works on
in-the-wild online image collections of an object and produces relightable 3D
assets for several use-cases such as AR/VR. To our knowledge, our method is the
first to tackle this severely unconstrained task with minimal user interaction.
Project page: https://markboss.me/publication/2022-samurai/ Video:
https://youtu.be/LlYuGDjXp-8";Mark Boss<author:sep>Andreas Engelhardt<author:sep>Abhishek Kar<author:sep>Yuanzhen Li<author:sep>Deqing Sun<author:sep>Jonathan T. Barron<author:sep>Hendrik P. A. Lensch<author:sep>Varun Jampani;http://arxiv.org/pdf/2205.15768v1;cs.CV;;nerf
2205.15595v1;http://arxiv.org/abs/2205.15595v1;2022-05-31;Novel View Synthesis for High-fidelity Headshot Scenes;"Rendering scenes with a high-quality human face from arbitrary viewpoints is
a practical and useful technique for many real-world applications. Recently,
Neural Radiance Fields (NeRF), a rendering technique that uses neural networks
to approximate classical ray tracing, have been considered as one of the
promising approaches for synthesizing novel views from a sparse set of images.
We find that NeRF can render new views while maintaining geometric consistency,
but it does not properly maintain skin details, such as moles and pores. These
details are important particularly for faces because when we look at an image
of a face, we are much more sensitive to details than when we look at other
objects. On the other hand, 3D Morpable Models (3DMMs) based on traditional
meshes and textures can perform well in terms of skin detail despite that it
has less precise geometry and cannot cover the head and the entire scene with
background. Based on these observations, we propose a method to use both NeRF
and 3DMM to synthesize a high-fidelity novel view of a scene with a face. Our
method learns a Generative Adversarial Network (GAN) to mix a NeRF-synthesized
image and a 3DMM-rendered image and produces a photorealistic scene with a face
preserving the skin details. Experiments with various real-world scenes
demonstrate the effectiveness of our approach. The code will be available on
https://github.com/showlab/headshot .";Satoshi Tsutsui<author:sep>Weijia Mao<author:sep>Sijing Lin<author:sep>Yunyi Zhu<author:sep>Murong Ma<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2205.15595v1;cs.CV;;nerf
2205.15838v4;http://arxiv.org/abs/2205.15838v4;2022-05-31;D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from  a Monocular Video;"Given a monocular video, segmenting and decoupling dynamic objects while
recovering the static environment is a widely studied problem in machine
intelligence. Existing solutions usually approach this problem in the image
domain, limiting their performance and understanding of the environment. We
introduce Decoupled Dynamic Neural Radiance Field (D$^2$NeRF), a
self-supervised approach that takes a monocular video and learns a 3D scene
representation which decouples moving objects, including their shadows, from
the static background. Our method represents the moving objects and the static
background by two separate neural radiance fields with only one allowing for
temporal changes. A naive implementation of this approach leads to the dynamic
component taking over the static one as the representation of the former is
inherently more general and prone to overfitting. To this end, we propose a
novel loss to promote correct separation of phenomena. We further propose a
shadow field network to detect and decouple dynamically moving shadows. We
introduce a new dataset containing various dynamic objects and shadows and
demonstrate that our method can achieve better performance than
state-of-the-art approaches in decoupling dynamic and static 3D objects,
occlusion and shadow removal, and image segmentation for moving objects.";Tianhao Wu<author:sep>Fangcheng Zhong<author:sep>Andrea Tagliasacchi<author:sep>Forrester Cole<author:sep>Cengiz Oztireli;http://arxiv.org/pdf/2205.15838v4;cs.CV;;nerf
2205.15723v2;http://arxiv.org/abs/2205.15723v2;2022-05-31;DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes;"Modeling dynamic scenes is important for many applications such as virtual
reality and telepresence. Despite achieving unprecedented fidelity for novel
view synthesis in dynamic scenes, existing methods based on Neural Radiance
Fields (NeRF) suffer from slow convergence (i.e., model training time measured
in days). In this paper, we present DeVRF, a novel representation to accelerate
learning dynamic radiance fields. The core of DeVRF is to model both the 3D
canonical space and 4D deformation field of a dynamic, non-rigid scene with
explicit and discrete voxel-based representations. However, it is quite
challenging to train such a representation which has a large number of model
parameters, often resulting in overfitting issues. To overcome this challenge,
we devise a novel static-to-dynamic learning paradigm together with a new data
capture setup that is convenient to deploy in practice. This paradigm unlocks
efficient learning of deformable radiance fields via utilizing the 3D
volumetric canonical space learnt from multi-view static images to ease the
learning of 4D voxel deformation field with only few-view dynamic sequences. To
further improve the efficiency of our DeVRF and its synthesized novel view's
quality, we conduct thorough explorations and identify a set of strategies. We
evaluate DeVRF on both synthetic and real-world dynamic scenes with different
types of deformation. Experiments demonstrate that DeVRF achieves two orders of
magnitude speedup (100x faster) with on-par high-fidelity results compared to
the previous state-of-the-art approaches. The code and dataset will be released
in https://github.com/showlab/DeVRF.";Jia-Wei Liu<author:sep>Yan-Pei Cao<author:sep>Weijia Mao<author:sep>Wenqiao Zhang<author:sep>David Junhao Zhang<author:sep>Jussi Keppo<author:sep>Ying Shan<author:sep>Xiaohu Qie<author:sep>Mike Zheng Shou;http://arxiv.org/pdf/2205.15723v2;cs.CV;Project page: https://jia-wei-liu.github.io/DeVRF/;nerf
2205.15585v2;http://arxiv.org/abs/2205.15585v2;2022-05-31;Decomposing NeRF for Editing via Feature Field Distillation;"Emerging neural radiance fields (NeRF) are a promising scene representation
for computer graphics, enabling high-quality 3D reconstruction and novel view
synthesis from image observations. However, editing a scene represented by a
NeRF is challenging, as the underlying connectionist representations such as
MLPs or voxel grids are not object-centric or compositional. In particular, it
has been difficult to selectively edit specific regions or objects. In this
work, we tackle the problem of semantic scene decomposition of NeRFs to enable
query-based local editing of the represented 3D scenes. We propose to distill
the knowledge of off-the-shelf, self-supervised 2D image feature extractors
such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the
radiance field. Given a user-specified query of various modalities such as
text, an image patch, or a point-and-click selection, 3D feature fields
semantically decompose 3D space without the need for re-training and enable us
to semantically select and edit regions in the radiance field. Our experiments
validate that the distilled feature fields (DFFs) can transfer recent progress
in 2D vision and language foundation models to 3D scene representations,
enabling convincing 3D segmentation and selective editing of emerging neural
graphics representations.";Sosuke Kobayashi<author:sep>Eiichi Matsumoto<author:sep>Vincent Sitzmann;http://arxiv.org/pdf/2205.15585v2;cs.CV;"Accepted to NeurIPS 2022
  https://pfnet-research.github.io/distilled-feature-fields/";nerf
2205.14870v2;http://arxiv.org/abs/2205.14870v2;2022-05-30;Compressible-composable NeRF via Rank-residual Decomposition;"Neural Radiance Field (NeRF) has emerged as a compelling method to represent
3D objects and scenes for photo-realistic rendering. However, its implicit
representation causes difficulty in manipulating the models like the explicit
mesh representation. Several recent advances in NeRF manipulation are usually
restricted by a shared renderer network, or suffer from large model size. To
circumvent the hurdle, in this paper, we present an explicit neural field
representation that enables efficient and convenient manipulation of models. To
achieve this goal, we learn a hybrid tensor rank decomposition of the scene
without neural networks. Motivated by the low-rank approximation property of
the SVD algorithm, we propose a rank-residual learning strategy to encourage
the preservation of primary information in lower ranks. The model size can then
be dynamically adjusted by rank truncation to control the levels of detail,
achieving near-optimal compression without extra optimization. Furthermore,
different models can be arbitrarily transformed and composed into one scene by
concatenating along the rank dimension. The growth of storage cost can also be
mitigated by compressing the unimportant objects in the composed scene. We
demonstrate that our method is able to achieve comparable rendering quality to
state-of-the-art methods, while enabling extra capability of compression and
composition. Code will be made available at https://github.com/ashawkey/CCNeRF.";Jiaxiang Tang<author:sep>Xiaokang Chen<author:sep>Jingbo Wang<author:sep>Gang Zeng;http://arxiv.org/pdf/2205.14870v2;cs.CV;NeurIPS 2022 camera-ready version;nerf
2205.14929v1;http://arxiv.org/abs/2205.14929v1;2022-05-30;Neural Volumetric Object Selection;"We introduce an approach for selecting objects in neural volumetric 3D
representations, such as multi-plane images (MPI) and neural radiance fields
(NeRF). Our approach takes a set of foreground and background 2D user scribbles
in one view and automatically estimates a 3D segmentation of the desired
object, which can be rendered into novel views. To achieve this result, we
propose a novel voxel feature embedding that incorporates the neural volumetric
3D representation and multi-view image features from all input views. To
evaluate our approach, we introduce a new dataset of human-provided
segmentation masks for depicted objects in real-world multi-view scene
captures. We show that our approach out-performs strong baselines, including 2D
segmentation and 3D segmentation approaches adapted to our task.";Zhongzheng Ren<author:sep>Aseem Agarwala<author:sep>Bryan Russell<author:sep>Alexander G. Schwing<author:sep>Oliver Wang;http://arxiv.org/pdf/2205.14929v1;cs.CV;CVPR 2022 camera ready;nerf
2205.15285v2;http://arxiv.org/abs/2205.15285v2;2022-05-30;Fast Dynamic Radiance Fields with Time-Aware Neural Voxels;"Neural radiance fields (NeRF) have shown great success in modeling 3D scenes
and synthesizing novel-view images. However, most previous NeRF methods take
much time to optimize one single scene. Explicit data structures, e.g. voxel
features, show great potential to accelerate the training process. However,
voxel features face two big challenges to be applied to dynamic scenes, i.e.
modeling temporal information and capturing different scales of point motions.
We propose a radiance field framework by representing scenes with time-aware
voxel features, named as TiNeuVox. A tiny coordinate deformation network is
introduced to model coarse motion trajectories and temporal information is
further enhanced in the radiance network. A multi-distance interpolation method
is proposed and applied on voxel features to model both small and large
motions. Our framework significantly accelerates the optimization of dynamic
radiance fields while maintaining high rendering quality. Empirical evaluation
is performed on both synthetic and real scenes. Our TiNeuVox completes training
with only 8 minutes and 8-MB storage cost while showing similar or even better
rendering performance than previous dynamic NeRF methods.";Jiemin Fang<author:sep>Taoran Yi<author:sep>Xinggang Wang<author:sep>Lingxi Xie<author:sep>Xiaopeng Zhang<author:sep>Wenyu Liu<author:sep>Matthias NieÃner<author:sep>Qi Tian;http://arxiv.org/pdf/2205.15285v2;cs.CV;SIGGRAPH Asia 2022. Project page: https://jaminfong.cn/tineuvox;nerf
2205.14332v2;http://arxiv.org/abs/2205.14332v2;2022-05-28;V4D: Voxel for 4D Novel View Synthesis;"Neural radiance fields have made a remarkable breakthrough in the novel view
synthesis task at the 3D static scene. However, for the 4D circumstance (e.g.,
dynamic scene), the performance of the existing method is still limited by the
capacity of the neural network, typically in a multilayer perceptron network
(MLP). In this paper, we utilize 3D Voxel to model the 4D neural radiance
field, short as V4D, where the 3D voxel has two formats. The first one is to
regularly model the 3D space and then use the sampled local 3D feature with the
time index to model the density field and the texture field by a tiny MLP. The
second one is in look-up tables (LUTs) format that is for the pixel-level
refinement, where the pseudo-surface produced by the volume rendering is
utilized as the guidance information to learn a 2D pixel-level refinement
mapping. The proposed LUTs-based refinement module achieves the performance
gain with little computational cost and could serve as the plug-and-play module
in the novel view synthesis task. Moreover, we propose a more effective
conditional positional encoding toward the 4D data that achieves performance
gain with negligible computational burdens. Extensive experiments demonstrate
that the proposed method achieves state-of-the-art performance at a low
computational cost.";Wanshui Gan<author:sep>Hongbin Xu<author:sep>Yi Huang<author:sep>Shifeng Chen<author:sep>Naoto Yokoya;http://arxiv.org/pdf/2205.14332v2;cs.CV;;
2205.14330v4;http://arxiv.org/abs/2205.14330v4;2022-05-28;Differentiable Point-Based Radiance Fields for Efficient View Synthesis;"We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.";Qiang Zhang<author:sep>Seung-Hwan Baek<author:sep>Szymon Rusinkiewicz<author:sep>Felix Heide;http://arxiv.org/pdf/2205.14330v4;cs.CV;;nerf
2205.13524v3;http://arxiv.org/abs/2205.13524v3;2022-05-26;PREF: Phasorial Embedding Fields for Compact Neural Representations;"We present an efficient frequency-based neural representation termed PREF: a
shallow MLP augmented with a phasor volume that covers significant border
spectra than previous Fourier feature mapping or Positional Encoding. At the
core is our compact 3D phasor volume where frequencies distribute uniformly
along a 2D plane and dilate along a 1D axis. To this end, we develop a tailored
and efficient Fourier transform that combines both Fast Fourier transform and
local interpolation to accelerate na\""ive Fourier mapping. We also introduce a
Parsvel regularizer that stables frequency-based learning. In these ways, Our
PREF reduces the costly MLP in the frequency-based representation, thereby
significantly closing the efficiency gap between it and other hybrid
representations, and improving its interpretability. Comprehensive experiments
demonstrate that our PREF is able to capture high-frequency details while
remaining compact and robust, including 2D image generalization, 3D signed
distance function regression and 5D neural radiance field reconstruction.";Binbin Huang<author:sep>Xinhao Yan<author:sep>Anpei Chen<author:sep>Shenghua Gao<author:sep>Jingyi Yu;http://arxiv.org/pdf/2205.13524v3;cs.CV;;
2205.12183v2;http://arxiv.org/abs/2205.12183v2;2022-05-24;StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D  Mutual Learning;"3D scene stylization aims at generating stylized images of the scene from
arbitrary novel views following a given set of style examples, while ensuring
consistency when rendered from different views. Directly applying methods for
image or video stylization to 3D scenes cannot achieve such consistency. Thanks
to recently proposed neural radiance fields (NeRF), we are able to represent a
3D scene in a consistent way. Consistent 3D scene stylization can be
effectively achieved by stylizing the corresponding NeRF. However, there is a
significant domain gap between style examples which are 2D images and NeRF
which is an implicit volumetric representation. To address this problem, we
propose a novel mutual learning framework for 3D scene stylization that
combines a 2D image stylization network and NeRF to fuse the stylization
ability of 2D stylization network with the 3D consistency of NeRF. We first
pre-train a standard NeRF of the 3D scene to be stylized and replace its color
prediction module with a style network to obtain a stylized NeRF. It is
followed by distilling the prior knowledge of spatial consistency from NeRF to
the 2D stylization network through an introduced consistency loss. We also
introduce a mimic loss to supervise the mutual learning of the NeRF style
module and fine-tune the 2D stylization decoder. In order to further make our
model handle ambiguities of 2D stylization results, we introduce learnable
latent codes that obey the probability distributions conditioned on the style.
They are attached to training samples as conditional inputs to better learn the
style module in our novel stylized NeRF. Experimental results demonstrate that
our method is superior to existing approaches in both visual quality and
long-range consistency.";Yi-Hua Huang<author:sep>Yue He<author:sep>Yu-Jie Yuan<author:sep>Yu-Kun Lai<author:sep>Lin Gao;http://arxiv.org/pdf/2205.12183v2;cs.GR;Accepted by CVPR 2022;nerf
2205.09351v3;http://arxiv.org/abs/2205.09351v3;2022-05-19;Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields;"Neural scene representations, such as Neural Radiance Fields (NeRF), are
based on training a multilayer perceptron (MLP) using a set of color images
with known poses. An increasing number of devices now produce RGB-D(color +
depth) information, which has been shown to be very important for a wide range
of tasks. Therefore, the aim of this paper is to investigate what improvements
can be made to these promising implicit representations by incorporating depth
information with the color images. In particular, the recently proposed
Mip-NeRF approach, which uses conical frustums instead of rays for volume
rendering, allows one to account for the varying area of a pixel with distance
from the camera center. The proposed method additionally models depth
uncertainty. This allows to address major limitations of NeRF-based approaches
including improving the accuracy of geometry, reduced artifacts, faster
training time, and shortened prediction time. Experiments are performed on
well-known benchmark scenes, and comparisons show improved accuracy in scene
geometry and photometric reconstruction, while reducing the training time by 3
- 5 times.";Arnab Dey<author:sep>Yassine Ahmine<author:sep>Andrew I. Comport;http://arxiv.org/pdf/2205.09351v3;cs.CV;;nerf
2205.08978v2;http://arxiv.org/abs/2205.08978v2;2022-05-18;Fast Neural Network based Solving of Partial Differential Equations;"We present a novel method for using Neural Networks (NNs) for finding
solutions to a class of Partial Differential Equations (PDEs). Our method
builds on recent advances in Neural Radiance Field research (NeRFs) and allows
for a NN to converge to a PDE solution much faster than classic Physically
Informed Neural Network (PINNs) approaches.";Jaroslaw Rzepecki<author:sep>Daniel Bates<author:sep>Chris Doran;http://arxiv.org/pdf/2205.08978v2;cs.LG;;nerf
2205.07058v2;http://arxiv.org/abs/2205.07058v2;2022-05-14;RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis;"We present a large-scale synthetic dataset for novel view synthesis
consisting of ~300k images rendered from nearly 2000 complex scenes using
high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset
is orders of magnitude larger than existing synthetic datasets for novel view
synthesis, thus providing a large unified benchmark for both training and
evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of
our dataset exhibit challenging variations in camera views, lighting, shape,
materials, and textures. Because our dataset is too large for existing methods
to process, we propose Sparse Voxel Light Field (SVLF), an efficient
voxel-based light field approach for novel view synthesis that achieves
comparable performance to NeRF on synthetic data, while being an order of
magnitude faster to train and two orders of magnitude faster to render. SVLF
achieves this speed by relying on a sparse voxel octree, careful voxel sampling
(requiring only a handful of queries per ray), and reduced network structure;
as well as ground truth depth maps at training time. Our dataset is generated
by NViSII, a Python-based ray tracing renderer, which is designed to be simple
for non-experts to use and share, flexible and powerful through its use of
scripting, and able to create high-quality and physically-based rendered
images. Experiments with a subset of our dataset allow us to compare standard
methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for
category-level modeling, pointing toward the need for future improvements in
this area.";Jonathan Tremblay<author:sep>Moustafa Meshry<author:sep>Alex Evans<author:sep>Jan Kautz<author:sep>Alexander Keller<author:sep>Sameh Khamis<author:sep>Thomas MÃ¼ller<author:sep>Charles Loop<author:sep>Nathan Morrical<author:sep>Koki Nagano<author:sep>Towaki Takikawa<author:sep>Stan Birchfield;http://arxiv.org/pdf/2205.07058v2;cs.CV;"ECCV 2022 Workshop on Learning to Generate 3D Shapes and Scenes.
  Project page at http://www.cs.umd.edu/~mmeshry/projects/rtmv";nerf
2205.05922v1;http://arxiv.org/abs/2205.05922v1;2022-05-12;Ray Priors through Reprojection: Improving Neural Radiance Fields for  Novel View Extrapolation;"Neural Radiance Fields (NeRF) have emerged as a potent paradigm for
representing scenes and synthesizing photo-realistic images. A main limitation
of conventional NeRFs is that they often fail to produce high-quality
renderings under novel viewpoints that are significantly different from the
training viewpoints. In this paper, instead of exploiting few-shot image
synthesis, we study the novel view extrapolation setting that (1) the training
images can well describe an object, and (2) there is a notable discrepancy
between the training and test viewpoints' distributions. We present RapNeRF
(RAy Priors) as a solution. Our insight is that the inherent appearances of a
3D surface's arbitrary visible projections should be consistent. We thus
propose a random ray casting policy that allows training unseen views using
seen views. Furthermore, we show that a ray atlas pre-computed from the
observed rays' viewing directions could further enhance the rendering quality
for extrapolated views. A main limitation is that RapNeRF would remove the
strong view-dependent effects because it leverages the multi-view consistency
property.";Jian Zhang<author:sep>Yuanqing Zhang<author:sep>Huan Fu<author:sep>Xiaowei Zhou<author:sep>Bowen Cai<author:sep>Jinchi Huang<author:sep>Rongfei Jia<author:sep>Binqiang Zhao<author:sep>Xing Tang;http://arxiv.org/pdf/2205.05922v1;cs.CV;;nerf
2205.05869v2;http://arxiv.org/abs/2205.05869v2;2022-05-12;View Synthesis with Sculpted Neural Points;"We address the task of view synthesis, generating novel views of a scene
given a set of images as input. In many recent works such as NeRF (Mildenhall
et al., 2020), the scene geometry is parameterized using neural implicit
representations (i.e., MLPs). Implicit neural representations have achieved
impressive visual quality but have drawbacks in computational efficiency. In
this work, we propose a new approach that performs view synthesis using point
clouds. It is the first point-based method that achieves better visual quality
than NeRF while being 100x faster in rendering speed. Our approach builds on
existing works on differentiable point-based rendering but introduces a novel
technique we call ""Sculpted Neural Points (SNP)"", which significantly improves
the robustness to errors and holes in the reconstructed point cloud. We further
propose to use view-dependent point features based on spherical harmonics to
capture non-Lambertian surfaces, and new designs in the point-based rendering
pipeline that further boost the performance. Finally, we show that our system
supports fine-grained scene editing. Code is available at
https://github.com/princeton-vl/SNP.";Yiming Zuo<author:sep>Jia Deng;http://arxiv.org/pdf/2205.05869v2;cs.CV;;nerf
2205.04978v1;http://arxiv.org/abs/2205.04978v1;2022-05-10;NeRF-Editing: Geometry Editing of Neural Radiance Fields;"Implicit neural rendering, especially Neural Radiance Field (NeRF), has shown
great potential in novel view synthesis of a scene. However, current NeRF-based
methods cannot enable users to perform user-controlled shape deformation in the
scene. While existing works have proposed some approaches to modify the
radiance field according to the user's constraints, the modification is limited
to color editing or object translation and rotation. In this paper, we propose
a method that allows users to perform controllable shape deformation on the
implicit representation of the scene, and synthesizes the novel view images of
the edited scene without re-training the network. Specifically, we establish a
correspondence between the extracted explicit mesh representation and the
implicit neural representation of the target scene. Users can first utilize
well-developed mesh-based deformation methods to deform the mesh representation
of the scene. Our method then utilizes user edits from the mesh representation
to bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining
the rendering results of the edited scene. Extensive experiments demonstrate
that our framework can achieve ideal editing results not only on synthetic
data, but also on real scenes captured by users.";Yu-Jie Yuan<author:sep>Yang-Tian Sun<author:sep>Yu-Kun Lai<author:sep>Yuewen Ma<author:sep>Rongfei Jia<author:sep>Lin Gao;http://arxiv.org/pdf/2205.04978v1;cs.GR;Accepted by CVPR 2022;nerf
2205.01389v1;http://arxiv.org/abs/2205.01389v1;2022-05-03;Sampling-free obstacle gradients and reactive planning in Neural  Radiance Fields (NeRF);"This work investigates the use of Neural implicit representations,
specifically Neural Radiance Fields (NeRF), for geometrical queries and motion
planning. We show that by adding the capacity to infer occupancy in a radius to
a pre-trained NeRF, we are effectively learning an approximation to a Euclidean
Signed Distance Field (ESDF). Using backward differentiation of the augmented
network, we obtain an obstacle gradient that is integrated into an obstacle
avoidance policy based on the Riemannian Motion Policies (RMP) framework. Thus,
our findings allow for very fast sampling-free obstacle avoidance planning in
the implicit representation.";Michael Pantic<author:sep>Cesar Cadena<author:sep>Roland Siegwart<author:sep>Lionel Ott;http://arxiv.org/pdf/2205.01389v1;cs.RO;"Accepted to the ""Motion Planning with Implicit Neural Representations
  of Geometry"" Workshop at ICRA 2022";nerf
2204.13426v1;http://arxiv.org/abs/2204.13426v1;2022-04-28;AE-NeRF: Auto-Encoding Neural Radiance Fields for 3D-Aware Object  Manipulation;"We propose a novel framework for 3D-aware object manipulation, called
Auto-Encoding Neural Radiance Fields (AE-NeRF). Our model, which is formulated
in an auto-encoder architecture, extracts disentangled 3D attributes such as 3D
shape, appearance, and camera pose from an image, and a high-quality image is
rendered from the attributes through disentangled generative Neural Radiance
Fields (NeRF). To improve the disentanglement ability, we present two losses,
global-local attribute consistency loss defined between input and output, and
swapped-attribute classification loss. Since training such auto-encoding
networks from scratch without ground-truth shape and appearance information is
non-trivial, we present a stage-wise training scheme, which dramatically helps
to boost the performance. We conduct experiments to demonstrate the
effectiveness of the proposed model over the latest methods and provide
extensive ablation studies.";Mira Kim<author:sep>Jaehoon Ko<author:sep>Kyusun Cho<author:sep>Junmyeong Choi<author:sep>Daewon Choi<author:sep>Seungryong Kim;http://arxiv.org/pdf/2204.13426v1;cs.CV;;nerf
2204.13696v1;http://arxiv.org/abs/2204.13696v1;2022-04-28;NeurMiPs: Neural Mixture of Planar Experts for View Synthesis;"We present Neural Mixtures of Planar Experts (NeurMiPs), a novel planar-based
scene representation for modeling geometry and appearance. NeurMiPs leverages a
collection of local planar experts in 3D space as the scene representation.
Each planar expert consists of the parameters of the local rectangular shape
representing geometry and a neural radiance field modeling the color and
opacity. We render novel views by calculating ray-plane intersections and
composite output colors and densities at intersected points to the image.
NeurMiPs blends the efficiency of explicit mesh rendering and flexibility of
the neural radiance field. Experiments demonstrate superior performance and
speed of our proposed method, compared to other 3D representations in novel
view synthesis.";Zhi-Hao Lin<author:sep>Wei-Chiu Ma<author:sep>Hao-Yu Hsu<author:sep>Yu-Chiang Frank Wang<author:sep>Shenlong Wang;http://arxiv.org/pdf/2204.13696v1;cs.CV;CVPR 2022. Project page: https://zhihao-lin.github.io/neurmips/;
2204.11798v1;http://arxiv.org/abs/2204.11798v1;2022-04-25;Generalizable Neural Performer: Learning Robust Radiance Fields for  Human Novel View Synthesis;"This work targets at using a general deep learning framework to synthesize
free-viewpoint images of arbitrary human performers, only requiring a sparse
number of camera views as inputs and skirting per-case fine-tuning. The large
variation of geometry and appearance, caused by articulated body poses, shapes
and clothing types, are the key bottlenecks of this task. To overcome these
challenges, we present a simple yet powerful framework, named Generalizable
Neural Performer (GNR), that learns a generalizable and robust neural body
representation over various geometry and appearance. Specifically, we compress
the light fields for novel view human rendering as conditional implicit neural
radiance fields from both geometry and appearance aspects. We first introduce
an Implicit Geometric Body Embedding strategy to enhance the robustness based
on both parametric 3D human body model and multi-view images hints. We further
propose a Screen-Space Occlusion-Aware Appearance Blending technique to
preserve the high-quality appearance, through interpolating source view
appearance to the radiance fields with a relax but approximate geometric
guidance.
  To evaluate our method, we present our ongoing effort of constructing a
dataset with remarkable complexity and diversity. The dataset GeneBody-1.0,
includes over 360M frames of 370 subjects under multi-view cameras capturing,
performing a large variety of pose actions, along with diverse body shapes,
clothing, accessories and hairdos. Experiments on GeneBody-1.0 and ZJU-Mocap
show better robustness of our methods than recent state-of-the-art
generalizable methods among all cross-dataset, unseen subjects and unseen poses
settings. We also demonstrate the competitiveness of our model compared with
cutting-edge case-specific ones. Dataset, code and model will be made publicly
available.";Wei Cheng<author:sep>Su Xu<author:sep>Jingtan Piao<author:sep>Chen Qian<author:sep>Wayne Wu<author:sep>Kwan-Yee Lin<author:sep>Hongsheng Li;http://arxiv.org/pdf/2204.11798v1;cs.CV;"Project Page: https://generalizable-neural-performer.github.io/
  Dataset: https://generalizable-neural-performer.github.io/genebody.html/";
2204.10516v2;http://arxiv.org/abs/2204.10516v2;2022-04-22;Implicit Object Mapping With Noisy Data;"Modelling individual objects in a scene as Neural Radiance Fields (NeRFs)
provides an alternative geometric scene representation that may benefit
downstream robotics tasks such as scene understanding and object manipulation.
However, we identify three challenges to using real-world training data
collected by a robot to train a NeRF: (i) The camera trajectories are
constrained, and full visual coverage is not guaranteed - especially when
obstructions to the objects of interest are present; (ii) the poses associated
with the images are noisy due to odometry or localization noise; (iii) the
objects are not easily isolated from the background. This paper evaluates the
extent to which above factors degrade the quality of the learnt implicit object
representation. We introduce a pipeline that decomposes a scene into multiple
individual object-NeRFs, using noisy object instance masks and bounding boxes,
and evaluate the sensitivity of this pipeline with respect to noisy poses,
instance masks, and the number of training images. We uncover that the
sensitivity to noisy instance masks can be partially alleviated with depth
supervision and quantify the importance of including the camera extrinsics in
the NeRF optimisation process.";Jad Abou-Chakra<author:sep>Feras Dayoub<author:sep>Niko SÃ¼nderhauf;http://arxiv.org/pdf/2204.10516v2;cs.RO;;nerf
2204.10850v1;http://arxiv.org/abs/2204.10850v1;2022-04-22;Control-NeRF: Editable Feature Volumes for Scene Rendering and  Manipulation;"We present a novel method for performing flexible, 3D-aware image content
manipulation while enabling high-quality novel view synthesis. While NeRF-based
approaches are effective for novel view synthesis, such models memorize the
radiance for every point in a scene within a neural network. Since these models
are scene-specific and lack a 3D scene representation, classical editing such
as shape manipulation, or combining scenes is not possible. Hence, editing and
combining NeRF-based scenes has not been demonstrated. With the aim of
obtaining interpretable and controllable scene representations, our model
couples learnt scene-specific feature volumes with a scene agnostic neural
rendering network. With this hybrid representation, we decouple neural
rendering from scene-specific geometry and appearance. We can generalize to
novel scenes by optimizing only the scene-specific 3D feature representation,
while keeping the parameters of the rendering network fixed. The rendering
function learnt during the initial training stage can thus be easily applied to
new scenes, making our approach more flexible. More importantly, since the
feature volumes are independent of the rendering model, we can manipulate and
combine scenes by editing their corresponding feature volumes. The edited
volume can then be plugged into the rendering model to synthesize high-quality
novel views. We demonstrate various scene manipulations, including mixing
scenes, deforming objects and inserting objects into scenes, while still
producing photo-realistic results.";Verica Lazova<author:sep>Vladimir Guzov<author:sep>Kyle Olszewski<author:sep>Sergey Tulyakov<author:sep>Gerard Pons-Moll;http://arxiv.org/pdf/2204.10850v1;cs.CV;;nerf
2204.09523v1;http://arxiv.org/abs/2204.09523v1;2022-04-20;SILVR: A Synthetic Immersive Large-Volume Plenoptic Dataset;"In six-degrees-of-freedom light-field (LF) experiences, the viewer's freedom
is limited by the extent to which the plenoptic function was sampled. Existing
LF datasets represent only small portions of the plenoptic function, such that
they either cover a small volume, or they have limited field of view.
Therefore, we propose a new LF image dataset ""SILVR"" that allows for
six-degrees-of-freedom navigation in much larger volumes while maintaining full
panoramic field of view. We rendered three different virtual scenes in various
configurations, where the number of views ranges from 642 to 2226. One of these
scenes (called Zen Garden) is a novel scene, and is made publicly available. We
chose to position the virtual cameras closely together in large cuboid and
spherical organisations ($2.2m^3$ to $48m^3$), equipped with 180{\deg} fish-eye
lenses. Every view is rendered to a color image and depth map of 2048px
$\times$ 2048px. Additionally, we present the software used to automate the
multi-view rendering process, as well as a lens-reprojection tool that converts
between images with panoramic or fish-eye projection to a standard rectilinear
(i.e., perspective) projection. Finally, we demonstrate how the proposed
dataset and software can be used to evaluate LF coding/rendering techniques(in
this case for training NeRFs with instant-ngp). As such, we provide the first
publicly-available LF dataset for large volumes of light with full panoramic
field of view";Martijn Courteaux<author:sep>Julie Artois<author:sep>Stijn De Pauw<author:sep>Peter Lambert<author:sep>Glenn Van Wallendael;http://arxiv.org/pdf/2204.09523v1;cs.GR;"In 13th ACM Multimedia Systems Conference (MMSys '22), June 14-17,
  2022, Athlone, Ireland. ACM, New York, NY, USA, 6 pages";nerf
2204.06837v1;http://arxiv.org/abs/2204.06837v1;2022-04-14;Modeling Indirect Illumination for Inverse Rendering;"Recent advances in implicit neural representations and differentiable
rendering make it possible to simultaneously recover the geometry and materials
of an object from multi-view RGB images captured under unknown static
illumination. Despite the promising results achieved, indirect illumination is
rarely modeled in previous methods, as it requires expensive recursive path
tracing which makes the inverse rendering computationally intractable. In this
paper, we propose a novel approach to efficiently recovering spatially-varying
indirect illumination. The key insight is that indirect illumination can be
conveniently derived from the neural radiance field learned from input images
instead of being estimated jointly with direct illumination and materials. By
properly modeling the indirect illumination and visibility of direct
illumination, interreflection- and shadow-free albedo can be recovered. The
experiments on both synthetic and real data demonstrate the superior
performance of our approach compared to previous work and its capability to
synthesize realistic renderings under novel viewpoints and illumination. Our
code and data are available at https://zju3dv.github.io/invrender/.";Yuanqing Zhang<author:sep>Jiaming Sun<author:sep>Xingyi He<author:sep>Huan Fu<author:sep>Rongfei Jia<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2204.06837v1;cs.CV;;
2204.05735v1;http://arxiv.org/abs/2204.05735v1;2022-04-12;GARF: Gaussian Activated Radiance Fields for High Fidelity  Reconstruction and Pose Estimation;"Despite Neural Radiance Fields (NeRF) showing compelling results in
photorealistic novel views synthesis of real-world scenes, most existing
approaches require accurate prior camera poses. Although approaches for jointly
recovering the radiance field and camera pose exist (BARF), they rely on a
cumbersome coarse-to-fine auxiliary positional embedding to ensure good
performance. We present Gaussian Activated neural Radiance Fields (GARF), a new
positional embedding-free neural radiance field architecture - employing
Gaussian activations - that outperforms the current state-of-the-art in terms
of high fidelity reconstruction and pose estimation.";Shin-Fang Chng<author:sep>Sameera Ramasinghe<author:sep>Jamie Sherrah<author:sep>Simon Lucey;http://arxiv.org/pdf/2204.05735v1;cs.CV;Project page: https://sfchng.github.io/garf/;nerf
2204.04668v2;http://arxiv.org/abs/2204.04668v2;2022-04-10;NAN: Noise-Aware NeRFs for Burst-Denoising;"Burst denoising is now more relevant than ever, as computational photography
helps overcome sensitivity issues inherent in mobile phones and small cameras.
A major challenge in burst-denoising is in coping with pixel misalignment,
which was so far handled with rather simplistic assumptions of simple motion,
or the ability to align in pre-processing. Such assumptions are not realistic
in the presence of large motion and high levels of noise. We show that Neural
Radiance Fields (NeRFs), originally suggested for physics-based novel-view
rendering, can serve as a powerful framework for burst denoising. NeRFs have an
inherent capability of handling noise as they integrate information from
multiple images, but they are limited in doing so, mainly since they build on
pixel-wise operations which are suitable to ideal imaging conditions. Our
approach, termed NAN, leverages inter-view and spatial information in NeRFs to
better deal with noise. It achieves state-of-the-art results in burst denoising
and is especially successful in coping with large movement and occlusions,
under very high levels of noise. With the rapid advances in accelerating NeRFs,
it could provide a powerful platform for denoising in challenging environments.";Naama Pearl<author:sep>Tali Treibitz<author:sep>Simon Korman;http://arxiv.org/pdf/2204.04668v2;cs.CV;to appear at CVPR 2022;nerf
2204.03715v1;http://arxiv.org/abs/2204.03715v1;2022-04-07;Gravitationally Lensed Black Hole Emission Tomography;"Measurements from the Event Horizon Telescope enabled the visualization of
light emission around a black hole for the first time. So far, these
measurements have been used to recover a 2D image under the assumption that the
emission field is static over the period of acquisition. In this work, we
propose BH-NeRF, a novel tomography approach that leverages gravitational
lensing to recover the continuous 3D emission field near a black hole. Compared
to other 3D reconstruction or tomography settings, this task poses two
significant challenges: first, rays near black holes follow curved paths
dictated by general relativity, and second, we only observe measurements from a
single viewpoint. Our method captures the unknown emission field using a
continuous volumetric function parameterized by a coordinate-based neural
network, and uses knowledge of Keplerian orbital dynamics to establish
correspondence between 3D points over time. Together, these enable BH-NeRF to
recover accurate 3D emission fields, even in challenging situations with sparse
measurements and uncertain orbital dynamics. This work takes the first steps in
showing how future measurements from the Event Horizon Telescope could be used
to recover evolving 3D emission around the supermassive black hole in our
Galactic center.";Aviad Levis<author:sep>Pratul P. Srinivasan<author:sep>Andrew A. Chael<author:sep>Ren Ng<author:sep>Katherine L. Bouman;http://arxiv.org/pdf/2204.03715v1;cs.CV;"To appear in the IEEE Proceedings of the Conference on Computer
  Vision and Pattern Recognition (CVPR), 2022. Supplemental material including
  accompanying pdf, code, and video highlight can be found in the project page:
  http://imaging.cms.caltech.edu/bhnerf/";nerf
2204.02585v3;http://arxiv.org/abs/2204.02585v3;2022-04-06;SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference;"Neural Radiance Fields (NeRF) has emerged as the state-of-the-art method for
novel view generation of complex scenes, but is very slow during inference.
Recently, there have been multiple works on speeding up NeRF inference, but the
state of the art methods for real-time NeRF inference rely on caching the
neural network output, which occupies several giga-bytes of disk space that
limits their real-world applicability. As caching the neural network of
original NeRF network is not feasible, Garbin et al. proposed ""FastNeRF"" which
factorizes the problem into 2 sub-networks - one which depends only on the 3D
coordinate of a sample point and one which depends only on the 2D camera
viewing direction. Although this factorization enables them to reduce the cache
size and perform inference at over 200 frames per second, the memory overhead
is still substantial. In this work, we propose SqueezeNeRF, which is more than
60 times memory-efficient than the sparse cache of FastNeRF and is still able
to render at more than 190 frames per second on a high spec GPU during
inference.";Krishna Wadhwani<author:sep>Tamaki Kojima;http://arxiv.org/pdf/2204.02585v3;cs.CV;"9 pages, 3 figures, 5 tables. Presented in the ""5th Efficient Deep
  Learning for Computer Vision"" CVPR 2022 Workshop""";nerf
2204.01943v3;http://arxiv.org/abs/2204.01943v3;2022-04-05;Unified Implicit Neural Stylization;"Representing visual signals by implicit representation (e.g., a coordinate
based deep network) has prevailed among many vision tasks. This work explores a
new intriguing direction: training a stylized implicit representation, using a
generalized approach that can apply to various 2D and 3D scenarios. We conduct
a pilot study on a variety of implicit functions, including 2D coordinate-based
representation, neural radiance field, and signed distance function. Our
solution is a Unified Implicit Neural Stylization framework, dubbed INS. In
contrary to vanilla implicit representation, INS decouples the ordinary
implicit function into a style implicit module and a content implicit module,
in order to separately encode the representations from the style image and
input scenes. An amalgamation module is then applied to aggregate these
information and synthesize the stylized output. To regularize the geometry in
3D scenes, we propose a novel self-distillation geometry consistency loss which
preserves the geometry fidelity of the stylized scenes. Comprehensive
experiments are conducted on multiple task settings, including novel view
synthesis of complex scenes, stylization for implicit surfaces, and fitting
images using MLPs. We further demonstrate that the learned representation is
continuous not only spatially but also style-wise, leading to effortlessly
interpolating between different styles and generating images with new mixed
styles. Please refer to the video on our project page for more view synthesis
results: https://zhiwenfan.github.io/INS.";Zhiwen Fan<author:sep>Yifan Jiang<author:sep>Peihao Wang<author:sep>Xinyu Gong<author:sep>Dejia Xu<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2204.01943v3;cs.CV;;
2204.01218v2;http://arxiv.org/abs/2204.01218v2;2022-04-04;Neural Rendering of Humans in Novel View and Pose from Monocular Video;"We introduce a new method that generates photo-realistic humans under novel
views and poses given a monocular video as input. Despite the significant
progress recently on this topic, with several methods exploring shared
canonical neural radiance fields in dynamic scene scenarios, learning a
user-controlled model for unseen poses remains a challenging task. To tackle
this problem, we introduce an effective method to a) integrate observations
across several frames and b) encode the appearance at each individual frame. We
accomplish this by utilizing both the human pose that models the body shape as
well as point clouds that partially cover the human as input. Our approach
simultaneously learns a shared set of latent codes anchored to the human pose
among several frames, and an appearance-dependent code anchored to incomplete
point clouds generated by each frame and its predicted depth. The former human
pose-based code models the shape of the performer whereas the latter point
cloud-based code predicts fine-level details and reasons about missing
structures at the unseen poses. To further recover non-visible regions in query
frames, we employ a temporal transformer to integrate features of points in
query frames and tracked body points from automatically-selected key frames.
Experiments on various sequences of dynamic humans from different datasets
including ZJU-MoCap show that our method significantly outperforms existing
approaches under unseen poses and novel views given monocular videos as input.";Tiantian Wang<author:sep>Nikolaos Sarafianos<author:sep>Ming-Hsuan Yang<author:sep>Tony Tung;http://arxiv.org/pdf/2204.01218v2;cs.CV;10 pages;
2204.00928v2;http://arxiv.org/abs/2204.00928v2;2022-04-02;SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single  Image;"Despite the rapid development of Neural Radiance Field (NeRF), the necessity
of dense covers largely prohibits its wider applications. While several recent
works have attempted to address this issue, they either operate with sparse
views (yet still, a few of them) or on simple objects/scenes. In this work, we
consider a more ambitious task: training neural radiance field, over
realistically complex visual scenes, by ""looking only once"", i.e., using only a
single view. To attain this goal, we present a Single View NeRF (SinNeRF)
framework consisting of thoughtfully designed semantic and geometry
regularizations. Specifically, SinNeRF constructs a semi-supervised learning
process, where we introduce and propagate geometry pseudo labels and semantic
pseudo labels to guide the progressive training process. Extensive experiments
are conducted on complex scene benchmarks, including NeRF synthetic dataset,
Local Light Field Fusion dataset, and DTU dataset. We show that even without
pre-training on multi-view datasets, SinNeRF can yield photo-realistic
novel-view synthesis results. Under the single image setting, SinNeRF
significantly outperforms the current state-of-the-art NeRF baselines in all
cases. Project page: https://vita-group.github.io/SinNeRF/";Dejia Xu<author:sep>Yifan Jiang<author:sep>Peihao Wang<author:sep>Zhiwen Fan<author:sep>Humphrey Shi<author:sep>Zhangyang Wang;http://arxiv.org/pdf/2204.00928v2;cs.CV;Project page: https://vita-group.github.io/SinNeRF/;nerf
2203.16875v2;http://arxiv.org/abs/2203.16875v2;2022-03-31;MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images;"There has been rapid progress recently on 3D human rendering, including novel
view synthesis and pose animation, based on the advances of neural radiance
fields (NeRF). However, most existing methods focus on person-specific training
and their training typically requires multi-view videos. This paper deals with
a new challenging task -- rendering novel views and novel poses for a person
unseen in training, using only multiview images as input. For this task, we
propose a simple yet effective method to train a generalizable NeRF with
multiview images as conditional input. The key ingredient is a dedicated
representation combining a canonical NeRF and a volume deformation scheme.
Using a canonical space enables our method to learn shared properties of human
and easily generalize to different people. Volume deformation is used to
connect the canonical space with input and target images and query image
features for radiance and density prediction. We leverage the parametric 3D
human model fitted on the input images to derive the deformation, which works
quite well in practice when combined with our canonical NeRF. The experiments
on both real and synthetic data with the novel view synthesis and pose
animation tasks collectively demonstrate the efficacy of our method.";Xiangjun Gao<author:sep>Jiaolong Yang<author:sep>Jongyoo Kim<author:sep>Sida Peng<author:sep>Zicheng Liu<author:sep>Xin Tong;http://arxiv.org/pdf/2203.16875v2;cs.CV;;nerf
2203.17261v2;http://arxiv.org/abs/2203.17261v2;2022-03-31;R2L: Distilling Neural Radiance Field to Neural Light Field for  Efficient Novel View Synthesis;"Recent research explosion on Neural Radiance Field (NeRF) shows the
encouraging potential to represent complex scenes with neural networks. One
major drawback of NeRF is its prohibitive inference time: Rendering a single
pixel requires querying the NeRF network hundreds of times. To resolve it,
existing efforts mainly attempt to reduce the number of required sampled
points. However, the problem of iterative sampling still exists. On the other
hand, Neural Light Field (NeLF) presents a more straightforward representation
over NeRF in novel view synthesis -- the rendering of a pixel amounts to one
single forward pass without ray-marching. In this work, we present a deep
residual MLP network (88 layers) to effectively learn the light field. We show
the key to successfully learning such a deep NeLF network is to have sufficient
data, for which we transfer the knowledge from a pre-trained NeRF model via
data distillation. Extensive experiments on both synthetic and real-world
scenes show the merits of our method over other counterpart algorithms. On the
synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x
runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average
PSNR improvement) rendering quality than NeRF without any customized
parallelism requirement.";Huan Wang<author:sep>Jian Ren<author:sep>Zeng Huang<author:sep>Kyle Olszewski<author:sep>Menglei Chai<author:sep>Yun Fu<author:sep>Sergey Tulyakov;http://arxiv.org/pdf/2203.17261v2;cs.CV;Accepted by ECCV 2022. Code: https://github.com/snap-research/R2L;nerf
2203.16626v1;http://arxiv.org/abs/2203.16626v1;2022-03-30;DDNeRF: Depth Distribution Neural Radiance Fields;"In recent years, the field of implicit neural representation has progressed
significantly. Models such as neural radiance fields (NeRF), which uses
relatively small neural networks, can represent high-quality scenes and achieve
state-of-the-art results for novel view synthesis. Training these types of
networks, however, is still computationally very expensive. We present depth
distribution neural radiance field (DDNeRF), a new method that significantly
increases sampling efficiency along rays during training while achieving
superior results for a given sampling budget. DDNeRF achieves this by learning
a more accurate representation of the density distribution along rays. More
specifically, we train a coarse model to predict the internal distribution of
the transparency of an input volume in addition to the volume's total density.
This finer distribution then guides the sampling procedure of the fine model.
This method allows us to use fewer samples during training while reducing
computational resources.";David Dadon<author:sep>Ohad Fried<author:sep>Yacov Hel-Or;http://arxiv.org/pdf/2203.16626v1;cs.CV;;nerf
2203.15946v2;http://arxiv.org/abs/2203.15946v2;2022-03-29;Towards Learning Neural Representations from Shadows;"We present a method that learns neural shadow fields which are neural scene
representations that are only learnt from the shadows present in the scene.
While traditional shape-from-shadow (SfS) algorithms reconstruct geometry from
shadows, they assume a fixed scanning setup and fail to generalize to complex
scenes. Neural rendering algorithms, on the other hand, rely on photometric
consistency between RGB images, but largely ignore physical cues such as
shadows, which have been shown to provide valuable information about the scene.
We observe that shadows are a powerful cue that can constrain neural scene
representations to learn SfS, and even outperform NeRF to reconstruct otherwise
hidden geometry. We propose a graphics-inspired differentiable approach to
render accurate shadows with volumetric rendering, predicting a shadow map that
can be compared to the ground truth shadow. Even with just binary shadow maps,
we show that neural rendering can localize the object and estimate coarse
geometry. Our approach reveals that sparse cues in images can be used to
estimate geometry using differentiable volumetric rendering. Moreover, our
framework is highly generalizable and can work alongside existing 3D
reconstruction techniques that otherwise only use photometric consistency.";Kushagra Tiwary<author:sep>Tzofi Klinghoffer<author:sep>Ramesh Raskar;http://arxiv.org/pdf/2203.15946v2;cs.CV;;nerf
2203.15798v1;http://arxiv.org/abs/2203.15798v1;2022-03-29;DRaCoN -- Differentiable Rasterization Conditioned Neural Radiance  Fields for Articulated Avatars;"Acquisition and creation of digital human avatars is an important problem
with applications to virtual telepresence, gaming, and human modeling. Most
contemporary approaches for avatar generation can be viewed either as 3D-based
methods, which use multi-view data to learn a 3D representation with appearance
(such as a mesh, implicit surface, or volume), or 2D-based methods which learn
photo-realistic renderings of avatars but lack accurate 3D representations. In
this work, we present, DRaCoN, a framework for learning full-body volumetric
avatars which exploits the advantages of both the 2D and 3D neural rendering
techniques. It consists of a Differentiable Rasterization module, DiffRas, that
synthesizes a low-resolution version of the target image along with additional
latent features guided by a parametric body model. The output of DiffRas is
then used as conditioning to our conditional neural 3D representation module
(c-NeRF) which generates the final high-res image along with body geometry
using volumetric rendering. While DiffRas helps in obtaining photo-realistic
image quality, c-NeRF, which employs signed distance fields (SDF) for 3D
representations, helps to obtain fine 3D geometric details. Experiments on the
challenging ZJU-MoCap and Human3.6M datasets indicate that DRaCoN outperforms
state-of-the-art methods both in terms of error metrics and visual quality.";Amit Raj<author:sep>Umar Iqbal<author:sep>Koki Nagano<author:sep>Sameh Khamis<author:sep>Pavlo Molchanov<author:sep>James Hays<author:sep>Jan Kautz;http://arxiv.org/pdf/2203.15798v1;cs.CV;Project page at https://dracon-avatars.github.io/;nerf
2203.15224v2;http://arxiv.org/abs/2203.15224v2;2022-03-29;Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene  Segmentation;"Large-scale training data with high-quality annotations is critical for
training semantic and instance segmentation models. Unfortunately, pixel-wise
annotation is labor-intensive and costly, raising the demand for more efficient
labeling strategies. In this work, we present a novel 3D-to-2D label transfer
method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and
instance labels from easy-to-obtain coarse 3D bounding primitives. Our method
utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D
semantic cues transferred from existing datasets. We demonstrate that this
combination allows for improved geometry guided by semantic information,
enabling rendering of accurate semantic maps across multiple views.
Furthermore, this fusion process resolves label ambiguity of the coarse 3D
annotations and filters noise in the 2D predictions. By inferring in 3D space
and rendering to 2D labels, our 2D semantic and instance labels are multi-view
consistent by design. Experimental results show that Panoptic NeRF outperforms
existing label transfer methods in terms of accuracy and multi-view consistency
on challenging urban scenes of the KITTI-360 dataset.";Xiao Fu<author:sep>Shangzhan Zhang<author:sep>Tianrun Chen<author:sep>Yichong Lu<author:sep>Lanyun Zhu<author:sep>Xiaowei Zhou<author:sep>Andreas Geiger<author:sep>Yiyi Liao;http://arxiv.org/pdf/2203.15224v2;cs.CV;Project page: https://fuxiao0719.github.io/projects/panopticnerf/;nerf
2203.15587v2;http://arxiv.org/abs/2203.15587v2;2022-03-26;RGB-D Neural Radiance Fields: Local Sampling for Faster Training;"Learning a 3D representation of a scene has been a challenging problem for
decades in computer vision. Recent advances in implicit neural representation
from images using neural radiance fields(NeRF) have shown promising results.
Some of the limitations of previous NeRF based methods include longer training
time, and inaccurate underlying geometry. The proposed method takes advantage
of RGB-D data to reduce training time by leveraging depth sensing to improve
local sampling. This paper proposes a depth-guided local sampling strategy and
a smaller neural network architecture to achieve faster training time without
compromising quality.";Arnab Dey<author:sep>Andrew I. Comport;http://arxiv.org/pdf/2203.15587v2;cs.CV;;nerf
2203.13800v1;http://arxiv.org/abs/2203.13800v1;2022-03-25;Continuous Dynamic-NeRF: Spline-NeRF;"The problem of reconstructing continuous functions over time is important for
problems such as reconstructing moving scenes, and interpolating between time
steps. Previous approaches that use deep-learning rely on regularization to
ensure that reconstructions are approximately continuous, which works well on
short sequences. As sequence length grows, though, it becomes more difficult to
regularize, and it becomes less feasible to learn only through regularization.
We propose a new architecture for function reconstruction based on classical
Bezier splines, which ensures $C^0$ and $C^1$-continuity, where $C^0$
continuity is that $\forall c:\lim\limits_{x\to c} f(x)
  = f(c)$, or more intuitively that there are no breaks at any point in the
function. In order to demonstrate our architecture, we reconstruct dynamic
scenes using Neural Radiance Fields, but hope it is clear that our approach is
general and can be applied to a variety of problems. We recover a Bezier spline
$B(\beta, t\in[0,1])$, parametrized by the control points $\beta$. Using Bezier
splines ensures reconstructions have $C^0$ and $C^1$ continuity, allowing for
guaranteed interpolation over time. We reconstruct $\beta$ with a multi-layer
perceptron (MLP), blending machine learning with classical animation
techniques. All code is available at https://github.com/JulianKnodt/nerf_atlas,
and datasets are from prior work.";Julian Knodt;http://arxiv.org/pdf/2203.13800v1;cs.CV;;nerf
2203.12575v2;http://arxiv.org/abs/2203.12575v2;2022-03-23;NeuMan: Neural Human Radiance Field from a Single Video;"Photorealistic rendering and reposing of humans is important for enabling
augmented reality experiences. We propose a novel framework to reconstruct the
human and the scene that can be rendered with novel human poses and views from
just a single in-the-wild video. Given a video captured by a moving camera, we
train two NeRF models: a human NeRF model and a scene NeRF model. To train
these models, we rely on existing methods to estimate the rough geometry of the
human and the scene. Those rough geometry estimates allow us to create a
warping field from the observation space to the canonical pose-independent
space, where we train the human model in. Our method is able to learn subject
specific details, including cloth wrinkles and accessories, from just a 10
seconds video clip, and to provide high quality renderings of the human under
novel poses, from novel views, together with the background.";Wei Jiang<author:sep>Kwang Moo Yi<author:sep>Golnoosh Samei<author:sep>Oncel Tuzel<author:sep>Anurag Ranjan;http://arxiv.org/pdf/2203.12575v2;cs.CV;;nerf
2203.11283v1;http://arxiv.org/abs/2203.11283v1;2022-03-21;NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction;"While NeRF has shown great success for neural reconstruction and rendering,
its limited MLP capacity and long per-scene optimization times make it
challenging to model large-scale indoor scenes. In contrast, classical 3D
reconstruction methods can handle large-scale scenes but do not produce
realistic renderings. We propose NeRFusion, a method that combines the
advantages of NeRF and TSDF-based fusion techniques to achieve efficient
large-scale reconstruction and photo-realistic rendering. We process the input
image sequence to predict per-frame local radiance fields via direct network
inference. These are then fused using a novel recurrent neural network that
incrementally reconstructs a global, sparse scene representation in real-time
at 22 fps. This global volume can be further fine-tuned to boost rendering
quality. We demonstrate that NeRFusion achieves state-of-the-art quality on
both large-scale indoor and small-scale object scenes, with substantially
faster reconstruction than NeRF and other recent methods.";Xiaoshuai Zhang<author:sep>Sai Bi<author:sep>Kalyan Sunkavalli<author:sep>Hao Su<author:sep>Zexiang Xu;http://arxiv.org/pdf/2203.11283v1;cs.CV;CVPR 2022;nerf
2203.10821v2;http://arxiv.org/abs/2203.10821v2;2022-03-21;Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance  Fields;"Image translation and manipulation have gain increasing attention along with
the rapid development of deep generative models. Although existing approaches
have brought impressive results, they mainly operated in 2D space. In light of
recent advances in NeRF-based 3D-aware generative models, we introduce a new
task, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene
modelled by NeRF, conditioned on one single-view semantic mask as input. To
kick-off this novel task, we propose the Sem2NeRF framework. In particular,
Sem2NeRF addresses the highly challenging task by encoding the semantic mask
into the latent code that controls the 3D scene representation of a pre-trained
decoder. To further improve the accuracy of the mapping, we integrate a new
region-aware learning strategy into the design of both the encoder and the
decoder. We verify the efficacy of the proposed Sem2NeRF and demonstrate that
it outperforms several strong baselines on two benchmark datasets. Code and
video are available at https://donydchen.github.io/sem2nerf/";Yuedong Chen<author:sep>Qianyi Wu<author:sep>Chuanxia Zheng<author:sep>Tat-Jen Cham<author:sep>Jianfei Cai;http://arxiv.org/pdf/2203.10821v2;cs.CV;"ECCV2022, Code: https://github.com/donydchen/sem2nerf Project Page:
  https://donydchen.github.io/sem2nerf/";nerf
2203.10192v1;http://arxiv.org/abs/2203.10192v1;2022-03-18;Conditional-Flow NeRF: Accurate 3D Modelling with Reliable Uncertainty  Quantification;"A critical limitation of current methods based on Neural Radiance Fields
(NeRF) is that they are unable to quantify the uncertainty associated with the
learned appearance and geometry of the scene. This information is paramount in
real applications such as medical diagnosis or autonomous driving where, to
reduce potentially catastrophic failures, the confidence on the model outputs
must be included into the decision-making process. In this context, we
introduce Conditional-Flow NeRF (CF-NeRF), a novel probabilistic framework to
incorporate uncertainty quantification into NeRF-based approaches. For this
purpose, our method learns a distribution over all possible radiance fields
modelling which is used to quantify the uncertainty associated with the
modelled scene. In contrast to previous approaches enforcing strong constraints
over the radiance field distribution, CF-NeRF learns it in a flexible and fully
data-driven manner by coupling Latent Variable Modelling and Conditional
Normalizing Flows. This strategy allows to obtain reliable uncertainty
estimation while preserving model expressivity. Compared to previous
state-of-the-art methods proposed for uncertainty quantification in NeRF, our
experiments show that the proposed method achieves significantly lower
prediction errors and more reliable uncertainty values for synthetic novel view
and depth-map estimation.";Jianxiong Shen<author:sep>Antonio Agudo<author:sep>Francesc Moreno-Noguer<author:sep>Adria Ruiz;http://arxiv.org/pdf/2203.10192v1;cs.CV;;nerf
2203.10157v2;http://arxiv.org/abs/2203.10157v2;2022-03-18;ViewFormer: NeRF-free Neural Rendering from Few Images Using  Transformers;"Novel view synthesis is a long-standing problem. In this work, we consider a
variant of the problem where we are given only a few context views sparsely
covering a scene or an object. The goal is to predict novel viewpoints in the
scene, which requires learning priors. The current state of the art is based on
Neural Radiance Field (NeRF), and while achieving impressive results, the
methods suffer from long training times as they require evaluating millions of
3D point samples via a neural network for each image. We propose a 2D-only
method that maps multiple context views and a query pose to a new image in a
single pass of a neural network. Our model uses a two-stage architecture
consisting of a codebook and a transformer model. The codebook is used to embed
individual images into a smaller latent space, and the transformer solves the
view synthesis task in this more compact space. To train our model efficiently,
we introduce a novel branching attention mechanism that allows us to use the
same model not only for neural rendering but also for camera pose estimation.
Experimental results on real-world scenes show that our approach is competitive
compared to NeRF-based methods while not reasoning explicitly in 3D, and it is
faster to train.";JonÃ¡Å¡ KulhÃ¡nek<author:sep>Erik Derner<author:sep>Torsten Sattler<author:sep>Robert BabuÅ¡ka;http://arxiv.org/pdf/2203.10157v2;cs.CV;ECCV 2022 poster;nerf
2203.09957v4;http://arxiv.org/abs/2203.09957v4;2022-03-18;Enhancement of Novel View Synthesis Using Omnidirectional Image  Completion;"In this study, we present a method for synthesizing novel views from a single
360-degree RGB-D image based on the neural radiance field (NeRF) . Prior
studies relied on the neighborhood interpolation capability of multi-layer
perceptrons to complete missing regions caused by occlusion and zooming, which
leads to artifacts. In the method proposed in this study, the input image is
reprojected to 360-degree RGB images at other camera positions, the missing
regions of the reprojected images are completed by a 2D image generative model,
and the completed images are utilized to train the NeRF. Because multiple
completed images contain inconsistencies in 3D, we introduce a method to learn
the NeRF model using a subset of completed images that cover the target scene
with less overlap of completed regions. The selection of such a subset of
images can be attributed to the maximum weight independent set problem, which
is solved through simulated annealing. Experiments demonstrated that the
proposed method can synthesize plausible novel views while preserving the
features of the scene for both artificial and real-world data.";Takayuki Hara<author:sep>Tatsuya Harada;http://arxiv.org/pdf/2203.09957v4;cs.CV;20 pages, 19 figures;nerf
2203.09517v2;http://arxiv.org/abs/2203.09517v2;2022-03-17;TensoRF: Tensorial Radiance Fields;"We present TensoRF, a novel approach to model and reconstruct radiance
fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a
scene as a 4D tensor, which represents a 3D voxel grid with per-voxel
multi-channel features. Our central idea is to factorize the 4D scene tensor
into multiple compact low-rank tensor components. We demonstrate that applying
traditional CP decomposition -- that factorizes tensors into rank-one
components with compact vectors -- in our framework leads to improvements over
vanilla NeRF. To further boost performance, we introduce a novel vector-matrix
(VM) decomposition that relaxes the low-rank constraints for two modes of a
tensor and factorizes tensors into compact vector and matrix factors. Beyond
superior rendering quality, our models with CP and VM decompositions lead to a
significantly lower memory footprint in comparison to previous and concurrent
works that directly optimize per-voxel features. Experimentally, we demonstrate
that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with
better rendering quality and even a smaller model size (<4 MB) compared to
NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality
and outperforms previous state-of-the-art methods, while reducing the
reconstruction time (<10 min) and retaining a compact model size (<75 MB).";Anpei Chen<author:sep>Zexiang Xu<author:sep>Andreas Geiger<author:sep>Jingyi Yu<author:sep>Hao Su;http://arxiv.org/pdf/2203.09517v2;cs.CV;Project Page: https://apchenstu.github.io/TensoRF/;nerf
2203.08896v2;http://arxiv.org/abs/2203.08896v2;2022-03-16;Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient  Objects and Shadow Modeling Using RPC Cameras;"We introduce the Satellite Neural Radiance Field (Sat-NeRF), a new end-to-end
model for learning multi-view satellite photogrammetry in the wild. Sat-NeRF
combines some of the latest trends in neural rendering with native satellite
camera models, represented by rational polynomial coefficient (RPC) functions.
The proposed method renders new views and infers surface models of similar
quality to those obtained with traditional state-of-the-art stereo pipelines.
Multi-date images exhibit significant changes in appearance, mainly due to
varying shadows and transient objects (cars, vegetation). Robustness to these
challenges is achieved by a shadow-aware irradiance model and uncertainty
weighting to deal with transient phenomena that cannot be explained by the
position of the sun. We evaluate Sat-NeRF using WorldView-3 images from
different locations and stress the advantages of applying a bundle adjustment
to the satellite camera models prior to training. This boosts the network
performance and can optionally be used to extract additional cues for depth
supervision.";Roger MarÃ­<author:sep>Gabriele Facciolo<author:sep>Thibaud Ehret;http://arxiv.org/pdf/2203.08896v2;cs.CV;Accepted at CVPR EarthVision Workshop 2022;nerf
2203.08133v4;http://arxiv.org/abs/2203.08133v4;2022-03-15;Animatable Implicit Neural Representations for Creating Realistic  Avatars from Videos;"This paper addresses the challenge of reconstructing an animatable human
model from a multi-view video. Some recent works have proposed to decompose a
non-rigidly deforming scene into a canonical neural radiance field and a set of
deformation fields that map observation-space points to the canonical space,
thereby enabling them to learn the dynamic scene from images. However, they
represent the deformation field as translational vector field or SE(3) field,
which makes the optimization highly under-constrained. Moreover, these
representations cannot be explicitly controlled by input motions. Instead, we
introduce a pose-driven deformation field based on the linear blend skinning
algorithm, which combines the blend weight field and the 3D human skeleton to
produce observation-to-canonical correspondences. Since 3D human skeletons are
more observable, they can regularize the learning of the deformation field.
Moreover, the pose-driven deformation field can be controlled by input skeletal
motions to generate new deformation fields to animate the canonical human
model. Experiments show that our approach significantly outperforms recent
human modeling methods. The code is available at
https://zju3dv.github.io/animatable_nerf/.";Sida Peng<author:sep>Zhen Xu<author:sep>Junting Dong<author:sep>Qianqian Wang<author:sep>Shangzhan Zhang<author:sep>Qing Shuai<author:sep>Hujun Bao<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2203.08133v4;cs.CV;"Project page: https://zju3dv.github.io/animatable_nerf/. arXiv admin
  note: substantial text overlap with arXiv:2105.02872";nerf
2203.07931v2;http://arxiv.org/abs/2203.07931v2;2022-03-15;DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video  Generation;"Conversation is an essential component of virtual avatar activities in the
metaverse. With the development of natural language processing, textual and
vocal conversation generation has achieved a significant breakthrough. However,
face-to-face conversations account for the vast majority of daily
conversations, while most existing methods focused on single-person talking
head generation. In this work, we take a step further and consider generating
realistic face-to-face conversation videos. Conversation generation is more
challenging than single-person talking head generation, since it not only
requires generating photo-realistic individual talking heads but also demands
the listener to respond to the speaker. In this paper, we propose a novel
unified framework based on neural radiance field (NeRF) to address this task.
Specifically, we model both the speaker and listener with a NeRF framework,
with different conditions to control individual expressions. The speaker is
driven by the audio signal, while the response of the listener depends on both
visual and acoustic information. In this way, face-to-face conversation videos
are generated between human avatars, with all the interlocutors modeled within
the same network. Moreover, to facilitate future research on this task, we
collect a new human conversation dataset containing 34 clips of videos.
Quantitative and qualitative experiments evaluate our method in different
aspects, e.g., image quality, pose sequence trend, and naturalness of the
rendering videos. Experimental results demonstrate that the avatars in the
resulting videos are able to perform a realistic conversation, and maintain
individual styles. All the code, data, and models will be made publicly
available.";Yichao Yan<author:sep>Zanwei Zhou<author:sep>Zi Wang<author:sep>Jingnan Gao<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2203.07931v2;cs.CV;;nerf
2203.06457v1;http://arxiv.org/abs/2203.06457v1;2022-03-12;3D-GIF: 3D-Controllable Object Generation via Implicit Factorized  Representations;"While NeRF-based 3D-aware image generation methods enable viewpoint control,
limitations still remain to be adopted to various 3D applications. Due to their
view-dependent and light-entangled volume representation, the 3D geometry
presents unrealistic quality and the color should be re-rendered for every
desired viewpoint. To broaden the 3D applicability from 3D-aware image
generation to 3D-controllable object generation, we propose the factorized
representations which are view-independent and light-disentangled, and training
schemes with randomly sampled light conditions. We demonstrate the superiority
of our method by visualizing factorized representations, re-lighted images, and
albedo-textured meshes. In addition, we show that our approach improves the
quality of the generated geometry via visualization and quantitative
comparison. To the best of our knowledge, this is the first work that extracts
albedo-textured meshes with unposed 2D images without any additional labels or
assumptions.";Minsoo Lee<author:sep>Chaeyeon Chung<author:sep>Hojun Cho<author:sep>Minjung Kim<author:sep>Sanghun Jung<author:sep>Jaegul Choo<author:sep>Minhyuk Sung;http://arxiv.org/pdf/2203.06457v1;cs.CV;;nerf
2203.05189v2;http://arxiv.org/abs/2203.05189v2;2022-03-10;NeRFocus: Neural Radiance Field for 3D Synthetic Defocus;"Neural radiance fields (NeRF) bring a new wave for 3D interactive
experiences. However, as an important part of the immersive experiences, the
defocus effects have not been fully explored within NeRF. Some recent
NeRF-based methods generate 3D defocus effects in a post-process fashion by
utilizing multiplane technology. Still, they are either time-consuming or
memory-consuming. This paper proposes a novel thin-lens-imaging-based NeRF
framework that can directly render various 3D defocus effects, dubbed NeRFocus.
Unlike the pinhole, the thin lens refracts rays of a scene point, so its
imaging on the sensor plane is scattered as a circle of confusion (CoC). A
direct solution sampling enough rays to approximate this process is
computationally expensive. Instead, we propose to inverse the thin lens imaging
to explicitly model the beam path for each point on the sensor plane and
generalize this paradigm to the beam path of each pixel, then use the
frustum-based volume rendering to render each pixel's beam path. We further
design an efficient probabilistic training (p-training) strategy to simplify
the training process vastly. Extensive experiments demonstrate that our
NeRFocus can achieve various 3D defocus effects with adjustable camera pose,
focus distance, and aperture size. Existing NeRF can be regarded as our special
case by setting aperture size as zero to render large depth-of-field images.
Despite such merits, NeRFocus does not sacrifice NeRF's original performance
(e.g., training and inference time, parameter consumption, rendering quality),
which implies its great potential for broader application and further
improvement. Code and video are available at
https://github.com/wyhuai/NeRFocus.";Yinhuai Wang<author:sep>Shuzhou Yang<author:sep>Yujie Hu<author:sep>Jian Zhang;http://arxiv.org/pdf/2203.05189v2;cs.CV;;nerf
2203.04802v2;http://arxiv.org/abs/2203.04802v2;2022-03-09;NeRF-Pose: A First-Reconstruct-Then-Regress Approach for  Weakly-supervised 6D Object Pose Estimation;"Pose estimation of 3D objects in monocular images is a fundamental and
long-standing problem in computer vision. Existing deep learning approaches for
6D pose estimation typically rely on the assumption of availability of 3D
object models and 6D pose annotations. However, precise annotation of 6D poses
in real data is intricate, time-consuming and not scalable, while synthetic
data scales well but lacks realism. To avoid these problems, we present a
weakly-supervised reconstruction-based pipeline, named NeRF-Pose, which needs
only 2D object segmentation and known relative camera poses during training.
Following the first-reconstruct-then-regress idea, we first reconstruct the
objects from multiple views in the form of an implicit neural representation.
Then, we train a pose regression network to predict pixel-wise 2D-3D
correspondences between images and the reconstructed model. At inference, the
approach only needs a single image as input. A NeRF-enabled PnP+RANSAC
algorithm is used to estimate stable and accurate pose from the predicted
correspondences. Experiments on LineMod and LineMod-Occlusion show that the
proposed method has state-of-the-art accuracy in comparison to the best 6D pose
estimation methods in spite of being trained only with weak labels. Besides, we
extend the Homebrewed DB dataset with more real training images to support the
weakly supervised task and achieve compelling results on this dataset. The
extended dataset and code will be released soon.";Fu Li<author:sep>Hao Yu<author:sep>Ivan Shugurov<author:sep>Benjamin Busam<author:sep>Shaowu Yang<author:sep>Slobodan Ilic;http://arxiv.org/pdf/2203.04802v2;cs.CV;;nerf
2203.04130v1;http://arxiv.org/abs/2203.04130v1;2022-03-08;NeReF: Neural Refractive Field for Fluid Surface Reconstruction and  Implicit Representation;"Existing neural reconstruction schemes such as Neural Radiance Field (NeRF)
are largely focused on modeling opaque objects. We present a novel neural
refractive field(NeReF) to recover wavefront of transparent fluids by
simultaneously estimating the surface position and normal of the fluid front.
Unlike prior arts that treat the reconstruction target as a single layer of the
surface, NeReF is specifically formulated to recover a volumetric normal field
with its corresponding density field. A query ray will be refracted by NeReF
according to its accumulated refractive point and normal, and we employ the
correspondences and uniqueness of refracted ray for NeReF optimization. We show
NeReF, as a global optimization scheme, can more robustly tackle refraction
distortions detrimental to traditional methods for correspondence matching.
Furthermore, the continuous NeReF representation of wavefront enables view
synthesis as well as normal integration. We validate our approach on both
synthetic and real data and show it is particularly suitable for sparse
multi-view acquisition. We hence build a small light field array and experiment
on various surface shapes to demonstrate high fidelity NeReF reconstruction.";Ziyu Wang<author:sep>Wei Yang<author:sep>Junming Cao<author:sep>Lan Xu<author:sep>Junqing Yu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2203.04130v1;cs.CV;;nerf
2203.03570v1;http://arxiv.org/abs/2203.03570v1;2022-03-07;Kubric: A scalable dataset generator;"Data is the driving force of machine learning, with the amount and quality of
training data often being more important for the performance of a system than
architecture and training details. But collecting, processing and annotating
real data at scale is difficult, expensive, and frequently raises additional
privacy, fairness and legal concerns. Synthetic data is a powerful tool with
the potential to address these shortcomings: 1) it is cheap 2) supports rich
ground-truth annotations 3) offers full control over data and 4) can circumvent
or mitigate problems regarding bias, privacy and licensing. Unfortunately,
software tools for effective data generation are less mature than those for
architecture design and training, which leads to fragmented generation efforts.
To address these problems we introduce Kubric, an open-source Python framework
that interfaces with PyBullet and Blender to generate photo-realistic scenes,
with rich annotations, and seamlessly scales to large jobs distributed over
thousands of machines, and generating TBs of data. We demonstrate the
effectiveness of Kubric by presenting a series of 13 different generated
datasets for tasks ranging from studying 3D NeRF models to optical flow
estimation. We release Kubric, the used assets, all of the generation code, as
well as the rendered datasets for reuse and modification.";Klaus Greff<author:sep>Francois Belletti<author:sep>Lucas Beyer<author:sep>Carl Doersch<author:sep>Yilun Du<author:sep>Daniel Duckworth<author:sep>David J. Fleet<author:sep>Dan Gnanapragasam<author:sep>Florian Golemo<author:sep>Charles Herrmann<author:sep>Thomas Kipf<author:sep>Abhijit Kundu<author:sep>Dmitry Lagun<author:sep>Issam Laradji<author:sep> Hsueh-Ti<author:sep> Liu<author:sep>Henning Meyer<author:sep>Yishu Miao<author:sep>Derek Nowrouzezahrai<author:sep>Cengiz Oztireli<author:sep>Etienne Pot<author:sep>Noha Radwan<author:sep>Daniel Rebain<author:sep>Sara Sabour<author:sep>Mehdi S. M. Sajjadi<author:sep>Matan Sela<author:sep>Vincent Sitzmann<author:sep>Austin Stone<author:sep>Deqing Sun<author:sep>Suhani Vora<author:sep>Ziyu Wang<author:sep>Tianhao Wu<author:sep>Kwang Moo Yi<author:sep>Fangcheng Zhong<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2203.03570v1;cs.CV;21 pages, CVPR2022;nerf
2203.01762v2;http://arxiv.org/abs/2203.01762v2;2022-03-03;NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural  Radiance Fields;"Deep learning has shown great potential for modeling the physical dynamics of
complex particle systems such as fluids. Existing approaches, however, require
the supervision of consecutive particle properties, including positions and
velocities. In this paper, we consider a partially observable scenario known as
fluid dynamics grounding, that is, inferring the state transitions and
interactions within the fluid particle systems from sequential visual
observations of the fluid surface. We propose a differentiable two-stage
network named NeuroFluid. Our approach consists of (i) a particle-driven neural
renderer, which involves fluid physical properties into the volume rendering
function, and (ii) a particle transition model optimized to reduce the
differences between the rendered and the observed images. NeuroFluid provides
the first solution to unsupervised learning of particle-based fluid dynamics by
training these two models jointly. It is shown to reasonably estimate the
underlying physics of fluids with different initial shapes, viscosity, and
densities.";Shanyan Guan<author:sep>Huayu Deng<author:sep>Yunbo Wang<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2203.01762v2;cs.LG;ICML 2022, the project page: https://syguan96.github.io/NeuroFluid/;
2203.01913v2;http://arxiv.org/abs/2203.01913v2;2022-03-03;NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance  Fields;"Thin, reflective objects such as forks and whisks are common in our daily
lives, but they are particularly challenging for robot perception because it is
hard to reconstruct them using commodity RGB-D cameras or multi-view stereo
techniques. While traditional pipelines struggle with objects like these,
Neural Radiance Fields (NeRFs) have recently been shown to be remarkably
effective for performing view synthesis on objects with thin structures or
reflective materials. In this paper we explore the use of NeRF as a new source
of supervision for robust robot vision systems. In particular, we demonstrate
that a NeRF representation of a scene can be used to train dense object
descriptors. We use an optimized NeRF to extract dense correspondences between
multiple views of an object, and then use these correspondences as training
data for learning a view-invariant representation of the object. NeRF's usage
of a density field allows us to reformulate the correspondence problem with a
novel distribution-of-depths formulation, as opposed to the conventional
approach of using a depth map. Dense correspondence models supervised with our
method significantly outperform off-the-shelf learned descriptors by 106%
(PCK@3px metric, more than doubling performance) and outperform our baseline
supervised with multi-view stereo by 29%. Furthermore, we demonstrate the
learned dense descriptors enable robots to perform accurate 6-degree of freedom
(6-DoF) pick and place of thin and reflective objects.";Lin Yen-Chen<author:sep>Pete Florence<author:sep>Jonathan T. Barron<author:sep>Tsung-Yi Lin<author:sep>Alberto Rodriguez<author:sep>Phillip Isola;http://arxiv.org/pdf/2203.01913v2;cs.RO;ICRA 2022, Website: https://yenchenlin.me/nerf-supervision/;nerf
2203.01914v2;http://arxiv.org/abs/2203.01914v2;2022-03-03;Playable Environments: Video Manipulation in Space and Time;"We present Playable Environments - a new representation for interactive video
generation and manipulation in space and time. With a single image at inference
time, our novel framework allows the user to move objects in 3D while
generating a video by providing a sequence of desired actions. The actions are
learnt in an unsupervised manner. The camera can be controlled to get the
desired viewpoint. Our method builds an environment state for each frame, which
can be manipulated by our proposed action module and decoded back to the image
space with volumetric rendering. To support diverse appearances of objects, we
extend neural radiance fields with style-based modulation. Our method trains on
a collection of various monocular videos requiring only the estimated camera
parameters and 2D object locations. To set a challenging benchmark, we
introduce two large scale video datasets with significant camera movements. As
evidenced by our experiments, playable environments enable several creative
applications not attainable by prior video synthesis works, including playable
3D video generation, stylization and manipulation. Further details, code and
examples are available at
https://willi-menapace.github.io/playable-environments-website";Willi Menapace<author:sep>StÃ©phane LathuiliÃ¨re<author:sep>Aliaksandr Siarohin<author:sep>Christian Theobalt<author:sep>Sergey Tulyakov<author:sep>Vladislav Golyanik<author:sep>Elisa Ricci;http://arxiv.org/pdf/2203.01914v2;cs.CV;CVPR 2022;
2203.01414v3;http://arxiv.org/abs/2203.01414v3;2022-03-01;ICARUS: A Specialized Architecture for Neural Radiance Fields Rendering;"The practical deployment of Neural Radiance Fields (NeRF) in rendering
applications faces several challenges, with the most critical one being low
rendering speed on even high-end graphic processing units (GPUs). In this
paper, we present ICARUS, a specialized accelerator architecture tailored for
NeRF rendering. Unlike GPUs using general purpose computing and memory
architectures for NeRF, ICARUS executes the complete NeRF pipeline using
dedicated plenoptic cores (PLCore) consisting of a positional encoding unit
(PEU), a multi-layer perceptron (MLP) engine, and a volume rendering unit
(VRU). A PLCore takes in positions \& directions and renders the corresponding
pixel colors without any intermediate data going off-chip for temporary storage
and exchange, which can be time and power consuming. To implement the most
expensive component of NeRF, i.e., the MLP, we transform the fully connected
operations to approximated reconfigurable multiple constant multiplications
(MCMs), where common subexpressions are shared across different multiplications
to improve the computation efficiency. We build a prototype ICARUS using
Synopsys HAPS-80 S104, a field programmable gate array (FPGA)-based prototyping
system for large-scale integrated circuits and systems design. We evaluate the
power-performance-area (PPA) of a PLCore using 40nm LP CMOS technology. Working
at 400 MHz, a single PLCore occupies 16.5 $mm^2$ and consumes 282.8 mW,
translating to 0.105 uJ/sample. The results are compared with those of GPU and
tensor processing unit (TPU) implementations.";Chaolin Rao<author:sep>Huangjie Yu<author:sep>Haochuan Wan<author:sep>Jindong Zhou<author:sep>Yueyang Zheng<author:sep>Yu Ma<author:sep>Anpei Chen<author:sep>Minye Wu<author:sep>Binzhe Yuan<author:sep>Pingqiang Zhou<author:sep>Xin Lou<author:sep>Jingyi Yu;http://arxiv.org/pdf/2203.01414v3;cs.AR;;nerf
2202.13162v1;http://arxiv.org/abs/2202.13162v1;2022-02-26;Pix2NeRF: Unsupervised Conditional $Ï$-GAN for Single Image to Neural  Radiance Fields Translation;"We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object
or a scene of a specific class, conditioned on a single input image. This is a
challenging task, as training NeRF requires multiple views of the same scene,
coupled with corresponding poses, which are hard to obtain. Our method is based
on $\pi$-GAN, a generative model for unconditional 3D-aware image synthesis,
which maps random latent codes to radiance fields of a class of objects. We
jointly optimize (1) the $\pi$-GAN objective to utilize its high-fidelity
3D-aware generation and (2) a carefully designed reconstruction objective. The
latter includes an encoder coupled with $\pi$-GAN generator to form an
auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is
unsupervised, capable of being trained with independent images without 3D,
multi-view, or pose supervision. Applications of our pipeline include 3d avatar
generation, object-centric novel view synthesis with a single input image, and
3d-aware super-resolution, to name a few.";Shengqu Cai<author:sep>Anton Obukhov<author:sep>Dengxin Dai<author:sep>Luc Van Gool;http://arxiv.org/pdf/2202.13162v1;cs.CV;16 pages, 10 figures;nerf
2202.11855v3;http://arxiv.org/abs/2202.11855v3;2022-02-24;Learning Multi-Object Dynamics with Compositional Neural Radiance Fields;"We present a method to learn compositional multi-object dynamics models from
image observations based on implicit object encoders, Neural Radiance Fields
(NeRFs), and graph neural networks. NeRFs have become a popular choice for
representing scenes due to their strong 3D prior. However, most NeRF approaches
are trained on a single scene, representing the whole scene with a global
model, making generalization to novel scenes, containing different numbers of
objects, challenging. Instead, we present a compositional, object-centric
auto-encoder framework that maps multiple views of the scene to a set of latent
vectors representing each object separately. The latent vectors parameterize
individual NeRFs from which the scene can be reconstructed. Based on those
latent vectors, we train a graph neural network dynamics model in the latent
space to achieve compositionality for dynamics prediction. A key feature of our
approach is that the latent vectors are forced to encode 3D information through
the NeRF decoder, which enables us to incorporate structural priors in learning
the dynamics models, making long-term predictions more stable compared to
several baselines. Simulated and real world experiments show that our method
can model and learn the dynamics of compositional scenes including rigid and
deformable objects. Video: https://dannydriess.github.io/compnerfdyn/";Danny Driess<author:sep>Zhiao Huang<author:sep>Yunzhu Li<author:sep>Russ Tedrake<author:sep>Marc Toussaint;http://arxiv.org/pdf/2202.11855v3;cs.CV;v3: real robot exp;nerf
2202.08614v2;http://arxiv.org/abs/2202.08614v2;2022-02-17;Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time;"Implicit neural representations such as Neural Radiance Field (NeRF) have
focused mainly on modeling static objects captured under multi-view settings
where real-time rendering can be achieved with smart data structures, e.g.,
PlenOctree. In this paper, we present a novel Fourier PlenOctree (FPO)
technique to tackle efficient neural modeling and real-time rendering of
dynamic scenes captured under the free-view video (FVV) setting. The key idea
in our FPO is a novel combination of generalized NeRF, PlenOctree
representation, volumetric fusion and Fourier transform. To accelerate FPO
construction, we present a novel coarse-to-fine fusion scheme that leverages
the generalizable NeRF technique to generate the tree via spatial blending. To
tackle dynamic scenes, we tailor the implicit network to model the Fourier
coefficients of timevarying density and color attributes. Finally, we construct
the FPO and train the Fourier coefficients directly on the leaves of a union
PlenOctree structure of the dynamic sequence. We show that the resulting FPO
enables compact memory overload to handle dynamic objects and supports
efficient fine-tuning. Extensive experiments show that the proposed method is
3000 times faster than the original NeRF and achieves over an order of
magnitude acceleration over SOTA while preserving high visual quality for the
free-viewpoint rendering of unseen dynamic scenes.";Liao Wang<author:sep>Jiakai Zhang<author:sep>Xinhang Liu<author:sep>Fuqiang Zhao<author:sep>Yanshun Zhang<author:sep>Yingliang Zhang<author:sep>Minye Wu<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2202.08614v2;cs.CV;Project page: https://aoliao12138.github.io/FPO/;nerf
2202.06088v1;http://arxiv.org/abs/2202.06088v1;2022-02-12;NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing;"Some of the most exciting experiences that Metaverse promises to offer, for
instance, live interactions with virtual characters in virtual environments,
require real-time photo-realistic rendering. 3D reconstruction approaches to
rendering, active or passive, still require extensive cleanup work to fix the
meshes or point clouds. In this paper, we present a neural volumography
technique called neural volumetric video or NeuVV to support immersive,
interactive, and spatial-temporal rendering of volumetric video contents with
photo-realism and in real-time. The core of NeuVV is to efficiently encode a
dynamic neural radiance field (NeRF) into renderable and editable primitives.
We introduce two types of factorization schemes: a hyper-spherical harmonics
(HH) decomposition for modeling smooth color variations over space and time and
a learnable basis representation for modeling abrupt density and color changes
caused by motion. NeuVV factorization can be integrated into a Video Octree
(VOctree) analogous to PlenOctree to significantly accelerate training while
reducing memory overhead. Real-time NeuVV rendering further enables a class of
immersive content editing tools. Specifically, NeuVV treats each VOctree as a
primitive and implements volume-based depth ordering and alpha blending to
realize spatial-temporal compositions for content re-purposing. For example, we
demonstrate positioning varied manifestations of the same performance at
different 3D locations with different timing, adjusting color/texture of the
performer's clothing, casting spotlight shadows and synthesizing distance
falloff lighting, etc, all at an interactive speed. We further develop a hybrid
neural-rasterization rendering framework to support consumer-level VR headsets
so that the aforementioned volumetric video viewing and editing, for the first
time, can be conducted immersively in virtual 3D space.";Jiakai Zhang<author:sep>Liao Wang<author:sep>Xinhang Liu<author:sep>Fuqiang Zhao<author:sep>Minzhang Li<author:sep>Haizhao Dai<author:sep>Boyuan Zhang<author:sep>Wei Yang<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2202.06088v1;cs.CV;;nerf
2202.04879v1;http://arxiv.org/abs/2202.04879v1;2022-02-10;PVSeRF: Joint Pixel-, Voxel- and Surface-Aligned Radiance Field for  Single-Image Novel View Synthesis;"We present PVSeRF, a learning framework that reconstructs neural radiance
fields from single-view RGB images, for novel view synthesis. Previous
solutions, such as pixelNeRF, rely only on pixel-aligned features and suffer
from feature ambiguity issues. As a result, they struggle with the
disentanglement of geometry and appearance, leading to implausible geometries
and blurry results. To address this challenge, we propose to incorporate
explicit geometry reasoning and combine it with pixel-aligned features for
radiance field prediction. Specifically, in addition to pixel-aligned features,
we further constrain the radiance field learning to be conditioned on i)
voxel-aligned features learned from a coarse volumetric grid and ii) fine
surface-aligned features extracted from a regressed point cloud. We show that
the introduction of such geometry-aware features helps to achieve a better
disentanglement between appearance and geometry, i.e. recovering more accurate
geometries and synthesizing higher quality images of novel views. Extensive
experiments against state-of-the-art methods on ShapeNet benchmarks demonstrate
the superiority of our approach for single-image novel view synthesis.";Xianggang Yu<author:sep>Jiapeng Tang<author:sep>Yipeng Qin<author:sep>Chenghong Li<author:sep>Linchao Bao<author:sep>Xiaoguang Han<author:sep>Shuguang Cui;http://arxiv.org/pdf/2202.04879v1;cs.CV;;nerf
2202.05263v1;http://arxiv.org/abs/2202.05263v1;2022-02-10;Block-NeRF: Scalable Large Scene Neural View Synthesis;"We present Block-NeRF, a variant of Neural Radiance Fields that can represent
large-scale environments. Specifically, we demonstrate that when scaling NeRF
to render city-scale scenes spanning multiple blocks, it is vital to decompose
the scene into individually trained NeRFs. This decomposition decouples
rendering time from scene size, enables rendering to scale to arbitrarily large
environments, and allows per-block updates of the environment. We adopt several
architectural changes to make NeRF robust to data captured over months under
different environmental conditions. We add appearance embeddings, learned pose
refinement, and controllable exposure to each individual NeRF, and introduce a
procedure for aligning appearance between adjacent NeRFs so that they can be
seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to
create the largest neural scene representation to date, capable of rendering an
entire neighborhood of San Francisco.";Matthew Tancik<author:sep>Vincent Casser<author:sep>Xinchen Yan<author:sep>Sabeek Pradhan<author:sep>Ben Mildenhall<author:sep>Pratul P. Srinivasan<author:sep>Jonathan T. Barron<author:sep>Henrik Kretzschmar;http://arxiv.org/pdf/2202.05263v1;cs.CV;Project page: https://waymo.com/research/block-nerf/;nerf
2202.01020v3;http://arxiv.org/abs/2202.01020v3;2022-02-02;MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware  CT-Projections from a Single X-ray;"Computed tomography (CT) is an effective medical imaging modality, widely
used in the field of clinical medicine for the diagnosis of various
pathologies. Advances in Multidetector CT imaging technology have enabled
additional functionalities, including generation of thin slice multiplanar
cross-sectional body imaging and 3D reconstructions. However, this involves
patients being exposed to a considerable dose of ionising radiation. Excessive
ionising radiation can lead to deterministic and harmful effects on the body.
This paper proposes a Deep Learning model that learns to reconstruct CT
projections from a few or even a single-view X-ray. This is based on a novel
architecture that builds from neural radiance fields, which learns a continuous
representation of CT scans by disentangling the shape and volumetric depth of
surface and internal anatomical structures from 2D images. Our model is trained
on chest and knee datasets, and we demonstrate qualitative and quantitative
high-fidelity renderings and compare our approach to other recent radiance
field-based methods. Our code and link to our datasets are available at
https://github.com/abrilcf/mednerf";Abril Corona-Figueroa<author:sep>Jonathan Frawley<author:sep>Sam Bond-Taylor<author:sep>Sarath Bethapudi<author:sep>Hubert P. H. Shum<author:sep>Chris G. Willcocks;http://arxiv.org/pdf/2202.01020v3;eess.IV;6 pages, 4 figures, accepted at IEEE EMBC 2022;nerf
2202.00181v3;http://arxiv.org/abs/2202.00181v3;2022-02-01;CLA-NeRF: Category-Level Articulated Neural Radiance Field;"We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field
that can perform view synthesis, part segmentation, and articulated pose
estimation. CLA-NeRF is trained at the object category level using no CAD
models and no depth, but a set of RGB images with ground truth camera poses and
part segments. During inference, it only takes a few RGB views (i.e., few-shot)
of an unseen 3D object instance within the known category to infer the object
part segmentation and the neural radiance field. Given an articulated pose as
input, CLA-NeRF can perform articulation-aware volume rendering to generate the
corresponding RGB image at any camera pose. Moreover, the articulated pose of
an object can be estimated via inverse rendering. In our experiments, we
evaluate the framework across five categories on both synthetic and real-world
data. In all cases, our method shows realistic deformation results and accurate
articulated pose estimation. We believe that both few-shot articulated object
rendering and articulated pose estimation open doors for robots to perceive and
interact with unseen articulated objects.";Wei-Cheng Tseng<author:sep>Hung-Ju Liao<author:sep>Lin Yen-Chen<author:sep>Min Sun;http://arxiv.org/pdf/2202.00181v3;cs.CV;accepted by ICRA 2022;nerf
2201.12204v3;http://arxiv.org/abs/2201.12204v3;2022-01-28;From data to functa: Your data point is a function and you can treat it  like one;"It is common practice in deep learning to represent a measurement of the
world on a discrete grid, e.g. a 2D grid of pixels. However, the underlying
signal represented by these measurements is often continuous, e.g. the scene
depicted in an image. A powerful continuous alternative is then to represent
these measurements using an implicit neural representation, a neural function
trained to output the appropriate measurement value for any input spatial
location. In this paper, we take this idea to its next level: what would it
take to perform deep learning on these functions instead, treating them as
data? In this context we refer to the data as functa, and propose a framework
for deep learning on functa. This view presents a number of challenges around
efficient conversion from data to functa, compact representation of functa, and
effectively solving downstream tasks on functa. We outline a recipe to overcome
these challenges and apply it to a wide range of data modalities including
images, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We
demonstrate that this approach has various compelling properties across data
modalities, in particular on the canonical tasks of generative modeling, data
imputation, novel view synthesis and classification. Code:
https://github.com/deepmind/functa";Emilien Dupont<author:sep>Hyunjik Kim<author:sep>S. M. Ali Eslami<author:sep>Danilo Rezende<author:sep>Dan Rosenbaum;http://arxiv.org/pdf/2201.12204v3;cs.LG;;nerf
2201.08845v7;http://arxiv.org/abs/2201.08845v7;2022-01-21;Point-NeRF: Point-based Neural Radiance Fields;"Volumetric neural rendering methods like NeRF generate high-quality view
synthesis results but are optimized per-scene leading to prohibitive
reconstruction time. On the other hand, deep multi-view stereo methods can
quickly reconstruct scene geometry via direct network inference. Point-NeRF
combines the advantages of these two approaches by using neural 3D point
clouds, with associated neural features, to model a radiance field. Point-NeRF
can be rendered efficiently by aggregating neural point features near scene
surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can
be initialized via direct inference of a pre-trained deep network to produce a
neural point cloud; this point cloud can be finetuned to surpass the visual
quality of NeRF with 30X faster training time. Point-NeRF can be combined with
other 3D reconstruction methods and handles the errors and outliers in such
methods via a novel pruning and growing mechanism. The experiments on the DTU,
the NeRF Synthetics , the ScanNet and the Tanks and Temples datasets
demonstrate Point-NeRF can surpass the existing methods and achieve the
state-of-the-art results.";Qiangeng Xu<author:sep>Zexiang Xu<author:sep>Julien Philip<author:sep>Sai Bi<author:sep>Zhixin Shu<author:sep>Kalyan Sunkavalli<author:sep>Ulrich Neumann;http://arxiv.org/pdf/2201.08845v7;cs.CV;Accepted to CVPR 2022 (Oral);nerf
2201.07786v1;http://arxiv.org/abs/2201.07786v1;2022-01-19;Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation;"Animating high-fidelity video portrait with speech audio is crucial for
virtual reality and digital entertainment. While most previous studies rely on
accurate explicit structural information, recent works explore the implicit
scene representation of Neural Radiance Fields (NeRF) for realistic generation.
In order to capture the inconsistent motions as well as the semantic difference
between human head and torso, some work models them via two individual sets of
NeRF, leading to unnatural results. In this work, we propose Semantic-aware
Speaking Portrait NeRF (SSP-NeRF), which creates delicate audio-driven
portraits using one unified set of NeRF. The proposed model can handle the
detailed local facial semantics and the global head-torso relationship through
two semantic-aware modules. Specifically, we first propose a Semantic-Aware
Dynamic Ray Sampling module with an additional parsing branch that facilitates
audio-driven volume rendering. Moreover, to enable portrait rendering in one
unified neural radiance field, a Torso Deformation module is designed to
stabilize the large-scale non-rigid torso motions. Extensive evaluations
demonstrate that our proposed approach renders more realistic video portraits
compared to previous methods. Project page:
https://alvinliu0.github.io/projects/SSP-NeRF";Xian Liu<author:sep>Yinghao Xu<author:sep>Qianyi Wu<author:sep>Hang Zhou<author:sep>Wayne Wu<author:sep>Bolei Zhou;http://arxiv.org/pdf/2201.07786v1;cs.CV;"12 pages, 3 figures. Project page:
  https://alvinliu0.github.io/projects/SSP-NeRF";nerf
2201.04623v1;http://arxiv.org/abs/2201.04623v1;2022-01-12;Virtual Elastic Objects;"We present Virtual Elastic Objects (VEOs): virtual objects that not only look
like their real-world counterparts but also behave like them, even when subject
to novel interactions. Achieving this presents multiple challenges: not only do
objects have to be captured including the physical forces acting on them, then
faithfully reconstructed and rendered, but also plausible material parameters
found and simulated. To create VEOs, we built a multi-view capture system that
captures objects under the influence of a compressed air stream. Building on
recent advances in model-free, dynamic Neural Radiance Fields, we reconstruct
the objects and corresponding deformation fields. We propose to use a
differentiable, particle-based simulator to use these deformation fields to
find representative material parameters, which enable us to run new
simulations. To render simulated objects, we devise a method for integrating
the simulation results with Neural Radiance Fields. The resulting method is
applicable to a wide range of scenarios: it can handle objects composed of
inhomogeneous material, with very different shapes, and it can simulate
interactions with other virtual objects. We present our results using a newly
collected dataset of 12 objects under a variety of force fields, which will be
shared with the community.";Hsiao-yu Chen<author:sep>Edgar Tretschk<author:sep>Tuur Stuyck<author:sep>Petr Kadlecek<author:sep>Ladislav Kavan<author:sep>Etienne Vouga<author:sep>Christoph Lassner;http://arxiv.org/pdf/2201.04623v1;cs.CV;;
2201.02533v2;http://arxiv.org/abs/2201.02533v2;2022-01-07;NeROIC: Neural Rendering of Objects from Online Image Collections;"We present a novel method to acquire object representations from online image
collections, capturing high-quality geometry and material properties of
arbitrary objects from photographs with varying cameras, illumination, and
backgrounds. This enables various object-centric rendering applications such as
novel-view synthesis, relighting, and harmonized background composition from
challenging in-the-wild input. Using a multi-stage approach extending neural
radiance fields, we first infer the surface geometry and refine the coarsely
estimated initial camera parameters, while leveraging coarse foreground object
masks to improve the training efficiency and geometry quality. We also
introduce a robust normal estimation technique which eliminates the effect of
geometric noise while retaining crucial details. Lastly, we extract surface
material properties and ambient illumination, represented in spherical
harmonics with extensions that handle transient elements, e.g. sharp shadows.
The union of these components results in a highly modular and efficient object
acquisition framework. Extensive evaluations and comparisons demonstrate the
advantages of our approach in capturing high-quality geometry and appearance
properties useful for rendering applications.";Zhengfei Kuang<author:sep>Kyle Olszewski<author:sep>Menglei Chai<author:sep>Zeng Huang<author:sep>Panos Achlioptas<author:sep>Sergey Tulyakov;http://arxiv.org/pdf/2201.02533v2;cs.CV;"SIGGRAPH 2022 (Journal Track). Project page:
  https://formyfamily.github.io/NeROIC/ Code repository:
  https://github.com/snap-research/NeROIC/";
2201.01683v2;http://arxiv.org/abs/2201.01683v2;2022-01-05;Surface-Aligned Neural Radiance Fields for Controllable 3D Human  Synthesis;"We propose a new method for reconstructing controllable implicit 3D human
models from sparse multi-view RGB videos. Our method defines the neural scene
representation on the mesh surface points and signed distances from the surface
of a human body mesh. We identify an indistinguishability issue that arises
when a point in 3D space is mapped to its nearest surface point on a mesh for
learning surface-aligned neural scene representation. To address this issue, we
propose projecting a point onto a mesh surface using a barycentric
interpolation with modified vertex normals. Experiments with the ZJU-MoCap and
Human3.6M datasets show that our approach achieves a higher quality in a
novel-view and novel-pose synthesis than existing methods. We also demonstrate
that our method easily supports the control of body shape and clothes. Project
page: https://pfnet-research.github.io/surface-aligned-nerf/.";Tianhan Xu<author:sep>Yasuhiro Fujita<author:sep>Eiichi Matsumoto;http://arxiv.org/pdf/2201.01683v2;cs.CV;"CVPR 2022. Project page:
  https://pfnet-research.github.io/surface-aligned-nerf/";nerf
2201.00791v1;http://arxiv.org/abs/2201.00791v1;2022-01-03;DFA-NeRF: Personalized Talking Head Generation via Disentangled Face  Attributes Neural Rendering;"While recent advances in deep neural networks have made it possible to render
high-quality images, generating photo-realistic and personalized talking head
remains challenging. With given audio, the key to tackling this task is
synchronizing lip movement and simultaneously generating personalized
attributes like head movement and eye blink. In this work, we observe that the
input audio is highly correlated to lip motion while less correlated to other
personalized attributes (e.g., head movements). Inspired by this, we propose a
novel framework based on neural radiance field to pursue high-fidelity and
personalized talking head generation. Specifically, neural radiance field takes
lip movements features and personalized attributes as two disentangled
conditions, where lip movements are directly predicted from the audio inputs to
achieve lip-synchronized generation. In the meanwhile, personalized attributes
are sampled from a probabilistic model, where we design a Transformer-based
variational autoencoder sampled from Gaussian Process to learn plausible and
natural-looking head pose and eye blink. Experiments on several benchmarks
demonstrate that our method achieves significantly better results than
state-of-the-art methods.";Shunyu Yao<author:sep>RuiZhe Zhong<author:sep>Yichao Yan<author:sep>Guangtao Zhai<author:sep>Xiaokang Yang;http://arxiv.org/pdf/2201.00791v1;cs.CV;;nerf
2112.15399v2;http://arxiv.org/abs/2112.15399v2;2021-12-31;InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering;"We present an information-theoretic regularization technique for few-shot
novel view synthesis based on neural implicit representation. The proposed
approach minimizes potential reconstruction inconsistency that happens due to
insufficient viewpoints by imposing the entropy constraint of the density in
each ray. In addition, to alleviate the potential degenerate issue when all
training images are acquired from almost redundant viewpoints, we further
incorporate the spatially smoothness constraint into the estimated images by
restricting information gains from a pair of rays with slightly different
viewpoints. The main idea of our algorithm is to make reconstructed scenes
compact along individual rays and consistent across rays in the neighborhood.
The proposed regularizers can be plugged into most of existing neural volume
rendering techniques based on NeRF in a straightforward way. Despite its
simplicity, we achieve consistently improved performance compared to existing
neural view synthesis methods by large margins on multiple standard benchmarks.";Mijeong Kim<author:sep>Seonguk Seo<author:sep>Bohyung Han;http://arxiv.org/pdf/2112.15399v2;cs.CV;CVPR 2022, Website: http://cv.snu.ac.kr/research/InfoNeRF;nerf
2112.12390v2;http://arxiv.org/abs/2112.12390v2;2021-12-23;Learning Implicit Body Representations from Double Diffusion Based  Neural Radiance Fields;"In this paper, we present a novel double diffusion based neural radiance
field, dubbed DD-NeRF, to reconstruct human body geometry and render the human
body appearance in novel views from a sparse set of images. We first propose a
double diffusion mechanism to achieve expressive representations of input
images by fully exploiting human body priors and image appearance details at
two levels. At the coarse level, we first model the coarse human body poses and
shapes via an unclothed 3D deformable vertex model as guidance. At the fine
level, we present a multi-view sampling network to capture subtle geometric
deformations and image detailed appearances, such as clothing and hair, from
multiple input views. Considering the sparsity of the two level features, we
diffuse them into feature volumes in the canonical space to construct neural
radiance fields. Then, we present a signed distance function (SDF) regression
network to construct body surfaces from the diffused features. Thanks to our
double diffused representations, our method can even synthesize novel views of
unseen subjects. Experiments on various datasets demonstrate that our approach
outperforms the state-of-the-art in both geometric reconstruction and novel
view synthesis.";Guangming Yao<author:sep>Hongzhi Wu<author:sep>Yi Yuan<author:sep>Lincheng Li<author:sep>Kun Zhou<author:sep>Xin Yu;http://arxiv.org/pdf/2112.12390v2;cs.CV;6 pages, 5 figures;nerf
2112.12761v3;http://arxiv.org/abs/2112.12761v3;2021-12-23;BANMo: Building Animatable 3D Neural Models from Many Casual Videos;"Prior work for articulated 3D shape reconstruction often relies on
specialized sensors (e.g., synchronized multi-camera systems), or pre-built 3D
deformable models (e.g., SMAL or SMPL). Such methods are not able to scale to
diverse sets of objects in the wild. We present BANMo, a method that requires
neither a specialized sensor nor a pre-defined template shape. BANMo builds
high-fidelity, articulated 3D models (including shape and animatable skinning
weights) from many monocular casual videos in a differentiable rendering
framework. While the use of many videos provides more coverage of camera views
and object articulations, they introduce significant challenges in establishing
correspondence across scenes with different backgrounds, illumination
conditions, etc. Our key insight is to merge three schools of thought; (1)
classic deformable shape models that make use of articulated bones and blend
skinning, (2) volumetric neural radiance fields (NeRFs) that are amenable to
gradient-based optimization, and (3) canonical embeddings that generate
correspondences between pixels and an articulated model. We introduce neural
blend skinning models that allow for differentiable and invertible articulated
deformations. When combined with canonical embeddings, such models allow us to
establish dense correspondences across videos that can be self-supervised with
cycle consistency. On real and synthetic datasets, BANMo shows higher-fidelity
3D reconstructions than prior works for humans and animals, with the ability to
render realistic images from novel viewpoints and poses. Project webpage:
banmo-www.github.io .";Gengshan Yang<author:sep>Minh Vo<author:sep>Natalia Neverova<author:sep>Deva Ramanan<author:sep>Andrea Vedaldi<author:sep>Hanbyul Joo;http://arxiv.org/pdf/2112.12761v3;cs.CV;CVPR 2022 camera-ready version (last update: May 2022);nerf
2112.10703v2;http://arxiv.org/abs/2112.10703v2;2021-12-20;Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual  Fly-Throughs;"We use neural radiance fields (NeRFs) to build interactive 3D environments
from large-scale visual captures spanning buildings or even multiple city
blocks collected primarily from drones. In contrast to single object scenes (on
which NeRFs are traditionally evaluated), our scale poses multiple challenges
including (1) the need to model thousands of images with varying lighting
conditions, each of which capture only a small subset of the scene, (2)
prohibitively large model capacities that make it infeasible to train on a
single GPU, and (3) significant challenges for fast rendering that would enable
interactive fly-throughs.
  To address these challenges, we begin by analyzing visibility statistics for
large-scale scenes, motivating a sparse network structure where parameters are
specialized to different regions of the scene. We introduce a simple geometric
clustering algorithm for data parallelism that partitions training images (or
rather pixels) into different NeRF submodules that can be trained in parallel.
  We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as
well as against our own drone footage, improving training speed by 3x and PSNR
by 12%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and
introduce a novel method that exploits temporal coherence. Our technique
achieves a 40x speedup over conventional NeRF rendering while remaining within
0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.";Haithem Turki<author:sep>Deva Ramanan<author:sep>Mahadev Satyanarayanan;http://arxiv.org/pdf/2112.10703v2;cs.CV;"CVPR 2022 Project page: https://meganerf.cmusatyalab.org GitHub:
  https://github.com/cmusatyalab/mega-nerf";nerf
2112.10759v2;http://arxiv.org/abs/2112.10759v2;2021-12-20;3D-aware Image Synthesis via Learning Structural and Textural  Representations;"Making generative models 3D-aware bridges the 2D image space and the 3D
physical world yet remains challenging. Recent attempts equip a Generative
Adversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D
coordinates to pixel values, as a 3D prior. However, the implicit function in
NeRF has a very local receptive field, making the generator hard to become
aware of the global structure. Meanwhile, NeRF is built on volume rendering
which can be too costly to produce high-resolution results, increasing the
optimization difficulty. To alleviate these two problems, we propose a novel
framework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis,
through explicitly learning a structural representation and a textural
representation. We first learn a feature volume to represent the underlying
structure, which is then converted to a feature field using a NeRF-like model.
The feature field is further accumulated into a 2D feature map as the textural
representation, followed by a neural renderer for appearance synthesis. Such a
design enables independent control of the shape and the appearance. Extensive
experiments on a wide range of datasets show that our approach achieves
sufficiently higher image quality and better 3D control than the previous
methods.";Yinghao Xu<author:sep>Sida Peng<author:sep>Ceyuan Yang<author:sep>Yujun Shen<author:sep>Bolei Zhou;http://arxiv.org/pdf/2112.10759v2;cs.CV;"CVPR 2022 camera-ready, Project page:
  https://genforce.github.io/volumegan/";nerf
2112.10203v2;http://arxiv.org/abs/2112.10203v2;2021-12-19;HVTR: Hybrid Volumetric-Textural Rendering for Human Avatars;"We propose a novel neural rendering pipeline, Hybrid Volumetric-Textural
Rendering (HVTR), which synthesizes virtual human avatars from arbitrary poses
efficiently and at high quality. First, we learn to encode articulated human
motions on a dense UV manifold of the human body surface. To handle complicated
motions (e.g., self-occlusions), we then leverage the encoded information on
the UV manifold to construct a 3D volumetric representation based on a dynamic
pose-conditioned neural radiance field. While this allows us to represent 3D
geometry with changing topology, volumetric rendering is computationally heavy.
Hence we employ only a rough volumetric representation using a pose-conditioned
downsampled neural radiance field (PD-NeRF), which we can render efficiently at
low resolutions. In addition, we learn 2D textural features that are fused with
rendered volumetric features in image space. The key advantage of our approach
is that we can then convert the fused features into a high-resolution,
high-quality avatar by a fast GAN-based textural renderer. We demonstrate that
hybrid rendering enables HVTR to handle complicated motions, render
high-quality avatars under user-controlled poses/shapes and even loose
clothing, and most importantly, be efficient at inference time. Our
experimental results also demonstrate state-of-the-art quantitative results.";Tao Hu<author:sep>Tao Yu<author:sep>Zerong Zheng<author:sep>He Zhang<author:sep>Yebin Liu<author:sep>Matthias Zwicker;http://arxiv.org/pdf/2112.10203v2;cs.CV;"Accepted to 3DV 2022. See more results at
  https://www.cs.umd.edu/~taohu/hvtr/ Demo:
  https://www.youtube.com/watch?v=LE0-YpbLlkY";nerf
2112.08867v3;http://arxiv.org/abs/2112.08867v3;2021-12-16;GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation;"3D-aware image generative modeling aims to generate 3D-consistent images with
explicitly controllable camera poses. Recent works have shown promising results
by training neural radiance field (NeRF) generators on unstructured 2D images,
but still can not generate highly-realistic images with fine details. A
critical reason is that the high memory and computation cost of volumetric
representation learning greatly restricts the number of point samples for
radiance integration during training. Deficient sampling not only limits the
expressive power of the generator to handle fine details but also impedes
effective GAN training due to the noise caused by unstable Monte Carlo
sampling. We propose a novel approach that regulates point sampling and
radiance field learning on 2D manifolds, embodied as a set of learned implicit
surfaces in the 3D volume. For each viewing ray, we calculate ray-surface
intersections and accumulate their radiance generated by the network. By
training and rendering such radiance manifolds, our generator can produce high
quality images with realistic fine details and strong visual 3D consistency.";Yu Deng<author:sep>Jiaolong Yang<author:sep>Jianfeng Xiang<author:sep>Xin Tong;http://arxiv.org/pdf/2112.08867v3;cs.CV;CVPR2022 Oral. Project page: https://yudeng.github.io/GRAM/;nerf
2112.09061v1;http://arxiv.org/abs/2112.09061v1;2021-12-16;Solving Inverse Problems with NerfGANs;"We introduce a novel framework for solving inverse problems using NeRF-style
generative models. We are interested in the problem of 3-D scene reconstruction
given a single 2-D image and known camera parameters. We show that naively
optimizing the latent space leads to artifacts and poor novel view rendering.
We attribute this problem to volume obstructions that are clear in the 3-D
geometry and become visible in the renderings of novel views. We propose a
novel radiance field regularization method to obtain better 3-D surfaces and
improved novel views given single view observations. Our method naturally
extends to general inverse problems including inpainting where one observes
only partially a single view. We experimentally evaluate our method, achieving
visual improvements and performance boosts over the baselines in a wide range
of tasks. Our method achieves $30-40\%$ MSE reduction and $15-25\%$ reduction
in LPIPS loss compared to the previous state of the art.";Giannis Daras<author:sep>Wen-Sheng Chu<author:sep>Abhishek Kumar<author:sep>Dmitry Lagun<author:sep>Alexandros G. Dimakis;http://arxiv.org/pdf/2112.09061v1;cs.CV;16 pages, 18 figures;nerf
2112.05504v4;http://arxiv.org/abs/2112.05504v4;2021-12-10;BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale  Scene Rendering;"Neural radiance fields (NeRF) has achieved outstanding performance in
modeling 3D objects and controlled scenes, usually under a single scale. In
this work, we focus on multi-scale cases where large changes in imagery are
observed at drastically different scales. This scenario vastly exists in
real-world 3D environments, such as city scenes, with views ranging from
satellite level that captures the overview of a city, to ground level imagery
showing complex details of an architecture; and can also be commonly identified
in landscape and delicate minecraft 3D models. The wide span of viewing
positions within these scenes yields multi-scale renderings with very different
levels of detail, which poses great challenges to neural radiance field and
biases it towards compromised results. To address these issues, we introduce
BungeeNeRF, a progressive neural radiance field that achieves level-of-detail
rendering across drastically varied scales. Starting from fitting distant views
with a shallow base block, as training progresses, new blocks are appended to
accommodate the emerging details in the increasingly closer views. The strategy
progressively activates high-frequency channels in NeRF's positional encoding
inputs and successively unfolds more complex details as the training proceeds.
We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale
scenes with drastically varying views on multiple data sources (city models,
synthetic, and drone captured data) and its support for high-quality rendering
in different levels of detail.";Yuanbo Xiangli<author:sep>Linning Xu<author:sep>Xingang Pan<author:sep>Nanxuan Zhao<author:sep>Anyi Rao<author:sep>Christian Theobalt<author:sep>Bo Dai<author:sep>Dahua Lin;http://arxiv.org/pdf/2112.05504v4;cs.CV;"Accepted to ECCV22; Previous version: CityNeRF: Building NeRF at City
  Scale; Project page can be found in https://city-super.github.io/citynerf";nerf
2112.05637v3;http://arxiv.org/abs/2112.05637v3;2021-12-10;HeadNeRF: A Real-time NeRF-based Parametric Head Model;"In this paper, we propose HeadNeRF, a novel NeRF-based parametric head model
that integrates the neural radiance field to the parametric representation of
the human head. It can render high fidelity head images in real-time on modern
GPUs, and supports directly controlling the generated images' rendering pose
and various semantic attributes. Different from existing related parametric
models, we use the neural radiance fields as a novel 3D proxy instead of the
traditional 3D textured mesh, which makes that HeadNeRF is able to generate
high fidelity images. However, the computationally expensive rendering process
of the original NeRF hinders the construction of the parametric NeRF model. To
address this issue, we adopt the strategy of integrating 2D neural rendering to
the rendering process of NeRF and design novel loss terms. As a result, the
rendering speed of HeadNeRF can be significantly accelerated, and the rendering
time of one frame is reduced from 5s to 25ms. The well designed loss terms also
improve the rendering accuracy, and the fine-level details of the human head,
such as the gaps between teeth, wrinkles, and beards, can be represented and
synthesized by HeadNeRF. Extensive experimental results and several
applications demonstrate its effectiveness. The trained parametric model is
available at https://github.com/CrisHY1995/headnerf.";Yang Hong<author:sep>Bo Peng<author:sep>Haiyao Xiao<author:sep>Ligang Liu<author:sep>Juyong Zhang;http://arxiv.org/pdf/2112.05637v3;cs.CV;"Accepted by CVPR2022. Project page:
  https://crishy1995.github.io/HeadNeRF-Project/";nerf
2112.05140v2;http://arxiv.org/abs/2112.05140v2;2021-12-09;NeRF for Outdoor Scene Relighting;"Photorealistic editing of outdoor scenes from photographs requires a profound
understanding of the image formation process and an accurate estimation of the
scene geometry, reflectance and illumination. A delicate manipulation of the
lighting can then be performed while keeping the scene albedo and geometry
unaltered. We present NeRF-OSR, i.e., the first approach for outdoor scene
relighting based on neural radiance fields. In contrast to the prior art, our
technique allows simultaneous editing of both scene illumination and camera
viewpoint using only a collection of outdoor photos shot in uncontrolled
settings. Moreover, it enables direct control over the scene illumination, as
defined through a spherical harmonics model. For evaluation, we collect a new
benchmark dataset of several outdoor sites photographed from multiple
viewpoints and at different times. For each time, a 360 degree environment map
is captured together with a colour-calibration chequerboard to allow accurate
numerical evaluations on real data against ground truth. Comparisons against
SoTA show that NeRF-OSR enables controllable lighting and viewpoint editing at
higher quality and with realistic self-shadowing reproduction. Our method and
the dataset are publicly available at https://4dqv.mpi-inf.mpg.de/NeRF-OSR/.";Viktor Rudnev<author:sep>Mohamed Elgharib<author:sep>William Smith<author:sep>Lingjie Liu<author:sep>Vladislav Golyanik<author:sep>Christian Theobalt;http://arxiv.org/pdf/2112.05140v2;cs.CV;"22 pages, 10 figures, 2 tables; ECCV 2022; project web page:
  https://4dqv.mpi-inf.mpg.de/NeRF-OSR/";nerf
2112.05139v3;http://arxiv.org/abs/2112.05139v3;2021-12-09;CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields;"We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural
radiance fields (NeRF). By leveraging the joint language-image embedding space
of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose
a unified framework that allows manipulating NeRF in a user-friendly way, using
either a short text prompt or an exemplar image. Specifically, to combine the
novel view synthesis capability of NeRF and the controllable manipulation
ability of latent representations from generative models, we introduce a
disentangled conditional NeRF architecture that allows individual control over
both shape and appearance. This is achieved by performing the shape
conditioning via applying a learned deformation field to the positional
encoding and deferring color conditioning to the volumetric rendering stage. To
bridge this disentangled latent representation to the CLIP embedding, we design
two code mappers that take a CLIP embedding as input and update the latent
codes to reflect the targeted editing. The mappers are trained with a
CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we
propose an inverse optimization method that accurately projects an input image
to the latent codes for manipulation to enable editing on real images. We
evaluate our approach by extensive experiments on a variety of text prompts and
exemplar images and also provide an intuitive interface for interactive
editing. Our implementation is available at
https://cassiepython.github.io/clipnerf/";Can Wang<author:sep>Menglei Chai<author:sep>Mingming He<author:sep>Dongdong Chen<author:sep>Jing Liao;http://arxiv.org/pdf/2112.05139v3;cs.CV;To Appear at CVPR 2022;nerf
2112.05131v1;http://arxiv.org/abs/2112.05131v1;2021-12-09;Plenoxels: Radiance Fields without Neural Networks;"We introduce Plenoxels (plenoptic voxels), a system for photorealistic view
synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical
harmonics. This representation can be optimized from calibrated images via
gradient methods and regularization without any neural components. On standard,
benchmark tasks, Plenoxels are optimized two orders of magnitude faster than
Neural Radiance Fields with no loss in visual quality.";Alex Yu<author:sep>Sara Fridovich-Keil<author:sep>Matthew Tancik<author:sep>Qinhong Chen<author:sep>Benjamin Recht<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2112.05131v1;cs.CV;For video and code, please see https://alexyu.net/plenoxels;
2112.04812v3;http://arxiv.org/abs/2112.04812v3;2021-12-09;Deep Visual Constraints: Neural Implicit Models for Manipulation  Planning from Visual Input;"Manipulation planning is the problem of finding a sequence of robot
configurations that involves interactions with objects in the scene, e.g.,
grasping and placing an object, or more general tool-use. To achieve such
interactions, traditional approaches require hand-engineering of object
representations and interaction constraints, which easily becomes tedious when
complex objects/interactions are considered. Inspired by recent advances in 3D
modeling, e.g. NeRF, we propose a method to represent objects as continuous
functions upon which constraint features are defined and jointly trained. In
particular, the proposed pixel-aligned representation is directly inferred from
images with known camera geometry and naturally acts as a perception component
in the whole manipulation pipeline, thereby enabling long-horizon planning only
from visual input. Project page:
https://sites.google.com/view/deep-visual-constraints";Jung-Su Ha<author:sep>Danny Driess<author:sep>Marc Toussaint;http://arxiv.org/pdf/2112.04812v3;cs.RO;IEEE Robotics and Automation Letters (RA-L) 2022;nerf
2112.04312v3;http://arxiv.org/abs/2112.04312v3;2021-12-08;Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural  Human Rendering;"In this work we develop a generalizable and efficient Neural Radiance Field
(NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under
settings with sparse camera views. Though existing NeRF-based methods can
synthesize rather realistic details for human body, they tend to produce poor
results when the input has self-occlusion, especially for unseen humans under
sparse views. Moreover, these methods often require a large number of sampling
points for rendering, which leads to low efficiency and limits their real-world
applicability. To address these challenges, we propose a Geometry-guided
Progressive NeRF (GP-NeRF). In particular, to better tackle self-occlusion, we
devise a geometry-guided multi-view feature integration approach that utilizes
the estimated geometry prior to integrate the incomplete information from input
views and construct a complete geometry volume for the target human body.
Meanwhile, for achieving higher rendering efficiency, we introduce a
progressive rendering pipeline through geometry guidance, which leverages the
geometric feature volume and the predicted density values to progressively
reduce the number of sampling points and speed up the rendering process.
Experiments on the ZJU-MoCap and THUman datasets show that our method
outperforms the state-of-the-arts significantly across multiple generalization
settings, while the time cost is reduced > 70% via applying our efficient
progressive rendering pipeline.";Mingfei Chen<author:sep>Jianfeng Zhang<author:sep>Xiangyu Xu<author:sep>Lijuan Liu<author:sep>Yujun Cai<author:sep>Jiashi Feng<author:sep>Shuicheng Yan;http://arxiv.org/pdf/2112.04312v3;cs.CV;;nerf
2112.03907v1;http://arxiv.org/abs/2112.03907v1;2021-12-07;Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance  Fields;"Neural Radiance Fields (NeRF) is a popular view synthesis technique that
represents a scene as a continuous volumetric function, parameterized by
multilayer perceptrons that provide the volume density and view-dependent
emitted radiance at each location. While NeRF-based techniques excel at
representing fine geometric structures with smoothly varying view-dependent
appearance, they often fail to accurately capture and reproduce the appearance
of glossy surfaces. We address this limitation by introducing Ref-NeRF, which
replaces NeRF's parameterization of view-dependent outgoing radiance with a
representation of reflected radiance and structures this function using a
collection of spatially-varying scene properties. We show that together with a
regularizer on normal vectors, our model significantly improves the realism and
accuracy of specular reflections. Furthermore, we show that our model's
internal representation of outgoing radiance is interpretable and useful for
scene editing.";Dor Verbin<author:sep>Peter Hedman<author:sep>Ben Mildenhall<author:sep>Todd Zickler<author:sep>Jonathan T. Barron<author:sep>Pratul P. Srinivasan;http://arxiv.org/pdf/2112.03907v1;cs.CV;Project page: https://dorverbin.github.io/refnerf/;nerf
2112.03517v1;http://arxiv.org/abs/2112.03517v1;2021-12-07;CG-NeRF: Conditional Generative Neural Radiance Fields;"While recent NeRF-based generative models achieve the generation of diverse
3D-aware images, these approaches have limitations when generating images that
contain user-specified characteristics. In this paper, we propose a novel
model, referred to as the conditional generative neural radiance fields
(CG-NeRF), which can generate multi-view images reflecting extra input
conditions such as images or texts. While preserving the common characteristics
of a given input condition, the proposed model generates diverse images in fine
detail. We propose: 1) a novel unified architecture which disentangles the
shape and appearance from a condition given in various forms and 2) the
pose-consistent diversity loss for generating multimodal outputs while
maintaining consistency of the view. Experimental results show that the
proposed method maintains consistent image quality on various condition types
and achieves superior fidelity and diversity compared to existing NeRF-based
generative models.";Kyungmin Jo<author:sep>Gyumin Shim<author:sep>Sanghun Jung<author:sep>Soyoung Yang<author:sep>Jaegul Choo;http://arxiv.org/pdf/2112.03517v1;cs.CV;;nerf
2112.02789v3;http://arxiv.org/abs/2112.02789v3;2021-12-06;HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs;"Recent neural human representations can produce high-quality multi-view
rendering but require using dense multi-view inputs and costly training. They
are hence largely limited to static models as training each frame is
infeasible. We present HumanNeRF - a generalizable neural representation - for
high-fidelity free-view synthesis of dynamic humans. Analogous to how IBRNet
assists NeRF by avoiding per-scene training, HumanNeRF employs an aggregated
pixel-alignment feature across multi-view inputs along with a pose embedded
non-rigid deformation field for tackling dynamic motions. The raw HumanNeRF can
already produce reasonable rendering on sparse video inputs of unseen subjects
and camera settings. To further improve the rendering quality, we augment our
solution with an appearance blending module for combining the benefits of both
neural volumetric rendering and neural texture blending. Extensive experiments
on various multi-view dynamic human datasets demonstrate the generalizability
and effectiveness of our approach in synthesizing photo-realistic free-view
humans under challenging motions and with very sparse camera view inputs.";Fuqiang Zhao<author:sep>Wei Yang<author:sep>Jiakai Zhang<author:sep>Pei Lin<author:sep>Yingliang Zhang<author:sep>Jingyi Yu<author:sep>Lan Xu;http://arxiv.org/pdf/2112.02789v3;cs.CV;https://zhaofuq.github.io/humannerf/;nerf
2112.03288v2;http://arxiv.org/abs/2112.03288v2;2021-12-06;Dense Depth Priors for Neural Radiance Fields from Sparse Input Views;"Neural radiance fields (NeRF) encode a scene into a neural representation
that enables photo-realistic rendering of novel views. However, a successful
reconstruction from RGB images requires a large number of input views taken
under static conditions - typically up to a few hundred images for room-size
scenes. Our method aims to synthesize novel views of whole rooms from an order
of magnitude fewer images. To this end, we leverage dense depth priors in order
to constrain the NeRF optimization. First, we take advantage of the sparse
depth data that is freely available from the structure from motion (SfM)
preprocessing step used to estimate camera poses. Second, we use depth
completion to convert these sparse points into dense depth maps and uncertainty
estimates, which are used to guide NeRF optimization. Our method enables
data-efficient novel view synthesis on challenging indoor scenes, using as few
as 18 images for an entire scene.";Barbara Roessle<author:sep>Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Pratul P. Srinivasan<author:sep>Matthias NieÃner;http://arxiv.org/pdf/2112.03288v2;cs.CV;"CVPR 2022, project page:
  https://barbararoessle.github.io/dense_depth_priors_nerf/ , video:
  https://youtu.be/zzkvvdcvksc";nerf
2112.02308v2;http://arxiv.org/abs/2112.02308v2;2021-12-04;MoFaNeRF: Morphable Facial Neural Radiance Field;"We propose a parametric model that maps free-view images into a vector space
of coded facial shape, expression and appearance with a neural radiance field,
namely Morphable Facial NeRF. Specifically, MoFaNeRF takes the coded facial
shape, expression and appearance along with space coordinate and view direction
as input to an MLP, and outputs the radiance of the space point for
photo-realistic image synthesis. Compared with conventional 3D morphable models
(3DMM), MoFaNeRF shows superiority in directly synthesizing photo-realistic
facial details even for eyes, mouths, and beards. Also, continuous face
morphing can be easily achieved by interpolating the input shape, expression
and appearance codes. By introducing identity-specific modulation and texture
encoder, our model synthesizes accurate photometric details and shows strong
representation ability. Our model shows strong ability on multiple applications
including image-based fitting, random generation, face rigging, face editing,
and novel view synthesis. Experiments show that our method achieves higher
representation ability than previous parametric models, and achieves
competitive performance in several applications. To the best of our knowledge,
our work is the first facial parametric model built upon a neural radiance
field that can be used in fitting, generation and manipulation. The code and
data is available at https://github.com/zhuhao-nju/mofanerf.";Yiyu Zhuang<author:sep>Hao Zhu<author:sep>Xusen Sun<author:sep>Xun Cao;http://arxiv.org/pdf/2112.02308v2;cs.CV;"accepted to ECCV2022; code available at
  http://github.com/zhuhao-nju/mofanerf";nerf
2112.01759v3;http://arxiv.org/abs/2112.01759v3;2021-12-03;NeRF-SR: High-Quality Neural Radiance Fields using Supersampling;"We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis
with mostly low-resolution (LR) inputs. Our method is built upon Neural
Radiance Fields (NeRF) that predicts per-point density and color with a
multi-layer perceptron. While producing images at arbitrary scales, NeRF
struggles with resolutions that go beyond observed images. Our key insight is
that NeRF benefits from 3D consistency, which means an observed pixel absorbs
information from nearby views. We first exploit it by a supersampling strategy
that shoots multiple rays at each image pixel, which further enforces
multi-view constraint at a sub-pixel level. Then, we show that NeRF-SR can
further boost the performance of supersampling by a refinement network that
leverages the estimated depth at hand to hallucinate details from related
patches on only one HR reference image. Experiment results demonstrate that
NeRF-SR generates high-quality results for novel view synthesis at HR on both
synthetic and real-world datasets without any external information.";Chen Wang<author:sep>Xian Wu<author:sep>Yuan-Chen Guo<author:sep>Song-Hai Zhang<author:sep>Yu-Wing Tai<author:sep>Shi-Min Hu;http://arxiv.org/pdf/2112.01759v3;cs.CV;"Accepted to MM 2022. Project Page:
  https://cwchenwang.github.io/NeRF-SR";nerf
2112.01983v2;http://arxiv.org/abs/2112.01983v2;2021-12-03;CoNeRF: Controllable Neural Radiance Fields;"We extend neural 3D representations to allow for intuitive and interpretable
user control beyond novel view rendering (i.e. camera control). We allow the
user to annotate which part of the scene one wishes to control with just a
small number of mask annotations in the training images. Our key idea is to
treat the attributes as latent variables that are regressed by the neural
network given the scene encoding. This leads to a few-shot learning framework,
where attributes are discovered automatically by the framework, when
annotations are not provided. We apply our method to various scenes with
different types of controllable attributes (e.g. expression control on human
faces, or state control in movement of inanimate objects). Overall, we
demonstrate, to the best of our knowledge, for the first time novel view and
novel attribute re-rendering of scenes from a single video.";Kacper Kania<author:sep>Kwang Moo Yi<author:sep>Marek Kowalski<author:sep>Tomasz TrzciÅski<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2112.01983v2;cs.CV;Project page: https://conerf.github.io/;nerf
2112.01517v3;http://arxiv.org/abs/2112.01517v3;2021-12-02;Efficient Neural Radiance Fields for Interactive Free-viewpoint Video;"This paper aims to tackle the challenge of efficiently producing interactive
free-viewpoint videos. Some recent works equip neural radiance fields with
image encoders, enabling them to generalize across scenes. When processing
dynamic scenes, they can simply treat each video frame as an individual scene
and perform novel view synthesis to generate free-viewpoint videos. However,
their rendering process is slow and cannot support interactive applications. A
major factor is that they sample lots of points in empty space when inferring
radiance fields. We propose a novel scene representation, called ENeRF, for the
fast creation of interactive free-viewpoint videos. Specifically, given
multi-view images at one frame, we first build the cascade cost volume to
predict the coarse geometry of the scene. The coarse geometry allows us to
sample few points near the scene surface, thereby significantly improving the
rendering speed. This process is fully differentiable, enabling us to jointly
learn the depth prediction and radiance field networks from RGB images.
Experiments on multiple benchmarks show that our approach exhibits competitive
performance while being at least 60 times faster than previous generalizable
radiance field methods.";Haotong Lin<author:sep>Sida Peng<author:sep>Zhen Xu<author:sep>Yunzhi Yan<author:sep>Qing Shuai<author:sep>Hujun Bao<author:sep>Xiaowei Zhou;http://arxiv.org/pdf/2112.01517v3;cs.CV;"SIGGRAPH Asia 2022; Project page: https://zju3dv.github.io/enerf/";nerf
2112.01422v2;http://arxiv.org/abs/2112.01422v2;2021-12-02;3D-Aware Semantic-Guided Generative Model for Human Synthesis;"Generative Neural Radiance Field (GNeRF) models, which extract implicit 3D
representations from 2D images, have recently been shown to produce realistic
images representing rigid/semi-rigid objects, such as human faces or cars.
However, they usually struggle to generate high-quality images representing
non-rigid objects, such as the human body, which is of a great interest for
many computer graphics applications. This paper proposes a 3D-aware
Semantic-Guided Generative Model (3D-SGAN) for human image synthesis, which
combines a GNeRF with a texture generator. The former learns an implicit 3D
representation of the human body and outputs a set of 2D semantic segmentation
masks. The latter transforms these semantic masks into a real image, adding a
realistic texture to the human appearance. Without requiring additional 3D
information, our model can learn 3D human representations with a
photo-realistic, controllable generation. Our experiments on the DeepFashion
dataset show that 3D-SGAN significantly outperforms the most recent baselines.
The code is available at https://github.com/zhangqianhui/3DSGAN";Jichao Zhang<author:sep>Enver Sangineto<author:sep>Hao Tang<author:sep>Aliaksandr Siarohin<author:sep>Zhun Zhong<author:sep>Nicu Sebe<author:sep>Wei Wang;http://arxiv.org/pdf/2112.01422v2;cs.CV;ECCV 2022. 29 pages;nerf
2112.01523v3;http://arxiv.org/abs/2112.01523v3;2021-12-02;Learning Neural Light Fields with Ray-Space Embedding Networks;"Neural radiance fields (NeRFs) produce state-of-the-art view synthesis
results. However, they are slow to render, requiring hundreds of network
evaluations per pixel to approximate a volume rendering integral. Baking NeRFs
into explicit data structures enables efficient rendering, but results in a
large increase in memory footprint and, in many cases, a quality reduction. In
this paper, we propose a novel neural light field representation that, in
contrast, is compact and directly predicts integrated radiance along rays. Our
method supports rendering with a single network evaluation per pixel for small
baseline light field datasets and can also be applied to larger baselines with
only a few evaluations per pixel. At the core of our approach is a ray-space
embedding network that maps the 4D ray-space manifold into an intermediate,
interpolable latent space. Our method achieves state-of-the-art quality on
dense forward-facing datasets such as the Stanford Light Field dataset. In
addition, for forward-facing scenes with sparser inputs we achieve results that
are competitive with NeRF-based approaches in terms of quality while providing
a better speed/quality/memory trade-off with far fewer network evaluations.";Benjamin Attal<author:sep>Jia-Bin Huang<author:sep>Michael Zollhoefer<author:sep>Johannes Kopf<author:sep>Changil Kim;http://arxiv.org/pdf/2112.01523v3;cs.CV;"CVPR 2022 camera ready revision. Major changes include: 1. Additional
  comparison to NeX on Stanford, RealFF, Shiny datasets 2. Experiment on 360
  degree lego bulldozer scene in the appendix, using Pluecker parameterization
  3. Moving student-teacher results to the appendix 4. Clarity edits -- in
  particular, making it clear that our Stanford evaluation *does not* use
  subdivision";nerf
2112.01455v2;http://arxiv.org/abs/2112.01455v2;2021-12-02;Zero-Shot Text-Guided Object Generation with Dream Fields;"We combine neural rendering with multi-modal image and text representations
to synthesize diverse 3D objects solely from natural language descriptions. Our
method, Dream Fields, can generate the geometry and color of a wide range of
objects without 3D supervision. Due to the scarcity of diverse, captioned 3D
data, prior methods only generate objects from a handful of categories, such as
ShapeNet. Instead, we guide generation with image-text models pre-trained on
large datasets of captioned images from the web. Our method optimizes a Neural
Radiance Field from many camera views so that rendered images score highly with
a target caption according to a pre-trained CLIP model. To improve fidelity and
visual quality, we introduce simple geometric priors, including
sparsity-inducing transmittance regularization, scene bounds, and new MLP
architectures. In experiments, Dream Fields produce realistic, multi-view
consistent object geometry and color from a variety of natural language
captions.";Ajay Jain<author:sep>Ben Mildenhall<author:sep>Jonathan T. Barron<author:sep>Pieter Abbeel<author:sep>Ben Poole;http://arxiv.org/pdf/2112.01455v2;cs.CV;CVPR 2022. 13 pages. Website: https://ajayj.com/dreamfields;
2112.00724v1;http://arxiv.org/abs/2112.00724v1;2021-12-01;RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from  Sparse Inputs;"Neural Radiance Fields (NeRF) have emerged as a powerful representation for
the task of novel view synthesis due to their simplicity and state-of-the-art
performance. Though NeRF can produce photorealistic renderings of unseen
viewpoints when many input views are available, its performance drops
significantly when this number is reduced. We observe that the majority of
artifacts in sparse input scenarios are caused by errors in the estimated scene
geometry, and by divergent behavior at the start of training. We address this
by regularizing the geometry and appearance of patches rendered from unobserved
viewpoints, and annealing the ray sampling space during training. We
additionally use a normalizing flow model to regularize the color of unobserved
viewpoints. Our model outperforms not only other methods that optimize over a
single scene, but in many cases also conditional models that are extensively
pre-trained on large multi-view datasets.";Michael Niemeyer<author:sep>Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Mehdi S. M. Sajjadi<author:sep>Andreas Geiger<author:sep>Noha Radwan;http://arxiv.org/pdf/2112.00724v1;cs.CV;"Project page available at
  https://m-niemeyer.github.io/regnerf/index.html";nerf
2111.15234v2;http://arxiv.org/abs/2111.15234v2;2021-11-30;NeRFReN: Neural Radiance Fields with Reflections;"Neural Radiance Fields (NeRF) has achieved unprecedented view synthesis
quality using coordinate-based neural scene representations. However, NeRF's
view dependency can only handle simple reflections like highlights but cannot
deal with complex reflections such as those from glass and mirrors. In these
scenarios, NeRF models the virtual image as real geometries which leads to
inaccurate depth estimation, and produces blurry renderings when the multi-view
consistency is violated as the reflected objects may only be seen under some of
the viewpoints. To overcome these issues, we introduce NeRFReN, which is built
upon NeRF to model scenes with reflections. Specifically, we propose to split a
scene into transmitted and reflected components, and model the two components
with separate neural radiance fields. Considering that this decomposition is
highly under-constrained, we exploit geometric priors and apply
carefully-designed training strategies to achieve reasonable decomposition
results. Experiments on various self-captured scenes show that our method
achieves high-quality novel view synthesis and physically sound depth
estimation results while enabling scene editing applications.";Yuan-Chen Guo<author:sep>Di Kang<author:sep>Linchao Bao<author:sep>Yu He<author:sep>Song-Hai Zhang;http://arxiv.org/pdf/2111.15234v2;cs.CV;"Accepted to CVPR 2022. Project page:
  https://bennyguo.github.io/nerfren/";nerf
2111.15246v3;http://arxiv.org/abs/2111.15246v3;2021-11-30;Hallucinated Neural Radiance Fields in the Wild;"Neural Radiance Fields (NeRF) has recently gained popularity for its
impressive novel view synthesis ability. This paper studies the problem of
hallucinated NeRF: i.e., recovering a realistic NeRF at a different time of day
from a group of tourism images. Existing solutions adopt NeRF with a
controllable appearance embedding to render novel views under various
conditions, but they cannot render view-consistent images with an unseen
appearance. To solve this problem, we present an end-to-end framework for
constructing a hallucinated NeRF, dubbed as Ha-NeRF. Specifically, we propose
an appearance hallucination module to handle time-varying appearances and
transfer them to novel views. Considering the complex occlusions of tourism
images, we introduce an anti-occlusion module to decompose the static subjects
for visibility accurately. Experimental results on synthetic data and real
tourism photo collections demonstrate that our method can hallucinate the
desired appearances and render occlusion-free images from different views. The
project and supplementary materials are available at
https://rover-xingyu.github.io/Ha-NeRF/.";Xingyu Chen<author:sep>Qi Zhang<author:sep>Xiaoyu Li<author:sep>Yue Chen<author:sep>Ying Feng<author:sep>Xuan Wang<author:sep>Jue Wang;http://arxiv.org/pdf/2111.15246v3;cs.CV;"Accepted by CVPR 2022. Project website:
  https://rover-xingyu.github.io/Ha-NeRF/";nerf
2111.15552v1;http://arxiv.org/abs/2111.15552v1;2021-11-30;NeuSample: Neural Sample Field for Efficient View Synthesis;"Neural radiance fields (NeRF) have shown great potentials in representing 3D
scenes and synthesizing novel views, but the computational overhead of NeRF at
the inference stage is still heavy. To alleviate the burden, we delve into the
coarse-to-fine, hierarchical sampling procedure of NeRF and point out that the
coarse stage can be replaced by a lightweight module which we name a neural
sample field. The proposed sample field maps rays into sample distributions,
which can be transformed into point coordinates and fed into radiance fields
for volume rendering. The overall framework is named as NeuSample. We perform
experiments on Realistic Synthetic 360$^{\circ}$ and Real Forward-Facing, two
popular 3D scene sets, and show that NeuSample achieves better rendering
quality than NeRF while enjoying a faster inference speed. NeuSample is further
compressed with a proposed sample field extraction method towards a better
trade-off between quality and speed.";Jiemin Fang<author:sep>Lingxi Xie<author:sep>Xinggang Wang<author:sep>Xiaopeng Zhang<author:sep>Wenyu Liu<author:sep>Qi Tian;http://arxiv.org/pdf/2111.15552v1;cs.CV;Project page: https://jaminfong.cn/neusample/;nerf
2111.15490v2;http://arxiv.org/abs/2111.15490v2;2021-11-30;FENeRF: Face Editing in Neural Radiance Fields;"Previous portrait image generation methods roughly fall into two categories:
2D GANs and 3D-aware GANs. 2D GANs can generate high fidelity portraits but
with low view consistency. 3D-aware GAN methods can maintain view consistency
but their generated images are not locally editable. To overcome these
limitations, we propose FENeRF, a 3D-aware generator that can produce
view-consistent and locally-editable portrait images. Our method uses two
decoupled latent codes to generate corresponding facial semantics and texture
in a spatial aligned 3D volume with shared geometry. Benefiting from such
underlying 3D representation, FENeRF can jointly render the boundary-aligned
image and semantic mask and use the semantic mask to edit the 3D volume via GAN
inversion. We further show such 3D representation can be learned from widely
available monocular image and semantic mask pairs. Moreover, we reveal that
joint learning semantics and texture helps to generate finer geometry. Our
experiments demonstrate that FENeRF outperforms state-of-the-art methods in
various face editing tasks.";Jingxiang Sun<author:sep>Xuan Wang<author:sep>Yong Zhang<author:sep>Xiaoyu Li<author:sep>Qi Zhang<author:sep>Yebin Liu<author:sep>Jue Wang;http://arxiv.org/pdf/2111.15490v2;cs.CV;Accepted to CVPR 2022. Project: https://mrtornado24.github.io/FENeRF/;nerf
2111.14292v2;http://arxiv.org/abs/2111.14292v2;2021-11-29;Deblur-NeRF: Neural Radiance Fields from Blurry Images;"Neural Radiance Field (NeRF) has gained considerable attention recently for
3D scene reconstruction and novel view synthesis due to its remarkable
synthesis quality. However, image blurriness caused by defocus or motion, which
often occurs when capturing scenes in the wild, significantly degrades its
reconstruction quality. To address this problem, We propose Deblur-NeRF, the
first method that can recover a sharp NeRF from blurry input. We adopt an
analysis-by-synthesis approach that reconstructs blurry views by simulating the
blurring process, thus making NeRF robust to blurry inputs. The core of this
simulation is a novel Deformable Sparse Kernel (DSK) module that models
spatially-varying blur kernels by deforming a canonical sparse kernel at each
spatial location. The ray origin of each kernel point is jointly optimized,
inspired by the physical blurring process. This module is parameterized as an
MLP that has the ability to be generalized to various blur types. Jointly
optimizing the NeRF and the DSK module allows us to restore a sharp NeRF. We
demonstrate that our method can be used on both camera motion blur and defocus
blur: the two most common types of blur in real scenes. Evaluation results on
both synthetic and real-world data show that our method outperforms several
baselines. The synthetic and real datasets along with the source code is
publicly available at https://limacv.github.io/deblurnerf/";Li Ma<author:sep>Xiaoyu Li<author:sep>Jing Liao<author:sep>Qi Zhang<author:sep>Xuan Wang<author:sep>Jue Wang<author:sep>Pedro V. Sander;http://arxiv.org/pdf/2111.14292v2;cs.CV;accepted in CVPR2022;nerf
2111.14451v4;http://arxiv.org/abs/2111.14451v4;2021-11-29;HDR-NeRF: High Dynamic Range Neural Radiance Fields;"We present High Dynamic Range Neural Radiance Fields (HDR-NeRF) to recover an
HDR radiance field from a set of low dynamic range (LDR) views with different
exposures. Using the HDR-NeRF, we are able to generate both novel HDR views and
novel LDR views under different exposures. The key to our method is to model
the physical imaging process, which dictates that the radiance of a scene point
transforms to a pixel value in the LDR image with two implicit functions: a
radiance field and a tone mapper. The radiance field encodes the scene radiance
(values vary from 0 to +infty), which outputs the density and radiance of a ray
by giving corresponding ray origin and ray direction. The tone mapper models
the mapping process that a ray hitting on the camera sensor becomes a pixel
value. The color of the ray is predicted by feeding the radiance and the
corresponding exposure time into the tone mapper. We use the classic volume
rendering technique to project the output radiance, colors, and densities into
HDR and LDR images, while only the input LDR images are used as the
supervision. We collect a new forward-facing HDR dataset to evaluate the
proposed method. Experimental results on synthetic and real-world scenes
validate that our method can not only accurately control the exposures of
synthesized views but also render views with a high dynamic range.";Xin Huang<author:sep>Qi Zhang<author:sep>Ying Feng<author:sep>Hongdong Li<author:sep>Xuan Wang<author:sep>Qing Wang;http://arxiv.org/pdf/2111.14451v4;cs.CV;"Accepted to CVPR 2022. Project page:
  https://xhuangcv.github.io/hdr-nerf/";nerf
2111.14643v1;http://arxiv.org/abs/2111.14643v1;2021-11-29;Urban Radiance Fields;"The goal of this work is to perform 3D reconstruction and novel view
synthesis from data captured by scanning platforms commonly deployed for world
mapping in urban outdoor environments (e.g., Street View). Given a sequence of
posed RGB images and lidar sweeps acquired by cameras and scanners moving
through an outdoor scene, we produce a model from which 3D surfaces can be
extracted and novel RGB images can be synthesized. Our approach extends Neural
Radiance Fields, which has been demonstrated to synthesize realistic novel
images for small scenes in controlled settings, with new methods for leveraging
asynchronously captured lidar data, for addressing exposure variation between
captured images, and for leveraging predicted image segmentations to supervise
densities on rays pointing at the sky. Each of these three extensions provides
significant performance improvements in experiments on Street View data. Our
system produces state-of-the-art 3D surface reconstructions and synthesizes
higher quality novel views in comparison to both traditional methods
(e.g.~COLMAP) and recent neural representations (e.g.~Mip-NeRF).";Konstantinos Rematas<author:sep>Andrew Liu<author:sep>Pratul P. Srinivasan<author:sep>Jonathan T. Barron<author:sep>Andrea Tagliasacchi<author:sep>Thomas Funkhouser<author:sep>Vittorio Ferrari;http://arxiv.org/pdf/2111.14643v1;cs.CV;Project: https://urban-radiance-fields.github.io/;nerf
2111.13539v2;http://arxiv.org/abs/2111.13539v2;2021-11-26;GeoNeRF: Generalizing NeRF with Geometry Priors;"We present GeoNeRF, a generalizable photorealistic novel view synthesis
method based on neural radiance fields. Our approach consists of two main
stages: a geometry reasoner and a renderer. To render a novel view, the
geometry reasoner first constructs cascaded cost volumes for each nearby source
view. Then, using a Transformer-based attention mechanism and the cascaded cost
volumes, the renderer infers geometry and appearance, and renders detailed
images via classical volume rendering techniques. This architecture, in
particular, allows sophisticated occlusion reasoning, gathering information
from consistent source views. Moreover, our method can easily be fine-tuned on
a single scene, and renders competitive results with per-scene optimized neural
rendering methods with a fraction of computational cost. Experiments show that
GeoNeRF outperforms state-of-the-art generalizable neural rendering models on
various synthetic and real datasets. Lastly, with a slight modification to the
geometry reasoner, we also propose an alternative model that adapts to RGBD
images. This model directly exploits the depth information often available
thanks to depth sensors. The implementation code is available at
https://www.idiap.ch/paper/geonerf.";Mohammad Mahdi Johari<author:sep>Yann Lepoittevin<author:sep>FranÃ§ois Fleuret;http://arxiv.org/pdf/2111.13539v2;cs.CV;CVPR2022;nerf
2111.13679v1;http://arxiv.org/abs/2111.13679v1;2021-11-26;NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw  Images;"Neural Radiance Fields (NeRF) is a technique for high quality novel view
synthesis from a collection of posed input images. Like most view synthesis
methods, NeRF uses tonemapped low dynamic range (LDR) as input; these images
have been processed by a lossy camera pipeline that smooths detail, clips
highlights, and distorts the simple noise distribution of raw sensor data. We
modify NeRF to instead train directly on linear raw images, preserving the
scene's full dynamic range. By rendering raw output images from the resulting
NeRF, we can perform novel high dynamic range (HDR) view synthesis tasks. In
addition to changing the camera viewpoint, we can manipulate focus, exposure,
and tonemapping after the fact. Although a single raw image appears
significantly more noisy than a postprocessed one, we show that NeRF is highly
robust to the zero-mean distribution of raw noise. When optimized over many
noisy raw inputs (25-200), NeRF produces a scene representation so accurate
that its rendered novel views outperform dedicated single and multi-image deep
raw denoisers run on the same wide baseline input images. As a result, our
method, which we call RawNeRF, can reconstruct scenes from extremely noisy
images captured in near-darkness.";Ben Mildenhall<author:sep>Peter Hedman<author:sep>Ricardo Martin-Brualla<author:sep>Pratul Srinivasan<author:sep>Jonathan T. Barron;http://arxiv.org/pdf/2111.13679v1;cs.CV;Project page: https://bmild.github.io/rawnerf/;nerf
2111.13112v1;http://arxiv.org/abs/2111.13112v1;2021-11-25;VaxNeRF: Revisiting the Classic for Voxel-Accelerated Neural Radiance  Field;"Neural Radiance Field (NeRF) is a popular method in data-driven 3D
reconstruction. Given its simplicity and high quality rendering, many NeRF
applications are being developed. However, NeRF's big limitation is its slow
speed. Many attempts are made to speeding up NeRF training and inference,
including intricate code-level optimization and caching, use of sophisticated
data structures, and amortization through multi-task and meta learning. In this
work, we revisit the basic building blocks of NeRF through the lens of classic
techniques before NeRF. We propose Voxel-Accelearated NeRF (VaxNeRF),
integrating NeRF with visual hull, a classic 3D reconstruction technique only
requiring binary foreground-background pixel labels per image. Visual hull,
which can be optimized in about 10 seconds, can provide coarse in-out field
separation to omit substantial amounts of network evaluations in NeRF. We
provide a clean fully-pythonic, JAX-based implementation on the popular JaxNeRF
codebase, consisting of only about 30 lines of code changes and a modular
visual hull subroutine, and achieve about 2-8x faster learning on top of the
highly-performative JaxNeRF baseline with zero degradation in rendering
quality. With sufficient compute, this effectively brings down full NeRF
training from hours to 30 minutes. We hope VaxNeRF -- a careful combination of
a classic technique with a deep method (that arguably replaced it) -- can
empower and accelerate new NeRF extensions and applications, with its
simplicity, portability, and reliable performance gains. Codes are available at
https://github.com/naruya/VaxNeRF .";Naruya Kondo<author:sep>Yuya Ikeda<author:sep>Andrea Tagliasacchi<author:sep>Yutaka Matsuo<author:sep>Yoichi Ochiai<author:sep>Shixiang Shane Gu;http://arxiv.org/pdf/2111.13112v1;cs.CV;;nerf
2111.12077v3;http://arxiv.org/abs/2111.12077v3;2021-11-23;Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields;"Though neural radiance fields (NeRF) have demonstrated impressive view
synthesis results on objects and small bounded regions of space, they struggle
on ""unbounded"" scenes, where the camera may point in any direction and content
may exist at any distance. In this setting, existing NeRF-like models often
produce blurry or low-resolution renderings (due to the unbalanced detail and
scale of nearby and distant objects), are slow to train, and may exhibit
artifacts due to the inherent ambiguity of the task of reconstructing a large
scene from a small set of images. We present an extension of mip-NeRF (a NeRF
variant that addresses sampling and aliasing) that uses a non-linear scene
parameterization, online distillation, and a novel distortion-based regularizer
to overcome the challenges presented by unbounded scenes. Our model, which we
dub ""mip-NeRF 360"" as we target scenes in which the camera rotates 360 degrees
around a point, reduces mean-squared error by 57% compared to mip-NeRF, and is
able to produce realistic synthesized views and detailed depth maps for highly
intricate, unbounded real-world scenes.";Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Dor Verbin<author:sep>Pratul P. Srinivasan<author:sep>Peter Hedman;http://arxiv.org/pdf/2111.12077v3;cs.CV;https://jonbarron.info/mipnerf360/;nerf
2111.11215v2;http://arxiv.org/abs/2111.11215v2;2021-11-22;Direct Voxel Grid Optimization: Super-fast Convergence for Radiance  Fields Reconstruction;"We present a super-fast convergence approach to reconstructing the per-scene
radiance field from a set of images that capture the scene with known poses.
This task, which is often applied to novel view synthesis, is recently
revolutionized by Neural Radiance Field (NeRF) for its state-of-the-art quality
and flexibility. However, NeRF and its variants require a lengthy training time
ranging from hours to days for a single scene. In contrast, our approach
achieves NeRF-comparable quality and converges rapidly from scratch in less
than 15 minutes with a single GPU. We adopt a representation consisting of a
density voxel grid for scene geometry and a feature voxel grid with a shallow
network for complex view-dependent appearance. Modeling with explicit and
discretized volume representations is not new, but we propose two simple yet
non-trivial techniques that contribute to fast convergence speed and
high-quality output. First, we introduce the post-activation interpolation on
voxel density, which is capable of producing sharp surfaces in lower grid
resolution. Second, direct voxel density optimization is prone to suboptimal
geometry solutions, so we robustify the optimization process by imposing
several priors. Finally, evaluation on five inward-facing benchmarks shows that
our method matches, if not surpasses, NeRF's quality, yet it only takes about
15 minutes to train from scratch for a new scene.";Cheng Sun<author:sep>Min Sun<author:sep>Hwann-Tzong Chen;http://arxiv.org/pdf/2111.11215v2;cs.CV;"Project page at https://sunset1995.github.io/dvgo/ ; Code at
  https://github.com/sunset1995/DirectVoxGO";nerf
2111.10427v2;http://arxiv.org/abs/2111.10427v2;2021-11-19;DIVeR: Real-time and Accurate Neural Radiance Fields with Deterministic  Integration for Volume Rendering;"DIVeR builds on the key ideas of NeRF and its variants -- density models and
volume rendering -- to learn 3D object models that can be rendered
realistically from small numbers of images. In contrast to all previous NeRF
methods, DIVeR uses deterministic rather than stochastic estimates of the
volume rendering integral. DIVeR's representation is a voxel based field of
features. To compute the volume rendering integral, a ray is broken into
intervals, one per voxel; components of the volume rendering integral are
estimated from the features for each interval using an MLP, and the components
are aggregated. As a result, DIVeR can render thin translucent structures that
are missed by other integrators. Furthermore, DIVeR's representation has
semantics that is relatively exposed compared to other such methods -- moving
feature vectors around in the voxel space results in natural edits. Extensive
qualitative and quantitative comparisons to current state-of-the-art methods
show that DIVeR produces models that (1) render at or above state-of-the-art
quality, (2) are very small without being baked, (3) render very fast without
being baked, and (4) can be edited in natural ways.";Liwen Wu<author:sep>Jae Yong Lee<author:sep>Anand Bhattad<author:sep>Yuxiong Wang<author:sep>David Forsyth;http://arxiv.org/pdf/2111.10427v2;cs.CV;;nerf
2111.09996v2;http://arxiv.org/abs/2111.09996v2;2021-11-19;LOLNeRF: Learn from One Look;"We present a method for learning a generative 3D model based on neural
radiance fields, trained solely from data with only single views of each
object. While generating realistic images is no longer a difficult task,
producing the corresponding 3D structure such that they can be rendered from
different views is non-trivial. We show that, unlike existing methods, one does
not need multi-view data to achieve this goal. Specifically, we show that by
reconstructing many images aligned to an approximate canonical pose with a
single network conditioned on a shared latent space, you can learn a space of
radiance fields that models shape and appearance for a class of objects. We
demonstrate this by training models to reconstruct object categories using
datasets that contain only one view of each subject without depth or geometry
information. Our experiments show that we achieve state-of-the-art results in
novel view synthesis and high-quality results for monocular depth prediction.";Daniel Rebain<author:sep>Mark Matthews<author:sep>Kwang Moo Yi<author:sep>Dmitry Lagun<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2111.09996v2;cs.CV;See https://lolnerf.github.io for additional results;nerf
2111.08988v1;http://arxiv.org/abs/2111.08988v1;2021-11-17;LVAC: Learned Volumetric Attribute Compression for Point Clouds using  Coordinate Based Networks;"We consider the attributes of a point cloud as samples of a vector-valued
volumetric function at discrete positions. To compress the attributes given the
positions, we compress the parameters of the volumetric function. We model the
volumetric function by tiling space into blocks, and representing the function
over each block by shifts of a coordinate-based, or implicit, neural network.
Inputs to the network include both spatial coordinates and a latent vector per
block. We represent the latent vectors using coefficients of the
region-adaptive hierarchical transform (RAHT) used in the MPEG geometry-based
point cloud codec G-PCC. The coefficients, which are highly compressible, are
rate-distortion optimized by back-propagation through a rate-distortion
Lagrangian loss in an auto-decoder configuration. The result outperforms RAHT
by 2--4 dB. This is the first work to compress volumetric functions represented
by local coordinate-based neural networks. As such, we expect it to be
applicable beyond point clouds, for example to compression of high-resolution
neural radiance fields.";Berivan Isik<author:sep>Philip A. Chou<author:sep>Sung Jin Hwang<author:sep>Nick Johnston<author:sep>George Toderici;http://arxiv.org/pdf/2111.08988v1;cs.GR;30 pages, 29 figures;
2111.04237v1;http://arxiv.org/abs/2111.04237v1;2021-11-08;Template NeRF: Towards Modeling Dense Shape Correspondences from  Category-Specific Object Images;"We present neural radiance fields (NeRF) with templates, dubbed
Template-NeRF, for modeling appearance and geometry and generating dense shape
correspondences simultaneously among objects of the same category from only
multi-view posed images, without the need of either 3D supervision or
ground-truth correspondence knowledge. The learned dense correspondences can be
readily used for various image-based tasks such as keypoint detection, part
segmentation, and texture transfer that previously require specific model
designs. Our method can also accommodate annotation transfer in a one or
few-shot manner, given only one or a few instances of the category. Using
periodic activation and feature-wise linear modulation (FiLM) conditioning, we
introduce deep implicit templates on 3D data into the 3D-aware image synthesis
pipeline NeRF. By representing object instances within the same category as
shape and appearance variation of a shared NeRF template, our proposed method
can achieve dense shape correspondences reasoning on images for a wide range of
object classes. We demonstrate the results and applications on both synthetic
and real-world data with competitive results compared with other methods based
on 3D information.";Jianfei Guo<author:sep>Zhiyuan Yang<author:sep>Xi Lin<author:sep>Qingfu Zhang;http://arxiv.org/pdf/2111.04237v1;cs.CV;10 pages, 8 figures;nerf
2110.14373v1;http://arxiv.org/abs/2110.14373v1;2021-10-27;Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition;"Decomposing a scene into its shape, reflectance and illumination is a
fundamental problem in computer vision and graphics. Neural approaches such as
NeRF have achieved remarkable success in view synthesis, but do not explicitly
perform decomposition and instead operate exclusively on radiance (the product
of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform
decomposition but struggle to accurately recover detailed illumination, thereby
significantly limiting realism. We propose a novel reflectance decomposition
network that can estimate shape, BRDF, and per-image illumination given a set
of object images captured under varying illumination. Our key technique is a
novel illumination integration network called Neural-PIL that replaces a costly
illumination integral operation in the rendering with a simple network query.
In addition, we also learn deep low-dimensional priors on BRDF and illumination
representations using novel smooth manifold auto-encoders. Our decompositions
can result in considerably better BRDF and light estimates enabling more
accurate novel view-synthesis and relighting compared to prior art. Project
page: https://markboss.me/publication/2021-neural-pil/";Mark Boss<author:sep>Varun Jampani<author:sep>Raphael Braun<author:sep>Ce Liu<author:sep>Jonathan T. Barron<author:sep>Hendrik P. A. Lensch;http://arxiv.org/pdf/2110.14373v1;cs.CV;"Project page: https://markboss.me/publication/2021-neural-pil/ Video:
  https://youtu.be/AsdAR5u3vQ8 - Accepted at NeurIPS 2021";nerf
2110.14217v1;http://arxiv.org/abs/2110.14217v1;2021-10-27;Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects;"The ability to grasp and manipulate transparent objects is a major challenge
for robots. Existing depth cameras have difficulty detecting, localizing, and
inferring the geometry of such objects. We propose using neural radiance fields
(NeRF) to detect, localize, and infer the geometry of transparent objects with
sufficient accuracy to find and grasp them securely. We leverage NeRF's
view-independent learned density, place lights to increase specular
reflections, and perform a transparency-aware depth-rendering that we feed into
the Dex-Net grasp planner. We show how additional lights create specular
reflections that improve the quality of the depth map, and test a setup for a
robot workcell equipped with an array of cameras to perform transparent object
manipulation. We also create synthetic and real datasets of transparent objects
in real-world settings, including singulated objects, cluttered tables, and the
top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are
able to reliably compute robust grasps on transparent objects, achieving 90%
and 100% grasp success rates in physical experiments on an ABB YuMi, on objects
where baseline methods fail.";Jeffrey Ichnowski<author:sep>Yahav Avigal<author:sep>Justin Kerr<author:sep>Ken Goldberg;http://arxiv.org/pdf/2110.14217v1;cs.RO;"11 pages, 9 figures, to be published in the Conference on Robot
  Learning (CoRL) 2021";nerf
2110.13746v2;http://arxiv.org/abs/2110.13746v2;2021-10-26;H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction  of Humans in Motion;"We present neural radiance fields for rendering and temporal (4D)
reconstruction of humans in motion (H-NeRF), as captured by a sparse set of
cameras or even from a monocular video. Our approach combines ideas from neural
scene representation, novel-view synthesis, and implicit statistical geometric
human representations, coupled using novel loss functions. Instead of learning
a radiance field with a uniform occupancy prior, we constrain it by a
structured implicit human body model, represented using signed distance
functions. This allows us to robustly fuse information from sparse views and
generalize well beyond the poses or views observed in training. Moreover, we
apply geometric constraints to co-learn the structure of the observed subject
-- including both body and clothing -- and to regularize the radiance field to
geometrically plausible solutions. Extensive experiments on multiple datasets
demonstrate the robustness and the accuracy of our approach, its generalization
capabilities significantly outside a small training set of poses and views, and
statistical extrapolation beyond the observed shape.";Hongyi Xu<author:sep>Thiemo Alldieck<author:sep>Cristian Sminchisescu;http://arxiv.org/pdf/2110.13746v2;cs.CV;;nerf
2110.12993v1;http://arxiv.org/abs/2110.12993v1;2021-10-25;Neural Relightable Participating Media Rendering;"Learning neural radiance fields of a scene has recently allowed realistic
novel view synthesis of the scene, but they are limited to synthesize images
under the original fixed lighting condition. Therefore, they are not flexible
for the eagerly desired tasks like relighting, scene editing and scene
composition. To tackle this problem, several recent methods propose to
disentangle reflectance and illumination from the radiance field. These methods
can cope with solid objects with opaque surfaces but participating media are
neglected. Also, they take into account only direct illumination or at most
one-bounce indirect illumination, thus suffer from energy loss due to ignoring
the high-order indirect illumination. We propose to learn neural
representations for participating media with a complete simulation of global
illumination. We estimate direct illumination via ray tracing and compute
indirect illumination with spherical harmonics. Our approach avoids computing
the lengthy indirect bounces and does not suffer from energy loss. Our
experiments on multiple scenes show that our approach achieves superior visual
quality and numerical performance compared to state-of-the-art methods, and it
can generalize to deal with solid objects with opaque surfaces as well.";Quan Zheng<author:sep>Gurprit Singh<author:sep>Hans-Peter Seidel;http://arxiv.org/pdf/2110.12993v1;cs.CV;Accepted to NeurIPS 2021;
2110.09788v1;http://arxiv.org/abs/2110.09788v1;2021-10-19;CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent  Pixel Synthesis;"The style-based GAN (StyleGAN) architecture achieved state-of-the-art results
for generating high-quality images, but it lacks explicit and precise control
over camera poses. The recently proposed NeRF-based GANs made great progress
towards 3D-aware generators, but they are unable to generate high-quality
images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that
is composed of a shallow NeRF network and a deep implicit neural representation
(INR) network. The generator synthesizes each pixel value independently without
any spatial convolution or upsampling operation. In addition, we diagnose the
problem of mirror symmetry that implies a suboptimal solution and solve it by
introducing an auxiliary discriminator. Trained on raw, single-view images,
CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of
6.97 for images at the $256\times256$ resolution on FFHQ. We also demonstrate
several interesting directions for CIPS-3D such as transfer learning and
3D-aware face stylization. The synthesis results are best viewed as videos, so
we recommend the readers to check our github project at
https://github.com/PeterouZh/CIPS-3D";Peng Zhou<author:sep>Lingxi Xie<author:sep>Bingbing Ni<author:sep>Qi Tian;http://arxiv.org/pdf/2110.09788v1;cs.CV;3D-aware GANs based on NeRF, https://github.com/PeterouZh/CIPS-3D;nerf
2110.08985v1;http://arxiv.org/abs/2110.08985v1;2021-10-18;StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image  Synthesis;"We propose StyleNeRF, a 3D-aware generative model for photo-realistic
high-resolution image synthesis with high multi-view consistency, which can be
trained on unstructured 2D images. Existing approaches either cannot synthesize
high-resolution images with fine details or yield noticeable 3D-inconsistent
artifacts. In addition, many of them lack control over style attributes and
explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF)
into a style-based generator to tackle the aforementioned challenges, i.e.,
improving rendering efficiency and 3D consistency for high-resolution image
generation. We perform volume rendering only to produce a low-resolution
feature map and progressively apply upsampling in 2D to address the first
issue. To mitigate the inconsistencies caused by 2D upsampling, we propose
multiple designs, including a better upsampler and a new regularization loss.
With these designs, StyleNeRF can synthesize high-resolution images at
interactive rates while preserving 3D consistency at high quality. StyleNeRF
also enables control of camera poses and different levels of styles, which can
generalize to unseen views. It also supports challenging tasks, including
zoom-in and-out, style mixing, inversion, and semantic editing.";Jiatao Gu<author:sep>Lingjie Liu<author:sep>Peng Wang<author:sep>Christian Theobalt;http://arxiv.org/pdf/2110.08985v1;cs.CV;24 pages, 19 figures. Project page: http://jiataogu.me/style_nerf/;nerf
2110.07604v3;http://arxiv.org/abs/2110.07604v3;2021-10-14;NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in  the Wild;"Recent history has seen a tremendous growth of work exploring implicit
representations of geometry and radiance, popularized through Neural Radiance
Fields (NeRF). Such works are fundamentally based on a (implicit) volumetric
representation of occupancy, allowing them to model diverse scene structure
including translucent objects and atmospheric obscurants. But because the vast
majority of real-world scenes are composed of well-defined surfaces, we
introduce a surface analog of such implicit models called Neural Reflectance
Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface
that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions.
Even more importantly, surface parameterizations allow NeRS to learn (neural)
bidirectional surface reflectance functions (BRDFs) that factorize
view-dependent appearance into environmental illumination, diffuse color
(albedo), and specular ""shininess."" Finally, rather than illustrating our
results on synthetic scenes or controlled in-the-lab capture, we assemble a
novel dataset of multi-view images from online marketplaces for selling goods.
Such ""in-the-wild"" multi-view image sets pose a number of challenges, including
a small number of views with unknown/rough camera estimates. We demonstrate
that surface-based neural reconstructions enable learning from such data,
outperforming volumetric neural rendering-based reconstructions. We hope that
NeRS serves as a first step toward building scalable, high-quality libraries of
real-world shape, materials, and illumination. The project page with code and
video visualizations can be found at https://jasonyzhang.com/ners.";Jason Y. Zhang<author:sep>Gengshan Yang<author:sep>Shubham Tulsiani<author:sep>Deva Ramanan;http://arxiv.org/pdf/2110.07604v3;cs.CV;In NeurIPS 2021. v2-3: Fixed minor typos;nerf
2110.06558v1;http://arxiv.org/abs/2110.06558v1;2021-10-13;LENS: Localization enhanced by NeRF synthesis;"Neural Radiance Fields (NeRF) have recently demonstrated photo-realistic
results for the task of novel view synthesis. In this paper, we propose to
apply novel view synthesis to the robot relocalization problem: we demonstrate
improvement of camera pose regression thanks to an additional synthetic dataset
rendered by the NeRF class of algorithm. To avoid spawning novel views in
irrelevant places we selected virtual camera locations from NeRF internal
representation of the 3D geometry of the scene. We further improved
localization accuracy of pose regressors using synthesized realistic and
geometry consistent images as data augmentation during training. At the time of
publication, our approach improved state of the art with a 60% lower error on
Cambridge Landmarks and 7-scenes datasets. Hence, the resulting accuracy
becomes comparable to structure-based methods, without any architecture
modification or domain adaptation constraints. Since our method allows almost
infinite generation of training data, we investigated limitations of camera
pose regression depending on size and distribution of data used for training on
public benchmarks. We concluded that pose regression accuracy is mostly bounded
by relatively small and biased datasets rather than capacity of the pose
regression model to solve the localization task.";Arthur Moreau<author:sep>Nathan Piasco<author:sep>Dzmitry Tsishkou<author:sep>Bogdan Stanciulescu<author:sep>Arnaud de La Fortelle;http://arxiv.org/pdf/2110.06558v1;cs.CV;Accepted at CoRL 2021;nerf
2110.05594v1;http://arxiv.org/abs/2110.05594v1;2021-10-11;Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo;"We present a modern solution to the multi-view photometric stereo problem
(MVPS). Our work suitably exploits the image formation model in a MVPS
experimental setup to recover the dense 3D reconstruction of an object from
images. We procure the surface orientation using a photometric stereo (PS)
image formation model and blend it with a multi-view neural radiance field
representation to recover the object's surface geometry. Contrary to the
previous multi-staged framework to MVPS, where the position, iso-depth
contours, or orientation measurements are estimated independently and then
fused later, our method is simple to implement and realize. Our method performs
neural rendering of multi-view images while utilizing surface normals estimated
by a deep photometric stereo network. We render the MVPS images by considering
the object's surface normals for each 3D sample point along the viewing
direction rather than explicitly using the density gradient in the volume space
via 3D occupancy information. We optimize the proposed neural radiance field
representation for the MVPS setup efficiently using a fully connected deep
network to recover the 3D geometry of an object. Extensive evaluation on the
DiLiGenT-MV benchmark dataset shows that our method performs better than the
approaches that perform only PS or only multi-view stereo (MVS) and provides
comparable results against the state-of-the-art multi-stage fusion methods.";Berk Kaya<author:sep>Suryansh Kumar<author:sep>Francesco Sarno<author:sep>Vittorio Ferrari<author:sep>Luc Van Gool;http://arxiv.org/pdf/2110.05594v1;cs.CV;Accepted for publication at IEEE/CVF WACV 2022. 18 pages;
2110.00276v1;http://arxiv.org/abs/2110.00276v1;2021-10-01;TyXe: Pyro-based Bayesian neural nets for Pytorch;"We introduce TyXe, a Bayesian neural network library built on top of Pytorch
and Pyro. Our leading design principle is to cleanly separate architecture,
prior, inference and likelihood specification, allowing for a flexible workflow
where users can quickly iterate over combinations of these components. In
contrast to existing packages TyXe does not implement any layer classes, and
instead relies on architectures defined in generic Pytorch code. TyXe then
provides modular choices for canonical priors, variational guides, inference
techniques, and layer selections for a Bayesian treatment of the specified
architecture. Sampling tricks for variance reduction, such as local
reparameterization or flipout, are implemented as effect handlers, which can be
applied independently of other specifications. We showcase the ease of use of
TyXe to explore Bayesian versions of popular models from various libraries: toy
regression with a pure Pytorch neural network; large-scale image classification
with torchvision ResNets; graph neural networks based on DGL; and Neural
Radiance Fields built on top of Pytorch3D. Finally, we provide convenient
abstractions for variational continual learning. In all cases the change from a
deterministic to a Bayesian neural network comes with minimal modifications to
existing code, offering a broad range of researchers and practitioners alike
practical access to uncertainty estimation techniques. The library is available
at https://github.com/TyXe-BDL/TyXe.";Hippolyt Ritter<author:sep>Theofanis Karaletsos;http://arxiv.org/pdf/2110.00276v1;stat.ML;Previously presented at PROBPROG 2020;
2110.00168v2;http://arxiv.org/abs/2110.00168v2;2021-10-01;Vision-Only Robot Navigation in a Neural Radiance World;"Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm
for the representation of natural, complex 3D scenes. NeRFs represent
continuous volumetric density and RGB values in a neural network, and generate
photo-realistic images from unseen camera viewpoints through ray tracing. We
propose an algorithm for navigating a robot through a 3D environment
represented as a NeRF using only an on-board RGB camera for localization. We
assume the NeRF for the scene has been pre-trained offline, and the robot's
objective is to navigate through unoccupied space in the NeRF to reach a goal
pose. We introduce a trajectory optimization algorithm that avoids collisions
with high-density regions in the NeRF based on a discrete time version of
differential flatness that is amenable to constraining the robot's full pose
and control inputs. We also introduce an optimization based filtering method to
estimate 6DoF pose and velocities for the robot in the NeRF given only an
onboard RGB camera. We combine the trajectory planner with the pose filter in
an online replanning loop to give a vision-based robot navigation pipeline. We
present simulation results with a quadrotor robot navigating through a jungle
gym environment, the inside of a church, and Stonehenge using only an RGB
camera. We also demonstrate an omnidirectional ground robot navigating through
the church, requiring it to reorient to fit through the narrow gap. Videos of
this work can be found at https://mikh3x4.github.io/nerf-navigation/ .";Michal Adamkiewicz<author:sep>Timothy Chen<author:sep>Adam Caccavale<author:sep>Rachel Gardner<author:sep>Preston Culbertson<author:sep>Jeannette Bohg<author:sep>Mac Schwager;http://arxiv.org/pdf/2110.00168v2;cs.RO;;nerf
2109.15271v2;http://arxiv.org/abs/2109.15271v2;2021-09-30;TÃ¶RF: Time-of-Flight Radiance Fields for Dynamic Scene View Synthesis;"Neural networks can represent and accurately reconstruct radiance fields for
static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes
captured with monocular video, with promising performance. However, the
monocular setting is known to be an under-constrained problem, and so methods
rely on data-driven priors for reconstructing dynamic content. We replace these
priors with measurements from a time-of-flight (ToF) camera, and introduce a
neural representation based on an image formation model for continuous-wave ToF
cameras. Instead of working with processed depth maps, we model the raw ToF
sensor measurements to improve reconstruction quality and avoid issues with low
reflectance regions, multi-path interference, and a sensor's limited
unambiguous depth range. We show that this approach improves robustness of
dynamic scene reconstruction to erroneous calibration and large motions, and
discuss the benefits and limitations of integrating RGB+ToF sensors that are
now available on modern smartphones.";Benjamin Attal<author:sep>Eliot Laidlaw<author:sep>Aaron Gokaslan<author:sep>Changil Kim<author:sep>Christian Richardt<author:sep>James Tompkin<author:sep>Matthew O'Toole;http://arxiv.org/pdf/2109.15271v2;cs.CV;"Accepted to NeurIPS 2021. Web page: https://imaging.cs.cmu.edu/torf/
  NeurIPS camera ready updates -- added quantitative comparisons to new
  methods, visual side-by-side comparisons performed on larger baseline camera
  sequences";nerf
2109.07448v1;http://arxiv.org/abs/2109.07448v1;2021-09-15;Neural Human Performer: Learning Generalizable Radiance Fields for Human  Performance Rendering;"In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary
human performance using sparse multi-view cameras. Recently, several works have
addressed this problem by learning person-specific neural radiance fields
(NeRF) to capture the appearance of a particular human. In parallel, some work
proposed to use pixel-aligned features to generalize radiance fields to
arbitrary new scenes and objects. Adopting such generalization approaches to
humans, however, is highly challenging due to the heavy occlusions and dynamic
articulations of body parts. To tackle this, we propose Neural Human Performer,
a novel approach that learns generalizable neural radiance fields based on a
parametric human body model for robust performance capture. Specifically, we
first introduce a temporal transformer that aggregates tracked visual features
based on the skeletal body motion over time. Moreover, a multi-view transformer
is proposed to perform cross-attention between the temporally-fused features
and the pixel-aligned features at each time step to integrate observations on
the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets
show that our method significantly outperforms recent generalizable NeRF
methods on unseen identities and poses. The video results and code are
available at https://youngjoongunc.github.io/nhp.";Youngjoong Kwon<author:sep>Dahun Kim<author:sep>Duygu Ceylan<author:sep>Henry Fuchs;http://arxiv.org/pdf/2109.07448v1;cs.CV;;nerf
2109.02123v3;http://arxiv.org/abs/2109.02123v3;2021-09-05;Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit  3D Representations;"Neural Radiance Fields (NeRF) has become a popular framework for learning
implicit 3D representations and addressing different tasks such as novel-view
synthesis or depth-map estimation. However, in downstream applications where
decisions need to be made based on automatic predictions, it is critical to
leverage the confidence associated with the model estimations. Whereas
uncertainty quantification is a long-standing problem in Machine Learning, it
has been largely overlooked in the recent NeRF literature. In this context, we
propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of
standard NeRF that learns a probability distribution over all the possible
radiance fields modeling the scene. This distribution allows to quantify the
uncertainty associated with the scene information provided by the model. S-NeRF
optimization is posed as a Bayesian learning problem which is efficiently
addressed using the Variational Inference framework. Exhaustive experiments
over benchmark datasets demonstrate that S-NeRF is able to provide more
reliable predictions and confidence values than generic approaches previously
proposed for uncertainty estimation in other domains.";Jianxiong Shen<author:sep>Adria Ruiz<author:sep>Antonio Agudo<author:sep>Francesc Moreno-Noguer;http://arxiv.org/pdf/2109.02123v3;cs.CV;;nerf
2109.01847v1;http://arxiv.org/abs/2109.01847v1;2021-09-04;Learning Object-Compositional Neural Radiance Field for Editable Scene  Rendering;"Implicit neural rendering techniques have shown promising results for novel
view synthesis. However, existing methods usually encode the entire scene as a
whole, which is generally not aware of the object identity and limits the
ability to the high-level editing tasks such as moving or adding furniture. In
this paper, we present a novel neural scene rendering system, which learns an
object-compositional neural radiance field and produces realistic rendering
with editing capability for a clustered and real-world scene. Specifically, we
design a novel two-pathway architecture, in which the scene branch encodes the
scene geometry and appearance, and the object branch encodes each standalone
object conditioned on learnable object activation codes. To survive the
training in heavily cluttered scenes, we propose a scene-guided training
strategy to solve the 3D space ambiguity in the occluded regions and learn
sharp boundaries for each object. Extensive experiments demonstrate that our
system not only achieves competitive performance for static scene novel-view
synthesis, but also produces realistic rendering for object-level editing.";Bangbang Yang<author:sep>Yinda Zhang<author:sep>Yinghao Xu<author:sep>Yijin Li<author:sep>Han Zhou<author:sep>Hujun Bao<author:sep>Guofeng Zhang<author:sep>Zhaopeng Cui;http://arxiv.org/pdf/2109.01847v1;cs.CV;"Accepted to ICCV 2021. Project Page:
  https://zju3dv.github.io/object_nerf";
2109.01750v1;http://arxiv.org/abs/2109.01750v1;2021-09-03;CodeNeRF: Disentangled Neural Radiance Fields for Object Categories;"CodeNeRF is an implicit 3D neural representation that learns the variation of
object shapes and textures across a category and can be trained, from a set of
posed images, to synthesize novel views of unseen objects. Unlike the original
NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture
by learning separate embeddings. At test time, given a single unposed image of
an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and
appearance codes via optimization. Unseen objects can be reconstructed from a
single image, and then rendered from new viewpoints or their shape and texture
edited by varying the latent codes. We conduct experiments on the SRN
benchmark, which show that CodeNeRF generalises well to unseen objects and
achieves on-par performance with methods that require known camera pose at test
time. Our results on real-world images demonstrate that CodeNeRF can bridge the
sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}";Wonbong Jang<author:sep>Lourdes Agapito;http://arxiv.org/pdf/2109.01750v1;cs.GR;10 pages, 15 figures, ICCV 2021;nerf
2109.01129v3;http://arxiv.org/abs/2109.01129v3;2021-09-02;NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor  Multi-view Stereo;"In this work, we present a new multi-view depth estimation method that
utilizes both conventional reconstruction and learning-based priors over the
recently proposed neural radiance fields (NeRF). Unlike existing neural network
based optimization method that relies on estimated correspondences, our method
directly optimizes over implicit volumes, eliminating the challenging step of
matching pixels in indoor scenes. The key to our approach is to utilize the
learning-based priors to guide the optimization process of NeRF. Our system
firstly adapts a monocular depth network over the target scene by finetuning on
its sparse SfM+MVS reconstruction from COLMAP. Then, we show that the
shape-radiance ambiguity of NeRF still exists in indoor environments and
propose to address the issue by employing the adapted depth priors to monitor
the sampling process of volume rendering. Finally, a per-pixel confidence map
acquired by error computation on the rendered image can be used to further
improve the depth quality. Experiments show that our proposed framework
significantly outperforms state-of-the-art methods on indoor scenes, with
surprising findings presented on the effectiveness of correspondence-based
optimization and NeRF-based optimization over the adapted depth priors. In
addition, we show that the guided optimization scheme does not sacrifice the
original synthesis capability of neural radiance fields, improving the
rendering quality on both seen and novel views. Code is available at
https://github.com/weiyithu/NerfingMVS.";Yi Wei<author:sep>Shaohui Liu<author:sep>Yongming Rao<author:sep>Wang Zhao<author:sep>Jiwen Lu<author:sep>Jie Zhou;http://arxiv.org/pdf/2109.01129v3;cs.CV;"To appear in ICCV 2021 (Oral). Project page:
  https://weiyithu.github.io/NerfingMVS/";nerf
2108.13826v2;http://arxiv.org/abs/2108.13826v2;2021-08-31;Self-Calibrating Neural Radiance Fields;"In this work, we propose a camera self-calibration algorithm for generic
cameras with arbitrary non-linear distortions. We jointly learn the geometry of
the scene and the accurate camera parameters without any calibration objects.
Our camera model consists of a pinhole model, a fourth order radial distortion,
and a generic noise model that can learn arbitrary non-linear camera
distortions. While traditional self-calibration algorithms mostly rely on
geometric constraints, we additionally incorporate photometric consistency.
This requires learning the geometry of the scene, and we use Neural Radiance
Fields (NeRF). We also propose a new geometric loss function, viz., projected
ray distance loss, to incorporate geometric consistency for complex non-linear
camera models. We validate our approach on standard real image datasets and
demonstrate that our model can learn the camera intrinsics and extrinsics
(pose) from scratch without COLMAP initialization. Also, we show that learning
accurate camera models in a differentiable manner allows us to improve PSNR
over baselines. Our module is an easy-to-use plugin that can be applied to NeRF
variants to improve performance. The code and data are currently available at
https://github.com/POSTECH-CVLab/SCNeRF.";Yoonwoo Jeong<author:sep>Seokjun Ahn<author:sep>Christopher Choy<author:sep>Animashree Anandkumar<author:sep>Minsu Cho<author:sep>Jaesik Park;http://arxiv.org/pdf/2108.13826v2;cs.CV;"Accepted in ICCV21, Project Page:
  https://postech-cvlab.github.io/SCNeRF/";nerf
2108.05577v1;http://arxiv.org/abs/2108.05577v1;2021-08-12;iButter: Neural Interactive Bullet Time Generator for Human  Free-viewpoint Rendering;"Generating ``bullet-time'' effects of human free-viewpoint videos is critical
for immersive visual effects and VR/AR experience. Recent neural advances still
lack the controllable and interactive bullet-time design ability for human
free-viewpoint rendering, especially under the real-time, dynamic and general
setting for our trajectory-aware task. To fill this gap, in this paper we
propose a neural interactive bullet-time generator (iButter) for
photo-realistic human free-viewpoint rendering from dense RGB streams, which
enables flexible and interactive design for human bullet-time visual effects.
Our iButter approach consists of a real-time preview and design stage as well
as a trajectory-aware refinement stage. During preview, we propose an
interactive bullet-time design approach by extending the NeRF rendering to a
real-time and dynamic setting and getting rid of the tedious per-scene
training. To this end, our bullet-time design stage utilizes a hybrid training
set, light-weight network design and an efficient silhouette-based sampling
strategy. During refinement, we introduce an efficient trajectory-aware scheme
within 20 minutes, which jointly encodes the spatial, temporal consistency and
semantic cues along the designed trajectory, achieving photo-realistic
bullet-time viewing experience of human activities. Extensive experiments
demonstrate the effectiveness of our approach for convenient interactive
bullet-time design and photo-realistic human free-viewpoint video generation.";Liao Wang<author:sep>Ziyu Wang<author:sep>Pei Lin<author:sep>Yuheng Jiang<author:sep>Xin Suo<author:sep>Minye Wu<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2108.05577v1;cs.CV;Accepted by ACM MM 2021;nerf
2108.04913v1;http://arxiv.org/abs/2108.04913v1;2021-08-10;FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face  Animation;"This paper presents a neural rendering method for controllable portrait video
synthesis. Recent advances in volumetric neural rendering, such as neural
radiance fields (NeRF), has enabled the photorealistic novel view synthesis of
static scenes with impressive results. However, modeling dynamic and
controllable objects as part of a scene with such scene representations is
still challenging. In this work, we design a system that enables both novel
view synthesis for portrait video, including the human subject and the scene
background, and explicit control of the facial expressions through a
low-dimensional expression representation. We leverage the expression space of
a 3D morphable face model (3DMM) to represent the distribution of human facial
expressions, and use it to condition the NeRF volumetric function. Furthermore,
we impose a spatial prior brought by 3DMM fitting to guide the network to learn
disentangled control for scene appearance and facial actions. We demonstrate
the effectiveness of our method on free view synthesis of portrait videos with
expression controls. To train a scene, our method only requires a short video
of a subject captured by a mobile device.";ShahRukh Athar<author:sep>Zhixin Shu<author:sep>Dimitris Samaras;http://arxiv.org/pdf/2108.04913v1;cs.CV;version 1.0.0;nerf
2108.04886v1;http://arxiv.org/abs/2108.04886v1;2021-08-10;Differentiable Surface Rendering via Non-Differentiable Sampling;"We present a method for differentiable rendering of 3D surfaces that supports
both explicit and implicit representations, provides derivatives at occlusion
boundaries, and is fast and simple to implement. The method first samples the
surface using non-differentiable rasterization, then applies differentiable,
depth-aware point splatting to produce the final image. Our approach requires
no differentiable meshing or rasterization steps, making it efficient for large
3D models and applicable to isosurfaces extracted from implicit surface
definitions. We demonstrate the effectiveness of our method for implicit-,
mesh-, and parametric-surface-based inverse rendering and neural-network
training applications. In particular, we show for the first time efficient,
differentiable rendering of an isosurface extracted from a neural radiance
field (NeRF), and demonstrate surface-based, rather than volume-based,
rendering of a NeRF.";Forrester Cole<author:sep>Kyle Genova<author:sep>Avneesh Sud<author:sep>Daniel Vlasic<author:sep>Zhoutong Zhang;http://arxiv.org/pdf/2108.04886v1;cs.GR;Accepted to ICCV 2021;nerf
2108.03880v2;http://arxiv.org/abs/2108.03880v2;2021-08-09;NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis;"Multi-View Stereo (MVS) is a core task in 3D computer vision. With the surge
of novel deep learning methods, learned MVS has surpassed the accuracy of
classical approaches, but still relies on building a memory intensive dense
cost volume. Novel View Synthesis (NVS) is a parallel line of research and has
recently seen an increase in popularity with Neural Radiance Field (NeRF)
models, which optimize a per scene radiance field. However, NeRF methods do not
generalize to novel scenes and are slow to train and test. We propose to bridge
the gap between these two methodologies with a novel network that can recover
3D scene geometry as a distance function, together with high-resolution color
images. Our method uses only a sparse set of images as input and can generalize
well to novel scenes. Additionally, we propose a coarse-to-fine sphere tracing
approach in order to significantly increase speed. We show on various datasets
that our method reaches comparable accuracy to per-scene optimized methods
while being able to generalize and running significantly faster. We provide the
source code at https://github.com/AIS-Bonn/neural_mvs";Radu Alexandru Rosu<author:sep>Sven Behnke;http://arxiv.org/pdf/2108.03880v2;cs.CV;"Accepted for International Joint Conference on Neural Networks
  (IJCNN) 2022. Code available at https://github.com/AIS-Bonn/neural_mvs";nerf
2107.11024v2;http://arxiv.org/abs/2107.11024v2;2021-07-23;A Deep Signed Directional Distance Function for Object Shape  Representation;"Neural networks that map 3D coordinates to signed distance function (SDF) or
occupancy values have enabled high-fidelity implicit representations of object
shape. This paper develops a new shape model that allows synthesizing novel
distance views by optimizing a continuous signed directional distance function
(SDDF). Similar to deep SDF models, our SDDF formulation can represent whole
categories of shapes and complete or interpolate across shapes from partial
input data. Unlike an SDF, which measures distance to the nearest surface in
any direction, an SDDF measures distance in a given direction. This allows
training an SDDF model without 3D shape supervision, using only distance
measurements, readily available from depth camera or Lidar sensors. Our model
also removes post-processing steps like surface extraction or rendering by
directly predicting distance at arbitrary locations and viewing directions.
Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which
train high-capacity black-box models, our model encodes by construction the
property that SDDF values decrease linearly along the viewing direction. This
structure constraint not only results in dimensionality reduction but also
provides analytical confidence about the accuracy of SDDF predictions,
regardless of the distance to the object surface.";Ehsan Zobeidi<author:sep>Nikolay Atanasov;http://arxiv.org/pdf/2107.11024v2;cs.CV;;
2107.04004v2;http://arxiv.org/abs/2107.04004v2;2021-07-08;3D Neural Scene Representations for Visuomotor Control;"Humans have a strong intuitive understanding of the 3D environment around us.
The mental model of the physics in our brain applies to objects of different
materials and enables us to perform a wide range of manipulation tasks that are
far beyond the reach of current robots. In this work, we desire to learn models
for dynamic 3D scenes purely from 2D visual observations. Our model combines
Neural Radiance Fields (NeRF) and time contrastive learning with an
autoencoding framework, which learns viewpoint-invariant 3D-aware scene
representations. We show that a dynamics model, constructed over the learned
representation space, enables visuomotor control for challenging manipulation
tasks involving both rigid bodies and fluids, where the target is specified in
a viewpoint different from what the robot operates on. When coupled with an
auto-decoding framework, it can even support goal specification from camera
viewpoints that are outside the training distribution. We further demonstrate
the richness of the learned 3D dynamics model by performing future prediction
and novel view synthesis. Finally, we provide detailed ablation studies
regarding different system designs and qualitative analysis of the learned
representations.";Yunzhu Li<author:sep>Shuang Li<author:sep>Vincent Sitzmann<author:sep>Pulkit Agrawal<author:sep>Antonio Torralba;http://arxiv.org/pdf/2107.04004v2;cs.RO;"Accepted to Conference on Robot Learning (CoRL 2021) as Oral
  Presentation. The first two authors contributed equally. Project Page:
  https://3d-representation-learning.github.io/nerf-dy/";nerf
2107.02791v2;http://arxiv.org/abs/2107.02791v2;2021-07-06;Depth-supervised NeRF: Fewer Views and Faster Training for Free;"A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting
incorrect geometries when given an insufficient number of input views. One
potential reason is that standard volumetric rendering does not enforce the
constraint that most of a scene's geometry consist of empty space and opaque
surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised
Neural Radiance Fields), a loss for learning radiance fields that takes
advantage of readily-available depth supervision. We leverage the fact that
current NeRF pipelines require images with known camera poses that are
typically estimated by running structure-from-motion (SFM). Crucially, SFM also
produces sparse 3D points that can be used as ""free"" depth supervision during
training: we add a loss to encourage the distribution of a ray's terminating
depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can
render better images given fewer training views while training 2-3x faster.
Further, we show that our loss is compatible with other recently proposed NeRF
methods, demonstrating that depth is a cheap and easily digestible supervisory
signal. And finally, we find that DS-NeRF can support other types of depth
supervision such as scanned depth sensors and RGB-D reconstruction outputs.";Kangle Deng<author:sep>Andrew Liu<author:sep>Jun-Yan Zhu<author:sep>Deva Ramanan;http://arxiv.org/pdf/2107.02791v2;cs.CV;"Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:
  https://github.com/dunbar12138/DSNeRF";nerf
2106.13870v2;http://arxiv.org/abs/2106.13870v2;2021-06-25;Scene Uncertainty and the Wellington Posterior of Deterministic Image  Classifiers;"We propose a method to estimate the uncertainty of the outcome of an image
classifier on a given input datum. Deep neural networks commonly used for image
classification are deterministic maps from an input image to an output class.
As such, their outcome on a given datum involves no uncertainty, so we must
specify what variability we are referring to when defining, measuring and
interpreting uncertainty, and attributing ""confidence"" to the outcome. To this
end, we introduce the Wellington Posterior, which is the distribution of
outcomes that would have been obtained in response to data that could have been
generated by the same scene that produced the given image. Since there are
infinitely many scenes that could have generated any given image, the
Wellington Posterior involves inductive transfer from scenes other than the one
portrayed. We explore the use of data augmentation, dropout, ensembling,
single-view reconstruction, and model linearization to compute a Wellington
Posterior. Additional methods include the use of conditional generative models
such as generative adversarial networks, neural radiance fields, and
conditional prior networks. We test these methods against the empirical
posterior obtained by performing inference on multiple images of the same
underlying scene. These developments are only a small step towards assessing
the reliability of deep network classifiers in a manner that is compatible with
safety-critical applications and human interpretation.";Stephanie Tsuei<author:sep>Aditya Golatkar<author:sep>Stefano Soatto;http://arxiv.org/pdf/2106.13870v2;cs.CV;;
2106.13629v2;http://arxiv.org/abs/2106.13629v2;2021-06-25;Animatable Neural Radiance Fields from Monocular RGB Videos;"We present animatable neural radiance fields (animatable NeRF) for detailed
human avatar creation from monocular videos. Our approach extends neural
radiance fields (NeRF) to the dynamic scenes with human movements via
introducing explicit pose-guided deformation while learning the scene
representation network. In particular, we estimate the human pose for each
frame and learn a constant canonical space for the detailed human template,
which enables natural shape deformation from the observation space to the
canonical space under the explicit control of the pose parameters. To
compensate for inaccurate pose estimation, we introduce the pose refinement
strategy that updates the initial pose during the learning process, which not
only helps to learn more accurate human reconstruction but also accelerates the
convergence. In experiments we show that the proposed approach achieves 1)
implicit human geometry and appearance reconstruction with high-quality
details, 2) photo-realistic rendering of the human from novel views, and 3)
animation of the human with novel poses.";Jianchuan Chen<author:sep>Ying Zhang<author:sep>Di Kang<author:sep>Xuefei Zhe<author:sep>Linchao Bao<author:sep>Xu Jia<author:sep>Huchuan Lu;http://arxiv.org/pdf/2106.13629v2;cs.CV;12 pages, 12 figures;nerf
2106.13228v2;http://arxiv.org/abs/2106.13228v2;2021-06-24;HyperNeRF: A Higher-Dimensional Representation for Topologically Varying  Neural Radiance Fields;"Neural Radiance Fields (NeRF) are able to reconstruct scenes with
unprecedented fidelity, and various recent works have extended NeRF to handle
dynamic scenes. A common approach to reconstruct such non-rigid scenes is
through the use of a learned deformation field mapping from coordinates in each
input image into a canonical template coordinate space. However, these
deformation-based approaches struggle to model changes in topology, as
topological changes require a discontinuity in the deformation field, but these
deformation fields are necessarily continuous. We address this limitation by
lifting NeRFs into a higher dimensional space, and by representing the 5D
radiance field corresponding to each individual input image as a slice through
this ""hyper-space"". Our method is inspired by level set methods, which model
the evolution of surfaces as slices through a higher dimensional surface. We
evaluate our method on two tasks: (i) interpolating smoothly between ""moments"",
i.e., configurations of the scene, seen in the input images while maintaining
visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
that our method, which we dub HyperNeRF, outperforms existing methods on both
tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for
interpolation and 8.6% for novel-view synthesis, as measured by LPIPS.
Additional videos, results, and visualizations are available at
https://hypernerf.github.io.";Keunhong Park<author:sep>Utkarsh Sinha<author:sep>Peter Hedman<author:sep>Jonathan T. Barron<author:sep>Sofien Bouaziz<author:sep>Dan B Goldman<author:sep>Ricardo Martin-Brualla<author:sep>Steven M. Seitz;http://arxiv.org/pdf/2106.13228v2;cs.CV;SIGGRAPH Asia 2021, Project page: https://hypernerf.github.io/;nerf
2106.10859v1;http://arxiv.org/abs/2106.10859v1;2021-06-21;Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single  Panorama;"We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first
method to the application of parallax-enabled novel panoramic view synthesis.
Recent works for novel view synthesis focus on perspective images with limited
field-of-view and require sufficient pictures captured in a specific condition.
Conversely, OmniNeRF can generate panorama images for unknown viewpoints given
a single equirectangular image as training data. To this end, we propose to
augment the single RGB-D panorama by projecting back and forth between a 3D
world and different 2D panoramic coordinates at different virtual camera
positions. By doing so, we are able to optimize an Omnidirectional Neural
Radiance Field with visible pixels collecting from omnidirectional viewing
angles at a fixed center for the estimation of new viewing angles from varying
camera positions. As a result, the proposed OmniNeRF achieves convincing
renderings of novel panoramic views that exhibit the parallax effect. We
showcase the effectiveness of each of our proposals on both synthetic and
real-world datasets.";Ching-Yu Hsu<author:sep>Cheng Sun<author:sep>Hwann-Tzong Chen;http://arxiv.org/pdf/2106.10859v1;cs.CV;;nerf
2106.10689v3;http://arxiv.org/abs/2106.10689v3;2021-06-20;NeuS: Learning Neural Implicit Surfaces by Volume Rendering for  Multi-view Reconstruction;"We present a novel neural surface reconstruction method, called NeuS, for
reconstructing objects and scenes with high fidelity from 2D image inputs.
Existing neural surface reconstruction approaches, such as DVR and IDR, require
foreground mask as supervision, easily get trapped in local minima, and
therefore struggle with the reconstruction of objects with severe
self-occlusion or thin structures. Meanwhile, recent neural methods for novel
view synthesis, such as NeRF and its variants, use volume rendering to produce
a neural scene representation with robustness of optimization, even for highly
complex objects. However, extracting high-quality surfaces from this learned
implicit representation is difficult because there are not sufficient surface
constraints in the representation. In NeuS, we propose to represent a surface
as the zero-level set of a signed distance function (SDF) and develop a new
volume rendering method to train a neural SDF representation. We observe that
the conventional volume rendering method causes inherent geometric errors (i.e.
bias) for surface reconstruction, and therefore propose a new formulation that
is free of bias in the first order of approximation, thus leading to more
accurate surface reconstruction even without the mask supervision. Experiments
on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the
state-of-the-arts in high-quality surface reconstruction, especially for
objects and scenes with complex structures and self-occlusion.";Peng Wang<author:sep>Lingjie Liu<author:sep>Yuan Liu<author:sep>Christian Theobalt<author:sep>Taku Komura<author:sep>Wenping Wang;http://arxiv.org/pdf/2106.10689v3;cs.CV;23 pages;nerf
2106.05264v1;http://arxiv.org/abs/2106.05264v1;2021-06-09;NeRF in detail: Learning to sample for view synthesis;"Neural radiance fields (NeRF) methods have demonstrated impressive novel view
synthesis performance. The core approach is to render individual rays by
querying a neural network at points sampled along the ray to obtain the density
and colour of the sampled points, and integrating this information using the
rendering equation. Since dense sampling is computationally prohibitive, a
common solution is to perform coarse-to-fine sampling.
  In this work we address a clear limitation of the vanilla coarse-to-fine
approach -- that it is based on a heuristic and not trained end-to-end for the
task at hand. We introduce a differentiable module that learns to propose
samples and their importance for the fine network, and consider and compare
multiple alternatives for its neural architecture. Training the proposal module
from scratch can be unstable due to lack of supervision, so an effective
pre-training strategy is also put forward. The approach, named `NeRF in detail'
(NeRF-ID), achieves superior view synthesis quality over NeRF and the
state-of-the-art on the synthetic Blender benchmark and on par or better
performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the
predicted sample importance, a 25% saving in computation can be achieved
without significantly sacrificing the rendering quality.";Relja ArandjeloviÄ<author:sep>Andrew Zisserman;http://arxiv.org/pdf/2106.05264v1;cs.CV;;nerf
2106.02019v2;http://arxiv.org/abs/2106.02019v2;2021-06-03;Neural Actor: Neural Free-view Synthesis of Human Actors with Pose  Control;"We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.";Lingjie Liu<author:sep>Marc Habermann<author:sep>Viktor Rudnev<author:sep>Kripasindhu Sarkar<author:sep>Jiatao Gu<author:sep>Christian Theobalt;http://arxiv.org/pdf/2106.02019v2;cs.CV;;
2106.01970v2;http://arxiv.org/abs/2106.01970v2;2021-06-03;NeRFactor: Neural Factorization of Shape and Reflectance Under an  Unknown Illumination;"We address the problem of recovering the shape and spatially-varying
reflectance of an object from multi-view images (and their camera poses) of an
object illuminated by one unknown lighting condition. This enables the
rendering of novel views of the object under arbitrary environment lighting and
editing of the object's material properties. The key to our approach, which we
call Neural Radiance Factorization (NeRFactor), is to distill the volumetric
geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020]
representation of the object into a surface representation and then jointly
refine the geometry while solving for the spatially-varying reflectance and
environment lighting. Specifically, NeRFactor recovers 3D neural fields of
surface normals, light visibility, albedo, and Bidirectional Reflectance
Distribution Functions (BRDFs) without any supervision, using only a
re-rendering loss, simple smoothness priors, and a data-driven BRDF prior
learned from real-world BRDF measurements. By explicitly modeling light
visibility, NeRFactor is able to separate shadows from albedo and synthesize
realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor
is able to recover convincing 3D models for free-viewpoint relighting in this
challenging and underconstrained capture setup for both synthetic and real
scenes. Qualitative and quantitative experiments show that NeRFactor
outperforms classic and deep learning-based state of the art across various
tasks. Our videos, code, and data are available at
people.csail.mit.edu/xiuming/projects/nerfactor/.";Xiuming Zhang<author:sep>Pratul P. Srinivasan<author:sep>Boyang Deng<author:sep>Paul Debevec<author:sep>William T. Freeman<author:sep>Jonathan T. Barron;http://arxiv.org/pdf/2106.01970v2;cs.CV;"Camera-ready version for SIGGRAPH Asia 2021. Project Page:
  https://people.csail.mit.edu/xiuming/projects/nerfactor/";nerf
2105.13016v3;http://arxiv.org/abs/2105.13016v3;2021-05-27;Stylizing 3D Scene via Implicit Representation and HyperNetwork;"In this work, we aim to address the 3D scene stylization problem - generating
stylized images of the scene at arbitrary novel view angles. A straightforward
solution is to combine existing novel view synthesis and image/video style
transfer approaches, which often leads to blurry results or inconsistent
appearance. Inspired by the high-quality results of the neural radiance fields
(NeRF) method, we propose a joint framework to directly render novel views with
the desired style. Our framework consists of two components: an implicit
representation of the 3D scene with the neural radiance fields model, and a
hypernetwork to transfer the style information into the scene representation.
In particular, our implicit representation model disentangles the scene into
the geometry and appearance branches, and the hypernetwork learns to predict
the parameters of the appearance branch from the reference style image. To
alleviate the training difficulties and memory burden, we propose a two-stage
training procedure and a patch sub-sampling approach to optimize the style and
content losses with the neural radiance fields model. After optimization, our
model is able to render consistent novel views at arbitrary view angles with
arbitrary style. Both quantitative evaluation and human subject study have
demonstrated that the proposed method generates faithful stylization results
with consistent appearance across different views.";Pei-Ze Chiang<author:sep>Meng-Shiun Tsai<author:sep>Hung-Yu Tseng<author:sep>Wei-sheng Lai<author:sep>Wei-Chen Chiu;http://arxiv.org/pdf/2105.13016v3;cs.CV;"Accepted to WACV2022; Project page:
  https://ztex08010518.github.io/3dstyletransfer/";nerf
2105.09103v1;http://arxiv.org/abs/2105.09103v1;2021-05-19;Recursive-NeRF: An Efficient and Dynamically Growing NeRF;"View synthesis methods using implicit continuous shape representations
learned from a set of images, such as the Neural Radiance Field (NeRF) method,
have gained increasing attention due to their high quality imagery and
scalability to high resolution. However, the heavy computation required by its
volumetric approach prevents NeRF from being useful in practice; minutes are
taken to render a single image of a few megapixels. Now, an image of a scene
can be rendered in a level-of-detail manner, so we posit that a complicated
region of the scene should be represented by a large neural network while a
small neural network is capable of encoding a simple region, enabling a balance
between efficiency and quality. Recursive-NeRF is our embodiment of this idea,
providing an efficient and adaptive rendering and training approach for NeRF.
The core of Recursive-NeRF learns uncertainties for query coordinates,
representing the quality of the predicted color and volumetric intensity at
each level. Only query coordinates with high uncertainties are forwarded to the
next level to a bigger neural network with a more powerful representational
capability. The final rendered image is a composition of results from neural
networks of all levels. Our evaluation on three public datasets shows that
Recursive-NeRF is more efficient than NeRF while providing state-of-the-art
quality. The code will be available at https://github.com/Gword/Recursive-NeRF.";Guo-Wei Yang<author:sep>Wen-Yang Zhou<author:sep>Hao-Yang Peng<author:sep>Dun Liang<author:sep>Tai-Jiang Mu<author:sep>Shi-Min Hu;http://arxiv.org/pdf/2105.09103v1;cs.CV;11 pages, 12 figures;nerf
2105.06466v2;http://arxiv.org/abs/2105.06466v2;2021-05-13;Editing Conditional Radiance Fields;"A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.";Steven Liu<author:sep>Xiuming Zhang<author:sep>Zhoutong Zhang<author:sep>Richard Zhang<author:sep>Jun-Yan Zhu<author:sep>Bryan Russell;http://arxiv.org/pdf/2105.06466v2;cs.CV;"Code: https://github.com/stevliu/editnerf Website:
  http://editnerf.csail.mit.edu/, v2 updated figure 8 and included additional
  details";nerf
2105.06468v1;http://arxiv.org/abs/2105.06468v1;2021-05-13;Dynamic View Synthesis from Dynamic Monocular Video;"We present an algorithm for generating novel views at arbitrary viewpoints
and any input time step given a monocular video of a dynamic scene. Our work
builds upon recent advances in neural implicit representation and uses
continuous and differentiable functions for modeling the time-varying structure
and the appearance of the scene. We jointly train a time-invariant static NeRF
and a time-varying dynamic NeRF, and learn how to blend the results in an
unsupervised manner. However, learning this implicit function from a single
video is highly ill-posed (with infinitely many solutions that match the input
video). To resolve the ambiguity, we introduce regularization losses to
encourage a more physically plausible solution. We show extensive quantitative
and qualitative results of dynamic view synthesis from casually captured
videos.";Chen Gao<author:sep>Ayush Saraf<author:sep>Johannes Kopf<author:sep>Jia-Bin Huang;http://arxiv.org/pdf/2105.06468v1;cs.CV;Project webpage: https://free-view-video.github.io/;nerf
2105.05994v1;http://arxiv.org/abs/2105.05994v1;2021-05-12;Neural Trajectory Fields for Dynamic Novel View Synthesis;"Recent approaches to render photorealistic views from a limited set of
photographs have pushed the boundaries of our interactions with pictures of
static scenes. The ability to recreate moments, that is, time-varying
sequences, is perhaps an even more interesting scenario, but it remains largely
unsolved. We introduce DCT-NeRF, a coordinatebased neural representation for
dynamic scenes. DCTNeRF learns smooth and stable trajectories over the input
sequence for each point in space. This allows us to enforce consistency between
any two frames in the sequence, which results in high quality reconstruction,
particularly in dynamic regions.";Chaoyang Wang<author:sep>Ben Eckart<author:sep>Simon Lucey<author:sep>Orazio Gallo;http://arxiv.org/pdf/2105.05994v1;cs.CV;;nerf
2105.06405v1;http://arxiv.org/abs/2105.06405v1;2021-05-11;Vision-based Neural Scene Representations for Spacecraft;"In advanced mission concepts with high levels of autonomy, spacecraft need to
internally model the pose and shape of nearby orbiting objects. Recent works in
neural scene representations show promising results for inferring generic
three-dimensional scenes from optical images. Neural Radiance Fields (NeRF)
have shown success in rendering highly specular surfaces using a large number
of images and their pose. More recently, Generative Radiance Fields (GRAF)
achieved full volumetric reconstruction of a scene from unposed images only,
thanks to the use of an adversarial framework to train a NeRF. In this paper,
we compare and evaluate the potential of NeRF and GRAF to render novel views
and extract the 3D shape of two different spacecraft, the Soil Moisture and
Ocean Salinity satellite of ESA's Living Planet Programme and a generic cube
sat. Considering the best performances of both models, we observe that NeRF has
the ability to render more accurate images regarding the material specularity
of the spacecraft and its pose. For its part, GRAF generates precise novel
views with accurate details even when parts of the satellites are shadowed
while having the significant advantage of not needing any information about the
relative pose.";Anne Mergy<author:sep>Gurvan Lecuyer<author:sep>Dawa Derksen<author:sep>Dario Izzo;http://arxiv.org/pdf/2105.06405v1;cs.CV;;nerf
2105.03120v1;http://arxiv.org/abs/2105.03120v1;2021-05-07;Neural 3D Scene Compression via Model Compression;"Rendering 3D scenes requires access to arbitrary viewpoints from the scene.
Storage of such a 3D scene can be done in two ways; (1) storing 2D images taken
from the 3D scene that can reconstruct the scene back through interpolations,
or (2) storing a representation of the 3D scene itself that already encodes
views from all directions. So far, traditional 3D compression methods have
focused on the first type of storage and compressed the original 2D images with
image compression techniques. With this approach, the user first decodes the
stored 2D images and then renders the 3D scene. However, this separated
procedure is inefficient since a large amount of 2D images have to be stored.
In this work, we take a different approach and compress a functional
representation of 3D scenes. In particular, we introduce a method to compress
3D scenes by compressing the neural networks that represent the scenes as
neural radiance fields. Our method provides more efficient storage of 3D scenes
since it does not store 2D images -- which are redundant when we render the
scene from the neural functional representation.";Berivan Isik;http://arxiv.org/pdf/2105.03120v1;cs.CV;Stanford CS 231A Final Project, 2021. WiCV at CVPR 2021;
2105.02872v2;http://arxiv.org/abs/2105.02872v2;2021-05-06;Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies;"This paper addresses the challenge of reconstructing an animatable human
model from a multi-view video. Some recent works have proposed to decompose a
non-rigidly deforming scene into a canonical neural radiance field and a set of
deformation fields that map observation-space points to the canonical space,
thereby enabling them to learn the dynamic scene from images. However, they
represent the deformation field as translational vector field or SE(3) field,
which makes the optimization highly under-constrained. Moreover, these
representations cannot be explicitly controlled by input motions. Instead, we
introduce neural blend weight fields to produce the deformation fields. Based
on the skeleton-driven deformation, blend weight fields are used with 3D human
skeletons to generate observation-to-canonical and canonical-to-observation
correspondences. Since 3D human skeletons are more observable, they can
regularize the learning of deformation fields. Moreover, the learned blend
weight fields can be combined with input skeletal motions to generate new
deformation fields to animate the human model. Experiments show that our
approach significantly outperforms recent human synthesis methods. The code and
supplementary materials are available at
https://zju3dv.github.io/animatable_nerf/.";Sida Peng<author:sep>Junting Dong<author:sep>Qianqian Wang<author:sep>Shangzhan Zhang<author:sep>Qing Shuai<author:sep>Xiaowei Zhou<author:sep>Hujun Bao;http://arxiv.org/pdf/2105.02872v2;cs.CV;"Accepted to ICCV 2021. The first two authors contributed equally to
  this paper. Project page: https://zju3dv.github.io/animatable_nerf/";nerf
2104.14786v1;http://arxiv.org/abs/2104.14786v1;2021-04-30;Editable Free-viewpoint Video Using a Layered Neural Representation;"Generating free-viewpoint videos is critical for immersive VR/AR experience
but recent neural advances still lack the editing ability to manipulate the
visual perception for large dynamic scenes. To fill this gap, in this paper we
propose the first approach for editable photo-realistic free-viewpoint video
generation for large-scale dynamic scenes using only sparse 16 cameras. The
core of our approach is a new layered neural representation, where each dynamic
entity including the environment itself is formulated into a space-time
coherent neural layered radiance representation called ST-NeRF. Such layered
representation supports fully perception and realistic manipulation of the
dynamic scene whilst still supporting a free viewing experience in a wide
range. In our ST-NeRF, the dynamic entity/layer is represented as continuous
functions, which achieves the disentanglement of location, deformation as well
as the appearance of the dynamic entity in a continuous and self-supervised
manner. We propose a scene parsing 4D label map tracking to disentangle the
spatial information explicitly, and a continuous deform module to disentangle
the temporal motion implicitly. An object-aware volume rendering scheme is
further introduced for the re-assembling of all the neural layers. We adopt a
novel layered loss and motion-aware ray sampling strategy to enable efficient
training for a large dynamic scene with multiple performers, Our framework
further enables a variety of editing functions, i.e., manipulating the scale
and location, duplicating or retiming individual neural layers to create
numerous visual effects while preserving high realism. Extensive experiments
demonstrate the effectiveness of our approach to achieve high-quality,
photo-realistic, and editable free-viewpoint video generation for dynamic
scenes.";Jiakai Zhang<author:sep>Xinhang Liu<author:sep>Xinyi Ye<author:sep>Fuqiang Zhao<author:sep>Yanshun Zhang<author:sep>Minye Wu<author:sep>Yingliang Zhang<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2104.14786v1;cs.CV;;nerf
2104.10078v2;http://arxiv.org/abs/2104.10078v2;2021-04-20;UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for  Multi-View Reconstruction;"Neural implicit 3D representations have emerged as a powerful paradigm for
reconstructing surfaces from multi-view images and synthesizing novel views.
Unfortunately, existing methods such as DVR or IDR require accurate per-pixel
object masks as supervision. At the same time, neural radiance fields have
revolutionized novel view synthesis. However, NeRF's estimated volume density
does not admit accurate surface reconstruction. Our key insight is that
implicit surface models and radiance fields can be formulated in a unified way,
enabling both surface and volume rendering using the same model. This unified
perspective enables novel, more efficient sampling procedures and the ability
to reconstruct accurate surfaces without input masks. We compare our method on
the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments
demonstrate that we outperform NeRF in terms of reconstruction quality while
performing on par with IDR without requiring masks.";Michael Oechsle<author:sep>Songyou Peng<author:sep>Andreas Geiger;http://arxiv.org/pdf/2104.10078v2;cs.CV;ICCV 2021 oral;nerf
2104.09877v1;http://arxiv.org/abs/2104.09877v1;2021-04-20;Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry;"We present a new generic method for shadow-aware multi-view satellite
photogrammetry of Earth Observation scenes. Our proposed method, the Shadow
Neural Radiance Field (S-NeRF) follows recent advances in implicit volumetric
representation learning. For each scene, we train S-NeRF using very high
spatial resolution optical images taken from known viewing angles. The learning
requires no labels or shape priors: it is self-supervised by an image
reconstruction loss. To accommodate for changing light source conditions both
from a directional light source (the Sun) and a diffuse light source (the sky),
we extend the NeRF approach in two ways. First, direct illumination from the
Sun is modeled via a local light source visibility field. Second, indirect
illumination from a diffuse light source is learned as a non-local color field
as a function of the position of the Sun. Quantitatively, the combination of
these factors reduces the altitude and color errors in shaded areas, compared
to NeRF. The S-NeRF methodology not only performs novel view synthesis and full
3D shape estimation, it also enables shadow detection, albedo synthesis, and
transient object filtering, without any explicit shape supervision.";Dawa Derksen<author:sep>Dario Izzo;http://arxiv.org/pdf/2104.09877v1;cs.CV;Accepted to CVPR2021 - EarthVision;nerf
2104.08418v1;http://arxiv.org/abs/2104.08418v1;2021-04-17;FiG-NeRF: Figure-Ground Neural Radiance Fields for 3D Object Category  Modelling;"We investigate the use of Neural Radiance Fields (NeRF) to learn high quality
3D object category models from collections of input images. In contrast to
previous work, we are able to do this whilst simultaneously separating
foreground objects from their varying backgrounds. We achieve this via a
2-component NeRF model, FiG-NeRF, that prefers explanation of the scene as a
geometrically constant background and a deformable foreground that represents
the object category. We show that this method can learn accurate 3D object
category models using only photometric supervision and casually captured images
of the objects. Additionally, our 2-part decomposition allows the model to
perform accurate and crisp amodal segmentation. We quantitatively evaluate our
method with view synthesis and image fidelity metrics, using synthetic,
lab-captured, and in-the-wild data. Our results demonstrate convincing 3D
object category modelling that exceed the performance of existing methods.";Christopher Xie<author:sep>Keunhong Park<author:sep>Ricardo Martin-Brualla<author:sep>Matthew Brown;http://arxiv.org/pdf/2104.08418v1;cs.CV;;nerf
2104.06935v1;http://arxiv.org/abs/2104.06935v1;2021-04-14;Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views  of Novel Scenes;"Recent neural view synthesis methods have achieved impressive quality and
realism, surpassing classical pipelines which rely on multi-view
reconstruction. State-of-the-Art methods, such as NeRF, are designed to learn a
single scene with a neural network and require dense multi-view inputs. Testing
on a new scene requires re-training from scratch, which takes 2-3 days. In this
work, we introduce Stereo Radiance Fields (SRF), a neural view synthesis
approach that is trained end-to-end, generalizes to new scenes, and requires
only sparse views at test time. The core idea is a neural architecture inspired
by classical multi-view stereo methods, which estimates surface points by
finding similar image regions in stereo images. In SRF, we predict color and
density for each 3D point given an encoding of its stereo correspondence in the
input images. The encoding is implicitly learned by an ensemble of pair-wise
similarities -- emulating classical stereo. Experiments show that SRF learns
structure instead of overfitting on a scene. We train on multiple scenes of the
DTU dataset and generalize to new ones without re-training, requiring only 10
sparse and spread-out views as input. We show that 10-15 minutes of fine-tuning
further improve the results, achieving significantly sharper, more detailed
results than scene-specific models. The code, model, and videos are available
at https://virtualhumans.mpi-inf.mpg.de/srf/.";Julian Chibane<author:sep>Aayush Bansal<author:sep>Verica Lazova<author:sep>Gerard Pons-Moll;http://arxiv.org/pdf/2104.06935v1;cs.CV;"IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2021";nerf
2104.06405v2;http://arxiv.org/abs/2104.06405v2;2021-04-13;BARF: Bundle-Adjusting Neural Radiance Fields;"Neural Radiance Fields (NeRF) have recently gained a surge of interest within
the computer vision community for its power to synthesize photorealistic novel
views of real-world scenes. One limitation of NeRF, however, is its requirement
of accurate camera poses to learn the scene representations. In this paper, we
propose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from
imperfect (or even unknown) camera poses -- the joint problem of learning
neural 3D representations and registering camera frames. We establish a
theoretical connection to classical image alignment and show that
coarse-to-fine registration is also applicable to NeRF. Furthermore, we show
that na\""ively applying positional encoding in NeRF has a negative impact on
registration with a synthesis-based objective. Experiments on synthetic and
real-world data show that BARF can effectively optimize the neural scene
representations and resolve large camera pose misalignment at the same time.
This enables view synthesis and localization of video sequences from unknown
camera poses, opening up new avenues for visual localization systems (e.g.
SLAM) and potential applications for dense 3D mapping and reconstruction.";Chen-Hsuan Lin<author:sep>Wei-Chiu Ma<author:sep>Antonio Torralba<author:sep>Simon Lucey;http://arxiv.org/pdf/2104.06405v2;cs.CV;"Accepted to ICCV 2021 as oral presentation (project page & code:
  https://chenhsuanlin.bitbucket.io/bundle-adjusting-NeRF)";nerf
2104.04532v3;http://arxiv.org/abs/2104.04532v3;2021-04-09;Neural RGB-D Surface Reconstruction;"Obtaining high-quality 3D reconstructions of room-scale scenes is of
paramount importance for upcoming applications in AR or VR. These range from
mixed reality applications for teleconferencing, virtual measuring, virtual
room planing, to robotic applications. While current volume-based view
synthesis methods that use neural radiance fields (NeRFs) show promising
results in reproducing the appearance of an object or scene, they do not
reconstruct an actual surface. The volumetric representation of the surface
based on densities leads to artifacts when a surface is extracted using
Marching Cubes, since during optimization, densities are accumulated along the
ray and are not used at a single sample point in isolation. Instead of this
volumetric representation of the surface, we propose to represent the surface
using an implicit function (truncated signed distance function). We show how to
incorporate this representation in the NeRF framework, and extend it to use
depth measurements from a commodity RGB-D sensor, such as a Kinect. In
addition, we propose a pose and camera refinement technique which improves the
overall reconstruction quality. In contrast to concurrent work on integrating
depth priors in NeRF which concentrates on novel view synthesis, our approach
is able to reconstruct high-quality, metrical 3D reconstructions.";Dejan AzinoviÄ<author:sep>Ricardo Martin-Brualla<author:sep>Dan B Goldman<author:sep>Matthias NieÃner<author:sep>Justus Thies;http://arxiv.org/pdf/2104.04532v3;cs.CV;"CVPR'22; Project page:
  https://dazinovic.github.io/neural-rgbd-surface-reconstruction/ Video:
  https://youtu.be/iWuSowPsC3g";nerf
2104.02607v2;http://arxiv.org/abs/2104.02607v2;2021-04-06;MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror  Catadioptric Imaging;"Photo-realistic neural reconstruction and rendering of the human portrait are
critical for numerous VR/AR applications. Still, existing solutions inherently
rely on multi-view capture settings, and the one-shot solution to get rid of
the tedious multi-view synchronization and calibration remains extremely
challenging. In this paper, we propose MirrorNeRF - a one-shot neural portrait
free-viewpoint rendering approach using a catadioptric imaging system with
multiple sphere mirrors and a single high-resolution digital camera, which is
the first to combine neural radiance field with catadioptric imaging so as to
enable one-shot photo-realistic human portrait reconstruction and rendering, in
a low-cost and casual capture setting. More specifically, we propose a
light-weight catadioptric system design with a sphere mirror array to enable
diverse ray sampling in the continuous 3D space as well as an effective online
calibration for the camera and the mirror array. Our catadioptric imaging
system can be easily deployed with a low budget and the casual capture ability
for convenient daily usages. We introduce a novel neural warping radiance field
representation to learn a continuous displacement field that implicitly
compensates for the misalignment due to our flexible system setting. We further
propose a density regularization scheme to leverage the inherent geometry
information from the catadioptric data in a self-supervision manner, which not
only improves the training efficiency but also provides more effective density
supervision for higher rendering quality. Extensive experiments demonstrate the
effectiveness and robustness of our scheme to achieve one-shot photo-realistic
and high-quality appearance free-viewpoint rendering for human portrait scenes.";Ziyu Wang<author:sep>Liao Wang<author:sep>Fuqiang Zhao<author:sep>Minye Wu<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2104.02607v2;cs.CV;;nerf
2104.01772v1;http://arxiv.org/abs/2104.01772v1;2021-04-05;Convolutional Neural Opacity Radiance Fields;"Photo-realistic modeling and rendering of fuzzy objects with complex opacity
are critical for numerous immersive VR/AR applications, but it suffers from
strong view-dependent brightness, color. In this paper, we propose a novel
scheme to generate opacity radiance fields with a convolutional neural renderer
for fuzzy objects, which is the first to combine both explicit opacity
supervision and convolutional mechanism into the neural radiance field
framework so as to enable high-quality appearance and global consistent alpha
mattes generation in arbitrary novel views. More specifically, we propose an
efficient sampling strategy along with both the camera rays and image plane,
which enables efficient radiance field sampling and learning in a patch-wise
manner, as well as a novel volumetric feature integration scheme that generates
per-patch hybrid feature embeddings to reconstruct the view-consistent
fine-detailed appearance and opacity output. We further adopt a patch-wise
adversarial training scheme to preserve both high-frequency appearance and
opacity details in a self-supervised framework. We also introduce an effective
multi-view image capture system to capture high-quality color and alpha maps
for challenging fuzzy objects. Extensive experiments on existing and our new
challenging fuzzy object dataset demonstrate that our method achieves
photo-realistic, globally consistent, and fined detailed appearance and opacity
free-viewpoint rendering for various fuzzy objects.";Haimin Luo<author:sep>Anpei Chen<author:sep>Qixuan Zhang<author:sep>Bai Pang<author:sep>Minye Wu<author:sep>Lan Xu<author:sep>Jingyi Yu;http://arxiv.org/pdf/2104.01772v1;cs.CV;;
2104.01148v1;http://arxiv.org/abs/2104.01148v1;2021-04-02;Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation;"We present ObSuRF, a method which turns a single image of a scene into a 3D
model represented as a set of Neural Radiance Fields (NeRFs), with each NeRF
corresponding to a different object. A single forward pass of an encoder
network outputs a set of latent vectors describing the objects in the scene.
These vectors are used independently to condition a NeRF decoder, defining the
geometry and appearance of each object. We make learning more computationally
efficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs
without explicit ray marching. After confirming that the model performs equal
or better than state of the art on three 2D image segmentation benchmarks, we
apply it to two multi-object 3D datasets: A multiview version of CLEVR, and a
novel dataset in which scenes are populated by ShapeNet models. We find that
after training ObSuRF on RGB-D views of training scenes, it is capable of not
only recovering the 3D geometry of a scene depicted in a single input image,
but also to segment it into objects, despite receiving no supervision in that
regard.";Karl Stelzner<author:sep>Kristian Kersting<author:sep>Adam R. Kosiorek;http://arxiv.org/pdf/2104.01148v1;cs.CV;"15 pages, 3 figures. For project page with videos, see
  http://stelzner.github.io/obsurf/";nerf
2104.00677v1;http://arxiv.org/abs/2104.00677v1;2021-04-01;Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis;"We present DietNeRF, a 3D neural scene representation estimated from a few
images. Neural Radiance Fields (NeRF) learn a continuous volumetric
representation of a scene through multi-view consistency, and can be rendered
from novel viewpoints by ray casting. While NeRF has an impressive ability to
reconstruct geometry and fine details given many images, up to 100 for
challenging 360{\deg} scenes, it often finds a degenerate solution to its image
reconstruction objective when only a few input views are available. To improve
few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic
consistency loss that encourages realistic renderings at novel poses. DietNeRF
is trained on individual scenes to (1) correctly render given input views from
the same pose, and (2) match high-level semantic attributes across different,
random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary
poses. We extract these semantics using a pre-trained visual encoder such as
CLIP, a Vision Transformer trained on hundreds of millions of diverse
single-view, 2D photographs mined from the web with natural language
supervision. In experiments, DietNeRF improves the perceptual quality of
few-shot view synthesis when learned from scratch, can render novel views with
as few as one observed image when pre-trained on a multi-view dataset, and
produces plausible completions of completely unobserved regions.";Ajay Jain<author:sep>Matthew Tancik<author:sep>Pieter Abbeel;http://arxiv.org/pdf/2104.00677v1;cs.CV;Project website: https://www.ajayj.com/dietnerf;nerf
2104.00587v1;http://arxiv.org/abs/2104.00587v1;2021-04-01;NeRF-VAE: A Geometry Aware 3D Scene Generative Model;"We propose NeRF-VAE, a 3D scene generative model that incorporates geometric
structure via NeRF and differentiable volume rendering. In contrast to NeRF,
our model takes into account shared structure across scenes, and is able to
infer the structure of a novel scene -- without the need to re-train -- using
amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts
previous generative models with convolution-based rendering which lacks
geometric structure. Our model is a VAE that learns a distribution over
radiance fields by conditioning them on a latent scene representation. We show
that, once trained, NeRF-VAE is able to infer and render
geometrically-consistent scenes from previously unseen 3D environments using
very few input images. We further demonstrate that NeRF-VAE generalizes well to
out-of-distribution cameras, while convolutional models do not. Finally, we
introduce and study an attention-based conditioning mechanism of NeRF-VAE's
decoder, which improves model performance.";Adam R. Kosiorek<author:sep>Heiko Strathmann<author:sep>Daniel Zoran<author:sep>Pol Moreno<author:sep>Rosalia Schneider<author:sep>SoÅa MokrÃ¡<author:sep>Danilo J. Rezende;http://arxiv.org/pdf/2104.00587v1;stat.ML;17 pages, 15 figures, under review;nerf
2103.17269v1;http://arxiv.org/abs/2103.17269v1;2021-03-31;CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields;"Tremendous progress in deep generative models has led to photorealistic image
synthesis. While achieving compelling results, most approaches operate in the
two-dimensional image domain, ignoring the three-dimensional nature of our
world. Several recent works therefore propose generative models which are
3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to
the image plane. This leads to impressive 3D consistency, but incorporating
such a bias comes at a price: the camera needs to be modeled as well. Current
approaches assume fixed intrinsics and a predefined prior over camera pose
ranges. As a result, parameter tuning is typically required for real-world
data, and results degrade if the data distribution is not matched. Our key
hypothesis is that learning a camera generator jointly with the image generator
leads to a more principled approach to 3D-aware image synthesis. Further, we
propose to decompose the scene into a background and foreground model, leading
to more efficient and disentangled scene representations. While training from
raw, unposed image collections, we learn a 3D- and camera-aware generative
model which faithfully recovers not only the image but also the camera data
distribution. At test time, our model generates images with explicit control
over the camera as well as the shape and appearance of the scene.";Michael Niemeyer<author:sep>Andreas Geiger;http://arxiv.org/pdf/2103.17269v1;cs.CV;;
2103.16365v2;http://arxiv.org/abs/2103.16365v2;2021-03-30;FoV-NeRF: Foveated Neural Radiance Fields for Virtual Reality;"Virtual Reality (VR) is becoming ubiquitous with the rise of consumer
displays and commercial VR platforms. Such displays require low latency and
high quality rendering of synthetic imagery with reduced compute overheads.
Recent advances in neural rendering showed promise of unlocking new
possibilities in 3D computer graphics via image-based representations of
virtual or physical environments. Specifically, the neural radiance fields
(NeRF) demonstrated that photo-realistic quality and continuous view changes of
3D scenes can be achieved without loss of view-dependent effects. While NeRF
can significantly benefit rendering for VR applications, it faces unique
challenges posed by high field-of-view, high resolution, and
stereoscopic/egocentric viewing, typically causing low quality and high latency
of the rendered images. In VR, this not only harms the interaction experience
but may also cause sickness. To tackle these problems toward
six-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first
gaze-contingent 3D neural representation and view synthesis method. We
incorporate the human psychophysics of visual- and stereo-acuity into an
egocentric neural representation of 3D scenery. We then jointly optimize the
latency/performance and visual quality while mutually bridging human perception
and neural scene synthesis to achieve perceptually high-quality immersive
interaction. We conducted both objective analysis and subjective studies to
evaluate the effectiveness of our approach. We find that our method
significantly reduces latency (up to 99% time reduction compared with NeRF)
without loss of high-fidelity rendering (perceptually identical to
full-resolution ground truth). The presented approach may serve as the first
step toward future VR/AR systems that capture, teleport, and visualize remote
environments in real-time.";Nianchen Deng<author:sep>Zhenyi He<author:sep>Jiannan Ye<author:sep>Budmonde Duinkharjav<author:sep>Praneeth Chakravarthula<author:sep>Xubo Yang<author:sep>Qi Sun;http://arxiv.org/pdf/2103.16365v2;cs.GR;9 pages;nerf
2103.15875v2;http://arxiv.org/abs/2103.15875v2;2021-03-29;In-Place Scene Labelling and Understanding with Implicit Scene  Representation;"Semantic labelling is highly correlated with geometry and radiance
reconstruction, as scene entities with similar shape and appearance are more
likely to come from similar classes. Recent implicit neural reconstruction
techniques are appealing as they do not require prior training data, but the
same fully self-supervised approach is not possible for semantics because
labels are human-defined properties.
  We extend neural radiance fields (NeRF) to jointly encode semantics with
appearance and geometry, so that complete and accurate 2D semantic labels can
be achieved using a small amount of in-place annotations specific to the scene.
The intrinsic multi-view consistency and smoothness of NeRF benefit semantics
by enabling sparse labels to efficiently propagate. We show the benefit of this
approach when labels are either sparse or very noisy in room-scale scenes. We
demonstrate its advantageous properties in various interesting applications
such as an efficient scene labelling tool, novel semantic view synthesis, label
denoising, super-resolution, label interpolation and multi-view semantic label
fusion in visual semantic mapping systems.";Shuaifeng Zhi<author:sep>Tristan Laidlow<author:sep>Stefan Leutenegger<author:sep>Andrew J. Davison;http://arxiv.org/pdf/2103.15875v2;cs.CV;"Camera ready version. To be published in Proceedings of IEEE
  International Conference on Computer Vision (ICCV 2021) as Oral Presentation.
  Project page with more videos: https://shuaifengzhi.com/Semantic-NeRF/";nerf
2103.15595v2;http://arxiv.org/abs/2103.15595v2;2021-03-29;MVSNeRF: Fast Generalizable Radiance Field Reconstruction from  Multi-View Stereo;"We present MVSNeRF, a novel neural rendering approach that can efficiently
reconstruct neural radiance fields for view synthesis. Unlike prior works on
neural radiance fields that consider per-scene optimization on densely captured
images, we propose a generic deep neural network that can reconstruct radiance
fields from only three nearby input views via fast network inference. Our
approach leverages plane-swept cost volumes (widely used in multi-view stereo)
for geometry-aware scene reasoning, and combines this with physically based
volume rendering for neural radiance field reconstruction. We train our network
on real objects in the DTU dataset, and test it on three different datasets to
evaluate its effectiveness and generalizability. Our approach can generalize
across scenes (even indoor scenes, completely different from our training
scenes of objects) and generate realistic view synthesis results using only
three input images, significantly outperforming concurrent works on
generalizable radiance field reconstruction. Moreover, if dense images are
captured, our estimated radiance field representation can be easily fine-tuned;
this leads to fast per-scene reconstruction with higher rendering quality and
substantially less optimization time than NeRF.";Anpei Chen<author:sep>Zexiang Xu<author:sep>Fuqiang Zhao<author:sep>Xiaoshuai Zhang<author:sep>Fanbo Xiang<author:sep>Jingyi Yu<author:sep>Hao Su;http://arxiv.org/pdf/2103.15595v2;cs.CV;"Project Page: https://apchenstu.github.io/mvsnerf/
  Code:https://github.com/apchenstu/mvsnerf";nerf
2103.15606v3;http://arxiv.org/abs/2103.15606v3;2021-03-29;GNeRF: GAN-based Neural Radiance Field without Posed Camera;"We introduce GNeRF, a framework to marry Generative Adversarial Networks
(GAN) with Neural Radiance Field (NeRF) reconstruction for the complex
scenarios with unknown and even randomly initialized camera poses. Recent
NeRF-based advances have gained popularity for remarkable realistic novel view
synthesis. However, most of them heavily rely on accurate camera poses
estimation, while few recent methods can only optimize the unknown camera poses
in roughly forward-facing scenes with relatively short camera trajectories and
require rough camera poses initialization. Differently, our GNeRF only utilizes
randomly initialized poses for complex outside-in scenarios. We propose a novel
two-phases end-to-end framework. The first phase takes the use of GANs into the
new realm for optimizing coarse camera poses and radiance fields jointly, while
the second phase refines them with additional photometric loss. We overcome
local minima using a hybrid and iterative optimization scheme. Extensive
experiments on a variety of synthetic and natural scenes demonstrate the
effectiveness of GNeRF. More impressively, our approach outperforms the
baselines favorably in those scenes with repeated patterns or even low textures
that are regarded as extremely challenging before.";Quan Meng<author:sep>Anpei Chen<author:sep>Haimin Luo<author:sep>Minye Wu<author:sep>Hao Su<author:sep>Lan Xu<author:sep>Xuming He<author:sep>Jingyi Yu;http://arxiv.org/pdf/2103.15606v3;cs.CV;ICCV 2021 (Oral);nerf
2103.14910v3;http://arxiv.org/abs/2103.14910v3;2021-03-27;MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis;"In this paper, we propose MINE to perform novel view synthesis and depth
estimation via dense 3D reconstruction from a single image. Our approach is a
continuous depth generalization of the Multiplane Images (MPI) by introducing
the NEural radiance fields (NeRF). Given a single image as input, MINE predicts
a 4-channel image (RGB and volume density) at arbitrary depth values to jointly
reconstruct the camera frustum and fill in occluded contents. The reconstructed
and inpainted frustum can then be easily rendered into novel RGB or depth views
using differentiable rendering. Extensive experiments on RealEstate10K, KITTI
and Flowers Light Fields show that our MINE outperforms state-of-the-art by a
large margin in novel view synthesis. We also achieve competitive results in
depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our
source code is available at https://github.com/vincentfung13/MINE";Jiaxin Li<author:sep>Zijian Feng<author:sep>Qi She<author:sep>Henghui Ding<author:sep>Changhu Wang<author:sep>Gim Hee Lee;http://arxiv.org/pdf/2103.14910v3;cs.CV;ICCV 2021. Main paper and supplementary materials;nerf
2103.14645v1;http://arxiv.org/abs/2103.14645v1;2021-03-26;Baking Neural Radiance Fields for Real-Time View Synthesis;"Neural volumetric representations such as Neural Radiance Fields (NeRF) have
emerged as a compelling technique for learning to represent 3D scenes from
images with the goal of rendering photorealistic images of the scene from
unobserved viewpoints. However, NeRF's computational requirements are
prohibitive for real-time applications: rendering views from a trained NeRF
requires querying a multilayer perceptron (MLP) hundreds of times per ray. We
present a method to train a NeRF, then precompute and store (i.e. ""bake"") it as
a novel representation called a Sparse Neural Radiance Grid (SNeRG) that
enables real-time rendering on commodity hardware. To achieve this, we
introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid
representation with learned feature vectors. The resulting scene representation
retains NeRF's ability to render fine geometric details and view-dependent
appearance, is compact (averaging less than 90 MB per scene), and can be
rendered in real-time (higher than 30 frames per second on a laptop GPU).
Actual screen captures are shown in our video.";Peter Hedman<author:sep>Pratul P. Srinivasan<author:sep>Ben Mildenhall<author:sep>Jonathan T. Barron<author:sep>Paul Debevec;http://arxiv.org/pdf/2103.14645v1;cs.CV;Project page: https://nerf.live;nerf
2103.13744v2;http://arxiv.org/abs/2103.13744v2;2021-03-25;KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs;"NeRF synthesizes novel views of a scene with unprecedented quality by fitting
a neural radiance field to RGB images. However, NeRF requires querying a deep
Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering
times, even on modern GPUs. In this paper, we demonstrate that real-time
rendering is possible by utilizing thousands of tiny MLPs instead of one single
large MLP. In our setting, each individual MLP only needs to represent parts of
the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining
this divide-and-conquer strategy with further optimizations, rendering is
accelerated by three orders of magnitude compared to the original NeRF model
without incurring high storage costs. Further, using teacher-student
distillation for training, we show that this speed-up can be achieved without
sacrificing visual quality.";Christian Reiser<author:sep>Songyou Peng<author:sep>Yiyi Liao<author:sep>Andreas Geiger;http://arxiv.org/pdf/2103.13744v2;cs.CV;"ICCV 2021. Code, pretrained models and an interactive viewer are
  available at https://github.com/creiser/kilonerf/";nerf
2103.14024v2;http://arxiv.org/abs/2103.14024v2;2021-03-25;PlenOctrees for Real-time Rendering of Neural Radiance Fields;"We introduce a method to render Neural Radiance Fields (NeRFs) in real time
using PlenOctrees, an octree-based 3D representation which supports
view-dependent effects. Our method can render 800x800 images at more than 150
FPS, which is over 3000 times faster than conventional NeRFs. We do so without
sacrificing quality while preserving the ability of NeRFs to perform
free-viewpoint rendering of scenes with arbitrary geometry and view-dependent
effects. Real-time performance is achieved by pre-tabulating the NeRF into a
PlenOctree. In order to preserve view-dependent effects such as specularities,
we factorize the appearance via closed-form spherical basis functions.
Specifically, we show that it is possible to train NeRFs to predict a spherical
harmonic representation of radiance, removing the viewing direction as an input
to the neural network. Furthermore, we show that PlenOctrees can be directly
optimized to further minimize the reconstruction loss, which leads to equal or
better quality compared to competing methods. Moreover, this octree
optimization step can be used to reduce the training time, as we no longer need
to wait for the NeRF training to converge fully. Our real-time neural rendering
approach may potentially enable new applications such as 6-DOF industrial and
product visualizations, as well as next generation AR/VR systems. PlenOctrees
are amenable to in-browser rendering as well; please visit the project page for
the interactive online demo, as well as video and code:
https://alexyu.net/plenoctrees";Alex Yu<author:sep>Ruilong Li<author:sep>Matthew Tancik<author:sep>Hao Li<author:sep>Ren Ng<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2103.14024v2;cs.CV;ICCV 2021 (Oral);nerf
2103.13415v3;http://arxiv.org/abs/2103.13415v3;2021-03-24;Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance  Fields;"The rendering procedure used by neural radiance fields (NeRF) samples a scene
with a single ray per pixel and may therefore produce renderings that are
excessively blurred or aliased when training or testing images observe scene
content at different resolutions. The straightforward solution of supersampling
by rendering with multiple rays per pixel is impractical for NeRF, because
rendering each ray requires querying a multilayer perceptron hundreds of times.
Our solution, which we call ""mip-NeRF"" (a la ""mipmap""), extends NeRF to
represent the scene at a continuously-valued scale. By efficiently rendering
anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable
aliasing artifacts and significantly improves NeRF's ability to represent fine
details, while also being 7% faster than NeRF and half the size. Compared to
NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with
NeRF and by 60% on a challenging multiscale variant of that dataset that we
present. Mip-NeRF is also able to match the accuracy of a brute-force
supersampled NeRF on our multiscale dataset while being 22x faster.";Jonathan T. Barron<author:sep>Ben Mildenhall<author:sep>Matthew Tancik<author:sep>Peter Hedman<author:sep>Ricardo Martin-Brualla<author:sep>Pratul P. Srinivasan;http://arxiv.org/pdf/2103.13415v3;cs.CV;;nerf
2103.12716v2;http://arxiv.org/abs/2103.12716v2;2021-03-23;UltraSR: Spatial Encoding is a Missing Key for Implicit Image  Function-based Arbitrary-Scale Super-Resolution;"The recent success of NeRF and other related implicit neural representation
methods has opened a new path for continuous image representation, where pixel
values no longer need to be looked up from stored discrete 2D arrays but can be
inferred from neural network models on a continuous spatial domain. Although
the recent work LIIF has demonstrated that such novel approaches can achieve
good performance on the arbitrary-scale super-resolution task, their upscaled
images frequently show structural distortion due to the inaccurate prediction
of high-frequency textures. In this work, we propose UltraSR, a simple yet
effective new network design based on implicit image functions in which we
deeply integrated spatial coordinates and periodic encoding with the implicit
neural representation. Through extensive experiments and ablation studies, we
show that spatial encoding is a missing key toward the next-stage
high-performing implicit image function. Our UltraSR sets new state-of-the-art
performance on the DIV2K benchmark under all super-resolution scales compared
to previous state-of-the-art methods. UltraSR also achieves superior
performance on other standard benchmark datasets in which it outperforms prior
works in almost all experiments.";Xingqian Xu<author:sep>Zhangyang Wang<author:sep>Humphrey Shi;http://arxiv.org/pdf/2103.12716v2;cs.CV;;nerf
2103.11078v3;http://arxiv.org/abs/2103.11078v3;2021-03-20;AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis;"Generating high-fidelity talking head video by fitting with the input audio
sequence is a challenging problem that receives considerable attentions
recently. In this paper, we address this problem with the aid of neural scene
representation networks. Our method is completely different from existing
methods that rely on intermediate representations like 2D landmarks or 3D face
models to bridge the gap between audio input and video output. Specifically,
the feature of input audio signal is directly fed into a conditional implicit
function to generate a dynamic neural radiance field, from which a
high-fidelity talking-head video corresponding to the audio signal is
synthesized using volume rendering. Another advantage of our framework is that
not only the head (with hair) region is synthesized as previous methods did,
but also the upper body is generated via two individual neural radiance fields.
Experimental results demonstrate that our novel framework can (1) produce
high-fidelity and natural results, and (2) support free adjustment of audio
signals, viewing directions, and background images. Code is available at
https://github.com/YudongGuo/AD-NeRF.";Yudong Guo<author:sep>Keyu Chen<author:sep>Sen Liang<author:sep>Yong-Jin Liu<author:sep>Hujun Bao<author:sep>Juyong Zhang;http://arxiv.org/pdf/2103.11078v3;cs.CV;"Project: https://yudongguo.github.io/ADNeRF/ Code:
  https://github.com/YudongGuo/AD-NeRF";nerf
2103.10380v2;http://arxiv.org/abs/2103.10380v2;2021-03-18;FastNeRF: High-Fidelity Neural Rendering at 200FPS;"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can
be used to encode complex 3D environments that can be rendered
photorealistically from novel viewpoints. Rendering these images is very
computationally demanding and recent improvements are still a long way from
enabling interactive rates, even on high-end hardware. Motivated by scenarios
on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based
system capable of rendering high fidelity photorealistic images at 200Hz on a
high-end consumer GPU. The core of our method is a graphics-inspired
factorization that allows for (i) compactly caching a deep radiance map at each
position in space, (ii) efficiently querying that map using ray directions to
estimate the pixel values in the rendered image. Extensive experiments show
that the proposed method is 3000 times faster than the original NeRF algorithm
and at least an order of magnitude faster than existing work on accelerating
NeRF, while maintaining visual quality and extensibility.";Stephan J. Garbin<author:sep>Marek Kowalski<author:sep>Matthew Johnson<author:sep>Jamie Shotton<author:sep>Julien Valentin;http://arxiv.org/pdf/2103.10380v2;cs.CV;"main paper: 10 pages, 6 figures; supplementary: 10 pages, 17 figures";nerf
2103.03231v4;http://arxiv.org/abs/2103.03231v4;2021-03-04;DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields  using Depth Oracle Networks;"The recent research explosion around implicit neural representations, such as
NeRF, shows that there is immense potential for implicitly storing high-quality
scene and lighting information in compact neural networks. However, one major
limitation preventing the use of NeRF in real-time rendering applications is
the prohibitive computational cost of excessive network evaluations along each
view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural
representations closer to practical rendering of synthetic content in real-time
applications, such as games and virtual reality. We show that the number of
samples required for each view ray can be significantly reduced when samples
are placed around surfaces in the scene without compromising image quality. To
this end, we propose a depth oracle network that predicts ray sample locations
for each view ray with a single network evaluation. We show that using a
classification network around logarithmically discretized and spherically
warped depth values is essential to encode surface locations rather than
directly estimating depth. The combination of these techniques leads to DONeRF,
our compact dual network design with a depth oracle network as its first step
and a locally sampled shading network for ray accumulation. With DONeRF, we
reduce the inference costs by up to 48x compared to NeRF when conditioning on
available ground truth depth information. Compared to concurrent acceleration
methods for raymarching-based neural representations, DONeRF does not require
additional memory for explicit caching or acceleration structures, and can
render interactively (20 frames per second) on a single GPU.";Thomas Neff<author:sep>Pascal Stadlbauer<author:sep>Mathias Parger<author:sep>Andreas Kurz<author:sep>Joerg H. Mueller<author:sep>Chakravarty R. Alla Chaitanya<author:sep>Anton Kaplanyan<author:sep>Markus Steinberger;http://arxiv.org/pdf/2103.03231v4;cs.CV;"Accepted to EGSR 2021 in the CGF track; Project website:
  https://depthoraclenerf.github.io/";nerf
2103.02597v2;http://arxiv.org/abs/2103.02597v2;2021-03-03;Neural 3D Video Synthesis from Multi-view Video;"We propose a novel approach for 3D video synthesis that is able to represent
multi-view video recordings of a dynamic real-world scene in a compact, yet
expressive representation that enables high-quality view synthesis and motion
interpolation. Our approach takes the high quality and compactness of static
neural radiance fields in a new direction: to a model-free, dynamic setting. At
the core of our approach is a novel time-conditioned neural radiance field that
represents scene dynamics using a set of compact latent codes. We are able to
significantly boost the training speed and perceptual quality of the generated
imagery by a novel hierarchical training scheme in combination with ray
importance sampling. Our learned representation is highly compact and able to
represent a 10 second 30 FPS multiview video recording by 18 cameras with a
model size of only 28MB. We demonstrate that our method can render
high-fidelity wide-angle novel views at over 1K resolution, even for complex
and dynamic scenes. We perform an extensive qualitative and quantitative
evaluation that shows that our approach outperforms the state of the art.
Project website: https://neural-3d-video.github.io/.";Tianye Li<author:sep>Mira Slavcheva<author:sep>Michael Zollhoefer<author:sep>Simon Green<author:sep>Christoph Lassner<author:sep>Changil Kim<author:sep>Tanner Schmidt<author:sep>Steven Lovegrove<author:sep>Michael Goesele<author:sep>Richard Newcombe<author:sep>Zhaoyang Lv;http://arxiv.org/pdf/2103.02597v2;cs.CV;"Accepted as an oral presentation for CVPR 2022. Project website:
  https://neural-3d-video.github.io/";
2103.01954v2;http://arxiv.org/abs/2103.01954v2;2021-03-02;Mixture of Volumetric Primitives for Efficient Neural Rendering;"Real-time rendering and animation of humans is a core function in games,
movies, and telepresence applications. Existing methods have a number of
drawbacks we aim to address with our work. Triangle meshes have difficulty
modeling thin structures like hair, volumetric representations like Neural
Volumes are too low-resolution given a reasonable memory budget, and
high-resolution implicit representations like Neural Radiance Fields are too
slow for use in real-time applications. We present Mixture of Volumetric
Primitives (MVP), a representation for rendering dynamic 3D content that
combines the completeness of volumetric representations with the efficiency of
primitive-based rendering, e.g., point-based or mesh-based methods. Our
approach achieves this by leveraging spatially shared computation with a
deconvolutional architecture and by minimizing computation in empty regions of
space with volumetric primitives that can move to cover only occupied regions.
Our parameterization supports the integration of correspondence and tracking
constraints, while being robust to areas where classical tracking fails, such
as around thin or translucent structures and areas with large topological
variability. MVP is a hybrid that generalizes both volumetric and
primitive-based representations. Through a series of extensive experiments we
demonstrate that it inherits the strengths of each, while avoiding many of
their limitations. We also compare our approach to several state-of-the-art
methods and demonstrate that MVP produces superior results in terms of quality
and runtime performance.";Stephen Lombardi<author:sep>Tomas Simon<author:sep>Gabriel Schwartz<author:sep>Michael Zollhoefer<author:sep>Yaser Sheikh<author:sep>Jason Saragih;http://arxiv.org/pdf/2103.01954v2;cs.GR;"13 pages; SIGGRAPH 2021";
2102.07064v4;http://arxiv.org/abs/2102.07064v4;2021-02-14;NeRF--: Neural Radiance Fields Without Known Camera Parameters;"Considering the problem of novel view synthesis (NVS) from only a set of 2D
images, we simplify the training process of Neural Radiance Field (NeRF) on
forward-facing scenes by removing the requirement of known or pre-computed
camera parameters, including both intrinsics and 6DoF poses. To this end, we
propose NeRF$--$, with three contributions: First, we show that the camera
parameters can be jointly optimised as learnable parameters with NeRF training,
through a photometric reconstruction; Second, to benchmark the camera parameter
estimation and the quality of novel view renderings, we introduce a new dataset
of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset
(BLEFF); Third, we conduct extensive analyses to understand the training
behaviours under various camera motions, and show that in most scenarios, the
joint optimisation pipeline can recover accurate camera parameters and achieve
comparable novel view synthesis quality as those trained with COLMAP
pre-computed camera parameters. Our code and data are available at
https://nerfmm.active.vision.";Zirui Wang<author:sep>Shangzhe Wu<author:sep>Weidi Xie<author:sep>Min Chen<author:sep>Victor Adrian Prisacariu;http://arxiv.org/pdf/2102.07064v4;cs.CV;"Project page see https://nerfmm.active.vision. Add a break point
  analysis experiment and release a BLEFF dataset";nerf
2102.06199v3;http://arxiv.org/abs/2102.06199v3;2021-02-11;A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape,  Appearance, and Pose;"While deep learning reshaped the classical motion capture pipeline with
feed-forward networks, generative models are required to recover fine alignment
via iterative refinement. Unfortunately, the existing models are usually
hand-crafted or learned in controlled conditions, only applicable to limited
domains. We propose a method to learn a generative neural body model from
unlabelled monocular videos by extending Neural Radiance Fields (NeRFs). We
equip them with a skeleton to apply to time-varying and articulated motion. A
key insight is that implicit models require the inverse of the forward
kinematics used in explicit surface models. Our reparameterization defines
spatial latent variables relative to the pose of body parts and thereby
overcomes ill-posed inverse operations with an overparameterization. This
enables learning volumetric body shape and appearance from scratch while
jointly refining the articulated pose; all without ground truth labels for
appearance, pose, or 3D shape on the input videos. When used for
novel-view-synthesis and motion capture, our neural model improves accuracy on
diverse datasets. Project website: https://lemonatsu.github.io/anerf/ .";Shih-Yang Su<author:sep>Frank Yu<author:sep>Michael Zollhoefer<author:sep>Helge Rhodin;http://arxiv.org/pdf/2102.06199v3;cs.CV;NeurIPS 2021. Project website: https://lemonatsu.github.io/anerf/;nerf
2102.04281v3;http://arxiv.org/abs/2102.04281v3;2021-02-08;Conditions de Kan sur les nerfs des $Ï$-catÃ©gories;"We show that the Street nerve of a strict $\omega$-category $C$ is a Kan
complex (respectively a quasi-category) if and only if the $n$-cells of $C$ for
$n\geq 1$ (respectively $n> 1$) are weakly invertible. Moreover, we equip
$\mathcal{N}(C)$ with a structure of saturated complicial set where the
$n$-simplices correspond to morphisms from the $n^{th}$ oriental to $C$ sending
the unique non-trivial $n$-cell of the domain to a weakly invertible cell of
$C$.";FÃ©lix Loubaton;http://arxiv.org/pdf/2102.04281v3;math.CT;52 pages, in French;nerf
2101.02697v1;http://arxiv.org/abs/2101.02697v1;2021-01-07;PVA: Pixel-aligned Volumetric Avatars;"Acquisition and rendering of photo-realistic human heads is a highly
challenging research problem of particular importance for virtual telepresence.
Currently, the highest quality is achieved by volumetric approaches trained in
a person specific manner on multi-view data. These models better represent fine
structure, such as hair, compared to simpler mesh-based models. Volumetric
models typically employ a global code to represent facial expressions, such
that they can be driven by a small set of animation parameters. While such
architectures achieve impressive rendering quality, they can not easily be
extended to the multi-identity setting. In this paper, we devise a novel
approach for predicting volumetric avatars of the human head given just a small
number of inputs. We enable generalization across identities by a novel
parameterization that combines neural radiance fields with local, pixel-aligned
features extracted directly from the inputs, thus sidestepping the need for
very deep or complex networks. Our approach is trained in an end-to-end manner
solely based on a photometric re-rendering loss without requiring explicit 3D
supervision.We demonstrate that our approach outperforms the existing state of
the art in terms of quality and is able to generate faithful facial expressions
in a multi-identity setting.";Amit Raj<author:sep>Michael Zollhoefer<author:sep>Tomas Simon<author:sep>Jason Saragih<author:sep>Shunsuke Saito<author:sep>James Hays<author:sep>Stephen Lombardi;http://arxiv.org/pdf/2101.02697v1;cs.CV;Project page located at https://volumetric-avatars.github.io/;
2101.00373v3;http://arxiv.org/abs/2101.00373v3;2021-01-02;Non-line-of-Sight Imaging via Neural Transient Fields;"We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging.
Previous solutions have sought to explicitly recover the 3D geometry (e.g., as
point clouds) or voxel density (e.g., within a pre-defined volume) of the
hidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF)
approach, we use a multi-layer perceptron (MLP) to represent the neural
transient field or NeTF. However, NeTF measures the transient over spherical
wavefronts rather than the radiance along lines. We therefore formulate a
spherical volume NeTF reconstruction pipeline, applicable to both confocal and
non-confocal setups. Compared with NeRF, NeTF samples a much sparser set of
viewpoints (scanning spots) and the sampling is highly uneven. We thus
introduce a Monte Carlo technique to improve the robustness in the
reconstruction. Comprehensive experiments on synthetic and real datasets
demonstrate NeTF provides higher quality reconstruction and preserves fine
details largely missing in the state-of-the-art.";Siyuan Shen<author:sep>Zi Wang<author:sep>Ping Liu<author:sep>Zhengqing Pan<author:sep>Ruiqian Li<author:sep>Tian Gao<author:sep>Shiying Li<author:sep>Jingyi Yu;http://arxiv.org/pdf/2101.00373v3;eess.IV;;nerf
2101.01602v1;http://arxiv.org/abs/2101.01602v1;2020-12-22;STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in  Motion with Neural Rendering;"We present STaR, a novel method that performs Self-supervised Tracking and
Reconstruction of dynamic scenes with rigid motion from multi-view RGB videos
without any manual annotation. Recent work has shown that neural networks are
surprisingly effective at the task of compressing many views of a scene into a
learned function which maps from a viewing ray to an observed radiance value
via volume rendering. Unfortunately, these methods lose all their predictive
power once any object in the scene has moved. In this work, we explicitly model
rigid motion of objects in the context of neural representations of radiance
fields. We show that without any additional human specified supervision, we can
reconstruct a dynamic scene with a single rigid object in motion by
simultaneously decomposing it into its two constituent parts and encoding each
with its own neural representation. We achieve this by jointly optimizing the
parameters of two neural radiance fields and a set of rigid poses which align
the two fields at each frame. On both synthetic and real world datasets, we
demonstrate that our method can render photorealistic novel views, where
novelty is measured on both spatial and temporal axes. Our factored
representation furthermore enables animation of unseen object motion.";Wentao Yuan<author:sep>Zhaoyang Lv<author:sep>Tanner Schmidt<author:sep>Steven Lovegrove;http://arxiv.org/pdf/2101.01602v1;cs.CV;;
2012.12247v4;http://arxiv.org/abs/2012.12247v4;2020-12-22;Non-Rigid Neural Radiance Fields: Reconstruction and Novel View  Synthesis of a Dynamic Scene From Monocular Video;"We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and
novel view synthesis approach for general non-rigid dynamic scenes. Our
approach takes RGB images of a dynamic scene as input (e.g., from a monocular
video recording), and creates a high-quality space-time geometry and appearance
representation. We show that a single handheld consumer-grade camera is
sufficient to synthesize sophisticated renderings of a dynamic scene from novel
virtual camera views, e.g. a `bullet-time' video effect. NR-NeRF disentangles
the dynamic scene into a canonical volume and its deformation. Scene
deformation is implemented as ray bending, where straight rays are deformed
non-rigidly. We also propose a novel rigidity network to better constrain rigid
regions of the scene, leading to more stable results. The ray bending and
rigidity network are trained without explicit supervision. Our formulation
enables dense correspondence estimation across views and time, and compelling
video editing applications such as motion exaggeration. Our code will be open
sourced.";Edgar Tretschk<author:sep>Ayush Tewari<author:sep>Vladislav Golyanik<author:sep>Michael ZollhÃ¶fer<author:sep>Christoph Lassner<author:sep>Christian Theobalt;http://arxiv.org/pdf/2012.12247v4;cs.CV;"Project page (incl. supplemental videos and code):
  https://vcai.mpi-inf.mpg.de/projects/nonrigid_nerf/ or
  https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/";nerf
2012.09955v1;http://arxiv.org/abs/2012.09955v1;2020-12-17;Learning Compositional Radiance Fields of Dynamic Human Heads;"Photorealistic rendering of dynamic humans is an important ability for
telepresence systems, virtual shopping, synthetic data generation, and more.
Recently, neural rendering methods, which combine techniques from computer
graphics and machine learning, have created high-fidelity models of humans and
objects. Some of these methods do not produce results with high-enough fidelity
for driveable human models (Neural Volumes) whereas others have extremely long
rendering times (NeRF). We propose a novel compositional 3D representation that
combines the best of previous methods to produce both higher-resolution and
faster results. Our representation bridges the gap between discrete and
continuous volumetric representations by combining a coarse 3D-structure-aware
grid of animation codes with a continuous learned scene function that maps
every position and its corresponding local animation code to its view-dependent
emitted radiance and local volume density. Differentiable volume rendering is
employed to compute photo-realistic novel views of the human head and upper
body as well as to train our novel representation end-to-end using only 2D
supervision. In addition, we show that the learned dynamic radiance field can
be used to synthesize novel unseen expressions based on a global animation
code. Our approach achieves state-of-the-art results for synthesizing novel
views of dynamic human heads and the upper body.";Ziyan Wang<author:sep>Timur Bagautdinov<author:sep>Stephen Lombardi<author:sep>Tomas Simon<author:sep>Jason Saragih<author:sep>Jessica Hodgins<author:sep>Michael ZollhÃ¶fer;http://arxiv.org/pdf/2012.09955v1;cs.CV;;nerf
2101.05204v2;http://arxiv.org/abs/2101.05204v2;2020-12-17;Neural Volume Rendering: NeRF And Beyond;"Besides the COVID-19 pandemic and political upheaval in the US, 2020 was also
the year in which neural volume rendering exploded onto the scene, triggered by
the impressive NeRF paper by Mildenhall et al. (2020). Both of us have tried to
capture this excitement, Frank on a blog post (Dellaert, 2020) and Yen-Chen in
a Github collection (Yen-Chen, 2020). This note is an annotated bibliography of
the relevant papers, and we posted the associated bibtex file on the
repository.";Frank Dellaert<author:sep>Lin Yen-Chen;http://arxiv.org/pdf/2101.05204v2;cs.CV;"Blog: https://dellaert.github.io/NeRF/ Bibtex:
  https://github.com/yenchenlin/awesome-NeRF";nerf
2012.08503v1;http://arxiv.org/abs/2012.08503v1;2020-12-15;Object-Centric Neural Scene Rendering;"We present a method for composing photorealistic scenes from captured images
of objects. Our work builds upon neural radiance fields (NeRFs), which
implicitly model the volumetric density and directionally-emitted radiance of a
scene. While NeRFs synthesize realistic pictures, they only model static scenes
and are closely tied to specific imaging conditions. This property makes NeRFs
hard to generalize to new scenarios, including new lighting or new arrangements
of objects. Instead of learning a scene radiance field as a NeRF does, we
propose to learn object-centric neural scattering functions (OSFs), a
representation that models per-object light transport implicitly using a
lighting- and view-dependent neural network. This enables rendering scenes even
when objects or lights move, without retraining. Combined with a volumetric
path tracing procedure, our framework is capable of rendering both intra- and
inter-object light transport effects including occlusions, specularities,
shadows, and indirect illumination. We evaluate our approach on scene
composition and show that it generalizes to novel illumination conditions,
producing photorealistic, physically accurate renderings of multi-object
scenes.";Michelle Guo<author:sep>Alireza Fathi<author:sep>Jiajun Wu<author:sep>Thomas Funkhouser;http://arxiv.org/pdf/2012.08503v1;cs.CV;"Summary Video: https://youtu.be/NtR7xgxSL1U Project Webpage:
  https://shellguo.com/osf";nerf
2012.05903v2;http://arxiv.org/abs/2012.05903v2;2020-12-10;Portrait Neural Radiance Fields from a Single Image;"We present a method for estimating Neural Radiance Fields (NeRF) from a
single headshot portrait. While NeRF has demonstrated high-quality view
synthesis, it requires multiple images of static scenes and thus impractical
for casual captures and moving subjects. In this work, we propose to pretrain
the weights of a multilayer perceptron (MLP), which implicitly models the
volumetric density and colors, with a meta-learning framework using a light
stage portrait dataset. To improve the generalization to unseen faces, we train
the MLP in the canonical coordinate space approximated by 3D face morphable
models. We quantitatively evaluate the method using controlled captures and
demonstrate the generalization to real portrait images, showing favorable
results against state-of-the-arts.";Chen Gao<author:sep>Yichang Shih<author:sep>Wei-Sheng Lai<author:sep>Chia-Kai Liang<author:sep>Jia-Bin Huang;http://arxiv.org/pdf/2012.05903v2;cs.CV;Project webpage: https://portrait-nerf.github.io/;nerf
2012.05877v3;http://arxiv.org/abs/2012.05877v3;2020-12-10;INeRF: Inverting Neural Radiance Fields for Pose Estimation;"We present iNeRF, a framework that performs mesh-free pose estimation by
""inverting"" a Neural RadianceField (NeRF). NeRFs have been shown to be
remarkably effective for the task of view synthesis - synthesizing
photorealistic novel views of real-world scenes or objects. In this work, we
investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,
RGB-only 6DoF pose estimation - given an image, find the translation and
rotation of a camera relative to a 3D object or scene. Our method assumes that
no object mesh models are available during either training or test time.
Starting from an initial pose estimate, we use gradient descent to minimize the
residual between pixels rendered from a NeRF and pixels in an observed image.
In our experiments, we first study 1) how to sample rays during pose refinement
for iNeRF to collect informative gradients and 2) how different batch sizes of
rays affect iNeRF on a synthetic dataset. We then show that for complex
real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating
the camera poses of novel images and using these images as additional training
data for NeRF. Finally, we show iNeRF can perform category-level object pose
estimation, including object instances not seen during training, with RGB
images by inverting a NeRF model inferred from a single view.";Lin Yen-Chen<author:sep>Pete Florence<author:sep>Jonathan T. Barron<author:sep>Alberto Rodriguez<author:sep>Phillip Isola<author:sep>Tsung-Yi Lin;http://arxiv.org/pdf/2012.05877v3;cs.CV;IROS 2021, Website: http://yenchenlin.me/inerf/;nerf
2012.03065v1;http://arxiv.org/abs/2012.03065v1;2020-12-05;Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar  Reconstruction;"We present dynamic neural radiance fields for modeling the appearance and
dynamics of a human face. Digitally modeling and reconstructing a talking human
is a key building-block for a variety of applications. Especially, for
telepresence applications in AR or VR, a faithful reproduction of the
appearance including novel viewpoints or head-poses is required. In contrast to
state-of-the-art approaches that model the geometry and material properties
explicitly, or are purely image-based, we introduce an implicit representation
of the head based on scene representation networks. To handle the dynamics of
the face, we combine our scene representation network with a low-dimensional
morphable model which provides explicit control over pose and expressions. We
use volumetric rendering to generate images from this hybrid representation and
demonstrate that such a dynamic neural scene representation can be learned from
monocular input data only, without the need of a specialized capture setup. In
our experiments, we show that this learned volumetric representation allows for
photo-realistic image generation that surpasses the quality of state-of-the-art
video-based reenactment methods.";Guy Gafni<author:sep>Justus Thies<author:sep>Michael ZollhÃ¶fer<author:sep>Matthias NieÃner;http://arxiv.org/pdf/2012.03065v1;cs.CV;"Video: https://youtu.be/m7oROLdQnjk | Project page:
  https://gafniguy.github.io/4D-Facial-Avatars/";
2012.02190v3;http://arxiv.org/abs/2012.02190v3;2020-12-03;pixelNeRF: Neural Radiance Fields from One or Few Images;"We propose pixelNeRF, a learning framework that predicts a continuous neural
scene representation conditioned on one or few input images. The existing
approach for constructing neural radiance fields involves optimizing the
representation to every scene independently, requiring many calibrated views
and significant compute time. We take a step towards resolving these
shortcomings by introducing an architecture that conditions a NeRF on image
inputs in a fully convolutional manner. This allows the network to be trained
across multiple scenes to learn a scene prior, enabling it to perform novel
view synthesis in a feed-forward manner from a sparse set of views (as few as
one). Leveraging the volume rendering approach of NeRF, our model can be
trained directly from images with no explicit 3D supervision. We conduct
extensive experiments on ShapeNet benchmarks for single image novel view
synthesis tasks with held-out objects as well as entire unseen categories. We
further demonstrate the flexibility of pixelNeRF by demonstrating it on
multi-object ShapeNet scenes and real scenes from the DTU dataset. In all
cases, pixelNeRF outperforms current state-of-the-art baselines for novel view
synthesis and single image 3D reconstruction. For the video and code, please
visit the project website: https://alexyu.net/pixelnerf";Alex Yu<author:sep>Vickie Ye<author:sep>Matthew Tancik<author:sep>Angjoo Kanazawa;http://arxiv.org/pdf/2012.02190v3;cs.CV;CVPR 2021;nerf
2011.13961v1;http://arxiv.org/abs/2011.13961v1;2020-11-27;D-NeRF: Neural Radiance Fields for Dynamic Scenes;"Neural rendering techniques combining machine learning with geometric
reasoning have arisen as one of the most promising approaches for synthesizing
novel views of a scene from a sparse set of images. Among these, stands out the
Neural radiance fields (NeRF), which trains a deep network to map 5D input
coordinates (representing spatial location and viewing direction) into a volume
density and view-dependent emitted radiance. However, despite achieving an
unprecedented level of photorealism on the generated images, NeRF is only
applicable to static scenes, where the same spatial location can be queried
from different images. In this paper we introduce D-NeRF, a method that extends
neural radiance fields to a dynamic domain, allowing to reconstruct and render
novel images of objects under rigid and non-rigid motions from a \emph{single}
camera moving around the scene. For this purpose we consider time as an
additional input to the system, and split the learning process in two main
stages: one that encodes the scene into a canonical space and another that maps
this canonical representation into the deformed scene at a particular time.
Both mappings are simultaneously learned using fully-connected networks. Once
the networks are trained, D-NeRF can render novel images, controlling both the
camera view and the time variable, and thus, the object movement. We
demonstrate the effectiveness of our approach on scenes with objects under
rigid, articulated and non-rigid motions. Code, model weights and the dynamic
scenes dataset will be released.";Albert Pumarola<author:sep>Enric Corona<author:sep>Gerard Pons-Moll<author:sep>Francesc Moreno-Noguer;http://arxiv.org/pdf/2011.13961v1;cs.CV;;nerf
2011.12948v5;http://arxiv.org/abs/2011.12948v5;2020-11-25;Nerfies: Deformable Neural Radiance Fields;"We present the first method capable of photorealistically reconstructing
deformable scenes using photos/videos captured casually from mobile phones. Our
approach augments neural radiance fields (NeRF) by optimizing an additional
continuous volumetric deformation field that warps each observed point into a
canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone
to local minima, and propose a coarse-to-fine optimization method for
coordinate-based models that allows for more robust optimization. By adapting
principles from geometry processing and physical simulation to NeRF-like
models, we propose an elastic regularization of the deformation field that
further improves robustness. We show that our method can turn casually captured
selfie photos/videos into deformable NeRF models that allow for photorealistic
renderings of the subject from arbitrary viewpoints, which we dub ""nerfies."" We
evaluate our method by collecting time-synchronized data using a rig with two
mobile phones, yielding train/validation images of the same pose at different
viewpoints. We show that our method faithfully reconstructs non-rigidly
deforming scenes and reproduces unseen views with high fidelity.";Keunhong Park<author:sep>Utkarsh Sinha<author:sep>Jonathan T. Barron<author:sep>Sofien Bouaziz<author:sep>Dan B Goldman<author:sep>Steven M. Seitz<author:sep>Ricardo Martin-Brualla;http://arxiv.org/pdf/2011.12948v5;cs.CV;ICCV 2021, Project page with videos: https://nerfies.github.io/;nerf
2011.12490v1;http://arxiv.org/abs/2011.12490v1;2020-11-25;DeRF: Decomposed Radiance Fields;"With the advent of Neural Radiance Fields (NeRF), neural networks can now
render novel views of a 3D scene with quality that fools the human eye. Yet,
generating these images is very computationally intensive, limiting their
applicability in practical scenarios. In this paper, we propose a technique
based on spatial decomposition capable of mitigating this issue. Our key
observation is that there are diminishing returns in employing larger (deeper
and/or wider) networks. Hence, we propose to spatially decompose a scene and
dedicate smaller networks for each decomposed part. When working together,
these networks can render the whole scene. This allows us near-constant
inference time regardless of the number of decomposed parts. Moreover, we show
that a Voronoi spatial decomposition is preferable for this purpose, as it is
provably compatible with the Painter's Algorithm for efficient and GPU-friendly
rendering. Our experiments show that for real-world scenes, our method provides
up to 3x more efficient inference than NeRF (with the same rendering quality),
or an improvement of up to 1.0~dB in PSNR (for the same inference cost).";Daniel Rebain<author:sep>Wei Jiang<author:sep>Soroosh Yazdani<author:sep>Ke Li<author:sep>Kwang Moo Yi<author:sep>Andrea Tagliasacchi;http://arxiv.org/pdf/2011.12490v1;cs.CV;;nerf
2010.07492v2;http://arxiv.org/abs/2010.07492v2;2020-10-15;NeRF++: Analyzing and Improving Neural Radiance Fields;"Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a
variety of capture settings, including 360 capture of bounded scenes and
forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer
perceptrons (MLPs) representing view-invariant opacity and view-dependent color
volumes to a set of training images, and samples novel views based on volume
rendering techniques. In this technical report, we first remark on radiance
fields and their potential ambiguities, namely the shape-radiance ambiguity,
and analyze NeRF's success in avoiding such ambiguities. Second, we address a
parametrization issue involved in applying NeRF to 360 captures of objects
within large-scale, unbounded 3D scenes. Our method improves view synthesis
fidelity in this challenging scenario. Code is available at
https://github.com/Kai-46/nerfplusplus.";Kai Zhang<author:sep>Gernot Riegler<author:sep>Noah Snavely<author:sep>Vladlen Koltun;http://arxiv.org/pdf/2010.07492v2;cs.CV;"Code is available at https://github.com/Kai-46/nerfplusplus; fix a
  minor formatting issue in Fig. 4";nerf
2008.02268v3;http://arxiv.org/abs/2008.02268v3;2020-08-05;NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo  Collections;"We present a learning-based method for synthesizing novel views of complex
scenes using only unstructured collections of in-the-wild photographs. We build
on Neural Radiance Fields (NeRF), which uses the weights of a multilayer
perceptron to model the density and color of a scene as a function of 3D
coordinates. While NeRF works well on images of static subjects captured under
controlled settings, it is incapable of modeling many ubiquitous, real-world
phenomena in uncontrolled images, such as variable illumination or transient
occluders. We introduce a series of extensions to NeRF to address these issues,
thereby enabling accurate reconstructions from unstructured image collections
taken from the internet. We apply our system, dubbed NeRF-W, to internet photo
collections of famous landmarks, and demonstrate temporally consistent novel
view renderings that are significantly closer to photorealism than the prior
state of the art.";Ricardo Martin-Brualla<author:sep>Noha Radwan<author:sep>Mehdi S. M. Sajjadi<author:sep>Jonathan T. Barron<author:sep>Alexey Dosovitskiy<author:sep>Daniel Duckworth;http://arxiv.org/pdf/2008.02268v3;cs.CV;"Project website: https://nerf-w.github.io. Ricardo Martin-Brualla,
  Noha Radwan, and Mehdi S. M. Sajjadi contributed equally to this work.
  Updated with results for three additional scenes";nerf
2007.11571v2;http://arxiv.org/abs/2007.11571v2;2020-07-22;Neural Sparse Voxel Fields;"Photo-realistic free-viewpoint rendering of real-world scenes using classical
computer graphics techniques is challenging, because it requires the difficult
step of capturing detailed appearance and geometry models. Recent studies have
demonstrated promising results by learning scene representations that
implicitly encode both geometry and appearance without 3D supervision. However,
existing approaches in practice often show blurry renderings caused by the
limited network capacity or the difficulty in finding accurate intersections of
camera rays with the scene geometry. Synthesizing high-resolution imagery from
these representations often requires time-consuming optical ray marching. In
this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene
representation for fast and high-quality free-viewpoint rendering. NSVF defines
a set of voxel-bounded implicit fields organized in a sparse voxel octree to
model local properties in each cell. We progressively learn the underlying
voxel structures with a differentiable ray-marching operation from only a set
of posed RGB images. With the sparse voxel octree structure, rendering novel
views can be accelerated by skipping the voxels containing no relevant scene
content. Our method is typically over 10 times faster than the state-of-the-art
(namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving
higher quality results. Furthermore, by utilizing an explicit sparse voxel
representation, our method can easily be applied to scene editing and scene
composition. We also demonstrate several challenging tasks, including
multi-scene learning, free-viewpoint rendering of a moving human, and
large-scale scene rendering. Code and data are available at our website:
https://github.com/facebookresearch/NSVF.";Lingjie Liu<author:sep>Jiatao Gu<author:sep>Kyaw Zaw Lin<author:sep>Tat-Seng Chua<author:sep>Christian Theobalt;http://arxiv.org/pdf/2007.11571v2;cs.CV;20 pages, in progress;nerf
2003.08934v2;http://arxiv.org/abs/2003.08934v2;2020-03-19;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis;"We present a method that achieves state-of-the-art results for synthesizing
novel views of complex scenes by optimizing an underlying continuous volumetric
scene function using a sparse set of input views. Our algorithm represents a
scene using a fully-connected (non-convolutional) deep network, whose input is
a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing
direction $(\theta, \phi)$) and whose output is the volume density and
view-dependent emitted radiance at that spatial location. We synthesize views
by querying 5D coordinates along camera rays and use classic volume rendering
techniques to project the output colors and densities into an image. Because
volume rendering is naturally differentiable, the only input required to
optimize our representation is a set of images with known camera poses. We
describe how to effectively optimize neural radiance fields to render
photorealistic novel views of scenes with complicated geometry and appearance,
and demonstrate results that outperform prior work on neural rendering and view
synthesis. View synthesis results are best viewed as videos, so we urge readers
to view our supplementary video for convincing comparisons.";Ben Mildenhall<author:sep>Pratul P. Srinivasan<author:sep>Matthew Tancik<author:sep>Jonathan T. Barron<author:sep>Ravi Ramamoorthi<author:sep>Ren Ng;http://arxiv.org/pdf/2003.08934v2;cs.CV;"ECCV 2020 (oral). Project page with videos and code:
  http://tancik.com/nerf";nerf
1403.5969v1;http://arxiv.org/abs/1403.5969v1;2014-03-24;Random Matrices and Erasure Robust Frames;"Data erasure can often occur in communication. Guarding against erasures
involves redundancy in data representation. Mathematically this may be achieved
by redundancy through the use of frames. One way to measure the robustness of a
frame against erasures is to examine the worst case condition number of the
frame with a certain number of vectors erased from the frame. The term {\em
numerically erasure-robust frames (NERFs)} was introduced in \cite{FicMix12} to
give a more precise characterization of erasure robustness of frames. In the
paper the authors established that random frames whose entries are drawn
independently from the standard normal distribution can be robust against up to
approximately 15\% erasures, and asked whether there exist frames that are
robust against erasures of more than 50\%. In this paper we show that with very
high probability random frames are, independent of the dimension, robust
against any amount of erasures as long as the number of remaining vectors is at
least $1+\delta$ times the dimension for some $\delta_0>0$. This is the best
possible result, and it also implies that the proportion of erasures can
arbitrarily close to 1 while still maintaining robustness. Our result depends
crucially on a new estimate for the smallest singular value of a rectangular
random matrix with independent standard normal entries.";Yang Wang;http://arxiv.org/pdf/1403.5969v1;cs.IT;;nerf
1210.0139v1;http://arxiv.org/abs/1210.0139v1;2012-09-29;Group-theoretic constructions of erasure-robust frames;"In the field of compressed sensing, a key problem remains open: to explicitly
construct matrices with the restricted isometry property (RIP) whose
performance rivals those generated using random matrix theory. In short, RIP
involves estimating the singular values of a combinatorially large number of
submatrices, seemingly requiring an enormous amount of computation in even
low-dimensional examples. In this paper, we consider a similar problem
involving submatrix singular value estimation, namely the problem of explicitly
constructing numerically erasure robust frames (NERFs). Such frames are the
latest invention in a long line of research concerning the design of linear
encoders that are robust against data loss. We begin by focusing on a subtle
difference between the definition of a NERF and that of an RIP matrix, one that
allows us to introduce a new computational trick for quickly estimating NERF
bounds. In short, we estimate these bounds by evaluating the frame analysis
operator at every point of an epsilon-net for the unit sphere. We then borrow
ideas from the theory of group frames to construct explicit frames and
epsilon-nets with such high degrees of symmetry that the requisite number of
operator evaluations is greatly reduced. We conclude with numerical results,
using these new ideas to quickly produce decent estimates of NERF bounds which
would otherwise take an eternity. Though the more important RIP problem remains
open, this work nevertheless demonstrates the feasibility of exploiting
symmetry to greatly reduce the computational burden of similar combinatorial
linear algebra problems.";Matthew Fickus<author:sep>John Jasper<author:sep>Dustin G. Mixon<author:sep>Jesse Peterson;http://arxiv.org/pdf/1210.0139v1;math.FA;;nerf
