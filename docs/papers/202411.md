---
layout: default
title: November 2024
parent: Papers
nav_order: 202411
---

<!---metadata--->


## A Nerf-Based Color Consistency Method for Remote Sensing Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-08 | Zongcheng Zuo, Yuanxiang Li, Tongtong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.05557v1){: .btn .btn-green } |

**Abstract**: Due to different seasons, illumination, and atmospheric conditions, the
photometric of the acquired image varies greatly, which leads to obvious
stitching seams at the edges of the mosaic image. Traditional methods can be
divided into two categories, one is absolute radiation correction and the other
is relative radiation normalization. We propose a NeRF-based method of color
consistency correction for multi-view images, which weaves image features
together using implicit expressions, and then re-illuminates feature space to
generate a fusion image with a new perspective. We chose Superview-1 satellite
images and UAV images with large range and time difference for the experiment.
Experimental results show that the synthesize image generated by our method has
excellent visual effect and smooth color transition at the edges.

Comments:
- 4 pages, 4 figures, The International Geoscience and Remote Sensing
  Symposium (IGARSS2023)

---

## Rate-aware Compression for NeRF-based Volumetric Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-08 | Zhiyu Zhang, Guo Lu, Huanxiong Liang, Zhengxue Cheng, Anni Tang, Li Song | cs.MM | [PDF](http://arxiv.org/pdf/2411.05322v1){: .btn .btn-green } |

**Abstract**: The neural radiance fields (NeRF) have advanced the development of 3D
volumetric video technology, but the large data volumes they involve pose
significant challenges for storage and transmission. To address these problems,
the existing solutions typically compress these NeRF representations after the
training stage, leading to a separation between representation training and
compression. In this paper, we try to directly learn a compact NeRF
representation for volumetric video in the training stage based on the proposed
rate-aware compression framework. Specifically, for volumetric video, we use a
simple yet effective modeling strategy to reduce temporal redundancy for the
NeRF representation. Then, during the training phase, an implicit entropy model
is utilized to estimate the bitrate of the NeRF representation. This entropy
model is then encoded into the bitstream to assist in the decoding of the NeRF
representation. This approach enables precise bitrate estimation, thereby
leading to a compact NeRF representation. Furthermore, we propose an adaptive
quantization strategy and learn the optimal quantization step for the NeRF
representations. Finally, the NeRF representation can be optimized by using the
rate-distortion trade-off. Our proposed compression framework can be used for
different representations and experimental results demonstrate that our
approach significantly reduces the storage size with marginal distortion and
achieves state-of-the-art rate-distortion performance for volumetric video on
the HumanRF and ReRF datasets. Compared to the previous state-of-the-art method
TeTriRF, we achieved an approximately -80% BD-rate on the HumanRF dataset and
-60% BD-rate on the ReRF dataset.

Comments:
- Accepted by ACM MM 2024 (Oral)

---

## SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on Larger  Objects for Mobile-Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Xun Tu, Karthik Desingh | cs.RO | [PDF](http://arxiv.org/pdf/2411.04386v1){: .btn .btn-green } |

**Abstract**: Grasp planning and estimation have been a longstanding research problem in
robotics, with two main approaches to find graspable poses on the objects: 1)
geometric approach, which relies on 3D models of objects and the gripper to
estimate valid grasp poses, and 2) data-driven, learning-based approach, with
models trained to identify grasp poses from raw sensor observations. The latter
assumes comprehensive geometric coverage during the training phase. However,
the data-driven approach is typically biased toward tabletop scenarios and
struggle to generalize to out-of-distribution scenarios with larger objects
(e.g. chair). Additionally, raw sensor data (e.g. RGB-D data) from a single
view of these larger objects is often incomplete and necessitates additional
observations. In this paper, we take a geometric approach, leveraging
advancements in object modeling (e.g. NeRF) to build an implicit model by
taking RGB images from views around the target object. This model enables the
extraction of explicit mesh model while also capturing the visual appearance
from novel viewpoints that is useful for perception tasks like object detection
and pose estimation. We further decompose the NeRF-reconstructed 3D mesh into
superquadrics (SQs) -- parametric geometric primitives, each mapped to a set of
precomputed grasp poses, allowing grasp composition on the target object based
on these primitives. Our proposed pipeline overcomes the problems: a) noisy
depth and incomplete view of the object, with a modeling step, and b)
generalization to objects of any size. For more qualitative results, refer to
the supplementary video and webpage https://bit.ly/3ZrOanU

Comments:
- 8 pages, 7 figures, submitted to ICRA 2025 for review

---

## Planar Reflection-Aware Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Chen Gao, Yipeng Wang, Changil Kim, Jia-Bin Huang, Johannes Kopf | cs.CV | [PDF](http://arxiv.org/pdf/2411.04984v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in
reconstructing complex scenes with high fidelity. However, NeRF's view
dependency can only handle low-frequency reflections. It falls short when
handling complex planar reflections, often interpreting them as erroneous scene
geometries and leading to duplicated and inaccurate scene representations. To
address this challenge, we introduce a reflection-aware NeRF that jointly
models planar reflectors, such as windows, and explicitly casts reflected rays
to capture the source of the high-frequency reflections. We query a single
radiance field to render the primary color and the source of the reflection. We
propose a sparse edge regularization to help utilize the true sources of
reflections for rendering planar reflections rather than creating a duplicate
along the primary ray at the same depth. As a result, we obtain accurate scene
geometry. Rendering along the primary ray results in a clean, reflection-free
view, while explicitly rendering along the reflected ray allows us to
reconstruct highly detailed reflections. Our extensive quantitative and
qualitative evaluations of real-world datasets demonstrate our method's
enhanced performance in accurately handling reflections.



---

## MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2411.04924v1){: .btn .btn-green } |

**Abstract**: We introduce MVSplat360, a feed-forward approach for 360{\deg} novel view
synthesis (NVS) of diverse real-world scenes, using only sparse observations.
This setting is inherently ill-posed due to minimal overlap among input views
and insufficient visual information provided, making it challenging for
conventional methods to achieve high-quality results. Our MVSplat360 addresses
this by effectively combining geometry-aware 3D reconstruction with temporally
consistent video generation. Specifically, it refactors a feed-forward 3D
Gaussian Splatting (3DGS) model to render features directly into the latent
space of a pre-trained Stable Video Diffusion (SVD) model, where these features
then act as pose and visual cues to guide the denoising process and produce
photorealistic 3D-consistent views. Our model is end-to-end trainable and
supports rendering arbitrary views with as few as 5 sparse input views. To
evaluate MVSplat360's performance, we introduce a new benchmark using the
challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual
quality compared to state-of-the-art methods on wide-sweeping or even 360{\deg}
NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the
effectiveness of our model. The video results are available on our project
page: https://donydchen.github.io/mvsplat360.

Comments:
- NeurIPS 2024, Project page: https://donydchen.github.io/mvsplat360,
  Code: https://github.com/donydchen/mvsplat360

---

## GANESH: Generalizable NeRF for Lensless Imaging

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Rakesh Raj Madavan, Akshat Kaimal, Badhrinarayanan K V, Vinayak Gupta, Rohit Choudhary, Chandrakala Shanmuganathan, Kaushik Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2411.04810v1){: .btn .btn-green } |

**Abstract**: Lensless imaging offers a significant opportunity to develop ultra-compact
cameras by removing the conventional bulky lens system. However, without a
focusing element, the sensor's output is no longer a direct image but a complex
multiplexed scene representation. Traditional methods have attempted to address
this challenge by employing learnable inversions and refinement models, but
these methods are primarily designed for 2D reconstruction and do not
generalize well to 3D reconstruction. We introduce GANESH, a novel framework
designed to enable simultaneous refinement and novel view synthesis from
multi-view lensless images. Unlike existing methods that require scene-specific
training, our approach supports on-the-fly inference without retraining on each
scene. Moreover, our framework allows us to tune our model to specific scenes,
enhancing the rendering and refinement quality. To facilitate research in this
area, we also present the first multi-view lensless dataset, LenslessScenes.
Extensive experiments demonstrate that our method outperforms current
approaches in reconstruction accuracy and refinement quality. Code and video
results are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/



---

## ProEdit: Simple Progression is All You Need for High-Quality 3D Scene  Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Jun-Kun Chen, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.05006v1){: .btn .btn-green } |

**Abstract**: This paper proposes ProEdit - a simple yet effective framework for
high-quality 3D scene editing guided by diffusion distillation in a novel
progressive manner. Inspired by the crucial observation that multi-view
inconsistency in scene editing is rooted in the diffusion model's large
feasible output space (FOS), our framework controls the size of FOS and reduces
inconsistency by decomposing the overall editing task into several subtasks,
which are then executed progressively on the scene. Within this framework, we
design a difficulty-aware subtask decomposition scheduler and an adaptive 3D
Gaussian splatting (3DGS) training strategy, ensuring high quality and
efficiency in performing each subtask. Extensive evaluation shows that our
ProEdit achieves state-of-the-art results in various scenes and challenging
editing tasks, all through a simple framework without any expensive or
sophisticated add-ons like distillation losses, components, or training
procedures. Notably, ProEdit also provides a new way to control, preview, and
select the "aggressivity" of editing operation during the editing process.

Comments:
- NeurIPS 2024. Project Page: https://immortalco.github.io/ProEdit/

---

## GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Jilan Mei, Junbo Li, Cai Meng | cs.CV | [PDF](http://arxiv.org/pdf/2411.03807v3){: .btn .btn-green } |

**Abstract**: This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.



---

## 3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical  Object Rearrangement

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Ziqi Lu, Jianbo Ye, John Leonard | cs.CV | [PDF](http://arxiv.org/pdf/2411.03706v1){: .btn .btn-green } |

**Abstract**: We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for
detecting physical object rearrangements in 3D scenes. Our approach estimates
3D object-level changes by comparing two sets of unaligned images taken at
different times. Leveraging 3DGS's novel view rendering and EfficientSAM's
zero-shot segmentation capabilities, we detect 2D object-level changes, which
are then associated and fused across views to estimate 3D changes. Our method
can detect changes in cluttered environments using sparse post-change images
within as little as 18s, using as few as a single new image. It does not rely
on depth input, user instructions, object classes, or object models -- An
object is recognized simply if it has been re-arranged. Our approach is
evaluated on both public and self-collected real-world datasets, achieving up
to 14% higher accuracy and three orders of magnitude faster performance
compared to the state-of-the-art radiance-field-based change detection method.
This significant performance boost enables a broad range of downstream
applications, where we highlight three key use cases: object reconstruction,
robot workspace reset, and 3DGS model update. Our code and data will be made
available at https://github.com/520xyxyzq/3DGS-CD.



---

## Structure Consistent Gaussian Splatting with Matching Prior for Few-shot  Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Rui Peng, Wangze Xu, Luyang Tang, Liwei Liao, Jianbo Jiao, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.03637v1){: .btn .btn-green } |

**Abstract**: Despite the substantial progress of novel view synthesis, existing methods,
either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian
Splatting (3DGS), suffer significant degradation when the input becomes sparse.
Numerous efforts have been introduced to alleviate this problem, but they still
struggle to synthesize satisfactory results efficiently, especially in the
large scene. In this paper, we propose SCGaussian, a Structure Consistent
Gaussian Splatting method using matching priors to learn 3D consistent scene
structure. Considering the high interdependence of Gaussian attributes, we
optimize the scene structure in two folds: rendering geometry and, more
importantly, the position of Gaussian primitives, which is hard to be directly
constrained in the vanilla 3DGS due to the non-structure property. To achieve
this, we present a hybrid Gaussian representation. Besides the ordinary
non-structure Gaussian primitives, our model also consists of ray-based
Gaussian primitives that are bound to matching rays and whose optimization of
their positions is restricted along the ray. Thus, we can utilize the matching
correspondence to directly enforce the position of these Gaussian primitives to
converge to the surface points where rays intersect. Extensive experiments on
forward-facing, surrounding, and complex large scenes show the effectiveness of
our approach with state-of-the-art performance and high efficiency. Code is
available at https://github.com/prstrive/SCGaussian.

Comments:
- NeurIPS 2024 Accepted

---

## LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Huibin Zhao, Weipeng Guan, Peng Lu | cs.RO | [PDF](http://arxiv.org/pdf/2411.02703v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown its ability in rapid rendering and
high-fidelity mapping. In this paper, we introduce LVI-GS, a tightly-coupled
LiDAR-Visual-Inertial mapping framework with 3DGS, which leverages the
complementary characteristics of LiDAR and image sensors to capture both
geometric structures and visual details of 3D scenes. To this end, the 3D
Gaussians are initialized from colourized LiDAR points and optimized using
differentiable rendering. In order to achieve high-fidelity mapping, we
introduce a pyramid-based training approach to effectively learn multi-level
features and incorporate depth loss derived from LiDAR measurements to improve
geometric feature perception. Through well-designed strategies for Gaussian-Map
expansion, keyframe selection, thread management, and custom CUDA acceleration,
our framework achieves real-time photo-realistic mapping. Numerical experiments
are performed to evaluate the superior performance of our method compared to
state-of-the-art 3D reconstruction systems.



---

## Exploring Seasonal Variability in the Context of Neural Radiance Fields  for 3D Reconstruction on Satellite Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Liv Kåreborn, Erica Ingerstad, Amanda Berg, Justus Karlsson, Leif Haglund | cs.CV | [PDF](http://arxiv.org/pdf/2411.02972v1){: .btn .btn-green } |

**Abstract**: In this work, the seasonal predictive capabilities of Neural Radiance Fields
(NeRF) applied to satellite images are investigated. Focusing on the
utilization of satellite data, the study explores how Sat-NeRF, a novel
approach in computer vision, performs in predicting seasonal variations across
different months. Through comprehensive analysis and visualization, the study
examines the model's ability to capture and predict seasonal changes,
highlighting specific challenges and strengths. Results showcase the impact of
the sun direction on predictions, revealing nuanced details in seasonal
transitions, such as snow cover, color accuracy, and texture representation in
different landscapes. Given these results, we propose Planet-NeRF, an extension
to Sat-NeRF capable of incorporating seasonal variability through a set of
month embedding vectors. Comparative evaluations reveal that Planet-NeRF
outperforms prior models in the case where seasonal changes are present. The
extensive evaluation combined with the proposed method offers promising avenues
for future research in this domain.



---

## CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model  Retrieval

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Xin Wen, Xuening Zhu, Renjiao Yi, Zhifeng Wang, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.02979v1){: .btn .btn-green } |

**Abstract**: Reconstructing from multi-view images is a longstanding problem in 3D vision,
where neural radiance fields (NeRFs) have shown great potential and get
realistic rendered images of novel views. Currently, most NeRF methods either
require accurate camera poses or a large number of input images, or even both.
Reconstructing NeRF from few-view images without poses is challenging and
highly ill-posed. To address this problem, we propose CAD-NeRF, a method
reconstructed from less than 10 images without any known poses. Specifically,
we build a mini library of several CAD models from ShapeNet and render them
from many random views. Given sparse-view input images, we run a model and pose
retrieval from the library, to get a model with similar shapes, serving as the
density supervision and pose initializations. Here we propose a multi-view pose
retrieval method to avoid pose conflicts among views, which is a new and unseen
problem in uncalibrated NeRF methods. Then, the geometry of the object is
trained by the CAD guidance. The deformation of the density field and camera
poses are optimized jointly. Then texture and density are trained and
fine-tuned as well. All training phases are in self-supervised manners.
Comprehensive evaluations of synthetic and real images show that CAD-NeRF
successfully learns accurate densities with a large deformation from retrieved
CAD models, showing the generalization abilities.

Comments:
- The article has been accepted by Frontiers of Computer Science (FCS)

---

## Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Xavier Timoneda, Markus Herb, Fabian Duerr, Daniel Goehring, Fisher Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.02969v1){: .btn .btn-green } |

**Abstract**: LiDAR Semantic Segmentation is a fundamental task in autonomous driving
perception consisting of associating each LiDAR point to a semantic label.
Fully-supervised models have widely tackled this task, but they require labels
for each scan, which either limits their domain or requires impractical amounts
of expensive annotations. Camera images, which are generally recorded alongside
LiDAR pointclouds, can be processed by the widely available 2D foundation
models, which are generic and dataset-agnostic. However, distilling knowledge
from 2D data to improve LiDAR perception raises domain adaptation challenges.
For example, the classical perspective projection suffers from the parallax
effect produced by the position shift between both sensors at their respective
capture times. We propose a Semi-Supervised Learning setup to leverage
unlabeled LiDAR pointclouds alongside distilled knowledge from the camera
images. To self-supervise our model on the unlabeled scans, we add an auxiliary
NeRF head and cast rays from the camera viewpoint over the unlabeled voxel
features. The NeRF head predicts densities and semantic logits at each sampled
ray location which are used for rendering pixel semantics. Concurrently, we
query the Segment-Anything (SAM) foundation model with the camera image to
generate a set of unlabeled generic masks. We fuse the masks with the rendered
pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel
predictions. During inference, we drop the NeRF head and run our model with
only LiDAR. We show the effectiveness of our approach in three public LiDAR
Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.

Comments:
- IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS) 2024

---

## HFGaussian: Learning Generalizable Gaussian Human with Integrated Human  Features

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Arnab Dey, Cheng-You Lu, Andrew I. Comport, Srinath Sridhar, Chin-Teng Lin, Jean Martinet | cs.CV | [PDF](http://arxiv.org/pdf/2411.03086v1){: .btn .btn-green } |

**Abstract**: Recent advancements in radiance field rendering show promising results in 3D
scene representation, where Gaussian splatting-based techniques emerge as
state-of-the-art due to their quality and efficiency. Gaussian splatting is
widely used for various applications, including 3D human representation.
However, previous 3D Gaussian splatting methods either use parametric body
models as additional information or fail to provide any underlying structure,
like human biomechanical features, which are essential for different
applications. In this paper, we present a novel approach called HFGaussian that
can estimate novel views and human features, such as the 3D skeleton, 3D key
points, and dense pose, from sparse input images in real time at 25 FPS. The
proposed method leverages generalizable Gaussian splatting technique to
represent the human subject and its associated features, enabling efficient and
generalizable reconstruction. By incorporating a pose regression network and
the feature splatting technique with Gaussian splatting, HFGaussian
demonstrates improved capabilities over existing 3D human methods, showcasing
the potential of 3D human representations with integrated biomechanics. We
thoroughly evaluate our HFGaussian method against the latest state-of-the-art
techniques in human Gaussian splatting and pose estimation, demonstrating its
real-time, state-of-the-art performance.



---

## Enhancing Exploratory Capability of Visual Navigation Using Uncertainty  of Implicit Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Yichen Wang, Qiming Liu, Zhe Liu, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2411.03487v1){: .btn .btn-green } |

**Abstract**: In the context of visual navigation in unknown scenes, both "exploration" and
"exploitation" are equally crucial. Robots must first establish environmental
cognition through exploration and then utilize the cognitive information to
accomplish target searches. However, most existing methods for image-goal
navigation prioritize target search over the generation of exploratory
behavior. To address this, we propose the Navigation with Uncertainty-driven
Exploration (NUE) pipeline, which uses an implicit and compact scene
representation, NeRF, as a cognitive structure. We estimate the uncertainty of
NeRF and augment the exploratory ability by the uncertainty to in turn
facilitate the construction of implicit representation. Simultaneously, we
extract memory information from NeRF to enhance the robot's reasoning ability
for determining the location of the target. Ultimately, we seamlessly combine
the two generated abilities to produce navigational actions. Our pipeline is
end-to-end, with the environmental cognitive structure being constructed
online. Extensive experimental results on image-goal navigation demonstrate the
capability of our pipeline to enhance exploratory behaviors, while also
enabling a natural transition from the exploration to exploitation phase. This
enables our model to outperform existing memory-based cognitive navigation
structures in terms of navigation performance.



---

## Object and Contact Point Tracking in Demonstrations Using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Michael Büttner, Jonathan Francis, Helge Rhodin, Andrew Melnik | cs.CV | [PDF](http://arxiv.org/pdf/2411.03555v1){: .btn .btn-green } |

**Abstract**: This paper introduces a method to enhance Interactive Imitation Learning
(IIL) by extracting touch interaction points and tracking object movement from
video demonstrations. The approach extends current IIL systems by providing
robots with detailed knowledge of both where and how to interact with objects,
particularly complex articulated ones like doors and drawers. By leveraging
cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for
tracking, this method allows robots to better understand and manipulate objects
in dynamic environments. The research lays the foundation for more effective
task learning and execution in autonomous robotic systems.

Comments:
- CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich,
  Germany

---

## SplatOverflow: Asynchronous Hardware Troubleshooting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Amritansh Kwatra, Tobias Wienberg, Ilan Mandel, Ritik Batra, Peter He, Francois Guimbretiere, Thijs Roumen | cs.HC | [PDF](http://arxiv.org/pdf/2411.02332v2){: .btn .btn-green } |

**Abstract**: As tools for designing and manufacturing hardware become more accessible,
smaller producers can develop and distribute novel hardware. However, there
aren't established tools to support end-user hardware troubleshooting or
routine maintenance. As a result, technical support for hardware remains ad-hoc
and challenging to scale. Inspired by software troubleshooting workflows like
StackOverflow, we propose a workflow for asynchronous hardware troubleshooting:
SplatOverflow. SplatOverflow creates a novel boundary object, the SplatOverflow
scene, that users reference to communicate about hardware. The scene comprises
a 3D Gaussian Splat of the user's hardware registered onto the hardware's CAD
model. The splat captures the current state of the hardware, and the registered
CAD model acts as a referential anchor for troubleshooting instructions. With
SplatOverflow, maintainers can directly address issues and author instructions
in the user's workspace. The instructions define workflows that can easily be
shared between users and recontextualized in new environments. In this paper,
we describe the design of SplatOverflow, detail the workflows it enables, and
illustrate its utility to different kinds of users. We also validate that
non-experts can use SplatOverflow to troubleshoot common problems with a 3D
printer in a user study.

Comments:
- Our accompanying video figure is available at:
  https://youtu.be/m4TKeBDuZkU

---

## A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Matthew McDermott, Jason Rife | cs.CV | [PDF](http://arxiv.org/pdf/2411.01725v1){: .btn .btn-green } |

**Abstract**: In this paper we reexamine the process through which a Neural Radiance Field
(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image
applications where camera pixels integrate light over time, LiDAR pulses arrive
at specific times. As such, multiple LiDAR returns are possible for any given
detector and the classification of these returns is inherently probabilistic.
Applying a traditional NeRF training routine can result in the network learning
phantom surfaces in free space between conflicting range measurements, similar
to how floater aberrations may be produced by an image model. We show that by
formulating loss as an integral of probability (rather than as an integral of
optical density) the network can learn multiple peaks for a given ray, allowing
the sampling of first, nth, or strongest returns from a single output channel.
Code is available at https://github.com/mcdermatt/PLINK



---

## NeRF-Aug: Data Augmentation for Robotics with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Eric Zhu, Mara Levy, Matthew Gwilliam, Abhinav Shrivastava | cs.RO | [PDF](http://arxiv.org/pdf/2411.02482v1){: .btn .btn-green } |

**Abstract**: Training a policy that can generalize to unknown objects is a long standing
challenge within the field of robotics. The performance of a policy often drops
significantly in situations where an object in the scene was not seen during
training. To solve this problem, we present NeRF-Aug, a novel method that is
capable of teaching a policy to interact with objects that are not present in
the dataset. This approach differs from existing approaches by leveraging the
speed and photorealism of a neural radiance field for augmentation. NeRF- Aug
both creates more photorealistic data and runs 3.83 times faster than existing
methods. We demonstrate the effectiveness of our method on 4 tasks with 11
novel objects that have no expert demonstration data. We achieve an average
69.1% success rate increase over existing methods. See video results at
https://nerf-aug.github.io.



---

## Modeling Uncertainty in 3D Gaussian Splatting through Continuous  Semantic Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Joey Wilson, Marcelino Almeida, Min Sun, Sachit Mahajan, Maani Ghaffari, Parker Ewen, Omid Ghasemalizadeh, Cheng-Hao Kuo, Arnie Sen | cs.RO | [PDF](http://arxiv.org/pdf/2411.02547v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel algorithm for probabilistically updating
and rasterizing semantic maps within 3D Gaussian Splatting (3D-GS). Although
previous methods have introduced algorithms which learn to rasterize features
in 3D-GS for enhanced scene understanding, 3D-GS can fail without warning which
presents a challenge for safety-critical robotic applications. To address this
gap, we propose a method which advances the literature of continuous semantic
mapping from voxels to ellipsoids, combining the precise structure of 3D-GS
with the ability to quantify uncertainty of probabilistic robotic maps. Given a
set of images, our algorithm performs a probabilistic semantic update directly
on the 3D ellipsoids to obtain an expectation and variance through the use of
conjugate priors. We also propose a probabilistic rasterization which returns
per-pixel segmentation predictions with quantifiable uncertainty. We compare
our method with similar probabilistic voxel-based methods to verify our
extension to 3D ellipsoids, and perform ablation studies on uncertainty
quantification and temporal smoothing.



---

## GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface  Reconstruction in Open Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Gaochao Song, Chong Cheng, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.01853v2){: .btn .btn-green } |

**Abstract**: In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.

Comments:
- NeurIPS 2024

---

## FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage  Training

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers | cs.CV | [PDF](http://arxiv.org/pdf/2411.02229v2){: .btn .btn-green } |

**Abstract**: The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.

Comments:
- Accepted by NeurIPS2024

---

## Real-Time Spatio-Temporal Reconstruction of Dynamic Endoscopic Scenes  with 4D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-02 | Fengze Li, Jishuai He, Jieming Ma, Zhijing Wu | cs.CV | [PDF](http://arxiv.org/pdf/2411.01218v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction is essential in robotic minimally invasive
surgery, providing crucial spatial information that enhances surgical precision
and outcomes. However, existing methods struggle to address the complex,
temporally dynamic nature of endoscopic scenes. This paper presents
ST-Endo4DGS, a novel framework that models the spatio-temporal volume of
dynamic endoscopic scenes using unbiased 4D Gaussian Splatting (4DGS)
primitives, parameterized by anisotropic ellipses with flexible 4D rotations.
This approach enables precise representation of deformable tissue dynamics,
capturing intricate spatial and temporal correlations in real time.
Additionally, we extend spherindrical harmonics to represent time-evolving
appearance, achieving realistic adaptations to lighting and view changes. A new
endoscopic normal alignment constraint (ENAC) further enhances geometric
fidelity by aligning rendered normals with depth-derived geometry. Extensive
evaluations show that ST-Endo4DGS outperforms existing methods in both visual
quality and real-time performance, establishing a new state-of-the-art in
dynamic scene reconstruction for endoscopic surgery.



---

## CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for  Large-Scale Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.00771v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field
reconstruction, manifesting efficient and high-fidelity novel view synthesis.
However, accurately representing surfaces, especially in large and complex
scenarios, remains a significant challenge due to the unstructured nature of
3DGS. In this paper, we present CityGaussianV2, a novel approach for
large-scale scene reconstruction that addresses critical challenges related to
geometric accuracy and efficiency. Building on the favorable generalization
capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and
scalability issues. Specifically, we implement a decomposed-gradient-based
densification and depth regression technique to eliminate blurry artifacts and
accelerate convergence. To scale up, we introduce an elongation filter that
mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we
optimize the CityGaussian pipeline for parallel training, achieving up to
10$\times$ compression, at least 25% savings in training time, and a 50%
decrease in memory usage. We also established standard geometry benchmarks
under large-scale scenes. Experimental results demonstrate that our method
strikes a promising balance between visual quality, geometric accuracy, as well
as storage and training costs. The project page is available at
https://dekuliutesla.github.io/CityGaussianV2/.

Comments:
- Project Page: https://dekuliutesla.github.io/CityGaussianV2/

---

## ZIM: Zero-Shot Image Matting for Anything

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Beomyoung Kim, Chanyong Shin, Joonhyun Jeong, Hyungsik Jung, Se-Yun Lee, Sewhan Chun, Dong-Hyun Hwang, Joonsang Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00626v1){: .btn .btn-green } |

**Abstract**: The recent segmentation foundation model, Segment Anything Model (SAM),
exhibits strong zero-shot segmentation capabilities, but it falls short in
generating fine-grained precise masks. To address this limitation, we propose a
novel zero-shot image matting model, called ZIM, with two key contributions:
First, we develop a label converter that transforms segmentation labels into
detailed matte labels, constructing the new SA1B-Matte dataset without costly
manual annotations. Training SAM with this dataset enables it to generate
precise matte masks while maintaining its zero-shot capability. Second, we
design the zero-shot matting model equipped with a hierarchical pixel decoder
to enhance mask representation, along with a prompt-aware masked attention
mechanism to improve performance by enabling the model to focus on regions
specified by visual prompts. We evaluate ZIM using the newly introduced
MicroMat-3K test set, which contains high-quality micro-level matte labels.
Experimental results show that ZIM outperforms existing methods in fine-grained
mask generation and zero-shot generalization. Furthermore, we demonstrate the
versatility of ZIM in various downstream tasks requiring precise masks, such as
image inpainting and 3D NeRF. Our contributions provide a robust foundation for
advancing zero-shot matting and its downstream applications across a wide range
of computer vision tasks. The code is available at
\url{https://github.com/naver-ai/ZIM}.

Comments:
- preprint (21 pages, 16 figures, and 8 tables)

---

## PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud  Understanding


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Jincen Jiang, Qianyu Zhou, Yuhang Li, Xinkui Zhao, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang, Xuequan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00632v1){: .btn .btn-green } |

**Abstract**: In this paper, we present PCoTTA, an innovative, pioneering framework for
Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding,
enhancing the model's transferability towards the continually changing target
domain. We introduce a multi-task setting for PCoTTA, which is practical and
realistic, handling multiple tasks within one unified model during the
continual adaptation. Our PCoTTA involves three key components: automatic
prototype mixture (APM), Gaussian Splatted feature shifting (GSFS), and
contrastive prototype repulsion (CPR). Firstly, APM is designed to
automatically mix the source prototypes with the learnable prototypes with a
similarity balancing factor, avoiding catastrophic forgetting. Then, GSFS
dynamically shifts the testing sample toward the source domain, mitigating
error accumulation in an online manner. In addition, CPR is proposed to pull
the nearest learnable prototype close to the testing feature and push it away
from other prototypes, making each prototype distinguishable during the
adaptation. Experimental comparisons lead to a new benchmark, demonstrating
PCoTTA's superiority in boosting the model's transferability towards the
continually changing target domain.

Comments:
- Accepted to NeurIPS 2024
