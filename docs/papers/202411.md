---
layout: default
title: November 2024
parent: Papers
nav_order: 202411
---

<!---metadata--->


## Textured Gaussians for Enhanced 3D Scene Appearance Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-27 | Brian Chao, Hung-Yu Tseng, Lorenzo Porzi, Chen Gao, Tuotuo Li, Qinbo Li, Ayush Saraf, Jia-Bin Huang, Johannes Kopf, Gordon Wetzstein, Changil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2411.18625v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.

Comments:
- Project website: https://textured-gaussians.github.io/

---

## GLS: Geometry-aware 3D Language Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-27 | Jiaxiong Qiu, Liu Liu, Zhizhong Su, Tianwei Lin | cs.CV | [PDF](http://arxiv.org/pdf/2411.18066v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has achieved significant performance
on indoor surface reconstruction and open-vocabulary segmentation. This paper
presents GLS, a unified framework of surface reconstruction and open-vocabulary
segmentation based on 3DGS. GLS extends two fields by exploring the correlation
between them. For indoor surface reconstruction, we introduce surface normal
prior as a geometric cue to guide the rendered normal, and use the normal error
to optimize the rendered depth. For open-vocabulary segmentation, we employ 2D
CLIP features to guide instance features and utilize DEVA masks to enhance
their view consistency. Extensive experiments demonstrate the effectiveness of
jointly optimizing surface reconstruction and open-vocabulary segmentation,
where GLS surpasses state-of-the-art approaches of each task on MuSHRoom,
ScanNet++, and LERF-OVS datasets. Code will be available at
https://github.com/JiaxiongQ/GLS.

Comments:
- Technical Report

---

## PhyCAGE: Physically Plausible Compositional 3D Asset Generation from a  Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-27 | Han Yan, Mingrui Zhang, Yang Li, Chao Ma, Pan Ji | cs.CV | [PDF](http://arxiv.org/pdf/2411.18548v1){: .btn .btn-green } |

**Abstract**: We present PhyCAGE, the first approach for physically plausible compositional
3D asset generation from a single image. Given an input image, we first
generate consistent multi-view images for components of the assets. These
images are then fitted with 3D Gaussian Splatting representations. To ensure
that the Gaussians representing objects are physically compatible with each
other, we introduce a Physical Simulation-Enhanced Score Distillation Sampling
(PSE-SDS) technique to further optimize the positions of the Gaussians. It is
achieved by setting the gradient of the SDS loss as the initial velocity of the
physical simulation, allowing the simulator to act as a physics-guided
optimizer that progressively corrects the Gaussians' positions to a physically
compatible state. Experimental results demonstrate that the proposed method can
generate physically plausible compositional 3D assets given a single image.

Comments:
- Project page: https://wolfball.github.io/phycage/

---

## HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene  Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-27 | Wei Zhang, Qing Cheng, David Skuddis, Niclas Zeller, Daniel Cremers, Norbert Haala | cs.RO | [PDF](http://arxiv.org/pdf/2411.17982v1){: .btn .btn-green } |

**Abstract**: We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast
and accurate monocular scene reconstruction using only RGB input. Existing
Neural SLAM or 3DGS-based SLAM methods often trade off between rendering
quality and geometry accuracy, our research demonstrates that both can be
achieved simultaneously with RGB input alone. The key idea of our approach is
to enhance the ability for geometry estimation by combining easy-to-obtain
monocular priors with learning-based dense SLAM, and then using 3D Gaussian
splatting as our core map representation to efficiently model the scene. Upon
loop closure, our method ensures on-the-fly global consistency through
efficient pose graph bundle adjustment and instant map updates by explicitly
deforming the 3D Gaussian units based on anchored keyframe updates.
Furthermore, we introduce a grid-based scale alignment strategy to maintain
improved scale consistency in prior depths for finer depth details. Through
extensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate
significant improvements over existing Neural SLAM methods and even surpass
RGB-D-based methods in both reconstruction and rendering quality. The project
page and source code will be made available at https://hi-slam2.github.io/.

Comments:
- Under review process

---

## HEMGS: A Hybrid Entropy Model for 3D Gaussian Splatting Data Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-27 | Lei Liu, Zhenghao Chen, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.18473v1){: .btn .btn-green } |

**Abstract**: Fast progress in 3D Gaussian Splatting (3DGS) has made 3D Gaussians popular
for 3D modeling and image rendering, but this creates big challenges in data
storage and transmission. To obtain a highly compact 3DGS representation, we
propose a hybrid entropy model for Gaussian Splatting (HEMGS) data compression,
which comprises two primary components, a hyperprior network and an
autoregressive network. To effectively reduce structural redundancy across
attributes, we apply a progressive coding algorithm to generate hyperprior
features, in which we use previously compressed attributes and location as
prior information. In particular, to better extract the location features from
these compressed attributes, we adopt a domain-aware and instance-aware
architecture to respectively capture domain-aware structural relations without
additional storage costs and reveal scene-specific features through MLPs.
Additionally, to reduce redundancy within each attribute, we leverage
relationships between neighboring compressed elements within the attributes
through an autoregressive network. Given its unique structure, we propose an
adaptive context coding algorithm with flexible receptive fields to effectively
capture adjacent compressed elements. Overall, we integrate our HEMGS into an
end-to-end optimized 3DGS compression framework and the extensive experimental
results on four benchmarks indicate that our method achieves about 40\% average
reduction in size while maintaining the rendering quality over our baseline
method and achieving state-of-the-art compression results.



---

## SmileSplat: Generalizable Gaussian Splats for Unconstrained Sparse  Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-27 | Yanyan Li, Yixin Fang, Federico Tombari, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2411.18072v1){: .btn .btn-green } |

**Abstract**: Sparse Multi-view Images can be Learned to predict explicit radiance fields
via Generalizable Gaussian Splatting approaches, which can achieve wider
application prospects in real-life when ground-truth camera parameters are not
required as inputs. In this paper, a novel generalizable Gaussian Splatting
method, SmileSplat, is proposed to reconstruct pixel-aligned Gaussian surfels
for diverse scenarios only requiring unconstrained sparse multi-view images.
First, Gaussian surfels are predicted based on the multi-head Gaussian
regression decoder, which can are represented with less degree-of-freedom but
have better multi-view consistency. Furthermore, the normal vectors of Gaussian
surfel are enhanced based on high-quality of normal priors. Second, the
Gaussians and camera parameters (both extrinsic and intrinsic) are optimized to
obtain high-quality Gaussian radiance fields for novel view synthesis tasks
based on the proposed Bundle-Adjusting Gaussian Splatting module. Extensive
experiments on novel view rendering and depth map prediction tasks are
conducted on public datasets, demonstrating that the proposed method achieves
state-of-the-art performance in various 3D vision tasks. More information can
be found on our project page
(https://yanyan-li.github.io/project/gs/smilesplat)



---

## Neural Surface Priors for Editable Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-27 | Jakub Szymkowiak, Weronika Jakubowska, Dawid Malarz, Weronika Smolak-Dyżewska, Maciej Zięba, Przemysław Musialski, Wojtek Pałubicki, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2411.18311v1){: .btn .btn-green } |

**Abstract**: In computer graphics, there is a need to recover easily modifiable
representations of 3D geometry and appearance from image data. We introduce a
novel method for this task using 3D Gaussian Splatting, which enables intuitive
scene editing through mesh adjustments. Starting with input images and camera
poses, we reconstruct the underlying geometry using a neural Signed Distance
Field and extract a high-quality mesh. Our model then estimates a set of
Gaussians, where each component is flat, and the opacity is conditioned on the
recovered neural surface. To facilitate editing, we produce a proxy
representation that encodes information about the Gaussians' shape and
position. Unlike other methods, our pipeline allows modifications applied to
the extracted mesh to be propagated to the proxy representation, from which we
recover the updated parameters of the Gaussians. This effectively transfers the
mesh edits back to the recovered appearance representation. By leveraging
mesh-guided transformations, our approach simplifies 3D scene editing and
offers improvements over existing methods in terms of usability and visual
fidelity of edits. The complete source code for this project can be accessed at
\url{https://github.com/WJakubowska/NeuralSurfacePriors}

Comments:
- 9 pages, 7 figures

---

## Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready  3D Characters


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-27 | Zhiyang Guo, Jinxu Xiang, Kai Ma, Wengang Zhou, Houqiang Li, Ran Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2411.18197v1){: .btn .btn-green } |

**Abstract**: 3D characters are essential to modern creative industries, but making them
animatable often demands extensive manual work in tasks like rigging and
skinning. Existing automatic rigging tools face several limitations, including
the necessity for manual annotations, rigid skeleton topologies, and limited
generalization across diverse shapes and poses. An alternative approach is to
generate animatable avatars pre-bound to a rigged template mesh. However, this
method often lacks flexibility and is typically limited to realistic human
shapes. To address these issues, we present Make-It-Animatable, a novel
data-driven method to make any 3D humanoid model ready for character animation
in less than one second, regardless of its shapes and poses. Our unified
framework generates high-quality blend weights, bones, and pose
transformations. By incorporating a particle-based shape autoencoder, our
approach supports various 3D representations, including meshes and 3D Gaussian
splats. Additionally, we employ a coarse-to-fine representation and a
structure-aware modeling strategy to ensure both accuracy and robustness, even
for characters with non-standard skeleton structures. We conducted extensive
experiments to validate our framework's effectiveness. Compared to existing
methods, our approach demonstrates significant improvements in both quality and
speed.

Comments:
- Project Page: https://jasongzy.github.io/Make-It-Animatable/

---

## SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-26 | Gyeongjin Kang, Jisang Yoo, Jihyeon Park, Seungtae Nam, Hyeonsoo Im, Sangheon Shin, Sangpil Kim, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2411.17190v2){: .btn .btn-green } |

**Abstract**: We propose SelfSplat, a novel 3D Gaussian Splatting model designed to perform
pose-free and 3D prior-free generalizable 3D reconstruction from unposed
multi-view images. These settings are inherently ill-posed due to the lack of
ground-truth data, learned geometric information, and the need to achieve
accurate 3D reconstruction without finetuning, making it difficult for
conventional methods to achieve high-quality results. Our model addresses these
challenges by effectively integrating explicit 3D representations with
self-supervised depth and pose estimation techniques, resulting in reciprocal
improvements in both pose accuracy and 3D reconstruction quality. Furthermore,
we incorporate a matching-aware pose estimation network and a depth refinement
module to enhance geometry consistency across views, ensuring more accurate and
stable 3D reconstructions. To present the performance of our method, we
evaluated it on large-scale real-world datasets, including RealEstate10K, ACID,
and DL3DV. SelfSplat achieves superior results over previous state-of-the-art
methods in both appearance and geometry quality, also demonstrates strong
cross-dataset generalization capabilities. Extensive ablation studies and
analysis also validate the effectiveness of our proposed methods. Code and
pretrained models are available at https://gynjn.github.io/selfsplat/

Comments:
- Project page: https://gynjn.github.io/selfsplat/

---

## DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-26 | Christian Homeyer, Leon Begiristain, Christoph Schnörr | cs.CV | [PDF](http://arxiv.org/pdf/2411.17660v1){: .btn .btn-green } |

**Abstract**: Recent progress in scene synthesis makes standalone SLAM systems purely based
on optimizing hyperprimitives with a Rendering objective possible
\cite{monogs}.
  However, the tracking performance still lacks behind traditional
\cite{orbslam} and end-to-end SLAM systems \cite{droid}.
  An optimal trade-off between robustness, speed and accuracy has not yet been
reached, especially for monocular video.
  In this paper, we introduce a SLAM system based on an end-to-end Tracker and
extend it with a Renderer based on recent 3D Gaussian Splatting techniques.
  Our framework \textbf{DroidSplat} achieves both SotA tracking and rendering
results on common SLAM benchmarks.
  We implemented multiple building blocks of modern SLAM systems to run in
parallel, allowing for fast inference on common consumer GPU's.
  Recent progress in monocular depth prediction and camera calibration allows
our system to achieve strong results even on in-the-wild data without known
camera intrinsics.
  Code will be available at \url{https://github.com/ChenHoy/DROID-Splat}.



---

## MLI-NeRF: Multi-Light Intrinsic-Aware Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-26 | Yixiong Yang, Shilin Hu, Haoyu Wu, Ramon Baldrich, Dimitris Samaras, Maria Vanrell | cs.CV | [PDF](http://arxiv.org/pdf/2411.17235v1){: .btn .btn-green } |

**Abstract**: Current methods for extracting intrinsic image components, such as
reflectance and shading, primarily rely on statistical priors. These methods
focus mainly on simple synthetic scenes and isolated objects and struggle to
perform well on challenging real-world data. To address this issue, we propose
MLI-NeRF, which integrates \textbf{M}ultiple \textbf{L}ight information in
\textbf{I}ntrinsic-aware \textbf{Ne}ural \textbf{R}adiance \textbf{F}ields. By
leveraging scene information provided by different light source positions
complementing the multi-view information, we generate pseudo-label images for
reflectance and shading to guide intrinsic image decomposition without the need
for ground truth data. Our method introduces straightforward supervision for
intrinsic component separation and ensures robustness across diverse scene
types. We validate our approach on both synthetic and real-world datasets,
outperforming existing state-of-the-art methods. Additionally, we demonstrate
its applicability to various image editing tasks. The code and data are
publicly available.

Comments:
- Accepted paper for the International Conference on 3D Vision 2025.
  Project page: https://github.com/liulisixin/MLI-NeRF

---

## 4D Scaffold Gaussian Splatting for Memory Efficient Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-26 | Woong Oh Cho, In Cho, Seoha Kim, Jeongmin Bae, Youngjung Uh, Seon Joo Kim | cs.CV | [PDF](http://arxiv.org/pdf/2411.17044v1){: .btn .btn-green } |

**Abstract**: Existing 4D Gaussian methods for dynamic scene reconstruction offer high
visual fidelity and fast rendering. However, these methods suffer from
excessive memory and storage demands, which limits their practical deployment.
This paper proposes a 4D anchor-based framework that retains visual quality and
rendering speed of 4D Gaussians while significantly reducing storage costs. Our
method extends 3D scaffolding to 4D space, and leverages sparse 4D grid-aligned
anchors with compressed feature vectors. Each anchor models a set of neural 4D
Gaussians, each of which represent a local spatiotemporal region. In addition,
we introduce a temporal coverage-aware anchor growing strategy to effectively
assign additional anchors to under-reconstructed dynamic regions. Our method
adjusts the accumulated gradients based on Gaussians' temporal coverage,
improving reconstruction quality in dynamic regions. To reduce the number of
anchors, we further present enhanced formulations of neural 4D Gaussians. These
include the neural velocity, and the temporal opacity derived from a
generalized Gaussian distribution. Experimental results demonstrate that our
method achieves state-of-the-art visual quality and 97.8% storage reduction
over 4DGS.



---

## Distractor-free Generalizable 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-26 | Yanqi Bao, Jing Liao, Jing Huo, Yang Gao | cs.CV | [PDF](http://arxiv.org/pdf/2411.17605v1){: .btn .btn-green } |

**Abstract**: We present DGGS, a novel framework addressing the previously unexplored
challenge of Distractor-free Generalizable 3D Gaussian Splatting (3DGS). It
accomplishes two key objectives: fortifying generalizable 3DGS against
distractor-laden data during both training and inference phases, while
successfully extending cross-scene adaptation capabilities to conventional
distractor-free approaches. To achieve these objectives, DGGS introduces a
scene-agnostic reference-based mask prediction and refinement methodology
during training phase, coupled with a training view selection strategy,
effectively improving distractor prediction accuracy and training stability.
Moreover, to address distractor-induced voids and artifacts during inference
stage, we propose a two-stage inference framework for better reference
selection based on the predicted distractor masks, complemented by a distractor
pruning module to eliminate residual distractor effects. Extensive
generalization experiments demonstrate DGGS's advantages under distractor-laden
conditions. Additionally, experimental results show that our scene-agnostic
mask inference achieves accuracy comparable to scene-specific trained methods.
Homepage is \url{https://github.com/bbbbby-99/DGGS}.



---

## The Radiance of Neural Fields: Democratizing Photorealistic and Dynamic  Robotic Simulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Georgina Nuthall, Richard Bowden, Oscar Mendez | cs.RO | [PDF](http://arxiv.org/pdf/2411.16940v1){: .btn .btn-green } |

**Abstract**: As robots increasingly coexist with humans, they must navigate complex,
dynamic environments rich in visual information and implicit social dynamics,
like when to yield or move through crowds. Addressing these challenges requires
significant advances in vision-based sensing and a deeper understanding of
socio-dynamic factors, particularly in tasks like navigation. To facilitate
this, robotics researchers need advanced simulation platforms offering dynamic,
photorealistic environments with realistic actors. Unfortunately, most existing
simulators fall short, prioritizing geometric accuracy over visual fidelity,
and employing unrealistic agents with fixed trajectories and low-quality
visuals. To overcome these limitations, we developed a simulator that
incorporates three essential elements: (1) photorealistic neural rendering of
environments, (2) neurally animated human entities with behavior management,
and (3) an ego-centric robotic agent providing multi-sensor output. By
utilizing advanced neural rendering techniques in a dual-NeRF simulator, our
system produces high-fidelity, photorealistic renderings of both environments
and human entities. Additionally, it integrates a state-of-the-art Social Force
Model to model dynamic human-human and human-robot interactions, creating the
first photorealistic and accessible human-robot simulation system powered by
neural rendering.

Comments:
- 8 pages, 5 figures

---

## NovelGS: Consistent Novel-view Denoising via Large Gaussian  Reconstruction Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Jinpeng Liu, Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Ying Shan, Yansong Tang | cs.CV | [PDF](http://arxiv.org/pdf/2411.16779v1){: .btn .btn-green } |

**Abstract**: We introduce NovelGS, a diffusion model for Gaussian Splatting (GS) given
sparse-view images. Recent works leverage feed-forward networks to generate
pixel-aligned Gaussians, which could be fast rendered. Unfortunately, the
method was unable to produce satisfactory results for areas not covered by the
input images due to the formulation of these methods. In contrast, we leverage
the novel view denoising through a transformer-based network to generate 3D
Gaussians. Specifically, by incorporating both conditional views and noisy
target views, the network predicts pixel-aligned Gaussians for each view.
During training, the rendered target and some additional views of the Gaussians
are supervised. During inference, the target views are iteratively rendered and
denoised from pure noise. Our approach demonstrates state-of-the-art
performance in addressing the multi-view image reconstruction challenge. Due to
generative modeling of unseen regions, NovelGS effectively reconstructs 3D
objects with consistent and sharp textures. Experimental results on publicly
available datasets indicate that NovelGS substantially surpasses existing
image-to-3D frameworks, both qualitatively and quantitatively. We also
demonstrate the potential of NovelGS in generative tasks, such as text-to-3D
and image-to-3D, by integrating it with existing multiview diffusion models. We
will make the code publicly accessible.



---

## SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting  for Autonomous Driving

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Georg Hess, Carl Lindström, Maryam Fatemi, Christoffer Petersson, Lennart Svensson | cs.CV | [PDF](http://arxiv.org/pdf/2411.16816v1){: .btn .btn-green } |

**Abstract**: Ensuring the safety of autonomous robots, such as self-driving vehicles,
requires extensive testing across diverse driving scenarios. Simulation is a
key ingredient for conducting such testing in a cost-effective and scalable
way. Neural rendering methods have gained popularity, as they can build
simulation environments from collected logs in a data-driven manner. However,
existing neural radiance field (NeRF) methods for sensor-realistic rendering of
camera and lidar data suffer from low rendering speeds, limiting their
applicability for large-scale testing. While 3D Gaussian Splatting (3DGS)
enables real-time rendering, current methods are limited to camera data and are
unable to render lidar data essential for autonomous driving. To address these
limitations, we propose SplatAD, the first 3DGS-based method for realistic,
real-time rendering of dynamic scenes for both camera and lidar data. SplatAD
accurately models key sensor-specific phenomena such as rolling shutter
effects, lidar intensity, and lidar ray dropouts, using purpose-built
algorithms to optimize rendering efficiency. Evaluation across three autonomous
driving datasets demonstrates that SplatAD achieves state-of-the-art rendering
quality with up to +2 PSNR for NVS and +3 PSNR for reconstruction while
increasing rendering speed over NeRF-based methods by an order of magnitude.
See https://research.zenseact.com/publications/splatad/ for our project page.



---

## PreF3R: Pose-Free Feed-Forward 3D Gaussian Splatting from  Variable-length Image Sequence

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Zequn Chen, Jiezhi Yang, Heng Yang | cs.CV | [PDF](http://arxiv.org/pdf/2411.16877v1){: .btn .btn-green } |

**Abstract**: We present PreF3R, Pose-Free Feed-forward 3D Reconstruction from an image
sequence of variable length. Unlike previous approaches, PreF3R removes the
need for camera calibration and reconstructs the 3D Gaussian field within a
canonical coordinate frame directly from a sequence of unposed images, enabling
efficient novel-view rendering. We leverage DUSt3R's ability for pair-wise 3D
structure reconstruction, and extend it to sequential multi-view input via a
spatial memory network, eliminating the need for optimization-based global
alignment. Additionally, PreF3R incorporates a dense Gaussian parameter
prediction head, which enables subsequent novel-view synthesis with
differentiable rasterization. This allows supervising our model with the
combination of photometric loss and pointmap regression loss, enhancing both
photorealism and structural accuracy. Given a sequence of ordered images,
PreF3R incrementally reconstructs the 3D Gaussian field at 20 FPS, therefore
enabling real-time novel-view rendering. Empirical experiments demonstrate that
PreF3R is an effective solution for the challenging task of pose-free
feed-forward novel-view synthesis, while also exhibiting robust generalization
to unseen scenes.

Comments:
- project page: https://computationalrobotics.seas.harvard.edu/PreF3R/

---

## U2NeRF: Unsupervised Underwater Image Restoration and Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Vinayak Gupta, Manoj S, Mukund Varma T, Kaushik Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2411.16172v1){: .btn .btn-green } |

**Abstract**: Underwater images suffer from colour shifts, low contrast, and haziness due
to light absorption, refraction, scattering and restoring these images has
warranted much attention. In this work, we present Unsupervised Underwater
Neural Radiance Field U2NeRF, a transformer-based architecture that learns to
render and restore novel views conditioned on multi-view geometry
simultaneously. Due to the absence of supervision, we attempt to implicitly
bake restoring capabilities onto the NeRF pipeline and disentangle the
predicted color into several components - scene radiance, direct transmission
map, backscatter transmission map, and global background light, and when
combined reconstruct the underwater image in a self-supervised manner. In
addition, we release an Underwater View Synthesis UVS dataset consisting of 12
underwater scenes, containing both synthetically-generated and real-world data.
Our experiments demonstrate that when optimized on a single scene, U2NeRF
outperforms several baselines by as much LPIPS 11%, UIQM 5%, UCIQE 4% (on
average) and showcases improved rendering and restoration capabilities. Code
will be made available upon acceptance.

Comments:
- ICLR Tiny Papers 2024. arXiv admin note: text overlap with
  arXiv:2207.13298

---

## G2SDF: Surface Reconstruction from Explicit Gaussians with Implicit SDFs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Kunyi Li, Michael Niemeyer, Zeyu Chen, Nassir Navab, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2411.16898v1){: .btn .btn-green } |

**Abstract**: State-of-the-art novel view synthesis methods such as 3D Gaussian Splatting
(3DGS) achieve remarkable visual quality. While 3DGS and its variants can be
rendered efficiently using rasterization, many tasks require access to the
underlying 3D surface, which remains challenging to extract due to the sparse
and explicit nature of this representation. In this paper, we introduce G2SDF,
a novel approach that addresses this limitation by integrating a neural
implicit Signed Distance Field (SDF) into the Gaussian Splatting framework. Our
method links the opacity values of Gaussians with their distances to the
surface, ensuring a closer alignment of Gaussians with the scene surface. To
extend this approach to unbounded scenes at varying scales, we propose a
normalization function that maps any range to a fixed interval. To further
enhance reconstruction quality, we leverage an off-the-shelf depth estimator as
pseudo ground truth during Gaussian Splatting optimization. By establishing a
differentiable connection between the explicit Gaussians and the implicit SDF,
our approach enables high-quality surface reconstruction and rendering.
Experimental results on several real-world datasets demonstrate that G2SDF
achieves superior reconstruction quality than prior works while maintaining the
efficiency of 3DGS.



---

## UnitedVLN: Generalizable Gaussian Splatting for Continuous  Vision-Language Navigation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Guangzhao Dai, Jian Zhao, Yuantao Chen, Yusen Qin, Hao Zhao, Guosen Xie, Yazhou Yao, Xiangbo Shu, Xuelong Li | cs.CV | [PDF](http://arxiv.org/pdf/2411.16053v1){: .btn .btn-green } |

**Abstract**: Vision-and-Language Navigation (VLN), where an agent follows instructions to
reach a target destination, has recently seen significant advancements. In
contrast to navigation in discrete environments with predefined trajectories,
VLN in Continuous Environments (VLN-CE) presents greater challenges, as the
agent is free to navigate any unobstructed location and is more vulnerable to
visual occlusions or blind spots. Recent approaches have attempted to address
this by imagining future environments, either through predicted future visual
images or semantic features, rather than relying solely on current
observations. However, these RGB-based and feature-based methods lack intuitive
appearance-level information or high-level semantic complexity crucial for
effective navigation. To overcome these limitations, we introduce a novel,
generalizable 3DGS-based pre-training paradigm, called UnitedVLN, which enables
agents to better explore future environments by unitedly rendering
high-fidelity 360 visual images and semantic features. UnitedVLN employs two
key schemes: search-then-query sampling and separate-then-united rendering,
which facilitate efficient exploitation of neural primitives, helping to
integrate both appearance and semantic information for more robust navigation.
Extensive experiments demonstrate that UnitedVLN outperforms state-of-the-art
methods on existing VLN-CE benchmarks.



---

## Quadratic Gaussian Splatting for Efficient and Detailed Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Ziyu Zhang, Binbin Huang, Hanqing Jiang, Liyang Zhou, Xiaojun Xiang, Shunhan Shen | cs.CV | [PDF](http://arxiv.org/pdf/2411.16392v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has attracted attention for its
superior rendering quality and speed over Neural Radiance Fields (NeRF). To
address 3DGS's limitations in surface representation, 2D Gaussian Splatting
(2DGS) introduced disks as scene primitives to model and reconstruct geometries
from multi-view images, offering view-consistent geometry. However, the disk's
first-order linear approximation often leads to over-smoothed results. We
propose Quadratic Gaussian Splatting (QGS), a novel method that replaces disks
with quadric surfaces, enhancing geometric fitting, whose code will be
open-sourced. QGS defines Gaussian distributions in non-Euclidean space,
allowing primitives to capture more complex textures. As a second-order surface
approximation, QGS also renders spatial curvature to guide the normal
consistency term, to effectively reduce over-smoothing. Moreover, QGS is a
generalized version of 2DGS that achieves more accurate and detailed
reconstructions, as verified by experiments on DTU and TNT, demonstrating its
effectiveness in surpassing current state-of-the-art methods in geometry
reconstruction. Our code willbe released as open source.



---

## Event-boosted Deformable 3D Gaussians for Fast Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Wenhao Xu, Wenming Weng, Yueyi Zhang, Ruikang Xu, Zhiwei Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2411.16180v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) enables real-time rendering but struggles with
fast motion due to low temporal resolution of RGB cameras. To address this, we
introduce the first approach combining event cameras, which capture
high-temporal-resolution, continuous motion data, with deformable 3D-GS for
fast dynamic scene reconstruction. We observe that threshold modeling for
events plays a crucial role in achieving high-quality reconstruction.
Therefore, we propose a GS-Threshold Joint Modeling (GTJM) strategy, creating a
mutually reinforcing process that greatly improves both 3D reconstruction and
threshold modeling. Moreover, we introduce a Dynamic-Static Decomposition (DSD)
strategy that first identifies dynamic areas by exploiting the inability of
static Gaussians to represent motions, then applies a buffer-based soft
decomposition to separate dynamic and static areas. This strategy accelerates
rendering by avoiding unnecessary deformation in static areas, and focuses on
dynamic areas to enhance fidelity. Our approach achieves high-fidelity dynamic
reconstruction at 156 FPS with a 400$\times$400 resolution on an RTX 3090 GPU.



---

## SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting  Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-25 | Hyojun Go, Byeongjun Park, Jiho Jang, Jin-Young Kim, Soonwoo Kwon, Changick Kim | cs.CV | [PDF](http://arxiv.org/pdf/2411.16443v1){: .btn .btn-green } |

**Abstract**: Text-based generation and editing of 3D scenes hold significant potential for
streamlining content creation through intuitive user interactions. While recent
advances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time
rendering, existing methods are often specialized and task-focused, lacking a
unified framework for both generation and editing. In this paper, we introduce
SplatFlow, a comprehensive framework that addresses this gap by enabling direct
3DGS generation and editing. SplatFlow comprises two main components: a
multi-view rectified flow (RF) model and a Gaussian Splatting Decoder
(GSDecoder). The multi-view RF model operates in latent space, generating
multi-view images, depths, and camera poses simultaneously, conditioned on text
prompts, thus addressing challenges like diverse scene scales and complex
camera trajectories in real-world settings. Then, the GSDecoder efficiently
translates these latent outputs into 3DGS representations through a
feed-forward 3DGS method. Leveraging training-free inversion and inpainting
techniques, SplatFlow enables seamless 3DGS editing and supports a broad range
of 3D tasks-including object editing, novel view synthesis, and camera pose
estimation-within a unified framework without requiring additional complex
pipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K
datasets, demonstrating its versatility and effectiveness in various 3D
generation, editing, and inpainting-based tasks.

Comments:
- Project Page: https://gohyojun15.github.io/SplatFlow/

---

## PG-SLAM: Photo-realistic and Geometry-aware RGB-D SLAM in Dynamic  Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-24 | Haoang Li, Xiangqi Meng, Xingxing Zuo, Zhe Liu, Hesheng Wang, Daniel Cremers | cs.RO | [PDF](http://arxiv.org/pdf/2411.15800v1){: .btn .btn-green } |

**Abstract**: Simultaneous localization and mapping (SLAM) has achieved impressive
performance in static environments. However, SLAM in dynamic environments
remains an open question. Many methods directly filter out dynamic objects,
resulting in incomplete scene reconstruction and limited accuracy of camera
localization. The other works express dynamic objects by point clouds, sparse
joints, or coarse meshes, which fails to provide a photo-realistic
representation. To overcome the above limitations, we propose a photo-realistic
and geometry-aware RGB-D SLAM method by extending Gaussian splatting. Our
method is composed of three main modules to 1) map the dynamic foreground
including non-rigid humans and rigid items, 2) reconstruct the static
background, and 3) localize the camera. To map the foreground, we focus on
modeling the deformations and/or motions. We consider the shape priors of
humans and exploit geometric and appearance constraints of humans and items.
For background mapping, we design an optimization strategy between neighboring
local maps by integrating appearance constraint into geometric alignment. As to
camera localization, we leverage both static background and dynamic foreground
to increase the observations for noise compensation. We explore the geometric
and appearance constraints by associating 3D Gaussians with 2D optical flows
and pixel patches. Experiments on various real-world datasets demonstrate that
our method outperforms state-of-the-art approaches in terms of camera
localization and scene representation. Source codes will be publicly available
upon paper acceptance.



---

## GSurf: 3D Reconstruction via Signed Distance Fields with Direct Gaussian  Supervision

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-24 | Xu Baixin, Hu Jiangbei, Li Jiaze, He Ying | cs.CV | [PDF](http://arxiv.org/pdf/2411.15723v1){: .btn .btn-green } |

**Abstract**: Surface reconstruction from multi-view images is a core challenge in 3D
vision. Recent studies have explored signed distance fields (SDF) within Neural
Radiance Fields (NeRF) to achieve high-fidelity surface reconstructions.
However, these approaches often suffer from slow training and rendering speeds
compared to 3D Gaussian splatting (3DGS). Current state-of-the-art techniques
attempt to fuse depth information to extract geometry from 3DGS, but frequently
result in incomplete reconstructions and fragmented surfaces. In this paper, we
introduce GSurf, a novel end-to-end method for learning a signed distance field
directly from Gaussian primitives. The continuous and smooth nature of SDF
addresses common issues in the 3DGS family, such as holes resulting from noisy
or missing depth data. By using Gaussian splatting for rendering, GSurf avoids
the redundant volume rendering typically required in other GS and SDF
integrations. Consequently, GSurf achieves faster training and rendering speeds
while delivering 3D reconstruction quality comparable to neural implicit
surface methods, such as VolSDF and NeuS. Experimental results across various
benchmark datasets demonstrate the effectiveness of our method in producing
high-fidelity 3D reconstructions.

Comments:
- see https://github.com/xubaixinxbx/Gsurf

---

## DynamicAvatars: Accurate Dynamic Facial Avatars Reconstruction and  Precise Editing with Diffusion Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-24 | Yangyang Qian, Yuan Sun, Yu Guo | cs.GR | [PDF](http://arxiv.org/pdf/2411.15732v1){: .btn .btn-green } |

**Abstract**: Generating and editing dynamic 3D head avatars are crucial tasks in virtual
reality and film production. However, existing methods often suffer from facial
distortions, inaccurate head movements, and limited fine-grained editing
capabilities. To address these challenges, we present DynamicAvatars, a dynamic
model that generates photorealistic, moving 3D head avatars from video clips
and parameters associated with facial positions and expressions. Our approach
enables precise editing through a novel prompt-based editing model, which
integrates user-provided prompts with guiding parameters derived from large
language models (LLMs). To achieve this, we propose a dual-tracking framework
based on Gaussian Splatting and introduce a prompt preprocessing module to
enhance editing stability. By incorporating a specialized GAN algorithm and
connecting it to our control module, which generates precise guiding parameters
from LLMs, we successfully address the limitations of existing methods.
Additionally, we develop a dynamic editing strategy that selectively utilizes
specific training datasets to improve the efficiency and adaptability of the
model for dynamic editing tasks.



---

## Bundle Adjusted Gaussian Avatars Deblurring

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-24 | Muyao Niu, Yifan Zhan, Qingtian Zhu, Zhuoxiao Li, Wei Wang, Zhihang Zhong, Xiao Sun, Yinqiang Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2411.16758v1){: .btn .btn-green } |

**Abstract**: The development of 3D human avatars from multi-view videos represents a
significant yet challenging task in the field. Recent advancements, including
3D Gaussian Splattings (3DGS), have markedly progressed this domain.
Nonetheless, existing techniques necessitate the use of high-quality sharp
images, which are often impractical to obtain in real-world settings due to
variations in human motion speed and intensity. In this study, we attempt to
explore deriving sharp intrinsic 3D human Gaussian avatars from blurry video
footage in an end-to-end manner. Our approach encompasses a 3D-aware,
physics-oriented model of blur formation attributable to human movement,
coupled with a 3D human motion model to clarify ambiguities found in
motion-induced blurry images. This methodology facilitates the concurrent
learning of avatar model parameters and the refinement of sub-frame motion
parameters from a coarse initialization. We have established benchmarks for
this task through a synthetic dataset derived from existing multi-view
captures, alongside a real-captured dataset acquired through a 360-degree
synchronous hybrid-exposure camera system. Comprehensive evaluations
demonstrate that our model surpasses existing baselines.

Comments:
- Codes and Data: https://github.com/MyNiuuu/BAGA

---

## ZeroGS: Training 3D Gaussian Splatting from Unposed Images

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-24 | Yu Chen, Rolandos Alexandros Potamias, Evangelos Ververas, Jifei Song, Jiankang Deng, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2411.15779v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) and 3D Gaussian Splatting (3DGS) are popular
techniques to reconstruct and render photo-realistic images. However, the
pre-requisite of running Structure-from-Motion (SfM) to get camera poses limits
their completeness. While previous methods can reconstruct from a few unposed
images, they are not applicable when images are unordered or densely captured.
In this work, we propose ZeroGS to train 3DGS from hundreds of unposed and
unordered images. Our method leverages a pretrained foundation model as the
neural scene representation. Since the accuracy of the predicted pointmaps does
not suffice for accurate image registration and high-fidelity image rendering,
we propose to mitigate the issue by initializing and finetuning the pretrained
model from a seed image. Images are then progressively registered and added to
the training buffer, which is further used to train the model. We also propose
to refine the camera poses and pointmaps by minimizing a point-to-camera ray
consistency loss across multiple views. Experiments on the LLFF dataset, the
MipNeRF360 dataset, and the Tanks-and-Temples dataset show that our method
recovers more accurate camera poses than state-of-the-art pose-free NeRF/3DGS
methods, and even renders higher quality images than 3DGS with COLMAP poses.
Our project page is available at https://aibluefisher.github.io/ZeroGS.

Comments:
- 16 pages, 12 figures

---

## Gassidy: Gaussian Splatting SLAM in Dynamic Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-23 | Long Wen, Shixin Li, Yu Zhang, Yuhong Huang, Jianjie Lin, Fengjunjie Pan, Zhenshan Bing, Alois Knoll | cs.RO | [PDF](http://arxiv.org/pdf/2411.15476v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) allows flexible adjustments to scene
representation, enabling continuous optimization of scene quality during dense
visual simultaneous localization and mapping (SLAM) in static environments.
However, 3DGS faces challenges in handling environmental disturbances from
dynamic objects with irregular movement, leading to degradation in both camera
tracking accuracy and map reconstruction quality. To address this challenge, we
develop an RGB-D dense SLAM which is called Gaussian Splatting SLAM in Dynamic
Environments (Gassidy). This approach calculates Gaussians to generate
rendering loss flows for each environmental component based on a designed
photometric-geometric loss function. To distinguish and filter environmental
disturbances, we iteratively analyze rendering loss flows to detect features
characterized by changes in loss values between dynamic objects and static
components. This process ensures a clean environment for accurate scene
reconstruction. Compared to state-of-the-art SLAM methods, experimental results
on open datasets show that Gassidy improves camera tracking precision by up to
97.9% and enhances map quality by up to 6%.

Comments:
- This paper is currently under reviewed for ICRA 2025

---

## SplatSDF: Boosting Neural Implicit SDF via Gaussian Splatting Fusion

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-23 | Runfa Blark Li, Keito Suzuki, Bang Du, Ki Myung Brian Le, Nikolay Atanasov, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2411.15468v1){: .btn .btn-green } |

**Abstract**: A signed distance function (SDF) is a useful representation for
continuous-space geometry and many related operations, including rendering,
collision checking, and mesh generation. Hence, reconstructing SDF from image
observations accurately and efficiently is a fundamental problem. Recently,
neural implicit SDF (SDF-NeRF) techniques, trained using volumetric rendering,
have gained a lot of attention. Compared to earlier truncated SDF (TSDF) fusion
algorithms that rely on depth maps and voxelize continuous space, SDF-NeRF
enables continuous-space SDF reconstruction with better geometric and
photometric accuracy. However, the accuracy and convergence speed of
scene-level SDF reconstruction require further improvements for many
applications. With the advent of 3D Gaussian Splatting (3DGS) as an explicit
representation with excellent rendering quality and speed, several works have
focused on improving SDF-NeRF by introducing consistency losses on depth and
surface normals between 3DGS and SDF-NeRF. However, loss-level connections
alone lead to incremental improvements. We propose a novel neural implicit SDF
called "SplatSDF" to fuse 3DGSandSDF-NeRF at an architecture level with
significant boosts to geometric and photometric accuracy and convergence speed.
Our SplatSDF relies on 3DGS as input only during training, and keeps the same
complexity and efficiency as the original SDF-NeRF during inference. Our method
outperforms state-of-the-art SDF-NeRF models on geometric and photometric
evaluation by the time of submission.



---

## NeRF Inpainting with Geometric Diffusion Prior and Balanced Score  Distillation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-23 | Menglin Zhang, Xin Luo, Yunwei Lan, Chang Liu, Rui Li, Kaidong Zhang, Ganlin Yang, Dong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2411.15551v1){: .btn .btn-green } |

**Abstract**: Recent advances in NeRF inpainting have leveraged pretrained diffusion models
to enhance performance. However, these methods often yield suboptimal results
due to their ineffective utilization of 2D diffusion priors. The limitations
manifest in two critical aspects: the inadequate capture of geometric
information by pretrained diffusion models and the suboptimal guidance provided
by existing Score Distillation Sampling (SDS) methods. To address these
problems, we introduce GB-NeRF, a novel framework that enhances NeRF inpainting
through improved utilization of 2D diffusion priors. Our approach incorporates
two key innovations: a fine-tuning strategy that simultaneously learns
appearance and geometric priors and a specialized normal distillation loss that
integrates these geometric priors into NeRF inpainting. We propose a technique
called Balanced Score Distillation (BSD) that surpasses existing methods such
as Score Distillation (SDS) and the improved version, Conditional Score
Distillation (CSD). BSD offers improved inpainting quality in appearance and
geometric aspects. Extensive experiments show that our method provides superior
appearance fidelity and geometric consistency compared to existing approaches.



---

## SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion  Flow Field for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-23 | Su Sun, Cheng Zhao, Zhuoyang Sun, Yingjie Victor Chen, Mei Chen | cs.CV | [PDF](http://arxiv.org/pdf/2411.15482v1){: .btn .btn-green } |

**Abstract**: Most existing Dynamic Gaussian Splatting methods for complex dynamic urban
scenarios rely on accurate object-level supervision from expensive manual
labeling, limiting their scalability in real-world applications. In this paper,
we introduce SplatFlow, a Self-Supervised Dynamic Gaussian Splatting within
Neural Motion Flow Fields (NMFF) to learn 4D space-time representations without
requiring tracked 3D bounding boxes, enabling accurate dynamic scene
reconstruction and novel view RGB, depth and flow synthesis. SplatFlow designs
a unified framework to seamlessly integrate time-dependent 4D Gaussian
representation within NMFF, where NMFF is a set of implicit functions to model
temporal motions of both LiDAR points and Gaussians as continuous motion flow
fields. Leveraging NMFF, SplatFlow effectively decomposes static background and
dynamic objects, representing them with 3D and 4D Gaussian primitives,
respectively. NMFF also models the status correspondences of each 4D Gaussian
across time, which aggregates temporal features to enhance cross-view
consistency of dynamic components. SplatFlow further improves dynamic scene
identification by distilling features from 2D foundational models into 4D
space-time representation. Comprehensive evaluations conducted on the Waymo
Open Dataset and KITTI Dataset validate SplatFlow's state-of-the-art (SOTA)
performance for both image reconstruction and novel view synthesis in dynamic
urban scenarios.



---

## EMD: Explicit Motion Modeling for High-Quality Street Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-23 | Xiaobao Wei, Qingpo Wuwu, Zhongyu Zhao, Zhuangzhe Wu, Nan Huang, Ming Lu, Ningning MA, Shanghang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.15582v1){: .btn .btn-green } |

**Abstract**: Photorealistic reconstruction of street scenes is essential for developing
real-world simulators in autonomous driving. While recent methods based on
3D/4D Gaussian Splatting (GS) have demonstrated promising results, they still
encounter challenges in complex street scenes due to the unpredictable motion
of dynamic objects. Current methods typically decompose street scenes into
static and dynamic objects, learning the Gaussians in either a supervised
manner (e.g., w/ 3D bounding-box) or a self-supervised manner (e.g., w/o 3D
bounding-box). However, these approaches do not effectively model the motions
of dynamic objects (e.g., the motion speed of pedestrians is clearly different
from that of vehicles), resulting in suboptimal scene decomposition. To address
this, we propose Explicit Motion Decomposition (EMD), which models the motions
of dynamic objects by introducing learnable motion embeddings to the Gaussians,
enhancing the decomposition in street scenes. The proposed EMD is a
plug-and-play approach applicable to various baseline methods. We also propose
tailored training strategies to apply EMD to both supervised and
self-supervised baselines. Through comprehensive experimentation, we illustrate
the effectiveness of our approach with various established baselines. The code
will be released at: https://qingpowuwu.github.io/emdgaussian.github.io/.



---

## VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-22 | Haiming Zhang, Wending Zhou, Yiyao Zhu, Xu Yan, Jiantao Gao, Dongfeng Bai, Yingjie Cai, Bingbing Liu, Shuguang Cui, Zhen Li | cs.CV | [PDF](http://arxiv.org/pdf/2411.14716v1){: .btn .btn-green } |

**Abstract**: This paper introduces VisionPAD, a novel self-supervised pre-training
paradigm designed for vision-centric algorithms in autonomous driving. In
contrast to previous approaches that employ neural rendering with explicit
depth supervision, VisionPAD utilizes more efficient 3D Gaussian Splatting to
reconstruct multi-view representations using only images as supervision.
Specifically, we introduce a self-supervised method for voxel velocity
estimation. By warping voxels to adjacent frames and supervising the rendered
outputs, the model effectively learns motion cues in the sequential data.
Furthermore, we adopt a multi-frame photometric consistency approach to enhance
geometric perception. It projects adjacent frames to the current frame based on
rendered depths and relative poses, boosting the 3D geometric representation
through pure image supervision. Extensive experiments on autonomous driving
datasets demonstrate that VisionPAD significantly improves performance in 3D
object detection, occupancy prediction and map segmentation, surpassing
state-of-the-art pre-training strategies by a considerable margin.



---

## 3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-22 | Jan Held, Renaud Vandeghen, Abdullah Hamdi, Adrien Deliege, Anthony Cioppa, Silvio Giancola, Andrea Vedaldi, Bernard Ghanem, Marc Van Droogenbroeck | cs.CV | [PDF](http://arxiv.org/pdf/2411.14974v2){: .btn .btn-green } |

**Abstract**: Recent advances in radiance field reconstruction, such as 3D Gaussian
Splatting (3DGS), have achieved high-quality novel view synthesis and fast
rendering by representing scenes with compositions of Gaussian primitives.
However, 3D Gaussians present several limitations for scene reconstruction.
Accurately capturing hard edges is challenging without significantly increasing
the number of Gaussians, creating a large memory footprint. Moreover, they
struggle to represent flat surfaces, as they are diffused in space. Without
hand-crafted regularizers, they tend to disperse irregularly around the actual
surface. To circumvent these issues, we introduce a novel method, named 3D
Convex Splatting (3DCS), which leverages 3D smooth convexes as primitives for
modeling geometrically-meaningful radiance fields from multi-view images.
Smooth convex shapes offer greater flexibility than Gaussians, allowing for a
better representation of 3D scenes with hard edges and dense volumes using
fewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achieves
superior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks and
Temples, and Deep Blending. Specifically, our method attains an improvement of
up to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining high
rendering speeds and reducing the number of required primitives. Our results
highlight the potential of 3D Convex Splatting to become the new standard for
high-quality scene reconstruction and novel view synthesis. Project page:
convexsplatting.github.io.

Comments:
- 13 pages, 13 figures, 10 tables

---

## Neural 4D Evolution under Large Topological Changes from 2D Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-22 | AmirHossein Naghi Razlighi, Tiago Novello, Asen Nachkov, Thomas Probst, Danda Paudel | cs.CV | [PDF](http://arxiv.org/pdf/2411.15018v1){: .btn .btn-green } |

**Abstract**: In the literature, it has been shown that the evolution of the known explicit
3D surface to the target one can be learned from 2D images using the
instantaneous flow field, where the known and target 3D surfaces may largely
differ in topology. We are interested in capturing 4D shapes whose topology
changes largely over time. We encounter that the straightforward extension of
the existing 3D-based method to the desired 4D case performs poorly.
  In this work, we address the challenges in extending 3D neural evolution to
4D under large topological changes by proposing two novel modifications. More
precisely, we introduce (i) a new architecture to discretize and encode the
deformation and learn the SDF and (ii) a technique to impose the temporal
consistency. (iii) Also, we propose a rendering scheme for color prediction
based on Gaussian splatting. Furthermore, to facilitate learning directly from
2D images, we propose a learning framework that can disentangle the geometry
and appearance from RGB images. This method of disentanglement, while also
useful for the 4D evolution problem that we are concentrating on, is also novel
and valid for static scenes. Our extensive experiments on various data provide
awesome results and, most importantly, open a new approach toward
reconstructing challenging scenes with significant topological changes and
deformations. Our source code and the dataset are publicly available at
https://github.com/insait-institute/N4DE.

Comments:
- 15 pages, 21 figures

---

## Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly  Training for 4D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-22 | Zhening Liu, Yingdong Hu, Xinjie Zhang, Jiawei Shao, Zehong Lin, Jun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.14847v1){: .btn .btn-green } |

**Abstract**: The recent development of 3D Gaussian Splatting (3DGS) has led to great
interest in 4D dynamic spatial reconstruction from multi-view visual inputs.
While existing approaches mainly rely on processing full-length multi-view
videos for 4D reconstruction, there has been limited exploration of iterative
online reconstruction methods that enable on-the-fly training and per-frame
streaming. Current 3DGS-based streaming methods treat the Gaussian primitives
uniformly and constantly renew the densified Gaussians, thereby overlooking the
difference between dynamic and static features and also neglecting the temporal
continuity in the scene. To address these limitations, we propose a novel
three-stage pipeline for iterative streamable 4D dynamic spatial
reconstruction. Our pipeline comprises a selective inheritance stage to
preserve temporal continuity, a dynamics-aware shift stage for distinguishing
dynamic and static primitives and optimizing their movements, and an
error-guided densification stage to accommodate emerging objects. Our method
achieves state-of-the-art performance in online 4D reconstruction,
demonstrating a 20% improvement in on-the-fly training speed, superior
representation quality, and real-time rendering capability. Project page:
https://www.liuzhening.top/DASS

Comments:
- Project page: https://www.liuzhening.top/DASS

---

## UniGaussian: Driving Scene Reconstruction from Multiple Camera Models  via Unified Gaussian Representations


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-22 | Yuan Ren, Guile Wu, Runhao Li, Zheyuan Yang, Yibo Liu, Xingxin Chen, Tongtong Cao, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2411.15355v1){: .btn .btn-green } |

**Abstract**: Urban scene reconstruction is crucial for real-world autonomous driving
simulators. Although existing methods have achieved photorealistic
reconstruction, they mostly focus on pinhole cameras and neglect fisheye
cameras. In fact, how to effectively simulate fisheye cameras in driving scene
remains an unsolved problem. In this work, we propose UniGaussian, a novel
approach that learns a unified 3D Gaussian representation from multiple camera
models for urban scene reconstruction in autonomous driving. Our contributions
are two-fold. First, we propose a new differentiable rendering method that
distorts 3D Gaussians using a series of affine transformations tailored to
fisheye camera models. This addresses the compatibility issue of 3D Gaussian
splatting with fisheye cameras, which is hindered by light ray distortion
caused by lenses or mirrors. Besides, our method maintains real-time rendering
while ensuring differentiability. Second, built on the differentiable rendering
method, we design a new framework that learns a unified Gaussian representation
from multiple camera models. By applying affine transformations to adapt
different camera models and regularizing the shared Gaussians with supervision
from different modalities, our framework learns a unified 3D Gaussian
representation with input data from multiple sources and achieves holistic
driving scene understanding. As a result, our approach models multiple sensors
(pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR
point clouds). Our experiments show that our method achieves superior rendering
quality and fast rendering speed for driving scene simulation.

Comments:
- Technical report

---

## SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting  and Dense Feature Matching

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-21 | Arjun P S, Andrew Melnik, Gora Chand Nandi | cs.RO | [PDF](http://arxiv.org/pdf/2411.14322v1){: .btn .btn-green } |

**Abstract**: Experience Goal Visual Rearrangement task stands as a foundational challenge
within Embodied AI, requiring an agent to construct a robust world model that
accurately captures the goal state. The agent uses this world model to restore
a shuffled scene to its original configuration, making an accurate
representation of the world essential for successfully completing the task. In
this work, we present a novel framework that leverages on 3D Gaussian Splatting
as a 3D scene representation for experience goal visual rearrangement task.
Recent advances in volumetric scene representation like 3D Gaussian Splatting,
offer fast rendering of high quality and photo-realistic novel views. Our
approach enables the agent to have consistent views of the current and the goal
setting of the rearrangement task, which enables the agent to directly compare
the goal state and the shuffled state of the world in image space. To compare
these views, we propose to use a dense feature matching method with visual
features extracted from a foundation model, leveraging its advantages of a more
universal feature representation, which facilitates robustness, and
generalization. We validate our approach on the AI2-THOR rearrangement
challenge benchmark and demonstrate improvements over the current state of the
art methods



---

## Unleashing the Potential of Multi-modal Foundation Models and Video  Diffusion for 4D Dynamic Physical Scene Simulation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-21 | Zhuoman Liu, Weicai Ye, Yan Luximon, Pengfei Wan, Di Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.14423v1){: .btn .btn-green } |

**Abstract**: Realistic simulation of dynamic scenes requires accurately capturing diverse
material properties and modeling complex object interactions grounded in
physical principles. However, existing methods are constrained to basic
material types with limited predictable parameters, making them insufficient to
represent the complexity of real-world materials. We introduce a novel approach
that leverages multi-modal foundation models and video diffusion to achieve
enhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to
identify material types and initialize material parameters through image
queries, while simultaneously inferring 3D Gaussian splats for detailed scene
representation. We further refine these material parameters using video
diffusion with a differentiable Material Point Method (MPM) and optical flow
guidance rather than render loss or Score Distillation Sampling (SDS) loss.
This integrated framework enables accurate prediction and realistic simulation
of dynamic interactions in real-world scenarios, advancing both accuracy and
flexibility in physics-based simulations.

Comments:
- Homepage: https://zhuomanliu.github.io/PhysFlow/

---

## NexusSplats: Efficient 3D Gaussian Splatting in the Wild

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-21 | Yuzhou Tang, Dejun Xu, Yongjie Hou, Zhenzhong Wang, Min Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2411.14514v2){: .btn .btn-green } |

**Abstract**: While 3D Gaussian Splatting (3DGS) has recently demonstrated remarkable
rendering quality and efficiency in 3D scene reconstruction, it struggles with
varying lighting conditions and incidental occlusions in real-world scenarios.
To accommodate varying lighting conditions, existing 3DGS extensions apply
color mapping to the massive Gaussian primitives with individually optimized
appearance embeddings. To handle occlusions, they predict pixel-wise
uncertainties via 2D image features for occlusion capture. Nevertheless, such
massive color mapping and pixel-wise uncertainty prediction strategies suffer
from not only additional computational costs but also coarse-grained lighting
and occlusion handling. In this work, we propose a nexus kernel-driven
approach, termed NexusSplats, for efficient and finer 3D scene reconstruction
under complex lighting and occlusion conditions. In particular, NexusSplats
leverages a novel light decoupling strategy where appearance embeddings are
optimized based on nexus kernels instead of massive Gaussian primitives, thus
accelerating reconstruction speeds while ensuring local color consistency for
finer textures. Additionally, a Gaussian-wise uncertainty mechanism is
developed, aligning 3D structures with 2D image features for fine-grained
occlusion handling. Experimental results demonstrate that NexusSplats achieves
state-of-the-art rendering quality while reducing reconstruction time by up to
70.4% compared to the current best in quality.

Comments:
- Project page: https://nexus-splats.github.io/

---

## Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable  Single-stage Image-to-3D Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-21 | Yuanhao Cai, He Zhang, Kai Zhang, Yixun Liang, Mengwei Ren, Fujun Luan, Qing Liu, Soo Ye Kim, Jianming Zhang, Zhifei Zhang, Yuqian Zhou, Zhe Lin, Alan Yuille | cs.CV | [PDF](http://arxiv.org/pdf/2411.14384v1){: .btn .btn-green } |

**Abstract**: Existing feed-forward image-to-3D methods mainly rely on 2D multi-view
diffusion models that cannot guarantee 3D consistency. These methods easily
collapse when changing the prompt view direction and mainly handle
object-centric prompt images. In this paper, we propose a novel single-stage 3D
diffusion model, DiffusionGS, for object and scene generation from a single
view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to
enforce view consistency and allow the model to generate robustly given prompt
views of any directions, beyond object-centric inputs. Plus, to improve the
capability and generalization ability of DiffusionGS, we scale up 3D training
data by developing a scene-object mixed training strategy. Experiments show
that our method enjoys better generation quality (2.20 dB higher in PSNR and
23.25 lower in FID) and over 5x faster speed (~6s on an A100 GPU) than SOTA
methods. The user study and text-to-3D applications also reveals the practical
values of our method. Our Project page at
https://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and
interactive generation results.

Comments:
- A novel one-stage 3DGS-based diffusion generates objects and scenes
  from a single view in ~6 seconds

---

## GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-20 | Xiaobao Wei, Peng Chen, Guangyu Li, Ming Lu, Hui Chen, Feng Tian | cs.CV | [PDF](http://arxiv.org/pdf/2411.12981v1){: .btn .btn-green } |

**Abstract**: Gaze estimation encounters generalization challenges when dealing with
out-of-distribution data. To address this problem, recent methods use neural
radiance fields (NeRF) to generate augmented data. However, existing methods
based on NeRF are computationally expensive and lack facial details. 3D
Gaussian Splatting (3DGS) has become the prevailing representation of neural
fields. While 3DGS has been extensively examined in head avatars, it faces
challenges with accurate gaze control and generalization across different
subjects. In this work, we propose GazeGaussian, a high-fidelity gaze
redirection method that uses a two-stream 3DGS model to represent the face and
eye regions separately. By leveraging the unstructured nature of 3DGS, we
develop a novel eye representation for rigid eye rotation based on the target
gaze direction. To enhance synthesis generalization across various subjects, we
integrate an expression-conditional module to guide the neural renderer.
Comprehensive experiments show that GazeGaussian outperforms existing methods
in rendering speed, gaze redirection accuracy, and facial synthesis across
multiple datasets. We also demonstrate that existing gaze estimation methods
can leverage GazeGaussian to improve their generalization performance. The code
will be available at: https://ucwxb.github.io/GazeGaussian/.



---

## Video2BEV: Transforming Drone Videos to BEVs for Video-based  Geo-localization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-20 | Hao Ju, Zhedong Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2411.13610v1){: .btn .btn-green } |

**Abstract**: Existing approaches to drone visual geo-localization predominantly adopt the
image-based setting, where a single drone-view snapshot is matched with images
from other platforms. Such task formulation, however, underutilizes the
inherent video output of the drone and is sensitive to occlusions and
environmental constraints. To address these limitations, we formulate a new
video-based drone geo-localization task and propose the Video2BEV paradigm.
This paradigm transforms the video into a Bird's Eye View (BEV), simplifying
the subsequent matching process. In particular, we employ Gaussian Splatting to
reconstruct a 3D scene and obtain the BEV projection. Different from the
existing transform methods, \eg, polar transform, our BEVs preserve more
fine-grained details without significant distortion. To further improve model
scalability toward diverse BEVs and satellite figures, our Video2BEV paradigm
also incorporates a diffusion-based module for generating hard negative
samples, which facilitates discriminative feature learning. To validate our
approach, we introduce UniV, a new video-based geo-localization dataset that
extends the image-based University-1652 dataset. UniV features flight paths at
$30^\circ$ and $45^\circ$ elevation angles with increased frame rates of up to
10 frames per second (FPS). Extensive experiments on the UniV dataset show that
our Video2BEV paradigm achieves competitive recall rates and outperforms
conventional video-based methods. Compared to other methods, our proposed
approach exhibits robustness at lower elevations with more occlusions.



---

## Sparse Input View Synthesis: 3D Representations and Reliable Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-20 | Nagabhushan Somraj | cs.CV | [PDF](http://arxiv.org/pdf/2411.13631v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis refers to the problem of synthesizing novel viewpoints
of a scene given the images from a few viewpoints. This is a fundamental
problem in computer vision and graphics, and enables a vast variety of
applications such as meta-verse, free-view watching of events, video gaming,
video stabilization and video compression. Recent 3D representations such as
radiance fields and multi-plane images significantly improve the quality of
images rendered from novel viewpoints. However, these models require a dense
sampling of input views for high quality renders. Their performance goes down
significantly when only a few input views are available. In this thesis, we
focus on the sparse input novel view synthesis problem for both static and
dynamic scenes. In the first part of this work, we mainly focus on sparse input
novel view synthesis of static scenes using neural radiance fields (NeRF). We
study the design of reliable and dense priors to better regularize the NeRF in
such situations. In particular, we propose a prior on the visibility of the
pixels in a pair of input views. We show that this visibility prior, which is
related to the relative depth of objects, is dense and more reliable than
existing priors on absolute depth. We compute the visibility prior using plane
sweep volumes without the need to train a neural network on large datasets. We
evaluate our approach on multiple datasets and show that our model outperforms
existing approaches for sparse input novel view synthesis. In the second part,
we aim to further improve the regularization by learning a scene-specific prior
that does not suffer from generalization issues. We achieve this by learning
the prior on the given scene alone without pre-training on large datasets. In
particular, we design augmented NeRFs to obtain better depth supervision in
certain regions of the scene for the main NeRF. Further, we extend this
framework to also apply to newer and faster radiance field models such as
TensoRF and ZipNeRF. Through extensive experiments on multiple datasets, we
show the superiority of our approach in sparse input novel view synthesis. The
design of sparse input fast dynamic radiance fields is severely constrained by
the lack of suitable representations and reliable priors for motion. We address
the first challenge by designing an explicit motion model based on factorized
volumes that is compact and optimizes quickly. We also introduce reliable
sparse flow priors to constrain the motion field, since we find that the
popularly employed dense optical flow priors are unreliable. We show the
benefits of our motion representation and reliable priors on multiple datasets.
In the final part of this thesis, we study the application of view synthesis
for frame rate upsampling in video gaming. Specifically, we consider the
problem of temporal view synthesis, where the goal is to predict the future
frames given the past frames and the camera motion. The key challenge here is
in predicting the future motion of the objects by estimating their past motion
and extrapolating it. We explore the use of multi-plane image representations
and scene depth to reliably estimate the object motion, particularly in the
occluded regions. We design a new database to effectively evaluate our approach
for temporal view synthesis of dynamic scenes and show that we achieve
state-of-the-art performance.

Comments:
- PhD Thesis of Nagabhushan S N, Dept of ECE, Indian Institute of
  Science (IISc); Advisor: Dr. Rajiv Soundararajan; Thesis Reviewers: Dr.
  Kaushik Mitra (IIT Madras), Dr. Aniket Bera (Purdue University); Submitted:
  May 2024; Accepted and Defended: Sep 2024; Abstract condensed, please check
  the PDF for full abstract

---

## FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-20 | Ola Shorinwa, Jiankai Sun, Mac Schwager | cs.CV | [PDF](http://arxiv.org/pdf/2411.13753v1){: .btn .btn-green } |

**Abstract**: We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting,
which seeks to address the main limitations of existing semantic Gaussian
Splatting methods, namely: slow training and rendering speeds; high memory
usage; and ambiguous semantic object localization. In deriving FAST-Splat , we
formulate open-vocabulary semantic Gaussian Splatting as the problem of
extending closed-set semantic distillation to the open-set (open-vocabulary)
setting, enabling FAST-Splat to provide precise semantic object localization
results, even when prompted with ambiguous user-provided natural-language
queries. Further, by exploiting the explicit form of the Gaussian Splatting
scene representation to the fullest extent, FAST-Splat retains the remarkable
training and rendering speeds of Gaussian Splatting. Specifically, while
existing semantic Gaussian Splatting methods distill semantics into a separate
neural field or utilize neural models for dimensionality reduction, FAST-Splat
directly augments each Gaussian with specific semantic codes, preserving the
training, rendering, and memory-usage advantages of Gaussian Splatting over
neural field methods. These Gaussian-specific semantic codes, together with a
hash-table, enable semantic similarity to be measured with open-vocabulary user
prompts and further enable FAST-Splat to respond with unambiguous semantic
object labels and 3D masks, unlike prior methods. In experiments, we
demonstrate that FAST-Splat is 4x to 6x faster to train with a 13x faster data
pre-processing step, achieves between 18x to 75x faster rendering speeds, and
requires about 3x smaller GPU memory, compared to the best-competing semantic
Gaussian Splatting methods. Further, FAST-Splat achieves relatively similar or
better semantic segmentation performance compared to existing methods. After
the review period, we will provide links to the project website and the
codebase.



---

## Robust SG-NeRF: Robust Scene Graph Aided Neural Surface Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-20 | Yi Gu, Dongjun Ye, Zhaorui Wang, Jiaxu Wang, Jiahang Cao, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.13620v1){: .btn .btn-green } |

**Abstract**: Neural surface reconstruction relies heavily on accurate camera poses as
input. Despite utilizing advanced pose estimators like COLMAP or ARKit, camera
poses can still be noisy. Existing pose-NeRF joint optimization methods handle
poses with small noise (inliers) effectively but struggle with large noise
(outliers), such as mirrored poses. In this work, we focus on mitigating the
impact of outlier poses. Our method integrates an inlier-outlier confidence
estimation scheme, leveraging scene graph information gathered during the data
preparation phase. Unlike previous works directly using rendering metrics as
the reference, we employ a detached color network that omits the viewing
direction as input to minimize the impact caused by shape-radiance ambiguities.
This enhanced confidence updating strategy effectively differentiates between
inlier and outlier poses, allowing us to sample more rays from inlier poses to
construct more reliable radiance fields. Additionally, we introduce a
re-projection loss based on the current Signed Distance Function (SDF) and pose
estimations, strengthening the constraints between matching image pairs. For
outlier poses, we adopt a Monte Carlo re-localization method to find better
solutions. We also devise a scene graph updating strategy to provide more
accurate information throughout the training process. We validate our approach
on the SG-NeRF and DTU datasets. Experimental results on various datasets
demonstrate that our methods can consistently improve the reconstruction
qualities and pose accuracies.

Comments:
- https://rsg-nerf.github.io/RSG-NeRF/

---

## Generating 3D-Consistent Videos from Unposed Internet Photos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-20 | Gene Chou, Kai Zhang, Sai Bi, Hao Tan, Zexiang Xu, Fujun Luan, Bharath Hariharan, Noah Snavely | cs.CV | [PDF](http://arxiv.org/pdf/2411.13549v1){: .btn .btn-green } |

**Abstract**: We address the problem of generating videos from unposed internet photos. A
handful of input images serve as keyframes, and our model interpolates between
them to simulate a path moving between the cameras. Given random images, a
model's ability to capture underlying geometry, recognize scene identity, and
relate frames in terms of camera position and orientation reflects a
fundamental understanding of 3D structure and scene layout. However, existing
video models such as Luma Dream Machine fail at this task. We design a
self-supervised method that takes advantage of the consistency of videos and
variability of multiview internet photos to train a scalable, 3D-aware video
model without any 3D annotations such as camera parameters. We validate that
our method outperforms all baselines in terms of geometric and appearance
consistency. We also show our model benefits applications that enable camera
control, such as 3D Gaussian Splatting. Our results suggest that we can scale
up scene-level 3D learning using only 2D data such as videos and multiview
internet photos.



---

## SCIGS: 3D Gaussians Splatting from a Snapshot Compressive Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Zixu Wang, Hao Yang, Yu Guo, Fei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.12471v1){: .btn .btn-green } |

**Abstract**: Snapshot Compressive Imaging (SCI) offers a possibility for capturing
information in high-speed dynamic scenes, requiring efficient reconstruction
method to recover scene information. Despite promising results, current deep
learning-based and NeRF-based reconstruction methods face challenges: 1) deep
learning-based reconstruction methods struggle to maintain 3D structural
consistency within scenes, and 2) NeRF-based reconstruction methods still face
limitations in handling dynamic scenes. To address these challenges, we propose
SCIGS, a variant of 3DGS, and develop a primitive-level transformation network
that utilizes camera pose stamps and Gaussian primitive coordinates as
embedding vectors. This approach resolves the necessity of camera pose in
vanilla 3DGS and enhances multi-view 3D structural consistency in dynamic
scenes by utilizing transformed primitives. Additionally, a high-frequency
filter is introduced to eliminate the artifacts generated during the
transformation. The proposed SCIGS is the first to reconstruct a 3D explicit
scene from a single compressed image, extending its application to dynamic 3D
scenes. Experiments on both static and dynamic scenes demonstrate that SCIGS
not only enhances SCI decoding but also outperforms current state-of-the-art
methods in reconstructing dynamic 3D scenes from a single compressed image. The
code will be made available upon publication.



---

## GaussianPretrain: A Simple Unified 3D Gaussian Representation for Visual  Pre-training in Autonomous Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Shaoqing Xu, Fang Li, Shengyin Jiang, Ziying Song, Li Liu, Zhi-xin Yang | cs.CV | [PDF](http://arxiv.org/pdf/2411.12452v1){: .btn .btn-green } |

**Abstract**: Self-supervised learning has made substantial strides in image processing,
while visual pre-training for autonomous driving is still in its infancy.
Existing methods often focus on learning geometric scene information while
neglecting texture or treating both aspects separately, hindering comprehensive
scene understanding. In this context, we are excited to introduce
GaussianPretrain, a novel pre-training paradigm that achieves a holistic
understanding of the scene by uniformly integrating geometric and texture
representations. Conceptualizing 3D Gaussian anchors as volumetric LiDAR
points, our method learns a deepened understanding of scenes to enhance
pre-training performance with detailed spatial structure and texture, achieving
that 40.6% faster than NeRF-based method UniPAD with 70% GPU memory only. We
demonstrate the effectiveness of GaussianPretrain across multiple 3D perception
tasks, showing significant performance improvements, such as a 7.05% increase
in NDS for 3D object detection, boosts mAP by 1.9% in HD map construction and
0.8% improvement on Occupancy prediction. These significant gains highlight
GaussianPretrain's theoretical innovation and strong practical potential,
promoting visual pre-training development for autonomous driving. Source code
will be available at https://github.com/Public-BOTs/GaussianPretrain

Comments:
- 10 pages, 5 figures

---

## Automated 3D Physical Simulation of Open-world Scene with Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Haoyu Zhao, Hao Wang, Xingyue Zhao, Hongqiu Wang, Zhiyu Wu, Chengjiang Long, Hua Zou | cs.CV | [PDF](http://arxiv.org/pdf/2411.12789v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D generation models have opened new possibilities for
simulating dynamic 3D object movements and customizing behaviors, yet creating
this content remains challenging. Current methods often require manual
assignment of precise physical properties for simulations or rely on video
generation models to predict them, which is computationally intensive. In this
paper, we rethink the usage of multi-modal large language model (MLLM) in
physics-based simulation, and present Sim Anything, a physics-based approach
that endows static 3D objects with interactive dynamics. We begin with detailed
scene reconstruction and object-level 3D open-vocabulary segmentation,
progressing to multi-view image in-painting. Inspired by human visual
reasoning, we propose MLLM-based Physical Property Perception (MLLM-P3) to
predict mean physical properties of objects in a zero-shot manner. Based on the
mean values and the object's geometry, the Material Property Distribution
Prediction model (MPDP) model then estimates the full distribution,
reformulating the problem as probability distribution estimation to reduce
computational costs. Finally, we simulate objects in an open-world scene with
particles sampled via the Physical-Geometric Adaptive Sampling (PGAS) strategy,
efficiently capturing complex deformations and significantly reducing
computational costs. Extensive experiments and user studies demonstrate our Sim
Anything achieves more realistic motion than state-of-the-art methods within 2
minutes on a single GPU.



---

## Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear  Kernels

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Haodong Chen, Runnan Chen, Qiang Qu, Zhaoqing Wang, Tongliang Liu, Xiaoming Chen, Yuk Ying Chung | cs.CV | [PDF](http://arxiv.org/pdf/2411.12440v2){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS) have substantially
improved novel view synthesis, enabling high-quality reconstruction and
real-time rendering. However, blurring artifacts, such as floating primitives
and over-reconstruction, remain challenging. Current methods address these
issues by refining scene structure, enhancing geometric representations,
addressing blur in training images, improving rendering consistency, and
optimizing density control, yet the role of kernel design remains
underexplored. We identify the soft boundaries of Gaussian ellipsoids as one of
the causes of these artifacts, limiting detail capture in high-frequency
regions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which
replaces Gaussian kernels with linear kernels to achieve sharper and more
precise results, particularly in high-frequency regions. Through evaluations on
three datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along
with a 30% FPS improvement over baseline 3DGS. The implementation will be made
publicly available upon acceptance.



---

## PR-ENDO: Physically Based Relightable Gaussian Splatting for Endoscopy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Joanna Kaleta, Weronika Smolak-Dyżewska, Dawid Malarz, Diego Dall'Alba, Przemysław Korzeniowski, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2411.12510v1){: .btn .btn-green } |

**Abstract**: Endoscopic procedures are crucial for colorectal cancer diagnosis, and
three-dimensional reconstruction of the environment for real-time novel-view
synthesis can significantly enhance diagnosis. We present PR-ENDO, a framework
that leverages 3D Gaussian Splatting within a physically based, relightable
model tailored for the complex acquisition conditions in endoscopy, such as
restricted camera rotations and strong view-dependent illumination. By
exploiting the connection between the camera and light source, our approach
introduces a relighting model to capture the intricate interactions between
light and tissue using physically based rendering and MLP. Existing methods
often produce artifacts and inconsistencies under these conditions, which
PR-ENDO overcomes by incorporating a specialized diffuse MLP that utilizes
light angles and normal vectors, achieving stable reconstructions even with
limited training camera rotations. We benchmarked our framework using a
publicly available dataset and a newly introduced dataset with wider camera
rotations. Our methods demonstrated superior image quality compared to baseline
approaches.



---

## Sketch-guided Cage-based 3D Gaussian Splatting Deformation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Tianhao Xie, Noam Aigerman, Eugene Belilovsky, Tiberiu Popa | cs.CV | [PDF](http://arxiv.org/pdf/2411.12168v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (GS) is one of the most promising novel 3D
representations that has received great interest in computer graphics and
computer vision. While various systems have introduced editing capabilities for
3D GS, such as those guided by text prompts, fine-grained control over
deformation remains an open challenge. In this work, we present a novel
sketch-guided 3D GS deformation system that allows users to intuitively modify
the geometry of a 3D GS model by drawing a silhouette sketch from a single
viewpoint. Our approach introduces a new deformation method that combines
cage-based deformations with a variant of Neural Jacobian Fields, enabling
precise, fine-grained control. Additionally, it leverages large-scale 2D
diffusion priors and ControlNet to ensure the generated deformations are
semantically plausible. Through a series of experiments, we demonstrate the
effectiveness of our method and showcase its ability to animate static 3D GS
models as one of its key applications.

Comments:
- 10 pages, 9 figures, project page:
  https://tianhaoxie.github.io/project/gs_deform/

---

## LiV-GS: LiDAR-Vision Integration for 3D Gaussian Splatting SLAM in  Outdoor Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Renxiang Xiao, Wei Liu, Yushuai Chen, Liang Hu | cs.RO | [PDF](http://arxiv.org/pdf/2411.12185v1){: .btn .btn-green } |

**Abstract**: We present LiV-GS, a LiDAR-visual SLAM system in outdoor environments that
leverages 3D Gaussian as a differentiable spatial representation. Notably,
LiV-GS is the first method that directly aligns discrete and sparse LiDAR data
with continuous differentiable Gaussian maps in large-scale outdoor scenes,
overcoming the limitation of fixed resolution in traditional LiDAR mapping. The
system aligns point clouds with Gaussian maps using shared covariance
attributes for front-end tracking and integrates the normal orientation into
the loss function to refines the Gaussian map. To reliably and stably update
Gaussians outside the LiDAR field of view, we introduce a novel conditional
Gaussian constraint that aligns these Gaussians closely with the nearest
reliable ones. The targeted adjustment enables LiV-GS to achieve fast and
accurate mapping with novel view synthesis at a rate of 7.98 FPS. Extensive
comparative experiments demonstrate LiV-GS's superior performance in SLAM,
image rendering and mapping. The successful cross-modal radar-LiDAR
localization highlights the potential of LiV-GS for applications in cross-modal
semantic positioning and object segmentation with Gaussian maps.



---

## Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive  Gaussian Densification

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Guangchi Fang, Bing Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.12788v1){: .btn .btn-green } |

**Abstract**: In this study, we explore the essential challenge of fast scene optimization
for Gaussian Splatting. Through a thorough analysis of the geometry modeling
process, we reveal that dense point clouds can be effectively reconstructed
early in optimization through Gaussian representations. This insight leads to
our approach of aggressive Gaussian densification, which provides a more
efficient alternative to conventional progressive densification methods. By
significantly increasing the number of critical Gaussians, we enhance the model
capacity to capture dense scene geometry at the early stage of optimization.
This strategy is seamlessly integrated into the Mini-Splatting densification
and simplification framework, enabling rapid convergence without compromising
quality. Additionally, we introduce visibility culling within Gaussian
Splatting, leveraging per-view Gaussian importance as precomputed visibility to
accelerate the optimization process. Our Mini-Splatting2 achieves a balanced
trade-off among optimization time, the number of Gaussians, and rendering
quality, establishing a strong baseline for future Gaussian-Splatting-based
works. Our work sets the stage for more efficient, high-quality 3D scene
modeling in real-world applications, and the code will be made available no
matter acceptance.



---

## RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Xinhai Li, Jialin Li, Ziheng Zhang, Rui Zhang, Fan Jia, Tiancai Wang, Haoqiang Fan, Kuo-Kun Tseng, Ruiping Wang | cs.RO | [PDF](http://arxiv.org/pdf/2411.11839v1){: .btn .btn-green } |

**Abstract**: Efficient acquisition of real-world embodied data has been increasingly
critical. However, large-scale demonstrations captured by remote operation tend
to take extremely high costs and fail to scale up the data size in an efficient
manner. Sampling the episodes under a simulated environment is a promising way
for large-scale collection while existing simulators fail to high-fidelity
modeling on texture and physics. To address these limitations, we introduce the
RoboGSim, a real2sim2real robotic simulator, powered by 3D Gaussian Splatting
and the physics engine. RoboGSim mainly includes four parts: Gaussian
Reconstructor, Digital Twins Builder, Scene Composer, and Interactive Engine.
It can synthesize the simulated data with novel views, objects, trajectories,
and scenes. RoboGSim also provides an online, reproducible, and safe evaluation
for different manipulation policies. The real2sim and sim2real transfer
experiments show a high consistency in the texture and physics. Moreover, the
effectiveness of synthetic data is validated under the real-world manipulated
tasks. We hope RoboGSim serves as a closed-loop simulator for fair comparison
on policy learning. More information can be found on our project page
https://robogsim.github.io/ .



---

## DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and  Surface Reconstruction for Urban Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Chensheng Peng, Chengwei Zhang, Yixiao Wang, Chenfeng Xu, Yichen Xie, Wenzhao Zheng, Kurt Keutzer, Masayoshi Tomizuka, Wei Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2411.11921v1){: .btn .btn-green } |

**Abstract**: We present DeSiRe-GS, a self-supervised gaussian splatting representation,
enabling effective static-dynamic decomposition and high-fidelity surface
reconstruction in complex driving scenarios. Our approach employs a two-stage
optimization pipeline of dynamic street Gaussians. In the first stage, we
extract 2D motion masks based on the observation that 3D Gaussian Splatting
inherently can reconstruct only the static regions in dynamic environments.
These extracted 2D motion priors are then mapped into the Gaussian space in a
differentiable manner, leveraging an efficient formulation of dynamic Gaussians
in the second stage. Combined with the introduced geometric regularizations,
our method are able to address the over-fitting issues caused by data sparsity
in autonomous driving, reconstructing physically plausible Gaussians that align
with object surfaces rather than floating in air. Furthermore, we introduce
temporal cross-view consistency to ensure coherence across time and viewpoints,
resulting in high-quality surface reconstruction. Comprehensive experiments
demonstrate the efficiency and effectiveness of DeSiRe-GS, surpassing prior
self-supervised arts and achieving accuracy comparable to methods relying on
external 3D bounding box annotations. Code is available at
\url{https://github.com/chengweialan/DeSiRe-GS}



---

## FruitNinja: 3D Object Interior Texture Generation with Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Fangyu Wu, Yuhao Chen | cs.CV | [PDF](http://arxiv.org/pdf/2411.12089v2){: .btn .btn-green } |

**Abstract**: In the real world, objects reveal internal textures when sliced or cut, yet
this behavior is not well-studied in 3D generation tasks today. For example,
slicing a virtual 3D watermelon should reveal flesh and seeds. Given that no
available dataset captures an object's full internal structure and collecting
data from all slices is impractical, generative methods become the obvious
approach. However, current 3D generation and inpainting methods often focus on
visible appearance and overlook internal textures. To bridge this gap, we
introduce FruitNinja, the first method to generate internal textures for 3D
objects undergoing geometric and topological changes. Our approach produces
objects via 3D Gaussian Splatting (3DGS) with both surface and interior
textures synthesized, enabling real-time slicing and rendering without
additional optimization. FruitNinja leverages a pre-trained diffusion model to
progressively inpaint cross-sectional views and applies voxel-grid-based
smoothing to achieve cohesive textures throughout the object. Our OpaqueAtom GS
strategy overcomes 3DGS limitations by employing densely distributed opaque
Gaussians, avoiding biases toward larger particles that destabilize training
and sharp color transitions for fine-grained textures. Experimental results
show that FruitNinja substantially outperforms existing approaches, showcasing
unmatched visual quality in real-time rendered internal views across arbitrary
geometry manipulations.



---

## Towards Degradation-Robust Reconstruction in Generalizable NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Chan Ho Park, Ka Leong Cheng, Zhicheng Wang, Qifeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2411.11691v1){: .btn .btn-green } |

**Abstract**: Generalizable Neural Radiance Field (GNeRF) across scenes has been proven to
be an effective way to avoid per-scene optimization by representing a scene
with deep image features of source images. However, despite its potential for
real-world applications, there has been limited research on the robustness of
GNeRFs to different types of degradation present in the source images. The lack
of such research is primarily attributed to the absence of a large-scale
dataset fit for training a degradation-robust generalizable NeRF model. To
address this gap and facilitate investigations into the degradation robustness
of 3D reconstruction tasks, we construct the Objaverse Blur Dataset, comprising
50,000 images from over 1000 settings featuring multiple levels of blur
degradation. In addition, we design a simple and model-agnostic module for
enhancing the degradation robustness of GNeRFs. Specifically, by extracting
3D-aware features through a lightweight depth estimator and denoiser, the
proposed module shows improvement on different popular methods in GNeRFs in
terms of both quantitative and visual quality over varying degradation types
and levels. Our dataset and code will be made publicly available.



---

## TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians  for Robust Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | DaDong Jiang, Zhihui Ke, Xiaobo Zhou, Zhi Hou, Xianghui Yang, Wenbo Hu, Tie Qiu, Chunchao Guo | cs.CV | [PDF](http://arxiv.org/pdf/2411.11941v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction is a long-term challenge in 3D vision. Recent
methods extend 3D Gaussian Splatting to dynamic scenes via additional
deformation fields and apply explicit constraints like motion flow to guide the
deformation. However, they learn motion changes from individual timestamps
independently, making it challenging to reconstruct complex scenes,
particularly when dealing with violent movement, extreme-shaped geometries, or
reflective surfaces. To address the above issue, we design a plug-and-play
module called TimeFormer to enable existing deformable 3D Gaussians
reconstruction methods with the ability to implicitly model motion patterns
from a learning perspective. Specifically, TimeFormer includes a Cross-Temporal
Transformer Encoder, which adaptively learns the temporal relationships of
deformable 3D Gaussians. Furthermore, we propose a two-stream optimization
strategy that transfers the motion knowledge learned from TimeFormer to the
base stream during the training phase. This allows us to remove TimeFormer
during inference, thereby preserving the original rendering speed. Extensive
experiments in the multi-view and monocular dynamic scenes validate qualitative
and quantitative improvement brought by TimeFormer. Project Page:
https://patrickddj.github.io/TimeFormer/



---

## LeC$^2$O-NeRF: Learning Continuous and Compact Large-Scale Occupancy for  Urban Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Zhenxing Mi, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.11374v1){: .btn .btn-green } |

**Abstract**: In NeRF, a critical problem is to effectively estimate the occupancy to guide
empty-space skipping and point sampling. Grid-based methods work well for
small-scale scenes. However, on large-scale scenes, they are limited by
predefined bounding boxes, grid resolutions, and high memory usage for grid
updates, and thus struggle to speed up training for large-scale, irregularly
bounded and complex urban scenes without sacrificing accuracy. In this paper,
we propose to learn a continuous and compact large-scale occupancy network,
which can classify 3D points as occupied or unoccupied points. We train this
occupancy network end-to-end together with the radiance field in a
self-supervised manner by three designs. First, we propose a novel imbalanced
occupancy loss to regularize the occupancy network. It makes the occupancy
network effectively control the ratio of unoccupied and occupied points,
motivated by the prior that most of 3D scene points are unoccupied. Second, we
design an imbalanced architecture containing a large scene network and a small
empty space network to separately encode occupied and unoccupied points
classified by the occupancy network. This imbalanced structure can effectively
model the imbalanced nature of occupied and unoccupied regions. Third, we
design an explicit density loss to guide the occupancy network, making the
density of unoccupied points smaller. As far as we know, we are the first to
learn a continuous and compact occupancy of large-scale NeRF by a network. In
our experiments, our occupancy network can quickly learn more compact, accurate
and smooth occupancy compared to the occupancy grid. With our learned occupancy
as guidance for empty space skipping on challenging large-scale benchmarks, our
method consistently obtains higher accuracy compared to the occupancy grid, and
our method can speed up state-of-the-art NeRF methods without sacrificing
accuracy.

Comments:
- 13 pages

---

## GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for  Real-Time Human-Scene Rendering from Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Boyao Zhou, Shunyuan Zheng, Hanzhang Tu, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2411.11363v1){: .btn .btn-green } |

**Abstract**: Differentiable rendering techniques have recently shown promising results for
free-viewpoint video synthesis of characters. However, such methods, either
Gaussian Splatting or neural implicit rendering, typically necessitate
per-subject optimization which does not meet the requirement of real-time
rendering in an interactive application. We propose a generalizable Gaussian
Splatting approach for high-resolution image rendering under a sparse-view
camera setting. To this end, we introduce Gaussian parameter maps defined on
the source views and directly regress Gaussian properties for instant novel
view synthesis without any fine-tuning or optimization. We train our Gaussian
parameter regression module on human-only data or human-scene data, jointly
with a depth estimation module to lift 2D parameter maps to 3D space. The
proposed framework is fully differentiable with both depth and rendering
supervision or with only rendering supervision. We further introduce a
regularization term and an epipolar attention mechanism to preserve geometry
consistency between two source views, especially when neglecting depth
supervision. Experiments on several datasets demonstrate that our method
outperforms state-of-the-art methods while achieving an exceeding rendering
speed.

Comments:
- Journal extension of CVPR 2024,Project
  page:https://yaourtb.github.io/GPS-Gaussian+

---

## VeGaS: Video Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-17 | Weronika Smolak-Dyżewska, Dawid Malarz, Kornel Howil, Jan Kaczmarczyk, Marcin Mazur, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2411.11024v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representations (INRs) employ neural networks to approximate
discrete data as continuous functions. In the context of video data, such
models can be utilized to transform the coordinates of pixel locations along
with frame occurrence times (or indices) into RGB color values. Although INRs
facilitate effective compression, they are unsuitable for editing purposes. One
potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such
as the Video Gaussian Representation (VGR), which is capable of encoding video
as a multitude of 3D Gaussians and is applicable for numerous video processing
operations, including editing. Nevertheless, in this case, the capacity for
modification is constrained to a limited set of basic transformations. To
address this issue, we introduce the Video Gaussian Splatting (VeGaS) model,
which enables realistic modifications of video data. To construct VeGaS, we
propose a novel family of Folded-Gaussian distributions designed to capture
nonlinear dynamics in a video stream and model consecutive frames by 2D
Gaussians obtained as respective conditional distributions. Our experiments
demonstrate that VeGaS outperforms state-of-the-art solutions in frame
reconstruction tasks and allows realistic modifications of video data. The code
is available at: https://github.com/gmum/VeGaS.



---

## Direct and Explicit 3D Generation from a Single Image


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-17 | Haoyu Wu, Meher Gitika Karumuri, Chuhang Zou, Seungbae Bang, Yuelong Li, Dimitris Samaras, Sunil Hadap | cs.CV | [PDF](http://arxiv.org/pdf/2411.10947v1){: .btn .btn-green } |

**Abstract**: Current image-to-3D approaches suffer from high computational costs and lack
scalability for high-resolution outputs. In contrast, we introduce a novel
framework to directly generate explicit surface geometry and texture using
multi-view 2D depth and RGB images along with 3D Gaussian features using a
repurposed Stable Diffusion model. We introduce a depth branch into U-Net for
efficient and high quality multi-view, cross-domain generation and incorporate
epipolar attention into the latent-to-pixel decoder for pixel-level multi-view
consistency. By back-projecting the generated depth pixels into 3D space, we
create a structured 3D representation that can be either rendered via Gaussian
splatting or extracted to high-quality meshes, thereby leveraging additional
novel view synthesis loss to further improve our performance. Extensive
experiments demonstrate that our method surpasses existing baselines in
geometry and texture quality while achieving significantly faster generation
time.

Comments:
- 3DV 2025, Project page: https://hao-yu-wu.github.io/gen3d/

---

## DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-16 | Mangyu Kong, Jaewon Lee, Seongwon Lee, Euntai Kim | cs.RO | [PDF](http://arxiv.org/pdf/2411.10722v1){: .btn .btn-green } |

**Abstract**: We introduce Dynamic Gaussian Splatting SLAM (DGS-SLAM), the first dynamic
SLAM framework built on the foundation of Gaussian Splatting. While recent
advancements in dense SLAM have leveraged Gaussian Splatting to enhance scene
representation, most approaches assume a static environment, making them
vulnerable to photometric and geometric inconsistencies caused by dynamic
objects. To address these challenges, we integrate Gaussian Splatting SLAM with
a robust filtering process to handle dynamic objects throughout the entire
pipeline, including Gaussian insertion and keyframe selection. Within this
framework, to further improve the accuracy of dynamic object removal, we
introduce a robust mask generation method that enforces photometric consistency
across keyframes, reducing noise from inaccurate segmentation and artifacts
such as shadows. Additionally, we propose the loop-aware window selection
mechanism, which utilizes unique keyframe IDs of 3D Gaussians to detect loops
between the current and past frames, facilitating joint optimization of the
current camera poses and the Gaussian map. DGS-SLAM achieves state-of-the-art
performance in both camera tracking and novel view synthesis on various dynamic
SLAM benchmarks, proving its effectiveness in handling real-world dynamic
scenes.

Comments:
- Preprint, Under review

---

## GSEditPro: 3D Gaussian Splatting Editing with Attention-based  Progressive Localization

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Yanhao Sun, RunZe Tian, Xiao Han, XinYao Liu, Yan Zhang, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.10033v1){: .btn .btn-green } |

**Abstract**: With the emergence of large-scale Text-to-Image(T2I) models and implicit 3D
representations like Neural Radiance Fields (NeRF), many text-driven generative
editing methods based on NeRF have appeared. However, the implicit encoding of
geometric and textural information poses challenges in accurately locating and
controlling objects during editing. Recently, significant advancements have
been made in the editing methods of 3D Gaussian Splatting, a real-time
rendering technology that relies on explicit representation. However, these
methods still suffer from issues including inaccurate localization and limited
manipulation over editing. To tackle these challenges, we propose GSEditPro, a
novel 3D scene editing framework which allows users to perform various creative
and precise editing using text prompts only. Leveraging the explicit nature of
the 3D Gaussian distribution, we introduce an attention-based progressive
localization module to add semantic labels to each Gaussian during rendering.
This enables precise localization on editing areas by classifying Gaussians
based on their relevance to the editing prompts derived from cross-attention
layers of the T2I model. Furthermore, we present an innovative editing
optimization method based on 3D Gaussian Splatting, obtaining stable and
refined editing results through the guidance of Score Distillation Sampling and
pseudo ground truth. We prove the efficacy of our method through extensive
experiments.

Comments:
- Pacific Graphics 2024

---

## GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars  from Monocular Video

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Jingxuan Chen | cs.CV | [PDF](http://arxiv.org/pdf/2411.09952v1){: .btn .btn-green } |

**Abstract**: Avatar modelling has broad applications in human animation and virtual
try-ons. Recent advancements in this field have focused on high-quality and
comprehensive human reconstruction but often overlook the separation of
clothing from the body. To bridge this gap, this paper introduces GGAvatar
(Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular
videos. Through advanced parameterized templates and unique phased training,
this model effectively achieves decoupled, editable, and realistic
reconstruction of clothed humans. Comparative evaluations with other costly
models confirm GGAvatar's superior quality and efficiency in modelling both
clothed humans and separable garments. The paper also showcases applications in
clothing editing, as illustrated in Figure 1, highlighting the model's benefits
and the advantages of effective disentanglement. The code is available at
https://github.com/J-X-Chen/GGAvatar/.

Comments:
- MMAsia'24 Accepted

---

## USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction  and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Kang Chen, Jiyuan Zhang, Zecheng Hao, Yajing Zheng, Tiejun Huang, Zhaofei Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.10504v1){: .btn .btn-green } |

**Abstract**: Spike cameras, as an innovative neuromorphic camera that captures scenes with
the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D
reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting
(3DGS). Previous spike-based 3D reconstruction approaches often employ a
casecased pipeline: starting with high-quality image reconstruction from spike
streams based on established spike-to-image reconstruction algorithms, then
progressing to camera pose estimation and 3D reconstruction. However, this
cascaded approach suffers from substantial cumulative errors, where quality
limitations of initial image reconstructions negatively impact pose estimation,
ultimately degrading the fidelity of the 3D reconstruction. To address these
issues, we propose a synergistic optimization framework, \textbf{USP-Gaussian},
that unifies spike-based image reconstruction, pose correction, and Gaussian
splatting into an end-to-end framework. Leveraging the multi-view consistency
afforded by 3DGS and the motion capture capability of the spike camera, our
framework enables a joint iterative optimization that seamlessly integrates
information between the spike-to-image network and 3DGS. Experiments on
synthetic datasets with accurate poses demonstrate that our method surpasses
previous approaches by effectively eliminating cascading errors. Moreover, we
integrate pose optimization to achieve robust 3D reconstruction in real-world
scenarios with inaccurate initial poses, outperforming alternative methods by
effectively reducing noise and preserving fine texture details. Our code, data
and trained models will be available at
\url{https://github.com/chenkang455/USP-Gaussian}.



---

## The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual  Localisation, Reconstruction and Radiance Field Methods

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Yifu Tao, Miguel Ángel Muñoz-Bañón, Lintong Zhang, Jiahao Wang, Lanke Frank Tarimo Fu, Maurice Fallon | cs.CV | [PDF](http://arxiv.org/pdf/2411.10546v1){: .btn .btn-green } |

**Abstract**: This paper introduces a large-scale multi-modal dataset captured in and
around well-known landmarks in Oxford using a custom-built multi-sensor
perception unit as well as a millimetre-accurate map from a Terrestrial LiDAR
Scanner (TLS). The perception unit includes three synchronised global shutter
colour cameras, an automotive 3D LiDAR scanner, and an inertial sensor - all
precisely calibrated. We also establish benchmarks for tasks involving
localisation, reconstruction, and novel-view synthesis, which enable the
evaluation of Simultaneous Localisation and Mapping (SLAM) methods,
Structure-from-Motion (SfM) and Multi-view Stereo (MVS) methods as well as
radiance field methods such as Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting. To evaluate 3D reconstruction the TLS 3D models are used as ground
truth. Localisation ground truth is computed by registering the mobile LiDAR
scans to the TLS 3D models. Radiance field methods are evaluated not only with
poses sampled from the input trajectory, but also from viewpoints that are from
trajectories which are distant from the training poses. Our evaluation
demonstrates a key limitation of state-of-the-art radiance field methods: we
show that they tend to overfit to the training poses/images and do not
generalise well to out-of-sequence poses. They also underperform in 3D
reconstruction compared to MVS systems using the same visual inputs. Our
dataset and benchmarks are intended to facilitate better integration of
radiance field methods and SLAM systems. The raw and processed data, along with
software for parsing and evaluation, can be accessed at
https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/.

Comments:
- Website: https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/

---

## SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D  Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Yutao Tang, Yuxiang Guo, Deming Li, Cheng Peng | cs.CV | [PDF](http://arxiv.org/pdf/2411.12592v1){: .btn .btn-green } |

**Abstract**: Recent efforts in Gaussian-Splat-based Novel View Synthesis can achieve
photorealistic rendering; however, such capability is limited in sparse-view
scenarios due to sparse initialization and over-fitting floaters. Recent
progress in depth estimation and alignment can provide dense point cloud with
few views; however, the resulting pose accuracy is suboptimal. In this work, we
present SPARS3R, which combines the advantages of accurate pose estimation from
Structure-from-Motion and dense point cloud from depth estimation. To this end,
SPARS3R first performs a Global Fusion Alignment process that maps a prior
dense point cloud to a sparse point cloud from Structure-from-Motion based on
triangulated correspondences. RANSAC is applied during this process to
distinguish inliers and outliers. SPARS3R then performs a second, Semantic
Outlier Alignment step, which extracts semantically coherent regions around the
outliers and performs local alignment in these regions. Along with several
improvements in the evaluation process, we demonstrate that SPARS3R can achieve
photorealistic rendering with sparse images and significantly outperforms
existing approaches.



---

## Efficient Density Control for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Xiaobin Deng, Changyu Diao, Min Li, Ruohan Yu, Duanqing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.10133v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) excels in novel view synthesis, balancing
advanced rendering quality with real-time performance. However, in trained
scenes, a large number of Gaussians with low opacity significantly increase
rendering costs. This issue arises due to flaws in the split and clone
operations during the densification process, which lead to extensive Gaussian
overlap and subsequent opacity reduction. To enhance the efficiency of Gaussian
utilization, we improve the adaptive density control of 3DGS. First, we
introduce a more efficient long-axis split operation to replace the original
clone and split, which mitigates Gaussian overlap and improves densification
efficiency.Second, we propose a simple adaptive pruning technique to reduce the
number of low-opacity Gaussians. Finally, by dynamically lowering the splitting
threshold and applying importance weighting, the efficiency of Gaussian
utilization is further improved.We evaluate our proposed method on various
challenging real-world datasets. Experimental results show that our Efficient
Density Control (EDC) can enhance both the rendering speed and quality.



---

## DyGASR: Dynamic Generalized Exponential Splatting with Surface Alignment  for Accelerated 3D Mesh Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-14 | Shengchao Zhao, Yundong Li | cs.CV | [PDF](http://arxiv.org/pdf/2411.09156v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS), which lead to
high-quality novel view synthesis and accelerated rendering, have remarkably
improved the quality of radiance field reconstruction. However, the extraction
of mesh from a massive number of minute 3D Gaussian points remains great
challenge due to the large volume of Gaussians and difficulty of representation
of sharp signals caused by their inherent low-pass characteristics. To address
this issue, we propose DyGASR, which utilizes generalized exponential function
instead of traditional 3D Gaussian to decrease the number of particles and
dynamically optimize the representation of the captured signal. In addition, it
is observed that reconstructing mesh with Generalized Exponential
Splatting(GES) without modifications frequently leads to failures since the
generalized exponential distribution centroids may not precisely align with the
scene surface. To overcome this, we adopt Sugar's approach and introduce
Generalized Surface Regularization (GSR), which reduces the smallest scaling
vector of each point cloud to zero and ensures normal alignment perpendicular
to the surface, facilitating subsequent Poisson surface mesh reconstruction.
Additionally, we propose a dynamic resolution adjustment strategy that utilizes
a cosine schedule to gradually increase image resolution from low to high
during the training stage, thus avoiding constant full resolution, which
significantly boosts the reconstruction speed. Our approach surpasses existing
3DGS-based mesh reconstruction methods, as evidenced by extensive evaluations
on various scene datasets, demonstrating a 25\% increase in speed, and a 30\%
reduction in memory usage.



---

## Adversarial Attacks Using Differentiable Rendering: A Survey

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-14 | Matthew Hull, Chao Zhang, Zsolt Kira, Duen Horng Chau | cs.LG | [PDF](http://arxiv.org/pdf/2411.09749v1){: .btn .btn-green } |

**Abstract**: Differentiable rendering methods have emerged as a promising means for
generating photo-realistic and physically plausible adversarial attacks by
manipulating 3D objects and scenes that can deceive deep neural networks
(DNNs). Recently, differentiable rendering capabilities have evolved
significantly into a diverse landscape of libraries, such as Mitsuba,
PyTorch3D, and methods like Neural Radiance Fields and 3D Gaussian Splatting
for solving inverse rendering problems that share conceptually similar
properties commonly used to attack DNNs, such as back-propagation and
optimization. However, the adversarial machine learning research community has
not yet fully explored or understood such capabilities for generating attacks.
Some key reasons are that researchers often have different attack goals, such
as misclassification or misdetection, and use different tasks to accomplish
these goals by manipulating different representation in a scene, such as the
mesh or texture of an object. This survey adopts a task-oriented unifying
framework that systematically summarizes common tasks, such as manipulating
textures, altering illumination, and modifying 3D meshes to exploit
vulnerabilities in DNNs. Our framework enables easy comparison of existing
works, reveals research gaps and spotlights exciting future research directions
in this rapidly evolving field. Through focusing on how these tasks enable
attacks on various DNNs such as image classification, facial recognition,
object detection, optical flow and depth estimation, our survey helps
researchers and practitioners better understand the vulnerabilities of computer
vision systems against photorealistic adversarial attacks that could threaten
real-world applications.



---

## Towards More Accurate Fake Detection on Images Generated from Advanced  Generative and Neural Rendering Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Chengdong Dong, Vijayakumar Bhagavatula, Zhenyu Zhou, Ajay Kumar | cs.CV | [PDF](http://arxiv.org/pdf/2411.08642v1){: .btn .btn-green } |

**Abstract**: The remarkable progress in neural-network-driven visual data generation,
especially with neural rendering techniques like Neural Radiance Fields and 3D
Gaussian splatting, offers a powerful alternative to GANs and diffusion models.
These methods can produce high-fidelity images and lifelike avatars,
highlighting the need for robust detection methods. In response, an
unsupervised training technique is proposed that enables the model to extract
comprehensive features from the Fourier spectrum magnitude, thereby overcoming
the challenges of reconstructing the spectrum due to its centrosymmetric
properties. By leveraging the spectral domain and dynamically combining it with
spatial domain information, we create a robust multimodal detector that
demonstrates superior generalization capabilities in identifying challenging
synthetic images generated by the latest image synthesis techniques. To address
the absence of a 3D neural rendering-based fake image database, we develop a
comprehensive database that includes images generated by diverse neural
rendering techniques, providing a robust foundation for evaluating and
advancing detection methods.

Comments:
- 13 pages, 8 Figures

---

## MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields  Representation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Peng Wang, Lingzhe Zhao, Yin Zhang, Shiyu Zhao, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2411.08279v1){: .btn .btn-green } |

**Abstract**: Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and
3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in
Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering,
particularly when using high-quality video sequences as input. However,
existing methods struggle with motion-blurred frames, which are common in
real-world scenarios like low-light or long-exposure conditions. This often
results in a significant reduction in both camera localization accuracy and map
reconstruction quality. To address this challenge, we propose a dense visual
SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our
approach integrates an efficient motion blur-aware tracker with either neural
radiance fields or Gaussian Splatting based mapper. By accurately modeling the
physical image formation process of motion-blurred images, our method
simultaneously learns 3D scene representation and estimates the cameras' local
trajectory during exposure time, enabling proactive compensation for motion
blur caused by camera movement. In our experiments, we demonstrate that
MBA-SLAM surpasses previous state-of-the-art methods in both camera
localization and map reconstruction, showcasing superior performance across a
range of datasets, including synthetic and real datasets featuring sharp images
as well as those affected by motion blur, highlighting the versatility and
robustness of our approach. Code is available at
https://github.com/WU-CVGL/MBA-SLAM.



---

## 4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Mijeong Kim, Jongwoo Lim, Bohyung Han | cs.CV | [PDF](http://arxiv.org/pdf/2411.08879v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis of dynamic scenes is becoming important in various
applications, including augmented and virtual reality. We propose a novel 4D
Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded
monocular videos. To overcome the overfitting problem of existing work for
these real-world videos, we introduce an uncertainty-aware regularization that
identifies uncertain regions with few observations and selectively imposes
additional priors based on diffusion models and depth smoothness on such
regions. This approach improves both the performance of novel view synthesis
and the quality of training image reconstruction. We also identify the
initialization problem of 4DGS in fast-moving dynamic regions, where the
Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks.
To initialize Gaussian primitives in such regions, we present a dynamic region
densification method using the estimated depth maps and scene flow. Our
experiments show that the proposed method improves the performance of 4DGS
reconstruction from a video captured by a handheld monocular camera and also
exhibits promising results in few-shot static scene reconstruction.

Comments:
- NeurIPS 2024

---

## Biomass phenotyping of oilseed rape through UAV multi-view oblique  imaging with 3DGS and SAM model

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Yutao Shen, Hongyu Zhou, Xin Yang, Xuqi Lu, Ziyue Guo, Lixi Jiang, Yong He, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2411.08453v1){: .btn .btn-green } |

**Abstract**: Biomass estimation of oilseed rape is crucial for optimizing crop
productivity and breeding strategies. While UAV-based imaging has advanced
high-throughput phenotyping, current methods often rely on orthophoto images,
which struggle with overlapping leaves and incomplete structural information in
complex field environments. This study integrates 3D Gaussian Splatting (3DGS)
with the Segment Anything Model (SAM) for precise 3D reconstruction and biomass
estimation of oilseed rape. UAV multi-view oblique images from 36 angles were
used to perform 3D reconstruction, with the SAM module enhancing point cloud
segmentation. The segmented point clouds were then converted into point cloud
volumes, which were fitted to ground-measured biomass using linear regression.
The results showed that 3DGS (7k and 30k iterations) provided high accuracy,
with peak signal-to-noise ratios (PSNR) of 27.43 and 29.53 and training times
of 7 and 49 minutes, respectively. This performance exceeded that of structure
from motion (SfM) and mipmap Neural Radiance Fields (Mip-NeRF), demonstrating
superior efficiency. The SAM module achieved high segmentation accuracy, with a
mean intersection over union (mIoU) of 0.961 and an F1-score of 0.980.
Additionally, a comparison of biomass extraction models found the point cloud
volume model to be the most accurate, with an determination coefficient (R2) of
0.976, root mean square error (RMSE) of 2.92 g/plant, and mean absolute
percentage error (MAPE) of 6.81%, outperforming both the plot crop volume and
individual crop volume models. This study highlights the potential of combining
3DGS with multi-view UAV imaging for improved biomass phenotyping.



---

## DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose  Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Yueming Xu, Haochen Jiang, Zhongyang Xiao, Jianfeng Feng, Li Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2411.08373v1){: .btn .btn-green } |

**Abstract**: Achieving robust and precise pose estimation in dynamic scenes is a
significant research challenge in Visual Simultaneous Localization and Mapping
(SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems
have proven effective in creating high-quality renderings using explicit 3D
Gaussian models, significantly improving environmental reconstruction fidelity.
However, these approaches depend on a static environment assumption and face
challenges in dynamic environments due to inconsistent observations of geometry
and photometry. To address this problem, we propose DG-SLAM, the first robust
dynamic visual SLAM system grounded in 3D Gaussians, which provides precise
camera pose estimation alongside high-fidelity reconstructions. Specifically,
we propose effective strategies, including motion mask generation, adaptive
Gaussian point management, and a hybrid camera tracking algorithm to improve
the accuracy and robustness of pose estimation. Extensive experiments
demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose
estimation, map reconstruction, and novel-view synthesis in dynamic scenes,
outperforming existing methods meanwhile preserving real-time rendering
ability.



---

## BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel  View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2411.08508v1){: .btn .btn-green } |

**Abstract**: We present billboard Splatting (BBSplat) - a novel approach for 3D scene
representation based on textured geometric primitives. BBSplat represents the
scene as a set of optimizable textured planar primitives with learnable RGB
textures and alpha-maps to control their shape. BBSplat primitives can be used
in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our
method's qualitative and quantitative improvements over 3D and 2D Gaussians are
most noticeable when fewer primitives are used, when BBSplat achieves over 1200
FPS. Our novel regularization term encourages textures to have a sparser
structure, unlocking an efficient compression that leads to a reduction in
storage space of the model. Our experiments show the efficiency of BBSplat on
standard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,
and Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics
compared to the state-of-the-art, especially for the case when fewer primitives
are used, which, on the other hand, leads to up to 2 times inference speed
improvement for the same rendering quality.



---

## Projecting Gaussian Ellipsoids While Avoiding Affine Projection  Approximation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Han Qi, Tao Cai, Xiyue Han | cs.CV | [PDF](http://arxiv.org/pdf/2411.07579v3){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting has dominated novel-view synthesis with its
real-time rendering speed and state-of-the-art rendering quality. However,
during the rendering process, the use of the Jacobian of the affine
approximation of the projection transformation leads to inevitable errors,
resulting in blurriness, artifacts and a lack of scene consistency in the final
rendered images. To address this issue, we introduce an ellipsoid-based
projection method to calculate the projection of Gaussian ellipsoid onto the
image plane, which is the primitive of 3D Gaussian Splatting. As our proposed
ellipsoid-based projection method cannot handle Gaussian ellipsoids with camera
origins inside them or parts lying below $z=0$ plane in the camera space, we
designed a pre-filtering strategy. Experiments over multiple widely adopted
benchmark datasets show that our ellipsoid-based projection method can enhance
the rendering quality of 3D Gaussian Splatting and its extensions.



---

## Material Transforms from Disentangled NeRF Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Ivan Lopes, Jean-François Lalonde, Raoul de Charette | cs.CV | [PDF](http://arxiv.org/pdf/2411.08037v1){: .btn .btn-green } |

**Abstract**: In this paper, we first propose a novel method for transferring material
transformations across different scenes. Building on disentangled Neural
Radiance Field (NeRF) representations, our approach learns to map Bidirectional
Reflectance Distribution Functions (BRDF) from pairs of scenes observed in
varying conditions, such as dry and wet. The learned transformations can then
be applied to unseen scenes with similar materials, therefore effectively
rendering the transformation learned with an arbitrary level of intensity.
Extensive experiments on synthetic scenes and real-world objects validate the
effectiveness of our approach, showing that it can learn various
transformations such as wetness, painting, coating, etc. Our results highlight
not only the versatility of our method but also its potential for practical
applications in computer graphics. We publish our method implementation, along
with our synthetic/real datasets on
https://github.com/astra-vision/BRDFTransform



---

## TomoGRAF: A Robust and Generalizable Reconstruction Network for  Single-View Computed Tomography

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Di Xu, Yang Yang, Hengjie Liu, Qihui Lyu, Martina Descovich, Dan Ruan, Ke Sheng | eess.IV | [PDF](http://arxiv.org/pdf/2411.08158v1){: .btn .btn-green } |

**Abstract**: Computed tomography (CT) provides high spatial resolution visualization of 3D
structures for scientific and clinical applications. Traditional
analytical/iterative CT reconstruction algorithms require hundreds of angular
data samplings, a condition that may not be met in practice due to physical and
mechanical limitations. Sparse view CT reconstruction has been proposed using
constrained optimization and machine learning methods with varying success,
less so for ultra-sparse view CT reconstruction with one to two views. Neural
radiance field (NeRF) is a powerful tool for reconstructing and rendering 3D
natural scenes from sparse views, but its direct application to 3D medical
image reconstruction has been minimally successful due to the differences
between optical and X-ray photon transportation. Here, we develop a novel
TomoGRAF framework incorporating the unique X-ray transportation physics to
reconstruct high-quality 3D volumes using ultra-sparse projections without
prior. TomoGRAF captures the CT imaging geometry, simulates the X-ray casting
and tracing process, and penalizes the difference between simulated and ground
truth CT sub-volume during training. We evaluated the performance of TomoGRAF
on an unseen dataset of distinct imaging characteristics from the training data
and demonstrated a vast leap in performance compared with state-of-the-art deep
learning and NeRF methods. TomoGRAF provides the first generalizable solution
for image-guided radiotherapy and interventional radiology applications, where
only one or a few X-ray views are available, but 3D volumetric information is
desired.



---

## GUS-IR: Gaussian Splatting with Unified Shading for Inverse Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Zhihao Liang, Hongdong Li, Kui Jia, Kailing Guo, Qi Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.07478v1){: .btn .btn-green } |

**Abstract**: Recovering the intrinsic physical attributes of a scene from images,
generally termed as the inverse rendering problem, has been a central and
challenging task in computer vision and computer graphics. In this paper, we
present GUS-IR, a novel framework designed to address the inverse rendering
problem for complicated scenes featuring rough and glossy surfaces. This paper
starts by analyzing and comparing two prominent shading techniques popularly
used for inverse rendering, forward shading and deferred shading, effectiveness
in handling complex materials. More importantly, we propose a unified shading
solution that combines the advantages of both techniques for better
decomposition. In addition, we analyze the normal modeling in 3D Gaussian
Splatting (3DGS) and utilize the shortest axis as normal for each particle in
GUS-IR, along with a depth-related regularization, resulting in improved
geometric representation and better shape reconstruction. Furthermore, we
enhance the probe-based baking scheme proposed by GS-IR to achieve more
accurate ambient occlusion modeling to better handle indirect illumination.
Extensive experiments have demonstrated the superior performance of GUS-IR in
achieving precise intrinsic decomposition and geometric representation,
supporting many downstream tasks (such as relighting, retouching) in computer
vision, graphics, and extended reality.

Comments:
- 15 pages, 11 figures

---

## HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Qiankun Gao, Jiarui Meng, Chengxiang Wen, Jie Chen, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.07541v1){: .btn .btn-green } |

**Abstract**: The online reconstruction of dynamic scenes from multi-view streaming videos
faces significant challenges in training, rendering and storage efficiency.
Harnessing superior learning speed and real-time rendering capabilities, 3D
Gaussian Splatting (3DGS) has recently demonstrated considerable potential in
this field. However, 3DGS can be inefficient in terms of storage and prone to
overfitting by excessively growing Gaussians, particularly with limited views.
This paper proposes an efficient framework, dubbed HiCoM, with three key
components. First, we construct a compact and robust initial 3DGS
representation using a perturbation smoothing strategy. Next, we introduce a
Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform
distribution and local consistency of 3D Gaussians to swiftly and accurately
learn motions across frames. Finally, we continually refine the 3DGS with
additional Gaussians, which are later merged into the initial 3DGS to maintain
consistency with the evolving scene. To preserve a compact representation, an
equivalent number of low-opacity Gaussians that minimally impact the
representation are removed before processing subsequent frames. Extensive
experiments conducted on two widely used datasets show that our framework
improves learning efficiency of the state-of-the-art methods by about $20\%$
and reduces the data storage by $85\%$, achieving competitive free-viewpoint
video synthesis quality but with higher robustness and stability. Moreover, by
parallel learning multiple frames simultaneously, our HiCoM decreases the
average training wall time to $<2$ seconds per frame with negligible
performance degradation, substantially boosting real-world applicability and
responsiveness.

Comments:
- Accepted to NeurIPS 2024; Code is avaliable at
  https://github.com/gqk/HiCoM

---

## GaussianCut: Interactive segmentation via graph cut for 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Umangi Jain, Ashkan Mirzaei, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2411.07555v1){: .btn .btn-green } |

**Abstract**: We introduce GaussianCut, a new method for interactive multiview segmentation
of scenes represented as 3D Gaussians. Our approach allows for selecting the
objects to be segmented by interacting with a single view. It accepts intuitive
user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian
Splatting (3DGS) as the underlying scene representation simplifies the
extraction of objects of interest which are considered to be a subset of the
scene's Gaussians. Our key idea is to represent the scene as a graph and use
the graph-cut algorithm to minimize an energy function to effectively partition
the Gaussians into foreground and background. To achieve this, we construct a
graph based on scene Gaussians and devise a segmentation-aligned energy
function on the graph to combine user inputs with scene properties. To obtain
an initial coarse segmentation, we leverage 2D image/video segmentation models
and further refine these coarse estimates using our graph construction. Our
empirical evaluations show the adaptability of GaussianCut across a diverse set
of scenes. GaussianCut achieves competitive performance with state-of-the-art
approaches for 3D segmentation without requiring any additional
segmentation-aware training.



---

## LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-11 | Zefan Qu, Ke Xu, Gerhard Petrus Hancke, Rynson W. H. Lau | cs.CV | [PDF](http://arxiv.org/pdf/2411.06757v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have shown remarkable performances in
producing novel-view images from high-quality scene images. However, hand-held
low-light photography challenges NeRFs as the captured images may
simultaneously suffer from low visibility, noise, and camera shakes. While
existing NeRF methods may handle either low light or motion, directly combining
them or incorporating additional image-based enhancement methods does not work
as these degradation factors are highly coupled. We observe that noise in
low-light images is always sharp regardless of camera shakes, which implies an
implicit order of these degradation factors within the image formation process.
To this end, we propose in this paper a novel model, named LuSh-NeRF, which can
reconstruct a clean and sharp NeRF from a group of hand-held low-light images.
The key idea of LuSh-NeRF is to sequentially model noise and blur in the images
via multi-view feature consistency and frequency information of NeRF,
respectively. Specifically, LuSh-NeRF includes a novel Scene-Noise
Decomposition (SND) module for decoupling the noise from the scene
representation and a novel Camera Trajectory Prediction (CTP) module for the
estimation of camera motions based on low-frequency scene information. To
facilitate training and evaluations, we construct a new dataset containing both
synthetic and real images. Experiments show that LuSh-NeRF outperforms existing
approaches. Our code and dataset can be found here:
https://github.com/quzefan/LuSh-NeRF.

Comments:
- Accepted by NeurIPS 2024

---

## A Hierarchical Compression Technique for 3D Gaussian Splatting  Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-11 | He Huang, Wenjie Huang, Qi Yang, Yiling Xu, Zhu li | cs.CV | [PDF](http://arxiv.org/pdf/2411.06976v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (GS) demonstrates excellent rendering quality and
generation speed in novel view synthesis. However, substantial data size poses
challenges for storage and transmission, making 3D GS compression an essential
technology. Current 3D GS compression research primarily focuses on developing
more compact scene representations, such as converting explicit 3D GS data into
implicit forms. In contrast, compression of the GS data itself has hardly been
explored. To address this gap, we propose a Hierarchical GS Compression (HGSC)
technique. Initially, we prune unimportant Gaussians based on importance scores
derived from both global and local significance, effectively reducing
redundancy while maintaining visual quality. An Octree structure is used to
compress 3D positions. Based on the 3D GS Octree, we implement a hierarchical
attribute compression strategy by employing a KD-tree to partition the 3D GS
into multiple blocks. We apply farthest point sampling to select anchor
primitives within each block and others as non-anchor primitives with varying
Levels of Details (LoDs). Anchor primitives serve as reference points for
predicting non-anchor primitives across different LoDs to reduce spatial
redundancy. For anchor primitives, we use the region adaptive hierarchical
transform to achieve near-lossless compression of various attributes. For
non-anchor primitives, each is predicted based on the k-nearest anchor
primitives. To further minimize prediction errors, the reconstructed LoD and
anchor primitives are combined to form new anchor primitives to predict the
next LoD. Our method notably achieves superior compression quality and a
significant data size reduction of over 4.5 times compared to the
state-of-the-art compression method on small scenes datasets.



---

## Adaptive and Temporally Consistent Gaussian Surfels for Multi-view  Dynamic Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-10 | Decai Chen, Brianne Oberson, Ingo Feldmann, Oliver Schreer, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2411.06602v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently achieved notable success in novel view
synthesis for dynamic scenes and geometry reconstruction in static scenes.
Building on these advancements, early methods have been developed for dynamic
surface reconstruction by globally optimizing entire sequences. However,
reconstructing dynamic scenes with significant topology changes, emerging or
disappearing objects, and rapid movements remains a substantial challenge,
particularly for long sequences. To address these issues, we propose AT-GS, a
novel method for reconstructing high-quality dynamic surfaces from multi-view
videos through per-frame incremental optimization. To avoid local minima across
frames, we introduce a unified and adaptive gradient-aware densification
strategy that integrates the strengths of conventional cloning and splitting
techniques. Additionally, we reduce temporal jittering in dynamic surfaces by
ensuring consistency in curvature maps across consecutive frames. Our method
achieves superior accuracy and temporal coherence in dynamic surface
reconstruction, delivering high-fidelity space-time novel view synthesis, even
in complex and challenging scenes. Extensive experiments on diverse multi-view
video datasets demonstrate the effectiveness of our approach, showing clear
advantages over baseline methods. Project page:
\url{https://fraunhoferhhi.github.io/AT-GS}



---

## Through the Curved Cover: Synthesizing Cover Aberrated Scenes with  Refractive Field

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-10 | Liuyue Xie, Jiancong Guo, Laszlo A. Jeni, Zhiheng Jia, Mingyang Li, Yunwen Zhou, Chao Guo | cs.CV | [PDF](http://arxiv.org/pdf/2411.06365v1){: .btn .btn-green } |

**Abstract**: Recent extended reality headsets and field robots have adopted covers to
protect the front-facing cameras from environmental hazards and falls. The
surface irregularities on the cover can lead to optical aberrations like
blurring and non-parametric distortions. Novel view synthesis methods like NeRF
and 3D Gaussian Splatting are ill-equipped to synthesize from sequences with
optical aberrations. To address this challenge, we introduce SynthCover to
enable novel view synthesis through protective covers for downstream extended
reality applications. SynthCover employs a Refractive Field that estimates the
cover's geometry, enabling precise analytical calculation of refracted rays.
Experiments on synthetic and real-world scenes demonstrate our method's ability
to accurately model scenes viewed through protective covers, achieving a
significant improvement in rendering quality compared to prior methods. We also
show that the model can adjust well to various cover geometries with synthetic
sequences captured with covers of different surface curvatures. To motivate
further studies on this problem, we provide the benchmarked dataset containing
real and synthetic walkable scenes captured with protective cover optical
aberrations.

Comments:
- WACV 2025

---

## SplatFormer: Point Transformer for Robust 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-10 | Yutong Chen, Marko Mihajlovic, Xiyi Chen, Yiming Wang, Sergey Prokudin, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2411.06390v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently transformed photorealistic
reconstruction, achieving high visual fidelity and real-time performance.
However, rendering quality significantly deteriorates when test views deviate
from the camera angles used during training, posing a major challenge for
applications in immersive free-viewpoint rendering and navigation. In this
work, we conduct a comprehensive evaluation of 3DGS and related novel view
synthesis methods under out-of-distribution (OOD) test camera scenarios. By
creating diverse test cases with synthetic and real-world datasets, we
demonstrate that most existing methods, including those incorporating various
regularization techniques and data-driven priors, struggle to generalize
effectively to OOD views. To address this limitation, we introduce SplatFormer,
the first point transformer model specifically designed to operate on Gaussian
splats. SplatFormer takes as input an initial 3DGS set optimized under limited
training views and refines it in a single forward pass, effectively removing
potential artifacts in OOD test views. To our knowledge, this is the first
successful application of point transformers directly on 3DGS sets, surpassing
the limitations of previous multi-scene training methods, which could handle
only a restricted number of input views during inference. Our model
significantly improves rendering quality under extreme novel views, achieving
state-of-the-art performance in these challenging scenarios and outperforming
various 3DGS regularization techniques, multi-scene models tailored for sparse
view synthesis, and diffusion-based frameworks.

Comments:
- Code and dataset: https://github.com/ChenYutongTHU/SplatFormer
  Project page: https://sergeyprokudin.github.io/splatformer/

---

## GaussianSpa: An "Optimizing-Sparsifying" Simplification Framework for  Compact and High-Quality 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-09 | Yangming Zhang, Wenqi Jia, Wei Niu, Miao Yin | cs.CV | [PDF](http://arxiv.org/pdf/2411.06019v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view
synthesis, leveraging continuous aggregations of Gaussian functions to model
scene geometry. However, 3DGS suffers from substantial memory requirements to
store the multitude of Gaussians, hindering its practicality. To address this
challenge, we introduce GaussianSpa, an optimization-based simplification
framework for compact and high-quality 3DGS. Specifically, we formulate the
simplification as an optimization problem associated with the 3DGS training.
Correspondingly, we propose an efficient "optimizing-sparsifying" solution that
alternately solves two independent sub-problems, gradually imposing strong
sparsity onto the Gaussians in the training process. Our comprehensive
evaluations on various datasets show the superiority of GaussianSpa over
existing state-of-the-art approaches. Notably, GaussianSpa achieves an average
PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with
10$\times$ fewer Gaussians compared to the vanilla 3DGS. Our project page is
available at https://gaussianspa.github.io/.

Comments:
- Project page at https://gaussianspa.github.io/

---

## AI-Driven Stylization of 3D Environments

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-09 | Yuanbo Chen, Yixiao Kang, Yukun Song, Cyrus Vachha, Sining Huang | cs.CV | [PDF](http://arxiv.org/pdf/2411.06067v1){: .btn .btn-green } |

**Abstract**: In this system, we discuss methods to stylize a scene of 3D primitive objects
into a higher fidelity 3D scene using novel 3D representations like NeRFs and
3D Gaussian Splatting. Our approach leverages existing image stylization
systems and image-to-3D generative models to create a pipeline that iteratively
stylizes and composites 3D objects into scenes. We show our results on adding
generated objects into a scene and discuss limitations.



---

## Rate-aware Compression for NeRF-based Volumetric Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-08 | Zhiyu Zhang, Guo Lu, Huanxiong Liang, Zhengxue Cheng, Anni Tang, Li Song | cs.MM | [PDF](http://arxiv.org/pdf/2411.05322v1){: .btn .btn-green } |

**Abstract**: The neural radiance fields (NeRF) have advanced the development of 3D
volumetric video technology, but the large data volumes they involve pose
significant challenges for storage and transmission. To address these problems,
the existing solutions typically compress these NeRF representations after the
training stage, leading to a separation between representation training and
compression. In this paper, we try to directly learn a compact NeRF
representation for volumetric video in the training stage based on the proposed
rate-aware compression framework. Specifically, for volumetric video, we use a
simple yet effective modeling strategy to reduce temporal redundancy for the
NeRF representation. Then, during the training phase, an implicit entropy model
is utilized to estimate the bitrate of the NeRF representation. This entropy
model is then encoded into the bitstream to assist in the decoding of the NeRF
representation. This approach enables precise bitrate estimation, thereby
leading to a compact NeRF representation. Furthermore, we propose an adaptive
quantization strategy and learn the optimal quantization step for the NeRF
representations. Finally, the NeRF representation can be optimized by using the
rate-distortion trade-off. Our proposed compression framework can be used for
different representations and experimental results demonstrate that our
approach significantly reduces the storage size with marginal distortion and
achieves state-of-the-art rate-distortion performance for volumetric video on
the HumanRF and ReRF datasets. Compared to the previous state-of-the-art method
TeTriRF, we achieved an approximately -80% BD-rate on the HumanRF dataset and
-60% BD-rate on the ReRF dataset.

Comments:
- Accepted by ACM MM 2024 (Oral)

---

## A Nerf-Based Color Consistency Method for Remote Sensing Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-08 | Zongcheng Zuo, Yuanxiang Li, Tongtong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.05557v1){: .btn .btn-green } |

**Abstract**: Due to different seasons, illumination, and atmospheric conditions, the
photometric of the acquired image varies greatly, which leads to obvious
stitching seams at the edges of the mosaic image. Traditional methods can be
divided into two categories, one is absolute radiation correction and the other
is relative radiation normalization. We propose a NeRF-based method of color
consistency correction for multi-view images, which weaves image features
together using implicit expressions, and then re-illuminates feature space to
generate a fusion image with a new perspective. We chose Superview-1 satellite
images and UAV images with large range and time difference for the experiment.
Experimental results show that the synthesize image generated by our method has
excellent visual effect and smooth color transition at the edges.

Comments:
- 4 pages, 4 figures, The International Geoscience and Remote Sensing
  Symposium (IGARSS2023)

---

## MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2411.04924v1){: .btn .btn-green } |

**Abstract**: We introduce MVSplat360, a feed-forward approach for 360{\deg} novel view
synthesis (NVS) of diverse real-world scenes, using only sparse observations.
This setting is inherently ill-posed due to minimal overlap among input views
and insufficient visual information provided, making it challenging for
conventional methods to achieve high-quality results. Our MVSplat360 addresses
this by effectively combining geometry-aware 3D reconstruction with temporally
consistent video generation. Specifically, it refactors a feed-forward 3D
Gaussian Splatting (3DGS) model to render features directly into the latent
space of a pre-trained Stable Video Diffusion (SVD) model, where these features
then act as pose and visual cues to guide the denoising process and produce
photorealistic 3D-consistent views. Our model is end-to-end trainable and
supports rendering arbitrary views with as few as 5 sparse input views. To
evaluate MVSplat360's performance, we introduce a new benchmark using the
challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual
quality compared to state-of-the-art methods on wide-sweeping or even 360{\deg}
NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the
effectiveness of our model. The video results are available on our project
page: https://donydchen.github.io/mvsplat360.

Comments:
- NeurIPS 2024, Project page: https://donydchen.github.io/mvsplat360,
  Code: https://github.com/donydchen/mvsplat360

---

## ProEdit: Simple Progression is All You Need for High-Quality 3D Scene  Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Jun-Kun Chen, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.05006v1){: .btn .btn-green } |

**Abstract**: This paper proposes ProEdit - a simple yet effective framework for
high-quality 3D scene editing guided by diffusion distillation in a novel
progressive manner. Inspired by the crucial observation that multi-view
inconsistency in scene editing is rooted in the diffusion model's large
feasible output space (FOS), our framework controls the size of FOS and reduces
inconsistency by decomposing the overall editing task into several subtasks,
which are then executed progressively on the scene. Within this framework, we
design a difficulty-aware subtask decomposition scheduler and an adaptive 3D
Gaussian splatting (3DGS) training strategy, ensuring high quality and
efficiency in performing each subtask. Extensive evaluation shows that our
ProEdit achieves state-of-the-art results in various scenes and challenging
editing tasks, all through a simple framework without any expensive or
sophisticated add-ons like distillation losses, components, or training
procedures. Notably, ProEdit also provides a new way to control, preview, and
select the "aggressivity" of editing operation during the editing process.

Comments:
- NeurIPS 2024. Project Page: https://immortalco.github.io/ProEdit/

---

## Planar Reflection-Aware Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Chen Gao, Yipeng Wang, Changil Kim, Jia-Bin Huang, Johannes Kopf | cs.CV | [PDF](http://arxiv.org/pdf/2411.04984v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in
reconstructing complex scenes with high fidelity. However, NeRF's view
dependency can only handle low-frequency reflections. It falls short when
handling complex planar reflections, often interpreting them as erroneous scene
geometries and leading to duplicated and inaccurate scene representations. To
address this challenge, we introduce a reflection-aware NeRF that jointly
models planar reflectors, such as windows, and explicitly casts reflected rays
to capture the source of the high-frequency reflections. We query a single
radiance field to render the primary color and the source of the reflection. We
propose a sparse edge regularization to help utilize the true sources of
reflections for rendering planar reflections rather than creating a duplicate
along the primary ray at the same depth. As a result, we obtain accurate scene
geometry. Rendering along the primary ray results in a clean, reflection-free
view, while explicitly rendering along the reflected ray allows us to
reconstruct highly detailed reflections. Our extensive quantitative and
qualitative evaluations of real-world datasets demonstrate our method's
enhanced performance in accurately handling reflections.



---

## GANESH: Generalizable NeRF for Lensless Imaging

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Rakesh Raj Madavan, Akshat Kaimal, Badhrinarayanan K V, Vinayak Gupta, Rohit Choudhary, Chandrakala Shanmuganathan, Kaushik Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2411.04810v1){: .btn .btn-green } |

**Abstract**: Lensless imaging offers a significant opportunity to develop ultra-compact
cameras by removing the conventional bulky lens system. However, without a
focusing element, the sensor's output is no longer a direct image but a complex
multiplexed scene representation. Traditional methods have attempted to address
this challenge by employing learnable inversions and refinement models, but
these methods are primarily designed for 2D reconstruction and do not
generalize well to 3D reconstruction. We introduce GANESH, a novel framework
designed to enable simultaneous refinement and novel view synthesis from
multi-view lensless images. Unlike existing methods that require scene-specific
training, our approach supports on-the-fly inference without retraining on each
scene. Moreover, our framework allows us to tune our model to specific scenes,
enhancing the rendering and refinement quality. To facilitate research in this
area, we also present the first multi-view lensless dataset, LenslessScenes.
Extensive experiments demonstrate that our method outperforms current
approaches in reconstruction accuracy and refinement quality. Code and video
results are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/



---

## SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on Larger  Objects for Mobile-Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Xun Tu, Karthik Desingh | cs.RO | [PDF](http://arxiv.org/pdf/2411.04386v2){: .btn .btn-green } |

**Abstract**: Grasp planning and estimation have been a longstanding research problem in
robotics, with two main approaches to find graspable poses on the objects: 1)
geometric approach, which relies on 3D models of objects and the gripper to
estimate valid grasp poses, and 2) data-driven, learning-based approach, with
models trained to identify grasp poses from raw sensor observations. The latter
assumes comprehensive geometric coverage during the training phase. However,
the data-driven approach is typically biased toward tabletop scenarios and
struggle to generalize to out-of-distribution scenarios with larger objects
(e.g. chair). Additionally, raw sensor data (e.g. RGB-D data) from a single
view of these larger objects is often incomplete and necessitates additional
observations. In this paper, we take a geometric approach, leveraging
advancements in object modeling (e.g. NeRF) to build an implicit model by
taking RGB images from views around the target object. This model enables the
extraction of explicit mesh model while also capturing the visual appearance
from novel viewpoints that is useful for perception tasks like object detection
and pose estimation. We further decompose the NeRF-reconstructed 3D mesh into
superquadrics (SQs) -- parametric geometric primitives, each mapped to a set of
precomputed grasp poses, allowing grasp composition on the target object based
on these primitives. Our proposed pipeline overcomes the problems: a) noisy
depth and incomplete view of the object, with a modeling step, and b)
generalization to objects of any size. For more qualitative results, refer to
the supplementary video and webpage https://bit.ly/3ZrOanU

Comments:
- 8 pages, 7 figures, submitted to ICRA 2025 for review

---

## 3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical  Object Rearrangement

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Ziqi Lu, Jianbo Ye, John Leonard | cs.CV | [PDF](http://arxiv.org/pdf/2411.03706v1){: .btn .btn-green } |

**Abstract**: We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for
detecting physical object rearrangements in 3D scenes. Our approach estimates
3D object-level changes by comparing two sets of unaligned images taken at
different times. Leveraging 3DGS's novel view rendering and EfficientSAM's
zero-shot segmentation capabilities, we detect 2D object-level changes, which
are then associated and fused across views to estimate 3D changes. Our method
can detect changes in cluttered environments using sparse post-change images
within as little as 18s, using as few as a single new image. It does not rely
on depth input, user instructions, object classes, or object models -- An
object is recognized simply if it has been re-arranged. Our approach is
evaluated on both public and self-collected real-world datasets, achieving up
to 14% higher accuracy and three orders of magnitude faster performance
compared to the state-of-the-art radiance-field-based change detection method.
This significant performance boost enables a broad range of downstream
applications, where we highlight three key use cases: object reconstruction,
robot workspace reset, and 3DGS model update. Our code and data will be made
available at https://github.com/520xyxyzq/3DGS-CD.



---

## GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Jilan Mei, Junbo Li, Cai Meng | cs.CV | [PDF](http://arxiv.org/pdf/2411.03807v3){: .btn .btn-green } |

**Abstract**: This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.



---

## Structure Consistent Gaussian Splatting with Matching Prior for Few-shot  Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Rui Peng, Wangze Xu, Luyang Tang, Liwei Liao, Jianbo Jiao, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.03637v1){: .btn .btn-green } |

**Abstract**: Despite the substantial progress of novel view synthesis, existing methods,
either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian
Splatting (3DGS), suffer significant degradation when the input becomes sparse.
Numerous efforts have been introduced to alleviate this problem, but they still
struggle to synthesize satisfactory results efficiently, especially in the
large scene. In this paper, we propose SCGaussian, a Structure Consistent
Gaussian Splatting method using matching priors to learn 3D consistent scene
structure. Considering the high interdependence of Gaussian attributes, we
optimize the scene structure in two folds: rendering geometry and, more
importantly, the position of Gaussian primitives, which is hard to be directly
constrained in the vanilla 3DGS due to the non-structure property. To achieve
this, we present a hybrid Gaussian representation. Besides the ordinary
non-structure Gaussian primitives, our model also consists of ray-based
Gaussian primitives that are bound to matching rays and whose optimization of
their positions is restricted along the ray. Thus, we can utilize the matching
correspondence to directly enforce the position of these Gaussian primitives to
converge to the surface points where rays intersect. Extensive experiments on
forward-facing, surrounding, and complex large scenes show the effectiveness of
our approach with state-of-the-art performance and high efficiency. Code is
available at https://github.com/prstrive/SCGaussian.

Comments:
- NeurIPS 2024 Accepted

---

## Exploring Seasonal Variability in the Context of Neural Radiance Fields  for 3D Reconstruction on Satellite Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Liv Kåreborn, Erica Ingerstad, Amanda Berg, Justus Karlsson, Leif Haglund | cs.CV | [PDF](http://arxiv.org/pdf/2411.02972v1){: .btn .btn-green } |

**Abstract**: In this work, the seasonal predictive capabilities of Neural Radiance Fields
(NeRF) applied to satellite images are investigated. Focusing on the
utilization of satellite data, the study explores how Sat-NeRF, a novel
approach in computer vision, performs in predicting seasonal variations across
different months. Through comprehensive analysis and visualization, the study
examines the model's ability to capture and predict seasonal changes,
highlighting specific challenges and strengths. Results showcase the impact of
the sun direction on predictions, revealing nuanced details in seasonal
transitions, such as snow cover, color accuracy, and texture representation in
different landscapes. Given these results, we propose Planet-NeRF, an extension
to Sat-NeRF capable of incorporating seasonal variability through a set of
month embedding vectors. Comparative evaluations reveal that Planet-NeRF
outperforms prior models in the case where seasonal changes are present. The
extensive evaluation combined with the proposed method offers promising avenues
for future research in this domain.



---

## Object and Contact Point Tracking in Demonstrations Using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Michael Büttner, Jonathan Francis, Helge Rhodin, Andrew Melnik | cs.CV | [PDF](http://arxiv.org/pdf/2411.03555v1){: .btn .btn-green } |

**Abstract**: This paper introduces a method to enhance Interactive Imitation Learning
(IIL) by extracting touch interaction points and tracking object movement from
video demonstrations. The approach extends current IIL systems by providing
robots with detailed knowledge of both where and how to interact with objects,
particularly complex articulated ones like doors and drawers. By leveraging
cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for
tracking, this method allows robots to better understand and manipulate objects
in dynamic environments. The research lays the foundation for more effective
task learning and execution in autonomous robotic systems.

Comments:
- CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich,
  Germany

---

## HFGaussian: Learning Generalizable Gaussian Human with Integrated Human  Features

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Arnab Dey, Cheng-You Lu, Andrew I. Comport, Srinath Sridhar, Chin-Teng Lin, Jean Martinet | cs.CV | [PDF](http://arxiv.org/pdf/2411.03086v1){: .btn .btn-green } |

**Abstract**: Recent advancements in radiance field rendering show promising results in 3D
scene representation, where Gaussian splatting-based techniques emerge as
state-of-the-art due to their quality and efficiency. Gaussian splatting is
widely used for various applications, including 3D human representation.
However, previous 3D Gaussian splatting methods either use parametric body
models as additional information or fail to provide any underlying structure,
like human biomechanical features, which are essential for different
applications. In this paper, we present a novel approach called HFGaussian that
can estimate novel views and human features, such as the 3D skeleton, 3D key
points, and dense pose, from sparse input images in real time at 25 FPS. The
proposed method leverages generalizable Gaussian splatting technique to
represent the human subject and its associated features, enabling efficient and
generalizable reconstruction. By incorporating a pose regression network and
the feature splatting technique with Gaussian splatting, HFGaussian
demonstrates improved capabilities over existing 3D human methods, showcasing
the potential of 3D human representations with integrated biomechanics. We
thoroughly evaluate our HFGaussian method against the latest state-of-the-art
techniques in human Gaussian splatting and pose estimation, demonstrating its
real-time, state-of-the-art performance.



---

## Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Xavier Timoneda, Markus Herb, Fabian Duerr, Daniel Goehring, Fisher Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.02969v1){: .btn .btn-green } |

**Abstract**: LiDAR Semantic Segmentation is a fundamental task in autonomous driving
perception consisting of associating each LiDAR point to a semantic label.
Fully-supervised models have widely tackled this task, but they require labels
for each scan, which either limits their domain or requires impractical amounts
of expensive annotations. Camera images, which are generally recorded alongside
LiDAR pointclouds, can be processed by the widely available 2D foundation
models, which are generic and dataset-agnostic. However, distilling knowledge
from 2D data to improve LiDAR perception raises domain adaptation challenges.
For example, the classical perspective projection suffers from the parallax
effect produced by the position shift between both sensors at their respective
capture times. We propose a Semi-Supervised Learning setup to leverage
unlabeled LiDAR pointclouds alongside distilled knowledge from the camera
images. To self-supervise our model on the unlabeled scans, we add an auxiliary
NeRF head and cast rays from the camera viewpoint over the unlabeled voxel
features. The NeRF head predicts densities and semantic logits at each sampled
ray location which are used for rendering pixel semantics. Concurrently, we
query the Segment-Anything (SAM) foundation model with the camera image to
generate a set of unlabeled generic masks. We fuse the masks with the rendered
pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel
predictions. During inference, we drop the NeRF head and run our model with
only LiDAR. We show the effectiveness of our approach in three public LiDAR
Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.

Comments:
- IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS) 2024

---

## LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Huibin Zhao, Weipeng Guan, Peng Lu | cs.RO | [PDF](http://arxiv.org/pdf/2411.02703v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown its ability in rapid rendering and
high-fidelity mapping. In this paper, we introduce LVI-GS, a tightly-coupled
LiDAR-Visual-Inertial mapping framework with 3DGS, which leverages the
complementary characteristics of LiDAR and image sensors to capture both
geometric structures and visual details of 3D scenes. To this end, the 3D
Gaussians are initialized from colourized LiDAR points and optimized using
differentiable rendering. In order to achieve high-fidelity mapping, we
introduce a pyramid-based training approach to effectively learn multi-level
features and incorporate depth loss derived from LiDAR measurements to improve
geometric feature perception. Through well-designed strategies for Gaussian-Map
expansion, keyframe selection, thread management, and custom CUDA acceleration,
our framework achieves real-time photo-realistic mapping. Numerical experiments
are performed to evaluate the superior performance of our method compared to
state-of-the-art 3D reconstruction systems.



---

## CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model  Retrieval

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Xin Wen, Xuening Zhu, Renjiao Yi, Zhifeng Wang, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.02979v1){: .btn .btn-green } |

**Abstract**: Reconstructing from multi-view images is a longstanding problem in 3D vision,
where neural radiance fields (NeRFs) have shown great potential and get
realistic rendered images of novel views. Currently, most NeRF methods either
require accurate camera poses or a large number of input images, or even both.
Reconstructing NeRF from few-view images without poses is challenging and
highly ill-posed. To address this problem, we propose CAD-NeRF, a method
reconstructed from less than 10 images without any known poses. Specifically,
we build a mini library of several CAD models from ShapeNet and render them
from many random views. Given sparse-view input images, we run a model and pose
retrieval from the library, to get a model with similar shapes, serving as the
density supervision and pose initializations. Here we propose a multi-view pose
retrieval method to avoid pose conflicts among views, which is a new and unseen
problem in uncalibrated NeRF methods. Then, the geometry of the object is
trained by the CAD guidance. The deformation of the density field and camera
poses are optimized jointly. Then texture and density are trained and
fine-tuned as well. All training phases are in self-supervised manners.
Comprehensive evaluations of synthetic and real images show that CAD-NeRF
successfully learns accurate densities with a large deformation from retrieved
CAD models, showing the generalization abilities.

Comments:
- The article has been accepted by Frontiers of Computer Science (FCS)

---

## Enhancing Exploratory Capability of Visual Navigation Using Uncertainty  of Implicit Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Yichen Wang, Qiming Liu, Zhe Liu, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2411.03487v1){: .btn .btn-green } |

**Abstract**: In the context of visual navigation in unknown scenes, both "exploration" and
"exploitation" are equally crucial. Robots must first establish environmental
cognition through exploration and then utilize the cognitive information to
accomplish target searches. However, most existing methods for image-goal
navigation prioritize target search over the generation of exploratory
behavior. To address this, we propose the Navigation with Uncertainty-driven
Exploration (NUE) pipeline, which uses an implicit and compact scene
representation, NeRF, as a cognitive structure. We estimate the uncertainty of
NeRF and augment the exploratory ability by the uncertainty to in turn
facilitate the construction of implicit representation. Simultaneously, we
extract memory information from NeRF to enhance the robot's reasoning ability
for determining the location of the target. Ultimately, we seamlessly combine
the two generated abilities to produce navigational actions. Our pipeline is
end-to-end, with the environmental cognitive structure being constructed
online. Extensive experimental results on image-goal navigation demonstrate the
capability of our pipeline to enhance exploratory behaviors, while also
enabling a natural transition from the exploration to exploitation phase. This
enables our model to outperform existing memory-based cognitive navigation
structures in terms of navigation performance.



---

## GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface  Reconstruction in Open Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Gaochao Song, Chong Cheng, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.01853v2){: .btn .btn-green } |

**Abstract**: In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.

Comments:
- NeurIPS 2024

---

## A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Matthew McDermott, Jason Rife | cs.CV | [PDF](http://arxiv.org/pdf/2411.01725v1){: .btn .btn-green } |

**Abstract**: In this paper we reexamine the process through which a Neural Radiance Field
(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image
applications where camera pixels integrate light over time, LiDAR pulses arrive
at specific times. As such, multiple LiDAR returns are possible for any given
detector and the classification of these returns is inherently probabilistic.
Applying a traditional NeRF training routine can result in the network learning
phantom surfaces in free space between conflicting range measurements, similar
to how floater aberrations may be produced by an image model. We show that by
formulating loss as an integral of probability (rather than as an integral of
optical density) the network can learn multiple peaks for a given ray, allowing
the sampling of first, nth, or strongest returns from a single output channel.
Code is available at https://github.com/mcdermatt/PLINK



---

## FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage  Training

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers | cs.CV | [PDF](http://arxiv.org/pdf/2411.02229v2){: .btn .btn-green } |

**Abstract**: The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.

Comments:
- Accepted by NeurIPS2024

---

## NeRF-Aug: Data Augmentation for Robotics with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Eric Zhu, Mara Levy, Matthew Gwilliam, Abhinav Shrivastava | cs.RO | [PDF](http://arxiv.org/pdf/2411.02482v1){: .btn .btn-green } |

**Abstract**: Training a policy that can generalize to unknown objects is a long standing
challenge within the field of robotics. The performance of a policy often drops
significantly in situations where an object in the scene was not seen during
training. To solve this problem, we present NeRF-Aug, a novel method that is
capable of teaching a policy to interact with objects that are not present in
the dataset. This approach differs from existing approaches by leveraging the
speed and photorealism of a neural radiance field for augmentation. NeRF- Aug
both creates more photorealistic data and runs 3.83 times faster than existing
methods. We demonstrate the effectiveness of our method on 4 tasks with 11
novel objects that have no expert demonstration data. We achieve an average
69.1% success rate increase over existing methods. See video results at
https://nerf-aug.github.io.



---

## SplatOverflow: Asynchronous Hardware Troubleshooting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Amritansh Kwatra, Tobias Wienberg, Ilan Mandel, Ritik Batra, Peter He, Francois Guimbretiere, Thijs Roumen | cs.HC | [PDF](http://arxiv.org/pdf/2411.02332v2){: .btn .btn-green } |

**Abstract**: As tools for designing and manufacturing hardware become more accessible,
smaller producers can develop and distribute novel hardware. However, there
aren't established tools to support end-user hardware troubleshooting or
routine maintenance. As a result, technical support for hardware remains ad-hoc
and challenging to scale. Inspired by software troubleshooting workflows like
StackOverflow, we propose a workflow for asynchronous hardware troubleshooting:
SplatOverflow. SplatOverflow creates a novel boundary object, the SplatOverflow
scene, that users reference to communicate about hardware. The scene comprises
a 3D Gaussian Splat of the user's hardware registered onto the hardware's CAD
model. The splat captures the current state of the hardware, and the registered
CAD model acts as a referential anchor for troubleshooting instructions. With
SplatOverflow, maintainers can directly address issues and author instructions
in the user's workspace. The instructions define workflows that can easily be
shared between users and recontextualized in new environments. In this paper,
we describe the design of SplatOverflow, detail the workflows it enables, and
illustrate its utility to different kinds of users. We also validate that
non-experts can use SplatOverflow to troubleshoot common problems with a 3D
printer in a user study.

Comments:
- Our accompanying video figure is available at:
  https://youtu.be/m4TKeBDuZkU

---

## Modeling Uncertainty in 3D Gaussian Splatting through Continuous  Semantic Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Joey Wilson, Marcelino Almeida, Min Sun, Sachit Mahajan, Maani Ghaffari, Parker Ewen, Omid Ghasemalizadeh, Cheng-Hao Kuo, Arnie Sen | cs.RO | [PDF](http://arxiv.org/pdf/2411.02547v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel algorithm for probabilistically updating
and rasterizing semantic maps within 3D Gaussian Splatting (3D-GS). Although
previous methods have introduced algorithms which learn to rasterize features
in 3D-GS for enhanced scene understanding, 3D-GS can fail without warning which
presents a challenge for safety-critical robotic applications. To address this
gap, we propose a method which advances the literature of continuous semantic
mapping from voxels to ellipsoids, combining the precise structure of 3D-GS
with the ability to quantify uncertainty of probabilistic robotic maps. Given a
set of images, our algorithm performs a probabilistic semantic update directly
on the 3D ellipsoids to obtain an expectation and variance through the use of
conjugate priors. We also propose a probabilistic rasterization which returns
per-pixel segmentation predictions with quantifiable uncertainty. We compare
our method with similar probabilistic voxel-based methods to verify our
extension to 3D ellipsoids, and perform ablation studies on uncertainty
quantification and temporal smoothing.



---

## Real-Time Spatio-Temporal Reconstruction of Dynamic Endoscopic Scenes  with 4D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-02 | Fengze Li, Jishuai He, Jieming Ma, Zhijing Wu | cs.CV | [PDF](http://arxiv.org/pdf/2411.01218v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction is essential in robotic minimally invasive
surgery, providing crucial spatial information that enhances surgical precision
and outcomes. However, existing methods struggle to address the complex,
temporally dynamic nature of endoscopic scenes. This paper presents
ST-Endo4DGS, a novel framework that models the spatio-temporal volume of
dynamic endoscopic scenes using unbiased 4D Gaussian Splatting (4DGS)
primitives, parameterized by anisotropic ellipses with flexible 4D rotations.
This approach enables precise representation of deformable tissue dynamics,
capturing intricate spatial and temporal correlations in real time.
Additionally, we extend spherindrical harmonics to represent time-evolving
appearance, achieving realistic adaptations to lighting and view changes. A new
endoscopic normal alignment constraint (ENAC) further enhances geometric
fidelity by aligning rendered normals with depth-derived geometry. Extensive
evaluations show that ST-Endo4DGS outperforms existing methods in both visual
quality and real-time performance, establishing a new state-of-the-art in
dynamic scene reconstruction for endoscopic surgery.



---

## CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for  Large-Scale Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.00771v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field
reconstruction, manifesting efficient and high-fidelity novel view synthesis.
However, accurately representing surfaces, especially in large and complex
scenarios, remains a significant challenge due to the unstructured nature of
3DGS. In this paper, we present CityGaussianV2, a novel approach for
large-scale scene reconstruction that addresses critical challenges related to
geometric accuracy and efficiency. Building on the favorable generalization
capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and
scalability issues. Specifically, we implement a decomposed-gradient-based
densification and depth regression technique to eliminate blurry artifacts and
accelerate convergence. To scale up, we introduce an elongation filter that
mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we
optimize the CityGaussian pipeline for parallel training, achieving up to
10$\times$ compression, at least 25% savings in training time, and a 50%
decrease in memory usage. We also established standard geometry benchmarks
under large-scale scenes. Experimental results demonstrate that our method
strikes a promising balance between visual quality, geometric accuracy, as well
as storage and training costs. The project page is available at
https://dekuliutesla.github.io/CityGaussianV2/.

Comments:
- Project Page: https://dekuliutesla.github.io/CityGaussianV2/

---

## PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud  Understanding


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Jincen Jiang, Qianyu Zhou, Yuhang Li, Xinkui Zhao, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang, Xuequan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00632v1){: .btn .btn-green } |

**Abstract**: In this paper, we present PCoTTA, an innovative, pioneering framework for
Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding,
enhancing the model's transferability towards the continually changing target
domain. We introduce a multi-task setting for PCoTTA, which is practical and
realistic, handling multiple tasks within one unified model during the
continual adaptation. Our PCoTTA involves three key components: automatic
prototype mixture (APM), Gaussian Splatted feature shifting (GSFS), and
contrastive prototype repulsion (CPR). Firstly, APM is designed to
automatically mix the source prototypes with the learnable prototypes with a
similarity balancing factor, avoiding catastrophic forgetting. Then, GSFS
dynamically shifts the testing sample toward the source domain, mitigating
error accumulation in an online manner. In addition, CPR is proposed to pull
the nearest learnable prototype close to the testing feature and push it away
from other prototypes, making each prototype distinguishable during the
adaptation. Experimental comparisons lead to a new benchmark, demonstrating
PCoTTA's superiority in boosting the model's transferability towards the
continually changing target domain.

Comments:
- Accepted to NeurIPS 2024

---

## ZIM: Zero-Shot Image Matting for Anything

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Beomyoung Kim, Chanyong Shin, Joonhyun Jeong, Hyungsik Jung, Se-Yun Lee, Sewhan Chun, Dong-Hyun Hwang, Joonsang Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00626v1){: .btn .btn-green } |

**Abstract**: The recent segmentation foundation model, Segment Anything Model (SAM),
exhibits strong zero-shot segmentation capabilities, but it falls short in
generating fine-grained precise masks. To address this limitation, we propose a
novel zero-shot image matting model, called ZIM, with two key contributions:
First, we develop a label converter that transforms segmentation labels into
detailed matte labels, constructing the new SA1B-Matte dataset without costly
manual annotations. Training SAM with this dataset enables it to generate
precise matte masks while maintaining its zero-shot capability. Second, we
design the zero-shot matting model equipped with a hierarchical pixel decoder
to enhance mask representation, along with a prompt-aware masked attention
mechanism to improve performance by enabling the model to focus on regions
specified by visual prompts. We evaluate ZIM using the newly introduced
MicroMat-3K test set, which contains high-quality micro-level matte labels.
Experimental results show that ZIM outperforms existing methods in fine-grained
mask generation and zero-shot generalization. Furthermore, we demonstrate the
versatility of ZIM in various downstream tasks requiring precise masks, such as
image inpainting and 3D NeRF. Our contributions provide a robust foundation for
advancing zero-shot matting and its downstream applications across a wide range
of computer vision tasks. The code is available at
\url{https://github.com/naver-ai/ZIM}.

Comments:
- preprint (21 pages, 16 figures, and 8 tables)
