---
layout: default
title: November 2024
parent: Papers
nav_order: 202411
---

<!---metadata--->


## SplatOverflow: Asynchronous Hardware Troubleshooting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Amritansh Kwatra, Tobias Wienberg, Ilan Mandel, Ritik Batra, Peter He, Francois Guimbretiere, Thijs Roumen | cs.HC | [PDF](http://arxiv.org/pdf/2411.02332v1){: .btn .btn-green } |

**Abstract**: As tools for designing and manufacturing hardware become more accessible,
smaller producers can develop and distribute novel hardware. However, there
aren't established tools to support end-user hardware troubleshooting or
routine maintenance. As a result, technical support for hardware remains ad-hoc
and challenging to scale. Inspired by software troubleshooting workflows like
StackOverflow, we propose a workflow for asynchronous hardware troubleshooting:
SplatOverflow. SplatOverflow creates a novel boundary object, the SplatOverflow
scene, that users reference to communicate about hardware. The scene comprises
a 3D Gaussian Splat of the user's hardware registered onto the hardware's CAD
model. The splat captures the current state of the hardware, and the registered
CAD model acts as a referential anchor for troubleshooting instructions. With
SplatOverflow, maintainers can directly address issues and author instructions
in the user's workspace. The instructions define workflows that can easily be
shared between users and recontextualized in new environments. In this paper,
we describe the design of SplatOverflow, detail the workflows it enables, and
illustrate its utility to different kinds of users. We also validate that
non-experts can use SplatOverflow to troubleshoot common problems with a 3D
printer in a user study.

Comments:
- Our accompanying video figure is available at:
  https://youtu.be/m4TKeBDuZkU

---

## FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage  Training

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers | cs.CV | [PDF](http://arxiv.org/pdf/2411.02229v1){: .btn .btn-green } |

**Abstract**: The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.

Comments:
- Accepted by NeurIPS2024

---

## GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface  Reconstruction in Open Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Gaochao Song, Chong Cheng, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.01853v1){: .btn .btn-green } |

**Abstract**: In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.

Comments:
- NeurIPS 2024

---

## A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Matthew McDermott, Jason Rife | cs.CV | [PDF](http://arxiv.org/pdf/2411.01725v1){: .btn .btn-green } |

**Abstract**: In this paper we reexamine the process through which a Neural Radiance Field
(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image
applications where camera pixels integrate light over time, LiDAR pulses arrive
at specific times. As such, multiple LiDAR returns are possible for any given
detector and the classification of these returns is inherently probabilistic.
Applying a traditional NeRF training routine can result in the network learning
phantom surfaces in free space between conflicting range measurements, similar
to how floater aberrations may be produced by an image model. We show that by
formulating loss as an integral of probability (rather than as an integral of
optical density) the network can learn multiple peaks for a given ray, allowing
the sampling of first, nth, or strongest returns from a single output channel.
Code is available at https://github.com/mcdermatt/PLINK



---

## Real-Time Spatio-Temporal Reconstruction of Dynamic Endoscopic Scenes  with 4D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-02 | Fengze Li, Jishuai He, Jieming Ma, Zhijing Wu | cs.CV | [PDF](http://arxiv.org/pdf/2411.01218v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction is essential in robotic minimally invasive
surgery, providing crucial spatial information that enhances surgical precision
and outcomes. However, existing methods struggle to address the complex,
temporally dynamic nature of endoscopic scenes. This paper presents
ST-Endo4DGS, a novel framework that models the spatio-temporal volume of
dynamic endoscopic scenes using unbiased 4D Gaussian Splatting (4DGS)
primitives, parameterized by anisotropic ellipses with flexible 4D rotations.
This approach enables precise representation of deformable tissue dynamics,
capturing intricate spatial and temporal correlations in real time.
Additionally, we extend spherindrical harmonics to represent time-evolving
appearance, achieving realistic adaptations to lighting and view changes. A new
endoscopic normal alignment constraint (ENAC) further enhances geometric
fidelity by aligning rendered normals with depth-derived geometry. Extensive
evaluations show that ST-Endo4DGS outperforms existing methods in both visual
quality and real-time performance, establishing a new state-of-the-art in
dynamic scene reconstruction for endoscopic surgery.



---

## CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for  Large-Scale Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.00771v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field
reconstruction, manifesting efficient and high-fidelity novel view synthesis.
However, accurately representing surfaces, especially in large and complex
scenarios, remains a significant challenge due to the unstructured nature of
3DGS. In this paper, we present CityGaussianV2, a novel approach for
large-scale scene reconstruction that addresses critical challenges related to
geometric accuracy and efficiency. Building on the favorable generalization
capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and
scalability issues. Specifically, we implement a decomposed-gradient-based
densification and depth regression technique to eliminate blurry artifacts and
accelerate convergence. To scale up, we introduce an elongation filter that
mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we
optimize the CityGaussian pipeline for parallel training, achieving up to
10$\times$ compression, at least 25% savings in training time, and a 50%
decrease in memory usage. We also established standard geometry benchmarks
under large-scale scenes. Experimental results demonstrate that our method
strikes a promising balance between visual quality, geometric accuracy, as well
as storage and training costs. The project page is available at
https://dekuliutesla.github.io/CityGaussianV2/.

Comments:
- Project Page: https://dekuliutesla.github.io/CityGaussianV2/

---

## PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud  Understanding


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Jincen Jiang, Qianyu Zhou, Yuhang Li, Xinkui Zhao, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang, Xuequan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00632v1){: .btn .btn-green } |

**Abstract**: In this paper, we present PCoTTA, an innovative, pioneering framework for
Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding,
enhancing the model's transferability towards the continually changing target
domain. We introduce a multi-task setting for PCoTTA, which is practical and
realistic, handling multiple tasks within one unified model during the
continual adaptation. Our PCoTTA involves three key components: automatic
prototype mixture (APM), Gaussian Splatted feature shifting (GSFS), and
contrastive prototype repulsion (CPR). Firstly, APM is designed to
automatically mix the source prototypes with the learnable prototypes with a
similarity balancing factor, avoiding catastrophic forgetting. Then, GSFS
dynamically shifts the testing sample toward the source domain, mitigating
error accumulation in an online manner. In addition, CPR is proposed to pull
the nearest learnable prototype close to the testing feature and push it away
from other prototypes, making each prototype distinguishable during the
adaptation. Experimental comparisons lead to a new benchmark, demonstrating
PCoTTA's superiority in boosting the model's transferability towards the
continually changing target domain.

Comments:
- Accepted to NeurIPS 2024

---

## ZIM: Zero-Shot Image Matting for Anything

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Beomyoung Kim, Chanyong Shin, Joonhyun Jeong, Hyungsik Jung, Se-Yun Lee, Sewhan Chun, Dong-Hyun Hwang, Joonsang Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00626v1){: .btn .btn-green } |

**Abstract**: The recent segmentation foundation model, Segment Anything Model (SAM),
exhibits strong zero-shot segmentation capabilities, but it falls short in
generating fine-grained precise masks. To address this limitation, we propose a
novel zero-shot image matting model, called ZIM, with two key contributions:
First, we develop a label converter that transforms segmentation labels into
detailed matte labels, constructing the new SA1B-Matte dataset without costly
manual annotations. Training SAM with this dataset enables it to generate
precise matte masks while maintaining its zero-shot capability. Second, we
design the zero-shot matting model equipped with a hierarchical pixel decoder
to enhance mask representation, along with a prompt-aware masked attention
mechanism to improve performance by enabling the model to focus on regions
specified by visual prompts. We evaluate ZIM using the newly introduced
MicroMat-3K test set, which contains high-quality micro-level matte labels.
Experimental results show that ZIM outperforms existing methods in fine-grained
mask generation and zero-shot generalization. Furthermore, we demonstrate the
versatility of ZIM in various downstream tasks requiring precise masks, such as
image inpainting and 3D NeRF. Our contributions provide a robust foundation for
advancing zero-shot matting and its downstream applications across a wide range
of computer vision tasks. The code is available at
\url{https://github.com/naver-ai/ZIM}.

Comments:
- preprint (21 pages, 16 figures, and 8 tables)
