---
layout: default
title: November 2024
parent: Papers
nav_order: 202411
---

<!---metadata--->


## PR-ENDO: Physically Based Relightable Gaussian Splatting for Endoscopy

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Joanna Kaleta, Weronika Smolak-Dyżewska, Dawid Malarz, Diego Dall'Alba, Przemysław Korzeniowski, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2411.12510v1){: .btn .btn-green } |

**Abstract**: Endoscopic procedures are crucial for colorectal cancer diagnosis, and
three-dimensional reconstruction of the environment for real-time novel-view
synthesis can significantly enhance diagnosis. We present PR-ENDO, a framework
that leverages 3D Gaussian Splatting within a physically based, relightable
model tailored for the complex acquisition conditions in endoscopy, such as
restricted camera rotations and strong view-dependent illumination. By
exploiting the connection between the camera and light source, our approach
introduces a relighting model to capture the intricate interactions between
light and tissue using physically based rendering and MLP. Existing methods
often produce artifacts and inconsistencies under these conditions, which
PR-ENDO overcomes by incorporating a specialized diffuse MLP that utilizes
light angles and normal vectors, achieving stable reconstructions even with
limited training camera rotations. We benchmarked our framework using a
publicly available dataset and a newly introduced dataset with wider camera
rotations. Our methods demonstrated superior image quality compared to baseline
approaches.



---

## LiV-GS: LiDAR-Vision Integration for 3D Gaussian Splatting SLAM in  Outdoor Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Renxiang Xiao, Wei Liu, Yushuai Chen, Liang Hu | cs.RO | [PDF](http://arxiv.org/pdf/2411.12185v1){: .btn .btn-green } |

**Abstract**: We present LiV-GS, a LiDAR-visual SLAM system in outdoor environments that
leverages 3D Gaussian as a differentiable spatial representation. Notably,
LiV-GS is the first method that directly aligns discrete and sparse LiDAR data
with continuous differentiable Gaussian maps in large-scale outdoor scenes,
overcoming the limitation of fixed resolution in traditional LiDAR mapping. The
system aligns point clouds with Gaussian maps using shared covariance
attributes for front-end tracking and integrates the normal orientation into
the loss function to refines the Gaussian map. To reliably and stably update
Gaussians outside the LiDAR field of view, we introduce a novel conditional
Gaussian constraint that aligns these Gaussians closely with the nearest
reliable ones. The targeted adjustment enables LiV-GS to achieve fast and
accurate mapping with novel view synthesis at a rate of 7.98 FPS. Extensive
comparative experiments demonstrate LiV-GS's superior performance in SLAM,
image rendering and mapping. The successful cross-modal radar-LiDAR
localization highlights the potential of LiV-GS for applications in cross-modal
semantic positioning and object segmentation with Gaussian maps.



---

## Sketch-guided Cage-based 3D Gaussian Splatting Deformation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Tianhao Xie, Noam Aigerman, Eugene Belilovsky, Tiberiu Popa | cs.CV | [PDF](http://arxiv.org/pdf/2411.12168v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (GS) is one of the most promising novel 3D
representations that has received great interest in computer graphics and
computer vision. While various systems have introduced editing capabilities for
3D GS, such as those guided by text prompts, fine-grained control over
deformation remains an open challenge. In this work, we present a novel
sketch-guided 3D GS deformation system that allows users to intuitively modify
the geometry of a 3D GS model by drawing a silhouette sketch from a single
viewpoint. Our approach introduces a new deformation method that combines
cage-based deformations with a variant of Neural Jacobian Fields, enabling
precise, fine-grained control. Additionally, it leverages large-scale 2D
diffusion priors and ControlNet to ensure the generated deformations are
semantically plausible. Through a series of experiments, we demonstrate the
effectiveness of our method and showcase its ability to animate static 3D GS
models as one of its key applications.

Comments:
- 10 pages, 9 figures

---

## SCIGS: 3D Gaussians Splatting from a Snapshot Compressive Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Zixu Wang, Hao Yang, Yu Guo, Fei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.12471v1){: .btn .btn-green } |

**Abstract**: Snapshot Compressive Imaging (SCI) offers a possibility for capturing
information in high-speed dynamic scenes, requiring efficient reconstruction
method to recover scene information. Despite promising results, current deep
learning-based and NeRF-based reconstruction methods face challenges: 1) deep
learning-based reconstruction methods struggle to maintain 3D structural
consistency within scenes, and 2) NeRF-based reconstruction methods still face
limitations in handling dynamic scenes. To address these challenges, we propose
SCIGS, a variant of 3DGS, and develop a primitive-level transformation network
that utilizes camera pose stamps and Gaussian primitive coordinates as
embedding vectors. This approach resolves the necessity of camera pose in
vanilla 3DGS and enhances multi-view 3D structural consistency in dynamic
scenes by utilizing transformed primitives. Additionally, a high-frequency
filter is introduced to eliminate the artifacts generated during the
transformation. The proposed SCIGS is the first to reconstruct a 3D explicit
scene from a single compressed image, extending its application to dynamic 3D
scenes. Experiments on both static and dynamic scenes demonstrate that SCIGS
not only enhances SCI decoding but also outperforms current state-of-the-art
methods in reconstructing dynamic 3D scenes from a single compressed image. The
code will be made available upon publication.



---

## Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear  Kernels

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Haodong Chen, Runnan Chen, Qiang Qu, Zhaoqing Wang, Tongliang Liu, Xiaoming Chen, Yuk Ying Chung | cs.CV | [PDF](http://arxiv.org/pdf/2411.12440v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS) have substantially
improved novel view synthesis, enabling high-quality reconstruction and
real-time rendering. However, blurring artifacts, such as floating primitives
and over-reconstruction, remain challenging. Current methods address these
issues by refining scene structure, enhancing geometric representations,
addressing blur in training images, improving rendering consistency, and
optimizing density control, yet the role of kernel design remains
underexplored. We identify the soft boundaries of Gaussian ellipsoids as one of
the causes of these artifacts, limiting detail capture in high-frequency
regions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which
replaces Gaussian kernels with linear kernels to achieve sharper and more
precise results, particularly in high-frequency regions. Through evaluations on
three datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along
with a 30% FPS improvement over baseline 3DGS. The implementation will be made
publicly available upon acceptance. \freefootnote{*Corresponding author.



---

## GaussianPretrain: A Simple Unified 3D Gaussian Representation for Visual  Pre-training in Autonomous Driving

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-19 | Shaoqing Xu, Fang Li, Shengyin Jiang, Ziying Song, Li Liu, Zhi-xin Yang | cs.CV | [PDF](http://arxiv.org/pdf/2411.12452v1){: .btn .btn-green } |

**Abstract**: Self-supervised learning has made substantial strides in image processing,
while visual pre-training for autonomous driving is still in its infancy.
Existing methods often focus on learning geometric scene information while
neglecting texture or treating both aspects separately, hindering comprehensive
scene understanding. In this context, we are excited to introduce
GaussianPretrain, a novel pre-training paradigm that achieves a holistic
understanding of the scene by uniformly integrating geometric and texture
representations. Conceptualizing 3D Gaussian anchors as volumetric LiDAR
points, our method learns a deepened understanding of scenes to enhance
pre-training performance with detailed spatial structure and texture, achieving
that 40.6% faster than NeRF-based method UniPAD with 70% GPU memory only. We
demonstrate the effectiveness of GaussianPretrain across multiple 3D perception
tasks, showing significant performance improvements, such as a 7.05% increase
in NDS for 3D object detection, boosts mAP by 1.9% in HD map construction and
0.8% improvement on Occupancy prediction. These significant gains highlight
GaussianPretrain's theoretical innovation and strong practical potential,
promoting visual pre-training development for autonomous driving. Source code
will be available at https://github.com/Public-BOTs/GaussianPretrain

Comments:
- 10 pages, 5 figures

---

## FruitNinja: 3D Object Interior Texture Generation with Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Fangyu Wu, Yuhao Chen | cs.CV | [PDF](http://arxiv.org/pdf/2411.12089v1){: .btn .btn-green } |

**Abstract**: In the real world, objects reveal internal textures when sliced or cut, yet
this behavior is not well-studied in 3D generation tasks today. For example,
slicing a virtual 3D watermelon should reveal flesh and seeds. Given that no
available dataset captures an object's full internal structure and collecting
data from all slices is impractical, generative methods become the obvious
approach. However, current 3D generation and inpainting methods often focus on
visible appearance and overlook internal textures. To bridge this gap, we
introduce FruitNinja, the first method to generate internal textures for 3D
objects undergoing geometric and topological changes. Our approach produces
objects via 3D Gaussian Splatting (3DGS) with both surface and interior
textures synthesized, enabling real-time slicing and rendering without
additional optimization. FruitNinja leverages a pre-trained diffusion model to
progressively inpaint cross-sectional views and applies voxel-grid-based
smoothing to achieve cohesive textures throughout the object. Our OpaqueAtom GS
strategy overcomes 3DGS limitations by employing densely distributed opaque
Gaussians, avoiding biases toward larger particles that destabilize training
and sharp color transitions for fine-grained textures. Experimental results
show that FruitNinja substantially outperforms existing approaches, showcasing
unmatched visual quality in real-time rendered internal views across arbitrary
geometry manipulations.



---

## DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and  Surface Reconstruction for Urban Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Chensheng Peng, Chengwei Zhang, Yixiao Wang, Chenfeng Xu, Yichen Xie, Wenzhao Zheng, Kurt Keutzer, Masayoshi Tomizuka, Wei Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2411.11921v1){: .btn .btn-green } |

**Abstract**: We present DeSiRe-GS, a self-supervised gaussian splatting representation,
enabling effective static-dynamic decomposition and high-fidelity surface
reconstruction in complex driving scenarios. Our approach employs a two-stage
optimization pipeline of dynamic street Gaussians. In the first stage, we
extract 2D motion masks based on the observation that 3D Gaussian Splatting
inherently can reconstruct only the static regions in dynamic environments.
These extracted 2D motion priors are then mapped into the Gaussian space in a
differentiable manner, leveraging an efficient formulation of dynamic Gaussians
in the second stage. Combined with the introduced geometric regularizations,
our method are able to address the over-fitting issues caused by data sparsity
in autonomous driving, reconstructing physically plausible Gaussians that align
with object surfaces rather than floating in air. Furthermore, we introduce
temporal cross-view consistency to ensure coherence across time and viewpoints,
resulting in high-quality surface reconstruction. Comprehensive experiments
demonstrate the efficiency and effectiveness of DeSiRe-GS, surpassing prior
self-supervised arts and achieving accuracy comparable to methods relying on
external 3D bounding box annotations. Code is available at
\url{https://github.com/chengweialan/DeSiRe-GS}



---

## TimeFormer: Capturing Temporal Relationships of Deformable 3D Gaussians  for Robust Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | DaDong Jiang, Zhihui Ke, Xiaobo Zhou, Zhi Hou, Xianghui Yang, Wenbo Hu, Tie Qiu, Chunchao Guo | cs.CV | [PDF](http://arxiv.org/pdf/2411.11941v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction is a long-term challenge in 3D vision. Recent
methods extend 3D Gaussian Splatting to dynamic scenes via additional
deformation fields and apply explicit constraints like motion flow to guide the
deformation. However, they learn motion changes from individual timestamps
independently, making it challenging to reconstruct complex scenes,
particularly when dealing with violent movement, extreme-shaped geometries, or
reflective surfaces. To address the above issue, we design a plug-and-play
module called TimeFormer to enable existing deformable 3D Gaussians
reconstruction methods with the ability to implicitly model motion patterns
from a learning perspective. Specifically, TimeFormer includes a Cross-Temporal
Transformer Encoder, which adaptively learns the temporal relationships of
deformable 3D Gaussians. Furthermore, we propose a two-stream optimization
strategy that transfers the motion knowledge learned from TimeFormer to the
base stream during the training phase. This allows us to remove TimeFormer
during inference, thereby preserving the original rendering speed. Extensive
experiments in the multi-view and monocular dynamic scenes validate qualitative
and quantitative improvement brought by TimeFormer. Project Page:
https://patrickddj.github.io/TimeFormer/



---

## RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Xinhai Li, Jialin Li, Ziheng Zhang, Rui Zhang, Fan Jia, Tiancai Wang, Haoqiang Fan, Kuo-Kun Tseng, Ruiping Wang | cs.RO | [PDF](http://arxiv.org/pdf/2411.11839v1){: .btn .btn-green } |

**Abstract**: Efficient acquisition of real-world embodied data has been increasingly
critical. However, large-scale demonstrations captured by remote operation tend
to take extremely high costs and fail to scale up the data size in an efficient
manner. Sampling the episodes under a simulated environment is a promising way
for large-scale collection while existing simulators fail to high-fidelity
modeling on texture and physics. To address these limitations, we introduce the
RoboGSim, a real2sim2real robotic simulator, powered by 3D Gaussian Splatting
and the physics engine. RoboGSim mainly includes four parts: Gaussian
Reconstructor, Digital Twins Builder, Scene Composer, and Interactive Engine.
It can synthesize the simulated data with novel views, objects, trajectories,
and scenes. RoboGSim also provides an online, reproducible, and safe evaluation
for different manipulation policies. The real2sim and sim2real transfer
experiments show a high consistency in the texture and physics. Moreover, the
effectiveness of synthetic data is validated under the real-world manipulated
tasks. We hope RoboGSim serves as a closed-loop simulator for fair comparison
on policy learning. More information can be found on our project page
https://robogsim.github.io/ .



---

## Towards Degradation-Robust Reconstruction in Generalizable NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Chan Ho Park, Ka Leong Cheng, Zhicheng Wang, Qifeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2411.11691v1){: .btn .btn-green } |

**Abstract**: Generalizable Neural Radiance Field (GNeRF) across scenes has been proven to
be an effective way to avoid per-scene optimization by representing a scene
with deep image features of source images. However, despite its potential for
real-world applications, there has been limited research on the robustness of
GNeRFs to different types of degradation present in the source images. The lack
of such research is primarily attributed to the absence of a large-scale
dataset fit for training a degradation-robust generalizable NeRF model. To
address this gap and facilitate investigations into the degradation robustness
of 3D reconstruction tasks, we construct the Objaverse Blur Dataset, comprising
50,000 images from over 1000 settings featuring multiple levels of blur
degradation. In addition, we design a simple and model-agnostic module for
enhancing the degradation robustness of GNeRFs. Specifically, by extracting
3D-aware features through a lightweight depth estimator and denoiser, the
proposed module shows improvement on different popular methods in GNeRFs in
terms of both quantitative and visual quality over varying degradation types
and levels. Our dataset and code will be made publicly available.



---

## LeC$^2$O-NeRF: Learning Continuous and Compact Large-Scale Occupancy for  Urban Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Zhenxing Mi, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.11374v1){: .btn .btn-green } |

**Abstract**: In NeRF, a critical problem is to effectively estimate the occupancy to guide
empty-space skipping and point sampling. Grid-based methods work well for
small-scale scenes. However, on large-scale scenes, they are limited by
predefined bounding boxes, grid resolutions, and high memory usage for grid
updates, and thus struggle to speed up training for large-scale, irregularly
bounded and complex urban scenes without sacrificing accuracy. In this paper,
we propose to learn a continuous and compact large-scale occupancy network,
which can classify 3D points as occupied or unoccupied points. We train this
occupancy network end-to-end together with the radiance field in a
self-supervised manner by three designs. First, we propose a novel imbalanced
occupancy loss to regularize the occupancy network. It makes the occupancy
network effectively control the ratio of unoccupied and occupied points,
motivated by the prior that most of 3D scene points are unoccupied. Second, we
design an imbalanced architecture containing a large scene network and a small
empty space network to separately encode occupied and unoccupied points
classified by the occupancy network. This imbalanced structure can effectively
model the imbalanced nature of occupied and unoccupied regions. Third, we
design an explicit density loss to guide the occupancy network, making the
density of unoccupied points smaller. As far as we know, we are the first to
learn a continuous and compact occupancy of large-scale NeRF by a network. In
our experiments, our occupancy network can quickly learn more compact, accurate
and smooth occupancy compared to the occupancy grid. With our learned occupancy
as guidance for empty space skipping on challenging large-scale benchmarks, our
method consistently obtains higher accuracy compared to the occupancy grid, and
our method can speed up state-of-the-art NeRF methods without sacrificing
accuracy.

Comments:
- 13 pages

---

## GPS-Gaussian+: Generalizable Pixel-wise 3D Gaussian Splatting for  Real-Time Human-Scene Rendering from Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-18 | Boyao Zhou, Shunyuan Zheng, Hanzhang Tu, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2411.11363v1){: .btn .btn-green } |

**Abstract**: Differentiable rendering techniques have recently shown promising results for
free-viewpoint video synthesis of characters. However, such methods, either
Gaussian Splatting or neural implicit rendering, typically necessitate
per-subject optimization which does not meet the requirement of real-time
rendering in an interactive application. We propose a generalizable Gaussian
Splatting approach for high-resolution image rendering under a sparse-view
camera setting. To this end, we introduce Gaussian parameter maps defined on
the source views and directly regress Gaussian properties for instant novel
view synthesis without any fine-tuning or optimization. We train our Gaussian
parameter regression module on human-only data or human-scene data, jointly
with a depth estimation module to lift 2D parameter maps to 3D space. The
proposed framework is fully differentiable with both depth and rendering
supervision or with only rendering supervision. We further introduce a
regularization term and an epipolar attention mechanism to preserve geometry
consistency between two source views, especially when neglecting depth
supervision. Experiments on several datasets demonstrate that our method
outperforms state-of-the-art methods while achieving an exceeding rendering
speed.

Comments:
- Journal extension of CVPR 2024,Project
  page:https://yaourtb.github.io/GPS-Gaussian+

---

## Direct and Explicit 3D Generation from a Single Image


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-17 | Haoyu Wu, Meher Gitika Karumuri, Chuhang Zou, Seungbae Bang, Yuelong Li, Dimitris Samaras, Sunil Hadap | cs.CV | [PDF](http://arxiv.org/pdf/2411.10947v1){: .btn .btn-green } |

**Abstract**: Current image-to-3D approaches suffer from high computational costs and lack
scalability for high-resolution outputs. In contrast, we introduce a novel
framework to directly generate explicit surface geometry and texture using
multi-view 2D depth and RGB images along with 3D Gaussian features using a
repurposed Stable Diffusion model. We introduce a depth branch into U-Net for
efficient and high quality multi-view, cross-domain generation and incorporate
epipolar attention into the latent-to-pixel decoder for pixel-level multi-view
consistency. By back-projecting the generated depth pixels into 3D space, we
create a structured 3D representation that can be either rendered via Gaussian
splatting or extracted to high-quality meshes, thereby leveraging additional
novel view synthesis loss to further improve our performance. Extensive
experiments demonstrate that our method surpasses existing baselines in
geometry and texture quality while achieving significantly faster generation
time.

Comments:
- 3DV 2025, Project page: https://hao-yu-wu.github.io/gen3d/

---

## VeGaS: Video Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-17 | Weronika Smolak-Dyżewska, Dawid Malarz, Kornel Howil, Jan Kaczmarczyk, Marcin Mazur, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2411.11024v1){: .btn .btn-green } |

**Abstract**: Implicit Neural Representations (INRs) employ neural networks to approximate
discrete data as continuous functions. In the context of video data, such
models can be utilized to transform the coordinates of pixel locations along
with frame occurrence times (or indices) into RGB color values. Although INRs
facilitate effective compression, they are unsuitable for editing purposes. One
potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such
as the Video Gaussian Representation (VGR), which is capable of encoding video
as a multitude of 3D Gaussians and is applicable for numerous video processing
operations, including editing. Nevertheless, in this case, the capacity for
modification is constrained to a limited set of basic transformations. To
address this issue, we introduce the Video Gaussian Splatting (VeGaS) model,
which enables realistic modifications of video data. To construct VeGaS, we
propose a novel family of Folded-Gaussian distributions designed to capture
nonlinear dynamics in a video stream and model consecutive frames by 2D
Gaussians obtained as respective conditional distributions. Our experiments
demonstrate that VeGaS outperforms state-of-the-art solutions in frame
reconstruction tasks and allows realistic modifications of video data. The code
is available at: https://github.com/gmum/VeGaS.



---

## DGS-SLAM: Gaussian Splatting SLAM in Dynamic Environment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-16 | Mangyu Kong, Jaewon Lee, Seongwon Lee, Euntai Kim | cs.RO | [PDF](http://arxiv.org/pdf/2411.10722v1){: .btn .btn-green } |

**Abstract**: We introduce Dynamic Gaussian Splatting SLAM (DGS-SLAM), the first dynamic
SLAM framework built on the foundation of Gaussian Splatting. While recent
advancements in dense SLAM have leveraged Gaussian Splatting to enhance scene
representation, most approaches assume a static environment, making them
vulnerable to photometric and geometric inconsistencies caused by dynamic
objects. To address these challenges, we integrate Gaussian Splatting SLAM with
a robust filtering process to handle dynamic objects throughout the entire
pipeline, including Gaussian insertion and keyframe selection. Within this
framework, to further improve the accuracy of dynamic object removal, we
introduce a robust mask generation method that enforces photometric consistency
across keyframes, reducing noise from inaccurate segmentation and artifacts
such as shadows. Additionally, we propose the loop-aware window selection
mechanism, which utilizes unique keyframe IDs of 3D Gaussians to detect loops
between the current and past frames, facilitating joint optimization of the
current camera poses and the Gaussian map. DGS-SLAM achieves state-of-the-art
performance in both camera tracking and novel view synthesis on various dynamic
SLAM benchmarks, proving its effectiveness in handling real-world dynamic
scenes.

Comments:
- Preprint, Under review

---

## GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars  from Monocular Video

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Jingxuan Chen | cs.CV | [PDF](http://arxiv.org/pdf/2411.09952v1){: .btn .btn-green } |

**Abstract**: Avatar modelling has broad applications in human animation and virtual
try-ons. Recent advancements in this field have focused on high-quality and
comprehensive human reconstruction but often overlook the separation of
clothing from the body. To bridge this gap, this paper introduces GGAvatar
(Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular
videos. Through advanced parameterized templates and unique phased training,
this model effectively achieves decoupled, editable, and realistic
reconstruction of clothed humans. Comparative evaluations with other costly
models confirm GGAvatar's superior quality and efficiency in modelling both
clothed humans and separable garments. The paper also showcases applications in
clothing editing, as illustrated in Figure 1, highlighting the model's benefits
and the advantages of effective disentanglement. The code is available at
https://github.com/J-X-Chen/GGAvatar/.

Comments:
- MMAsia'24 Accepted

---

## GSEditPro: 3D Gaussian Splatting Editing with Attention-based  Progressive Localization

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Yanhao Sun, RunZe Tian, Xiao Han, XinYao Liu, Yan Zhang, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.10033v1){: .btn .btn-green } |

**Abstract**: With the emergence of large-scale Text-to-Image(T2I) models and implicit 3D
representations like Neural Radiance Fields (NeRF), many text-driven generative
editing methods based on NeRF have appeared. However, the implicit encoding of
geometric and textural information poses challenges in accurately locating and
controlling objects during editing. Recently, significant advancements have
been made in the editing methods of 3D Gaussian Splatting, a real-time
rendering technology that relies on explicit representation. However, these
methods still suffer from issues including inaccurate localization and limited
manipulation over editing. To tackle these challenges, we propose GSEditPro, a
novel 3D scene editing framework which allows users to perform various creative
and precise editing using text prompts only. Leveraging the explicit nature of
the 3D Gaussian distribution, we introduce an attention-based progressive
localization module to add semantic labels to each Gaussian during rendering.
This enables precise localization on editing areas by classifying Gaussians
based on their relevance to the editing prompts derived from cross-attention
layers of the T2I model. Furthermore, we present an innovative editing
optimization method based on 3D Gaussian Splatting, obtaining stable and
refined editing results through the guidance of Score Distillation Sampling and
pseudo ground truth. We prove the efficacy of our method through extensive
experiments.

Comments:
- Pacific Graphics 2024

---

## USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction  and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Kang Chen, Jiyuan Zhang, Zecheng Hao, Yajing Zheng, Tiejun Huang, Zhaofei Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.10504v1){: .btn .btn-green } |

**Abstract**: Spike cameras, as an innovative neuromorphic camera that captures scenes with
the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D
reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting
(3DGS). Previous spike-based 3D reconstruction approaches often employ a
casecased pipeline: starting with high-quality image reconstruction from spike
streams based on established spike-to-image reconstruction algorithms, then
progressing to camera pose estimation and 3D reconstruction. However, this
cascaded approach suffers from substantial cumulative errors, where quality
limitations of initial image reconstructions negatively impact pose estimation,
ultimately degrading the fidelity of the 3D reconstruction. To address these
issues, we propose a synergistic optimization framework, \textbf{USP-Gaussian},
that unifies spike-based image reconstruction, pose correction, and Gaussian
splatting into an end-to-end framework. Leveraging the multi-view consistency
afforded by 3DGS and the motion capture capability of the spike camera, our
framework enables a joint iterative optimization that seamlessly integrates
information between the spike-to-image network and 3DGS. Experiments on
synthetic datasets with accurate poses demonstrate that our method surpasses
previous approaches by effectively eliminating cascading errors. Moreover, we
integrate pose optimization to achieve robust 3D reconstruction in real-world
scenarios with inaccurate initial poses, outperforming alternative methods by
effectively reducing noise and preserving fine texture details. Our code, data
and trained models will be available at
\url{https://github.com/chenkang455/USP-Gaussian}.



---

## Efficient Density Control for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Xiaobin Deng, Changyu Diao, Min Li, Ruohan Yu, Duanqing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.10133v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) excels in novel view synthesis, balancing
advanced rendering quality with real-time performance. However, in trained
scenes, a large number of Gaussians with low opacity significantly increase
rendering costs. This issue arises due to flaws in the split and clone
operations during the densification process, which lead to extensive Gaussian
overlap and subsequent opacity reduction. To enhance the efficiency of Gaussian
utilization, we improve the adaptive density control of 3DGS. First, we
introduce a more efficient long-axis split operation to replace the original
clone and split, which mitigates Gaussian overlap and improves densification
efficiency.Second, we propose a simple adaptive pruning technique to reduce the
number of low-opacity Gaussians. Finally, by dynamically lowering the splitting
threshold and applying importance weighting, the efficiency of Gaussian
utilization is further improved.We evaluate our proposed method on various
challenging real-world datasets. Experimental results show that our Efficient
Density Control (EDC) can enhance both the rendering speed and quality.



---

## The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual  Localisation, Reconstruction and Radiance Field Methods

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Yifu Tao, Miguel Ángel Muñoz-Bañón, Lintong Zhang, Jiahao Wang, Lanke Frank Tarimo Fu, Maurice Fallon | cs.CV | [PDF](http://arxiv.org/pdf/2411.10546v1){: .btn .btn-green } |

**Abstract**: This paper introduces a large-scale multi-modal dataset captured in and
around well-known landmarks in Oxford using a custom-built multi-sensor
perception unit as well as a millimetre-accurate map from a Terrestrial LiDAR
Scanner (TLS). The perception unit includes three synchronised global shutter
colour cameras, an automotive 3D LiDAR scanner, and an inertial sensor - all
precisely calibrated. We also establish benchmarks for tasks involving
localisation, reconstruction, and novel-view synthesis, which enable the
evaluation of Simultaneous Localisation and Mapping (SLAM) methods,
Structure-from-Motion (SfM) and Multi-view Stereo (MVS) methods as well as
radiance field methods such as Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting. To evaluate 3D reconstruction the TLS 3D models are used as ground
truth. Localisation ground truth is computed by registering the mobile LiDAR
scans to the TLS 3D models. Radiance field methods are evaluated not only with
poses sampled from the input trajectory, but also from viewpoints that are from
trajectories which are distant from the training poses. Our evaluation
demonstrates a key limitation of state-of-the-art radiance field methods: we
show that they tend to overfit to the training poses/images and do not
generalise well to out-of-sequence poses. They also underperform in 3D
reconstruction compared to MVS systems using the same visual inputs. Our
dataset and benchmarks are intended to facilitate better integration of
radiance field methods and SLAM systems. The raw and processed data, along with
software for parsing and evaluation, can be accessed at
https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/.

Comments:
- Website: https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/

---

## SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D  Reconstruction


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-15 | Yutao Tang, Yuxiang Guo, Deming Li, Cheng Peng | cs.CV | [PDF](http://arxiv.org/pdf/2411.12592v1){: .btn .btn-green } |

**Abstract**: Recent efforts in Gaussian-Splat-based Novel View Synthesis can achieve
photorealistic rendering; however, such capability is limited in sparse-view
scenarios due to sparse initialization and over-fitting floaters. Recent
progress in depth estimation and alignment can provide dense point cloud with
few views; however, the resulting pose accuracy is suboptimal. In this work, we
present SPARS3R, which combines the advantages of accurate pose estimation from
Structure-from-Motion and dense point cloud from depth estimation. To this end,
SPARS3R first performs a Global Fusion Alignment process that maps a prior
dense point cloud to a sparse point cloud from Structure-from-Motion based on
triangulated correspondences. RANSAC is applied during this process to
distinguish inliers and outliers. SPARS3R then performs a second, Semantic
Outlier Alignment step, which extracts semantically coherent regions around the
outliers and performs local alignment in these regions. Along with several
improvements in the evaluation process, we demonstrate that SPARS3R can achieve
photorealistic rendering with sparse images and significantly outperforms
existing approaches.



---

## DyGASR: Dynamic Generalized Exponential Splatting with Surface Alignment  for Accelerated 3D Mesh Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-14 | Shengchao Zhao, Yundong Li | cs.CV | [PDF](http://arxiv.org/pdf/2411.09156v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting (3DGS), which lead to
high-quality novel view synthesis and accelerated rendering, have remarkably
improved the quality of radiance field reconstruction. However, the extraction
of mesh from a massive number of minute 3D Gaussian points remains great
challenge due to the large volume of Gaussians and difficulty of representation
of sharp signals caused by their inherent low-pass characteristics. To address
this issue, we propose DyGASR, which utilizes generalized exponential function
instead of traditional 3D Gaussian to decrease the number of particles and
dynamically optimize the representation of the captured signal. In addition, it
is observed that reconstructing mesh with Generalized Exponential
Splatting(GES) without modifications frequently leads to failures since the
generalized exponential distribution centroids may not precisely align with the
scene surface. To overcome this, we adopt Sugar's approach and introduce
Generalized Surface Regularization (GSR), which reduces the smallest scaling
vector of each point cloud to zero and ensures normal alignment perpendicular
to the surface, facilitating subsequent Poisson surface mesh reconstruction.
Additionally, we propose a dynamic resolution adjustment strategy that utilizes
a cosine schedule to gradually increase image resolution from low to high
during the training stage, thus avoiding constant full resolution, which
significantly boosts the reconstruction speed. Our approach surpasses existing
3DGS-based mesh reconstruction methods, as evidenced by extensive evaluations
on various scene datasets, demonstrating a 25\% increase in speed, and a 30\%
reduction in memory usage.



---

## Adversarial Attacks Using Differentiable Rendering: A Survey

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-14 | Matthew Hull, Chao Zhang, Zsolt Kira, Duen Horng Chau | cs.LG | [PDF](http://arxiv.org/pdf/2411.09749v1){: .btn .btn-green } |

**Abstract**: Differentiable rendering methods have emerged as a promising means for
generating photo-realistic and physically plausible adversarial attacks by
manipulating 3D objects and scenes that can deceive deep neural networks
(DNNs). Recently, differentiable rendering capabilities have evolved
significantly into a diverse landscape of libraries, such as Mitsuba,
PyTorch3D, and methods like Neural Radiance Fields and 3D Gaussian Splatting
for solving inverse rendering problems that share conceptually similar
properties commonly used to attack DNNs, such as back-propagation and
optimization. However, the adversarial machine learning research community has
not yet fully explored or understood such capabilities for generating attacks.
Some key reasons are that researchers often have different attack goals, such
as misclassification or misdetection, and use different tasks to accomplish
these goals by manipulating different representation in a scene, such as the
mesh or texture of an object. This survey adopts a task-oriented unifying
framework that systematically summarizes common tasks, such as manipulating
textures, altering illumination, and modifying 3D meshes to exploit
vulnerabilities in DNNs. Our framework enables easy comparison of existing
works, reveals research gaps and spotlights exciting future research directions
in this rapidly evolving field. Through focusing on how these tasks enable
attacks on various DNNs such as image classification, facial recognition,
object detection, optical flow and depth estimation, our survey helps
researchers and practitioners better understand the vulnerabilities of computer
vision systems against photorealistic adversarial attacks that could threaten
real-world applications.



---

## MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields  Representation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Peng Wang, Lingzhe Zhao, Yin Zhang, Shiyu Zhao, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2411.08279v1){: .btn .btn-green } |

**Abstract**: Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and
3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in
Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering,
particularly when using high-quality video sequences as input. However,
existing methods struggle with motion-blurred frames, which are common in
real-world scenarios like low-light or long-exposure conditions. This often
results in a significant reduction in both camera localization accuracy and map
reconstruction quality. To address this challenge, we propose a dense visual
SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our
approach integrates an efficient motion blur-aware tracker with either neural
radiance fields or Gaussian Splatting based mapper. By accurately modeling the
physical image formation process of motion-blurred images, our method
simultaneously learns 3D scene representation and estimates the cameras' local
trajectory during exposure time, enabling proactive compensation for motion
blur caused by camera movement. In our experiments, we demonstrate that
MBA-SLAM surpasses previous state-of-the-art methods in both camera
localization and map reconstruction, showcasing superior performance across a
range of datasets, including synthetic and real datasets featuring sharp images
as well as those affected by motion blur, highlighting the versatility and
robustness of our approach. Code is available at
https://github.com/WU-CVGL/MBA-SLAM.



---

## 4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Mijeong Kim, Jongwoo Lim, Bohyung Han | cs.CV | [PDF](http://arxiv.org/pdf/2411.08879v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis of dynamic scenes is becoming important in various
applications, including augmented and virtual reality. We propose a novel 4D
Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded
monocular videos. To overcome the overfitting problem of existing work for
these real-world videos, we introduce an uncertainty-aware regularization that
identifies uncertain regions with few observations and selectively imposes
additional priors based on diffusion models and depth smoothness on such
regions. This approach improves both the performance of novel view synthesis
and the quality of training image reconstruction. We also identify the
initialization problem of 4DGS in fast-moving dynamic regions, where the
Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks.
To initialize Gaussian primitives in such regions, we present a dynamic region
densification method using the estimated depth maps and scene flow. Our
experiments show that the proposed method improves the performance of 4DGS
reconstruction from a video captured by a handheld monocular camera and also
exhibits promising results in few-shot static scene reconstruction.

Comments:
- NeurIPS 2024

---

## DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose  Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Yueming Xu, Haochen Jiang, Zhongyang Xiao, Jianfeng Feng, Li Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2411.08373v1){: .btn .btn-green } |

**Abstract**: Achieving robust and precise pose estimation in dynamic scenes is a
significant research challenge in Visual Simultaneous Localization and Mapping
(SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems
have proven effective in creating high-quality renderings using explicit 3D
Gaussian models, significantly improving environmental reconstruction fidelity.
However, these approaches depend on a static environment assumption and face
challenges in dynamic environments due to inconsistent observations of geometry
and photometry. To address this problem, we propose DG-SLAM, the first robust
dynamic visual SLAM system grounded in 3D Gaussians, which provides precise
camera pose estimation alongside high-fidelity reconstructions. Specifically,
we propose effective strategies, including motion mask generation, adaptive
Gaussian point management, and a hybrid camera tracking algorithm to improve
the accuracy and robustness of pose estimation. Extensive experiments
demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose
estimation, map reconstruction, and novel-view synthesis in dynamic scenes,
outperforming existing methods meanwhile preserving real-time rendering
ability.



---

## BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel  View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2411.08508v1){: .btn .btn-green } |

**Abstract**: We present billboard Splatting (BBSplat) - a novel approach for 3D scene
representation based on textured geometric primitives. BBSplat represents the
scene as a set of optimizable textured planar primitives with learnable RGB
textures and alpha-maps to control their shape. BBSplat primitives can be used
in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our
method's qualitative and quantitative improvements over 3D and 2D Gaussians are
most noticeable when fewer primitives are used, when BBSplat achieves over 1200
FPS. Our novel regularization term encourages textures to have a sparser
structure, unlocking an efficient compression that leads to a reduction in
storage space of the model. Our experiments show the efficiency of BBSplat on
standard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,
and Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics
compared to the state-of-the-art, especially for the case when fewer primitives
are used, which, on the other hand, leads to up to 2 times inference speed
improvement for the same rendering quality.



---

## Towards More Accurate Fake Detection on Images Generated from Advanced  Generative and Neural Rendering Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Chengdong Dong, Vijayakumar Bhagavatula, Zhenyu Zhou, Ajay Kumar | cs.CV | [PDF](http://arxiv.org/pdf/2411.08642v1){: .btn .btn-green } |

**Abstract**: The remarkable progress in neural-network-driven visual data generation,
especially with neural rendering techniques like Neural Radiance Fields and 3D
Gaussian splatting, offers a powerful alternative to GANs and diffusion models.
These methods can produce high-fidelity images and lifelike avatars,
highlighting the need for robust detection methods. In response, an
unsupervised training technique is proposed that enables the model to extract
comprehensive features from the Fourier spectrum magnitude, thereby overcoming
the challenges of reconstructing the spectrum due to its centrosymmetric
properties. By leveraging the spectral domain and dynamically combining it with
spatial domain information, we create a robust multimodal detector that
demonstrates superior generalization capabilities in identifying challenging
synthetic images generated by the latest image synthesis techniques. To address
the absence of a 3D neural rendering-based fake image database, we develop a
comprehensive database that includes images generated by diverse neural
rendering techniques, providing a robust foundation for evaluating and
advancing detection methods.

Comments:
- 13 pages, 8 Figures

---

## Biomass phenotyping of oilseed rape through UAV multi-view oblique  imaging with 3DGS and SAM model

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Yutao Shen, Hongyu Zhou, Xin Yang, Xuqi Lu, Ziyue Guo, Lixi Jiang, Yong He, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2411.08453v1){: .btn .btn-green } |

**Abstract**: Biomass estimation of oilseed rape is crucial for optimizing crop
productivity and breeding strategies. While UAV-based imaging has advanced
high-throughput phenotyping, current methods often rely on orthophoto images,
which struggle with overlapping leaves and incomplete structural information in
complex field environments. This study integrates 3D Gaussian Splatting (3DGS)
with the Segment Anything Model (SAM) for precise 3D reconstruction and biomass
estimation of oilseed rape. UAV multi-view oblique images from 36 angles were
used to perform 3D reconstruction, with the SAM module enhancing point cloud
segmentation. The segmented point clouds were then converted into point cloud
volumes, which were fitted to ground-measured biomass using linear regression.
The results showed that 3DGS (7k and 30k iterations) provided high accuracy,
with peak signal-to-noise ratios (PSNR) of 27.43 and 29.53 and training times
of 7 and 49 minutes, respectively. This performance exceeded that of structure
from motion (SfM) and mipmap Neural Radiance Fields (Mip-NeRF), demonstrating
superior efficiency. The SAM module achieved high segmentation accuracy, with a
mean intersection over union (mIoU) of 0.961 and an F1-score of 0.980.
Additionally, a comparison of biomass extraction models found the point cloud
volume model to be the most accurate, with an determination coefficient (R2) of
0.976, root mean square error (RMSE) of 2.92 g/plant, and mean absolute
percentage error (MAPE) of 6.81%, outperforming both the plot crop volume and
individual crop volume models. This study highlights the potential of combining
3DGS with multi-view UAV imaging for improved biomass phenotyping.



---

## TomoGRAF: A Robust and Generalizable Reconstruction Network for  Single-View Computed Tomography

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Di Xu, Yang Yang, Hengjie Liu, Qihui Lyu, Martina Descovich, Dan Ruan, Ke Sheng | eess.IV | [PDF](http://arxiv.org/pdf/2411.08158v1){: .btn .btn-green } |

**Abstract**: Computed tomography (CT) provides high spatial resolution visualization of 3D
structures for scientific and clinical applications. Traditional
analytical/iterative CT reconstruction algorithms require hundreds of angular
data samplings, a condition that may not be met in practice due to physical and
mechanical limitations. Sparse view CT reconstruction has been proposed using
constrained optimization and machine learning methods with varying success,
less so for ultra-sparse view CT reconstruction with one to two views. Neural
radiance field (NeRF) is a powerful tool for reconstructing and rendering 3D
natural scenes from sparse views, but its direct application to 3D medical
image reconstruction has been minimally successful due to the differences
between optical and X-ray photon transportation. Here, we develop a novel
TomoGRAF framework incorporating the unique X-ray transportation physics to
reconstruct high-quality 3D volumes using ultra-sparse projections without
prior. TomoGRAF captures the CT imaging geometry, simulates the X-ray casting
and tracing process, and penalizes the difference between simulated and ground
truth CT sub-volume during training. We evaluated the performance of TomoGRAF
on an unseen dataset of distinct imaging characteristics from the training data
and demonstrated a vast leap in performance compared with state-of-the-art deep
learning and NeRF methods. TomoGRAF provides the first generalizable solution
for image-guided radiotherapy and interventional radiology applications, where
only one or a few X-ray views are available, but 3D volumetric information is
desired.



---

## Projecting Gaussian Ellipsoids While Avoiding Affine Projection  Approximation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Han Qi, Tao Cai, Xiyue Han | cs.CV | [PDF](http://arxiv.org/pdf/2411.07579v3){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting has dominated novel-view synthesis with its
real-time rendering speed and state-of-the-art rendering quality. However,
during the rendering process, the use of the Jacobian of the affine
approximation of the projection transformation leads to inevitable errors,
resulting in blurriness, artifacts and a lack of scene consistency in the final
rendered images. To address this issue, we introduce an ellipsoid-based
projection method to calculate the projection of Gaussian ellipsoid onto the
image plane, which is the primitive of 3D Gaussian Splatting. As our proposed
ellipsoid-based projection method cannot handle Gaussian ellipsoids with camera
origins inside them or parts lying below $z=0$ plane in the camera space, we
designed a pre-filtering strategy. Experiments over multiple widely adopted
benchmark datasets show that our ellipsoid-based projection method can enhance
the rendering quality of 3D Gaussian Splatting and its extensions.



---

## GUS-IR: Gaussian Splatting with Unified Shading for Inverse Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Zhihao Liang, Hongdong Li, Kui Jia, Kailing Guo, Qi Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.07478v1){: .btn .btn-green } |

**Abstract**: Recovering the intrinsic physical attributes of a scene from images,
generally termed as the inverse rendering problem, has been a central and
challenging task in computer vision and computer graphics. In this paper, we
present GUS-IR, a novel framework designed to address the inverse rendering
problem for complicated scenes featuring rough and glossy surfaces. This paper
starts by analyzing and comparing two prominent shading techniques popularly
used for inverse rendering, forward shading and deferred shading, effectiveness
in handling complex materials. More importantly, we propose a unified shading
solution that combines the advantages of both techniques for better
decomposition. In addition, we analyze the normal modeling in 3D Gaussian
Splatting (3DGS) and utilize the shortest axis as normal for each particle in
GUS-IR, along with a depth-related regularization, resulting in improved
geometric representation and better shape reconstruction. Furthermore, we
enhance the probe-based baking scheme proposed by GS-IR to achieve more
accurate ambient occlusion modeling to better handle indirect illumination.
Extensive experiments have demonstrated the superior performance of GUS-IR in
achieving precise intrinsic decomposition and geometric representation,
supporting many downstream tasks (such as relighting, retouching) in computer
vision, graphics, and extended reality.

Comments:
- 15 pages, 11 figures

---

## Material Transforms from Disentangled NeRF Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Ivan Lopes, Jean-François Lalonde, Raoul de Charette | cs.CV | [PDF](http://arxiv.org/pdf/2411.08037v1){: .btn .btn-green } |

**Abstract**: In this paper, we first propose a novel method for transferring material
transformations across different scenes. Building on disentangled Neural
Radiance Field (NeRF) representations, our approach learns to map Bidirectional
Reflectance Distribution Functions (BRDF) from pairs of scenes observed in
varying conditions, such as dry and wet. The learned transformations can then
be applied to unseen scenes with similar materials, therefore effectively
rendering the transformation learned with an arbitrary level of intensity.
Extensive experiments on synthetic scenes and real-world objects validate the
effectiveness of our approach, showing that it can learn various
transformations such as wetness, painting, coating, etc. Our results highlight
not only the versatility of our method but also its potential for practical
applications in computer graphics. We publish our method implementation, along
with our synthetic/real datasets on
https://github.com/astra-vision/BRDFTransform



---

## HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Qiankun Gao, Jiarui Meng, Chengxiang Wen, Jie Chen, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.07541v1){: .btn .btn-green } |

**Abstract**: The online reconstruction of dynamic scenes from multi-view streaming videos
faces significant challenges in training, rendering and storage efficiency.
Harnessing superior learning speed and real-time rendering capabilities, 3D
Gaussian Splatting (3DGS) has recently demonstrated considerable potential in
this field. However, 3DGS can be inefficient in terms of storage and prone to
overfitting by excessively growing Gaussians, particularly with limited views.
This paper proposes an efficient framework, dubbed HiCoM, with three key
components. First, we construct a compact and robust initial 3DGS
representation using a perturbation smoothing strategy. Next, we introduce a
Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform
distribution and local consistency of 3D Gaussians to swiftly and accurately
learn motions across frames. Finally, we continually refine the 3DGS with
additional Gaussians, which are later merged into the initial 3DGS to maintain
consistency with the evolving scene. To preserve a compact representation, an
equivalent number of low-opacity Gaussians that minimally impact the
representation are removed before processing subsequent frames. Extensive
experiments conducted on two widely used datasets show that our framework
improves learning efficiency of the state-of-the-art methods by about $20\%$
and reduces the data storage by $85\%$, achieving competitive free-viewpoint
video synthesis quality but with higher robustness and stability. Moreover, by
parallel learning multiple frames simultaneously, our HiCoM decreases the
average training wall time to $<2$ seconds per frame with negligible
performance degradation, substantially boosting real-world applicability and
responsiveness.

Comments:
- Accepted to NeurIPS 2024; Code is avaliable at
  https://github.com/gqk/HiCoM

---

## GaussianCut: Interactive segmentation via graph cut for 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Umangi Jain, Ashkan Mirzaei, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2411.07555v1){: .btn .btn-green } |

**Abstract**: We introduce GaussianCut, a new method for interactive multiview segmentation
of scenes represented as 3D Gaussians. Our approach allows for selecting the
objects to be segmented by interacting with a single view. It accepts intuitive
user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian
Splatting (3DGS) as the underlying scene representation simplifies the
extraction of objects of interest which are considered to be a subset of the
scene's Gaussians. Our key idea is to represent the scene as a graph and use
the graph-cut algorithm to minimize an energy function to effectively partition
the Gaussians into foreground and background. To achieve this, we construct a
graph based on scene Gaussians and devise a segmentation-aligned energy
function on the graph to combine user inputs with scene properties. To obtain
an initial coarse segmentation, we leverage 2D image/video segmentation models
and further refine these coarse estimates using our graph construction. Our
empirical evaluations show the adaptability of GaussianCut across a diverse set
of scenes. GaussianCut achieves competitive performance with state-of-the-art
approaches for 3D segmentation without requiring any additional
segmentation-aware training.



---

## LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-11 | Zefan Qu, Ke Xu, Gerhard Petrus Hancke, Rynson W. H. Lau | cs.CV | [PDF](http://arxiv.org/pdf/2411.06757v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have shown remarkable performances in
producing novel-view images from high-quality scene images. However, hand-held
low-light photography challenges NeRFs as the captured images may
simultaneously suffer from low visibility, noise, and camera shakes. While
existing NeRF methods may handle either low light or motion, directly combining
them or incorporating additional image-based enhancement methods does not work
as these degradation factors are highly coupled. We observe that noise in
low-light images is always sharp regardless of camera shakes, which implies an
implicit order of these degradation factors within the image formation process.
To this end, we propose in this paper a novel model, named LuSh-NeRF, which can
reconstruct a clean and sharp NeRF from a group of hand-held low-light images.
The key idea of LuSh-NeRF is to sequentially model noise and blur in the images
via multi-view feature consistency and frequency information of NeRF,
respectively. Specifically, LuSh-NeRF includes a novel Scene-Noise
Decomposition (SND) module for decoupling the noise from the scene
representation and a novel Camera Trajectory Prediction (CTP) module for the
estimation of camera motions based on low-frequency scene information. To
facilitate training and evaluations, we construct a new dataset containing both
synthetic and real images. Experiments show that LuSh-NeRF outperforms existing
approaches. Our code and dataset can be found here:
https://github.com/quzefan/LuSh-NeRF.

Comments:
- Accepted by NeurIPS 2024

---

## A Hierarchical Compression Technique for 3D Gaussian Splatting  Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-11 | He Huang, Wenjie Huang, Qi Yang, Yiling Xu, Zhu li | cs.CV | [PDF](http://arxiv.org/pdf/2411.06976v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (GS) demonstrates excellent rendering quality and
generation speed in novel view synthesis. However, substantial data size poses
challenges for storage and transmission, making 3D GS compression an essential
technology. Current 3D GS compression research primarily focuses on developing
more compact scene representations, such as converting explicit 3D GS data into
implicit forms. In contrast, compression of the GS data itself has hardly been
explored. To address this gap, we propose a Hierarchical GS Compression (HGSC)
technique. Initially, we prune unimportant Gaussians based on importance scores
derived from both global and local significance, effectively reducing
redundancy while maintaining visual quality. An Octree structure is used to
compress 3D positions. Based on the 3D GS Octree, we implement a hierarchical
attribute compression strategy by employing a KD-tree to partition the 3D GS
into multiple blocks. We apply farthest point sampling to select anchor
primitives within each block and others as non-anchor primitives with varying
Levels of Details (LoDs). Anchor primitives serve as reference points for
predicting non-anchor primitives across different LoDs to reduce spatial
redundancy. For anchor primitives, we use the region adaptive hierarchical
transform to achieve near-lossless compression of various attributes. For
non-anchor primitives, each is predicted based on the k-nearest anchor
primitives. To further minimize prediction errors, the reconstructed LoD and
anchor primitives are combined to form new anchor primitives to predict the
next LoD. Our method notably achieves superior compression quality and a
significant data size reduction of over 4.5 times compared to the
state-of-the-art compression method on small scenes datasets.



---

## SplatFormer: Point Transformer for Robust 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-10 | Yutong Chen, Marko Mihajlovic, Xiyi Chen, Yiming Wang, Sergey Prokudin, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2411.06390v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently transformed photorealistic
reconstruction, achieving high visual fidelity and real-time performance.
However, rendering quality significantly deteriorates when test views deviate
from the camera angles used during training, posing a major challenge for
applications in immersive free-viewpoint rendering and navigation. In this
work, we conduct a comprehensive evaluation of 3DGS and related novel view
synthesis methods under out-of-distribution (OOD) test camera scenarios. By
creating diverse test cases with synthetic and real-world datasets, we
demonstrate that most existing methods, including those incorporating various
regularization techniques and data-driven priors, struggle to generalize
effectively to OOD views. To address this limitation, we introduce SplatFormer,
the first point transformer model specifically designed to operate on Gaussian
splats. SplatFormer takes as input an initial 3DGS set optimized under limited
training views and refines it in a single forward pass, effectively removing
potential artifacts in OOD test views. To our knowledge, this is the first
successful application of point transformers directly on 3DGS sets, surpassing
the limitations of previous multi-scene training methods, which could handle
only a restricted number of input views during inference. Our model
significantly improves rendering quality under extreme novel views, achieving
state-of-the-art performance in these challenging scenarios and outperforming
various 3DGS regularization techniques, multi-scene models tailored for sparse
view synthesis, and diffusion-based frameworks.

Comments:
- Code and dataset: https://github.com/ChenYutongTHU/SplatFormer
  Project page: https://sergeyprokudin.github.io/splatformer/

---

## Adaptive and Temporally Consistent Gaussian Surfels for Multi-view  Dynamic Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-10 | Decai Chen, Brianne Oberson, Ingo Feldmann, Oliver Schreer, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2411.06602v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently achieved notable success in novel view
synthesis for dynamic scenes and geometry reconstruction in static scenes.
Building on these advancements, early methods have been developed for dynamic
surface reconstruction by globally optimizing entire sequences. However,
reconstructing dynamic scenes with significant topology changes, emerging or
disappearing objects, and rapid movements remains a substantial challenge,
particularly for long sequences. To address these issues, we propose AT-GS, a
novel method for reconstructing high-quality dynamic surfaces from multi-view
videos through per-frame incremental optimization. To avoid local minima across
frames, we introduce a unified and adaptive gradient-aware densification
strategy that integrates the strengths of conventional cloning and splitting
techniques. Additionally, we reduce temporal jittering in dynamic surfaces by
ensuring consistency in curvature maps across consecutive frames. Our method
achieves superior accuracy and temporal coherence in dynamic surface
reconstruction, delivering high-fidelity space-time novel view synthesis, even
in complex and challenging scenes. Extensive experiments on diverse multi-view
video datasets demonstrate the effectiveness of our approach, showing clear
advantages over baseline methods. Project page:
\url{https://fraunhoferhhi.github.io/AT-GS}



---

## Through the Curved Cover: Synthesizing Cover Aberrated Scenes with  Refractive Field

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-10 | Liuyue Xie, Jiancong Guo, Laszlo A. Jeni, Zhiheng Jia, Mingyang Li, Yunwen Zhou, Chao Guo | cs.CV | [PDF](http://arxiv.org/pdf/2411.06365v1){: .btn .btn-green } |

**Abstract**: Recent extended reality headsets and field robots have adopted covers to
protect the front-facing cameras from environmental hazards and falls. The
surface irregularities on the cover can lead to optical aberrations like
blurring and non-parametric distortions. Novel view synthesis methods like NeRF
and 3D Gaussian Splatting are ill-equipped to synthesize from sequences with
optical aberrations. To address this challenge, we introduce SynthCover to
enable novel view synthesis through protective covers for downstream extended
reality applications. SynthCover employs a Refractive Field that estimates the
cover's geometry, enabling precise analytical calculation of refracted rays.
Experiments on synthetic and real-world scenes demonstrate our method's ability
to accurately model scenes viewed through protective covers, achieving a
significant improvement in rendering quality compared to prior methods. We also
show that the model can adjust well to various cover geometries with synthetic
sequences captured with covers of different surface curvatures. To motivate
further studies on this problem, we provide the benchmarked dataset containing
real and synthetic walkable scenes captured with protective cover optical
aberrations.

Comments:
- WACV 2025

---

## GaussianSpa: An "Optimizing-Sparsifying" Simplification Framework for  Compact and High-Quality 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-09 | Yangming Zhang, Wenqi Jia, Wei Niu, Miao Yin | cs.CV | [PDF](http://arxiv.org/pdf/2411.06019v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view
synthesis, leveraging continuous aggregations of Gaussian functions to model
scene geometry. However, 3DGS suffers from substantial memory requirements to
store the multitude of Gaussians, hindering its practicality. To address this
challenge, we introduce GaussianSpa, an optimization-based simplification
framework for compact and high-quality 3DGS. Specifically, we formulate the
simplification as an optimization problem associated with the 3DGS training.
Correspondingly, we propose an efficient "optimizing-sparsifying" solution that
alternately solves two independent sub-problems, gradually imposing strong
sparsity onto the Gaussians in the training process. Our comprehensive
evaluations on various datasets show the superiority of GaussianSpa over
existing state-of-the-art approaches. Notably, GaussianSpa achieves an average
PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with
10$\times$ fewer Gaussians compared to the vanilla 3DGS. Our project page is
available at https://gaussianspa.github.io/.

Comments:
- Project page at https://gaussianspa.github.io/

---

## AI-Driven Stylization of 3D Environments

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-09 | Yuanbo Chen, Yixiao Kang, Yukun Song, Cyrus Vachha, Sining Huang | cs.CV | [PDF](http://arxiv.org/pdf/2411.06067v1){: .btn .btn-green } |

**Abstract**: In this system, we discuss methods to stylize a scene of 3D primitive objects
into a higher fidelity 3D scene using novel 3D representations like NeRFs and
3D Gaussian Splatting. Our approach leverages existing image stylization
systems and image-to-3D generative models to create a pipeline that iteratively
stylizes and composites 3D objects into scenes. We show our results on adding
generated objects into a scene and discuss limitations.



---

## A Nerf-Based Color Consistency Method for Remote Sensing Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-08 | Zongcheng Zuo, Yuanxiang Li, Tongtong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.05557v1){: .btn .btn-green } |

**Abstract**: Due to different seasons, illumination, and atmospheric conditions, the
photometric of the acquired image varies greatly, which leads to obvious
stitching seams at the edges of the mosaic image. Traditional methods can be
divided into two categories, one is absolute radiation correction and the other
is relative radiation normalization. We propose a NeRF-based method of color
consistency correction for multi-view images, which weaves image features
together using implicit expressions, and then re-illuminates feature space to
generate a fusion image with a new perspective. We chose Superview-1 satellite
images and UAV images with large range and time difference for the experiment.
Experimental results show that the synthesize image generated by our method has
excellent visual effect and smooth color transition at the edges.

Comments:
- 4 pages, 4 figures, The International Geoscience and Remote Sensing
  Symposium (IGARSS2023)

---

## Rate-aware Compression for NeRF-based Volumetric Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-08 | Zhiyu Zhang, Guo Lu, Huanxiong Liang, Zhengxue Cheng, Anni Tang, Li Song | cs.MM | [PDF](http://arxiv.org/pdf/2411.05322v1){: .btn .btn-green } |

**Abstract**: The neural radiance fields (NeRF) have advanced the development of 3D
volumetric video technology, but the large data volumes they involve pose
significant challenges for storage and transmission. To address these problems,
the existing solutions typically compress these NeRF representations after the
training stage, leading to a separation between representation training and
compression. In this paper, we try to directly learn a compact NeRF
representation for volumetric video in the training stage based on the proposed
rate-aware compression framework. Specifically, for volumetric video, we use a
simple yet effective modeling strategy to reduce temporal redundancy for the
NeRF representation. Then, during the training phase, an implicit entropy model
is utilized to estimate the bitrate of the NeRF representation. This entropy
model is then encoded into the bitstream to assist in the decoding of the NeRF
representation. This approach enables precise bitrate estimation, thereby
leading to a compact NeRF representation. Furthermore, we propose an adaptive
quantization strategy and learn the optimal quantization step for the NeRF
representations. Finally, the NeRF representation can be optimized by using the
rate-distortion trade-off. Our proposed compression framework can be used for
different representations and experimental results demonstrate that our
approach significantly reduces the storage size with marginal distortion and
achieves state-of-the-art rate-distortion performance for volumetric video on
the HumanRF and ReRF datasets. Compared to the previous state-of-the-art method
TeTriRF, we achieved an approximately -80% BD-rate on the HumanRF dataset and
-60% BD-rate on the ReRF dataset.

Comments:
- Accepted by ACM MM 2024 (Oral)

---

## GANESH: Generalizable NeRF for Lensless Imaging

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Rakesh Raj Madavan, Akshat Kaimal, Badhrinarayanan K V, Vinayak Gupta, Rohit Choudhary, Chandrakala Shanmuganathan, Kaushik Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2411.04810v1){: .btn .btn-green } |

**Abstract**: Lensless imaging offers a significant opportunity to develop ultra-compact
cameras by removing the conventional bulky lens system. However, without a
focusing element, the sensor's output is no longer a direct image but a complex
multiplexed scene representation. Traditional methods have attempted to address
this challenge by employing learnable inversions and refinement models, but
these methods are primarily designed for 2D reconstruction and do not
generalize well to 3D reconstruction. We introduce GANESH, a novel framework
designed to enable simultaneous refinement and novel view synthesis from
multi-view lensless images. Unlike existing methods that require scene-specific
training, our approach supports on-the-fly inference without retraining on each
scene. Moreover, our framework allows us to tune our model to specific scenes,
enhancing the rendering and refinement quality. To facilitate research in this
area, we also present the first multi-view lensless dataset, LenslessScenes.
Extensive experiments demonstrate that our method outperforms current
approaches in reconstruction accuracy and refinement quality. Code and video
results are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/



---

## MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2411.04924v1){: .btn .btn-green } |

**Abstract**: We introduce MVSplat360, a feed-forward approach for 360{\deg} novel view
synthesis (NVS) of diverse real-world scenes, using only sparse observations.
This setting is inherently ill-posed due to minimal overlap among input views
and insufficient visual information provided, making it challenging for
conventional methods to achieve high-quality results. Our MVSplat360 addresses
this by effectively combining geometry-aware 3D reconstruction with temporally
consistent video generation. Specifically, it refactors a feed-forward 3D
Gaussian Splatting (3DGS) model to render features directly into the latent
space of a pre-trained Stable Video Diffusion (SVD) model, where these features
then act as pose and visual cues to guide the denoising process and produce
photorealistic 3D-consistent views. Our model is end-to-end trainable and
supports rendering arbitrary views with as few as 5 sparse input views. To
evaluate MVSplat360's performance, we introduce a new benchmark using the
challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual
quality compared to state-of-the-art methods on wide-sweeping or even 360{\deg}
NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the
effectiveness of our model. The video results are available on our project
page: https://donydchen.github.io/mvsplat360.

Comments:
- NeurIPS 2024, Project page: https://donydchen.github.io/mvsplat360,
  Code: https://github.com/donydchen/mvsplat360

---

## SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on Larger  Objects for Mobile-Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Xun Tu, Karthik Desingh | cs.RO | [PDF](http://arxiv.org/pdf/2411.04386v2){: .btn .btn-green } |

**Abstract**: Grasp planning and estimation have been a longstanding research problem in
robotics, with two main approaches to find graspable poses on the objects: 1)
geometric approach, which relies on 3D models of objects and the gripper to
estimate valid grasp poses, and 2) data-driven, learning-based approach, with
models trained to identify grasp poses from raw sensor observations. The latter
assumes comprehensive geometric coverage during the training phase. However,
the data-driven approach is typically biased toward tabletop scenarios and
struggle to generalize to out-of-distribution scenarios with larger objects
(e.g. chair). Additionally, raw sensor data (e.g. RGB-D data) from a single
view of these larger objects is often incomplete and necessitates additional
observations. In this paper, we take a geometric approach, leveraging
advancements in object modeling (e.g. NeRF) to build an implicit model by
taking RGB images from views around the target object. This model enables the
extraction of explicit mesh model while also capturing the visual appearance
from novel viewpoints that is useful for perception tasks like object detection
and pose estimation. We further decompose the NeRF-reconstructed 3D mesh into
superquadrics (SQs) -- parametric geometric primitives, each mapped to a set of
precomputed grasp poses, allowing grasp composition on the target object based
on these primitives. Our proposed pipeline overcomes the problems: a) noisy
depth and incomplete view of the object, with a modeling step, and b)
generalization to objects of any size. For more qualitative results, refer to
the supplementary video and webpage https://bit.ly/3ZrOanU

Comments:
- 8 pages, 7 figures, submitted to ICRA 2025 for review

---

## ProEdit: Simple Progression is All You Need for High-Quality 3D Scene  Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Jun-Kun Chen, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.05006v1){: .btn .btn-green } |

**Abstract**: This paper proposes ProEdit - a simple yet effective framework for
high-quality 3D scene editing guided by diffusion distillation in a novel
progressive manner. Inspired by the crucial observation that multi-view
inconsistency in scene editing is rooted in the diffusion model's large
feasible output space (FOS), our framework controls the size of FOS and reduces
inconsistency by decomposing the overall editing task into several subtasks,
which are then executed progressively on the scene. Within this framework, we
design a difficulty-aware subtask decomposition scheduler and an adaptive 3D
Gaussian splatting (3DGS) training strategy, ensuring high quality and
efficiency in performing each subtask. Extensive evaluation shows that our
ProEdit achieves state-of-the-art results in various scenes and challenging
editing tasks, all through a simple framework without any expensive or
sophisticated add-ons like distillation losses, components, or training
procedures. Notably, ProEdit also provides a new way to control, preview, and
select the "aggressivity" of editing operation during the editing process.

Comments:
- NeurIPS 2024. Project Page: https://immortalco.github.io/ProEdit/

---

## Planar Reflection-Aware Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Chen Gao, Yipeng Wang, Changil Kim, Jia-Bin Huang, Johannes Kopf | cs.CV | [PDF](http://arxiv.org/pdf/2411.04984v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in
reconstructing complex scenes with high fidelity. However, NeRF's view
dependency can only handle low-frequency reflections. It falls short when
handling complex planar reflections, often interpreting them as erroneous scene
geometries and leading to duplicated and inaccurate scene representations. To
address this challenge, we introduce a reflection-aware NeRF that jointly
models planar reflectors, such as windows, and explicitly casts reflected rays
to capture the source of the high-frequency reflections. We query a single
radiance field to render the primary color and the source of the reflection. We
propose a sparse edge regularization to help utilize the true sources of
reflections for rendering planar reflections rather than creating a duplicate
along the primary ray at the same depth. As a result, we obtain accurate scene
geometry. Rendering along the primary ray results in a clean, reflection-free
view, while explicitly rendering along the reflected ray allows us to
reconstruct highly detailed reflections. Our extensive quantitative and
qualitative evaluations of real-world datasets demonstrate our method's
enhanced performance in accurately handling reflections.



---

## Structure Consistent Gaussian Splatting with Matching Prior for Few-shot  Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Rui Peng, Wangze Xu, Luyang Tang, Liwei Liao, Jianbo Jiao, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.03637v1){: .btn .btn-green } |

**Abstract**: Despite the substantial progress of novel view synthesis, existing methods,
either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian
Splatting (3DGS), suffer significant degradation when the input becomes sparse.
Numerous efforts have been introduced to alleviate this problem, but they still
struggle to synthesize satisfactory results efficiently, especially in the
large scene. In this paper, we propose SCGaussian, a Structure Consistent
Gaussian Splatting method using matching priors to learn 3D consistent scene
structure. Considering the high interdependence of Gaussian attributes, we
optimize the scene structure in two folds: rendering geometry and, more
importantly, the position of Gaussian primitives, which is hard to be directly
constrained in the vanilla 3DGS due to the non-structure property. To achieve
this, we present a hybrid Gaussian representation. Besides the ordinary
non-structure Gaussian primitives, our model also consists of ray-based
Gaussian primitives that are bound to matching rays and whose optimization of
their positions is restricted along the ray. Thus, we can utilize the matching
correspondence to directly enforce the position of these Gaussian primitives to
converge to the surface points where rays intersect. Extensive experiments on
forward-facing, surrounding, and complex large scenes show the effectiveness of
our approach with state-of-the-art performance and high efficiency. Code is
available at https://github.com/prstrive/SCGaussian.

Comments:
- NeurIPS 2024 Accepted

---

## GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Jilan Mei, Junbo Li, Cai Meng | cs.CV | [PDF](http://arxiv.org/pdf/2411.03807v3){: .btn .btn-green } |

**Abstract**: This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.



---

## 3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical  Object Rearrangement

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Ziqi Lu, Jianbo Ye, John Leonard | cs.CV | [PDF](http://arxiv.org/pdf/2411.03706v1){: .btn .btn-green } |

**Abstract**: We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for
detecting physical object rearrangements in 3D scenes. Our approach estimates
3D object-level changes by comparing two sets of unaligned images taken at
different times. Leveraging 3DGS's novel view rendering and EfficientSAM's
zero-shot segmentation capabilities, we detect 2D object-level changes, which
are then associated and fused across views to estimate 3D changes. Our method
can detect changes in cluttered environments using sparse post-change images
within as little as 18s, using as few as a single new image. It does not rely
on depth input, user instructions, object classes, or object models -- An
object is recognized simply if it has been re-arranged. Our approach is
evaluated on both public and self-collected real-world datasets, achieving up
to 14% higher accuracy and three orders of magnitude faster performance
compared to the state-of-the-art radiance-field-based change detection method.
This significant performance boost enables a broad range of downstream
applications, where we highlight three key use cases: object reconstruction,
robot workspace reset, and 3DGS model update. Our code and data will be made
available at https://github.com/520xyxyzq/3DGS-CD.



---

## Enhancing Exploratory Capability of Visual Navigation Using Uncertainty  of Implicit Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Yichen Wang, Qiming Liu, Zhe Liu, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2411.03487v1){: .btn .btn-green } |

**Abstract**: In the context of visual navigation in unknown scenes, both "exploration" and
"exploitation" are equally crucial. Robots must first establish environmental
cognition through exploration and then utilize the cognitive information to
accomplish target searches. However, most existing methods for image-goal
navigation prioritize target search over the generation of exploratory
behavior. To address this, we propose the Navigation with Uncertainty-driven
Exploration (NUE) pipeline, which uses an implicit and compact scene
representation, NeRF, as a cognitive structure. We estimate the uncertainty of
NeRF and augment the exploratory ability by the uncertainty to in turn
facilitate the construction of implicit representation. Simultaneously, we
extract memory information from NeRF to enhance the robot's reasoning ability
for determining the location of the target. Ultimately, we seamlessly combine
the two generated abilities to produce navigational actions. Our pipeline is
end-to-end, with the environmental cognitive structure being constructed
online. Extensive experimental results on image-goal navigation demonstrate the
capability of our pipeline to enhance exploratory behaviors, while also
enabling a natural transition from the exploration to exploitation phase. This
enables our model to outperform existing memory-based cognitive navigation
structures in terms of navigation performance.



---

## Object and Contact Point Tracking in Demonstrations Using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Michael Büttner, Jonathan Francis, Helge Rhodin, Andrew Melnik | cs.CV | [PDF](http://arxiv.org/pdf/2411.03555v1){: .btn .btn-green } |

**Abstract**: This paper introduces a method to enhance Interactive Imitation Learning
(IIL) by extracting touch interaction points and tracking object movement from
video demonstrations. The approach extends current IIL systems by providing
robots with detailed knowledge of both where and how to interact with objects,
particularly complex articulated ones like doors and drawers. By leveraging
cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for
tracking, this method allows robots to better understand and manipulate objects
in dynamic environments. The research lays the foundation for more effective
task learning and execution in autonomous robotic systems.

Comments:
- CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich,
  Germany

---

## Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Xavier Timoneda, Markus Herb, Fabian Duerr, Daniel Goehring, Fisher Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.02969v1){: .btn .btn-green } |

**Abstract**: LiDAR Semantic Segmentation is a fundamental task in autonomous driving
perception consisting of associating each LiDAR point to a semantic label.
Fully-supervised models have widely tackled this task, but they require labels
for each scan, which either limits their domain or requires impractical amounts
of expensive annotations. Camera images, which are generally recorded alongside
LiDAR pointclouds, can be processed by the widely available 2D foundation
models, which are generic and dataset-agnostic. However, distilling knowledge
from 2D data to improve LiDAR perception raises domain adaptation challenges.
For example, the classical perspective projection suffers from the parallax
effect produced by the position shift between both sensors at their respective
capture times. We propose a Semi-Supervised Learning setup to leverage
unlabeled LiDAR pointclouds alongside distilled knowledge from the camera
images. To self-supervise our model on the unlabeled scans, we add an auxiliary
NeRF head and cast rays from the camera viewpoint over the unlabeled voxel
features. The NeRF head predicts densities and semantic logits at each sampled
ray location which are used for rendering pixel semantics. Concurrently, we
query the Segment-Anything (SAM) foundation model with the camera image to
generate a set of unlabeled generic masks. We fuse the masks with the rendered
pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel
predictions. During inference, we drop the NeRF head and run our model with
only LiDAR. We show the effectiveness of our approach in three public LiDAR
Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.

Comments:
- IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS) 2024

---

## CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model  Retrieval

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Xin Wen, Xuening Zhu, Renjiao Yi, Zhifeng Wang, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.02979v1){: .btn .btn-green } |

**Abstract**: Reconstructing from multi-view images is a longstanding problem in 3D vision,
where neural radiance fields (NeRFs) have shown great potential and get
realistic rendered images of novel views. Currently, most NeRF methods either
require accurate camera poses or a large number of input images, or even both.
Reconstructing NeRF from few-view images without poses is challenging and
highly ill-posed. To address this problem, we propose CAD-NeRF, a method
reconstructed from less than 10 images without any known poses. Specifically,
we build a mini library of several CAD models from ShapeNet and render them
from many random views. Given sparse-view input images, we run a model and pose
retrieval from the library, to get a model with similar shapes, serving as the
density supervision and pose initializations. Here we propose a multi-view pose
retrieval method to avoid pose conflicts among views, which is a new and unseen
problem in uncalibrated NeRF methods. Then, the geometry of the object is
trained by the CAD guidance. The deformation of the density field and camera
poses are optimized jointly. Then texture and density are trained and
fine-tuned as well. All training phases are in self-supervised manners.
Comprehensive evaluations of synthetic and real images show that CAD-NeRF
successfully learns accurate densities with a large deformation from retrieved
CAD models, showing the generalization abilities.

Comments:
- The article has been accepted by Frontiers of Computer Science (FCS)

---

## Exploring Seasonal Variability in the Context of Neural Radiance Fields  for 3D Reconstruction on Satellite Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Liv Kåreborn, Erica Ingerstad, Amanda Berg, Justus Karlsson, Leif Haglund | cs.CV | [PDF](http://arxiv.org/pdf/2411.02972v1){: .btn .btn-green } |

**Abstract**: In this work, the seasonal predictive capabilities of Neural Radiance Fields
(NeRF) applied to satellite images are investigated. Focusing on the
utilization of satellite data, the study explores how Sat-NeRF, a novel
approach in computer vision, performs in predicting seasonal variations across
different months. Through comprehensive analysis and visualization, the study
examines the model's ability to capture and predict seasonal changes,
highlighting specific challenges and strengths. Results showcase the impact of
the sun direction on predictions, revealing nuanced details in seasonal
transitions, such as snow cover, color accuracy, and texture representation in
different landscapes. Given these results, we propose Planet-NeRF, an extension
to Sat-NeRF capable of incorporating seasonal variability through a set of
month embedding vectors. Comparative evaluations reveal that Planet-NeRF
outperforms prior models in the case where seasonal changes are present. The
extensive evaluation combined with the proposed method offers promising avenues
for future research in this domain.



---

## LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Huibin Zhao, Weipeng Guan, Peng Lu | cs.RO | [PDF](http://arxiv.org/pdf/2411.02703v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown its ability in rapid rendering and
high-fidelity mapping. In this paper, we introduce LVI-GS, a tightly-coupled
LiDAR-Visual-Inertial mapping framework with 3DGS, which leverages the
complementary characteristics of LiDAR and image sensors to capture both
geometric structures and visual details of 3D scenes. To this end, the 3D
Gaussians are initialized from colourized LiDAR points and optimized using
differentiable rendering. In order to achieve high-fidelity mapping, we
introduce a pyramid-based training approach to effectively learn multi-level
features and incorporate depth loss derived from LiDAR measurements to improve
geometric feature perception. Through well-designed strategies for Gaussian-Map
expansion, keyframe selection, thread management, and custom CUDA acceleration,
our framework achieves real-time photo-realistic mapping. Numerical experiments
are performed to evaluate the superior performance of our method compared to
state-of-the-art 3D reconstruction systems.



---

## HFGaussian: Learning Generalizable Gaussian Human with Integrated Human  Features

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Arnab Dey, Cheng-You Lu, Andrew I. Comport, Srinath Sridhar, Chin-Teng Lin, Jean Martinet | cs.CV | [PDF](http://arxiv.org/pdf/2411.03086v1){: .btn .btn-green } |

**Abstract**: Recent advancements in radiance field rendering show promising results in 3D
scene representation, where Gaussian splatting-based techniques emerge as
state-of-the-art due to their quality and efficiency. Gaussian splatting is
widely used for various applications, including 3D human representation.
However, previous 3D Gaussian splatting methods either use parametric body
models as additional information or fail to provide any underlying structure,
like human biomechanical features, which are essential for different
applications. In this paper, we present a novel approach called HFGaussian that
can estimate novel views and human features, such as the 3D skeleton, 3D key
points, and dense pose, from sparse input images in real time at 25 FPS. The
proposed method leverages generalizable Gaussian splatting technique to
represent the human subject and its associated features, enabling efficient and
generalizable reconstruction. By incorporating a pose regression network and
the feature splatting technique with Gaussian splatting, HFGaussian
demonstrates improved capabilities over existing 3D human methods, showcasing
the potential of 3D human representations with integrated biomechanics. We
thoroughly evaluate our HFGaussian method against the latest state-of-the-art
techniques in human Gaussian splatting and pose estimation, demonstrating its
real-time, state-of-the-art performance.



---

## GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface  Reconstruction in Open Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Gaochao Song, Chong Cheng, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.01853v2){: .btn .btn-green } |

**Abstract**: In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.

Comments:
- NeurIPS 2024

---

## A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Matthew McDermott, Jason Rife | cs.CV | [PDF](http://arxiv.org/pdf/2411.01725v1){: .btn .btn-green } |

**Abstract**: In this paper we reexamine the process through which a Neural Radiance Field
(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image
applications where camera pixels integrate light over time, LiDAR pulses arrive
at specific times. As such, multiple LiDAR returns are possible for any given
detector and the classification of these returns is inherently probabilistic.
Applying a traditional NeRF training routine can result in the network learning
phantom surfaces in free space between conflicting range measurements, similar
to how floater aberrations may be produced by an image model. We show that by
formulating loss as an integral of probability (rather than as an integral of
optical density) the network can learn multiple peaks for a given ray, allowing
the sampling of first, nth, or strongest returns from a single output channel.
Code is available at https://github.com/mcdermatt/PLINK



---

## FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage  Training

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers | cs.CV | [PDF](http://arxiv.org/pdf/2411.02229v2){: .btn .btn-green } |

**Abstract**: The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.

Comments:
- Accepted by NeurIPS2024

---

## Modeling Uncertainty in 3D Gaussian Splatting through Continuous  Semantic Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Joey Wilson, Marcelino Almeida, Min Sun, Sachit Mahajan, Maani Ghaffari, Parker Ewen, Omid Ghasemalizadeh, Cheng-Hao Kuo, Arnie Sen | cs.RO | [PDF](http://arxiv.org/pdf/2411.02547v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel algorithm for probabilistically updating
and rasterizing semantic maps within 3D Gaussian Splatting (3D-GS). Although
previous methods have introduced algorithms which learn to rasterize features
in 3D-GS for enhanced scene understanding, 3D-GS can fail without warning which
presents a challenge for safety-critical robotic applications. To address this
gap, we propose a method which advances the literature of continuous semantic
mapping from voxels to ellipsoids, combining the precise structure of 3D-GS
with the ability to quantify uncertainty of probabilistic robotic maps. Given a
set of images, our algorithm performs a probabilistic semantic update directly
on the 3D ellipsoids to obtain an expectation and variance through the use of
conjugate priors. We also propose a probabilistic rasterization which returns
per-pixel segmentation predictions with quantifiable uncertainty. We compare
our method with similar probabilistic voxel-based methods to verify our
extension to 3D ellipsoids, and perform ablation studies on uncertainty
quantification and temporal smoothing.



---

## NeRF-Aug: Data Augmentation for Robotics with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Eric Zhu, Mara Levy, Matthew Gwilliam, Abhinav Shrivastava | cs.RO | [PDF](http://arxiv.org/pdf/2411.02482v1){: .btn .btn-green } |

**Abstract**: Training a policy that can generalize to unknown objects is a long standing
challenge within the field of robotics. The performance of a policy often drops
significantly in situations where an object in the scene was not seen during
training. To solve this problem, we present NeRF-Aug, a novel method that is
capable of teaching a policy to interact with objects that are not present in
the dataset. This approach differs from existing approaches by leveraging the
speed and photorealism of a neural radiance field for augmentation. NeRF- Aug
both creates more photorealistic data and runs 3.83 times faster than existing
methods. We demonstrate the effectiveness of our method on 4 tasks with 11
novel objects that have no expert demonstration data. We achieve an average
69.1% success rate increase over existing methods. See video results at
https://nerf-aug.github.io.



---

## SplatOverflow: Asynchronous Hardware Troubleshooting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Amritansh Kwatra, Tobias Wienberg, Ilan Mandel, Ritik Batra, Peter He, Francois Guimbretiere, Thijs Roumen | cs.HC | [PDF](http://arxiv.org/pdf/2411.02332v2){: .btn .btn-green } |

**Abstract**: As tools for designing and manufacturing hardware become more accessible,
smaller producers can develop and distribute novel hardware. However, there
aren't established tools to support end-user hardware troubleshooting or
routine maintenance. As a result, technical support for hardware remains ad-hoc
and challenging to scale. Inspired by software troubleshooting workflows like
StackOverflow, we propose a workflow for asynchronous hardware troubleshooting:
SplatOverflow. SplatOverflow creates a novel boundary object, the SplatOverflow
scene, that users reference to communicate about hardware. The scene comprises
a 3D Gaussian Splat of the user's hardware registered onto the hardware's CAD
model. The splat captures the current state of the hardware, and the registered
CAD model acts as a referential anchor for troubleshooting instructions. With
SplatOverflow, maintainers can directly address issues and author instructions
in the user's workspace. The instructions define workflows that can easily be
shared between users and recontextualized in new environments. In this paper,
we describe the design of SplatOverflow, detail the workflows it enables, and
illustrate its utility to different kinds of users. We also validate that
non-experts can use SplatOverflow to troubleshoot common problems with a 3D
printer in a user study.

Comments:
- Our accompanying video figure is available at:
  https://youtu.be/m4TKeBDuZkU

---

## Real-Time Spatio-Temporal Reconstruction of Dynamic Endoscopic Scenes  with 4D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-02 | Fengze Li, Jishuai He, Jieming Ma, Zhijing Wu | cs.CV | [PDF](http://arxiv.org/pdf/2411.01218v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction is essential in robotic minimally invasive
surgery, providing crucial spatial information that enhances surgical precision
and outcomes. However, existing methods struggle to address the complex,
temporally dynamic nature of endoscopic scenes. This paper presents
ST-Endo4DGS, a novel framework that models the spatio-temporal volume of
dynamic endoscopic scenes using unbiased 4D Gaussian Splatting (4DGS)
primitives, parameterized by anisotropic ellipses with flexible 4D rotations.
This approach enables precise representation of deformable tissue dynamics,
capturing intricate spatial and temporal correlations in real time.
Additionally, we extend spherindrical harmonics to represent time-evolving
appearance, achieving realistic adaptations to lighting and view changes. A new
endoscopic normal alignment constraint (ENAC) further enhances geometric
fidelity by aligning rendered normals with depth-derived geometry. Extensive
evaluations show that ST-Endo4DGS outperforms existing methods in both visual
quality and real-time performance, establishing a new state-of-the-art in
dynamic scene reconstruction for endoscopic surgery.



---

## PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud  Understanding


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Jincen Jiang, Qianyu Zhou, Yuhang Li, Xinkui Zhao, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang, Xuequan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00632v1){: .btn .btn-green } |

**Abstract**: In this paper, we present PCoTTA, an innovative, pioneering framework for
Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding,
enhancing the model's transferability towards the continually changing target
domain. We introduce a multi-task setting for PCoTTA, which is practical and
realistic, handling multiple tasks within one unified model during the
continual adaptation. Our PCoTTA involves three key components: automatic
prototype mixture (APM), Gaussian Splatted feature shifting (GSFS), and
contrastive prototype repulsion (CPR). Firstly, APM is designed to
automatically mix the source prototypes with the learnable prototypes with a
similarity balancing factor, avoiding catastrophic forgetting. Then, GSFS
dynamically shifts the testing sample toward the source domain, mitigating
error accumulation in an online manner. In addition, CPR is proposed to pull
the nearest learnable prototype close to the testing feature and push it away
from other prototypes, making each prototype distinguishable during the
adaptation. Experimental comparisons lead to a new benchmark, demonstrating
PCoTTA's superiority in boosting the model's transferability towards the
continually changing target domain.

Comments:
- Accepted to NeurIPS 2024

---

## CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for  Large-Scale Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.00771v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field
reconstruction, manifesting efficient and high-fidelity novel view synthesis.
However, accurately representing surfaces, especially in large and complex
scenarios, remains a significant challenge due to the unstructured nature of
3DGS. In this paper, we present CityGaussianV2, a novel approach for
large-scale scene reconstruction that addresses critical challenges related to
geometric accuracy and efficiency. Building on the favorable generalization
capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and
scalability issues. Specifically, we implement a decomposed-gradient-based
densification and depth regression technique to eliminate blurry artifacts and
accelerate convergence. To scale up, we introduce an elongation filter that
mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we
optimize the CityGaussian pipeline for parallel training, achieving up to
10$\times$ compression, at least 25% savings in training time, and a 50%
decrease in memory usage. We also established standard geometry benchmarks
under large-scale scenes. Experimental results demonstrate that our method
strikes a promising balance between visual quality, geometric accuracy, as well
as storage and training costs. The project page is available at
https://dekuliutesla.github.io/CityGaussianV2/.

Comments:
- Project Page: https://dekuliutesla.github.io/CityGaussianV2/

---

## ZIM: Zero-Shot Image Matting for Anything

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Beomyoung Kim, Chanyong Shin, Joonhyun Jeong, Hyungsik Jung, Se-Yun Lee, Sewhan Chun, Dong-Hyun Hwang, Joonsang Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00626v1){: .btn .btn-green } |

**Abstract**: The recent segmentation foundation model, Segment Anything Model (SAM),
exhibits strong zero-shot segmentation capabilities, but it falls short in
generating fine-grained precise masks. To address this limitation, we propose a
novel zero-shot image matting model, called ZIM, with two key contributions:
First, we develop a label converter that transforms segmentation labels into
detailed matte labels, constructing the new SA1B-Matte dataset without costly
manual annotations. Training SAM with this dataset enables it to generate
precise matte masks while maintaining its zero-shot capability. Second, we
design the zero-shot matting model equipped with a hierarchical pixel decoder
to enhance mask representation, along with a prompt-aware masked attention
mechanism to improve performance by enabling the model to focus on regions
specified by visual prompts. We evaluate ZIM using the newly introduced
MicroMat-3K test set, which contains high-quality micro-level matte labels.
Experimental results show that ZIM outperforms existing methods in fine-grained
mask generation and zero-shot generalization. Furthermore, we demonstrate the
versatility of ZIM in various downstream tasks requiring precise masks, such as
image inpainting and 3D NeRF. Our contributions provide a robust foundation for
advancing zero-shot matting and its downstream applications across a wide range
of computer vision tasks. The code is available at
\url{https://github.com/naver-ai/ZIM}.

Comments:
- preprint (21 pages, 16 figures, and 8 tables)
