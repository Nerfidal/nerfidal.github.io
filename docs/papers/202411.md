---
layout: default
title: November 2024
parent: Papers
nav_order: 202411
---

<!---metadata--->


## 4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Mijeong Kim, Jongwoo Lim, Bohyung Han | cs.CV | [PDF](http://arxiv.org/pdf/2411.08879v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis of dynamic scenes is becoming important in various
applications, including augmented and virtual reality. We propose a novel 4D
Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded
monocular videos. To overcome the overfitting problem of existing work for
these real-world videos, we introduce an uncertainty-aware regularization that
identifies uncertain regions with few observations and selectively imposes
additional priors based on diffusion models and depth smoothness on such
regions. This approach improves both the performance of novel view synthesis
and the quality of training image reconstruction. We also identify the
initialization problem of 4DGS in fast-moving dynamic regions, where the
Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks.
To initialize Gaussian primitives in such regions, we present a dynamic region
densification method using the estimated depth maps and scene flow. Our
experiments show that the proposed method improves the performance of 4DGS
reconstruction from a video captured by a handheld monocular camera and also
exhibits promising results in few-shot static scene reconstruction.

Comments:
- NeurIPS 2024

---

## DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose  Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Yueming Xu, Haochen Jiang, Zhongyang Xiao, Jianfeng Feng, Li Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2411.08373v1){: .btn .btn-green } |

**Abstract**: Achieving robust and precise pose estimation in dynamic scenes is a
significant research challenge in Visual Simultaneous Localization and Mapping
(SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems
have proven effective in creating high-quality renderings using explicit 3D
Gaussian models, significantly improving environmental reconstruction fidelity.
However, these approaches depend on a static environment assumption and face
challenges in dynamic environments due to inconsistent observations of geometry
and photometry. To address this problem, we propose DG-SLAM, the first robust
dynamic visual SLAM system grounded in 3D Gaussians, which provides precise
camera pose estimation alongside high-fidelity reconstructions. Specifically,
we propose effective strategies, including motion mask generation, adaptive
Gaussian point management, and a hybrid camera tracking algorithm to improve
the accuracy and robustness of pose estimation. Extensive experiments
demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose
estimation, map reconstruction, and novel-view synthesis in dynamic scenes,
outperforming existing methods meanwhile preserving real-time rendering
ability.



---

## MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields  Representation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Peng Wang, Lingzhe Zhao, Yin Zhang, Shiyu Zhao, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2411.08279v1){: .btn .btn-green } |

**Abstract**: Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and
3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in
Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering,
particularly when using high-quality video sequences as input. However,
existing methods struggle with motion-blurred frames, which are common in
real-world scenarios like low-light or long-exposure conditions. This often
results in a significant reduction in both camera localization accuracy and map
reconstruction quality. To address this challenge, we propose a dense visual
SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our
approach integrates an efficient motion blur-aware tracker with either neural
radiance fields or Gaussian Splatting based mapper. By accurately modeling the
physical image formation process of motion-blurred images, our method
simultaneously learns 3D scene representation and estimates the cameras' local
trajectory during exposure time, enabling proactive compensation for motion
blur caused by camera movement. In our experiments, we demonstrate that
MBA-SLAM surpasses previous state-of-the-art methods in both camera
localization and map reconstruction, showcasing superior performance across a
range of datasets, including synthetic and real datasets featuring sharp images
as well as those affected by motion blur, highlighting the versatility and
robustness of our approach. Code is available at
https://github.com/WU-CVGL/MBA-SLAM.



---

## Towards More Accurate Fake Detection on Images Generated from Advanced  Generative and Neural Rendering Models

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Chengdong Dong, Vijayakumar Bhagavatula, Zhenyu Zhou, Ajay Kumar | cs.CV | [PDF](http://arxiv.org/pdf/2411.08642v1){: .btn .btn-green } |

**Abstract**: The remarkable progress in neural-network-driven visual data generation,
especially with neural rendering techniques like Neural Radiance Fields and 3D
Gaussian splatting, offers a powerful alternative to GANs and diffusion models.
These methods can produce high-fidelity images and lifelike avatars,
highlighting the need for robust detection methods. In response, an
unsupervised training technique is proposed that enables the model to extract
comprehensive features from the Fourier spectrum magnitude, thereby overcoming
the challenges of reconstructing the spectrum due to its centrosymmetric
properties. By leveraging the spectral domain and dynamically combining it with
spatial domain information, we create a robust multimodal detector that
demonstrates superior generalization capabilities in identifying challenging
synthetic images generated by the latest image synthesis techniques. To address
the absence of a 3D neural rendering-based fake image database, we develop a
comprehensive database that includes images generated by diverse neural
rendering techniques, providing a robust foundation for evaluating and
advancing detection methods.

Comments:
- 13 pages, 8 Figures

---

## Biomass phenotyping of oilseed rape through UAV multi-view oblique  imaging with 3DGS and SAM model

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | Yutao Shen, Hongyu Zhou, Xin Yang, Xuqi Lu, Ziyue Guo, Lixi Jiang, Yong He, Haiyan Cen | cs.CV | [PDF](http://arxiv.org/pdf/2411.08453v1){: .btn .btn-green } |

**Abstract**: Biomass estimation of oilseed rape is crucial for optimizing crop
productivity and breeding strategies. While UAV-based imaging has advanced
high-throughput phenotyping, current methods often rely on orthophoto images,
which struggle with overlapping leaves and incomplete structural information in
complex field environments. This study integrates 3D Gaussian Splatting (3DGS)
with the Segment Anything Model (SAM) for precise 3D reconstruction and biomass
estimation of oilseed rape. UAV multi-view oblique images from 36 angles were
used to perform 3D reconstruction, with the SAM module enhancing point cloud
segmentation. The segmented point clouds were then converted into point cloud
volumes, which were fitted to ground-measured biomass using linear regression.
The results showed that 3DGS (7k and 30k iterations) provided high accuracy,
with peak signal-to-noise ratios (PSNR) of 27.43 and 29.53 and training times
of 7 and 49 minutes, respectively. This performance exceeded that of structure
from motion (SfM) and mipmap Neural Radiance Fields (Mip-NeRF), demonstrating
superior efficiency. The SAM module achieved high segmentation accuracy, with a
mean intersection over union (mIoU) of 0.961 and an F1-score of 0.980.
Additionally, a comparison of biomass extraction models found the point cloud
volume model to be the most accurate, with an determination coefficient (R2) of
0.976, root mean square error (RMSE) of 2.92 g/plant, and mean absolute
percentage error (MAPE) of 6.81%, outperforming both the plot crop volume and
individual crop volume models. This study highlights the potential of combining
3DGS with multi-view UAV imaging for improved biomass phenotyping.



---

## BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel  View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-13 | David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2411.08508v1){: .btn .btn-green } |

**Abstract**: We present billboard Splatting (BBSplat) - a novel approach for 3D scene
representation based on textured geometric primitives. BBSplat represents the
scene as a set of optimizable textured planar primitives with learnable RGB
textures and alpha-maps to control their shape. BBSplat primitives can be used
in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our
method's qualitative and quantitative improvements over 3D and 2D Gaussians are
most noticeable when fewer primitives are used, when BBSplat achieves over 1200
FPS. Our novel regularization term encourages textures to have a sparser
structure, unlocking an efficient compression that leads to a reduction in
storage space of the model. Our experiments show the efficiency of BBSplat on
standard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,
and Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics
compared to the state-of-the-art, especially for the case when fewer primitives
are used, which, on the other hand, leads to up to 2 times inference speed
improvement for the same rendering quality.



---

## TomoGRAF: A Robust and Generalizable Reconstruction Network for  Single-View Computed Tomography

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Di Xu, Yang Yang, Hengjie Liu, Qihui Lyu, Martina Descovich, Dan Ruan, Ke Sheng | eess.IV | [PDF](http://arxiv.org/pdf/2411.08158v1){: .btn .btn-green } |

**Abstract**: Computed tomography (CT) provides high spatial resolution visualization of 3D
structures for scientific and clinical applications. Traditional
analytical/iterative CT reconstruction algorithms require hundreds of angular
data samplings, a condition that may not be met in practice due to physical and
mechanical limitations. Sparse view CT reconstruction has been proposed using
constrained optimization and machine learning methods with varying success,
less so for ultra-sparse view CT reconstruction with one to two views. Neural
radiance field (NeRF) is a powerful tool for reconstructing and rendering 3D
natural scenes from sparse views, but its direct application to 3D medical
image reconstruction has been minimally successful due to the differences
between optical and X-ray photon transportation. Here, we develop a novel
TomoGRAF framework incorporating the unique X-ray transportation physics to
reconstruct high-quality 3D volumes using ultra-sparse projections without
prior. TomoGRAF captures the CT imaging geometry, simulates the X-ray casting
and tracing process, and penalizes the difference between simulated and ground
truth CT sub-volume during training. We evaluated the performance of TomoGRAF
on an unseen dataset of distinct imaging characteristics from the training data
and demonstrated a vast leap in performance compared with state-of-the-art deep
learning and NeRF methods. TomoGRAF provides the first generalizable solution
for image-guided radiotherapy and interventional radiology applications, where
only one or a few X-ray views are available, but 3D volumetric information is
desired.



---

## GUS-IR: Gaussian Splatting with Unified Shading for Inverse Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Zhihao Liang, Hongdong Li, Kui Jia, Kailing Guo, Qi Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.07478v1){: .btn .btn-green } |

**Abstract**: Recovering the intrinsic physical attributes of a scene from images,
generally termed as the inverse rendering problem, has been a central and
challenging task in computer vision and computer graphics. In this paper, we
present GUS-IR, a novel framework designed to address the inverse rendering
problem for complicated scenes featuring rough and glossy surfaces. This paper
starts by analyzing and comparing two prominent shading techniques popularly
used for inverse rendering, forward shading and deferred shading, effectiveness
in handling complex materials. More importantly, we propose a unified shading
solution that combines the advantages of both techniques for better
decomposition. In addition, we analyze the normal modeling in 3D Gaussian
Splatting (3DGS) and utilize the shortest axis as normal for each particle in
GUS-IR, along with a depth-related regularization, resulting in improved
geometric representation and better shape reconstruction. Furthermore, we
enhance the probe-based baking scheme proposed by GS-IR to achieve more
accurate ambient occlusion modeling to better handle indirect illumination.
Extensive experiments have demonstrated the superior performance of GUS-IR in
achieving precise intrinsic decomposition and geometric representation,
supporting many downstream tasks (such as relighting, retouching) in computer
vision, graphics, and extended reality.

Comments:
- 15 pages, 11 figures

---

## GaussianCut: Interactive segmentation via graph cut for 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Umangi Jain, Ashkan Mirzaei, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2411.07555v1){: .btn .btn-green } |

**Abstract**: We introduce GaussianCut, a new method for interactive multiview segmentation
of scenes represented as 3D Gaussians. Our approach allows for selecting the
objects to be segmented by interacting with a single view. It accepts intuitive
user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian
Splatting (3DGS) as the underlying scene representation simplifies the
extraction of objects of interest which are considered to be a subset of the
scene's Gaussians. Our key idea is to represent the scene as a graph and use
the graph-cut algorithm to minimize an energy function to effectively partition
the Gaussians into foreground and background. To achieve this, we construct a
graph based on scene Gaussians and devise a segmentation-aligned energy
function on the graph to combine user inputs with scene properties. To obtain
an initial coarse segmentation, we leverage 2D image/video segmentation models
and further refine these coarse estimates using our graph construction. Our
empirical evaluations show the adaptability of GaussianCut across a diverse set
of scenes. GaussianCut achieves competitive performance with state-of-the-art
approaches for 3D segmentation without requiring any additional
segmentation-aware training.



---

## Projecting Gaussian Ellipsoids While Avoiding Affine Projection  Approximation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Han Qi, Tao Cai, Xiyue Han | cs.CV | [PDF](http://arxiv.org/pdf/2411.07579v2){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting has dominated novel-view synthesis with its
real-time rendering speed and state-of-the-art rendering quality. However,
during the rendering process, the use of the Jacobian of the affine
approximation of the projection transformation leads to inevitable errors,
resulting in blurriness, artifacts and a lack of scene consistency in the final
rendered images. To address this issue, we introduce an ellipsoid-based
projection method to calculate the projection of Gaussian ellipsoid on the
image plane, witch is the primitive of 3D Gaussian Splatting. As our proposed
ellipsoid-based projection method cannot handle Gaussian ellipsoids with camera
origins inside them or parts lying below $z=0$ plane in the camera space, we
designed a pre-filtering strategy. Experiments over multiple widely adopted
benchmark datasets show that using our ellipsoid-based projection method can
enhance the rendering quality of 3D Gaussian Splatting and its extensions.



---

## HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Qiankun Gao, Jiarui Meng, Chengxiang Wen, Jie Chen, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.07541v1){: .btn .btn-green } |

**Abstract**: The online reconstruction of dynamic scenes from multi-view streaming videos
faces significant challenges in training, rendering and storage efficiency.
Harnessing superior learning speed and real-time rendering capabilities, 3D
Gaussian Splatting (3DGS) has recently demonstrated considerable potential in
this field. However, 3DGS can be inefficient in terms of storage and prone to
overfitting by excessively growing Gaussians, particularly with limited views.
This paper proposes an efficient framework, dubbed HiCoM, with three key
components. First, we construct a compact and robust initial 3DGS
representation using a perturbation smoothing strategy. Next, we introduce a
Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform
distribution and local consistency of 3D Gaussians to swiftly and accurately
learn motions across frames. Finally, we continually refine the 3DGS with
additional Gaussians, which are later merged into the initial 3DGS to maintain
consistency with the evolving scene. To preserve a compact representation, an
equivalent number of low-opacity Gaussians that minimally impact the
representation are removed before processing subsequent frames. Extensive
experiments conducted on two widely used datasets show that our framework
improves learning efficiency of the state-of-the-art methods by about $20\%$
and reduces the data storage by $85\%$, achieving competitive free-viewpoint
video synthesis quality but with higher robustness and stability. Moreover, by
parallel learning multiple frames simultaneously, our HiCoM decreases the
average training wall time to $<2$ seconds per frame with negligible
performance degradation, substantially boosting real-world applicability and
responsiveness.

Comments:
- Accepted to NeurIPS 2024; Code is avaliable at
  https://github.com/gqk/HiCoM

---

## Material Transforms from Disentangled NeRF Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-12 | Ivan Lopes, Jean-François Lalonde, Raoul de Charette | cs.CV | [PDF](http://arxiv.org/pdf/2411.08037v1){: .btn .btn-green } |

**Abstract**: In this paper, we first propose a novel method for transferring material
transformations across different scenes. Building on disentangled Neural
Radiance Field (NeRF) representations, our approach learns to map Bidirectional
Reflectance Distribution Functions (BRDF) from pairs of scenes observed in
varying conditions, such as dry and wet. The learned transformations can then
be applied to unseen scenes with similar materials, therefore effectively
rendering the transformation learned with an arbitrary level of intensity.
Extensive experiments on synthetic scenes and real-world objects validate the
effectiveness of our approach, showing that it can learn various
transformations such as wetness, painting, coating, etc. Our results highlight
not only the versatility of our method but also its potential for practical
applications in computer graphics. We publish our method implementation, along
with our synthetic/real datasets on
https://github.com/astra-vision/BRDFTransform



---

## A Hierarchical Compression Technique for 3D Gaussian Splatting  Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-11 | He Huang, Wenjie Huang, Qi Yang, Yiling Xu, Zhu li | cs.CV | [PDF](http://arxiv.org/pdf/2411.06976v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (GS) demonstrates excellent rendering quality and
generation speed in novel view synthesis. However, substantial data size poses
challenges for storage and transmission, making 3D GS compression an essential
technology. Current 3D GS compression research primarily focuses on developing
more compact scene representations, such as converting explicit 3D GS data into
implicit forms. In contrast, compression of the GS data itself has hardly been
explored. To address this gap, we propose a Hierarchical GS Compression (HGSC)
technique. Initially, we prune unimportant Gaussians based on importance scores
derived from both global and local significance, effectively reducing
redundancy while maintaining visual quality. An Octree structure is used to
compress 3D positions. Based on the 3D GS Octree, we implement a hierarchical
attribute compression strategy by employing a KD-tree to partition the 3D GS
into multiple blocks. We apply farthest point sampling to select anchor
primitives within each block and others as non-anchor primitives with varying
Levels of Details (LoDs). Anchor primitives serve as reference points for
predicting non-anchor primitives across different LoDs to reduce spatial
redundancy. For anchor primitives, we use the region adaptive hierarchical
transform to achieve near-lossless compression of various attributes. For
non-anchor primitives, each is predicted based on the k-nearest anchor
primitives. To further minimize prediction errors, the reconstructed LoD and
anchor primitives are combined to form new anchor primitives to predict the
next LoD. Our method notably achieves superior compression quality and a
significant data size reduction of over 4.5 times compared to the
state-of-the-art compression method on small scenes datasets.



---

## LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-11 | Zefan Qu, Ke Xu, Gerhard Petrus Hancke, Rynson W. H. Lau | cs.CV | [PDF](http://arxiv.org/pdf/2411.06757v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have shown remarkable performances in
producing novel-view images from high-quality scene images. However, hand-held
low-light photography challenges NeRFs as the captured images may
simultaneously suffer from low visibility, noise, and camera shakes. While
existing NeRF methods may handle either low light or motion, directly combining
them or incorporating additional image-based enhancement methods does not work
as these degradation factors are highly coupled. We observe that noise in
low-light images is always sharp regardless of camera shakes, which implies an
implicit order of these degradation factors within the image formation process.
To this end, we propose in this paper a novel model, named LuSh-NeRF, which can
reconstruct a clean and sharp NeRF from a group of hand-held low-light images.
The key idea of LuSh-NeRF is to sequentially model noise and blur in the images
via multi-view feature consistency and frequency information of NeRF,
respectively. Specifically, LuSh-NeRF includes a novel Scene-Noise
Decomposition (SND) module for decoupling the noise from the scene
representation and a novel Camera Trajectory Prediction (CTP) module for the
estimation of camera motions based on low-frequency scene information. To
facilitate training and evaluations, we construct a new dataset containing both
synthetic and real images. Experiments show that LuSh-NeRF outperforms existing
approaches. Our code and dataset can be found here:
https://github.com/quzefan/LuSh-NeRF.

Comments:
- Accepted by NeurIPS 2024

---

## SplatFormer: Point Transformer for Robust 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-10 | Yutong Chen, Marko Mihajlovic, Xiyi Chen, Yiming Wang, Sergey Prokudin, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2411.06390v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently transformed photorealistic
reconstruction, achieving high visual fidelity and real-time performance.
However, rendering quality significantly deteriorates when test views deviate
from the camera angles used during training, posing a major challenge for
applications in immersive free-viewpoint rendering and navigation. In this
work, we conduct a comprehensive evaluation of 3DGS and related novel view
synthesis methods under out-of-distribution (OOD) test camera scenarios. By
creating diverse test cases with synthetic and real-world datasets, we
demonstrate that most existing methods, including those incorporating various
regularization techniques and data-driven priors, struggle to generalize
effectively to OOD views. To address this limitation, we introduce SplatFormer,
the first point transformer model specifically designed to operate on Gaussian
splats. SplatFormer takes as input an initial 3DGS set optimized under limited
training views and refines it in a single forward pass, effectively removing
potential artifacts in OOD test views. To our knowledge, this is the first
successful application of point transformers directly on 3DGS sets, surpassing
the limitations of previous multi-scene training methods, which could handle
only a restricted number of input views during inference. Our model
significantly improves rendering quality under extreme novel views, achieving
state-of-the-art performance in these challenging scenarios and outperforming
various 3DGS regularization techniques, multi-scene models tailored for sparse
view synthesis, and diffusion-based frameworks.

Comments:
- Code and dataset: https://github.com/ChenYutongTHU/SplatFormer
  Project page: https://sergeyprokudin.github.io/splatformer/

---

## Adaptive and Temporally Consistent Gaussian Surfels for Multi-view  Dynamic Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-10 | Decai Chen, Brianne Oberson, Ingo Feldmann, Oliver Schreer, Anna Hilsmann, Peter Eisert | cs.CV | [PDF](http://arxiv.org/pdf/2411.06602v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has recently achieved notable success in novel view
synthesis for dynamic scenes and geometry reconstruction in static scenes.
Building on these advancements, early methods have been developed for dynamic
surface reconstruction by globally optimizing entire sequences. However,
reconstructing dynamic scenes with significant topology changes, emerging or
disappearing objects, and rapid movements remains a substantial challenge,
particularly for long sequences. To address these issues, we propose AT-GS, a
novel method for reconstructing high-quality dynamic surfaces from multi-view
videos through per-frame incremental optimization. To avoid local minima across
frames, we introduce a unified and adaptive gradient-aware densification
strategy that integrates the strengths of conventional cloning and splitting
techniques. Additionally, we reduce temporal jittering in dynamic surfaces by
ensuring consistency in curvature maps across consecutive frames. Our method
achieves superior accuracy and temporal coherence in dynamic surface
reconstruction, delivering high-fidelity space-time novel view synthesis, even
in complex and challenging scenes. Extensive experiments on diverse multi-view
video datasets demonstrate the effectiveness of our approach, showing clear
advantages over baseline methods. Project page:
\url{https://fraunhoferhhi.github.io/AT-GS}



---

## Through the Curved Cover: Synthesizing Cover Aberrated Scenes with  Refractive Field

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-10 | Liuyue Xie, Jiancong Guo, Laszlo A. Jeni, Zhiheng Jia, Mingyang Li, Yunwen Zhou, Chao Guo | cs.CV | [PDF](http://arxiv.org/pdf/2411.06365v1){: .btn .btn-green } |

**Abstract**: Recent extended reality headsets and field robots have adopted covers to
protect the front-facing cameras from environmental hazards and falls. The
surface irregularities on the cover can lead to optical aberrations like
blurring and non-parametric distortions. Novel view synthesis methods like NeRF
and 3D Gaussian Splatting are ill-equipped to synthesize from sequences with
optical aberrations. To address this challenge, we introduce SynthCover to
enable novel view synthesis through protective covers for downstream extended
reality applications. SynthCover employs a Refractive Field that estimates the
cover's geometry, enabling precise analytical calculation of refracted rays.
Experiments on synthetic and real-world scenes demonstrate our method's ability
to accurately model scenes viewed through protective covers, achieving a
significant improvement in rendering quality compared to prior methods. We also
show that the model can adjust well to various cover geometries with synthetic
sequences captured with covers of different surface curvatures. To motivate
further studies on this problem, we provide the benchmarked dataset containing
real and synthetic walkable scenes captured with protective cover optical
aberrations.

Comments:
- WACV 2025

---

## AI-Driven Stylization of 3D Environments

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-09 | Yuanbo Chen, Yixiao Kang, Yukun Song, Cyrus Vachha, Sining Huang | cs.CV | [PDF](http://arxiv.org/pdf/2411.06067v1){: .btn .btn-green } |

**Abstract**: In this system, we discuss methods to stylize a scene of 3D primitive objects
into a higher fidelity 3D scene using novel 3D representations like NeRFs and
3D Gaussian Splatting. Our approach leverages existing image stylization
systems and image-to-3D generative models to create a pipeline that iteratively
stylizes and composites 3D objects into scenes. We show our results on adding
generated objects into a scene and discuss limitations.



---

## GaussianSpa: An "Optimizing-Sparsifying" Simplification Framework for  Compact and High-Quality 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-09 | Yangming Zhang, Wenqi Jia, Wei Niu, Miao Yin | cs.CV | [PDF](http://arxiv.org/pdf/2411.06019v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view
synthesis, leveraging continuous aggregations of Gaussian functions to model
scene geometry. However, 3DGS suffers from substantial memory requirements to
store the multitude of Gaussians, hindering its practicality. To address this
challenge, we introduce GaussianSpa, an optimization-based simplification
framework for compact and high-quality 3DGS. Specifically, we formulate the
simplification as an optimization problem associated with the 3DGS training.
Correspondingly, we propose an efficient "optimizing-sparsifying" solution that
alternately solves two independent sub-problems, gradually imposing strong
sparsity onto the Gaussians in the training process. Our comprehensive
evaluations on various datasets show the superiority of GaussianSpa over
existing state-of-the-art approaches. Notably, GaussianSpa achieves an average
PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with
10$\times$ fewer Gaussians compared to the vanilla 3DGS. Our project page is
available at https://gaussianspa.github.io/.

Comments:
- Project page at https://gaussianspa.github.io/

---

## A Nerf-Based Color Consistency Method for Remote Sensing Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-08 | Zongcheng Zuo, Yuanxiang Li, Tongtong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.05557v1){: .btn .btn-green } |

**Abstract**: Due to different seasons, illumination, and atmospheric conditions, the
photometric of the acquired image varies greatly, which leads to obvious
stitching seams at the edges of the mosaic image. Traditional methods can be
divided into two categories, one is absolute radiation correction and the other
is relative radiation normalization. We propose a NeRF-based method of color
consistency correction for multi-view images, which weaves image features
together using implicit expressions, and then re-illuminates feature space to
generate a fusion image with a new perspective. We chose Superview-1 satellite
images and UAV images with large range and time difference for the experiment.
Experimental results show that the synthesize image generated by our method has
excellent visual effect and smooth color transition at the edges.

Comments:
- 4 pages, 4 figures, The International Geoscience and Remote Sensing
  Symposium (IGARSS2023)

---

## Rate-aware Compression for NeRF-based Volumetric Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-08 | Zhiyu Zhang, Guo Lu, Huanxiong Liang, Zhengxue Cheng, Anni Tang, Li Song | cs.MM | [PDF](http://arxiv.org/pdf/2411.05322v1){: .btn .btn-green } |

**Abstract**: The neural radiance fields (NeRF) have advanced the development of 3D
volumetric video technology, but the large data volumes they involve pose
significant challenges for storage and transmission. To address these problems,
the existing solutions typically compress these NeRF representations after the
training stage, leading to a separation between representation training and
compression. In this paper, we try to directly learn a compact NeRF
representation for volumetric video in the training stage based on the proposed
rate-aware compression framework. Specifically, for volumetric video, we use a
simple yet effective modeling strategy to reduce temporal redundancy for the
NeRF representation. Then, during the training phase, an implicit entropy model
is utilized to estimate the bitrate of the NeRF representation. This entropy
model is then encoded into the bitstream to assist in the decoding of the NeRF
representation. This approach enables precise bitrate estimation, thereby
leading to a compact NeRF representation. Furthermore, we propose an adaptive
quantization strategy and learn the optimal quantization step for the NeRF
representations. Finally, the NeRF representation can be optimized by using the
rate-distortion trade-off. Our proposed compression framework can be used for
different representations and experimental results demonstrate that our
approach significantly reduces the storage size with marginal distortion and
achieves state-of-the-art rate-distortion performance for volumetric video on
the HumanRF and ReRF datasets. Compared to the previous state-of-the-art method
TeTriRF, we achieved an approximately -80% BD-rate on the HumanRF dataset and
-60% BD-rate on the ReRF dataset.

Comments:
- Accepted by ACM MM 2024 (Oral)

---

## MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2411.04924v1){: .btn .btn-green } |

**Abstract**: We introduce MVSplat360, a feed-forward approach for 360{\deg} novel view
synthesis (NVS) of diverse real-world scenes, using only sparse observations.
This setting is inherently ill-posed due to minimal overlap among input views
and insufficient visual information provided, making it challenging for
conventional methods to achieve high-quality results. Our MVSplat360 addresses
this by effectively combining geometry-aware 3D reconstruction with temporally
consistent video generation. Specifically, it refactors a feed-forward 3D
Gaussian Splatting (3DGS) model to render features directly into the latent
space of a pre-trained Stable Video Diffusion (SVD) model, where these features
then act as pose and visual cues to guide the denoising process and produce
photorealistic 3D-consistent views. Our model is end-to-end trainable and
supports rendering arbitrary views with as few as 5 sparse input views. To
evaluate MVSplat360's performance, we introduce a new benchmark using the
challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual
quality compared to state-of-the-art methods on wide-sweeping or even 360{\deg}
NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the
effectiveness of our model. The video results are available on our project
page: https://donydchen.github.io/mvsplat360.

Comments:
- NeurIPS 2024, Project page: https://donydchen.github.io/mvsplat360,
  Code: https://github.com/donydchen/mvsplat360

---

## ProEdit: Simple Progression is All You Need for High-Quality 3D Scene  Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Jun-Kun Chen, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.05006v1){: .btn .btn-green } |

**Abstract**: This paper proposes ProEdit - a simple yet effective framework for
high-quality 3D scene editing guided by diffusion distillation in a novel
progressive manner. Inspired by the crucial observation that multi-view
inconsistency in scene editing is rooted in the diffusion model's large
feasible output space (FOS), our framework controls the size of FOS and reduces
inconsistency by decomposing the overall editing task into several subtasks,
which are then executed progressively on the scene. Within this framework, we
design a difficulty-aware subtask decomposition scheduler and an adaptive 3D
Gaussian splatting (3DGS) training strategy, ensuring high quality and
efficiency in performing each subtask. Extensive evaluation shows that our
ProEdit achieves state-of-the-art results in various scenes and challenging
editing tasks, all through a simple framework without any expensive or
sophisticated add-ons like distillation losses, components, or training
procedures. Notably, ProEdit also provides a new way to control, preview, and
select the "aggressivity" of editing operation during the editing process.

Comments:
- NeurIPS 2024. Project Page: https://immortalco.github.io/ProEdit/

---

## GANESH: Generalizable NeRF for Lensless Imaging

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Rakesh Raj Madavan, Akshat Kaimal, Badhrinarayanan K V, Vinayak Gupta, Rohit Choudhary, Chandrakala Shanmuganathan, Kaushik Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2411.04810v1){: .btn .btn-green } |

**Abstract**: Lensless imaging offers a significant opportunity to develop ultra-compact
cameras by removing the conventional bulky lens system. However, without a
focusing element, the sensor's output is no longer a direct image but a complex
multiplexed scene representation. Traditional methods have attempted to address
this challenge by employing learnable inversions and refinement models, but
these methods are primarily designed for 2D reconstruction and do not
generalize well to 3D reconstruction. We introduce GANESH, a novel framework
designed to enable simultaneous refinement and novel view synthesis from
multi-view lensless images. Unlike existing methods that require scene-specific
training, our approach supports on-the-fly inference without retraining on each
scene. Moreover, our framework allows us to tune our model to specific scenes,
enhancing the rendering and refinement quality. To facilitate research in this
area, we also present the first multi-view lensless dataset, LenslessScenes.
Extensive experiments demonstrate that our method outperforms current
approaches in reconstruction accuracy and refinement quality. Code and video
results are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/



---

## Planar Reflection-Aware Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Chen Gao, Yipeng Wang, Changil Kim, Jia-Bin Huang, Johannes Kopf | cs.CV | [PDF](http://arxiv.org/pdf/2411.04984v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in
reconstructing complex scenes with high fidelity. However, NeRF's view
dependency can only handle low-frequency reflections. It falls short when
handling complex planar reflections, often interpreting them as erroneous scene
geometries and leading to duplicated and inaccurate scene representations. To
address this challenge, we introduce a reflection-aware NeRF that jointly
models planar reflectors, such as windows, and explicitly casts reflected rays
to capture the source of the high-frequency reflections. We query a single
radiance field to render the primary color and the source of the reflection. We
propose a sparse edge regularization to help utilize the true sources of
reflections for rendering planar reflections rather than creating a duplicate
along the primary ray at the same depth. As a result, we obtain accurate scene
geometry. Rendering along the primary ray results in a clean, reflection-free
view, while explicitly rendering along the reflected ray allows us to
reconstruct highly detailed reflections. Our extensive quantitative and
qualitative evaluations of real-world datasets demonstrate our method's
enhanced performance in accurately handling reflections.



---

## SuperQ-GRASP: Superquadrics-based Grasp Pose Estimation on Larger  Objects for Mobile-Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-07 | Xun Tu, Karthik Desingh | cs.RO | [PDF](http://arxiv.org/pdf/2411.04386v2){: .btn .btn-green } |

**Abstract**: Grasp planning and estimation have been a longstanding research problem in
robotics, with two main approaches to find graspable poses on the objects: 1)
geometric approach, which relies on 3D models of objects and the gripper to
estimate valid grasp poses, and 2) data-driven, learning-based approach, with
models trained to identify grasp poses from raw sensor observations. The latter
assumes comprehensive geometric coverage during the training phase. However,
the data-driven approach is typically biased toward tabletop scenarios and
struggle to generalize to out-of-distribution scenarios with larger objects
(e.g. chair). Additionally, raw sensor data (e.g. RGB-D data) from a single
view of these larger objects is often incomplete and necessitates additional
observations. In this paper, we take a geometric approach, leveraging
advancements in object modeling (e.g. NeRF) to build an implicit model by
taking RGB images from views around the target object. This model enables the
extraction of explicit mesh model while also capturing the visual appearance
from novel viewpoints that is useful for perception tasks like object detection
and pose estimation. We further decompose the NeRF-reconstructed 3D mesh into
superquadrics (SQs) -- parametric geometric primitives, each mapped to a set of
precomputed grasp poses, allowing grasp composition on the target object based
on these primitives. Our proposed pipeline overcomes the problems: a) noisy
depth and incomplete view of the object, with a modeling step, and b)
generalization to objects of any size. For more qualitative results, refer to
the supplementary video and webpage https://bit.ly/3ZrOanU

Comments:
- 8 pages, 7 figures, submitted to ICRA 2025 for review

---

## Structure Consistent Gaussian Splatting with Matching Prior for Few-shot  Novel View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Rui Peng, Wangze Xu, Luyang Tang, Liwei Liao, Jianbo Jiao, Ronggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.03637v1){: .btn .btn-green } |

**Abstract**: Despite the substantial progress of novel view synthesis, existing methods,
either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian
Splatting (3DGS), suffer significant degradation when the input becomes sparse.
Numerous efforts have been introduced to alleviate this problem, but they still
struggle to synthesize satisfactory results efficiently, especially in the
large scene. In this paper, we propose SCGaussian, a Structure Consistent
Gaussian Splatting method using matching priors to learn 3D consistent scene
structure. Considering the high interdependence of Gaussian attributes, we
optimize the scene structure in two folds: rendering geometry and, more
importantly, the position of Gaussian primitives, which is hard to be directly
constrained in the vanilla 3DGS due to the non-structure property. To achieve
this, we present a hybrid Gaussian representation. Besides the ordinary
non-structure Gaussian primitives, our model also consists of ray-based
Gaussian primitives that are bound to matching rays and whose optimization of
their positions is restricted along the ray. Thus, we can utilize the matching
correspondence to directly enforce the position of these Gaussian primitives to
converge to the surface points where rays intersect. Extensive experiments on
forward-facing, surrounding, and complex large scenes show the effectiveness of
our approach with state-of-the-art performance and high efficiency. Code is
available at https://github.com/prstrive/SCGaussian.

Comments:
- NeurIPS 2024 Accepted

---

## 3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical  Object Rearrangement

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Ziqi Lu, Jianbo Ye, John Leonard | cs.CV | [PDF](http://arxiv.org/pdf/2411.03706v1){: .btn .btn-green } |

**Abstract**: We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for
detecting physical object rearrangements in 3D scenes. Our approach estimates
3D object-level changes by comparing two sets of unaligned images taken at
different times. Leveraging 3DGS's novel view rendering and EfficientSAM's
zero-shot segmentation capabilities, we detect 2D object-level changes, which
are then associated and fused across views to estimate 3D changes. Our method
can detect changes in cluttered environments using sparse post-change images
within as little as 18s, using as few as a single new image. It does not rely
on depth input, user instructions, object classes, or object models -- An
object is recognized simply if it has been re-arranged. Our approach is
evaluated on both public and self-collected real-world datasets, achieving up
to 14% higher accuracy and three orders of magnitude faster performance
compared to the state-of-the-art radiance-field-based change detection method.
This significant performance boost enables a broad range of downstream
applications, where we highlight three key use cases: object reconstruction,
robot workspace reset, and 3DGS model update. Our code and data will be made
available at https://github.com/520xyxyzq/3DGS-CD.



---

## GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-06 | Jilan Mei, Junbo Li, Cai Meng | cs.CV | [PDF](http://arxiv.org/pdf/2411.03807v3){: .btn .btn-green } |

**Abstract**: This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.



---

## HFGaussian: Learning Generalizable Gaussian Human with Integrated Human  Features

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Arnab Dey, Cheng-You Lu, Andrew I. Comport, Srinath Sridhar, Chin-Teng Lin, Jean Martinet | cs.CV | [PDF](http://arxiv.org/pdf/2411.03086v1){: .btn .btn-green } |

**Abstract**: Recent advancements in radiance field rendering show promising results in 3D
scene representation, where Gaussian splatting-based techniques emerge as
state-of-the-art due to their quality and efficiency. Gaussian splatting is
widely used for various applications, including 3D human representation.
However, previous 3D Gaussian splatting methods either use parametric body
models as additional information or fail to provide any underlying structure,
like human biomechanical features, which are essential for different
applications. In this paper, we present a novel approach called HFGaussian that
can estimate novel views and human features, such as the 3D skeleton, 3D key
points, and dense pose, from sparse input images in real time at 25 FPS. The
proposed method leverages generalizable Gaussian splatting technique to
represent the human subject and its associated features, enabling efficient and
generalizable reconstruction. By incorporating a pose regression network and
the feature splatting technique with Gaussian splatting, HFGaussian
demonstrates improved capabilities over existing 3D human methods, showcasing
the potential of 3D human representations with integrated biomechanics. We
thoroughly evaluate our HFGaussian method against the latest state-of-the-art
techniques in human Gaussian splatting and pose estimation, demonstrating its
real-time, state-of-the-art performance.



---

## LVI-GS: Tightly-coupled LiDAR-Visual-Inertial SLAM using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Huibin Zhao, Weipeng Guan, Peng Lu | cs.RO | [PDF](http://arxiv.org/pdf/2411.02703v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has shown its ability in rapid rendering and
high-fidelity mapping. In this paper, we introduce LVI-GS, a tightly-coupled
LiDAR-Visual-Inertial mapping framework with 3DGS, which leverages the
complementary characteristics of LiDAR and image sensors to capture both
geometric structures and visual details of 3D scenes. To this end, the 3D
Gaussians are initialized from colourized LiDAR points and optimized using
differentiable rendering. In order to achieve high-fidelity mapping, we
introduce a pyramid-based training approach to effectively learn multi-level
features and incorporate depth loss derived from LiDAR measurements to improve
geometric feature perception. Through well-designed strategies for Gaussian-Map
expansion, keyframe selection, thread management, and custom CUDA acceleration,
our framework achieves real-time photo-realistic mapping. Numerical experiments
are performed to evaluate the superior performance of our method compared to
state-of-the-art 3D reconstruction systems.



---

## CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model  Retrieval

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Xin Wen, Xuening Zhu, Renjiao Yi, Zhifeng Wang, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2411.02979v1){: .btn .btn-green } |

**Abstract**: Reconstructing from multi-view images is a longstanding problem in 3D vision,
where neural radiance fields (NeRFs) have shown great potential and get
realistic rendered images of novel views. Currently, most NeRF methods either
require accurate camera poses or a large number of input images, or even both.
Reconstructing NeRF from few-view images without poses is challenging and
highly ill-posed. To address this problem, we propose CAD-NeRF, a method
reconstructed from less than 10 images without any known poses. Specifically,
we build a mini library of several CAD models from ShapeNet and render them
from many random views. Given sparse-view input images, we run a model and pose
retrieval from the library, to get a model with similar shapes, serving as the
density supervision and pose initializations. Here we propose a multi-view pose
retrieval method to avoid pose conflicts among views, which is a new and unseen
problem in uncalibrated NeRF methods. Then, the geometry of the object is
trained by the CAD guidance. The deformation of the density field and camera
poses are optimized jointly. Then texture and density are trained and
fine-tuned as well. All training phases are in self-supervised manners.
Comprehensive evaluations of synthetic and real images show that CAD-NeRF
successfully learns accurate densities with a large deformation from retrieved
CAD models, showing the generalization abilities.

Comments:
- The article has been accepted by Frontiers of Computer Science (FCS)

---

## Exploring Seasonal Variability in the Context of Neural Radiance Fields  for 3D Reconstruction on Satellite Imagery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Liv Kåreborn, Erica Ingerstad, Amanda Berg, Justus Karlsson, Leif Haglund | cs.CV | [PDF](http://arxiv.org/pdf/2411.02972v1){: .btn .btn-green } |

**Abstract**: In this work, the seasonal predictive capabilities of Neural Radiance Fields
(NeRF) applied to satellite images are investigated. Focusing on the
utilization of satellite data, the study explores how Sat-NeRF, a novel
approach in computer vision, performs in predicting seasonal variations across
different months. Through comprehensive analysis and visualization, the study
examines the model's ability to capture and predict seasonal changes,
highlighting specific challenges and strengths. Results showcase the impact of
the sun direction on predictions, revealing nuanced details in seasonal
transitions, such as snow cover, color accuracy, and texture representation in
different landscapes. Given these results, we propose Planet-NeRF, an extension
to Sat-NeRF capable of incorporating seasonal variability through a set of
month embedding vectors. Comparative evaluations reveal that Planet-NeRF
outperforms prior models in the case where seasonal changes are present. The
extensive evaluation combined with the proposed method offers promising avenues
for future research in this domain.



---

## Enhancing Exploratory Capability of Visual Navigation Using Uncertainty  of Implicit Scene Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Yichen Wang, Qiming Liu, Zhe Liu, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2411.03487v1){: .btn .btn-green } |

**Abstract**: In the context of visual navigation in unknown scenes, both "exploration" and
"exploitation" are equally crucial. Robots must first establish environmental
cognition through exploration and then utilize the cognitive information to
accomplish target searches. However, most existing methods for image-goal
navigation prioritize target search over the generation of exploratory
behavior. To address this, we propose the Navigation with Uncertainty-driven
Exploration (NUE) pipeline, which uses an implicit and compact scene
representation, NeRF, as a cognitive structure. We estimate the uncertainty of
NeRF and augment the exploratory ability by the uncertainty to in turn
facilitate the construction of implicit representation. Simultaneously, we
extract memory information from NeRF to enhance the robot's reasoning ability
for determining the location of the target. Ultimately, we seamlessly combine
the two generated abilities to produce navigational actions. Our pipeline is
end-to-end, with the environmental cognitive structure being constructed
online. Extensive experimental results on image-goal navigation demonstrate the
capability of our pipeline to enhance exploratory behaviors, while also
enabling a natural transition from the exploration to exploitation phase. This
enables our model to outperform existing memory-based cognitive navigation
structures in terms of navigation performance.



---

## Object and Contact Point Tracking in Demonstrations Using 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Michael Büttner, Jonathan Francis, Helge Rhodin, Andrew Melnik | cs.CV | [PDF](http://arxiv.org/pdf/2411.03555v1){: .btn .btn-green } |

**Abstract**: This paper introduces a method to enhance Interactive Imitation Learning
(IIL) by extracting touch interaction points and tracking object movement from
video demonstrations. The approach extends current IIL systems by providing
robots with detailed knowledge of both where and how to interact with objects,
particularly complex articulated ones like doors and drawers. By leveraging
cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for
tracking, this method allows robots to better understand and manipulate objects
in dynamic environments. The research lays the foundation for more effective
task learning and execution in autonomous robotic systems.

Comments:
- CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich,
  Germany

---

## Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-05 | Xavier Timoneda, Markus Herb, Fabian Duerr, Daniel Goehring, Fisher Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.02969v1){: .btn .btn-green } |

**Abstract**: LiDAR Semantic Segmentation is a fundamental task in autonomous driving
perception consisting of associating each LiDAR point to a semantic label.
Fully-supervised models have widely tackled this task, but they require labels
for each scan, which either limits their domain or requires impractical amounts
of expensive annotations. Camera images, which are generally recorded alongside
LiDAR pointclouds, can be processed by the widely available 2D foundation
models, which are generic and dataset-agnostic. However, distilling knowledge
from 2D data to improve LiDAR perception raises domain adaptation challenges.
For example, the classical perspective projection suffers from the parallax
effect produced by the position shift between both sensors at their respective
capture times. We propose a Semi-Supervised Learning setup to leverage
unlabeled LiDAR pointclouds alongside distilled knowledge from the camera
images. To self-supervise our model on the unlabeled scans, we add an auxiliary
NeRF head and cast rays from the camera viewpoint over the unlabeled voxel
features. The NeRF head predicts densities and semantic logits at each sampled
ray location which are used for rendering pixel semantics. Concurrently, we
query the Segment-Anything (SAM) foundation model with the camera image to
generate a set of unlabeled generic masks. We fuse the masks with the rendered
pixel semantics from LiDAR to produce pseudo-labels that supervise the pixel
predictions. During inference, we drop the NeRF head and run our model with
only LiDAR. We show the effectiveness of our approach in three public LiDAR
Semantic Segmentation benchmarks: nuScenes, SemanticKITTI and ScribbleKITTI.

Comments:
- IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS) 2024

---

## FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage  Training

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers | cs.CV | [PDF](http://arxiv.org/pdf/2411.02229v2){: .btn .btn-green } |

**Abstract**: The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.

Comments:
- Accepted by NeurIPS2024

---

## SplatOverflow: Asynchronous Hardware Troubleshooting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Amritansh Kwatra, Tobias Wienberg, Ilan Mandel, Ritik Batra, Peter He, Francois Guimbretiere, Thijs Roumen | cs.HC | [PDF](http://arxiv.org/pdf/2411.02332v2){: .btn .btn-green } |

**Abstract**: As tools for designing and manufacturing hardware become more accessible,
smaller producers can develop and distribute novel hardware. However, there
aren't established tools to support end-user hardware troubleshooting or
routine maintenance. As a result, technical support for hardware remains ad-hoc
and challenging to scale. Inspired by software troubleshooting workflows like
StackOverflow, we propose a workflow for asynchronous hardware troubleshooting:
SplatOverflow. SplatOverflow creates a novel boundary object, the SplatOverflow
scene, that users reference to communicate about hardware. The scene comprises
a 3D Gaussian Splat of the user's hardware registered onto the hardware's CAD
model. The splat captures the current state of the hardware, and the registered
CAD model acts as a referential anchor for troubleshooting instructions. With
SplatOverflow, maintainers can directly address issues and author instructions
in the user's workspace. The instructions define workflows that can easily be
shared between users and recontextualized in new environments. In this paper,
we describe the design of SplatOverflow, detail the workflows it enables, and
illustrate its utility to different kinds of users. We also validate that
non-experts can use SplatOverflow to troubleshoot common problems with a 3D
printer in a user study.

Comments:
- Our accompanying video figure is available at:
  https://youtu.be/m4TKeBDuZkU

---

## A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Matthew McDermott, Jason Rife | cs.CV | [PDF](http://arxiv.org/pdf/2411.01725v1){: .btn .btn-green } |

**Abstract**: In this paper we reexamine the process through which a Neural Radiance Field
(NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image
applications where camera pixels integrate light over time, LiDAR pulses arrive
at specific times. As such, multiple LiDAR returns are possible for any given
detector and the classification of these returns is inherently probabilistic.
Applying a traditional NeRF training routine can result in the network learning
phantom surfaces in free space between conflicting range measurements, similar
to how floater aberrations may be produced by an image model. We show that by
formulating loss as an integral of probability (rather than as an integral of
optical density) the network can learn multiple peaks for a given ray, allowing
the sampling of first, nth, or strongest returns from a single output channel.
Code is available at https://github.com/mcdermatt/PLINK



---

## NeRF-Aug: Data Augmentation for Robotics with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Eric Zhu, Mara Levy, Matthew Gwilliam, Abhinav Shrivastava | cs.RO | [PDF](http://arxiv.org/pdf/2411.02482v1){: .btn .btn-green } |

**Abstract**: Training a policy that can generalize to unknown objects is a long standing
challenge within the field of robotics. The performance of a policy often drops
significantly in situations where an object in the scene was not seen during
training. To solve this problem, we present NeRF-Aug, a novel method that is
capable of teaching a policy to interact with objects that are not present in
the dataset. This approach differs from existing approaches by leveraging the
speed and photorealism of a neural radiance field for augmentation. NeRF- Aug
both creates more photorealistic data and runs 3.83 times faster than existing
methods. We demonstrate the effectiveness of our method on 4 tasks with 11
novel objects that have no expert demonstration data. We achieve an average
69.1% success rate increase over existing methods. See video results at
https://nerf-aug.github.io.



---

## Modeling Uncertainty in 3D Gaussian Splatting through Continuous  Semantic Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Joey Wilson, Marcelino Almeida, Min Sun, Sachit Mahajan, Maani Ghaffari, Parker Ewen, Omid Ghasemalizadeh, Cheng-Hao Kuo, Arnie Sen | cs.RO | [PDF](http://arxiv.org/pdf/2411.02547v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel algorithm for probabilistically updating
and rasterizing semantic maps within 3D Gaussian Splatting (3D-GS). Although
previous methods have introduced algorithms which learn to rasterize features
in 3D-GS for enhanced scene understanding, 3D-GS can fail without warning which
presents a challenge for safety-critical robotic applications. To address this
gap, we propose a method which advances the literature of continuous semantic
mapping from voxels to ellipsoids, combining the precise structure of 3D-GS
with the ability to quantify uncertainty of probabilistic robotic maps. Given a
set of images, our algorithm performs a probabilistic semantic update directly
on the 3D ellipsoids to obtain an expectation and variance through the use of
conjugate priors. We also propose a probabilistic rasterization which returns
per-pixel segmentation predictions with quantifiable uncertainty. We compare
our method with similar probabilistic voxel-based methods to verify our
extension to 3D ellipsoids, and perform ablation studies on uncertainty
quantification and temporal smoothing.



---

## GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface  Reconstruction in Open Scenes

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-04 | Gaochao Song, Chong Cheng, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2411.01853v2){: .btn .btn-green } |

**Abstract**: In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.

Comments:
- NeurIPS 2024

---

## Real-Time Spatio-Temporal Reconstruction of Dynamic Endoscopic Scenes  with 4D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-02 | Fengze Li, Jishuai He, Jieming Ma, Zhijing Wu | cs.CV | [PDF](http://arxiv.org/pdf/2411.01218v1){: .btn .btn-green } |

**Abstract**: Dynamic scene reconstruction is essential in robotic minimally invasive
surgery, providing crucial spatial information that enhances surgical precision
and outcomes. However, existing methods struggle to address the complex,
temporally dynamic nature of endoscopic scenes. This paper presents
ST-Endo4DGS, a novel framework that models the spatio-temporal volume of
dynamic endoscopic scenes using unbiased 4D Gaussian Splatting (4DGS)
primitives, parameterized by anisotropic ellipses with flexible 4D rotations.
This approach enables precise representation of deformable tissue dynamics,
capturing intricate spatial and temporal correlations in real time.
Additionally, we extend spherindrical harmonics to represent time-evolving
appearance, achieving realistic adaptations to lighting and view changes. A new
endoscopic normal alignment constraint (ENAC) further enhances geometric
fidelity by aligning rendered normals with depth-derived geometry. Extensive
evaluations show that ST-Endo4DGS outperforms existing methods in both visual
quality and real-time performance, establishing a new state-of-the-art in
dynamic scene reconstruction for endoscopic surgery.



---

## ZIM: Zero-Shot Image Matting for Anything

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Beomyoung Kim, Chanyong Shin, Joonhyun Jeong, Hyungsik Jung, Se-Yun Lee, Sewhan Chun, Dong-Hyun Hwang, Joonsang Yu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00626v1){: .btn .btn-green } |

**Abstract**: The recent segmentation foundation model, Segment Anything Model (SAM),
exhibits strong zero-shot segmentation capabilities, but it falls short in
generating fine-grained precise masks. To address this limitation, we propose a
novel zero-shot image matting model, called ZIM, with two key contributions:
First, we develop a label converter that transforms segmentation labels into
detailed matte labels, constructing the new SA1B-Matte dataset without costly
manual annotations. Training SAM with this dataset enables it to generate
precise matte masks while maintaining its zero-shot capability. Second, we
design the zero-shot matting model equipped with a hierarchical pixel decoder
to enhance mask representation, along with a prompt-aware masked attention
mechanism to improve performance by enabling the model to focus on regions
specified by visual prompts. We evaluate ZIM using the newly introduced
MicroMat-3K test set, which contains high-quality micro-level matte labels.
Experimental results show that ZIM outperforms existing methods in fine-grained
mask generation and zero-shot generalization. Furthermore, we demonstrate the
versatility of ZIM in various downstream tasks requiring precise masks, such as
image inpainting and 3D NeRF. Our contributions provide a robust foundation for
advancing zero-shot matting and its downstream applications across a wide range
of computer vision tasks. The code is available at
\url{https://github.com/naver-ai/ZIM}.

Comments:
- preprint (21 pages, 16 figures, and 8 tables)

---

## PCoTTA: Continual Test-Time Adaptation for Multi-Task Point Cloud  Understanding


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Jincen Jiang, Qianyu Zhou, Yuhang Li, Xinkui Zhao, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang, Xuequan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2411.00632v1){: .btn .btn-green } |

**Abstract**: In this paper, we present PCoTTA, an innovative, pioneering framework for
Continual Test-Time Adaptation (CoTTA) in multi-task point cloud understanding,
enhancing the model's transferability towards the continually changing target
domain. We introduce a multi-task setting for PCoTTA, which is practical and
realistic, handling multiple tasks within one unified model during the
continual adaptation. Our PCoTTA involves three key components: automatic
prototype mixture (APM), Gaussian Splatted feature shifting (GSFS), and
contrastive prototype repulsion (CPR). Firstly, APM is designed to
automatically mix the source prototypes with the learnable prototypes with a
similarity balancing factor, avoiding catastrophic forgetting. Then, GSFS
dynamically shifts the testing sample toward the source domain, mitigating
error accumulation in an online manner. In addition, CPR is proposed to pull
the nearest learnable prototype close to the testing feature and push it away
from other prototypes, making each prototype distinguishable during the
adaptation. Experimental comparisons lead to a new benchmark, demonstrating
PCoTTA's superiority in boosting the model's transferability towards the
continually changing target domain.

Comments:
- Accepted to NeurIPS 2024

---

## CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for  Large-Scale Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-11-01 | Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2411.00771v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field
reconstruction, manifesting efficient and high-fidelity novel view synthesis.
However, accurately representing surfaces, especially in large and complex
scenarios, remains a significant challenge due to the unstructured nature of
3DGS. In this paper, we present CityGaussianV2, a novel approach for
large-scale scene reconstruction that addresses critical challenges related to
geometric accuracy and efficiency. Building on the favorable generalization
capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and
scalability issues. Specifically, we implement a decomposed-gradient-based
densification and depth regression technique to eliminate blurry artifacts and
accelerate convergence. To scale up, we introduce an elongation filter that
mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we
optimize the CityGaussian pipeline for parallel training, achieving up to
10$\times$ compression, at least 25% savings in training time, and a 50%
decrease in memory usage. We also established standard geometry benchmarks
under large-scale scenes. Experimental results demonstrate that our method
strikes a promising balance between visual quality, geometric accuracy, as well
as storage and training costs. The project page is available at
https://dekuliutesla.github.io/CityGaussianV2/.

Comments:
- Project Page: https://dekuliutesla.github.io/CityGaussianV2/
