---
layout: default
title: May 2024
parent: Papers
nav_order: 202405
---

<!---metadata--->


## NPGA: Neural Parametric Gaussian Avatars

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-29 | Simon Giebenhain, Tobias Kirschstein, Martin Rünz, Lourdes Agapito, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2405.19331v1){: .btn .btn-green } |

**Abstract**: The creation of high-fidelity, digital versions of human heads is an
important stepping stone in the process of further integrating virtual
components into our everyday lives. Constructing such avatars is a challenging
research problem, due to a high demand for photo-realism and real-time
rendering performance. In this work, we propose Neural Parametric Gaussian
Avatars (NPGA), a data-driven approach to create high-fidelity, controllable
avatars from multi-view video recordings. We build our method around 3D
Gaussian Splatting for its highly efficient rendering and to inherit the
topological flexibility of point clouds. In contrast to previous work, we
condition our avatars' dynamics on the rich expression space of neural
parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we
distill the backward deformation field of our underlying NPHM into forward
deformations which are compatible with rasterization-based rendering. All
remaining fine-scale, expression-dependent details are learned from the
multi-view videos. To increase the representational capacity of our avatars, we
augment the canonical Gaussian point cloud using per-primitive latent features
which govern its dynamic behavior. To regularize this increased dynamic
expressivity, we propose Laplacian terms on the latent features and predicted
dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating
that NPGA significantly outperforms the previous state-of-the-art avatars on
the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate
animation capabilities from real-world monocular videos.

Comments:
- Project Page: see https://simongiebenhain.github.io/NPGA/ ; Youtube
  Video: see https://www.youtube.com/watch?v=NGRxAYbIkus

---

## Neural Radiance Fields for Novel View Synthesis in Monocular Gastroscopy

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-29 | Zijie Jiang, Yusuke Monno, Masatoshi Okutomi, Sho Suzuki, Kenji Miki | cs.CV | [PDF](http://arxiv.org/pdf/2405.18863v1){: .btn .btn-green } |

**Abstract**: Enabling the synthesis of arbitrarily novel viewpoint images within a
patient's stomach from pre-captured monocular gastroscopic images is a
promising topic in stomach diagnosis. Typical methods to achieve this objective
integrate traditional 3D reconstruction techniques, including
structure-from-motion (SfM) and Poisson surface reconstruction. These methods
produce explicit 3D representations, such as point clouds and meshes, thereby
enabling the rendering of the images from novel viewpoints. However, the
existence of low-texture and non-Lambertian regions within the stomach often
results in noisy and incomplete reconstructions of point clouds and meshes,
hindering the attainment of high-quality image rendering. In this paper, we
apply the emerging technique of neural radiance fields (NeRF) to monocular
gastroscopic data for synthesizing photo-realistic images for novel viewpoints.
To address the performance degradation due to view sparsity in local regions of
monocular gastroscopy, we incorporate geometry priors from a pre-reconstructed
point cloud into the training of NeRF, which introduces a novel geometry-based
loss to both pre-captured observed views and generated unobserved views.
Compared to other recent NeRF methods, our approach showcases high-fidelity
image renderings from novel viewpoints within the stomach both qualitatively
and quantitatively.

Comments:
- Accepted for EMBC 2024

---

## LP-3DGS: Learning to Prune 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-29 | Zhaoliang Zhang, Tianchen Song, Yongjae Lee, Li Yang, Cheng Peng, Rama Chellappa, Deliang Fan | cs.CV | [PDF](http://arxiv.org/pdf/2405.18784v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has become one of the mainstream
methodologies for novel view synthesis (NVS) due to its high quality and fast
rendering speed. However, as a point-based scene representation, 3DGS
potentially generates a large number of Gaussians to fit the scene, leading to
high memory usage. Improvements that have been proposed require either an
empirical and preset pruning ratio or importance score threshold to prune the
point cloud. Such hyperparamter requires multiple rounds of training to
optimize and achieve the maximum pruning ratio, while maintaining the rendering
quality for each scene. In this work, we propose learning-to-prune 3DGS
(LP-3DGS), where a trainable binary mask is applied to the importance score
that can find optimal pruning ratio automatically. Instead of using the
traditional straight-through estimator (STE) method to approximate the binary
mask gradient, we redesign the masking function to leverage the Gumbel-Sigmoid
method, making it differentiable and compatible with the existing training
process of 3DGS. Extensive experiments have shown that LP-3DGS consistently
produces a good balance that is both efficient and high quality.



---

## NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the  Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-29 | Weining Ren, Zihan Zhu, Boyang Sun, Jiaqi Chen, Marc Pollefeys, Songyou Peng | cs.CV | [PDF](http://arxiv.org/pdf/2405.18715v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have shown remarkable success in synthesizing
photorealistic views from multi-view images of static scenes, but face
challenges in dynamic, real-world environments with distractors like moving
objects, shadows, and lighting changes. Existing methods manage controlled
environments and low occlusion ratios but fall short in render quality,
especially under high occlusion scenarios. In this paper, we introduce NeRF
On-the-go, a simple yet effective approach that enables the robust synthesis of
novel views in complex, in-the-wild scenes from only casually captured image
sequences. Delving into uncertainty, our method not only efficiently eliminates
distractors, even when they are predominant in captures, but also achieves a
notably faster convergence speed. Through comprehensive experiments on various
scenes, our method demonstrates a significant improvement over state-of-the-art
techniques. This advancement opens new avenues for NeRF in diverse and dynamic
real-world applications.

Comments:
- CVPR 2024, first two authors contributed equally. Project Page:
  https://nerf-on-the-go.github.io

---

## A Grid-Free Fluid Solver based on Gaussian Spatial Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Jingrui Xing, Bin Wang, Mengyu Chu, Baoquan Chen | cs.GR | [PDF](http://arxiv.org/pdf/2405.18133v1){: .btn .btn-green } |

**Abstract**: We present a grid-free fluid solver featuring a novel Gaussian
representation. Drawing inspiration from the expressive capabilities of 3D
Gaussian Splatting in multi-view image reconstruction, we model the continuous
flow velocity as a weighted sum of multiple Gaussian functions. Leveraging this
representation, we derive differential operators for the field and implement a
time-dependent PDE solver using the traditional operator splitting method.
Compared to implicit neural representations as another continuous spatial
representation with increasing attention, our method with flexible 3D Gaussians
presents enhanced accuracy on vorticity preservation. Moreover, we apply
physics-driven strategies to accelerate the optimization-based time integration
of Gaussian functions. This temporal evolution surpasses previous work based on
implicit neural representation with reduced computational time and memory.
Although not surpassing the quality of state-of-the-art Eulerian methods in
fluid simulation, experiments and ablation studies indicate the potential of
our memory-efficient representation. With enriched spatial information, our
method exhibits a distinctive perspective combining the advantages of Eulerian
and Lagrangian approaches.



---

## Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Xiangjun Gao, Xiaoyu Li, Yiyu Zhuang, Qi Zhang, Wenbo Hu, Chaopeng Zhang, Yao Yao, Ying Shan, Long Quan | cs.GR | [PDF](http://arxiv.org/pdf/2405.17811v1){: .btn .btn-green } |

**Abstract**: Neural 3D representations such as Neural Radiance Fields (NeRF), excel at
producing photo-realistic rendering results but lack the flexibility for
manipulation and editing which is crucial for content creation. Previous works
have attempted to address this issue by deforming a NeRF in canonical space or
manipulating the radiance field based on an explicit mesh. However,
manipulating NeRF is not highly controllable and requires a long training and
inference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely
high-fidelity novel view synthesis can be achieved using an explicit
point-based 3D representation with much faster training and rendering speed.
However, there is still a lack of effective means to manipulate 3DGS freely
while maintaining rendering quality. In this work, we aim to tackle the
challenge of achieving manipulable photo-realistic rendering. We propose to
utilize a triangular mesh to manipulate 3DGS directly with self-adaptation.
This approach reduces the need to design various algorithms for different types
of Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding
and adapting method, we can achieve 3DGS manipulation and preserve
high-fidelity rendering after manipulation. Our approach is capable of handling
large deformations, local manipulations, and soft body simulations while
keeping high-quality rendering. Furthermore, we demonstrate that our method is
also effective with inaccurate meshes extracted from 3DGS. Experiments
conducted demonstrate the effectiveness of our method and its superiority over
baseline approaches.

Comments:
- Project page here: https://gaoxiangjun.github.io/mani_gs/

---

## 3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Qihang Zhang, Yinghao Xu, Chaoyang Wang, Hsin-Ying Lee, Gordon Wetzstein, Bolei Zhou, Ceyuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2405.18424v1){: .btn .btn-green } |

**Abstract**: Scene image editing is crucial for entertainment, photography, and
advertising design. Existing methods solely focus on either 2D individual
object or 3D global scene editing. This results in a lack of a unified approach
to effectively control and manipulate scenes at the 3D level with different
levels of granularity. In this work, we propose 3DitScene, a novel and unified
scene editing framework leveraging language-guided disentangled Gaussian
Splatting that enables seamless editing from 2D to 3D, allowing precise control
over scene composition and individual objects. We first incorporate 3D
Gaussians that are refined through generative priors and optimization
techniques. Language features from CLIP then introduce semantics into 3D
geometry for object disentanglement. With the disentangled Gaussians, 3DitScene
allows for manipulation at both the global and individual levels,
revolutionizing creative expression and empowering control over scenes and
objects. Experimental results demonstrate the effectiveness and versatility of
3DitScene in scene image editing. Code and online demo can be found at our
project homepage: https://zqh0253.github.io/3DitScene/.



---

## SafeguardGS: 3D Gaussian Primitive Pruning While Avoiding Catastrophic  Scene Destruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Yongjae Lee, Zhaoliang Zhang, Deliang Fan | cs.CV | [PDF](http://arxiv.org/pdf/2405.17793v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has made a significant stride in novel view
synthesis, demonstrating top-notch rendering quality while achieving real-time
rendering speed. However, the excessively large number of Gaussian primitives
resulting from 3DGS' suboptimal densification process poses a major challenge,
slowing down frame-per-second (FPS) and demanding considerable memory cost,
making it unfavorable for low-end devices. To cope with this issue, many
follow-up studies have suggested various pruning techniques, often in
combination with different score functions, to optimize rendering performance.
Nonetheless, a comprehensive discussion regarding their effectiveness and
implications across all techniques is missing. In this paper, we first
categorize 3DGS pruning techniques into two types: Cross-view pruning and
pixel-wise pruning, which differ in their approaches to rank primitives. Our
subsequent experiments reveal that while cross-view pruning leads to disastrous
quality drops under extreme Gaussian primitives decimation, the pixel-wise
pruning technique not only sustains relatively high rendering quality with
minuscule performance degradation but also provides a reasonable minimum
boundary for pruning. Building on this observation, we further propose multiple
variations of score functions and empirically discover that the color-weighted
score function outperforms others for discriminating insignificant primitives
for rendering. We believe our research provides valuable insights for
optimizing 3DGS pruning strategies for future works.

Comments:
- Comprehensive experiments are in progress

---

## A Refined 3D Gaussian Representation for High-Quality Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Bin Zhang, Bi Zeng, Zexin Peng | cs.CV | [PDF](http://arxiv.org/pdf/2405.17891v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) has revolutionized
three-dimensional (3D) reconstruction with its implicit representation.
Building upon NeRF, 3D Gaussian Splatting (3D-GS) has departed from the
implicit representation of neural networks and instead directly represents
scenes as point clouds with Gaussian-shaped distributions. While this shift has
notably elevated the rendering quality and speed of radiance fields but
inevitably led to a significant increase in memory usage. Additionally,
effectively rendering dynamic scenes in 3D-GS has emerged as a pressing
challenge. To address these concerns, this paper purposes a refined 3D Gaussian
representation for high-quality dynamic scene reconstruction. Firstly, we use a
deformable multi-layer perceptron (MLP) network to capture the dynamic offset
of Gaussian points and express the color features of points through hash
encoding and a tiny MLP to reduce storage requirements. Subsequently, we
introduce a learnable denoising mask coupled with denoising loss to eliminate
noise points from the scene, thereby further compressing 3D Gaussian model.
Finally, motion noise of points is mitigated through static constraints and
motion consistency constraints. Experimental results demonstrate that our
method surpasses existing approaches in rendering quality and speed, while
significantly reducing the memory usage associated with 3D-GS, making it highly
suitable for various tasks such as novel view synthesis, and dynamic mapping.



---

## Self-supervised Pre-training for Transferable Multi-modal Perception

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Xiaohao Xu, Tianyi Zhang, Jinrong Yang, Matthew Johnson-Roberson, Xiaonan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2405.17942v1){: .btn .btn-green } |

**Abstract**: In autonomous driving, multi-modal perception models leveraging inputs from
multiple sensors exhibit strong robustness in degraded environments. However,
these models face challenges in efficiently and effectively transferring
learned representations across different modalities and tasks. This paper
presents NeRF-Supervised Masked Auto Encoder (NS-MAE), a self-supervised
pre-training paradigm for transferable multi-modal representation learning.
NS-MAE is designed to provide pre-trained model initializations for efficient
and high-performance fine-tuning. Our approach uses masked multi-modal
reconstruction in neural radiance fields (NeRF), training the model to
reconstruct missing or corrupted input data across multiple modalities.
Specifically, multi-modal embeddings are extracted from corrupted LiDAR point
clouds and images, conditioned on specific view directions and locations. These
embeddings are then rendered into projected multi-modal feature maps using
neural rendering techniques. The original multi-modal signals serve as
reconstruction targets for the rendered feature maps, facilitating
self-supervised representation learning. Extensive experiments demonstrate the
promising transferability of NS-MAE representations across diverse multi-modal
and single-modal perception models. This transferability is evaluated on
various 3D perception downstream tasks, such as 3D object detection and BEV map
segmentation, using different amounts of fine-tuning labeled data. Our code
will be released to support the community.

Comments:
- 8 pages. arXiv admin note: substantial text overlap with
  arXiv:2311.13750

---

## RT-GS2: Real-Time Generalizable Semantic Segmentation for 3D Gaussian  Representations of Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Mihnea-Bogdan Jurca, Remco Royen, Ion Giosan, Adrian Munteanu | cs.CV | [PDF](http://arxiv.org/pdf/2405.18033v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has revolutionized the world of novel view synthesis by
achieving high rendering performance in real-time. Recently, studies have
focused on enriching these 3D representations with semantic information for
downstream tasks. In this paper, we introduce RT-GS2, the first generalizable
semantic segmentation method employing Gaussian Splatting. While existing
Gaussian Splatting-based approaches rely on scene-specific training, RT-GS2
demonstrates the ability to generalize to unseen scenes. Our method adopts a
new approach by first extracting view-independent 3D Gaussian features in a
self-supervised manner, followed by a novel View-Dependent / View-Independent
(VDVI) feature fusion to enhance semantic consistency over different views.
Extensive experimentation on three different datasets showcases RT-GS2's
superiority over the state-of-the-art methods in semantic segmentation quality,
exemplified by a 8.01% increase in mIoU on the Replica dataset. Moreover, our
method achieves real-time performance of 27.03 FPS, marking an astonishing 901
times speedup compared to existing approaches. This work represents a
significant advancement in the field by introducing, to the best of our
knowledge, the first real-time generalizable semantic segmentation method for
3D Gaussian representations of radiance fields.



---

## EG4D: Explicit Generation of 4D Object without Score Distillation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Qi Sun, Zhiyang Guo, Ziyu Wan, Jing Nathan Yan, Shengming Yin, Wengang Zhou, Jing Liao, Houqiang Li | cs.CV | [PDF](http://arxiv.org/pdf/2405.18132v1){: .btn .btn-green } |

**Abstract**: In recent years, the increasing demand for dynamic 3D assets in design and
gaming applications has given rise to powerful generative pipelines capable of
synthesizing high-quality 4D objects. Previous methods generally rely on score
distillation sampling (SDS) algorithm to infer the unseen views and motion of
4D objects, thus leading to unsatisfactory results with defects like
over-saturation and Janus problem. Therefore, inspired by recent progress of
video diffusion models, we propose to optimize a 4D representation by
explicitly generating multi-view videos from one input image. However, it is
far from trivial to handle practical challenges faced by such a pipeline,
including dramatic temporal inconsistency, inter-frame geometry and texture
diversity, and semantic defects brought by video generation results. To address
these issues, we propose DG4D, a novel multi-stage framework that generates
high-quality and consistent 4D assets without score distillation. Specifically,
collaborative techniques and solutions are developed, including an attention
injection strategy to synthesize temporal-consistent multi-view videos, a
robust and efficient dynamic reconstruction method based on Gaussian Splatting,
and a refinement stage with diffusion prior for semantic restoration. The
qualitative results and user preference study demonstrate that our framework
outperforms the baselines in generation quality by a considerable margin. Code
will be released at \url{https://github.com/jasongzy/EG4D}.



---

## HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal  High-Frequency Components for Endoscopic Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Haoyu Zhao, Xingyue Zhao, Lingting Zhu, Weixi Zheng, Yongchao Xu | cs.CV | [PDF](http://arxiv.org/pdf/2405.17872v2){: .btn .btn-green } |

**Abstract**: Robot-assisted minimally invasive surgery benefits from enhancing dynamic
scene reconstruction, as it improves surgical outcomes. While Neural Radiance
Fields (NeRF) have been effective in scene reconstruction, their slow inference
speeds and lengthy training durations limit their applicability. To overcome
these limitations, 3D Gaussian Splatting (3D-GS) based methods have emerged as
a recent trend, offering rapid inference capabilities and superior 3D quality.
However, these methods still struggle with under-reconstruction in both static
and dynamic scenes. In this paper, we propose HFGS, a novel approach for
deformable endoscopic reconstruction that addresses these challenges from
spatial and temporal frequency perspectives. Our approach incorporates
deformation fields to better handle dynamic scenes and introduces Spatial
High-Frequency Emphasis Reconstruction (SHF) to minimize discrepancies in
spatial frequency spectra between the rendered image and its ground truth.
Additionally, we introduce Temporal High-Frequency Emphasis Reconstruction
(THF) to enhance dynamic awareness in neural rendering by leveraging flow
priors, focusing optimization on motion-intensive parts. Extensive experiments
on two widely used benchmarks demonstrate that HFGS achieves superior rendering
quality. Our code will be available.

Comments:
- 13 pages, 4 figures

---

## NegGS: Negative Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Artur Kasymov, Bartosz Czekaj, Marcin Mazur, Przemysław Spurek | cs.GR | [PDF](http://arxiv.org/pdf/2405.18163v1){: .btn .btn-green } |

**Abstract**: One of the key advantages of 3D rendering is its ability to simulate
intricate scenes accurately. One of the most widely used methods for this
purpose is Gaussian Splatting, a novel approach that is known for its rapid
training and inference capabilities. In essence, Gaussian Splatting involves
incorporating data about the 3D objects of interest into a series of Gaussian
distributions, each of which can then be depicted in 3D in a manner analogous
to traditional meshes. It is regrettable that the use of Gaussians in Gaussian
Splatting is currently somewhat restrictive due to their perceived linear
nature. In practice, 3D objects are often composed of complex curves and highly
nonlinear structures. This issue can to some extent be alleviated by employing
a multitude of Gaussian components to reflect the complex, nonlinear structures
accurately. However, this approach results in a considerable increase in time
complexity. This paper introduces the concept of negative Gaussians, which are
interpreted as items with negative colors. The rationale behind this approach
is based on the density distribution created by dividing the probability
density functions (PDFs) of two Gaussians, which we refer to as Diff-Gaussian.
Such a distribution can be used to approximate structures such as donut and
moon-shaped datasets. Experimental findings indicate that the application of
these techniques enhances the modeling of high-frequency elements with rapid
color transitions. Additionally, it improves the representation of shadows. To
the best of our knowledge, this is the first paper to extend the simple
elipsoid shapes of Gaussian Splatting to more complex nonlinear structures.



---

## GFlow: Recovering 4D World from Monocular Video

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Shizun Wang, Xingyi Yang, Qiuhong Shen, Zhenxiang Jiang, Xinchao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2405.18426v1){: .btn .btn-green } |

**Abstract**: Reconstructing 4D scenes from video inputs is a crucial yet challenging task.
Conventional methods usually rely on the assumptions of multi-view video
inputs, known camera parameters, or static scenes, all of which are typically
absent under in-the-wild scenarios. In this paper, we relax all these
constraints and tackle a highly ambitious but practical task, which we termed
as AnyV4D: we assume only one monocular video is available without any camera
parameters as input, and we aim to recover the dynamic 4D world alongside the
camera poses. To this end, we introduce GFlow, a new framework that utilizes
only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit
representation, entailing a flow of Gaussian splatting through space and time.
GFlow first clusters the scene into still and moving parts, then applies a
sequential optimization process that optimizes camera poses and the dynamics of
3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity
among neighboring points and smooth movement across frames. Since dynamic
scenes always introduce new content, we also propose a new pixel-wise
densification strategy for Gaussian points to integrate new visual content.
Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also
enables tracking of any points across frames without the need for prior
training and segments moving objects from the scene in an unsupervised way.
Additionally, the camera poses of each frame can be derived from GFlow,
allowing for rendering novel views of a video scene through changing camera
pose. By employing the explicit representation, we may readily conduct
scene-level or object-level editing as desired, underscoring its versatility
and power. Visit our project website at: https://littlepure2333.github.io/GFlow

Comments:
- Project page: https://littlepure2333.github.io/GFlow

---

## Deform3DGS: Flexible Deformation for Fast Surgical Scene Reconstruction  with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Shuojue Yang, Qian Li, Daiyun Shen, Bingchen Gong, Qi Dou, Yueming Jin | cs.CV | [PDF](http://arxiv.org/pdf/2405.17835v2){: .btn .btn-green } |

**Abstract**: Tissue deformation poses a key challenge for accurate surgical scene
reconstruction. Despite yielding high reconstruction quality, existing methods
suffer from slow rendering speeds and long training times, limiting their
intraoperative applicability. Motivated by recent progress in 3D Gaussian
Splatting, an emerging technology in real-time 3D rendering, this work presents
a novel fast reconstruction framework, termed Deform3DGS, for deformable
tissues during endoscopic surgery. Specifically, we introduce 3D GS into
surgical scenes by integrating a point cloud initialization to improve
reconstruction. Furthermore, we propose a novel flexible deformation modeling
scheme (FDM) to learn tissue deformation dynamics at the level of individual
Gaussians. Our FDM can model the surface deformation with efficient
representations, allowing for real-time rendering performance. More
importantly, FDM significantly accelerates surgical scene reconstruction,
demonstrating considerable clinical values, particularly in intraoperative
settings where time efficiency is crucial. Experiments on DaVinci robotic
surgery videos indicate the efficacy of our approach, showcasing superior
reconstruction fidelity PSNR: (37.90) and rendering speed (338.8 FPS) while
substantially reducing training time to only 1 minute/scene.

Comments:
- 10 pages, 2 figures, conference paper

---

## 3D StreetUnveiler with Semantic-Aware 2DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Jingwei Xu, Yikai Wang, Yiqun Zhao, Yanwei Fu, Shenghua Gao | cs.CV | [PDF](http://arxiv.org/pdf/2405.18416v1){: .btn .btn-green } |

**Abstract**: Unveiling an empty street from crowded observations captured by in-car
cameras is crucial for autonomous driving. However, removing all temporary
static objects, such as stopped vehicles and standing pedestrians, presents a
significant challenge. Unlike object-centric 3D inpainting, which relies on
thorough observation in a small scene, street scenes involve long trajectories
that differ from previous 3D inpainting tasks. The camera-centric moving
environment of captured videos further complicates the task due to the limited
degree and time duration of object observation. To address these obstacles, we
introduce StreetUnveiler to reconstruct an empty street. StreetUnveiler learns
a 3D representation of the empty street from crowded observations. Our
representation is based on the hard-label semantic 2D Gaussian Splatting (2DGS)
for its scalability and ability to identify Gaussians to be removed. We inpaint
rendered image after removing unwanted Gaussians to provide pseudo-labels and
subsequently re-optimize the 2DGS. Given its temporal continuous movement, we
divide the empty street scene into observed, partial-observed, and unobserved
regions, which we propose to locate through a rendered alpha map. This
decomposition helps us to minimize the regions that need to be inpainted. To
enhance the temporal consistency of the inpainting, we introduce a novel
time-reversal framework to inpaint frames in reverse order and use later frames
as references for earlier frames to fully utilize the long-trajectory
observations. Our experiments conducted on the street scene dataset
successfully reconstructed a 3D representation of the empty street. The mesh
representation of the empty street can be extracted for further applications.
Project page and more visualizations can be found at:
https://streetunveiler.github.io



---

## FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View  Synthesis of Indoor Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-28 | Yunsong Wang, Tianxin Huang, Hanlin Chen, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2405.17958v1){: .btn .btn-green } |

**Abstract**: Empowering 3D Gaussian Splatting with generalization ability is appealing.
However, existing generalizable 3D Gaussian Splatting methods are largely
confined to narrow-range interpolation between stereo images due to their heavy
backbones, thus lacking the ability to accurately localize 3D Gaussian and
support free-view synthesis across wide view range. In this paper, we present a
novel framework FreeSplat that is capable of reconstructing geometrically
consistent 3D scenes from long sequence input towards free-view
synthesis.Specifically, we firstly introduce Low-cost Cross-View Aggregation
achieved by constructing adaptive cost volumes among nearby views and
aggregating features using a multi-scale structure. Subsequently, we present
the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in
overlapping view regions and to aggregate features observed across multiple
views. Additionally, we propose a simple but effective free-view training
strategy that ensures robust view synthesis across broader view range
regardless of the number of views. Our empirical results demonstrate
state-of-the-art novel view synthesis peformances in both novel view rendered
color maps quality and depth maps accuracy across different numbers of input
views. We also show that FreeSplat performs inference more efficiently and can
effectively reduce redundant Gaussians, offering the possibility of
feed-forward large scene reconstruction without depth priors.



---

## DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam  Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-27 | Linhan Wang, Kai Cheng, Shuo Lei, Shengkun Wang, Wei Yin, Chenyang Lei, Xiaoxiao Long, Chang-Tien Lu | cs.CV | [PDF](http://arxiv.org/pdf/2405.17705v2){: .btn .btn-green } |

**Abstract**: We present DC-Gaussian, a new method for generating novel views from
in-vehicle dash cam videos. While neural rendering techniques have made
significant strides in driving scenarios, existing methods are primarily
designed for videos collected by autonomous vehicles. However, these videos are
limited in both quantity and diversity compared to dash cam videos, which are
more widely used across various types of vehicles and capture a broader range
of scenarios. Dash cam videos often suffer from severe obstructions such as
reflections and occlusions on the windshields, which significantly impede the
application of neural rendering techniques. To address this challenge, we
develop DC-Gaussian based on the recent real-time neural rendering technique 3D
Gaussian Splatting (3DGS). Our approach includes an adaptive image
decomposition module to model reflections and occlusions in a unified manner.
Additionally, we introduce illumination-aware obstruction modeling to manage
reflections and occlusions under varying lighting conditions. Lastly, we employ
a geometry-guided Gaussian enhancement strategy to improve rendering details by
incorporating additional geometry priors. Experiments on self-captured and
public dash cam videos show that our method not only achieves state-of-the-art
performance in novel view synthesis, but also accurately reconstructing
captured scenes getting rid of obstructions.

Comments:
- 9 pages,7 figures;project page:
  https://linhanwang.github.io/dcgaussian/

---

## SA-GS: Semantic-Aware Gaussian Splatting for Large Scene Reconstruction  with Geometry Constrain

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-27 | Butian Xiong, Xiaoyu Ye, Tze Ho Elden Tse, Kai Han, Shuguang Cui, Zhen Li | cs.CV | [PDF](http://arxiv.org/pdf/2405.16923v2){: .btn .btn-green } |

**Abstract**: With the emergence of Gaussian Splats, recent efforts have focused on
large-scale scene geometric reconstruction. However, most of these efforts
either concentrate on memory reduction or spatial space division, neglecting
information in the semantic space. In this paper, we propose a novel method,
named SA-GS, for fine-grained 3D geometry reconstruction using semantic-aware
3D Gaussian Splats. Specifically, we leverage prior information stored in large
vision models such as SAM and DINO to generate semantic masks. We then
introduce a geometric complexity measurement function to serve as soft
regularization, guiding the shape of each Gaussian Splat within specific
semantic areas. Additionally, we present a method that estimates the expected
number of Gaussian Splats in different semantic areas, effectively providing a
lower bound for Gaussian Splats in these areas. Subsequently, we extract the
point cloud using a novel probability density-based extraction method,
transforming Gaussian Splats into a point cloud crucial for downstream tasks.
Our method also offers the potential for detailed semantic inquiries while
maintaining high image-based reconstruction results. We provide extensive
experiments on publicly available large-scale scene reconstruction datasets
with highly accurate point clouds as ground truth and our novel dataset. Our
results demonstrate the superiority of our method over current state-of-the-art
Gaussian Splats reconstruction methods by a significant margin in terms of
geometric-based measurement metrics. Code and additional results will soon be
available on our project page.

Comments:
- Might need more comparison, will be add later

---

## F-3DGS: Factorized Coordinates and Representations for 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-27 | Xiangyu Sun, Joo Chan Lee, Daniel Rho, Jong Hwan Ko, Usman Ali, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2405.17083v2){: .btn .btn-green } |

**Abstract**: The neural radiance field (NeRF) has made significant strides in representing
3D scenes and synthesizing novel views. Despite its advancements, the high
computational costs of NeRF have posed challenges for its deployment in
resource-constrained environments and real-time applications. As an alternative
to NeRF-like neural rendering methods, 3D Gaussian Splatting (3DGS) offers
rapid rendering speeds while maintaining excellent image quality. However, as
it represents objects and scenes using a myriad of Gaussians, it requires
substantial storage to achieve high-quality representation. To mitigate the
storage overhead, we propose Factorized 3D Gaussian Splatting (F-3DGS), a novel
approach that drastically reduces storage requirements while preserving image
quality. Inspired by classical matrix and tensor factorization techniques, our
method represents and approximates dense clusters of Gaussians with
significantly fewer Gaussians through efficient factorization. We aim to
efficiently represent dense 3D Gaussians by approximating them with a limited
amount of information for each axis and their combinations. This method allows
us to encode a substantially large number of Gaussians along with their
essential attributes -- such as color, scale, and rotation -- necessary for
rendering using a relatively small number of elements. Extensive experimental
results demonstrate that F-3DGS achieves a significant reduction in storage
costs while maintaining comparable quality in rendered images.

Comments:
- Our project page including code is available at
  https://xiangyu1sun.github.io/Factorize-3DGS/

---

## GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary  Semantic-space Hyperplane

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-27 | Yansong Qu, Shaohui Dai, Xinyang Li, Jianghang Lin, Liujuan Cao, Shengchuan Zhang, Rongrong Ji | cs.CV | [PDF](http://arxiv.org/pdf/2405.17596v1){: .btn .btn-green } |

**Abstract**: 3D open-vocabulary scene understanding, crucial for advancing augmented
reality and robotic applications, involves interpreting and locating specific
regions within a 3D space as directed by natural language instructions. To this
end, we introduce GOI, a framework that integrates semantic features from 2D
vision-language foundation models into 3D Gaussian Splatting (3DGS) and
identifies 3D Gaussians of Interest using an Optimizable Semantic-space
Hyperplane. Our approach includes an efficient compression method that utilizes
scene priors to condense noisy high-dimensional semantic features into compact
low-dimensional vectors, which are subsequently embedded in 3DGS. During the
open-vocabulary querying process, we adopt a distinct approach compared to
existing methods, which depend on a manually set fixed empirical threshold to
select regions based on their semantic feature distance to the query text
embedding. This traditional approach often lacks universal accuracy, leading to
challenges in precisely identifying specific target areas. Instead, our method
treats the feature selection process as a hyperplane division within the
feature space, retaining only those features that are highly relevant to the
query. We leverage off-the-shelf 2D Referring Expression Segmentation (RES)
models to fine-tune the semantic-space hyperplane, enabling a more precise
distinction between target regions and others. This fine-tuning substantially
improves the accuracy of open-vocabulary queries, ensuring the precise
localization of pertinent 3D Gaussians. Extensive experiments demonstrate GOI's
superiority over previous state-of-the-art methods. Our project page is
available at https://goi-hyperplane.github.io/ .

Comments:
- Our project page is available at https://goi-hyperplane.github.io/

---

## MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion  Scaffolds

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-27 | Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, Kostas Daniilidis | cs.CV | [PDF](http://arxiv.org/pdf/2405.17421v1){: .btn .btn-green } |

**Abstract**: We introduce 4D Motion Scaffolds (MoSca), a neural information processing
system designed to reconstruct and synthesize novel views of dynamic scenes
from monocular videos captured casually in the wild. To address such a
challenging and ill-posed inverse problem, we leverage prior knowledge from
foundational vision models, lift the video data to a novel Motion Scaffold
(MoSca) representation, which compactly and smoothly encodes the underlying
motions / deformations. The scene geometry and appearance are then disentangled
from the deformation field, and are encoded by globally fusing the Gaussians
anchored onto the MoSca and optimized via Gaussian Splatting. Additionally,
camera poses can be seamlessly initialized and refined during the dynamic
rendering process, without the need for other pose estimation tools.
Experiments demonstrate state-of-the-art performance on dynamic rendering
benchmarks.

Comments:
- project page: https://www.cis.upenn.edu/~leijh/projects/mosca

---

## DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for  Refocusing,Defocus Rendering and Blur Removal

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-27 | Yujie Wang, Praneeth Chakravarthula, Baoquan Chen | cs.CV | [PDF](http://arxiv.org/pdf/2405.17351v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting-based techniques have recently advanced 3D scene
reconstruction and novel view synthesis, achieving high-quality real-time
rendering. However, these approaches are inherently limited by the underlying
pinhole camera assumption in modeling the images and hence only work for
All-in-Focus (AiF) sharp image inputs. This severely affects their
applicability in real-world scenarios where images often exhibit defocus blur
due to the limited depth-of-field (DOF) of imaging devices. Additionally,
existing 3D Gaussian Splatting (3DGS) methods also do not support rendering of
DOF effects.
  To address these challenges, we introduce DOF-GS that allows for rendering
adjustable DOF effects, removing defocus blur as well as refocusing of 3D
scenes, all from multi-view images degraded by defocus blur. To this end, we
re-imagine the traditional Gaussian Splatting pipeline by employing a finite
aperture camera model coupled with explicit, differentiable defocus rendering
guided by the Circle-of-Confusion (CoC). The proposed framework provides for
dynamic adjustment of DOF effects by changing the aperture and focal distance
of the underlying camera model on-demand. It also enables rendering varying DOF
effects of 3D scenes post-optimization, and generating AiF images from
defocused training images. Furthermore, we devise a joint optimization strategy
to further enhance details in the reconstructed scenes by jointly optimizing
rendered defocused and AiF images. Our experimental results indicate that
DOF-GS produces high-quality sharp all-in-focus renderings conditioned on
inputs compromised by defocus blur, with the training process incurring only a
modest increase in GPU memory consumption. We further demonstrate the
applications of the proposed method for adjustable defocus rendering and
refocusing of the 3D scene from input images degraded by defocus blur.



---

## Memorize What Matters: Emergent Scene Decomposition from Multitraverse

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-27 | Yiming Li, Zehong Wang, Yue Wang, Zhiding Yu, Zan Gojcic, Marco Pavone, Chen Feng, Jose M. Alvarez | cs.CV | [PDF](http://arxiv.org/pdf/2405.17187v1){: .btn .btn-green } |

**Abstract**: Humans naturally retain memories of permanent elements, while ephemeral
moments often slip through the cracks of memory. This selective retention is
crucial for robotic perception, localization, and mapping. To endow robots with
this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised,
camera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM
converts multitraverse RGB videos from the same region into a Gaussian-based
environmental map while concurrently performing 2D ephemeral object
segmentation. Our key observation is that the environment remains consistent
across traversals, while objects frequently change. This allows us to exploit
self-supervision from repeated traversals to achieve environment-object
decomposition. More specifically, 3DGM formulates multitraverse environmental
mapping as a robust differentiable rendering problem, treating pixels of the
environment and objects as inliers and outliers, respectively. Using robust
feature distillation, feature residuals mining, and robust optimization, 3DGM
jointly performs 3D mapping and 2D segmentation without human intervention. We
build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets,
to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and
neural rendering. Extensive results verify the effectiveness and potential of
our method for self-driving and robotics.

Comments:
- Project page: https://3d-gaussian-mapping.github.io

---

## PyGS: Large-scale Scene Representation with Pyramidal 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-27 | Zipeng Wang, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2405.16829v3){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in
synthesizing photorealistic images of large-scale scenes. However, they are
often plagued by a loss of fine details and long rendering durations. 3D
Gaussian Splatting has recently been introduced as a potent alternative,
achieving both high-fidelity visual results and accelerated rendering
performance. Nonetheless, scaling 3D Gaussian Splatting is fraught with
challenges. Specifically, large-scale scenes grapples with the integration of
objects across multiple scales and disparate viewpoints, which often leads to
compromised efficacy as the Gaussians need to balance between detail levels.
Furthermore, the generation of initialization points via COLMAP from
large-scale dataset is both computationally demanding and prone to incomplete
reconstructions. To address these challenges, we present Pyramidal 3D Gaussian
Splatting (PyGS) with NeRF Initialization. Our approach represent the scene
with a hierarchical assembly of Gaussians arranged in a pyramidal fashion. The
top level of the pyramid is composed of a few large Gaussians, while each
subsequent layer accommodates a denser collection of smaller Gaussians. We
effectively initialize these pyramidal Gaussians through sampling a rapidly
trained grid-based NeRF at various frequencies. We group these pyramidal
Gaussians into clusters and use a compact weighting network to dynamically
determine the influence of each pyramid level of each cluster considering
camera viewpoint during rendering. Our method achieves a significant
performance leap across multiple large-scale datasets and attains a rendering
time that is over 400 times faster than current state-of-the-art approaches.



---

## Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-26 | Erik Sandström, Keisuke Tateno, Michael Oechsle, Michael Niemeyer, Luc Van Gool, Martin R. Oswald, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2405.16544v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has emerged as a powerful representation of geometry
and appearance for RGB-only dense Simultaneous Localization and Mapping (SLAM),
as it provides a compact dense map representation while enabling efficient and
high-quality map rendering. However, existing methods show significantly worse
reconstruction quality than competing methods using other 3D representations,
e.g. neural points clouds, since they either do not employ global map and pose
optimization or make use of monocular depth. In response, we propose the first
RGB-only SLAM system with a dense 3D Gaussian map representation that utilizes
all benefits of globally optimized tracking by adapting dynamically to keyframe
pose and depth updates by actively deforming the 3D Gaussian map. Moreover, we
find that refining the depth updates in inaccurate areas with a monocular depth
estimator further improves the accuracy of the 3D reconstruction. Our
experiments on the Replica, TUM-RGBD, and ScanNet datasets indicate the
effectiveness of globally optimized 3D Gaussians, as the approach achieves
superior or on par performance with existing RGB-only SLAM methods methods in
tracking, mapping and rendering accuracy while yielding small map sizes and
fast runtimes. The source code is available at
https://github.com/eriksandstroem/Splat-SLAM.

Comments:
- 21 pages

---

## Diffusion4D: Fast Spatial-temporal Consistent 4D Generation via Video  Diffusion Models


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-26 | Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos N. Plataniotis, Yao Zhao, Yunchao Wei | cs.CV | [PDF](http://arxiv.org/pdf/2405.16645v1){: .btn .btn-green } |

**Abstract**: The availability of large-scale multimodal datasets and advancements in
diffusion models have significantly accelerated progress in 4D content
generation. Most prior approaches rely on multiple image or video diffusion
models, utilizing score distillation sampling for optimization or generating
pseudo novel views for direct supervision. However, these methods are hindered
by slow optimization speeds and multi-view inconsistency issues. Spatial and
temporal consistency in 4D geometry has been extensively explored respectively
in 3D-aware diffusion models and traditional monocular video diffusion models.
Building on this foundation, we propose a strategy to migrate the temporal
consistency in video diffusion models to the spatial-temporal consistency
required for 4D generation. Specifically, we present a novel framework,
\textbf{Diffusion4D}, for efficient and scalable 4D content generation.
Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware
video diffusion model capable of synthesizing orbital views of dynamic 3D
assets. To control the dynamic strength of these assets, we introduce a
3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel
motion magnitude reconstruction loss and 3D-aware classifier-free guidance to
refine the learning and generation of motion dynamics. After obtaining orbital
views of the 4D asset, we perform explicit 4D construction with Gaussian
splatting in a coarse-to-fine manner. The synthesized multi-view consistent 4D
image set enables us to swiftly generate high-fidelity and diverse 4D assets
within just several minutes. Extensive experiments demonstrate that our method
surpasses prior state-of-the-art techniques in terms of generation efficiency
and 4D geometry consistency across various prompt modalities.

Comments:
- Project page: https://vita-group.github.io/Diffusion4D

---

## Sp2360: Sparse-view 360 Scene Reconstruction using Cascaded 2D Diffusion  Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-26 | Soumava Paul, Christopher Wewer, Bernt Schiele, Jan Eric Lenssen | cs.CV | [PDF](http://arxiv.org/pdf/2405.16517v1){: .btn .btn-green } |

**Abstract**: We aim to tackle sparse-view reconstruction of a 360 3D scene using priors
from latent diffusion models (LDM). The sparse-view setting is ill-posed and
underconstrained, especially for scenes where the camera rotates 360 degrees
around a point, as no visual information is available beyond some frontal views
focused on the central object(s) of interest. In this work, we show that
pretrained 2D diffusion models can strongly improve the reconstruction of a
scene with low-cost fine-tuning. Specifically, we present SparseSplat360
(Sp2360), a method that employs a cascade of in-painting and artifact removal
models to fill in missing details and clean novel views. Due to superior
training and rendering speeds, we use an explicit scene representation in the
form of 3D Gaussians over NeRF-based implicit representations. We propose an
iterative update strategy to fuse generated pseudo novel views with existing 3D
Gaussians fitted to the initial sparse inputs. As a result, we obtain a
multi-view consistent scene representation with details coherent with the
observed inputs. Our evaluation on the challenging Mip-NeRF360 dataset shows
that our proposed 2D to 3D distillation algorithm considerably improves the
performance of a regularized version of 3DGS adapted to a sparse-view setting
and outperforms existing sparse-view reconstruction methods in 360 scene
reconstruction. Qualitatively, our method generates entire 360 scenes from as
few as 9 input views, with a high degree of foreground and background detail.

Comments:
- 18 pages, 10 figures, 4 tables

---

## HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed  via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-24 | Yuanhao Cai, Zihao Xiao, Yixun Liang, Minghan Qin, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, Alan Yuille | cs.CV | [PDF](http://arxiv.org/pdf/2405.15125v2){: .btn .btn-green } |

**Abstract**: High dynamic range (HDR) novel view synthesis (NVS) aims to create
photorealistic images from novel viewpoints using HDR imaging techniques. The
rendered HDR images capture a wider range of brightness levels containing more
details of the scene than normal low dynamic range (LDR) images. Existing HDR
NVS methods are mainly based on NeRF. They suffer from long training time and
slow inference speed. In this paper, we propose a new framework, High Dynamic
Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views
and reconstruct LDR images with a user input exposure time. Specifically, we
design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses
spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to
render LDR color. The HDR and LDR colors are then fed into two Parallel
Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views.
To establish the data foundation for the research of 3D Gaussian
splatting-based methods in HDR NVS, we recalibrate the camera parameters and
compute the initial positions for Gaussian point clouds. Experiments
demonstrate that our HDR-GS surpasses the state-of-the-art NeRF-based method by
3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000x inference speed and
only requiring 6.3% training time. Code, models, and recalibrated data will be
publicly available at https://github.com/caiyuanhao1998/HDR-GS

Comments:
- The first 3D Gaussian Splatting-based method for HDR imaging

---

## Feature Splatting for Better Novel View Synthesis with Low Overlap

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-24 | T. Berriel Martins, Javier Civera | cs.CV | [PDF](http://arxiv.org/pdf/2405.15518v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting has emerged as a very promising scene representation,
achieving state-of-the-art quality in novel view synthesis significantly faster
than competing alternatives. However, its use of spherical harmonics to
represent scene colors limits the expressivity of 3D Gaussians and, as a
consequence, the capability of the representation to generalize as we move away
from the training views. In this paper, we propose to encode the color
information of 3D Gaussians into per-Gaussian feature vectors, which we denote
as Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are
first "splatted" into the image plane, then the corresponding feature vectors
are alpha-blended, and finally the blended vector is decoded by a small MLP to
render the RGB pixel values. To further inform the model, we concatenate a
camera embedding to the blended feature vector, to condition the decoding also
on the viewpoint information. Our experiments show that these novel model for
encoding the radiance considerably improves novel view synthesis for low
overlap views that are distant from the training views. Finally, we also show
the capacity and convenience of our feature vector representation,
demonstrating its capability not only to generate RGB values for novel views,
but also their per-pixel semantic labels. We will release the code upon
acceptance.
  Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting



---

## GS-Hider: Hiding Messages into 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-24 | Xuanyu Zhang, Jiarui Meng, Runyi Li, Zhipei Xu, Yongbing Zhang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2405.15118v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has already become the emerging research focus
in the fields of 3D scene reconstruction and novel view synthesis. Given that
training a 3DGS requires a significant amount of time and computational cost,
it is crucial to protect the copyright, integrity, and privacy of such 3D
assets. Steganography, as a crucial technique for encrypted transmission and
copyright protection, has been extensively studied. However, it still lacks
profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS
possesses two distinct features: 1) explicit 3D representation; and 2)
real-time rendering speeds. These characteristics result in the 3DGS point
cloud files being public and transparent, with each Gaussian point having a
clear physical significance. Therefore, ensuring the security and fidelity of
the original 3D scene while embedding information into the 3DGS point cloud
files is an extremely challenging task. To solve the above-mentioned issue, we
first propose a steganography framework for 3DGS, dubbed GS-Hider, which can
embed 3D scenes and images into original GS point clouds in an invisible manner
and accurately extract the hidden messages. Specifically, we design a coupled
secured feature attribute to replace the original 3DGS's spherical harmonics
coefficients and then use a scene decoder and a message decoder to disentangle
the original RGB scene and the hidden message. Extensive experiments
demonstrated that the proposed GS-Hider can effectively conceal multimodal
messages without compromising rendering quality and possesses exceptional
security, robustness, capacity, and flexibility. Our project is available at:
https://xuanyuzhang21.github.io/project/gshider.

Comments:
- 3DGS steganography

---

## GSDeformer: Direct Cage-based Deformation for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-24 | Jiajun Huang, Hongchuan Yu | cs.CV | [PDF](http://arxiv.org/pdf/2405.15491v1){: .btn .btn-green } |

**Abstract**: We present GSDeformer, a method that achieves free-form deformation on 3D
Gaussian Splatting(3DGS) without requiring any architectural changes. Our
method extends cage-based deformation, a traditional mesh deformation method,
to 3DGS. This is done by converting 3DGS into a novel proxy point cloud
representation, where its deformation can be used to infer the transformations
to apply on the 3D gaussians making up 3DGS. We also propose an automatic cage
construction algorithm for 3DGS to minimize manual work. Our method does not
modify the underlying architecture of 3DGS. Therefore, any existing trained
vanilla 3DGS can be easily edited by our method. We compare the deformation
capability of our method against other existing methods, demonstrating the ease
of use and comparable quality of our method, despite being more direct and thus
easier to integrate with other concurrent developments on 3DGS.

Comments:
- For project page, see https://jhuangbu.github.io/gsdeformer

---

## Neural Elevation Models for Terrain Mapping and Path Planning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-24 | Adam Dai, Shubh Gupta, Grace Gao | cs.RO | [PDF](http://arxiv.org/pdf/2405.15227v1){: .btn .btn-green } |

**Abstract**: This work introduces Neural Elevations Models (NEMos), which adapt Neural
Radiance Fields to a 2.5D continuous and differentiable terrain model. In
contrast to traditional terrain representations such as digital elevation
models, NEMos can be readily generated from imagery, a low-cost data source,
and provide a lightweight representation of terrain through an implicit
continuous and differentiable height field. We propose a novel method for
jointly training a height field and radiance field within a NeRF framework,
leveraging quantile regression. Additionally, we introduce a path planning
algorithm that performs gradient-based optimization of a continuous cost
function for minimizing distance, slope changes, and control effort, enabled by
differentiability of the height field. We perform experiments on simulated and
real-world terrain imagery, demonstrating NEMos ability to generate
high-quality reconstructions and produce smoother paths compared to discrete
path planning methods. Future work will explore the incorporation of features
and semantics into the height field, creating a generalized terrain model.



---

## DisC-GS: Discontinuity-aware Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-24 | Haoxuan Qu, Zhuoling Li, Hossein Rahmani, Yujun Cai, Jun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2405.15196v1){: .btn .btn-green } |

**Abstract**: Recently, Gaussian Splatting, a method that represents a 3D scene as a
collection of Gaussian distributions, has gained significant attention in
addressing the task of novel view synthesis. In this paper, we highlight a
fundamental limitation of Gaussian Splatting: its inability to accurately
render discontinuities and boundaries in images due to the continuous nature of
Gaussian distributions. To address this issue, we propose a novel framework
enabling Gaussian Splatting to perform discontinuity-aware image rendering.
Additionally, we introduce a B\'ezier-boundary gradient approximation strategy
within our framework to keep the ``differentiability'' of the proposed
discontinuity-aware rendering process. Extensive experiments demonstrate the
efficacy of our framework.



---

## Tele-Aloha: A Low-budget and High-authenticity Telepresence System Using  Sparse RGB Cameras

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Hanzhang Tu, Ruizhi Shao, Xue Dong, Shunyuan Zheng, Hao Zhang, Lili Chen, Meili Wang, Wenyu Li, Siyan Ma, Shengping Zhang, Boyao Zhou, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2405.14866v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a low-budget and high-authenticity bidirectional
telepresence system, Tele-Aloha, targeting peer-to-peer communication
scenarios. Compared to previous systems, Tele-Aloha utilizes only four sparse
RGB cameras, one consumer-grade GPU, and one autostereoscopic screen to achieve
high-resolution (2048x2048), real-time (30 fps), low-latency (less than 150ms)
and robust distant communication. As the core of Tele-Aloha, we propose an
efficient novel view synthesis algorithm for upper-body. Firstly, we design a
cascaded disparity estimator for obtaining a robust geometry cue. Additionally
a neural rasterizer via Gaussian Splatting is introduced to project latent
features onto target view and to decode them into a reduced resolution.
Further, given the high-quality captured data, we leverage weighted blending
mechanism to refine the decoded image into the final resolution of 2K.
Exploiting world-leading autostereoscopic display and low-latency iris
tracking, users are able to experience a strong three-dimensional sense even
without any wearable head-mounted display device. Altogether, our telepresence
system demonstrates the sense of co-presence in real-life experiments,
inspiring the next generation of communication.

Comments:
- Paper accepted by SIGGRAPH 2024. Project page:
  http://118.178.32.38/c/Tele-Aloha/

---

## EvGGS: A Collaborative Learning Framework for Event-based Generalizable  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Jiaxu Wang, Junhao He, Ziyi Zhang, Mingyuan Sun, Jingkai Sun, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2405.14959v1){: .btn .btn-green } |

**Abstract**: Event cameras offer promising advantages such as high dynamic range and low
latency, making them well-suited for challenging lighting conditions and
fast-moving scenarios. However, reconstructing 3D scenes from raw event streams
is difficult because event data is sparse and does not carry absolute color
information. To release its potential in 3D reconstruction, we propose the
first event-based generalizable 3D reconstruction framework, called EvGGS,
which reconstructs scenes as 3D Gaussians from only event input in a
feedforward manner and can generalize to unseen cases without any retraining.
This framework includes a depth estimation module, an intensity reconstruction
module, and a Gaussian regression module. These submodules connect in a
cascading manner, and we collaboratively train them with a designed joint loss
to make them mutually promote. To facilitate related studies, we build a novel
event-based 3D dataset with various material objects and calibrated labels of
grayscale images, depth maps, camera poses, and silhouettes. Experiments show
models that have jointly trained significantly outperform those trained
individually. Our approach performs better than all baselines in reconstruction
quality, and depth/intensity predictions with satisfactory rendering speed.



---

## NeRF-Casting: Improved View-Dependent Appearance with Consistent  Reflections

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Dor Verbin, Pratul P. Srinivasan, Peter Hedman, Ben Mildenhall, Benjamin Attal, Richard Szeliski, Jonathan T. Barron | cs.CV | [PDF](http://arxiv.org/pdf/2405.14871v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) typically struggle to reconstruct and render
highly specular objects, whose appearance varies quickly with changes in
viewpoint. Recent works have improved NeRF's ability to render detailed
specular appearance of distant environment illumination, but are unable to
synthesize consistent reflections of closer content. Moreover, these techniques
rely on large computationally-expensive neural networks to model outgoing
radiance, which severely limits optimization and rendering speed. We address
these issues with an approach based on ray tracing: instead of querying an
expensive neural network for the outgoing view-dependent radiance at points
along each camera ray, our model casts reflection rays from these points and
traces them through the NeRF representation to render feature vectors which are
decoded into color using a small inexpensive network. We demonstrate that our
model outperforms prior methods for view synthesis of scenes containing shiny
objects, and that it is the only existing NeRF method that can synthesize
photorealistic specular appearance and reflections in real-world scenes, while
requiring comparable optimization time to current state-of-the-art view
synthesis models.

Comments:
- Project page: http://nerf-casting.github.io

---

## RoGS: Large Scale Road Surface Reconstruction based on 2D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Zhiheng Feng, Wenhua Wu, Hesheng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2405.14342v2){: .btn .btn-green } |

**Abstract**: Road surface reconstruction plays a crucial role in autonomous driving, which
can be used for road lane perception and autolabeling tasks. Recently,
mesh-based road surface reconstruction algorithms show promising reconstruction
results. However, these mesh-based methods suffer from slow speed and poor
rendering quality. In contrast, the 3D Gaussian Splatting (3DGS) shows superior
rendering speed and quality. Although 3DGS employs explicit Gaussian spheres to
represent the scene, it lacks the ability to directly represent the geometric
information of the scene. To address this limitation, we propose a novel
large-scale road surface reconstruction approach based on 2D Gaussian Splatting
(2DGS), named RoGS. The geometric shape of the road is explicitly represented
using 2D Gaussian surfels, where each surfel stores color, semantics, and
geometric information. Compared to Gaussian spheres, the Gaussian surfels
aligns more closely with the physical reality of the road. Distinct from
previous initialization methods that rely on point clouds for Gaussian spheres,
we introduce a trajectory-based initialization for Gaussian surfels. Thanks to
the explicit representation of the Gaussian surfels and a good initialization,
our method achieves a significant acceleration while improving reconstruction
quality. We achieve excellent results in reconstruction of roads surfaces in a
variety of challenging real-world scenes.



---

## JointRF: End-to-End Joint Optimization for Dynamic Neural Radiance Field  Representation and Compression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Zihan Zheng, Houqiang Zhong, Qiang Hu, Xiaoyun Zhang, Li Song, Ya Zhang, Yanfeng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2405.14452v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) excels in photo-realistically static scenes,
inspiring numerous efforts to facilitate volumetric videos. However, rendering
dynamic and long-sequence radiance fields remains challenging due to the
significant data required to represent volumetric videos. In this paper, we
propose a novel end-to-end joint optimization scheme of dynamic NeRF
representation and compression, called JointRF, thus achieving significantly
improved quality and compression efficiency against the previous methods.
Specifically, JointRF employs a compact residual feature grid and a coefficient
feature grid to represent the dynamic NeRF. This representation handles large
motions without compromising quality while concurrently diminishing temporal
redundancy. We also introduce a sequential feature compression subnetwork to
further reduce spatial-temporal redundancy. Finally, the representation and
compression subnetworks are end-to-end trained combined within the JointRF.
Extensive experiments demonstrate that JointRF can achieve superior compression
performance across various datasets.

Comments:
- 8 pages, 5 figures

---

## Camera Relocalization in Shadow-free Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Shiyao Xu, Caiyun Liu, Yuantao Chen, Zhenxin Zhu, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2405.14824v1){: .btn .btn-green } |

**Abstract**: Camera relocalization is a crucial problem in computer vision and robotics.
Recent advancements in neural radiance fields (NeRFs) have shown promise in
synthesizing photo-realistic images. Several works have utilized NeRFs for
refining camera poses, but they do not account for lighting changes that can
affect scene appearance and shadow regions, causing a degraded pose
optimization process. In this paper, we propose a two-staged pipeline that
normalizes images with varying lighting and shadow conditions to improve camera
relocalization. We implement our scene representation upon a hash-encoded NeRF
which significantly boosts up the pose optimization process. To account for the
noisy image gradient computing problem in grid-based NeRFs, we further propose
a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient
averaging technique to smoothen the process. Experimental results on several
datasets with varying lighting conditions demonstrate that our method achieves
state-of-the-art results in camera relocalization under varying lighting
conditions. Code and data will be made publicly available.

Comments:
- Accepted by ICRA 2024. 8 pages, 5 figures, 3 tables. Codes and
  dataset: https://github.com/hnrna/ShadowfreeNeRF-CameraReloc

---

## Neural Directional Encoding for Efficient and Accurate View-Dependent  Appearance Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Liwen Wu, Sai Bi, Zexiang Xu, Fujun Luan, Kai Zhang, Iliyan Georgiev, Kalyan Sunkavalli, Ravi Ramamoorthi | cs.CV | [PDF](http://arxiv.org/pdf/2405.14847v1){: .btn .btn-green } |

**Abstract**: Novel-view synthesis of specular objects like shiny metals or glossy paints
remains a significant challenge. Not only the glossy appearance but also global
illumination effects, including reflections of other objects in the
environment, are critical components to faithfully reproduce a scene. In this
paper, we present Neural Directional Encoding (NDE), a view-dependent
appearance encoding of neural radiance fields (NeRF) for rendering specular
objects. NDE transfers the concept of feature-grid-based spatial encoding to
the angular domain, significantly improving the ability to model high-frequency
angular signals. In contrast to previous methods that use encoding functions
with only angular input, we additionally cone-trace spatial features to obtain
a spatially varying directional encoding, which addresses the challenging
interreflection effects. Extensive experiments on both synthetic and real
datasets show that a NeRF model with NDE (1) outperforms the state of the art
on view synthesis of specular objects, and (2) works with small networks to
allow fast (real-time) inference. The project webpage and source code are
available at: \url{https://lwwu2.github.io/nde/}.

Comments:
- Accepted to CVPR 2024

---

## TIGER: Text-Instructed 3D Gaussian Retrieval and Coherent Editing


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Teng Xu, Jiamin Chen, Peng Chen, Youjia Zhang, Junqing Yu, Wei Yang | cs.CV | [PDF](http://arxiv.org/pdf/2405.14455v1){: .btn .btn-green } |

**Abstract**: Editing objects within a scene is a critical functionality required across a
broad spectrum of applications in computer vision and graphics. As 3D Gaussian
Splatting (3DGS) emerges as a frontier in scene representation, the effective
modification of 3D Gaussian scenes has become increasingly vital. This process
entails accurately retrieve the target objects and subsequently performing
modifications based on instructions. Though available in pieces, existing
techniques mainly embed sparse semantics into Gaussians for retrieval, and rely
on an iterative dataset update paradigm for editing, leading to over-smoothing
or inconsistency issues. To this end, this paper proposes a systematic
approach, namely TIGER, for coherent text-instructed 3D Gaussian retrieval and
editing. In contrast to the top-down language grounding approach for 3D
Gaussians, we adopt a bottom-up language aggregation strategy to generate a
denser language embedded 3D Gaussians that supports open-vocabulary retrieval.
To overcome the over-smoothing and inconsistency issues in editing, we propose
a Coherent Score Distillation (CSD) that aggregates a 2D image editing
diffusion model and a multi-view diffusion model for score distillation,
producing multi-view consistent editing with much finer details. In various
experiments, we demonstrate that our TIGER is able to accomplish more
consistent and realistic edits than prior work.



---

## MagicDrive3D: Controllable 3D Generation for Any-View Rendering in  Street Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Ruiyuan Gao, Kai Chen, Zhihao Li, Lanqing Hong, Zhenguo Li, Qiang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2405.14475v1){: .btn .btn-green } |

**Abstract**: While controllable generative models for images and videos have achieved
remarkable success, high-quality models for 3D scenes, particularly in
unbounded scenarios like autonomous driving, remain underdeveloped due to high
data acquisition costs. In this paper, we introduce MagicDrive3D, a novel
pipeline for controllable 3D street scene generation that supports
multi-condition control, including BEV maps, 3D objects, and text descriptions.
Unlike previous methods that reconstruct before training the generative models,
MagicDrive3D first trains a video generation model and then reconstructs from
the generated data. This innovative approach enables easily controllable
generation and static scene acquisition, resulting in high-quality scene
reconstruction. To address the minor errors in generated content, we propose
deformable Gaussian splatting with monocular depth initialization and
appearance modeling to manage exposure discrepancies across viewpoints.
Validated on the nuScenes dataset, MagicDrive3D generates diverse, high-quality
3D driving scenes that support any-view rendering and enhance downstream tasks
like BEV segmentation. Our results demonstrate the framework's superior
performance, showcasing its transformative potential for autonomous driving
simulation and beyond.



---

## LDM: Large Tensorial SDF Model for Textured Mesh Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Rengan Xie, Wenting Zheng, Kai Huang, Yizheng Chen, Qi Wang, Qi Ye, Wei Chen, Yuchi Huo | cs.GR | [PDF](http://arxiv.org/pdf/2405.14580v1){: .btn .btn-green } |

**Abstract**: Previous efforts have managed to generate production-ready 3D assets from
text or images. However, these methods primarily employ NeRF or 3D Gaussian
representations, which are not adept at producing smooth, high-quality
geometries required by modern rendering pipelines. In this paper, we propose
LDM, a novel feed-forward framework capable of generating high-fidelity,
illumination-decoupled textured mesh from a single image or text prompts. We
firstly utilize a multi-view diffusion model to generate sparse multi-view
inputs from single images or text prompts, and then a transformer-based model
is trained to predict a tensorial SDF field from these sparse multi-view image
inputs. Finally, we employ a gradient-based mesh optimization layer to refine
this model, enabling it to produce an SDF field from which high-quality
textured meshes can be extracted. Extensive experiments demonstrate that our
method can generate diverse, high-quality 3D mesh assets with corresponding
decomposed RGB textures within seconds.



---

## D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-23 | Joanna Waczyńska, Piotr Borycki, Joanna Kaleta, Sławomir Tadeja, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2405.14276v2){: .btn .btn-green } |

**Abstract**: Over the past years, we have observed an abundance of approaches for modeling
dynamic 3D scenes using Gaussian Splatting (GS). Such solutions use GS to
represent the scene's structure and the neural network to model dynamics. Such
approaches allow fast rendering and extracting each element of such a dynamic
scene. However, modifying such objects over time is challenging. SC-GS (Sparse
Controlled Gaussian Splatting) enhanced with Deformed Control Points partially
solves this issue. However, this approach necessitates selecting elements that
need to be kept fixed, as well as centroids that should be adjusted throughout
editing. Moreover, this task poses additional difficulties regarding the
re-productivity of such editing. To address this, we propose Dynamic
Multi-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspired
representation of dynamic GS. Additionally, we propose a strategy of linking
parameterized Gaussian splats, forming a Triangle Soup with the estimated mesh.
Consequently, we can separately construct new trajectories for the 3D objects
composing the scene. Thus, we can make the scene's dynamic editable over time
or while maintaining partial dynamics.



---

## Gaussian Time Machine: A Real-Time Rendering Methodology for  Time-Variant Appearances

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-22 | Licheng Shen, Ho Ngai Chow, Lingyun Wang, Tong Zhang, Mengqiu Wang, Yuxing Han | cs.CV | [PDF](http://arxiv.org/pdf/2405.13694v1){: .btn .btn-green } |

**Abstract**: Recent advancements in neural rendering techniques have significantly
enhanced the fidelity of 3D reconstruction. Notably, the emergence of 3D
Gaussian Splatting (3DGS) has marked a significant milestone by adopting a
discrete scene representation, facilitating efficient training and real-time
rendering. Several studies have successfully extended the real-time rendering
capability of 3DGS to dynamic scenes. However, a challenge arises when training
images are captured under vastly differing weather and lighting conditions.
This scenario poses a challenge for 3DGS and its variants in achieving accurate
reconstructions. Although NeRF-based methods (NeRF-W, CLNeRF) have shown
promise in handling such challenging conditions, their computational demands
hinder real-time rendering capabilities. In this paper, we present Gaussian
Time Machine (GTM) which models the time-dependent attributes of Gaussian
primitives with discrete time embedding vectors decoded by a lightweight
Multi-Layer-Perceptron(MLP). By adjusting the opacity of Gaussian primitives,
we can reconstruct visibility changes of objects. We further propose a
decomposed color model for improved geometric consistency. GTM achieved
state-of-the-art rendering fidelity on 3 datasets and is 100 times faster than
NeRF-based counterparts in rendering. Moreover, GTM successfully disentangles
the appearance changes and renders smooth appearance interpolation.

Comments:
- 14 pages, 6 figures

---

## DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D  Reconstruction Via Gaussian Consensus

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-22 | Yu Chen, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2405.13943v1){: .btn .btn-green } |

**Abstract**: The recent advances in 3D Gaussian Splatting (3DGS) show promising results on
the novel view synthesis (NVS) task. With its superior rendering performance
and high-fidelity rendering quality, 3DGS is excelling at its previous NeRF
counterparts. The most recent 3DGS method focuses either on improving the
instability of rendering efficiency or reducing the model size. On the other
hand, the training efficiency of 3DGS on large-scale scenes has not gained much
attention. In this work, we propose DoGaussian, a method that trains 3DGS
distributedly. Our method first decomposes a scene into K blocks and then
introduces the Alternating Direction Method of Multipliers (ADMM) into the
training procedure of 3DGS. During training, our DoGaussian maintains one
global 3DGS model on the master node and K local 3DGS models on the slave
nodes. The K local 3DGS models are dropped after training and we only query the
global 3DGS model during inference. The training time is reduced by scene
decomposition, and the training convergence and stability are guaranteed
through the consensus on the shared 3D Gaussians. Our method accelerates the
training of 3DGS by 6+ times when evaluated on large-scale scenes while
concurrently achieving state-of-the-art rendering quality. Our project page is
available at https://aibluefisher.github.io/DoGaussian.



---

## LAGA: Layered 3D Avatar Generation and Customization via Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-21 | Jia Gong, Shenyu Ji, Lin Geng Foo, Kang Chen, Hossein Rahmani, Jun Liu | cs.GR | [PDF](http://arxiv.org/pdf/2405.12663v1){: .btn .btn-green } |

**Abstract**: Creating and customizing a 3D clothed avatar from textual descriptions is a
critical and challenging task. Traditional methods often treat the human body
and clothing as inseparable, limiting users' ability to freely mix and match
garments. In response to this limitation, we present LAyered Gaussian Avatar
(LAGA), a carefully designed framework enabling the creation of high-fidelity
decomposable avatars with diverse garments. By decoupling garments from avatar,
our framework empowers users to conviniently edit avatars at the garment level.
Our approach begins by modeling the avatar using a set of Gaussian points
organized in a layered structure, where each layer corresponds to a specific
garment or the human body itself. To generate high-quality garments for each
layer, we introduce a coarse-to-fine strategy for diverse garment generation
and a novel dual-SDS loss function to maintain coherence between the generated
garments and avatar components, including the human body and other garments.
Moreover, we introduce three regularization losses to guide the movement of
Gaussians for garment transfer, allowing garments to be freely transferred to
various avatars. Extensive experimentation demonstrates that our approach
surpasses existing methods in the generation of 3D clothed humans.



---

## MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-21 | Hongsheng Wang, Xiang Cai, Xi Sun, Jinhong Yue, Shengyu Zhang, Feng Lin, Fei Wu | cs.CV | [PDF](http://arxiv.org/pdf/2405.12806v1){: .btn .btn-green } |

**Abstract**: Single-view clothed human reconstruction holds a central position in virtual
reality applications, especially in contexts involving intricate human motions.
It presents notable challenges in achieving realistic clothing deformation.
Current methodologies often overlook the influence of motion on surface
deformation, resulting in surfaces lacking the constraints imposed by global
motion. To overcome these limitations, we introduce an innovative framework,
Motion-Based 3D Clothed Humans Synthesis (MOSS), which employs kinematic
information to achieve motion-aware Gaussian split on the human surface. Our
framework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS)
and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher
distribution to propagate global motion across the body surface. The density
and rotation factors of this distribution explicitly control the Gaussians,
thereby enhancing the realism of the reconstructed surface. Additionally, to
address local occlusions in single-view, based on KGAS, UID identifies
significant surfaces, and geometric reconstruction is performed to compensate
for these deformations. Experimental results demonstrate that MOSS achieves
state-of-the-art visual quality in 3D clothed human synthesis from monocular
videos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94%
and 16.75% in LPIPS* respectively. Codes are available at
https://wanghongsheng01.github.io/MOSS/.



---

## Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-21 | Hongsheng Wang, Weiyue Zhang, Sihao Liu, Xinrui Zhou, Shengyu Zhang, Fei Wu, Feng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2405.12477v1){: .btn .btn-green } |

**Abstract**: Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human
reconstruction, it primarily relies on 2D pixel-level supervision, overlooking
the geometric complexity and topological relationships of different body parts.
To address this gap, we introduce the Hierarchical Graph Human Gaussian Control
(HUGS) framework for achieving high-fidelity 3D human reconstruction. Our
approach involves leveraging explicitly semantic priors of body parts to ensure
the consistency of geometric topology, thereby enabling the capture of the
complex geometrical and topological associations among body parts.
Additionally, we disentangle high-frequency features from global human features
to refine surface details in body parts. Extensive experiments demonstrate that
our method exhibits superior performance in human body reconstruction,
particularly in enhancing surface details and accurately reconstructing body
part junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/.



---

## Leveraging Neural Radiance Fields for Pose Estimation of an Unknown  Space Object during Proximity Operations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-21 | Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer | cs.CV | [PDF](http://arxiv.org/pdf/2405.12728v1){: .btn .btn-green } |

**Abstract**: We address the estimation of the 6D pose of an unknown target spacecraft
relative to a monocular camera, a key step towards the autonomous rendezvous
and proximity operations required by future Active Debris Removal missions. We
present a novel method that enables an "off-the-shelf" spacecraft pose
estimator, which is supposed to known the target CAD model, to be applied on an
unknown target. Our method relies on an in-the wild NeRF, i.e., a Neural
Radiance Field that employs learnable appearance embeddings to represent
varying illumination conditions found in natural scenes. We train the NeRF
model using a sparse collection of images that depict the target, and in turn
generate a large dataset that is diverse both in terms of viewpoint and
illumination. This dataset is then used to train the pose estimation network.
We validate our method on the Hardware-In-the-Loop images of SPEED+ that
emulate lighting conditions close to those encountered on orbit. We demonstrate
that our method successfully enables the training of an off-the-shelf
spacecraft pose estimation network from a sparse set of images. Furthermore, we
show that a network trained using our method performs similarly to a model
trained on synthetic images generated using the CAD model of the target.



---

## Fast Generalizable Gaussian Splatting Reconstruction from Multi-View  Stereo

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-20 | Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2405.12218v1){: .btn .btn-green } |

**Abstract**: We present MVSGaussian, a new generalizable 3D Gaussian representation
approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct
unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware
Gaussian representations and decode them into Gaussian parameters. 2) To
further enhance performance, we propose a hybrid Gaussian rendering that
integrates an efficient volume rendering design for novel view synthesis. 3) To
support fast fine-tuning for specific scenes, we introduce a multi-view
geometric consistent aggregation strategy to effectively aggregate the point
clouds generated by the generalizable model, serving as the initialization for
per-scene optimization. Compared with previous generalizable NeRF-based
methods, which typically require minutes of fine-tuning and seconds of
rendering per image, MVSGaussian achieves real-time rendering with better
synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian
achieves better view synthesis with less training computational cost. Extensive
experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples
datasets validate that MVSGaussian attains state-of-the-art performance with
convincing generalizability, real-time rendering speed, and fast per-scene
optimization.

Comments:
- Project page: https://mvsgaussian.github.io/

---

## AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-20 | Rong Liu, Rui Xu, Yue Hu, Meida Chen, Andrew Feng | cs.CV | [PDF](http://arxiv.org/pdf/2405.12369v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently advanced radiance field
reconstruction by offering superior capabilities for novel view synthesis and
real-time rendering speed. However, its strategy of blending optimization and
adaptive density control might lead to sub-optimal results; it can sometimes
yield noisy geometry and blurry artifacts due to prioritizing optimizing large
Gaussians at the cost of adequately densifying smaller ones. To address this,
we introduce AtomGS, consisting of Atomized Proliferation and Geometry-Guided
Optimization. The Atomized Proliferation constrains ellipsoid Gaussians of
various sizes into more uniform-sized Atom Gaussians. The strategy enhances the
representation of areas with fine features by placing greater emphasis on
densification in accordance with scene details. In addition, we proposed a
Geometry-Guided Optimization approach that incorporates an Edge-Aware Normal
Loss. This optimization method effectively smooths flat surfaces while
preserving intricate details. Our evaluation shows that AtomGS outperforms
existing state-of-the-art methods in rendering quality. Additionally, it
achieves competitive accuracy in geometry reconstruction and offers a
significant improvement in training speed over other SDF-based methods. More
interactive demos can be found in our website
(https://rongliu-leo.github.io/AtomGS/).



---

## GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and  Texture Details

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-20 | Boqian Li, Xuan Li, Ying Jiang, Tianyi Xie, Feng Gao, Huamin Wang, Yin Yang, Chenfanfu Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2405.12420v1){: .btn .btn-green } |

**Abstract**: Traditional 3D garment creation is labor-intensive, involving sketching,
modeling, UV mapping, and texturing, which are time-consuming and costly.
Recent advances in diffusion-based generative models have enabled new
possibilities for 3D garment generation from text prompts, images, and videos.
However, existing methods either suffer from inconsistencies among multi-view
images or require additional processes to separate cloth from the underlying
human model. In this paper, we propose GarmentDreamer, a novel method that
leverages 3D Gaussian Splatting (GS) as guidance to generate wearable,
simulation-ready 3D garment meshes from text prompts. In contrast to using
multi-view images directly predicted by generative models as guidance, our 3DGS
guidance ensures consistent optimization in both garment deformation and
texture synthesis. Our method introduces a novel garment augmentation module,
guided by normal and RGBA information, and employs implicit Neural Texture
Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate
diverse geometric and texture details. We validate the effectiveness of our
approach through comprehensive qualitative and quantitative experiments,
showcasing the superior performance of GarmentDreamer over state-of-the-art
alternatives. Our project page is available at:
https://xuan-li.github.io/GarmentDreamerDemo/.



---

## Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with  Anchor Gaussian Guided Texture Warping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-20 | Tianhao Wu, Jing Yang, Zhilin Guo, Jingyi Wan, Fangcheng Zhong, Cengiz Oztireli | cs.CV | [PDF](http://arxiv.org/pdf/2405.12069v2){: .btn .btn-green } |

**Abstract**: By equipping the most recent 3D Gaussian Splatting representation with head
3D morphable models (3DMM), existing methods manage to create head avatars with
high fidelity. However, most existing methods only reconstruct a head without
the body, substantially limiting their application scenarios. We found that
naively applying Gaussians to model the clothed chest and shoulders tends to
result in blurry reconstruction and noisy floaters under novel poses. This is
because of the fundamental limitation of Gaussians and point clouds -- each
Gaussian or point can only have a single directional radiance without spatial
variance, therefore an unnecessarily large number of them is required to
represent complicated spatially varying texture, even for simple geometry. In
contrast, we propose to model the body part with a neural texture that consists
of coarse and pose-dependent fine colors. To properly render the body texture
for each view and pose without accurate geometry nor UV mapping, we optimize
another sparse set of Gaussians as anchors that constrain the neural warping
field that maps image plane coordinates to the texture space. We demonstrate
that Gaussian Head & Shoulders can fit the high-frequency details on the
clothed upper body with high fidelity and potentially improve the accuracy and
fidelity of the head region. We evaluate our method with casual phone-captured
and internet videos and show our method archives superior reconstruction
quality and robustness in both self and cross reenactment tasks. To fully
utilize the efficient rendering speed of Gaussian splatting, we additionally
propose an accelerated inference method of our trained model without
Multi-Layer Perceptron (MLP) queries and reach a stable rendering speed of
around 130 FPS for any subjects.

Comments:
- Project Page: https://gaussian-head-shoulders.netlify.app/

---

## NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-20 | Fotios Logothetis, Ignas Budvytis, Roberto Cipolla | cs.CV | [PDF](http://arxiv.org/pdf/2405.12057v1){: .btn .btn-green } |

**Abstract**: In this work we present a novel multi-view photometric stereo (PS) method.
Like many works in 3D reconstruction we are leveraging neural shape
representations and learnt renderers. However, our work differs from the
state-of-the-art multi-view PS methods such as PS-NeRF or SuperNormal we
explicity leverage per-pixel intensity renderings rather than relying mainly on
estimated normals.
  We model point light attenuation and explicitly raytrace cast shadows in
order to best approximate each points incoming radiance. This is used as input
to a fully neural material renderer that uses minimal prior assumptions and it
is jointly optimised with the surface. Finally, estimated normal and
segmentation maps can also incorporated in order to maximise the surface
accuracy.
  Our method is among the first to outperform the classical approach of
DiLiGenT-MV and achieves average 0.2mm Chamfer distance for objects imaged at
approx 1.5m distance away with approximate 400x400 resolution. Moreover, we
show robustness to poor normals in low light count scenario, achieving 0.27mm
Chamfer distance when pixel rendering is used instead of estimated normals.



---

## MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror  Reflections

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-20 | Jiayue Liu, Xiao Tang, Freeman Cheng, Roy Yang, Zhihao Li, Jianzhuang Liu, Yi Huang, Jiaqi Lin, Shiyong Liu, Xiaofei Wu, Songcen Xu, Chun Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2405.11921v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting showcases notable advancements in photo-realistic and
real-time novel view synthesis. However, it faces challenges in modeling mirror
reflections, which exhibit substantial appearance variations from different
viewpoints. To tackle this problem, we present MirrorGaussian, the first method
for mirror scene reconstruction with real-time rendering based on 3D Gaussian
Splatting. The key insight is grounded on the mirror symmetry between the
real-world space and the virtual mirror space. We introduce an intuitive
dual-rendering strategy that enables differentiable rasterization of both the
real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the
former about the mirror plane. All 3D Gaussians are jointly optimized with the
mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality
and real-time rendering in scenes with mirrors, empowering scene editing like
adding new mirrors and objects. Comprehensive experiments on multiple datasets
demonstrate that our approach significantly outperforms existing methods,
achieving state-of-the-art results. Project page:
https://mirror-gaussian.github.io/.



---

## CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-20 | Jiawei Zhang, Jiahe Li, Xiaohan Yu, Lei Huang, Lin Gu, Jin Zheng, Xiao Bai | cs.CV | [PDF](http://arxiv.org/pdf/2405.12110v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D
Gaussians to represent a scene. With sparse training views, 3DGS easily suffers
from overfitting, negatively impacting the reconstruction quality. This paper
introduces a new co-regularization perspective for improving sparse-view 3DGS.
When training two 3D Gaussian radiance fields with the same sparse views of a
scene, we observe that the two radiance fields exhibit \textit{point
disagreement} and \textit{rendering disagreement} that can unsupervisedly
predict reconstruction quality, stemming from the sampling implementation in
densification. We further quantify the point disagreement and rendering
disagreement by evaluating the registration between Gaussians' point
representations and calculating differences in their rendered pixels. The
empirical study demonstrates the negative correlation between the two
disagreements and accurate reconstruction, which allows us to identify
inaccurate reconstruction without accessing ground-truth information. Based on
the study, we propose CoR-GS, which identifies and suppresses inaccurate
reconstruction based on the two disagreements: (\romannumeral1) Co-pruning
considers Gaussians that exhibit high point disagreement in inaccurate
positions and prunes them. (\romannumeral2) Pseudo-view co-regularization
considers pixels that exhibit high rendering disagreement are inaccurately
rendered and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and
Blender demonstrate that CoR-GS effectively regularizes the scene geometry,
reconstructs the compact representations, and achieves state-of-the-art novel
view synthesis quality under sparse training views.

Comments:
- Project page: https://jiaw-z.github.io/CoR-GS/

---

## Embracing Radiance Field Rendering in 6G: Over-the-Air Training and  Inference with 3D Contents

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-20 | Guanlin Wu, Zhonghao Lyu, Juyong Zhang, Jie Xu | cs.IT | [PDF](http://arxiv.org/pdf/2405.12155v1){: .btn .btn-green } |

**Abstract**: The efficient representation, transmission, and reconstruction of
three-dimensional (3D) contents are becoming increasingly important for
sixth-generation (6G) networks that aim to merge virtual and physical worlds
for offering immersive communication experiences. Neural radiance field (NeRF)
and 3D Gaussian splatting (3D-GS) have recently emerged as two promising 3D
representation techniques based on radiance field rendering, which are able to
provide photorealistic rendering results for complex scenes. Therefore,
embracing NeRF and 3D-GS in 6G networks is envisioned to be a prominent
solution to support emerging 3D applications with enhanced quality of
experience. This paper provides a comprehensive overview on the integration of
NeRF and 3D-GS in 6G. First, we review the basics of the radiance field
rendering techniques, and highlight their applications and implementation
challenges over wireless networks. Next, we consider the over-the-air training
of NeRF and 3D-GS models over wireless networks by presenting various learning
techniques. We particularly focus on the federated learning design over a
hierarchical device-edge-cloud architecture. Then, we discuss three practical
rendering architectures of NeRF and 3D-GS models at wireless network edge. We
provide model compression approaches to facilitate the transmission of radiance
field models, and present rendering acceleration approaches and joint
computation and communication designs to enhance the rendering efficiency. In
particular, we propose a new semantic communication enabled 3D content
transmission design, in which the radiance field models are exploited as the
semantic knowledge base to reduce the communication overhead for distributed
inference. Furthermore, we present the utilization of radiance field rendering
in wireless applications like radio mapping and radio imaging.

Comments:
- 15 pages,7 figures

---

## R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless  Environments

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-19 | Huiying Yang, Zihan Jin, Chenhao Wu, Rujing Xiong, Robert Caiming Qiu, Zenan Ling | cs.IT | [PDF](http://arxiv.org/pdf/2405.11541v1){: .btn .btn-green } |

**Abstract**: Recently, ray tracing has gained renewed interest with the advent of
Reflective Intelligent Surfaces (RIS) technology, a key enabler of 6G wireless
communications due to its capability of intelligent manipulation of
electromagnetic waves. However, accurately modeling RIS-enabled wireless
environments poses significant challenges due to the complex variations caused
by various environmental factors and the mobility of RISs. In this paper, we
propose a novel modeling approach using Neural Radiance Fields (NeRF) to
characterize the dynamics of electromagnetic fields in such environments. Our
method utilizes NeRF-based ray tracing to intuitively capture and visualize the
complex dynamics of signal propagation, effectively modeling the complete
signal pathways from the transmitter to the RIS, and from the RIS to the
receiver. This two-stage process accurately characterizes multiple complex
transmission paths, enhancing our understanding of signal behavior in
real-world scenarios. Our approach predicts the signal field for any specified
RIS placement and receiver location, facilitating efficient RIS deployment.
Experimental evaluations using both simulated and real-world data validate the
significant benefits of our methodology.



---

## Searching Realistic-Looking Adversarial Objects For Autonomous Driving  Systems

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-19 | Shengxiang Sun, Shenzhe Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2405.11629v1){: .btn .btn-green } |

**Abstract**: Numerous studies on adversarial attacks targeting self-driving policies fail
to incorporate realistic-looking adversarial objects, limiting real-world
applicability. Building upon prior research that facilitated the transition of
adversarial objects from simulations to practical applications, this paper
discusses a modified gradient-based texture optimization method to discover
realistic-looking adversarial objects. While retaining the core architecture
and techniques of the prior research, the proposed addition involves an entity
termed the 'Judge'. This agent assesses the texture of a rendered object,
assigning a probability score reflecting its realism. This score is integrated
into the loss function to encourage the NeRF object renderer to concurrently
learn realistic and adversarial textures. The paper analyzes four strategies
for developing a robust 'Judge': 1) Leveraging cutting-edge vision-language
models. 2) Fine-tuning open-sourced vision-language models. 3) Pretraining
neurosymbolic systems. 4) Utilizing traditional image processing techniques.
Our findings indicate that strategies 1) and 4) yield less reliable outcomes,
pointing towards strategies 2) or 3) as more promising directions for future
research.



---

## MotionGS : Compact Gaussian Splatting SLAM by Motion Filter

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-18 | Xinli Guo, Peng Han, Weidong Zhang, Hongtian Chen | cs.CV | [PDF](http://arxiv.org/pdf/2405.11129v1){: .btn .btn-green } |

**Abstract**: With their high-fidelity scene representation capability, the attention of
SLAM field is deeply attracted by the Neural Radiation Field (NeRF) and 3D
Gaussian Splatting (3DGS). Recently, there has been a Surge in NeRF-based SLAM,
while 3DGS-based SLAM is sparse. A novel 3DGS-based SLAM approach with a fusion
of deep visual feature, dual keyframe selection and 3DGS is presented in this
paper. Compared with the existing methods, the proposed selectively tracking is
achieved by feature extraction and motion filter on each frame. The joint
optimization of pose and 3D Gaussian runs through the entire mapping process.
Additionally, the coarse-to-fine pose estimation and compact Gaussian scene
representation are implemented by dual keyfeature selection and novel loss
functions. Experimental results demonstrate that the proposed algorithm not
only outperforms the existing methods in tracking and mapping, but also has
less memory usage.



---

## Dreamer XL: Towards High-Resolution Text-to-3D Generation via Trajectory  Score Matching

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-18 | Xingyu Miao, Haoran Duan, Varun Ojha, Jun Song, Tejal Shah, Yang Long, Rajiv Ranjan | cs.CV | [PDF](http://arxiv.org/pdf/2405.11252v1){: .btn .btn-green } |

**Abstract**: In this work, we propose a novel Trajectory Score Matching (TSM) method that
aims to solve the pseudo ground truth inconsistency problem caused by the
accumulated error in Interval Score Matching (ISM) when using the Denoising
Diffusion Implicit Models (DDIM) inversion process. Unlike ISM which adopts the
inversion process of DDIM to calculate on a single path, our TSM method
leverages the inversion process of DDIM to generate two paths from the same
starting point for calculation. Since both paths start from the same starting
point, TSM can reduce the accumulated error compared to ISM, thus alleviating
the problem of pseudo ground truth inconsistency. TSM enhances the stability
and consistency of the model's generated paths during the distillation process.
We demonstrate this experimentally and further show that ISM is a special case
of TSM. Furthermore, to optimize the current multi-stage optimization process
from high-resolution text to 3D generation, we adopt Stable Diffusion XL for
guidance. In response to the issues of abnormal replication and splitting
caused by unstable gradients during the 3D Gaussian splatting process when
using Stable Diffusion XL, we propose a pixel-by-pixel gradient clipping
method. Extensive experiments show that our model significantly surpasses the
state-of-the-art models in terms of visual quality and performance. Code:
\url{https://github.com/xingy038/Dreamer-XL}.



---

## Photorealistic 3D Urban Scene Reconstruction and Point Cloud Extraction  using Google Earth Imagery and Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-17 | Kyle Gao, Dening Lu, Hongjie He, Linlin Xu, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2405.11021v1){: .btn .btn-green } |

**Abstract**: 3D urban scene reconstruction and modelling is a crucial research area in
remote sensing with numerous applications in academia, commerce, industry, and
administration. Recent advancements in view synthesis models have facilitated
photorealistic 3D reconstruction solely from 2D images. Leveraging Google Earth
imagery, we construct a 3D Gaussian Splatting model of the Waterloo region
centered on the University of Waterloo and are able to achieve view-synthesis
results far exceeding previous 3D view-synthesis results based on neural
radiance fields which we demonstrate in our benchmark. Additionally, we
retrieved the 3D geometry of the scene using the 3D point cloud extracted from
the 3D Gaussian Splatting model which we benchmarked against our Multi-
View-Stereo dense reconstruction of the scene, thereby reconstructing both the
3D geometry and photorealistic lighting of the large-scale urban scene through
3D Gaussian Splatting



---

## ART3D: 3D Gaussian Splatting for Text-Guided Artistic Scenes Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-17 | Pengzhi Li, Chengshuai Tang, Qinxuan Huang, Zhiheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2405.10508v1){: .btn .btn-green } |

**Abstract**: In this paper, we explore the existing challenges in 3D artistic scene
generation by introducing ART3D, a novel framework that combines diffusion
models and 3D Gaussian splatting techniques. Our method effectively bridges the
gap between artistic and realistic images through an innovative image semantic
transfer algorithm. By leveraging depth information and an initial artistic
image, we generate a point cloud map, addressing domain differences.
Additionally, we propose a depth consistency module to enhance 3D scene
consistency. Finally, the 3D scene serves as initial points for optimizing
Gaussian splats. Experimental results demonstrate ART3D's superior performance
in both content and structural consistency metrics when compared to existing
methods. ART3D significantly advances the field of AI in art creation by
providing an innovative solution for generating high-quality 3D artistic
scenes.

Comments:
- Accepted at CVPR 2024 Workshop on AI3DG

---

## When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks  via Multi-modal Large Language Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-16 | Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, Philip H Torr, Marc Pollefeys, Matthias Nießner, Ian D Reid, Angel X. Chang, Iro Laina, Victor Adrian Prisacariu | cs.CV | [PDF](http://arxiv.org/pdf/2405.10255v1){: .btn .btn-green } |

**Abstract**: As large language models (LLMs) evolve, their integration with 3D spatial
data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for
understanding and interacting with physical spaces. This survey provides a
comprehensive overview of the methodologies enabling LLMs to process,
understand, and generate 3D data. Highlighting the unique advantages of LLMs,
such as in-context learning, step-by-step reasoning, open-vocabulary
capabilities, and extensive world knowledge, we underscore their potential to
significantly advance spatial comprehension and interaction within embodied
Artificial Intelligence (AI) systems. Our investigation spans various 3D data
representations, from point clouds to Neural Radiance Fields (NeRFs). It
examines their integration with LLMs for tasks such as 3D scene understanding,
captioning, question-answering, and dialogue, as well as LLM-based agents for
spatial reasoning, planning, and navigation. The paper also includes a brief
review of other methods that integrate 3D and language. The meta-analysis
presented in this paper reveals significant progress yet underscores the
necessity for novel approaches to harness the full potential of 3D-LLMs. Hence,
with this paper, we aim to chart a course for future research that explores and
expands the capabilities of 3D-LLMs in understanding and interacting with the
complex 3D world. To support this survey, we have established a project page
where papers related to our topic are organized and listed:
https://github.com/ActiveVisionLab/Awesome-LLM-3D.



---

## GS-Planner: A Gaussian-Splatting-based Planning Framework for Active  High-Fidelity Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-16 | Rui Jin, Yuman Gao, Haojian Lu, Fei Gao | cs.RO | [PDF](http://arxiv.org/pdf/2405.10142v1){: .btn .btn-green } |

**Abstract**: Active reconstruction technique enables robots to autonomously collect scene
data for full coverage, relieving users from tedious and time-consuming data
capturing process. However, designed based on unsuitable scene representations,
existing methods show unrealistic reconstruction results or the inability of
online quality evaluation. Due to the recent advancements in explicit radiance
field technology, online active high-fidelity reconstruction has become
achievable. In this paper, we propose GS-Planner, a planning framework for
active high-fidelity reconstruction using 3D Gaussian Splatting. With
improvement on 3DGS to recognize unobserved regions, we evaluate the
reconstruction quality and completeness of 3DGS map online to guide the robot.
Then we design a sampling-based active reconstruction strategy to explore the
unobserved areas and improve the reconstruction geometric and textural quality.
To establish a complete robot active reconstruction system, we choose quadrotor
as the robotic platform for its high agility. Then we devise a safety
constraint with 3DGS to generate executable trajectories for quadrotor
navigation in the 3DGS map. To validate the effectiveness of our method, we
conduct extensive experiments and ablation studies in highly realistic
simulation scenes.



---

## From NeRFs to Gaussian Splats, and Back

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-15 | Siming He, Zach Osman, Pratik Chaudhari | cs.CV | [PDF](http://arxiv.org/pdf/2405.09717v1){: .btn .btn-green } |

**Abstract**: For robotics applications where there is a limited number of (typically
ego-centric) views, parametric representations such as neural radiance fields
(NeRFs) generalize better than non-parametric ones such as Gaussian splatting
(GS) to views that are very different from those in the training data; GS
however can render much faster than NeRFs. We develop a procedure to convert
back and forth between the two. Our approach achieves the best of both NeRFs
(superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact
representation) and GS (real-time rendering and ability for easily modifying
the representation); the computational cost of these conversions is minor
compared to training the two from scratch.



---

## Dynamic NeRF: A Review

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-14 | Jinwei Lin | cs.CV | [PDF](http://arxiv.org/pdf/2405.08609v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field(NeRF) is an novel implicit method to achieve the 3D
reconstruction and representation with a high resolution. After the first
research of NeRF is proposed, NeRF has gained a robust developing power and is
booming in the 3D modeling, representation and reconstruction areas. However
the first and most of the followed research projects based on NeRF is static,
which are weak in the practical applications. Therefore, more researcher are
interested and focused on the study of dynamic NeRF that is more feasible and
useful in practical applications or situations. Compared with the static NeRF,
implementing the Dynamic NeRF is more difficult and complex. But Dynamic is
more potential in the future even is the basic of Editable NeRF. In this
review, we made a detailed and abundant statement for the development and
important implementation principles of Dynamci NeRF. The analysis of main
principle and development of Dynamic NeRF is from 2021 to 2023, including the
most of the Dynamic NeRF projects. What is more, with colorful and novel
special designed figures and table, We also made a detailed comparison and
analysis of different features of various of Dynamic. Besides, we analyzed and
discussed the key methods to implement a Dynamic NeRF. The volume of the
reference papers is large. The statements and comparisons are multidimensional.
With a reading of this review, the whole development history and most of the
main design method or principles of Dynamic NeRF can be easy understood and
gained.

Comments:
- 25 pages

---

## Synergistic Integration of Coordinate Network and Tensorial Feature for  Improving Neural Radiance Fields from Sparse Inputs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-13 | Mingyu Kim, Jun-Seong Kim, Se-Young Yun, Jin-Hwa Kim | cs.CV | [PDF](http://arxiv.org/pdf/2405.07857v1){: .btn .btn-green } |

**Abstract**: The multi-plane representation has been highlighted for its fast training and
inference across static and dynamic neural radiance fields. This approach
constructs relevant features via projection onto learnable grids and
interpolating adjacent vertices. However, it has limitations in capturing
low-frequency details and tends to overuse parameters for low-frequency
features due to its bias toward fine details, despite its multi-resolution
concept. This phenomenon leads to instability and inefficiency when training
poses are sparse. In this work, we propose a method that synergistically
integrates multi-plane representation with a coordinate-based network known for
strong bias toward low-frequency signals. The coordinate-based network is
responsible for capturing low-frequency details, while the multi-plane
representation focuses on capturing fine-grained details. We demonstrate that
using residual connections between them seamlessly preserves their own inherent
properties. Additionally, the proposed progressive training scheme accelerates
the disentanglement of these two features. We empirically show that the
proposed method achieves comparable results to explicit encoding with fewer
parameters, and particularly, it outperforms others for the static and dynamic
NeRFs under sparse inputs.

Comments:
- ICML2024 ; Project page is accessible at
  https://mingyukim87.github.io/SynergyNeRF ; Code is available at
  https://github.com/MingyuKim87/SynergyNeRF

---

## GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting  Editing with Image Prompting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-13 | Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao | cs.CV | [PDF](http://arxiv.org/pdf/2405.07472v1){: .btn .btn-green } |

**Abstract**: The increasing prominence of e-commerce has underscored the importance of
Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D
realm and rely heavily on extensive data for training. Research on 3D VTON
primarily centers on garment-body shape compatibility, a topic extensively
covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion
model has now been adapted for 3D editing via multi-viewpoint editing. In this
work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating
Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless
transition from 2D to 3D VTON, we propose, for the first time, the use of only
images as editing prompts for 3D editing. To further address issues, e.g., face
blurring, garment inaccuracy, and degraded viewpoint quality during editing, we
devise a three-stage refinement strategy to gradually mitigate potential
issues. Furthermore, we introduce a new editing strategy termed Edit Recall
Reconstruction (ERR) to tackle the limitations of previous editing strategies
in leading to complex geometric changes. Our comprehensive experiments
demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D
VTON while also establishing a novel starting point for image-prompting 3D
scene editing.

Comments:
- On-going work

---

## Hologram: Realtime Holographic Overlays via LiDAR Augmented  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-12 | Ekansh Agrawal | cs.CV | [PDF](http://arxiv.org/pdf/2405.07178v1){: .btn .btn-green } |

**Abstract**: Guided by the hologram technology of the infamous Star Wars franchise, I
present an application that creates real-time holographic overlays using LiDAR
augmented 3D reconstruction. Prior attempts involve SLAM or NeRFs which either
require highly calibrated scenes, incur steep computation costs, or fail to
render dynamic scenes. I propose 3 high-fidelity reconstruction tools that can
run on a portable device, such as a iPhone 14 Pro, which can allow for metric
accurate facial reconstructions. My systems enable interactive and immersive
holographic experiences that can be used for a wide range of applications,
including augmented reality, telepresence, and entertainment.



---

## Point Resampling and Ray Transformation Aid to Editable NeRF Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-12 | Zhenyang Li, Zilong Chen, Feifan Qu, Mingqing Wang, Yizhou Zhao, Kai Zhang, Yifan Peng | cs.CV | [PDF](http://arxiv.org/pdf/2405.07306v1){: .btn .btn-green } |

**Abstract**: In NeRF-aided editing tasks, object movement presents difficulties in
supervision generation due to the introduction of variability in object
positions. Moreover, the removal operations of certain scene objects often lead
to empty regions, presenting challenges for NeRF models in inpainting them
effectively. We propose an implicit ray transformation strategy, allowing for
direct manipulation of the 3D object's pose by operating on the neural-point in
NeRF rays. To address the challenge of inpainting potential empty regions, we
present a plug-and-play inpainting module, dubbed differentiable neural-point
resampling (DNR), which interpolates those regions in 3D space at the original
ray locations within the implicit space, thereby facilitating object removal &
scene inpainting tasks. Importantly, employing DNR effectively narrows the gap
between ground truth and predicted implicit features, potentially increasing
the mutual information (MI) of the features across rays. Then, we leverage DNR
and ray transformation to construct a point-based editable NeRF pipeline
PR^2T-NeRF. Results primarily evaluated on 3D object removal & inpainting tasks
indicate that our pipeline achieves state-of-the-art performance. In addition,
our pipeline supports high-quality rendering visualization for diverse editing
operations without necessitating extra supervision.



---

## TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural  Radiance Field Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-11 | Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu | cs.CV | [PDF](http://arxiv.org/pdf/2405.07027v1){: .btn .btn-green } |

**Abstract**: The reliance on accurate camera poses is a significant barrier to the
widespread deployment of Neural Radiance Fields (NeRF) models for 3D
reconstruction and SLAM tasks. The existing method introduces monocular depth
priors to jointly optimize the camera poses and NeRF, which fails to fully
exploit the depth priors and neglects the impact of their inherent noise. In
this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that
enables training NeRF from unknown camera poses - by jointly optimizing
learnable parameters of the radiance field and camera poses. Our approach
explicitly utilizes monocular depth priors through three key advancements: 1)
we propose a novel depth-based ray sampling strategy based on the truncated
normal distribution, which improves the convergence speed and accuracy of pose
estimation; 2) to circumvent local minima and refine depth geometry, we
introduce a coarse-to-fine training strategy that progressively improves the
depth precision; 3) we propose a more robust inter-frame point constraint that
enhances robustness against depth noise during training. The experimental
results on three datasets demonstrate that TD-NeRF achieves superior
performance in the joint optimization of camera pose and NeRF, surpassing prior
works, and generates more accurate depth geometry. The implementation of our
method has been released at https://github.com/nubot-nudt/TD-NeRF.



---

## Direct Learning of Mesh and Appearance via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-11 | Ancheng Lin, Jun Li | cs.CV | [PDF](http://arxiv.org/pdf/2405.06945v1){: .btn .btn-green } |

**Abstract**: Accurately reconstructing a 3D scene including explicit geometry information
is both attractive and challenging. Geometry reconstruction can benefit from
incorporating differentiable appearance models, such as Neural Radiance Fields
and 3D Gaussian Splatting (3DGS). In this work, we propose a learnable scene
model that incorporates 3DGS with an explicit geometry representation, namely a
mesh. Our model learns the mesh and appearance in an end-to-end manner, where
we bind 3D Gaussians to the mesh faces and perform differentiable rendering of
3DGS to obtain photometric supervision. The model creates an effective
information pathway to supervise the learning of the scene, including the mesh.
Experimental results demonstrate that the learned scene model not only achieves
state-of-the-art rendering quality but also supports manipulation using the
explicit mesh. In addition, our model has a unique advantage in adapting to
scene updates, thanks to the end-to-end learning of both mesh and appearance.



---

## Aerial-NeRF: Adaptive Spatial Partitioning and Sampling for Large-Scale  Aerial Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu | cs.CV | [PDF](http://arxiv.org/pdf/2405.06214v1){: .btn .btn-green } |

**Abstract**: Recent progress in large-scale scene rendering has yielded Neural Radiance
Fields (NeRF)-based models with an impressive ability to synthesize scenes
across small objects and indoor scenes. Nevertheless, extending this idea to
large-scale aerial rendering poses two critical problems. Firstly, a single
NeRF cannot render the entire scene with high-precision for complex large-scale
aerial datasets since the sampling range along each view ray is insufficient to
cover buildings adequately. Secondly, traditional NeRFs are infeasible to train
on one GPU to enable interactive fly-throughs for modeling massive images.
Instead, existing methods typically separate the whole scene into multiple
regions and train a NeRF on each region, which are unaccustomed to different
flight trajectories and difficult to achieve fast rendering. To that end, we
propose Aerial-NeRF with three innovative modifications for jointly adapting
NeRF in large-scale aerial rendering: (1) Designing an adaptive spatial
partitioning and selection method based on drones' poses to adapt different
flight trajectories; (2) Using similarity of poses instead of (expert) network
for rendering speedup to determine which region a new viewpoint belongs to; (3)
Developing an adaptive sampling approach for rendering performance improvement
to cover the entire buildings at different heights. Extensive experiments have
conducted to verify the effectiveness and efficiency of Aerial-NeRF, and new
state-of-the-art results have been achieved on two public large-scale aerial
datasets and presented SCUTic dataset. Note that our model allows us to perform
rendering over 4 times as fast as compared to multiple competitors. Our
dataset, code, and model are publicly available at https://drliuqi.github.io/.



---

## OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Jinwei Lin | cs.CV | [PDF](http://arxiv.org/pdf/2405.06547v1){: .btn .btn-green } |

**Abstract**: One image to editable dynamic 3D model and video generation is novel
direction and change in the research area of single image to 3D representation
or 3D reconstruction of image. Gaussian Splatting has demonstrated its
advantages in implicit 3D reconstruction, compared with the original Neural
Radiance Fields. As the rapid development of technologies and principles,
people tried to used the Stable Diffusion models to generate targeted models
with text instructions. However, using the normal implicit machine learning
methods is hard to gain the precise motions and actions control, further more,
it is difficult to generate a long content and semantic continuous 3D video. To
address this issue, we propose the OneTo3D, a method and theory to used one
single image to generate the editable 3D model and generate the targeted
semantic continuous time-unlimited 3D video. We used a normal basic Gaussian
Splatting model to generate the 3D model from a single image, which requires
less volume of video memory and computer calculation ability. Subsequently, we
designed an automatic generation and self-adaptive binding mechanism for the
object armature. Combined with the re-editable motions and actions analyzing
and controlling algorithm we proposed, we can achieve a better performance than
the SOTA projects in the area of building the 3D model precise motions and
actions control, and generating a stable semantic continuous time-unlimited 3D
video with the input text instructions. Here we will analyze the detailed
implementation methods and theories analyses. Relative comparisons and
conclusions will be presented. The project code is open source.

Comments:
- 24 pages, 13 figures, 2 tables

---

## SketchDream: Sketch-based Text-to-3D Generation and Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Feng-Lin Liu, Hongbo Fu, Yu-Kun Lai, Lin Gao | cs.GR | [PDF](http://arxiv.org/pdf/2405.06461v2){: .btn .btn-green } |

**Abstract**: Existing text-based 3D generation methods generate attractive results but
lack detailed geometry control. Sketches, known for their conciseness and
expressiveness, have contributed to intuitive 3D modeling but are confined to
producing texture-less mesh models within predefined categories. Integrating
sketch and text simultaneously for 3D generation promises enhanced control over
geometry and appearance but faces challenges from 2D-to-3D translation
ambiguity and multi-modal condition integration. Moreover, further editing of
3D models in arbitrary views will give users more freedom to customize their
models. However, it is difficult to achieve high generation quality, preserve
unedited regions, and manage proper interactions between shape components. To
solve the above issues, we propose a text-driven 3D content generation and
editing method, SketchDream, which supports NeRF generation from given
hand-drawn sketches and achieves free-view sketch-based local editing. To
tackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view
image generation diffusion model, which leverages depth guidance to establish
spatial correspondence. A 3D ControlNet with a 3D attention module is utilized
to control multi-view images and ensure their 3D consistency. To support local
editing, we further propose a coarse-to-fine editing approach: the coarse phase
analyzes component interactions and provides 3D masks to label edited regions,
while the fine stage generates realistic results with refined details by local
enhancement. Extensive experiments validate that our method generates
higher-quality results compared with a combination of 2D ControlNet and
image-to-3D generation techniques and achieves detailed control compared with
existing diffusion-based 3D editing approaches.



---

## Residual-NeRF: Learning Residual NeRFs for Transparent Object  Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski | cs.CV | [PDF](http://arxiv.org/pdf/2405.06181v1){: .btn .btn-green } |

**Abstract**: Transparent objects are ubiquitous in industry, pharmaceuticals, and
households. Grasping and manipulating these objects is a significant challenge
for robots. Existing methods have difficulty reconstructing complete depth maps
for challenging transparent objects, leaving holes in the depth reconstruction.
Recent work has shown neural radiance fields (NeRFs) work well for depth
perception in scenes with transparent objects, and these depth maps can be used
to grasp transparent objects with high accuracy. NeRF-based depth
reconstruction can still struggle with especially challenging transparent
objects and lighting conditions. In this work, we propose Residual-NeRF, a
method to improve depth perception and training speed for transparent objects.
Robots often operate in the same area, such as a kitchen. By first learning a
background NeRF of the scene without transparent objects to be manipulated, we
reduce the ambiguity faced by learning the changes with the new object. We
propose training two additional networks: a residual NeRF learns to infer
residual RGB values and densities, and a Mixnet learns how to combine
background and residual NeRFs. We contribute synthetic and real experiments
that suggest Residual-NeRF improves depth perception of transparent objects.
The results on synthetic data suggest Residual-NeRF outperforms the baselines
with a 46.1% lower RMSE and a 29.5% lower MAE. Real-world qualitative
experiments suggest Residual-NeRF leads to more robust depth maps with less
noise and fewer holes. Website: https://residual-nerf.github.io



---

## I3DGS: Improve 3D Gaussian Splatting from Multiple Dimensions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Jinwei Lin | cs.CV | [PDF](http://arxiv.org/pdf/2405.06408v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting is a novel method for 3D view synthesis, which can gain
an implicit neural learning rendering result than the traditional neural
rendering technology but keep the more high-definition fast rendering speed.
But it is still difficult to achieve a fast enough efficiency on 3D Gaussian
Splatting for the practical applications. To Address this issue, we propose the
I3DS, a synthetic model performance improvement evaluation solution and
experiments test. From multiple and important levels or dimensions of the
original 3D Gaussian Splatting, we made more than two thousand various kinds of
experiments to test how the selected different items and components can make an
impact on the training efficiency of the 3D Gaussian Splatting model. In this
paper, we will share abundant and meaningful experiences and methods about how
to improve the training, performance and the impacts caused by different items
of the model. A special but normal Integer compression in base 95 and a
floating-point compression in base 94 with ASCII encoding and decoding
mechanism is presented. Many real and effective experiments and test results or
phenomena will be recorded. After a series of reasonable fine-tuning, I3DS can
gain excellent performance improvements than the previous one. The project code
is available as open source.

Comments:
- 16 pages

---

## MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth  Smooth Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2405.06241v1){: .btn .btn-green } |

**Abstract**: This letter introduces a novel framework for dense Visual Simultaneous
Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently Gaussian
Splatting-based SLAM has yielded promising results, but rely on RGB-D input and
is weak in tracking. To address these limitations, we uniquely integrates
advanced sparse visual odometry with a dense Gaussian Splatting scene
representation for the first time, thereby eliminating the dependency on depth
maps typical of Gaussian Splatting-based SLAM systems and enhancing tracking
robustness. Here, the sparse visual odometry tracks camera poses in RGB stream,
while Gaussian Splatting handles map reconstruction. These components are
interconnected through a Multi-View Stereo (MVS) depth estimation network. And
we propose a depth smooth loss to reduce the negative effect of estimated depth
maps. Furthermore, the consistency in scale between the sparse visual odometry
and the dense Gaussian map is preserved by Sparse-Dense Adjustment Ring (SDAR).
We have evaluated our system across various synthetic and real-world datasets.
The accuracy of our pose estimation surpasses existing methods and achieves
state-of-the-art performance. Additionally, it outperforms previous monocular
methods in terms of novel view synthesis fidelity, matching the results of
neural SLAM systems that utilize RGB-D input.

Comments:
- This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible

---

## LIVE: LaTex Interactive Visual Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Jinwei Lin | cs.HC | [PDF](http://arxiv.org/pdf/2405.06762v1){: .btn .btn-green } |

**Abstract**: LaTex coding is one of the main methods of writing an academic paper. When
writing a paper, abundant proper visual or graphic components will represent
more information volume than the textual data. However, most of the
implementation of LaTex graphic items are designed as static items that have
some weaknesses in representing more informative figures or tables with an
interactive reading experience. To address this problem, we propose LIVE, a
novel design methods idea to design interactive LaTex graphic items. To make a
lucid representation of the main idea of LIVE, we designed several novels
representing implementations that are interactive and enough explanation for
the basic level principles. Using LIVE can design more graphic items, which we
call the Gitems, and easily and automatically get the relationship of the
mutual application of a specific range of papers, which will add more vitality
and performance factors into writing of traditional papers especially the
review papers. For vividly representing the functions of LIVE, we use the
papers from NeRF as the example reference papers. The code of the
implementation project is open source.

Comments:
- 8 pages, double column, ieee

---

## NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via  Generative Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh | cs.CV | [PDF](http://arxiv.org/pdf/2405.05749v2){: .btn .btn-green } |

**Abstract**: Audio-driven talking head generation is advancing from 2D to 3D content.
Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to
synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based
approach typically requires a large number of paired audio-visual data for each
identity, thereby limiting the scalability of the method. Although there have
been attempts to generate audio-driven 3D talking head animations with a single
image, the results are often unsatisfactory due to insufficient information on
obscured regions in the image. In this paper, we mainly focus on addressing the
overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where
facial animations are synthesized primarily in front-facing perspectives. We
propose a novel method, NeRFFaceSpeech, which enables to produce high-quality
3D-aware talking head. Using prior knowledge of generative models combined with
NeRF, our method can craft a 3D-consistent facial feature space corresponding
to a single image. Our spatial synchronization method employs audio-correlated
vertex dynamics of a parametric face model to transform static image features
into dynamic visuals through ray deformation, ensuring realistic 3D facial
motion. Moreover, we introduce LipaintNet that can replenish the lacking
information in the inner-mouth area, which can not be obtained from a given
single image. The network is trained in a self-supervised manner by utilizing
the generative capabilities without additional data. The comprehensive
experiments demonstrate the superiority of our method in generating
audio-driven talking heads from a single image with enhanced 3D consistency
compared to previous approaches. In addition, we introduce a quantitative way
of measuring the robustness of a model against pose changes for the first time,
which has been possible only qualitatively.

Comments:
- 11 pages, 5 figures

---

## NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Mingrui Li, Jingwei Huang, Lei Sun, Aaron Xuxiang Tian, Tianchen Deng, Hongyu Wang | cs.RO | [PDF](http://arxiv.org/pdf/2405.05702v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has garnered widespread attention due to its exceptional
performance. Consequently, SLAM systems based on Gaussian Splatting have
emerged, leveraging its capabilities for rapid real-time rendering and
high-fidelity mapping. However, current Gaussian Splatting SLAM systems usually
struggle with large scene representation and lack effective loop closure
adjustments and scene generalization capabilities. To address these issues, we
introduce NGM-SLAM, the first GS-SLAM system that utilizes neural radiance
field submaps for progressive scene expression, effectively integrating the
strengths of neural radiance fields and 3D Gaussian Splatting. We have
developed neural implicit submaps as supervision and achieve high-quality scene
expression and online loop closure adjustments through Gaussian rendering of
fused submaps. Our results on multiple real-world scenes and large-scale scene
datasets demonstrate that our method can achieve accurate gap filling and
high-quality scene expression, supporting both monocular, stereo, and RGB-D
inputs, and achieving state-of-the-art scene reconstruction and tracking
performance.

Comments:
- 9pages, 4 figures

---

## DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian  Representation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Sitian Shen, Jing Xu, Yuheng Yuan, Xingyi Yang, Qiuhong Shen, Xinchao Wang | cs.GR | [PDF](http://arxiv.org/pdf/2405.05800v1){: .btn .btn-green } |

**Abstract**: User-friendly 3D object editing is a challenging task that has attracted
significant attention recently. The limitations of direct 3D object editing
without 2D prior knowledge have prompted increased attention towards utilizing
2D generative models for 3D editing. While existing methods like Instruct
NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly
due to semantic guided editing. In the realm of 3D representation, 3D Gaussian
Splatting emerges as a promising approach for its efficiency and natural
explicit property, facilitating precise editing tasks. Building upon these
insights, we propose DragGaussian, a 3D object drag-editing framework based on
3D Gaussian Splatting, leveraging diffusion models for interactive image
editing with open-vocabulary input. This framework enables users to perform
drag-based editing on pre-trained 3D Gaussian object models, producing modified
2D images through multi-view consistent editing. Our contributions include the
introduction of a new task, the development of DragGaussian for interactive
point-based 3D editing, and comprehensive validation of its effectiveness
through qualitative and quantitative experiments.



---

## RPBG: Towards Robust Neural Point-based Graphics in the Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Qingtian Zhu, Zizhuang Wei, Zhongtian Zheng, Yifan Zhan, Zhuyu Yao, Jiawang Zhang, Kejian Wu, Yinqiang Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2405.05663v1){: .btn .btn-green } |

**Abstract**: Point-based representations have recently gained popularity in novel view
synthesis, for their unique advantages, e.g., intuitive geometric
representation, simple manipulation, and faster convergence. However, based on
our observation, these point-based neural re-rendering methods are only
expected to perform well under ideal conditions and suffer from noisy, patchy
points and unbounded scenes, which are challenging to handle but defacto common
in real applications. To this end, we revisit one such influential method,
known as Neural Point-based Graphics (NPBG), as our baseline, and propose
Robust Point-based Graphics (RPBG). We in-depth analyze the factors that
prevent NPBG from achieving satisfactory renderings on generic datasets, and
accordingly reform the pipeline to make it more robust to varying datasets
in-the-wild. Inspired by the practices in image restoration, we greatly enhance
the neural renderer to enable the attention-based correction of point
visibility and the inpainting of incomplete rasterization, with only acceptable
overheads. We also seek for a simple and lightweight alternative for
environment modeling and an iterative method to alleviate the problem of poor
geometry. By thorough evaluation on a wide range of datasets with different
shooting conditions and camera trajectories, RPBG stably outperforms the
baseline by a large margin, and exhibits its great robustness over
state-of-the-art NeRF-based variants. Code available at
https://github.com/QT-Zhu/RPBG.



---

## FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Yikun Ma, Dandan Zhan, Zhi Jin | cs.CV | [PDF](http://arxiv.org/pdf/2405.05768v1){: .btn .btn-green } |

**Abstract**: Text-driven 3D indoor scene generation holds broad applications, ranging from
gaming and smart homes to AR/VR applications. Fast and high-fidelity scene
generation is paramount for ensuring user-friendly experiences. However,
existing methods are characterized by lengthy generation processes or
necessitate the intricate manual specification of motion parameters, which
introduces inconvenience for users. Furthermore, these methods often rely on
narrow-field viewpoint iterative generations, compromising global consistency
and overall scene quality. To address these issues, we propose FastScene, a
framework for fast and higher-quality 3D scene generation, while maintaining
the scene consistency. Specifically, given a text prompt, we generate a
panorama and estimate its depth, since the panorama encompasses information
about the entire scene and exhibits explicit geometric constraints. To obtain
high-quality novel views, we introduce the Coarse View Synthesis (CVS) and
Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene
consistency and view quality. Subsequently, we utilize Multi-View Projection
(MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for
scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses
other methods in both generation speed and quality with better scene
consistency. Notably, guided only by a text prompt, FastScene can generate a 3D
scene within a mere 15 minutes, which is at least one hour faster than
state-of-the-art methods, making it a paradigm for user-friendly scene
generation.

Comments:
- Accepted by IJCAI-2024

---

## Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Yuhang Ming, Xingrui Yang, Weihan Wang, Zheng Chen, Jinglun Feng, Yifan Xing, Guofeng Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2405.05526v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D
scene representation, offering high-fidelity renderings and reconstructions
from a set of sparse and unstructured sensor data. In the context of autonomous
robotics, where perception and understanding of the environment are pivotal,
NeRF holds immense promise for improving performance. In this paper, we present
a comprehensive survey and analysis of the state-of-the-art techniques for
utilizing NeRF to enhance the capabilities of autonomous robots. We especially
focus on the perception, localization and navigation, and decision-making
modules of autonomous robots and delve into tasks crucial for autonomous
operation, including 3D reconstruction, segmentation, pose estimation,
simultaneous localization and mapping (SLAM), navigation and planning, and
interaction. Our survey meticulously benchmarks existing NeRF-based methods,
providing insights into their strengths and limitations. Moreover, we explore
promising avenues for future research and development in this domain. Notably,
we discuss the integration of advanced techniques such as 3D Gaussian splatting
(3DGS), large language models (LLM), and generative AIs, envisioning enhanced
reconstruction efficiency, scene understanding, decision-making capabilities.
This survey serves as a roadmap for researchers seeking to leverage NeRFs to
empower autonomous robots, paving the way for innovative solutions that can
navigate and interact seamlessly in complex environments.

Comments:
- 32 pages, 5 figures, 8 tables

---

## ${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-08 | Ning Wang, Lefei Zhang, Angel X Chang | cs.CV | [PDF](http://arxiv.org/pdf/2405.05010v1){: .btn .btn-green } |

**Abstract**: Neural fields (NeRF) have emerged as a promising approach for representing
continuous 3D scenes. Nevertheless, the lack of semantic encoding in NeRFs
poses a significant challenge for scene decomposition. To address this
challenge, we present a single model, Multi-Modal Decomposition NeRF
(${M^2D}$NeRF), that is capable of both text-based and visual patch-based
edits. Specifically, we use multi-modal feature distillation to integrate
teacher features from pretrained visual and language models into 3D semantic
feature volumes, thereby facilitating consistent 3D editing. To enforce
consistency between the visual and language features in our 3D feature volumes,
we introduce a multi-modal similarity constraint. We also introduce a
patch-based joint contrastive loss that helps to encourage object-regions to
coalesce in the 3D feature space, resulting in more precise boundaries.
Experiments on various real-world scenes show superior performance in 3D scene
decomposition tasks compared to prior NeRF-based methods.



---

## GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of  Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-08 | Yuanhao Gong | cs.CV | [PDF](http://arxiv.org/pdf/2405.05446v1){: .btn .btn-green } |

**Abstract**: The 3D Gaussian splatting methods are getting popular. However, they work
directly on the signal, leading to a dense representation of the signal. Even
with some techniques such as pruning or distillation, the results are still
dense. In this paper, we propose to model the gradient of the original signal.
The gradients are much sparser than the original signal. Therefore, the
gradients use much less Gaussian splats, leading to the more efficient storage
and thus higher computational performance during both training and rendering.
Thanks to the sparsity, during the view synthesis, only a small mount of pixels
are needed, leading to much higher computational performance ($100\sim
1000\times$ faster). And the 2D image can be recovered from the gradients via
solving a Poisson equation with linear computation complexity. Several
experiments are performed to confirm the sparseness of the gradients and the
computation performance of the proposed method. The method can be applied
various applications, such as human body modeling and indoor environment
modeling.

Comments:
- arXiv admin note: text overlap with arXiv:2404.09105

---

## Novel View Synthesis with Neural Radiance Fields for Industrial Robot  Applications

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-07 | Markus Hillemann, Robert Langendörfer, Max Heiken, Max Mehltretter, Andreas Schenk, Martin Weinmann, Stefan Hinz, Christian Heipke, Markus Ulrich | cs.CV | [PDF](http://arxiv.org/pdf/2405.04345v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become a rapidly growing research field
with the potential to revolutionize typical photogrammetric workflows, such as
those used for 3D scene reconstruction. As input, NeRFs require multi-view
images with corresponding camera poses as well as the interior orientation. In
the typical NeRF workflow, the camera poses and the interior orientation are
estimated in advance with Structure from Motion (SfM). But the quality of the
resulting novel views, which depends on different parameters such as the number
and distribution of available images, as well as the accuracy of the related
camera poses and interior orientation, is difficult to predict. In addition,
SfM is a time-consuming pre-processing step, and its quality strongly depends
on the image content. Furthermore, the undefined scaling factor of SfM hinders
subsequent steps in which metric information is required. In this paper, we
evaluate the potential of NeRFs for industrial robot applications. We propose
an alternative to SfM pre-processing: we capture the input images with a
calibrated camera that is attached to the end effector of an industrial robot
and determine accurate camera poses with metric scale based on the robot
kinematics. We then investigate the quality of the novel views by comparing
them to ground truth, and by computing an internal quality measure based on
ensemble methods. For evaluation purposes, we acquire multiple datasets that
pose challenges for reconstruction typical of industrial applications, like
reflective objects, poor texture, and fine structures. We show that the
robot-based pose determination reaches similar accuracy as SfM in non-demanding
cases, while having clear advantages in more challenging scenarios. Finally, we
present first results of applying the ensemble method to estimate the quality
of the synthetic novel view in the absence of a ground truth.

Comments:
- 8 pages, 8 figures, accepted for publication in The International
  Archives of the Photogrammetry, Remote Sensing and Spatial Information
  Sciences (ISPRS Archives) 2024

---

## DistGrid: Scalable Scene Reconstruction with Distributed  Multi-resolution Hash Grid

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-07 | Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou | cs.CV | [PDF](http://arxiv.org/pdf/2405.04416v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled
and indoor scene reconstruction. However, there exist some challenges when
reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network
capacity, while volume-based NeRFs are heavily memory-consuming when the scene
resolution increases. Recent approaches propose to geographically partition the
scene and learn each sub-region using an individual NeRF. Such partitioning
strategies help volume-based NeRF exceed the single GPU memory limit and scale
to larger scenes. However, this approach requires multiple background NeRF to
handle out-of-partition rays, which leads to redundancy of learning. Inspired
by the fact that the background of current partition is the foreground of
adjacent partition, we propose a scalable scene reconstruction method based on
joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is
divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding
Boxes, and a novel segmented volume rendering method is proposed to handle
cross-boundary rays, thereby eliminating the need for background NeRFs. The
experiments demonstrate that our method outperforms existing methods on all
evaluated large-scale scenes, and provides visually plausible scene
reconstruction. The scalability of our method on reconstruction quality is
further evaluated qualitatively and quantitatively.

Comments:
- Originally submitted to Siggraph Asia 2023

---

## Splat-MOVER: Multi-Stage, Open-Vocabulary Robotic Manipulation via  Editable Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-07 | Ola Shorinwa, Johnathan Tucker, Aliyah Smith, Aiden Swann, Timothy Chen, Roya Firoozi, Monroe Kennedy III, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2405.04378v2){: .btn .btn-green } |

**Abstract**: We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic
manipulation, which leverages the editability of Gaussian Splatting (GSplat)
scene representations to enable multi-stage manipulation tasks. Splat-MOVER
consists of: (i) ASK-Splat, a GSplat representation that distills latent codes
for language semantics and grasp affordance into the 3D scene. ASK-Splat
enables geometric, semantic, and affordance understanding of 3D scenes, which
is critical for many robotics tasks; (ii) SEE-Splat, a real-time scene-editing
module using 3D semantic masking and infilling to visualize the motions of
objects that result from robot interactions in the real-world. SEE-Splat
creates a "digital twin" of the evolving environment throughout the
manipulation task; and (iii) Grasp-Splat, a grasp generation module that uses
ASK-Splat and SEE-Splat to propose candidate grasps for open-world objects.
ASK-Splat is trained in real-time from RGB images in a brief scanning phase
prior to operation, while SEE-Splat and Grasp-Splat run in real-time during
operation. We demonstrate the superior performance of Splat-MOVER in hardware
experiments on a Kinova robot compared to two recent baselines in four
single-stage, open-vocabulary manipulation tasks, as well as in four
multi-stage manipulation tasks using the edited scene to reflect scene changes
due to prior manipulation stages, which is not possible with the existing
baselines. Code for this project and a link to the project page will be made
available soon.



---

## Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-06 | Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård | cs.CV | [PDF](http://arxiv.org/pdf/2405.03417v1){: .btn .btn-green } |

**Abstract**: Image-based 3D reconstruction is a challenging task that involves inferring
the 3D shape of an object or scene from a set of input images. Learning-based
methods have gained attention for their ability to directly estimate 3D shapes.
This review paper focuses on state-of-the-art techniques for 3D reconstruction,
including the generation of novel, unseen views. An overview of recent
developments in the Gaussian Splatting method is provided, covering input
types, model structures, output representations, and training strategies.
Unresolved challenges and future directions are also discussed. Given the rapid
progress in this domain and the numerous opportunities for enhancing 3D
reconstruction methods, a comprehensive examination of algorithms appears
essential. Consequently, this study offers a thorough overview of the latest
advancements in Gaussian Splatting.

Comments:
- 24 pages

---

## A Construct-Optimize Approach to Sparse View Synthesis without Camera  Pose

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-06 | Kaiwen Jiang, Yang Fu, Mukund Varma T, Yash Belhe, Xiaolong Wang, Hao Su, Ravi Ramamoorthi | cs.CV | [PDF](http://arxiv.org/pdf/2405.03659v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis from a sparse set of input images is a challenging
problem of great practical interest, especially when camera poses are absent or
inaccurate. Direct optimization of camera poses and usage of estimated depths
in neural radiance field algorithms usually do not produce good results because
of the coupling between poses and depths, and inaccuracies in monocular depth
estimation. In this paper, we leverage the recent 3D Gaussian splatting method
to develop a novel construct-and-optimize method for sparse view synthesis
without camera poses. Specifically, we construct a solution progressively by
using monocular depth and projecting pixels back into the 3D world. During
construction, we optimize the solution by detecting 2D correspondences between
training views and the corresponding rendered images. We develop a unified
differentiable pipeline for camera registration and adjustment of both camera
poses and depths, followed by back-projection. We also introduce a novel notion
of an expected surface in Gaussian splatting, which is critical to our
optimization. These steps enable a coarse solution, which can then be low-pass
filtered and refined using standard optimization methods. We demonstrate
results on the Tanks and Temples and Static Hikes datasets with as few as three
widely-spaced views, showing significantly better quality than competing
methods, including those with approximate camera pose information. Moreover,
our results improve with more views and outperform previous InstantNGP and
Gaussian Splatting algorithms even when using half the dataset.



---

## MVIP-NeRF: Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-05 | Honghua Chen, Chen Change Loy, Xingang Pan | cs.CV | [PDF](http://arxiv.org/pdf/2405.02859v1){: .btn .btn-green } |

**Abstract**: Despite the emergence of successful NeRF inpainting methods built upon
explicit RGB and depth 2D inpainting supervisions, these methods are inherently
constrained by the capabilities of their underlying 2D inpainters. This is due
to two key reasons: (i) independently inpainting constituent images results in
view-inconsistent imagery, and (ii) 2D inpainters struggle to ensure
high-quality geometry completion and alignment with inpainted RGB images.
  To overcome these limitations, we propose a novel approach called MVIP-NeRF
that harnesses the potential of diffusion priors for NeRF inpainting,
addressing both appearance and geometry aspects. MVIP-NeRF performs joint
inpainting across multiple views to reach a consistent solution, which is
achieved via an iterative optimization process based on Score Distillation
Sampling (SDS). Apart from recovering the rendered RGB images, we also extract
normal maps as a geometric representation and define a normal SDS loss that
motivates accurate geometry inpainting and alignment with the appearance.
Additionally, we formulate a multi-view SDS score function to distill
generative priors simultaneously from different view images, ensuring
consistent visual completion when dealing with large view variations. Our
experimental results show better appearance and geometry recovery than previous
NeRF inpainting methods.

Comments:
- 14 pages, 10 figures, conference

---

## Blending Distributed NeRFs with Tri-stage Robust Pose Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-05 | Baijun Ye, Caiyun Liu, Xiaoyu Ye, Yuantao Chen, Yuhai Wang, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2405.02880v1){: .btn .btn-green } |

**Abstract**: Due to the limited model capacity, leveraging distributed Neural Radiance
Fields (NeRFs) for modeling extensive urban environments has become a
necessity. However, current distributed NeRF registration approaches encounter
aliasing artifacts, arising from discrepancies in rendering resolutions and
suboptimal pose precision. These factors collectively deteriorate the fidelity
of pose estimation within NeRF frameworks, resulting in occlusion artifacts
during the NeRF blending stage. In this paper, we present a distributed NeRF
system with tri-stage pose optimization. In the first stage, precise poses of
images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine
strategy. In the second stage, we incorporate the inverting Mip-NeRF 360,
coupled with the truncated dynamic low-pass filter, to enable the achievement
of robust and precise poses, termed Frame2Model optimization. On top of this,
we obtain a coarse transformation between NeRFs in different coordinate
systems. In the third stage, we fine-tune the transformation between NeRFs by
Model2Model pose optimization. After obtaining precise transformation
parameters, we proceed to implement NeRF blending, showcasing superior
performance metrics in both real-world and simulation scenarios. Codes and data
will be publicly available at https://github.com/boilcy/Distributed-NeRF.



---

## ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface  Uncertainty

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-04 | Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2405.02568v1){: .btn .btn-green } |

**Abstract**: Active learning in 3D scene reconstruction has been widely studied, as
selecting informative training views is critical for the reconstruction.
Recently, Neural Radiance Fields (NeRF) variants have shown performance
increases in active 3D reconstruction using image rendering or geometric
uncertainty. However, the simultaneous consideration of both uncertainties in
selecting informative views remains unexplored, while utilizing different types
of uncertainty can reduce the bias that arises in the early training stage with
sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate
views considering both uncertainties. ActiveNeuS provides a way to accumulate
image rendering uncertainty while avoiding the bias that the estimated
densities can introduce. ActiveNeuS computes the neural implicit surface
uncertainty, providing the color uncertainty along with the surface
information. It efficiently handles the bias by using the surface information
and a grid, enabling the fast selection of diverse viewpoints. Our method
outperforms previous works on popular datasets, Blender and DTU, showing that
the views selected by ActiveNeuS significantly improve performance.



---

## TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for  Dynamic UAV-based Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-04 | Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon | cs.CV | [PDF](http://arxiv.org/pdf/2405.02762v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a new approach to bridge the domain gap between
synthetic and real-world data for un- manned aerial vehicle (UAV)-based
perception. Our formu- lation is designed for dynamic scenes, consisting of
moving objects or human actions, where the goal is to recognize the pose or
actions. We propose an extension of K-Planes Neural Radiance Field (NeRF),
wherein our algorithm stores a set of tiered feature vectors. The tiered
feature vectors are generated to effectively model conceptual information about
a scene as well as an image decoder that transforms output feature maps into
RGB images. Our technique leverages the information amongst both static and
dynamic objects within a scene and is able to capture salient scene attributes
of high altitude videos. We evaluate its performance on challenging datasets,
including Okutama Action and UG2, and observe considerable improvement in
accuracy over state of the art aerial perception algorithms.

Comments:
- 8 pages, submitted to IROS2024

---

## Learning Robot Soccer from Egocentric Vision with Deep Reinforcement  Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil Sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, Nicolas Heess | cs.RO | [PDF](http://arxiv.org/pdf/2405.02425v1){: .btn .btn-green } |

**Abstract**: We apply multi-agent deep reinforcement learning (RL) to train end-to-end
robot soccer policies with fully onboard computation and sensing via egocentric
RGB vision. This setting reflects many challenges of real-world robotics,
including active perception, agile full-body control, and long-horizon planning
in a dynamic, partially-observable, multi-agent domain. We rely on large-scale,
simulation-based data generation to obtain complex behaviors from egocentric
vision which can be successfully transferred to physical robots using low-cost
sensors. To achieve adequate visual realism, our simulation combines rigid-body
physics with learned, realistic rendering via multiple Neural Radiance Fields
(NeRFs). We combine teacher-based multi-agent RL and cross-experiment data
reuse to enable the discovery of sophisticated soccer strategies. We analyze
active-perception behaviors including object tracking and ball seeking that
emerge when simply optimizing perception-agnostic soccer play. The agents
display equivalent levels of performance and agility as policies with access to
privileged, ground-truth state. To our knowledge, this paper constitutes a
first demonstration of end-to-end training for multi-agent robot soccer,
mapping raw pixel observations to joint-level actions, that can be deployed in
the real world. Videos of the game-play and analyses can be seen on our website
https://sites.google.com/view/vision-soccer .



---

## Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic  Solids

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2405.02386v1){: .btn .btn-green } |

**Abstract**: Despite significant advancements in Neural Radiance Fields (NeRFs), the
renderings may still suffer from aliasing and blurring artifacts, since it
remains a fundamental challenge to effectively and efficiently characterize
anisotropic areas induced by the cone-casting procedure. This paper introduces
a Ripmap-Encoded Platonic Solid representation to precisely and efficiently
featurize 3D anisotropic areas, achieving high-fidelity anti-aliasing
renderings. Central to our approach are two key components: Platonic Solid
Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D
space onto the unparalleled faces of a certain Platonic solid, such that the
anisotropic 3D areas can be projected onto planes with distinguishable
characterization. Meanwhile, each face of the Platonic solid is encoded by the
Ripmap encoding, which is constructed by anisotropically pre-filtering a
learnable feature grid, to enable featurzing the projected anisotropic areas
both precisely and efficiently by the anisotropic area-sampling. Extensive
experiments on both well-established synthetic datasets and a newly captured
real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art
rendering quality, particularly excelling in the fine details of repetitive
structures and textures, while maintaining relatively swift training times.

Comments:
- SIGGRAPH 2024, Project page: https://junchenliu77.github.io/Rip-NeRF
  , Code: https://github.com/JunchenLiu77/Rip-NeRF

---

## HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft  HoloLens 2

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Miriam Jäger, Theodor Kapler, Michael Feßenbecker, Felix Birkelbach, Markus Hillemann, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2405.02005v1){: .btn .btn-green } |

**Abstract**: In the fields of photogrammetry, computer vision and computer graphics, the
task of neural 3D scene reconstruction has led to the exploration of various
techniques. Among these, 3D Gaussian Splatting stands out for its explicit
representation of scenes using 3D Gaussians, making it appealing for tasks like
3D point cloud extraction and surface reconstruction. Motivated by its
potential, we address the domain of 3D scene reconstruction, aiming to leverage
the capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting.
We present HoloGS, a novel workflow utilizing HoloLens sensor data, which
bypasses the need for pre-processing steps like Structure from Motion by
instantly accessing the required input data i.e. the images, camera poses and
the point cloud from depth sensing. We provide comprehensive investigations,
including the training process and the rendering quality, assessed through the
Peak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified
point cloud from Gaussian centers, measured by Chamfer Distance. We evaluate
our approach on two self-captured scenes: An outdoor scene of a cultural
heritage statue and an indoor scene of a fine-structured plant. Our results
show that the HoloLens data, including RGB images, corresponding camera poses,
and depth sensing based point clouds to initialize the Gaussians, are suitable
as input for 3D Gaussian Splatting.

Comments:
- 8 pages, 9 figures, 2 tables. Will be published in the ISPRS The
  International Archives of Photogrammetry, Remote Sensing and Spatial
  Information Sciences

---

## WateRF: Robust Watermarks in Radiance Fields for Protection of  Copyrights

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2405.02066v1){: .btn .btn-green } |

**Abstract**: The advances in the Neural Radiance Fields (NeRF) research offer extensive
applications in diverse domains, but protecting their copyrights has not yet
been researched in depth. Recently, NeRF watermarking has been considered one
of the pivotal solutions for safely deploying NeRF-based 3D representations.
However, existing methods are designed to apply only to implicit or explicit
NeRF representations. In this work, we introduce an innovative watermarking
method that can be employed in both representations of NeRF. This is achieved
by fine-tuning NeRF to embed binary messages in the rendering process. In
detail, we propose utilizing the discrete wavelet transform in the NeRF space
for watermarking. Furthermore, we adopt a deferred back-propagation technique
and introduce a combination with the patch-wise loss to improve rendering
quality and bit accuracy with minimum trade-offs. We evaluate our method in
three different aspects: capacity, invisibility, and robustness of the embedded
watermarks in the 2D-rendered images. Our method achieves state-of-the-art
performance with faster training speed over the compared state-of-the-art
methods.



---

## NeRF in Robotics: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-02 | Guangming Wang, Lei Pan, Songyou Peng, Shaohui Liu, Chenfeng Xu, Yanzi Miao, Wei Zhan, Masayoshi Tomizuka, Marc Pollefeys, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2405.01333v1){: .btn .btn-green } |

**Abstract**: Meticulous 3D environment representations have been a longstanding goal in
computer vision and robotics fields. The recent emergence of neural implicit
representations has introduced radical innovation to this field as implicit
representations enable numerous capabilities. Among these, the Neural Radiance
Field (NeRF) has sparked a trend because of the huge representational
advantages, such as simplified mathematical models, compact environment
storage, and continuous scene representations. Apart from computer vision, NeRF
has also shown tremendous potential in the field of robotics. Thus, we create
this survey to provide a comprehensive understanding of NeRF in the field of
robotics. By exploring the advantages and limitations of NeRF, as well as its
current applications and future potential, we hope to shed light on this
promising area of research. Our survey is divided into two main sections:
\textit{The Application of NeRF in Robotics} and \textit{The Advance of NeRF in
Robotics}, from the perspective of how NeRF enters the field of robotics. In
the first section, we introduce and analyze some works that have been or could
be used in the field of robotics from the perception and interaction
perspectives. In the second section, we show some works related to improving
NeRF's own properties, which are essential for deploying NeRF in the field of
robotics. In the discussion section of the review, we summarize the existing
challenges and provide some valuable future research directions for reference.

Comments:
- 21 pages, 19 figures

---

## LidaRF: Delving into Lidar for Neural Radiance Field on Street Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Shanlin Sun, Bingbing Zhuang, Ziyu Jiang, Buyu Liu, Xiaohui Xie, Manmohan Chandraker | cs.CV | [PDF](http://arxiv.org/pdf/2405.00900v2){: .btn .btn-green } |

**Abstract**: Photorealistic simulation plays a crucial role in applications such as
autonomous driving, where advances in neural radiance fields (NeRFs) may allow
better scalability through the automatic creation of digital 3D assets.
However, reconstruction quality suffers on street scenes due to largely
collinear camera motions and sparser samplings at higher speeds. On the other
hand, the application often demands rendering from camera views that deviate
from the inputs to accurately simulate behaviors like lane changes. In this
paper, we propose several insights that allow a better utilization of Lidar
data to improve NeRF quality on street scenes. First, our framework learns a
geometric scene representation from Lidar, which is fused with the implicit
grid-based representation for radiance decoding, thereby supplying stronger
geometric information offered by explicit point cloud. Second, we put forth a
robust occlusion-aware depth supervision scheme, which allows utilizing
densified Lidar points by accumulation. Third, we generate augmented training
views from Lidar points for further improvement. Our insights translate to
largely improved novel view synthesis under real driving scenes.

Comments:
- CVPR2024 Highlights

---

## Spectrally Pruned Gaussian Fields with Neural Compensation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Runyi Yang, Zhenxin Zhu, Zhou Jiang, Baijun Ye, Xiaoxue Chen, Yifei Zhang, Yuantao Chen, Jian Zhao, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2405.00676v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered
attention for its fast rendering speed and high rendering quality. However,
this comes with high memory consumption, e.g., a well-trained Gaussian field
may utilize three million Gaussian primitives and over 700 MB of memory. We
credit this high memory footprint to the lack of consideration for the
relationship between primitives. In this paper, we propose a memory-efficient
Gaussian field named SUNDAE with spectral pruning and neural compensation. On
one hand, we construct a graph on the set of Gaussian primitives to model their
relationship and design a spectral down-sampling module to prune out primitives
while preserving desired signals. On the other hand, to compensate for the
quality loss of pruning Gaussians, we exploit a lightweight neural network head
to mix splatted features, which effectively compensates for quality losses
while capturing the relationship between primitives in its weights. We
demonstrate the performance of SUNDAE with extensive results. For example,
SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla
Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB
memory, on the Mip-NeRF360 dataset. Codes are publicly available at
https://runyiyang.github.io/projects/SUNDAE/.

Comments:
- Code: https://github.com/RunyiYang/SUNDAE Project page:
  https://runyiyang.github.io/projects/SUNDAE/

---

## NeRF-Guided Unsupervised Learning of RGB-D Registration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2405.00507v1){: .btn .btn-green } |

**Abstract**: This paper focuses on training a robust RGB-D registration model without
ground-truth pose supervision. Existing methods usually adopt a pairwise
training strategy based on differentiable rendering, which enforces the
photometric and the geometric consistency between the two registered frames as
supervision. However, this frame-to-frame framework suffers from poor
multi-view consistency due to factors such as lighting changes, geometry
occlusion and reflective materials. In this paper, we present NeRF-UR, a novel
frame-to-model optimization framework for unsupervised RGB-D registration.
Instead of frame-to-frame consistency, we leverage the neural radiance field
(NeRF) as a global model of the scene and use the consistency between the input
and the NeRF-rerendered frames for pose optimization. This design can
significantly improve the robustness in scenarios with poor multi-view
consistency and provides better learning signal for the registration model.
Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset,
Sim-RGBD, through a photo-realistic simulator to warm up the registration
model. By first training the registration model on Sim-RGBD and later
unsupervisedly fine-tuning on real data, our framework enables distilling the
capability of feature extraction and registration from simulation to reality.
Our method outperforms the state-of-the-art counterparts on two popular indoor
RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper
reproduction.



---

## Depth Priors in Removal Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhihao Guo, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2405.00630v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown impressive results in 3D
reconstruction and generating novel views. A key challenge within NeRF is the
editing of reconstructed scenes, such as object removal, which requires
maintaining consistency across multiple views and ensuring high-quality
synthesised perspectives. Previous studies have incorporated depth priors,
typically from LiDAR or sparse depth measurements provided by COLMAP, to
improve the performance of object removal in NeRF. However, these methods are
either costly or time-consuming. In this paper, we propose a novel approach
that integrates monocular depth estimates with NeRF-based object removal models
to significantly reduce time consumption and enhance the robustness and quality
of scene generation and object removal. We conducted a thorough evaluation of
COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy
in depth map generation. Our findings suggest that COLMAP can serve as an
effective alternative to a ground truth depth map where such information is
missing or costly to obtain. Additionally, we integrated various monocular
depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess
their capacity to improve object removal performance. Our experimental results
highlight the potential of monocular depth estimation to substantially improve
NeRF applications.

Comments:
- 15 pages
