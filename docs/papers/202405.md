---
layout: default
title: May 2024
parent: Papers
nav_order: 202405
---

<!---metadata--->


## WateRF: Robust Watermarks in Radiance Fields for Protection of  Copyrights

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2405.02066v1){: .btn .btn-green } |

**Abstract**: The advances in the Neural Radiance Fields (NeRF) research offer extensive
applications in diverse domains, but protecting their copyrights has not yet
been researched in depth. Recently, NeRF watermarking has been considered one
of the pivotal solutions for safely deploying NeRF-based 3D representations.
However, existing methods are designed to apply only to implicit or explicit
NeRF representations. In this work, we introduce an innovative watermarking
method that can be employed in both representations of NeRF. This is achieved
by fine-tuning NeRF to embed binary messages in the rendering process. In
detail, we propose utilizing the discrete wavelet transform in the NeRF space
for watermarking. Furthermore, we adopt a deferred back-propagation technique
and introduce a combination with the patch-wise loss to improve rendering
quality and bit accuracy with minimum trade-offs. We evaluate our method in
three different aspects: capacity, invisibility, and robustness of the embedded
watermarks in the 2D-rendered images. Our method achieves state-of-the-art
performance with faster training speed over the compared state-of-the-art
methods.



---

## HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft  HoloLens 2

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Miriam Jäger, Theodor Kapler, Michael Feßenbecker, Felix Birkelbach, Markus Hillemann, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2405.02005v1){: .btn .btn-green } |

**Abstract**: In the fields of photogrammetry, computer vision and computer graphics, the
task of neural 3D scene reconstruction has led to the exploration of various
techniques. Among these, 3D Gaussian Splatting stands out for its explicit
representation of scenes using 3D Gaussians, making it appealing for tasks like
3D point cloud extraction and surface reconstruction. Motivated by its
potential, we address the domain of 3D scene reconstruction, aiming to leverage
the capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting.
We present HoloGS, a novel workflow utilizing HoloLens sensor data, which
bypasses the need for pre-processing steps like Structure from Motion by
instantly accessing the required input data i.e. the images, camera poses and
the point cloud from depth sensing. We provide comprehensive investigations,
including the training process and the rendering quality, assessed through the
Peak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified
point cloud from Gaussian centers, measured by Chamfer Distance. We evaluate
our approach on two self-captured scenes: An outdoor scene of a cultural
heritage statue and an indoor scene of a fine-structured plant. Our results
show that the HoloLens data, including RGB images, corresponding camera poses,
and depth sensing based point clouds to initialize the Gaussians, are suitable
as input for 3D Gaussian Splatting.

Comments:
- 8 pages, 9 figures, 2 tables. Will be published in the ISPRS The
  International Archives of Photogrammetry, Remote Sensing and Spatial
  Information Sciences

---

## NeRF in Robotics: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-02 | Guangming Wang, Lei Pan, Songyou Peng, Shaohui Liu, Chenfeng Xu, Yanzi Miao, Wei Zhan, Masayoshi Tomizuka, Marc Pollefeys, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2405.01333v1){: .btn .btn-green } |

**Abstract**: Meticulous 3D environment representations have been a longstanding goal in
computer vision and robotics fields. The recent emergence of neural implicit
representations has introduced radical innovation to this field as implicit
representations enable numerous capabilities. Among these, the Neural Radiance
Field (NeRF) has sparked a trend because of the huge representational
advantages, such as simplified mathematical models, compact environment
storage, and continuous scene representations. Apart from computer vision, NeRF
has also shown tremendous potential in the field of robotics. Thus, we create
this survey to provide a comprehensive understanding of NeRF in the field of
robotics. By exploring the advantages and limitations of NeRF, as well as its
current applications and future potential, we hope to shed light on this
promising area of research. Our survey is divided into two main sections:
\textit{The Application of NeRF in Robotics} and \textit{The Advance of NeRF in
Robotics}, from the perspective of how NeRF enters the field of robotics. In
the first section, we introduce and analyze some works that have been or could
be used in the field of robotics from the perception and interaction
perspectives. In the second section, we show some works related to improving
NeRF's own properties, which are essential for deploying NeRF in the field of
robotics. In the discussion section of the review, we summarize the existing
challenges and provide some valuable future research directions for reference.

Comments:
- 21 pages, 19 figures

---

## DiL-NeRF: Delving into Lidar for Neural Radiance Field on Street Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Shanlin Sun, Bingbing Zhuang, Ziyu Jiang, Buyu Liu, Xiaohui Xie, Manmohan Chandraker | cs.CV | [PDF](http://arxiv.org/pdf/2405.00900v1){: .btn .btn-green } |

**Abstract**: Photorealistic simulation plays a crucial role in applications such as
autonomous driving, where advances in neural radiance fields (NeRFs) may allow
better scalability through the automatic creation of digital 3D assets.
However, reconstruction quality suffers on street scenes due to largely
collinear camera motions and sparser samplings at higher speeds. On the other
hand, the application often demands rendering from camera views that deviate
from the inputs to accurately simulate behaviors like lane changes. In this
paper, we propose several insights that allow a better utilization of Lidar
data to improve NeRF quality on street scenes. First, our framework learns a
geometric scene representation from Lidar, which is fused with the implicit
grid-based representation for radiance decoding, thereby supplying stronger
geometric information offered by explicit point cloud. Second, we put forth a
robust occlusion-aware depth supervision scheme, which allows utilizing
densified Lidar points by accumulation. Third, we generate augmented training
views from Lidar points for further improvement. Our insights translate to
largely improved novel view synthesis under real driving scenes.

Comments:
- CVPR2024 Highlights

---

## Spectrally Pruned Gaussian Fields with Neural Compensation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Runyi Yang, Zhenxin Zhu, Zhou Jiang, Baijun Ye, Xiaoxue Chen, Yifei Zhang, Yuantao Chen, Jian Zhao, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2405.00676v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered
attention for its fast rendering speed and high rendering quality. However,
this comes with high memory consumption, e.g., a well-trained Gaussian field
may utilize three million Gaussian primitives and over 700 MB of memory. We
credit this high memory footprint to the lack of consideration for the
relationship between primitives. In this paper, we propose a memory-efficient
Gaussian field named SUNDAE with spectral pruning and neural compensation. On
one hand, we construct a graph on the set of Gaussian primitives to model their
relationship and design a spectral down-sampling module to prune out primitives
while preserving desired signals. On the other hand, to compensate for the
quality loss of pruning Gaussians, we exploit a lightweight neural network head
to mix splatted features, which effectively compensates for quality losses
while capturing the relationship between primitives in its weights. We
demonstrate the performance of SUNDAE with extensive results. For example,
SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla
Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB
memory, on the Mip-NeRF360 dataset. Codes are publicly available at
https://runyiyang.github.io/projects/SUNDAE/.

Comments:
- Code: https://github.com/RunyiYang/SUNDAE Project page:
  https://runyiyang.github.io/projects/SUNDAE/

---

## Depth Priors in Removal Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhihao Guo, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2405.00630v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown impressive results in 3D
reconstruction and generating novel views. A key challenge within NeRF is the
editing of reconstructed scenes, such as object removal, which requires
maintaining consistency across multiple views and ensuring high-quality
synthesised perspectives. Previous studies have incorporated depth priors,
typically from LiDAR or sparse depth measurements provided by COLMAP, to
improve the performance of object removal in NeRF. However, these methods are
either costly or time-consuming. In this paper, we propose a novel approach
that integrates monocular depth estimates with NeRF-based object removal models
to significantly reduce time consumption and enhance the robustness and quality
of scene generation and object removal. We conducted a thorough evaluation of
COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy
in depth map generation. Our findings suggest that COLMAP can serve as an
effective alternative to a ground truth depth map where such information is
missing or costly to obtain. Additionally, we integrated various monocular
depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess
their capacity to improve object removal performance. Our experimental results
highlight the potential of monocular depth estimation to substantially improve
NeRF applications.

Comments:
- 15 pages

---

## NeRF-Guided Unsupervised Learning of RGB-D Registration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2405.00507v1){: .btn .btn-green } |

**Abstract**: This paper focuses on training a robust RGB-D registration model without
ground-truth pose supervision. Existing methods usually adopt a pairwise
training strategy based on differentiable rendering, which enforces the
photometric and the geometric consistency between the two registered frames as
supervision. However, this frame-to-frame framework suffers from poor
multi-view consistency due to factors such as lighting changes, geometry
occlusion and reflective materials. In this paper, we present NeRF-UR, a novel
frame-to-model optimization framework for unsupervised RGB-D registration.
Instead of frame-to-frame consistency, we leverage the neural radiance field
(NeRF) as a global model of the scene and use the consistency between the input
and the NeRF-rerendered frames for pose optimization. This design can
significantly improve the robustness in scenarios with poor multi-view
consistency and provides better learning signal for the registration model.
Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset,
Sim-RGBD, through a photo-realistic simulator to warm up the registration
model. By first training the registration model on Sim-RGBD and later
unsupervisedly fine-tuning on real data, our framework enables distilling the
capability of feature extraction and registration from simulation to reality.
Our method outperforms the state-of-the-art counterparts on two popular indoor
RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper
reproduction.


