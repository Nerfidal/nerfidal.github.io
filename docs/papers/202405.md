---
layout: default
title: May 2024
parent: Papers
nav_order: 202405
---

<!---metadata--->


## Spectrally Pruned Gaussian Fields with Neural Compensation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Runyi Yang, Zhenxin Zhu, Zhou Jiang, Baijun Ye, Xiaoxue Chen, Yifei Zhang, Yuantao Chen, Jian Zhao, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2405.00676v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered
attention for its fast rendering speed and high rendering quality. However,
this comes with high memory consumption, e.g., a well-trained Gaussian field
may utilize three million Gaussian primitives and over 700 MB of memory. We
credit this high memory footprint to the lack of consideration for the
relationship between primitives. In this paper, we propose a memory-efficient
Gaussian field named SUNDAE with spectral pruning and neural compensation. On
one hand, we construct a graph on the set of Gaussian primitives to model their
relationship and design a spectral down-sampling module to prune out primitives
while preserving desired signals. On the other hand, to compensate for the
quality loss of pruning Gaussians, we exploit a lightweight neural network head
to mix splatted features, which effectively compensates for quality losses
while capturing the relationship between primitives in its weights. We
demonstrate the performance of SUNDAE with extensive results. For example,
SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla
Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB
memory, on the Mip-NeRF360 dataset. Codes are publicly available at
https://runyiyang.github.io/projects/SUNDAE/.

Comments:
- Code: https://github.com/RunyiYang/SUNDAE Project page:
  https://runyiyang.github.io/projects/SUNDAE/

---

## Depth Priors in Removal Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhihao Guo, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2405.00630v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown impressive results in 3D
reconstruction and generating novel views. A key challenge within NeRF is the
editing of reconstructed scenes, such as object removal, which requires
maintaining consistency across multiple views and ensuring high-quality
synthesised perspectives. Previous studies have incorporated depth priors,
typically from LiDAR or sparse depth measurements provided by COLMAP, to
improve the performance of object removal in NeRF. However, these methods are
either costly or time-consuming. In this paper, we propose a novel approach
that integrates monocular depth estimates with NeRF-based object removal models
to significantly reduce time consumption and enhance the robustness and quality
of scene generation and object removal. We conducted a thorough evaluation of
COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy
in depth map generation. Our findings suggest that COLMAP can serve as an
effective alternative to a ground truth depth map where such information is
missing or costly to obtain. Additionally, we integrated various monocular
depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess
their capacity to improve object removal performance. Our experimental results
highlight the potential of monocular depth estimation to substantially improve
NeRF applications.

Comments:
- 15 pages

---

## NeRF-Guided Unsupervised Learning of RGB-D Registration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2405.00507v1){: .btn .btn-green } |

**Abstract**: This paper focuses on training a robust RGB-D registration model without
ground-truth pose supervision. Existing methods usually adopt a pairwise
training strategy based on differentiable rendering, which enforces the
photometric and the geometric consistency between the two registered frames as
supervision. However, this frame-to-frame framework suffers from poor
multi-view consistency due to factors such as lighting changes, geometry
occlusion and reflective materials. In this paper, we present NeRF-UR, a novel
frame-to-model optimization framework for unsupervised RGB-D registration.
Instead of frame-to-frame consistency, we leverage the neural radiance field
(NeRF) as a global model of the scene and use the consistency between the input
and the NeRF-rerendered frames for pose optimization. This design can
significantly improve the robustness in scenarios with poor multi-view
consistency and provides better learning signal for the registration model.
Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset,
Sim-RGBD, through a photo-realistic simulator to warm up the registration
model. By first training the registration model on Sim-RGBD and later
unsupervisedly fine-tuning on real data, our framework enables distilling the
capability of feature extraction and registration from simulation to reality.
Our method outperforms the state-of-the-art counterparts on two popular indoor
RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper
reproduction.


