---
layout: default
title: May 2024
parent: Papers
nav_order: 202405
---

<!---metadata--->


## ${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-08 | Ning Wang, Lefei Zhang, Angel X Chang | cs.CV | [PDF](http://arxiv.org/pdf/2405.05010v1){: .btn .btn-green } |

**Abstract**: Neural fields (NeRF) have emerged as a promising approach for representing
continuous 3D scenes. Nevertheless, the lack of semantic encoding in NeRFs
poses a significant challenge for scene decomposition. To address this
challenge, we present a single model, Multi-Modal Decomposition NeRF
(${M^2D}$NeRF), that is capable of both text-based and visual patch-based
edits. Specifically, we use multi-modal feature distillation to integrate
teacher features from pretrained visual and language models into 3D semantic
feature volumes, thereby facilitating consistent 3D editing. To enforce
consistency between the visual and language features in our 3D feature volumes,
we introduce a multi-modal similarity constraint. We also introduce a
patch-based joint contrastive loss that helps to encourage object-regions to
coalesce in the 3D feature space, resulting in more precise boundaries.
Experiments on various real-world scenes show superior performance in 3D scene
decomposition tasks compared to prior NeRF-based methods.



---

## Novel View Synthesis with Neural Radiance Fields for Industrial Robot  Applications

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-07 | Markus Hillemann, Robert Langendörfer, Max Heiken, Max Mehltretter, Andreas Schenk, Martin Weinmann, Stefan Hinz, Christian Heipke, Markus Ulrich | cs.CV | [PDF](http://arxiv.org/pdf/2405.04345v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become a rapidly growing research field
with the potential to revolutionize typical photogrammetric workflows, such as
those used for 3D scene reconstruction. As input, NeRFs require multi-view
images with corresponding camera poses as well as the interior orientation. In
the typical NeRF workflow, the camera poses and the interior orientation are
estimated in advance with Structure from Motion (SfM). But the quality of the
resulting novel views, which depends on different parameters such as the number
and distribution of available images, as well as the accuracy of the related
camera poses and interior orientation, is difficult to predict. In addition,
SfM is a time-consuming pre-processing step, and its quality strongly depends
on the image content. Furthermore, the undefined scaling factor of SfM hinders
subsequent steps in which metric information is required. In this paper, we
evaluate the potential of NeRFs for industrial robot applications. We propose
an alternative to SfM pre-processing: we capture the input images with a
calibrated camera that is attached to the end effector of an industrial robot
and determine accurate camera poses with metric scale based on the robot
kinematics. We then investigate the quality of the novel views by comparing
them to ground truth, and by computing an internal quality measure based on
ensemble methods. For evaluation purposes, we acquire multiple datasets that
pose challenges for reconstruction typical of industrial applications, like
reflective objects, poor texture, and fine structures. We show that the
robot-based pose determination reaches similar accuracy as SfM in non-demanding
cases, while having clear advantages in more challenging scenarios. Finally, we
present first results of applying the ensemble method to estimate the quality
of the synthetic novel view in the absence of a ground truth.

Comments:
- 8 pages, 8 figures, accepted for publication in The International
  Archives of the Photogrammetry, Remote Sensing and Spatial Information
  Sciences (ISPRS Archives) 2024

---

## DistGrid: Scalable Scene Reconstruction with Distributed  Multi-resolution Hash Grid

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-07 | Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou | cs.CV | [PDF](http://arxiv.org/pdf/2405.04416v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled
and indoor scene reconstruction. However, there exist some challenges when
reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network
capacity, while volume-based NeRFs are heavily memory-consuming when the scene
resolution increases. Recent approaches propose to geographically partition the
scene and learn each sub-region using an individual NeRF. Such partitioning
strategies help volume-based NeRF exceed the single GPU memory limit and scale
to larger scenes. However, this approach requires multiple background NeRF to
handle out-of-partition rays, which leads to redundancy of learning. Inspired
by the fact that the background of current partition is the foreground of
adjacent partition, we propose a scalable scene reconstruction method based on
joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is
divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding
Boxes, and a novel segmented volume rendering method is proposed to handle
cross-boundary rays, thereby eliminating the need for background NeRFs. The
experiments demonstrate that our method outperforms existing methods on all
evaluated large-scale scenes, and provides visually plausible scene
reconstruction. The scalability of our method on reconstruction quality is
further evaluated qualitatively and quantitatively.

Comments:
- Originally submitted to Siggraph Asia 2023

---

## $\textbf{Splat-MOVER}$: Multi-Stage, Open-Vocabulary Robotic  Manipulation via Editable Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-07 | Ola Shorinwa, Johnathan Tucker, Aliyah Smith, Aiden Swann, Timothy Chen, Roya Firoozi, Monroe Kennedy III, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2405.04378v1){: .btn .btn-green } |

**Abstract**: We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic
manipulation, which leverages the editability of Gaussian Splatting (GSplat)
scene representations to enable multi-stage manipulation tasks. Splat-MOVER
consists of: (i) $\textit{ASK-Splat}$, a GSplat representation that distills
latent codes for language semantics and grasp affordance into the 3D scene.
ASK-Splat enables geometric, semantic, and affordance understanding of 3D
scenes, which is critical for many robotics tasks; (ii) $\textit{SEE-Splat}$, a
real-time scene-editing module using 3D semantic masking and infilling to
visualize the motions of objects that result from robot interactions in the
real-world. SEE-Splat creates a "digital twin" of the evolving environment
throughout the manipulation task; and (iii) $\textit{Grasp-Splat}$, a grasp
generation module that uses ASK-Splat and SEE-Splat to propose candidate grasps
for open-world objects. ASK-Splat is trained in real-time from RGB images in a
brief scanning phase prior to operation, while SEE-Splat and Grasp-Splat run in
real-time during operation. We demonstrate the superior performance of
Splat-MOVER in hardware experiments on a Kinova robot compared to two recent
baselines in four single-stage, open-vocabulary manipulation tasks, as well as
in four multi-stage manipulation tasks using the edited scene to reflect scene
changes due to prior manipulation stages, which is not possible with the
existing baselines. Code for this project and a link to the project page will
be made available soon.



---

## Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-06 | Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård | cs.CV | [PDF](http://arxiv.org/pdf/2405.03417v1){: .btn .btn-green } |

**Abstract**: Image-based 3D reconstruction is a challenging task that involves inferring
the 3D shape of an object or scene from a set of input images. Learning-based
methods have gained attention for their ability to directly estimate 3D shapes.
This review paper focuses on state-of-the-art techniques for 3D reconstruction,
including the generation of novel, unseen views. An overview of recent
developments in the Gaussian Splatting method is provided, covering input
types, model structures, output representations, and training strategies.
Unresolved challenges and future directions are also discussed. Given the rapid
progress in this domain and the numerous opportunities for enhancing 3D
reconstruction methods, a comprehensive examination of algorithms appears
essential. Consequently, this study offers a thorough overview of the latest
advancements in Gaussian Splatting.

Comments:
- 24 pages

---

## A Construct-Optimize Approach to Sparse View Synthesis without Camera  Pose

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-06 | Kaiwen Jiang, Yang Fu, Mukund Varma T, Yash Belhe, Xiaolong Wang, Hao Su, Ravi Ramamoorthi | cs.CV | [PDF](http://arxiv.org/pdf/2405.03659v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis from a sparse set of input images is a challenging
problem of great practical interest, especially when camera poses are absent or
inaccurate. Direct optimization of camera poses and usage of estimated depths
in neural radiance field algorithms usually do not produce good results because
of the coupling between poses and depths, and inaccuracies in monocular depth
estimation. In this paper, we leverage the recent 3D Gaussian splatting method
to develop a novel construct-and-optimize method for sparse view synthesis
without camera poses. Specifically, we construct a solution progressively by
using monocular depth and projecting pixels back into the 3D world. During
construction, we optimize the solution by detecting 2D correspondences between
training views and the corresponding rendered images. We develop a unified
differentiable pipeline for camera registration and adjustment of both camera
poses and depths, followed by back-projection. We also introduce a novel notion
of an expected surface in Gaussian splatting, which is critical to our
optimization. These steps enable a coarse solution, which can then be low-pass
filtered and refined using standard optimization methods. We demonstrate
results on the Tanks and Temples and Static Hikes datasets with as few as three
widely-spaced views, showing significantly better quality than competing
methods, including those with approximate camera pose information. Moreover,
our results improve with more views and outperform previous InstantNGP and
Gaussian Splatting algorithms even when using half the dataset.



---

## Blending Distributed NeRFs with Tri-stage Robust Pose Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-05 | Baijun Ye, Caiyun Liu, Xiaoyu Ye, Yuantao Chen, Yuhai Wang, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2405.02880v1){: .btn .btn-green } |

**Abstract**: Due to the limited model capacity, leveraging distributed Neural Radiance
Fields (NeRFs) for modeling extensive urban environments has become a
necessity. However, current distributed NeRF registration approaches encounter
aliasing artifacts, arising from discrepancies in rendering resolutions and
suboptimal pose precision. These factors collectively deteriorate the fidelity
of pose estimation within NeRF frameworks, resulting in occlusion artifacts
during the NeRF blending stage. In this paper, we present a distributed NeRF
system with tri-stage pose optimization. In the first stage, precise poses of
images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine
strategy. In the second stage, we incorporate the inverting Mip-NeRF 360,
coupled with the truncated dynamic low-pass filter, to enable the achievement
of robust and precise poses, termed Frame2Model optimization. On top of this,
we obtain a coarse transformation between NeRFs in different coordinate
systems. In the third stage, we fine-tune the transformation between NeRFs by
Model2Model pose optimization. After obtaining precise transformation
parameters, we proceed to implement NeRF blending, showcasing superior
performance metrics in both real-world and simulation scenarios. Codes and data
will be publicly available at https://github.com/boilcy/Distributed-NeRF.



---

## MVIP-NeRF: Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-05 | Honghua Chen, Chen Change Loy, Xingang Pan | cs.CV | [PDF](http://arxiv.org/pdf/2405.02859v1){: .btn .btn-green } |

**Abstract**: Despite the emergence of successful NeRF inpainting methods built upon
explicit RGB and depth 2D inpainting supervisions, these methods are inherently
constrained by the capabilities of their underlying 2D inpainters. This is due
to two key reasons: (i) independently inpainting constituent images results in
view-inconsistent imagery, and (ii) 2D inpainters struggle to ensure
high-quality geometry completion and alignment with inpainted RGB images.
  To overcome these limitations, we propose a novel approach called MVIP-NeRF
that harnesses the potential of diffusion priors for NeRF inpainting,
addressing both appearance and geometry aspects. MVIP-NeRF performs joint
inpainting across multiple views to reach a consistent solution, which is
achieved via an iterative optimization process based on Score Distillation
Sampling (SDS). Apart from recovering the rendered RGB images, we also extract
normal maps as a geometric representation and define a normal SDS loss that
motivates accurate geometry inpainting and alignment with the appearance.
Additionally, we formulate a multi-view SDS score function to distill
generative priors simultaneously from different view images, ensuring
consistent visual completion when dealing with large view variations. Our
experimental results show better appearance and geometry recovery than previous
NeRF inpainting methods.

Comments:
- 14 pages, 10 figures, conference

---

## TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for  Dynamic UAV-based Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-04 | Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon | cs.CV | [PDF](http://arxiv.org/pdf/2405.02762v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a new approach to bridge the domain gap between
synthetic and real-world data for un- manned aerial vehicle (UAV)-based
perception. Our formu- lation is designed for dynamic scenes, consisting of
moving objects or human actions, where the goal is to recognize the pose or
actions. We propose an extension of K-Planes Neural Radiance Field (NeRF),
wherein our algorithm stores a set of tiered feature vectors. The tiered
feature vectors are generated to effectively model conceptual information about
a scene as well as an image decoder that transforms output feature maps into
RGB images. Our technique leverages the information amongst both static and
dynamic objects within a scene and is able to capture salient scene attributes
of high altitude videos. We evaluate its performance on challenging datasets,
including Okutama Action and UG2, and observe considerable improvement in
accuracy over state of the art aerial perception algorithms.

Comments:
- 8 pages, submitted to IROS2024

---

## ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface  Uncertainty

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-04 | Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2405.02568v1){: .btn .btn-green } |

**Abstract**: Active learning in 3D scene reconstruction has been widely studied, as
selecting informative training views is critical for the reconstruction.
Recently, Neural Radiance Fields (NeRF) variants have shown performance
increases in active 3D reconstruction using image rendering or geometric
uncertainty. However, the simultaneous consideration of both uncertainties in
selecting informative views remains unexplored, while utilizing different types
of uncertainty can reduce the bias that arises in the early training stage with
sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate
views considering both uncertainties. ActiveNeuS provides a way to accumulate
image rendering uncertainty while avoiding the bias that the estimated
densities can introduce. ActiveNeuS computes the neural implicit surface
uncertainty, providing the color uncertainty along with the surface
information. It efficiently handles the bias by using the surface information
and a grid, enabling the fast selection of diverse viewpoints. Our method
outperforms previous works on popular datasets, Blender and DTU, showing that
the views selected by ActiveNeuS significantly improve performance.



---

## Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic  Solids

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2405.02386v1){: .btn .btn-green } |

**Abstract**: Despite significant advancements in Neural Radiance Fields (NeRFs), the
renderings may still suffer from aliasing and blurring artifacts, since it
remains a fundamental challenge to effectively and efficiently characterize
anisotropic areas induced by the cone-casting procedure. This paper introduces
a Ripmap-Encoded Platonic Solid representation to precisely and efficiently
featurize 3D anisotropic areas, achieving high-fidelity anti-aliasing
renderings. Central to our approach are two key components: Platonic Solid
Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D
space onto the unparalleled faces of a certain Platonic solid, such that the
anisotropic 3D areas can be projected onto planes with distinguishable
characterization. Meanwhile, each face of the Platonic solid is encoded by the
Ripmap encoding, which is constructed by anisotropically pre-filtering a
learnable feature grid, to enable featurzing the projected anisotropic areas
both precisely and efficiently by the anisotropic area-sampling. Extensive
experiments on both well-established synthetic datasets and a newly captured
real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art
rendering quality, particularly excelling in the fine details of repetitive
structures and textures, while maintaining relatively swift training times.

Comments:
- SIGGRAPH 2024, Project page: https://junchenliu77.github.io/Rip-NeRF
  , Code: https://github.com/JunchenLiu77/Rip-NeRF

---

## Learning Robot Soccer from Egocentric Vision with Deep Reinforcement  Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil Sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, Nicolas Heess | cs.RO | [PDF](http://arxiv.org/pdf/2405.02425v1){: .btn .btn-green } |

**Abstract**: We apply multi-agent deep reinforcement learning (RL) to train end-to-end
robot soccer policies with fully onboard computation and sensing via egocentric
RGB vision. This setting reflects many challenges of real-world robotics,
including active perception, agile full-body control, and long-horizon planning
in a dynamic, partially-observable, multi-agent domain. We rely on large-scale,
simulation-based data generation to obtain complex behaviors from egocentric
vision which can be successfully transferred to physical robots using low-cost
sensors. To achieve adequate visual realism, our simulation combines rigid-body
physics with learned, realistic rendering via multiple Neural Radiance Fields
(NeRFs). We combine teacher-based multi-agent RL and cross-experiment data
reuse to enable the discovery of sophisticated soccer strategies. We analyze
active-perception behaviors including object tracking and ball seeking that
emerge when simply optimizing perception-agnostic soccer play. The agents
display equivalent levels of performance and agility as policies with access to
privileged, ground-truth state. To our knowledge, this paper constitutes a
first demonstration of end-to-end training for multi-agent robot soccer,
mapping raw pixel observations to joint-level actions, that can be deployed in
the real world. Videos of the game-play and analyses can be seen on our website
https://sites.google.com/view/vision-soccer .



---

## WateRF: Robust Watermarks in Radiance Fields for Protection of  Copyrights

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2405.02066v1){: .btn .btn-green } |

**Abstract**: The advances in the Neural Radiance Fields (NeRF) research offer extensive
applications in diverse domains, but protecting their copyrights has not yet
been researched in depth. Recently, NeRF watermarking has been considered one
of the pivotal solutions for safely deploying NeRF-based 3D representations.
However, existing methods are designed to apply only to implicit or explicit
NeRF representations. In this work, we introduce an innovative watermarking
method that can be employed in both representations of NeRF. This is achieved
by fine-tuning NeRF to embed binary messages in the rendering process. In
detail, we propose utilizing the discrete wavelet transform in the NeRF space
for watermarking. Furthermore, we adopt a deferred back-propagation technique
and introduce a combination with the patch-wise loss to improve rendering
quality and bit accuracy with minimum trade-offs. We evaluate our method in
three different aspects: capacity, invisibility, and robustness of the embedded
watermarks in the 2D-rendered images. Our method achieves state-of-the-art
performance with faster training speed over the compared state-of-the-art
methods.



---

## HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft  HoloLens 2

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Miriam Jäger, Theodor Kapler, Michael Feßenbecker, Felix Birkelbach, Markus Hillemann, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2405.02005v1){: .btn .btn-green } |

**Abstract**: In the fields of photogrammetry, computer vision and computer graphics, the
task of neural 3D scene reconstruction has led to the exploration of various
techniques. Among these, 3D Gaussian Splatting stands out for its explicit
representation of scenes using 3D Gaussians, making it appealing for tasks like
3D point cloud extraction and surface reconstruction. Motivated by its
potential, we address the domain of 3D scene reconstruction, aiming to leverage
the capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting.
We present HoloGS, a novel workflow utilizing HoloLens sensor data, which
bypasses the need for pre-processing steps like Structure from Motion by
instantly accessing the required input data i.e. the images, camera poses and
the point cloud from depth sensing. We provide comprehensive investigations,
including the training process and the rendering quality, assessed through the
Peak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified
point cloud from Gaussian centers, measured by Chamfer Distance. We evaluate
our approach on two self-captured scenes: An outdoor scene of a cultural
heritage statue and an indoor scene of a fine-structured plant. Our results
show that the HoloLens data, including RGB images, corresponding camera poses,
and depth sensing based point clouds to initialize the Gaussians, are suitable
as input for 3D Gaussian Splatting.

Comments:
- 8 pages, 9 figures, 2 tables. Will be published in the ISPRS The
  International Archives of Photogrammetry, Remote Sensing and Spatial
  Information Sciences

---

## NeRF in Robotics: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-02 | Guangming Wang, Lei Pan, Songyou Peng, Shaohui Liu, Chenfeng Xu, Yanzi Miao, Wei Zhan, Masayoshi Tomizuka, Marc Pollefeys, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2405.01333v1){: .btn .btn-green } |

**Abstract**: Meticulous 3D environment representations have been a longstanding goal in
computer vision and robotics fields. The recent emergence of neural implicit
representations has introduced radical innovation to this field as implicit
representations enable numerous capabilities. Among these, the Neural Radiance
Field (NeRF) has sparked a trend because of the huge representational
advantages, such as simplified mathematical models, compact environment
storage, and continuous scene representations. Apart from computer vision, NeRF
has also shown tremendous potential in the field of robotics. Thus, we create
this survey to provide a comprehensive understanding of NeRF in the field of
robotics. By exploring the advantages and limitations of NeRF, as well as its
current applications and future potential, we hope to shed light on this
promising area of research. Our survey is divided into two main sections:
\textit{The Application of NeRF in Robotics} and \textit{The Advance of NeRF in
Robotics}, from the perspective of how NeRF enters the field of robotics. In
the first section, we introduce and analyze some works that have been or could
be used in the field of robotics from the perception and interaction
perspectives. In the second section, we show some works related to improving
NeRF's own properties, which are essential for deploying NeRF in the field of
robotics. In the discussion section of the review, we summarize the existing
challenges and provide some valuable future research directions for reference.

Comments:
- 21 pages, 19 figures

---

## Depth Priors in Removal Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhihao Guo, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2405.00630v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown impressive results in 3D
reconstruction and generating novel views. A key challenge within NeRF is the
editing of reconstructed scenes, such as object removal, which requires
maintaining consistency across multiple views and ensuring high-quality
synthesised perspectives. Previous studies have incorporated depth priors,
typically from LiDAR or sparse depth measurements provided by COLMAP, to
improve the performance of object removal in NeRF. However, these methods are
either costly or time-consuming. In this paper, we propose a novel approach
that integrates monocular depth estimates with NeRF-based object removal models
to significantly reduce time consumption and enhance the robustness and quality
of scene generation and object removal. We conducted a thorough evaluation of
COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy
in depth map generation. Our findings suggest that COLMAP can serve as an
effective alternative to a ground truth depth map where such information is
missing or costly to obtain. Additionally, we integrated various monocular
depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess
their capacity to improve object removal performance. Our experimental results
highlight the potential of monocular depth estimation to substantially improve
NeRF applications.

Comments:
- 15 pages

---

## NeRF-Guided Unsupervised Learning of RGB-D Registration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2405.00507v1){: .btn .btn-green } |

**Abstract**: This paper focuses on training a robust RGB-D registration model without
ground-truth pose supervision. Existing methods usually adopt a pairwise
training strategy based on differentiable rendering, which enforces the
photometric and the geometric consistency between the two registered frames as
supervision. However, this frame-to-frame framework suffers from poor
multi-view consistency due to factors such as lighting changes, geometry
occlusion and reflective materials. In this paper, we present NeRF-UR, a novel
frame-to-model optimization framework for unsupervised RGB-D registration.
Instead of frame-to-frame consistency, we leverage the neural radiance field
(NeRF) as a global model of the scene and use the consistency between the input
and the NeRF-rerendered frames for pose optimization. This design can
significantly improve the robustness in scenarios with poor multi-view
consistency and provides better learning signal for the registration model.
Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset,
Sim-RGBD, through a photo-realistic simulator to warm up the registration
model. By first training the registration model on Sim-RGBD and later
unsupervisedly fine-tuning on real data, our framework enables distilling the
capability of feature extraction and registration from simulation to reality.
Our method outperforms the state-of-the-art counterparts on two popular indoor
RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper
reproduction.



---

## LidaRF: Delving into Lidar for Neural Radiance Field on Street Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Shanlin Sun, Bingbing Zhuang, Ziyu Jiang, Buyu Liu, Xiaohui Xie, Manmohan Chandraker | cs.CV | [PDF](http://arxiv.org/pdf/2405.00900v2){: .btn .btn-green } |

**Abstract**: Photorealistic simulation plays a crucial role in applications such as
autonomous driving, where advances in neural radiance fields (NeRFs) may allow
better scalability through the automatic creation of digital 3D assets.
However, reconstruction quality suffers on street scenes due to largely
collinear camera motions and sparser samplings at higher speeds. On the other
hand, the application often demands rendering from camera views that deviate
from the inputs to accurately simulate behaviors like lane changes. In this
paper, we propose several insights that allow a better utilization of Lidar
data to improve NeRF quality on street scenes. First, our framework learns a
geometric scene representation from Lidar, which is fused with the implicit
grid-based representation for radiance decoding, thereby supplying stronger
geometric information offered by explicit point cloud. Second, we put forth a
robust occlusion-aware depth supervision scheme, which allows utilizing
densified Lidar points by accumulation. Third, we generate augmented training
views from Lidar points for further improvement. Our insights translate to
largely improved novel view synthesis under real driving scenes.

Comments:
- CVPR2024 Highlights

---

## Spectrally Pruned Gaussian Fields with Neural Compensation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Runyi Yang, Zhenxin Zhu, Zhou Jiang, Baijun Ye, Xiaoxue Chen, Yifei Zhang, Yuantao Chen, Jian Zhao, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2405.00676v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered
attention for its fast rendering speed and high rendering quality. However,
this comes with high memory consumption, e.g., a well-trained Gaussian field
may utilize three million Gaussian primitives and over 700 MB of memory. We
credit this high memory footprint to the lack of consideration for the
relationship between primitives. In this paper, we propose a memory-efficient
Gaussian field named SUNDAE with spectral pruning and neural compensation. On
one hand, we construct a graph on the set of Gaussian primitives to model their
relationship and design a spectral down-sampling module to prune out primitives
while preserving desired signals. On the other hand, to compensate for the
quality loss of pruning Gaussians, we exploit a lightweight neural network head
to mix splatted features, which effectively compensates for quality losses
while capturing the relationship between primitives in its weights. We
demonstrate the performance of SUNDAE with extensive results. For example,
SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla
Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB
memory, on the Mip-NeRF360 dataset. Codes are publicly available at
https://runyiyang.github.io/projects/SUNDAE/.

Comments:
- Code: https://github.com/RunyiYang/SUNDAE Project page:
  https://runyiyang.github.io/projects/SUNDAE/
