---
layout: default
title: May 2024
parent: Papers
nav_order: 202405
---

<!---metadata--->


## Dynamic NeRF: A Review

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-14 | Jinwei Lin | cs.CV | [PDF](http://arxiv.org/pdf/2405.08609v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field(NeRF) is an novel implicit method to achieve the 3D
reconstruction and representation with a high resolution. After the first
research of NeRF is proposed, NeRF has gained a robust developing power and is
booming in the 3D modeling, representation and reconstruction areas. However
the first and most of the followed research projects based on NeRF is static,
which are weak in the practical applications. Therefore, more researcher are
interested and focused on the study of dynamic NeRF that is more feasible and
useful in practical applications or situations. Compared with the static NeRF,
implementing the Dynamic NeRF is more difficult and complex. But Dynamic is
more potential in the future even is the basic of Editable NeRF. In this
review, we made a detailed and abundant statement for the development and
important implementation principles of Dynamci NeRF. The analysis of main
principle and development of Dynamic NeRF is from 2021 to 2023, including the
most of the Dynamic NeRF projects. What is more, with colorful and novel
special designed figures and table, We also made a detailed comparison and
analysis of different features of various of Dynamic. Besides, we analyzed and
discussed the key methods to implement a Dynamic NeRF. The volume of the
reference papers is large. The statements and comparisons are multidimensional.
With a reading of this review, the whole development history and most of the
main design method or principles of Dynamic NeRF can be easy understood and
gained.

Comments:
- 25 pages

---

## Synergistic Integration of Coordinate Network and Tensorial Feature for  Improving Neural Radiance Fields from Sparse Inputs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-13 | Mingyu Kim, Jun-Seong Kim, Se-Young Yun, Jin-Hwa Kim | cs.CV | [PDF](http://arxiv.org/pdf/2405.07857v1){: .btn .btn-green } |

**Abstract**: The multi-plane representation has been highlighted for its fast training and
inference across static and dynamic neural radiance fields. This approach
constructs relevant features via projection onto learnable grids and
interpolating adjacent vertices. However, it has limitations in capturing
low-frequency details and tends to overuse parameters for low-frequency
features due to its bias toward fine details, despite its multi-resolution
concept. This phenomenon leads to instability and inefficiency when training
poses are sparse. In this work, we propose a method that synergistically
integrates multi-plane representation with a coordinate-based network known for
strong bias toward low-frequency signals. The coordinate-based network is
responsible for capturing low-frequency details, while the multi-plane
representation focuses on capturing fine-grained details. We demonstrate that
using residual connections between them seamlessly preserves their own inherent
properties. Additionally, the proposed progressive training scheme accelerates
the disentanglement of these two features. We empirically show that the
proposed method achieves comparable results to explicit encoding with fewer
parameters, and particularly, it outperforms others for the static and dynamic
NeRFs under sparse inputs.

Comments:
- ICML2024 ; Project page is accessible at
  https://mingyukim87.github.io/SynergyNeRF ; Code is available at
  https://github.com/MingyuKim87/SynergyNeRF

---

## GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting  Editing with Image Prompting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-13 | Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao | cs.CV | [PDF](http://arxiv.org/pdf/2405.07472v1){: .btn .btn-green } |

**Abstract**: The increasing prominence of e-commerce has underscored the importance of
Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D
realm and rely heavily on extensive data for training. Research on 3D VTON
primarily centers on garment-body shape compatibility, a topic extensively
covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion
model has now been adapted for 3D editing via multi-viewpoint editing. In this
work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating
Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless
transition from 2D to 3D VTON, we propose, for the first time, the use of only
images as editing prompts for 3D editing. To further address issues, e.g., face
blurring, garment inaccuracy, and degraded viewpoint quality during editing, we
devise a three-stage refinement strategy to gradually mitigate potential
issues. Furthermore, we introduce a new editing strategy termed Edit Recall
Reconstruction (ERR) to tackle the limitations of previous editing strategies
in leading to complex geometric changes. Our comprehensive experiments
demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D
VTON while also establishing a novel starting point for image-prompting 3D
scene editing.

Comments:
- On-going work

---

## Point Resampling and Ray Transformation Aid to Editable NeRF Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-12 | Zhenyang Li, Zilong Chen, Feifan Qu, Mingqing Wang, Yizhou Zhao, Kai Zhang, Yifan Peng | cs.CV | [PDF](http://arxiv.org/pdf/2405.07306v1){: .btn .btn-green } |

**Abstract**: In NeRF-aided editing tasks, object movement presents difficulties in
supervision generation due to the introduction of variability in object
positions. Moreover, the removal operations of certain scene objects often lead
to empty regions, presenting challenges for NeRF models in inpainting them
effectively. We propose an implicit ray transformation strategy, allowing for
direct manipulation of the 3D object's pose by operating on the neural-point in
NeRF rays. To address the challenge of inpainting potential empty regions, we
present a plug-and-play inpainting module, dubbed differentiable neural-point
resampling (DNR), which interpolates those regions in 3D space at the original
ray locations within the implicit space, thereby facilitating object removal &
scene inpainting tasks. Importantly, employing DNR effectively narrows the gap
between ground truth and predicted implicit features, potentially increasing
the mutual information (MI) of the features across rays. Then, we leverage DNR
and ray transformation to construct a point-based editable NeRF pipeline
PR^2T-NeRF. Results primarily evaluated on 3D object removal & inpainting tasks
indicate that our pipeline achieves state-of-the-art performance. In addition,
our pipeline supports high-quality rendering visualization for diverse editing
operations without necessitating extra supervision.



---

## Hologram: Realtime Holographic Overlays via LiDAR Augmented  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-12 | Ekansh Agrawal | cs.CV | [PDF](http://arxiv.org/pdf/2405.07178v1){: .btn .btn-green } |

**Abstract**: Guided by the hologram technology of the infamous Star Wars franchise, I
present an application that creates real-time holographic overlays using LiDAR
augmented 3D reconstruction. Prior attempts involve SLAM or NeRFs which either
require highly calibrated scenes, incur steep computation costs, or fail to
render dynamic scenes. I propose 3 high-fidelity reconstruction tools that can
run on a portable device, such as a iPhone 14 Pro, which can allow for metric
accurate facial reconstructions. My systems enable interactive and immersive
holographic experiences that can be used for a wide range of applications,
including augmented reality, telepresence, and entertainment.



---

## TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural  Radiance Field Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-11 | Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu | cs.CV | [PDF](http://arxiv.org/pdf/2405.07027v1){: .btn .btn-green } |

**Abstract**: The reliance on accurate camera poses is a significant barrier to the
widespread deployment of Neural Radiance Fields (NeRF) models for 3D
reconstruction and SLAM tasks. The existing method introduces monocular depth
priors to jointly optimize the camera poses and NeRF, which fails to fully
exploit the depth priors and neglects the impact of their inherent noise. In
this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that
enables training NeRF from unknown camera poses - by jointly optimizing
learnable parameters of the radiance field and camera poses. Our approach
explicitly utilizes monocular depth priors through three key advancements: 1)
we propose a novel depth-based ray sampling strategy based on the truncated
normal distribution, which improves the convergence speed and accuracy of pose
estimation; 2) to circumvent local minima and refine depth geometry, we
introduce a coarse-to-fine training strategy that progressively improves the
depth precision; 3) we propose a more robust inter-frame point constraint that
enhances robustness against depth noise during training. The experimental
results on three datasets demonstrate that TD-NeRF achieves superior
performance in the joint optimization of camera pose and NeRF, surpassing prior
works, and generates more accurate depth geometry. The implementation of our
method has been released at https://github.com/nubot-nudt/TD-NeRF.



---

## Direct Learning of Mesh and Appearance via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-11 | Ancheng Lin, Jun Li | cs.CV | [PDF](http://arxiv.org/pdf/2405.06945v1){: .btn .btn-green } |

**Abstract**: Accurately reconstructing a 3D scene including explicit geometry information
is both attractive and challenging. Geometry reconstruction can benefit from
incorporating differentiable appearance models, such as Neural Radiance Fields
and 3D Gaussian Splatting (3DGS). In this work, we propose a learnable scene
model that incorporates 3DGS with an explicit geometry representation, namely a
mesh. Our model learns the mesh and appearance in an end-to-end manner, where
we bind 3D Gaussians to the mesh faces and perform differentiable rendering of
3DGS to obtain photometric supervision. The model creates an effective
information pathway to supervise the learning of the scene, including the mesh.
Experimental results demonstrate that the learned scene model not only achieves
state-of-the-art rendering quality but also supports manipulation using the
explicit mesh. In addition, our model has a unique advantage in adapting to
scene updates, thanks to the end-to-end learning of both mesh and appearance.



---

## Residual-NeRF: Learning Residual NeRFs for Transparent Object  Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski | cs.CV | [PDF](http://arxiv.org/pdf/2405.06181v1){: .btn .btn-green } |

**Abstract**: Transparent objects are ubiquitous in industry, pharmaceuticals, and
households. Grasping and manipulating these objects is a significant challenge
for robots. Existing methods have difficulty reconstructing complete depth maps
for challenging transparent objects, leaving holes in the depth reconstruction.
Recent work has shown neural radiance fields (NeRFs) work well for depth
perception in scenes with transparent objects, and these depth maps can be used
to grasp transparent objects with high accuracy. NeRF-based depth
reconstruction can still struggle with especially challenging transparent
objects and lighting conditions. In this work, we propose Residual-NeRF, a
method to improve depth perception and training speed for transparent objects.
Robots often operate in the same area, such as a kitchen. By first learning a
background NeRF of the scene without transparent objects to be manipulated, we
reduce the ambiguity faced by learning the changes with the new object. We
propose training two additional networks: a residual NeRF learns to infer
residual RGB values and densities, and a Mixnet learns how to combine
background and residual NeRFs. We contribute synthetic and real experiments
that suggest Residual-NeRF improves depth perception of transparent objects.
The results on synthetic data suggest Residual-NeRF outperforms the baselines
with a 46.1% lower RMSE and a 29.5% lower MAE. Real-world qualitative
experiments suggest Residual-NeRF leads to more robust depth maps with less
noise and fewer holes. Website: https://residual-nerf.github.io



---

## I3DGS: Improve 3D Gaussian Splatting from Multiple Dimensions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Jinwei Lin | cs.CV | [PDF](http://arxiv.org/pdf/2405.06408v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting is a novel method for 3D view synthesis, which can gain
an implicit neural learning rendering result than the traditional neural
rendering technology but keep the more high-definition fast rendering speed.
But it is still difficult to achieve a fast enough efficiency on 3D Gaussian
Splatting for the practical applications. To Address this issue, we propose the
I3DS, a synthetic model performance improvement evaluation solution and
experiments test. From multiple and important levels or dimensions of the
original 3D Gaussian Splatting, we made more than two thousand various kinds of
experiments to test how the selected different items and components can make an
impact on the training efficiency of the 3D Gaussian Splatting model. In this
paper, we will share abundant and meaningful experiences and methods about how
to improve the training, performance and the impacts caused by different items
of the model. A special but normal Integer compression in base 95 and a
floating-point compression in base 94 with ASCII encoding and decoding
mechanism is presented. Many real and effective experiments and test results or
phenomena will be recorded. After a series of reasonable fine-tuning, I3DS can
gain excellent performance improvements than the previous one. The project code
is available as open source.

Comments:
- 16 pages

---

## MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth  Smooth Regularization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2405.06241v1){: .btn .btn-green } |

**Abstract**: This letter introduces a novel framework for dense Visual Simultaneous
Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently Gaussian
Splatting-based SLAM has yielded promising results, but rely on RGB-D input and
is weak in tracking. To address these limitations, we uniquely integrates
advanced sparse visual odometry with a dense Gaussian Splatting scene
representation for the first time, thereby eliminating the dependency on depth
maps typical of Gaussian Splatting-based SLAM systems and enhancing tracking
robustness. Here, the sparse visual odometry tracks camera poses in RGB stream,
while Gaussian Splatting handles map reconstruction. These components are
interconnected through a Multi-View Stereo (MVS) depth estimation network. And
we propose a depth smooth loss to reduce the negative effect of estimated depth
maps. Furthermore, the consistency in scale between the sparse visual odometry
and the dense Gaussian map is preserved by Sparse-Dense Adjustment Ring (SDAR).
We have evaluated our system across various synthetic and real-world datasets.
The accuracy of our pose estimation surpasses existing methods and achieves
state-of-the-art performance. Additionally, it outperforms previous monocular
methods in terms of novel view synthesis fidelity, matching the results of
neural SLAM systems that utilize RGB-D input.

Comments:
- This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible

---

## Aerial-NeRF: Adaptive Spatial Partitioning and Sampling for Large-Scale  Aerial Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu | cs.CV | [PDF](http://arxiv.org/pdf/2405.06214v1){: .btn .btn-green } |

**Abstract**: Recent progress in large-scale scene rendering has yielded Neural Radiance
Fields (NeRF)-based models with an impressive ability to synthesize scenes
across small objects and indoor scenes. Nevertheless, extending this idea to
large-scale aerial rendering poses two critical problems. Firstly, a single
NeRF cannot render the entire scene with high-precision for complex large-scale
aerial datasets since the sampling range along each view ray is insufficient to
cover buildings adequately. Secondly, traditional NeRFs are infeasible to train
on one GPU to enable interactive fly-throughs for modeling massive images.
Instead, existing methods typically separate the whole scene into multiple
regions and train a NeRF on each region, which are unaccustomed to different
flight trajectories and difficult to achieve fast rendering. To that end, we
propose Aerial-NeRF with three innovative modifications for jointly adapting
NeRF in large-scale aerial rendering: (1) Designing an adaptive spatial
partitioning and selection method based on drones' poses to adapt different
flight trajectories; (2) Using similarity of poses instead of (expert) network
for rendering speedup to determine which region a new viewpoint belongs to; (3)
Developing an adaptive sampling approach for rendering performance improvement
to cover the entire buildings at different heights. Extensive experiments have
conducted to verify the effectiveness and efficiency of Aerial-NeRF, and new
state-of-the-art results have been achieved on two public large-scale aerial
datasets and presented SCUTic dataset. Note that our model allows us to perform
rendering over 4 times as fast as compared to multiple competitors. Our
dataset, code, and model are publicly available at https://drliuqi.github.io/.



---

## OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Jinwei Lin | cs.CV | [PDF](http://arxiv.org/pdf/2405.06547v1){: .btn .btn-green } |

**Abstract**: One image to editable dynamic 3D model and video generation is novel
direction and change in the research area of single image to 3D representation
or 3D reconstruction of image. Gaussian Splatting has demonstrated its
advantages in implicit 3D reconstruction, compared with the original Neural
Radiance Fields. As the rapid development of technologies and principles,
people tried to used the Stable Diffusion models to generate targeted models
with text instructions. However, using the normal implicit machine learning
methods is hard to gain the precise motions and actions control, further more,
it is difficult to generate a long content and semantic continuous 3D video. To
address this issue, we propose the OneTo3D, a method and theory to used one
single image to generate the editable 3D model and generate the targeted
semantic continuous time-unlimited 3D video. We used a normal basic Gaussian
Splatting model to generate the 3D model from a single image, which requires
less volume of video memory and computer calculation ability. Subsequently, we
designed an automatic generation and self-adaptive binding mechanism for the
object armature. Combined with the re-editable motions and actions analyzing
and controlling algorithm we proposed, we can achieve a better performance than
the SOTA projects in the area of building the 3D model precise motions and
actions control, and generating a stable semantic continuous time-unlimited 3D
video with the input text instructions. Here we will analyze the detailed
implementation methods and theories analyses. Relative comparisons and
conclusions will be presented. The project code is open source.

Comments:
- 24 pages, 13 figures, 2 tables

---

## LIVE: LaTex Interactive Visual Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Jinwei Lin | cs.HC | [PDF](http://arxiv.org/pdf/2405.06762v1){: .btn .btn-green } |

**Abstract**: LaTex coding is one of the main methods of writing an academic paper. When
writing a paper, abundant proper visual or graphic components will represent
more information volume than the textual data. However, most of the
implementation of LaTex graphic items are designed as static items that have
some weaknesses in representing more informative figures or tables with an
interactive reading experience. To address this problem, we propose LIVE, a
novel design methods idea to design interactive LaTex graphic items. To make a
lucid representation of the main idea of LIVE, we designed several novels
representing implementations that are interactive and enough explanation for
the basic level principles. Using LIVE can design more graphic items, which we
call the Gitems, and easily and automatically get the relationship of the
mutual application of a specific range of papers, which will add more vitality
and performance factors into writing of traditional papers especially the
review papers. For vividly representing the functions of LIVE, we use the
papers from NeRF as the example reference papers. The code of the
implementation project is open source.

Comments:
- 8 pages, double column, ieee

---

## SketchDream: Sketch-based Text-to-3D Generation and Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-10 | Feng-Lin Liu, Hongbo Fu, Yu-Kun Lai, Lin Gao | cs.GR | [PDF](http://arxiv.org/pdf/2405.06461v2){: .btn .btn-green } |

**Abstract**: Existing text-based 3D generation methods generate attractive results but
lack detailed geometry control. Sketches, known for their conciseness and
expressiveness, have contributed to intuitive 3D modeling but are confined to
producing texture-less mesh models within predefined categories. Integrating
sketch and text simultaneously for 3D generation promises enhanced control over
geometry and appearance but faces challenges from 2D-to-3D translation
ambiguity and multi-modal condition integration. Moreover, further editing of
3D models in arbitrary views will give users more freedom to customize their
models. However, it is difficult to achieve high generation quality, preserve
unedited regions, and manage proper interactions between shape components. To
solve the above issues, we propose a text-driven 3D content generation and
editing method, SketchDream, which supports NeRF generation from given
hand-drawn sketches and achieves free-view sketch-based local editing. To
tackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view
image generation diffusion model, which leverages depth guidance to establish
spatial correspondence. A 3D ControlNet with a 3D attention module is utilized
to control multi-view images and ensure their 3D consistency. To support local
editing, we further propose a coarse-to-fine editing approach: the coarse phase
analyzes component interactions and provides 3D masks to label edited regions,
while the fine stage generates realistic results with refined details by local
enhancement. Extensive experiments validate that our method generates
higher-quality results compared with a combination of 2D ControlNet and
image-to-3D generation techniques and achieves detailed control compared with
existing diffusion-based 3D editing approaches.



---

## DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian  Representation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Sitian Shen, Jing Xu, Yuheng Yuan, Xingyi Yang, Qiuhong Shen, Xinchao Wang | cs.GR | [PDF](http://arxiv.org/pdf/2405.05800v1){: .btn .btn-green } |

**Abstract**: User-friendly 3D object editing is a challenging task that has attracted
significant attention recently. The limitations of direct 3D object editing
without 2D prior knowledge have prompted increased attention towards utilizing
2D generative models for 3D editing. While existing methods like Instruct
NeRF-to-NeRF offer a solution, they often lack user-friendliness, particularly
due to semantic guided editing. In the realm of 3D representation, 3D Gaussian
Splatting emerges as a promising approach for its efficiency and natural
explicit property, facilitating precise editing tasks. Building upon these
insights, we propose DragGaussian, a 3D object drag-editing framework based on
3D Gaussian Splatting, leveraging diffusion models for interactive image
editing with open-vocabulary input. This framework enables users to perform
drag-based editing on pre-trained 3D Gaussian object models, producing modified
2D images through multi-view consistent editing. Our contributions include the
introduction of a new task, the development of DragGaussian for interactive
point-based 3D editing, and comprehensive validation of its effectiveness
through qualitative and quantitative experiments.



---

## Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Yuhang Ming, Xingrui Yang, Weihan Wang, Zheng Chen, Jinglun Feng, Yifan Xing, Guofeng Zhang | cs.RO | [PDF](http://arxiv.org/pdf/2405.05526v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D
scene representation, offering high-fidelity renderings and reconstructions
from a set of sparse and unstructured sensor data. In the context of autonomous
robotics, where perception and understanding of the environment are pivotal,
NeRF holds immense promise for improving performance. In this paper, we present
a comprehensive survey and analysis of the state-of-the-art techniques for
utilizing NeRF to enhance the capabilities of autonomous robots. We especially
focus on the perception, localization and navigation, and decision-making
modules of autonomous robots and delve into tasks crucial for autonomous
operation, including 3D reconstruction, segmentation, pose estimation,
simultaneous localization and mapping (SLAM), navigation and planning, and
interaction. Our survey meticulously benchmarks existing NeRF-based methods,
providing insights into their strengths and limitations. Moreover, we explore
promising avenues for future research and development in this domain. Notably,
we discuss the integration of advanced techniques such as 3D Gaussian splatting
(3DGS), large language models (LLM), and generative AIs, envisioning enhanced
reconstruction efficiency, scene understanding, decision-making capabilities.
This survey serves as a roadmap for researchers seeking to leverage NeRFs to
empower autonomous robots, paving the way for innovative solutions that can
navigate and interact seamlessly in complex environments.

Comments:
- 32 pages, 5 figures, 8 tables

---

## FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Yikun Ma, Dandan Zhan, Zhi Jin | cs.CV | [PDF](http://arxiv.org/pdf/2405.05768v1){: .btn .btn-green } |

**Abstract**: Text-driven 3D indoor scene generation holds broad applications, ranging from
gaming and smart homes to AR/VR applications. Fast and high-fidelity scene
generation is paramount for ensuring user-friendly experiences. However,
existing methods are characterized by lengthy generation processes or
necessitate the intricate manual specification of motion parameters, which
introduces inconvenience for users. Furthermore, these methods often rely on
narrow-field viewpoint iterative generations, compromising global consistency
and overall scene quality. To address these issues, we propose FastScene, a
framework for fast and higher-quality 3D scene generation, while maintaining
the scene consistency. Specifically, given a text prompt, we generate a
panorama and estimate its depth, since the panorama encompasses information
about the entire scene and exhibits explicit geometric constraints. To obtain
high-quality novel views, we introduce the Coarse View Synthesis (CVS) and
Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene
consistency and view quality. Subsequently, we utilize Multi-View Projection
(MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for
scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses
other methods in both generation speed and quality with better scene
consistency. Notably, guided only by a text prompt, FastScene can generate a 3D
scene within a mere 15 minutes, which is at least one hour faster than
state-of-the-art methods, making it a paradigm for user-friendly scene
generation.

Comments:
- Accepted by IJCAI-2024

---

## RPBG: Towards Robust Neural Point-based Graphics in the Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Qingtian Zhu, Zizhuang Wei, Zhongtian Zheng, Yifan Zhan, Zhuyu Yao, Jiawang Zhang, Kejian Wu, Yinqiang Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2405.05663v1){: .btn .btn-green } |

**Abstract**: Point-based representations have recently gained popularity in novel view
synthesis, for their unique advantages, e.g., intuitive geometric
representation, simple manipulation, and faster convergence. However, based on
our observation, these point-based neural re-rendering methods are only
expected to perform well under ideal conditions and suffer from noisy, patchy
points and unbounded scenes, which are challenging to handle but defacto common
in real applications. To this end, we revisit one such influential method,
known as Neural Point-based Graphics (NPBG), as our baseline, and propose
Robust Point-based Graphics (RPBG). We in-depth analyze the factors that
prevent NPBG from achieving satisfactory renderings on generic datasets, and
accordingly reform the pipeline to make it more robust to varying datasets
in-the-wild. Inspired by the practices in image restoration, we greatly enhance
the neural renderer to enable the attention-based correction of point
visibility and the inpainting of incomplete rasterization, with only acceptable
overheads. We also seek for a simple and lightweight alternative for
environment modeling and an iterative method to alleviate the problem of poor
geometry. By thorough evaluation on a wide range of datasets with different
shooting conditions and camera trajectories, RPBG stably outperforms the
baseline by a large margin, and exhibits its great robustness over
state-of-the-art NeRF-based variants. Code available at
https://github.com/QT-Zhu/RPBG.



---

## NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Mingrui Li, Jingwei Huang, Lei Sun, Aaron Xuxiang Tian, Tianchen Deng, Hongyu Wang | cs.RO | [PDF](http://arxiv.org/pdf/2405.05702v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has garnered widespread attention due to its exceptional
performance. Consequently, SLAM systems based on Gaussian Splatting have
emerged, leveraging its capabilities for rapid real-time rendering and
high-fidelity mapping. However, current Gaussian Splatting SLAM systems usually
struggle with large scene representation and lack effective loop closure
adjustments and scene generalization capabilities. To address these issues, we
introduce NGM-SLAM, the first GS-SLAM system that utilizes neural radiance
field submaps for progressive scene expression, effectively integrating the
strengths of neural radiance fields and 3D Gaussian Splatting. We have
developed neural implicit submaps as supervision and achieve high-quality scene
expression and online loop closure adjustments through Gaussian rendering of
fused submaps. Our results on multiple real-world scenes and large-scale scene
datasets demonstrate that our method can achieve accurate gap filling and
high-quality scene expression, supporting both monocular, stereo, and RGB-D
inputs, and achieving state-of-the-art scene reconstruction and tracking
performance.

Comments:
- 9pages, 4 figures

---

## NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via  Generative Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-09 | Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh | cs.CV | [PDF](http://arxiv.org/pdf/2405.05749v2){: .btn .btn-green } |

**Abstract**: Audio-driven talking head generation is advancing from 2D to 3D content.
Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to
synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based
approach typically requires a large number of paired audio-visual data for each
identity, thereby limiting the scalability of the method. Although there have
been attempts to generate audio-driven 3D talking head animations with a single
image, the results are often unsatisfactory due to insufficient information on
obscured regions in the image. In this paper, we mainly focus on addressing the
overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where
facial animations are synthesized primarily in front-facing perspectives. We
propose a novel method, NeRFFaceSpeech, which enables to produce high-quality
3D-aware talking head. Using prior knowledge of generative models combined with
NeRF, our method can craft a 3D-consistent facial feature space corresponding
to a single image. Our spatial synchronization method employs audio-correlated
vertex dynamics of a parametric face model to transform static image features
into dynamic visuals through ray deformation, ensuring realistic 3D facial
motion. Moreover, we introduce LipaintNet that can replenish the lacking
information in the inner-mouth area, which can not be obtained from a given
single image. The network is trained in a self-supervised manner by utilizing
the generative capabilities without additional data. The comprehensive
experiments demonstrate the superiority of our method in generating
audio-driven talking heads from a single image with enhanced 3D consistency
compared to previous approaches. In addition, we introduce a quantitative way
of measuring the robustness of a model against pose changes for the first time,
which has been possible only qualitatively.

Comments:
- 11 pages, 5 figures

---

## GDGS: Gradient Domain Gaussian Splatting for Sparse Representation of  Radiance Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-08 | Yuanhao Gong | cs.CV | [PDF](http://arxiv.org/pdf/2405.05446v1){: .btn .btn-green } |

**Abstract**: The 3D Gaussian splatting methods are getting popular. However, they work
directly on the signal, leading to a dense representation of the signal. Even
with some techniques such as pruning or distillation, the results are still
dense. In this paper, we propose to model the gradient of the original signal.
The gradients are much sparser than the original signal. Therefore, the
gradients use much less Gaussian splats, leading to the more efficient storage
and thus higher computational performance during both training and rendering.
Thanks to the sparsity, during the view synthesis, only a small mount of pixels
are needed, leading to much higher computational performance ($100\sim
1000\times$ faster). And the 2D image can be recovered from the gradients via
solving a Poisson equation with linear computation complexity. Several
experiments are performed to confirm the sparseness of the gradients and the
computation performance of the proposed method. The method can be applied
various applications, such as human body modeling and indoor environment
modeling.

Comments:
- arXiv admin note: text overlap with arXiv:2404.09105

---

## ${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-08 | Ning Wang, Lefei Zhang, Angel X Chang | cs.CV | [PDF](http://arxiv.org/pdf/2405.05010v1){: .btn .btn-green } |

**Abstract**: Neural fields (NeRF) have emerged as a promising approach for representing
continuous 3D scenes. Nevertheless, the lack of semantic encoding in NeRFs
poses a significant challenge for scene decomposition. To address this
challenge, we present a single model, Multi-Modal Decomposition NeRF
(${M^2D}$NeRF), that is capable of both text-based and visual patch-based
edits. Specifically, we use multi-modal feature distillation to integrate
teacher features from pretrained visual and language models into 3D semantic
feature volumes, thereby facilitating consistent 3D editing. To enforce
consistency between the visual and language features in our 3D feature volumes,
we introduce a multi-modal similarity constraint. We also introduce a
patch-based joint contrastive loss that helps to encourage object-regions to
coalesce in the 3D feature space, resulting in more precise boundaries.
Experiments on various real-world scenes show superior performance in 3D scene
decomposition tasks compared to prior NeRF-based methods.



---

## Novel View Synthesis with Neural Radiance Fields for Industrial Robot  Applications

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-07 | Markus Hillemann, Robert Langendörfer, Max Heiken, Max Mehltretter, Andreas Schenk, Martin Weinmann, Stefan Hinz, Christian Heipke, Markus Ulrich | cs.CV | [PDF](http://arxiv.org/pdf/2405.04345v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become a rapidly growing research field
with the potential to revolutionize typical photogrammetric workflows, such as
those used for 3D scene reconstruction. As input, NeRFs require multi-view
images with corresponding camera poses as well as the interior orientation. In
the typical NeRF workflow, the camera poses and the interior orientation are
estimated in advance with Structure from Motion (SfM). But the quality of the
resulting novel views, which depends on different parameters such as the number
and distribution of available images, as well as the accuracy of the related
camera poses and interior orientation, is difficult to predict. In addition,
SfM is a time-consuming pre-processing step, and its quality strongly depends
on the image content. Furthermore, the undefined scaling factor of SfM hinders
subsequent steps in which metric information is required. In this paper, we
evaluate the potential of NeRFs for industrial robot applications. We propose
an alternative to SfM pre-processing: we capture the input images with a
calibrated camera that is attached to the end effector of an industrial robot
and determine accurate camera poses with metric scale based on the robot
kinematics. We then investigate the quality of the novel views by comparing
them to ground truth, and by computing an internal quality measure based on
ensemble methods. For evaluation purposes, we acquire multiple datasets that
pose challenges for reconstruction typical of industrial applications, like
reflective objects, poor texture, and fine structures. We show that the
robot-based pose determination reaches similar accuracy as SfM in non-demanding
cases, while having clear advantages in more challenging scenarios. Finally, we
present first results of applying the ensemble method to estimate the quality
of the synthetic novel view in the absence of a ground truth.

Comments:
- 8 pages, 8 figures, accepted for publication in The International
  Archives of the Photogrammetry, Remote Sensing and Spatial Information
  Sciences (ISPRS Archives) 2024

---

## DistGrid: Scalable Scene Reconstruction with Distributed  Multi-resolution Hash Grid

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-07 | Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou | cs.CV | [PDF](http://arxiv.org/pdf/2405.04416v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled
and indoor scene reconstruction. However, there exist some challenges when
reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network
capacity, while volume-based NeRFs are heavily memory-consuming when the scene
resolution increases. Recent approaches propose to geographically partition the
scene and learn each sub-region using an individual NeRF. Such partitioning
strategies help volume-based NeRF exceed the single GPU memory limit and scale
to larger scenes. However, this approach requires multiple background NeRF to
handle out-of-partition rays, which leads to redundancy of learning. Inspired
by the fact that the background of current partition is the foreground of
adjacent partition, we propose a scalable scene reconstruction method based on
joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is
divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding
Boxes, and a novel segmented volume rendering method is proposed to handle
cross-boundary rays, thereby eliminating the need for background NeRFs. The
experiments demonstrate that our method outperforms existing methods on all
evaluated large-scale scenes, and provides visually plausible scene
reconstruction. The scalability of our method on reconstruction quality is
further evaluated qualitatively and quantitatively.

Comments:
- Originally submitted to Siggraph Asia 2023

---

## Splat-MOVER: Multi-Stage, Open-Vocabulary Robotic Manipulation via  Editable Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-07 | Ola Shorinwa, Johnathan Tucker, Aliyah Smith, Aiden Swann, Timothy Chen, Roya Firoozi, Monroe Kennedy III, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2405.04378v2){: .btn .btn-green } |

**Abstract**: We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic
manipulation, which leverages the editability of Gaussian Splatting (GSplat)
scene representations to enable multi-stage manipulation tasks. Splat-MOVER
consists of: (i) ASK-Splat, a GSplat representation that distills latent codes
for language semantics and grasp affordance into the 3D scene. ASK-Splat
enables geometric, semantic, and affordance understanding of 3D scenes, which
is critical for many robotics tasks; (ii) SEE-Splat, a real-time scene-editing
module using 3D semantic masking and infilling to visualize the motions of
objects that result from robot interactions in the real-world. SEE-Splat
creates a "digital twin" of the evolving environment throughout the
manipulation task; and (iii) Grasp-Splat, a grasp generation module that uses
ASK-Splat and SEE-Splat to propose candidate grasps for open-world objects.
ASK-Splat is trained in real-time from RGB images in a brief scanning phase
prior to operation, while SEE-Splat and Grasp-Splat run in real-time during
operation. We demonstrate the superior performance of Splat-MOVER in hardware
experiments on a Kinova robot compared to two recent baselines in four
single-stage, open-vocabulary manipulation tasks, as well as in four
multi-stage manipulation tasks using the edited scene to reflect scene changes
due to prior manipulation stages, which is not possible with the existing
baselines. Code for this project and a link to the project page will be made
available soon.



---

## A Construct-Optimize Approach to Sparse View Synthesis without Camera  Pose

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-06 | Kaiwen Jiang, Yang Fu, Mukund Varma T, Yash Belhe, Xiaolong Wang, Hao Su, Ravi Ramamoorthi | cs.CV | [PDF](http://arxiv.org/pdf/2405.03659v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis from a sparse set of input images is a challenging
problem of great practical interest, especially when camera poses are absent or
inaccurate. Direct optimization of camera poses and usage of estimated depths
in neural radiance field algorithms usually do not produce good results because
of the coupling between poses and depths, and inaccuracies in monocular depth
estimation. In this paper, we leverage the recent 3D Gaussian splatting method
to develop a novel construct-and-optimize method for sparse view synthesis
without camera poses. Specifically, we construct a solution progressively by
using monocular depth and projecting pixels back into the 3D world. During
construction, we optimize the solution by detecting 2D correspondences between
training views and the corresponding rendered images. We develop a unified
differentiable pipeline for camera registration and adjustment of both camera
poses and depths, followed by back-projection. We also introduce a novel notion
of an expected surface in Gaussian splatting, which is critical to our
optimization. These steps enable a coarse solution, which can then be low-pass
filtered and refined using standard optimization methods. We demonstrate
results on the Tanks and Temples and Static Hikes datasets with as few as three
widely-spaced views, showing significantly better quality than competing
methods, including those with approximate camera pose information. Moreover,
our results improve with more views and outperform previous InstantNGP and
Gaussian Splatting algorithms even when using half the dataset.



---

## Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-06 | Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård | cs.CV | [PDF](http://arxiv.org/pdf/2405.03417v1){: .btn .btn-green } |

**Abstract**: Image-based 3D reconstruction is a challenging task that involves inferring
the 3D shape of an object or scene from a set of input images. Learning-based
methods have gained attention for their ability to directly estimate 3D shapes.
This review paper focuses on state-of-the-art techniques for 3D reconstruction,
including the generation of novel, unseen views. An overview of recent
developments in the Gaussian Splatting method is provided, covering input
types, model structures, output representations, and training strategies.
Unresolved challenges and future directions are also discussed. Given the rapid
progress in this domain and the numerous opportunities for enhancing 3D
reconstruction methods, a comprehensive examination of algorithms appears
essential. Consequently, this study offers a thorough overview of the latest
advancements in Gaussian Splatting.

Comments:
- 24 pages

---

## MVIP-NeRF: Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-05 | Honghua Chen, Chen Change Loy, Xingang Pan | cs.CV | [PDF](http://arxiv.org/pdf/2405.02859v1){: .btn .btn-green } |

**Abstract**: Despite the emergence of successful NeRF inpainting methods built upon
explicit RGB and depth 2D inpainting supervisions, these methods are inherently
constrained by the capabilities of their underlying 2D inpainters. This is due
to two key reasons: (i) independently inpainting constituent images results in
view-inconsistent imagery, and (ii) 2D inpainters struggle to ensure
high-quality geometry completion and alignment with inpainted RGB images.
  To overcome these limitations, we propose a novel approach called MVIP-NeRF
that harnesses the potential of diffusion priors for NeRF inpainting,
addressing both appearance and geometry aspects. MVIP-NeRF performs joint
inpainting across multiple views to reach a consistent solution, which is
achieved via an iterative optimization process based on Score Distillation
Sampling (SDS). Apart from recovering the rendered RGB images, we also extract
normal maps as a geometric representation and define a normal SDS loss that
motivates accurate geometry inpainting and alignment with the appearance.
Additionally, we formulate a multi-view SDS score function to distill
generative priors simultaneously from different view images, ensuring
consistent visual completion when dealing with large view variations. Our
experimental results show better appearance and geometry recovery than previous
NeRF inpainting methods.

Comments:
- 14 pages, 10 figures, conference

---

## Blending Distributed NeRFs with Tri-stage Robust Pose Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-05 | Baijun Ye, Caiyun Liu, Xiaoyu Ye, Yuantao Chen, Yuhai Wang, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2405.02880v1){: .btn .btn-green } |

**Abstract**: Due to the limited model capacity, leveraging distributed Neural Radiance
Fields (NeRFs) for modeling extensive urban environments has become a
necessity. However, current distributed NeRF registration approaches encounter
aliasing artifacts, arising from discrepancies in rendering resolutions and
suboptimal pose precision. These factors collectively deteriorate the fidelity
of pose estimation within NeRF frameworks, resulting in occlusion artifacts
during the NeRF blending stage. In this paper, we present a distributed NeRF
system with tri-stage pose optimization. In the first stage, precise poses of
images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine
strategy. In the second stage, we incorporate the inverting Mip-NeRF 360,
coupled with the truncated dynamic low-pass filter, to enable the achievement
of robust and precise poses, termed Frame2Model optimization. On top of this,
we obtain a coarse transformation between NeRFs in different coordinate
systems. In the third stage, we fine-tune the transformation between NeRFs by
Model2Model pose optimization. After obtaining precise transformation
parameters, we proceed to implement NeRF blending, showcasing superior
performance metrics in both real-world and simulation scenarios. Codes and data
will be publicly available at https://github.com/boilcy/Distributed-NeRF.



---

## TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for  Dynamic UAV-based Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-04 | Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon | cs.CV | [PDF](http://arxiv.org/pdf/2405.02762v1){: .btn .btn-green } |

**Abstract**: In this paper, we present a new approach to bridge the domain gap between
synthetic and real-world data for un- manned aerial vehicle (UAV)-based
perception. Our formu- lation is designed for dynamic scenes, consisting of
moving objects or human actions, where the goal is to recognize the pose or
actions. We propose an extension of K-Planes Neural Radiance Field (NeRF),
wherein our algorithm stores a set of tiered feature vectors. The tiered
feature vectors are generated to effectively model conceptual information about
a scene as well as an image decoder that transforms output feature maps into
RGB images. Our technique leverages the information amongst both static and
dynamic objects within a scene and is able to capture salient scene attributes
of high altitude videos. We evaluate its performance on challenging datasets,
including Okutama Action and UG2, and observe considerable improvement in
accuracy over state of the art aerial perception algorithms.

Comments:
- 8 pages, submitted to IROS2024

---

## ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface  Uncertainty

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-04 | Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2405.02568v1){: .btn .btn-green } |

**Abstract**: Active learning in 3D scene reconstruction has been widely studied, as
selecting informative training views is critical for the reconstruction.
Recently, Neural Radiance Fields (NeRF) variants have shown performance
increases in active 3D reconstruction using image rendering or geometric
uncertainty. However, the simultaneous consideration of both uncertainties in
selecting informative views remains unexplored, while utilizing different types
of uncertainty can reduce the bias that arises in the early training stage with
sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate
views considering both uncertainties. ActiveNeuS provides a way to accumulate
image rendering uncertainty while avoiding the bias that the estimated
densities can introduce. ActiveNeuS computes the neural implicit surface
uncertainty, providing the color uncertainty along with the surface
information. It efficiently handles the bias by using the surface information
and a grid, enabling the fast selection of diverse viewpoints. Our method
outperforms previous works on popular datasets, Blender and DTU, showing that
the views selected by ActiveNeuS significantly improve performance.



---

## WateRF: Robust Watermarks in Radiance Fields for Protection of  Copyrights

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim | cs.CV | [PDF](http://arxiv.org/pdf/2405.02066v1){: .btn .btn-green } |

**Abstract**: The advances in the Neural Radiance Fields (NeRF) research offer extensive
applications in diverse domains, but protecting their copyrights has not yet
been researched in depth. Recently, NeRF watermarking has been considered one
of the pivotal solutions for safely deploying NeRF-based 3D representations.
However, existing methods are designed to apply only to implicit or explicit
NeRF representations. In this work, we introduce an innovative watermarking
method that can be employed in both representations of NeRF. This is achieved
by fine-tuning NeRF to embed binary messages in the rendering process. In
detail, we propose utilizing the discrete wavelet transform in the NeRF space
for watermarking. Furthermore, we adopt a deferred back-propagation technique
and introduce a combination with the patch-wise loss to improve rendering
quality and bit accuracy with minimum trade-offs. We evaluate our method in
three different aspects: capacity, invisibility, and robustness of the embedded
watermarks in the 2D-rendered images. Our method achieves state-of-the-art
performance with faster training speed over the compared state-of-the-art
methods.



---

## HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft  HoloLens 2

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Miriam Jäger, Theodor Kapler, Michael Feßenbecker, Felix Birkelbach, Markus Hillemann, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2405.02005v1){: .btn .btn-green } |

**Abstract**: In the fields of photogrammetry, computer vision and computer graphics, the
task of neural 3D scene reconstruction has led to the exploration of various
techniques. Among these, 3D Gaussian Splatting stands out for its explicit
representation of scenes using 3D Gaussians, making it appealing for tasks like
3D point cloud extraction and surface reconstruction. Motivated by its
potential, we address the domain of 3D scene reconstruction, aiming to leverage
the capabilities of the Microsoft HoloLens 2 for instant 3D Gaussian Splatting.
We present HoloGS, a novel workflow utilizing HoloLens sensor data, which
bypasses the need for pre-processing steps like Structure from Motion by
instantly accessing the required input data i.e. the images, camera poses and
the point cloud from depth sensing. We provide comprehensive investigations,
including the training process and the rendering quality, assessed through the
Peak Signal-to-Noise Ratio, and the geometric 3D accuracy of the densified
point cloud from Gaussian centers, measured by Chamfer Distance. We evaluate
our approach on two self-captured scenes: An outdoor scene of a cultural
heritage statue and an indoor scene of a fine-structured plant. Our results
show that the HoloLens data, including RGB images, corresponding camera poses,
and depth sensing based point clouds to initialize the Gaussians, are suitable
as input for 3D Gaussian Splatting.

Comments:
- 8 pages, 9 figures, 2 tables. Will be published in the ISPRS The
  International Archives of Photogrammetry, Remote Sensing and Spatial
  Information Sciences

---

## Learning Robot Soccer from Egocentric Vision with Deep Reinforcement  Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil Sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, Nicolas Heess | cs.RO | [PDF](http://arxiv.org/pdf/2405.02425v1){: .btn .btn-green } |

**Abstract**: We apply multi-agent deep reinforcement learning (RL) to train end-to-end
robot soccer policies with fully onboard computation and sensing via egocentric
RGB vision. This setting reflects many challenges of real-world robotics,
including active perception, agile full-body control, and long-horizon planning
in a dynamic, partially-observable, multi-agent domain. We rely on large-scale,
simulation-based data generation to obtain complex behaviors from egocentric
vision which can be successfully transferred to physical robots using low-cost
sensors. To achieve adequate visual realism, our simulation combines rigid-body
physics with learned, realistic rendering via multiple Neural Radiance Fields
(NeRFs). We combine teacher-based multi-agent RL and cross-experiment data
reuse to enable the discovery of sophisticated soccer strategies. We analyze
active-perception behaviors including object tracking and ball seeking that
emerge when simply optimizing perception-agnostic soccer play. The agents
display equivalent levels of performance and agility as policies with access to
privileged, ground-truth state. To our knowledge, this paper constitutes a
first demonstration of end-to-end training for multi-agent robot soccer,
mapping raw pixel observations to joint-level actions, that can be deployed in
the real world. Videos of the game-play and analyses can be seen on our website
https://sites.google.com/view/vision-soccer .



---

## Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic  Solids

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-03 | Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2405.02386v1){: .btn .btn-green } |

**Abstract**: Despite significant advancements in Neural Radiance Fields (NeRFs), the
renderings may still suffer from aliasing and blurring artifacts, since it
remains a fundamental challenge to effectively and efficiently characterize
anisotropic areas induced by the cone-casting procedure. This paper introduces
a Ripmap-Encoded Platonic Solid representation to precisely and efficiently
featurize 3D anisotropic areas, achieving high-fidelity anti-aliasing
renderings. Central to our approach are two key components: Platonic Solid
Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D
space onto the unparalleled faces of a certain Platonic solid, such that the
anisotropic 3D areas can be projected onto planes with distinguishable
characterization. Meanwhile, each face of the Platonic solid is encoded by the
Ripmap encoding, which is constructed by anisotropically pre-filtering a
learnable feature grid, to enable featurzing the projected anisotropic areas
both precisely and efficiently by the anisotropic area-sampling. Extensive
experiments on both well-established synthetic datasets and a newly captured
real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art
rendering quality, particularly excelling in the fine details of repetitive
structures and textures, while maintaining relatively swift training times.

Comments:
- SIGGRAPH 2024, Project page: https://junchenliu77.github.io/Rip-NeRF
  , Code: https://github.com/JunchenLiu77/Rip-NeRF

---

## NeRF in Robotics: A Survey

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-02 | Guangming Wang, Lei Pan, Songyou Peng, Shaohui Liu, Chenfeng Xu, Yanzi Miao, Wei Zhan, Masayoshi Tomizuka, Marc Pollefeys, Hesheng Wang | cs.RO | [PDF](http://arxiv.org/pdf/2405.01333v1){: .btn .btn-green } |

**Abstract**: Meticulous 3D environment representations have been a longstanding goal in
computer vision and robotics fields. The recent emergence of neural implicit
representations has introduced radical innovation to this field as implicit
representations enable numerous capabilities. Among these, the Neural Radiance
Field (NeRF) has sparked a trend because of the huge representational
advantages, such as simplified mathematical models, compact environment
storage, and continuous scene representations. Apart from computer vision, NeRF
has also shown tremendous potential in the field of robotics. Thus, we create
this survey to provide a comprehensive understanding of NeRF in the field of
robotics. By exploring the advantages and limitations of NeRF, as well as its
current applications and future potential, we hope to shed light on this
promising area of research. Our survey is divided into two main sections:
\textit{The Application of NeRF in Robotics} and \textit{The Advance of NeRF in
Robotics}, from the perspective of how NeRF enters the field of robotics. In
the first section, we introduce and analyze some works that have been or could
be used in the field of robotics from the perception and interaction
perspectives. In the second section, we show some works related to improving
NeRF's own properties, which are essential for deploying NeRF in the field of
robotics. In the discussion section of the review, we summarize the existing
challenges and provide some valuable future research directions for reference.

Comments:
- 21 pages, 19 figures

---

## NeRF-Guided Unsupervised Learning of RGB-D Registration

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu | cs.CV | [PDF](http://arxiv.org/pdf/2405.00507v1){: .btn .btn-green } |

**Abstract**: This paper focuses on training a robust RGB-D registration model without
ground-truth pose supervision. Existing methods usually adopt a pairwise
training strategy based on differentiable rendering, which enforces the
photometric and the geometric consistency between the two registered frames as
supervision. However, this frame-to-frame framework suffers from poor
multi-view consistency due to factors such as lighting changes, geometry
occlusion and reflective materials. In this paper, we present NeRF-UR, a novel
frame-to-model optimization framework for unsupervised RGB-D registration.
Instead of frame-to-frame consistency, we leverage the neural radiance field
(NeRF) as a global model of the scene and use the consistency between the input
and the NeRF-rerendered frames for pose optimization. This design can
significantly improve the robustness in scenarios with poor multi-view
consistency and provides better learning signal for the registration model.
Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset,
Sim-RGBD, through a photo-realistic simulator to warm up the registration
model. By first training the registration model on Sim-RGBD and later
unsupervisedly fine-tuning on real data, our framework enables distilling the
capability of feature extraction and registration from simulation to reality.
Our method outperforms the state-of-the-art counterparts on two popular indoor
RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper
reproduction.



---

## Depth Priors in Removal Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Zhihao Guo, Peng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2405.00630v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown impressive results in 3D
reconstruction and generating novel views. A key challenge within NeRF is the
editing of reconstructed scenes, such as object removal, which requires
maintaining consistency across multiple views and ensuring high-quality
synthesised perspectives. Previous studies have incorporated depth priors,
typically from LiDAR or sparse depth measurements provided by COLMAP, to
improve the performance of object removal in NeRF. However, these methods are
either costly or time-consuming. In this paper, we propose a novel approach
that integrates monocular depth estimates with NeRF-based object removal models
to significantly reduce time consumption and enhance the robustness and quality
of scene generation and object removal. We conducted a thorough evaluation of
COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy
in depth map generation. Our findings suggest that COLMAP can serve as an
effective alternative to a ground truth depth map where such information is
missing or costly to obtain. Additionally, we integrated various monocular
depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess
their capacity to improve object removal performance. Our experimental results
highlight the potential of monocular depth estimation to substantially improve
NeRF applications.

Comments:
- 15 pages

---

## LidaRF: Delving into Lidar for Neural Radiance Field on Street Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Shanlin Sun, Bingbing Zhuang, Ziyu Jiang, Buyu Liu, Xiaohui Xie, Manmohan Chandraker | cs.CV | [PDF](http://arxiv.org/pdf/2405.00900v2){: .btn .btn-green } |

**Abstract**: Photorealistic simulation plays a crucial role in applications such as
autonomous driving, where advances in neural radiance fields (NeRFs) may allow
better scalability through the automatic creation of digital 3D assets.
However, reconstruction quality suffers on street scenes due to largely
collinear camera motions and sparser samplings at higher speeds. On the other
hand, the application often demands rendering from camera views that deviate
from the inputs to accurately simulate behaviors like lane changes. In this
paper, we propose several insights that allow a better utilization of Lidar
data to improve NeRF quality on street scenes. First, our framework learns a
geometric scene representation from Lidar, which is fused with the implicit
grid-based representation for radiance decoding, thereby supplying stronger
geometric information offered by explicit point cloud. Second, we put forth a
robust occlusion-aware depth supervision scheme, which allows utilizing
densified Lidar points by accumulation. Third, we generate augmented training
views from Lidar points for further improvement. Our insights translate to
largely improved novel view synthesis under real driving scenes.

Comments:
- CVPR2024 Highlights

---

## Spectrally Pruned Gaussian Fields with Neural Compensation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-05-01 | Runyi Yang, Zhenxin Zhu, Zhou Jiang, Baijun Ye, Xiaoxue Chen, Yifei Zhang, Yuantao Chen, Jian Zhao, Hao Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2405.00676v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered
attention for its fast rendering speed and high rendering quality. However,
this comes with high memory consumption, e.g., a well-trained Gaussian field
may utilize three million Gaussian primitives and over 700 MB of memory. We
credit this high memory footprint to the lack of consideration for the
relationship between primitives. In this paper, we propose a memory-efficient
Gaussian field named SUNDAE with spectral pruning and neural compensation. On
one hand, we construct a graph on the set of Gaussian primitives to model their
relationship and design a spectral down-sampling module to prune out primitives
while preserving desired signals. On the other hand, to compensate for the
quality loss of pruning Gaussians, we exploit a lightweight neural network head
to mix splatted features, which effectively compensates for quality losses
while capturing the relationship between primitives in its weights. We
demonstrate the performance of SUNDAE with extensive results. For example,
SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla
Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB
memory, on the Mip-NeRF360 dataset. Codes are publicly available at
https://runyiyang.github.io/projects/SUNDAE/.

Comments:
- Code: https://github.com/RunyiYang/SUNDAE Project page:
  https://runyiyang.github.io/projects/SUNDAE/
