---
layout: default
title: 2022
parent: Papers
nav_order: 2
---
<!---metadata--->

## NeRF-Gaze: A Head-Eye Redirection Parametric Model for Gaze Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-30 | Pengwei Yin, Jiawu Dai, Jingjing Wang, Di Xie, Shiliang Pu | cs.CV | [PDF](http://arxiv.org/pdf/2212.14710v1){: .btn .btn-green } |

**Abstract**: Gaze estimation is the fundamental basis for many visual tasks. Yet, the high
cost of acquiring gaze datasets with 3D annotations hinders the optimization
and application of gaze estimation models. In this work, we propose a novel
Head-Eye redirection parametric model based on Neural Radiance Field, which
allows dense gaze data generation with view consistency and accurate gaze
direction. Moreover, our head-eye redirection parametric model can decouple the
face and eyes for separate neural rendering, so it can achieve the purpose of
separately controlling the attributes of the face, identity, illumination, and
eye gaze direction. Thus diverse 3D-aware gaze datasets could be obtained by
manipulating the latent code belonging to different face attributions in an
unsupervised manner. Extensive experiments on several benchmarks demonstrate
the effectiveness of our method in domain generalization and domain adaptation
for gaze estimation tasks.

Comments:
- 10 pages, 8 figures, submitted to CVPR 2023

---

## Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and  Text-to-Image Diffusion Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-28 | Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Shenghua Gao | cs.CV | [PDF](http://arxiv.org/pdf/2212.14704v2){: .btn .btn-green } |

**Abstract**: Recent CLIP-guided 3D optimization methods, such as DreamFields and
PureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D
synthesis. However, due to scratch training and random initialization without
prior knowledge, these methods often fail to generate accurate and faithful 3D
structures that conform to the input text. In this paper, we make the first
attempt to introduce explicit 3D shape priors into the CLIP-guided 3D
optimization process. Specifically, we first generate a high-quality 3D shape
from the input text in the text-to-shape stage as a 3D shape prior. We then use
it as the initialization of a neural radiance field and optimize it with the
full prompt. To address the challenging text-to-shape generation task, we
present a simple yet effective approach that directly bridges the text and
image modalities with a powerful text-to-image diffusion model. To narrow the
style domain gap between the images synthesized by the text-to-image diffusion
model and shape renderings used to train the image-to-shape generator, we
further propose to jointly optimize a learnable text prompt and fine-tune the
text-to-image diffusion model for rendering-style image generation. Our method,
Dream3D, is capable of generating imaginative 3D content with superior visual
quality and shape accuracy compared to state-of-the-art methods.

Comments:
- Accepted by CVPR 2023. Project page:
  https://bluestyle97.github.io/dream3d/

---

## MonoNeRF: Learning a Generalizable Dynamic Radiance Field from Monocular  Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-26 | Fengrui Tian, Shaoyi Du, Yueqi Duan | cs.CV | [PDF](http://arxiv.org/pdf/2212.13056v3){: .btn .btn-green } |

**Abstract**: In this paper, we target at the problem of learning a generalizable dynamic
radiance field from monocular videos. Different from most existing NeRF methods
that are based on multiple views, monocular videos only contain one view at
each timestamp, thereby suffering from ambiguity along the view direction in
estimating point features and scene flows. Previous studies such as DynNeRF
disambiguate point features by positional encoding, which is not transferable
and severely limits the generalization ability. As a result, these methods have
to train one independent model for each scene and suffer from heavy
computational costs when applying to increasing monocular videos in real-world
applications. To address this, We propose MonoNeRF to simultaneously learn
point features and scene flows with point trajectory and feature correspondence
constraints across frames. More specifically, we learn an implicit velocity
field to estimate point trajectory from temporal features with Neural ODE,
which is followed by a flow-based feature aggregation module to obtain spatial
features along the point trajectory. We jointly optimize temporal and spatial
features in an end-to-end manner. Experiments show that our MonoNeRF is able to
learn from multiple scenes and support new applications such as scene editing,
unseen frame synthesis, and fast novel scene adaptation. Codes are available at
https://github.com/tianfr/MonoNeRF.

Comments:
- Accepted by ICCV 2023

---

## PaletteNeRF: Palette-based Color Editing for NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-25 | Qiling Wu, Jianchao Tan, Kun Xu | cs.CV | [PDF](http://arxiv.org/pdf/2212.12871v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) is a powerful tool to faithfully generate novel
views for scenes with only sparse captured images. Despite its strong
capability for representing 3D scenes and their appearance, its editing ability
is very limited. In this paper, we propose a simple but effective extension of
vanilla NeRF, named PaletteNeRF, to enable efficient color editing on
NeRF-represented scenes. Motivated by recent palette-based image decomposition
works, we approximate each pixel color as a sum of palette colors modulated by
additive weights. Instead of predicting pixel colors as in vanilla NeRFs, our
method predicts additive weights. The underlying NeRF backbone could also be
replaced with more recent NeRF models such as KiloNeRF to achieve real-time
editing. Experimental results demonstrate that our method achieves efficient,
view-consistent, and artifact-free color editing on a wide range of
NeRF-represented scenes.

Comments:
- 12 pages, 10 figures

---

## Removing Objects From Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-22 | Silvan Weder, Guillermo Garcia-Hernando, Aron Monszpart, Marc Pollefeys, Gabriel Brostow, Michael Firman, Sara Vicente | cs.CV | [PDF](http://arxiv.org/pdf/2212.11966v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) are emerging as a ubiquitous scene
representation that allows for novel view synthesis. Increasingly, NeRFs will
be shareable with other people. Before sharing a NeRF, though, it might be
desirable to remove personal information or unsightly objects. Such removal is
not easily achieved with the current NeRF editing frameworks. We propose a
framework to remove objects from a NeRF representation created from an RGB-D
sequence. Our NeRF inpainting method leverages recent work in 2D image
inpainting and is guided by a user-provided mask. Our algorithm is underpinned
by a confidence based view selection procedure. It chooses which of the
individual 2D inpainted images to use in the creation of the NeRF, so that the
resulting inpainted NeRF is 3D consistent. We show that our method for NeRF
editing is effective for synthesizing plausible inpaintings in a multi-view
coherent manner. We validate our approach using a new and still-challenging
dataset for the task of NeRF inpainting.

---

## Incremental Neural Implicit Representation with Uncertainty-Filtered  Knowledge Distillation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-21 | Mengqi Guo, Chen Li, Hanlin Chen, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2212.10950v2){: .btn .btn-green } |

**Abstract**: Recent neural implicit representations (NIRs) have achieved great success in
the tasks of 3D reconstruction and novel view synthesis. However, they suffer
from the catastrophic forgetting problem when continuously learning from
streaming data without revisiting the previously seen data. This limitation
prohibits the application of existing NIRs to scenarios where images come in
sequentially. In view of this, we explore the task of incremental learning for
NIRs in this work. We design a student-teacher framework to mitigate the
catastrophic forgetting problem. Specifically, we iterate the process of using
the student as the teacher at the end of each time step and let the teacher
guide the training of the student in the next step. As a result, the student
network is able to learn new information from the streaming data and retain old
knowledge from the teacher network simultaneously. Although intuitive, naively
applying the student-teacher pipeline does not work well in our task. Not all
information from the teacher network is helpful since it is only trained with
the old data. To alleviate this problem, we further introduce a random inquirer
and an uncertainty-based filter to filter useful information. Our proposed
method is general and thus can be adapted to different implicit representations
such as neural radiance field (NeRF) and neural SDF. Extensive experimental
results for both 3D reconstruction and novel view synthesis demonstrate the
effectiveness of our approach compared to different baselines.

---

## PaletteNeRF: Palette-based Appearance Editing of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-21 | Zhengfei Kuang, Fujun Luan, Sai Bi, Zhixin Shu, Gordon Wetzstein, Kalyan Sunkavalli | cs.CV | [PDF](http://arxiv.org/pdf/2212.10699v2){: .btn .btn-green } |

**Abstract**: Recent advances in neural radiance fields have enabled the high-fidelity 3D
reconstruction of complex scenes for novel view synthesis. However, it remains
underexplored how the appearance of such representations can be efficiently
edited while maintaining photorealism.
  In this work, we present PaletteNeRF, a novel method for photorealistic
appearance editing of neural radiance fields (NeRF) based on 3D color
decomposition. Our method decomposes the appearance of each 3D point into a
linear combination of palette-based bases (i.e., 3D segmentations defined by a
group of NeRF-type functions) that are shared across the scene. While our
palette-based bases are view-independent, we also predict a view-dependent
function to capture the color residual (e.g., specular shading). During
training, we jointly optimize the basis functions and the color palettes, and
we also introduce novel regularizers to encourage the spatial coherence of the
decomposition.
  Our method allows users to efficiently edit the appearance of the 3D scene by
modifying the color palettes. We also extend our framework with compressed
semantic features for semantic-aware appearance editing. We demonstrate that
our technique is superior to baseline methods both quantitatively and
qualitatively for appearance editing of complex real-world scenes.

---

## Correspondence Distillation from NeRF-based GAN

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-19 | Yushi Lan, Chen Change Loy, Bo Dai | cs.CV | [PDF](http://arxiv.org/pdf/2212.09735v2){: .btn .btn-green } |

**Abstract**: The neural radiance field (NeRF) has shown promising results in preserving
the fine details of objects and scenes. However, unlike mesh-based
representations, it remains an open problem to build dense correspondences
across different NeRFs of the same category, which is essential in many
downstream tasks. The main difficulties of this problem lie in the implicit
nature of NeRF and the lack of ground-truth correspondence annotations. In this
paper, we show it is possible to bypass these challenges by leveraging the rich
semantics and structural priors encapsulated in a pre-trained NeRF-based GAN.
Specifically, we exploit such priors from three aspects, namely 1) a dual
deformation field that takes latent codes as global structural indicators, 2) a
learning objective that regards generator features as geometric-aware local
descriptors, and 3) a source of infinite object-specific NeRF samples. Our
experiments demonstrate that such priors lead to 3D dense correspondence that
is accurate, smooth, and robust. We also show that established dense
correspondence across NeRFs can effectively enable many NeRF-based downstream
applications such as texture transfer.

Comments:
- Project page: https://nirvanalan.github.io/projects/DDF/index.html

---

## StyleTRF: Stylizing Tensorial Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-19 | Rahul Goel, Sirikonda Dhawal, Saurabh Saini, P. J. Narayanan | cs.CV | [PDF](http://arxiv.org/pdf/2212.09330v1){: .btn .btn-green } |

**Abstract**: Stylized view generation of scenes captured casually using a camera has
received much attention recently. The geometry and appearance of the scene are
typically captured as neural point sets or neural radiance fields in the
previous work. An image stylization method is used to stylize the captured
appearance by training its network jointly or iteratively with the structure
capture network. The state-of-the-art SNeRF method trains the NeRF and
stylization network in an alternating manner. These methods have high training
time and require joint optimization. In this work, we present StyleTRF, a
compact, quick-to-optimize strategy for stylized view generation using TensoRF.
The appearance part is fine-tuned using sparse stylized priors of a few views
rendered using the TensoRF representation for a few iterations. Our method thus
effectively decouples style-adaption from view capture and is much faster than
the previous methods. We show state-of-the-art results on several scenes used
for this purpose.

Comments:
- Accepted at ICVGIP-2022

---

## SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input  Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-18 | Abdullah Hamdi, Bernard Ghanem, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2212.09100v3){: .btn .btn-green } |

**Abstract**: Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel
view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels
for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage
machine learning and adoption of SRFs as a 3D representation, we present SPARF,
a large-scale ShapeNet-based synthetic dataset for novel view synthesis
consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at
high resolution (400 X 400 pixels). The dataset is orders of magnitude larger
than existing synthetic datasets for novel view synthesis and includes more
than one million 3D-optimized radiance fields with multiple voxel resolutions.
Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate
sparse voxel radiance fields from only few views. This is done by using the
densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs
partial SRFs from few/one images and a specialized SRF loss to learn to
generate high-quality sparse voxel radiance fields that can be rendered from
novel views. Our approach achieves state-of-the-art results in the task of
unconstrained novel view synthesis based on few views on ShapeNet as compared
to recent baselines. The SPARF dataset is made public with the code and models
on the project website https://abdullahamdi.com/sparf/ .

Comments:
- published at ICCV 2023 workshop proceedings

---

## Masked Wavelet Representation for Compact Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-18 | Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan Lee, Jong Hwan Ko, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2212.09069v2){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) have demonstrated the potential of
coordinate-based neural representation (neural fields or implicit neural
representation) in neural rendering. However, using a multi-layer perceptron
(MLP) to represent a 3D scene or object requires enormous computational
resources and time. There have been recent studies on how to reduce these
computational inefficiencies by using additional data structures, such as grids
or trees. Despite the promising performance, the explicit data structure
necessitates a substantial amount of memory. In this work, we present a method
to reduce the size without compromising the advantages of having additional
data structures. In detail, we propose using the wavelet transform on
grid-based neural fields. Grid-based neural fields are for fast convergence,
and the wavelet transform, whose efficiency has been demonstrated in
high-performance standard codecs, is to improve the parameter efficiency of
grids. Furthermore, in order to achieve a higher sparsity of grid coefficients
while maintaining reconstruction quality, we present a novel trainable masking
approach. Experimental results demonstrate that non-spatial grid coefficients,
such as wavelet coefficients, are capable of attaining a higher level of
sparsity than spatial grid coefficients, resulting in a more compact
representation. With our proposed mask and compression pipeline, we achieved
state-of-the-art performance within a memory budget of 2 MB. Our code is
available at https://github.com/daniel03c1/masked_wavelet_nerf.

Comments:
- Accepted to CVPR 2023

---

## MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-16 | Jaeyoung Chung, Kanggeon Lee, Sungyong Baik, Kyoung Mu Lee | cs.CV | [PDF](http://arxiv.org/pdf/2212.08328v2){: .btn .btn-green } |

**Abstract**: Hinged on the representation power of neural networks, neural radiance fields
(NeRF) have recently emerged as one of the promising and widely applicable
methods for 3D object and scene representation. However, NeRF faces challenges
in practical applications, such as large-scale scenes and edge devices with a
limited amount of memory, where data needs to be processed sequentially. Under
such incremental learning scenarios, neural networks are known to suffer
catastrophic forgetting: easily forgetting previously seen data after training
with new data. We observe that previous incremental learning algorithms are
limited by either low performance or memory scalability issues. As such, we
develop a Memory-Efficient Incremental Learning algorithm for NeRF (MEIL-NeRF).
MEIL-NeRF takes inspiration from NeRF itself in that a neural network can serve
as a memory that provides the pixel RGB values, given rays as queries. Upon the
motivation, our framework learns which rays to query NeRF to extract previous
pixel values. The extracted pixel values are then used to train NeRF in a
self-distillation manner to prevent catastrophic forgetting. As a result,
MEIL-NeRF demonstrates constant memory consumption and competitive performance.

Comments:
- 18 pages. For the project page, see
  https://robot0321.github.io/meil-nerf/index.html

---

## SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-15 | Sicheng Li, Hao Li, Yue Wang, Yiyi Liao, Lu Yu | cs.CV | [PDF](http://arxiv.org/pdf/2212.08476v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated superior novel view synthesis
performance but are slow at rendering. To speed up the volume rendering
process, many acceleration methods have been proposed at the cost of large
memory consumption. To push the frontier of the efficiency-memory trade-off, we
explore a new perspective to accelerate NeRF rendering, leveraging a key fact
that the viewpoint change is usually smooth and continuous in interactive
viewpoint control. This allows us to leverage the information of preceding
viewpoints to reduce the number of rendered pixels as well as the number of
sampled points along the ray of the remaining pixels. In our pipeline, a
low-resolution feature map is rendered first by volume rendering, then a
lightweight 2D neural renderer is applied to generate the output image at
target resolution leveraging the features of preceding and current frames. We
show that the proposed method can achieve competitive rendering quality while
reducing the rendering time with little memory overhead, enabling 30FPS at
1080P image resolution with a low memory footprint.

---

## Real-Time Neural Light Field on Mobile Devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-15 | Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, Denys Makoviichuk, Sergey Tulyakov, Jian Ren | cs.CV | [PDF](http://arxiv.org/pdf/2212.08057v2){: .btn .btn-green } |

**Abstract**: Recent efforts in Neural Rendering Fields (NeRF) have shown impressive
results on novel view synthesis by utilizing implicit neural representation to
represent 3D scenes. Due to the process of volumetric rendering, the inference
speed for NeRF is extremely slow, limiting the application scenarios of
utilizing NeRF on resource-constrained hardware, such as mobile devices. Many
works have been conducted to reduce the latency of running NeRF models.
However, most of them still require high-end GPU for acceleration or extra
storage memory, which is all unavailable on mobile devices. Another emerging
direction utilizes the neural light field (NeLF) for speedup, as only one
forward pass is performed on a ray to predict the pixel color. Nevertheless, to
reach a similar rendering quality as NeRF, the network in NeLF is designed with
intensive computation, which is not mobile-friendly. In this work, we propose
an efficient network that runs in real-time on mobile devices for neural
rendering. We follow the setting of NeLF to train our network. Unlike existing
works, we introduce a novel network architecture that runs efficiently on
mobile devices with low latency and small size, i.e., saving $15\times \sim
24\times$ storage compared with MobileNeRF. Our model achieves high-resolution
generation while maintaining real-time inference for both synthetic and
real-world scenes on mobile devices, e.g., $18.04$ms (iPhone 13) for rendering
one $1008\times756$ image of real 3D scenes. Additionally, we achieve similar
image quality as NeRF and better quality than MobileNeRF (PSNR $26.15$ vs.
$25.91$ on the real-world forward-facing dataset).

Comments:
- CVPR 2023. Project page: https://snap-research.github.io/MobileR2L/
  Code: https://github.com/snap-research/MobileR2L/

---

## VolRecon: Volume Rendering of Signed Ray Distance Functions for  Generalizable Multi-View Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-15 | Yufan Ren, Fangjinhua Wang, Tong Zhang, Marc Pollefeys, Sabine Süsstrunk | cs.CV | [PDF](http://arxiv.org/pdf/2212.08067v2){: .btn .btn-green } |

**Abstract**: The success of the Neural Radiance Fields (NeRF) in novel view synthesis has
inspired researchers to propose neural implicit scene reconstruction. However,
most existing neural implicit reconstruction methods optimize per-scene
parameters and therefore lack generalizability to new scenes. We introduce
VolRecon, a novel generalizable implicit reconstruction method with Signed Ray
Distance Function (SRDF). To reconstruct the scene with fine details and little
noise, VolRecon combines projection features aggregated from multi-view
features, and volume features interpolated from a coarse global feature volume.
Using a ray transformer, we compute SRDF values of sampled points on a ray and
then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by
about 30% in sparse view reconstruction and achieves comparable accuracy as
MVSNet in full view reconstruction. Furthermore, our approach exhibits good
generalization performance on the large-scale ETH3D benchmark.

---

## NeRF-Art: Text-Driven Neural Radiance Fields Stylization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-15 | Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao | cs.CV | [PDF](http://arxiv.org/pdf/2212.08070v1){: .btn .btn-green } |

**Abstract**: As a powerful representation of 3D scenes, the neural radiance field (NeRF)
enables high-quality novel view synthesis from multi-view images. Stylizing
NeRF, however, remains challenging, especially on simulating a text-guided
style with both the appearance and the geometry altered simultaneously. In this
paper, we present NeRF-Art, a text-guided NeRF stylization approach that
manipulates the style of a pre-trained NeRF model with a simple text prompt.
Unlike previous approaches that either lack sufficient geometry deformations
and texture details or require meshes to guide the stylization, our method can
shift a 3D scene to the target style characterized by desired geometry and
appearance variations without any mesh guidance. This is achieved by
introducing a novel global-local contrastive learning strategy, combined with
the directional constraint to simultaneously control both the trajectory and
the strength of the target style. Moreover, we adopt a weight regularization
method to effectively suppress cloudy artifacts and geometry noises which arise
easily when the density field is transformed during geometry stylization.
Through extensive experiments on various styles, we demonstrate that our method
is effective and robust regarding both single-view stylization quality and
cross-view consistency. The code and more results can be found in our project
page: https://cassiepython.github.io/nerfart/.

Comments:
- Project page: https://cassiepython.github.io/nerfart/

---

## NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-14 | Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, Victor Adrian Prisacariu | cs.CV | [PDF](http://arxiv.org/pdf/2212.07388v3){: .btn .btn-green } |

**Abstract**: Training a Neural Radiance Field (NeRF) without pre-computed camera poses is
challenging. Recent advances in this direction demonstrate the possibility of
jointly optimising a NeRF and camera poses in forward-facing scenes. However,
these methods still face difficulties during dramatic camera movement. We
tackle this challenging problem by incorporating undistorted monocular depth
priors. These priors are generated by correcting scale and shift parameters
during training, with which we are then able to constrain the relative poses
between consecutive frames. This constraint is achieved using our proposed
novel loss functions. Experiments on real-world indoor and outdoor scenes show
that our method can handle challenging camera trajectories and outperforms
existing methods in terms of novel view rendering quality and pose estimation
accuracy. Our project page is https://nope-nerf.active.vision.

---

## Rodin: A Generative Model for Sculpting 3D Digital Avatars Using  Diffusion



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-12 | Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, Baining Guo | cs.CV | [PDF](http://arxiv.org/pdf/2212.06135v1){: .btn .btn-green } |

**Abstract**: This paper presents a 3D generative model that uses diffusion models to
automatically generate 3D digital avatars represented as neural radiance
fields. A significant challenge in generating such avatars is that the memory
and processing costs in 3D are prohibitive for producing the rich details
required for high-quality avatars. To tackle this problem we propose the
roll-out diffusion network (Rodin), which represents a neural radiance field as
multiple 2D feature maps and rolls out these maps into a single 2D feature
plane within which we perform 3D-aware diffusion. The Rodin model brings the
much-needed computational efficiency while preserving the integrity of
diffusion in 3D by using 3D-aware convolution that attends to projected
features in the 2D feature plane according to their original relationship in
3D. We also use latent conditioning to orchestrate the feature generation for
global coherence, leading to high-fidelity avatars and enabling their semantic
editing based on text prompts. Finally, we use hierarchical synthesis to
further enhance details. The 3D avatars generated by our model compare
favorably with those produced by existing generative techniques. We can
generate highly detailed avatars with realistic hairstyles and facial hair like
beards. We also demonstrate 3D avatar generation from image or text as well as
text-guided editability.

Comments:
- Project Webpage: https://3d-avatar-diffusion.microsoft.com/

---

## 4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-09 | Zhongshu Wang, Lingzhi Li, Zhen Shen, Li Shen, Liefeng Bo | cs.CV | [PDF](http://arxiv.org/pdf/2212.04701v2){: .btn .btn-green } |

**Abstract**: In this paper, we present a novel and effective framework, named 4K-NeRF, to
pursue high fidelity view synthesis on the challenging scenarios of ultra high
resolutions, building on the methodology of neural radiance fields (NeRF). The
rendering procedure of NeRF-based methods typically relies on a pixel-wise
manner in which rays (or pixels) are treated independently on both training and
inference phases, limiting its representational ability on describing subtle
details, especially when lifting to a extremely high resolution. We address the
issue by exploring ray correlation to enhance high-frequency details recovery.
Particularly, we use the 3D-aware encoder to model geometric information
effectively in a lower resolution space and recover fine details through the
3D-aware decoder, conditioned on ray features and depths estimated by the
encoder. Joint training with patch-based sampling further facilitates our
method incorporating the supervision from perception oriented regularization
beyond pixel-wise loss. Benefiting from the use of geometry-aware local
context, our method can significantly boost rendering quality on high-frequency
details compared with modern NeRF methods, and achieve the state-of-the-art
visual quality on 4K ultra-high-resolution scenarios. Code Available at
\url{https://github.com/frozoul/4K-NeRF}

---

## Few-View Object Reconstruction with Unknown Categories and Camera Poses



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-08 | Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, Yuke Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2212.04492v2){: .btn .btn-green } |

**Abstract**: While object reconstruction has made great strides in recent years, current
methods typically require densely captured images and/or known camera poses,
and generalize poorly to novel object categories. To step toward object
reconstruction in the wild, this work explores reconstructing general
real-world objects from a few images without known camera poses or object
categories. The crux of our work is solving two fundamental 3D vision problems
-- shape reconstruction and pose estimation -- in a unified approach. Our
approach captures the synergies of these two problems: reliable camera pose
estimation gives rise to accurate shape reconstruction, and the accurate
reconstruction, in turn, induces robust correspondence between different views
and facilitates pose estimation. Our method FORGE predicts 3D features from
each view and leverages them in conjunction with the input images to establish
cross-view correspondence for estimating relative camera poses. The 3D features
are then transformed by the estimated poses into a shared space and are fused
into a neural radiance field. The reconstruction results are rendered by volume
rendering techniques, enabling us to train the model without 3D shape
ground-truth. Our experiments show that FORGE reliably reconstructs objects
from five views. Our pose estimation method outperforms existing ones by a
large margin. The reconstruction results under predicted poses are comparable
to the ones using ground-truth poses. The performance on novel testing
categories matches the results on categories seen during training. Project
page: https://ut-austin-rpl.github.io/FORGE/

---

## GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-08 | Alessandro Ruzzi, Xiangwei Shi, Xi Wang, Gengyan Li, Shalini De Mello, Hyung Jin Chang, Xucong Zhang, Otmar Hilliges | cs.CV | [PDF](http://arxiv.org/pdf/2212.04823v2){: .btn .btn-green } |

**Abstract**: We propose GazeNeRF, a 3D-aware method for the task of gaze redirection.
Existing gaze redirection methods operate on 2D images and struggle to generate
3D consistent results. Instead, we build on the intuition that the face region
and eyeballs are separate 3D structures that move in a coordinated yet
independent fashion. Our method leverages recent advancements in conditional
image-based neural radiance fields and proposes a two-stream architecture that
predicts volumetric features for the face and eye regions separately. Rigidly
transforming the eye features via a 3D rotation matrix provides fine-grained
control over the desired gaze angle. The final, redirected image is then
attained via differentiable volume compositing. Our experiments show that this
architecture outperforms naively conditioned NeRF baselines as well as previous
state-of-the-art 2D gaze redirection methods in terms of redirection accuracy
and identity preservation.

Comments:
- Accepted at CVPR 2023. Github page:
  https://github.com/AlessandroRuzzi/GazeNeRF

---

## NeRFEditor: Differentiable Style Decomposition for Full 3D Scene Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-07 | Chunyi Sun, Yanbin Liu, Junlin Han, Stephen Gould | cs.CV | [PDF](http://arxiv.org/pdf/2212.03848v2){: .btn .btn-green } |

**Abstract**: We present NeRFEditor, an efficient learning framework for 3D scene editing,
which takes a video captured over 360{\deg} as input and outputs a
high-quality, identity-preserving stylized 3D scene. Our method supports
diverse types of editing such as guided by reference images, text prompts, and
user interactions. We achieve this by encouraging a pre-trained StyleGAN model
and a NeRF model to learn from each other mutually. Specifically, we use a NeRF
model to generate numerous image-angle pairs to train an adjustor, which can
adjust the StyleGAN latent code to generate high-fidelity stylized images for
any given angle. To extrapolate editing to GAN out-of-domain views, we devise
another module that is trained in a self-supervised learning manner. This
module maps novel-view images to the hidden space of StyleGAN that allows
StyleGAN to generate stylized images on novel views. These two modules together
produce guided images in 360{\deg}views to finetune a NeRF to make stylization
effects, where a stable fine-tuning strategy is proposed to achieve this.
Experiments show that NeRFEditor outperforms prior work on benchmark and
real-world scenes with better editability, fidelity, and identity preservation.

Comments:
- Project page: https://chuny1.github.io/NeRFEditor/nerfeditor.html

---

## Non-uniform Sampling Strategies for NeRF on 360{\textdegree} images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-07 | Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa | cs.CV | [PDF](http://arxiv.org/pdf/2212.03635v1){: .btn .btn-green } |

**Abstract**: In recent years, the performance of novel view synthesis using perspective
images has dramatically improved with the advent of neural radiance fields
(NeRF). This study proposes two novel techniques that effectively build NeRF
for 360{\textdegree} omnidirectional images. Due to the characteristics of a
360{\textdegree} image of ERP format that has spatial distortion in their high
latitude regions and a 360{\textdegree} wide viewing angle, NeRF's general ray
sampling strategy is ineffective. Hence, the view synthesis accuracy of NeRF is
limited and learning is not efficient. We propose two non-uniform ray sampling
schemes for NeRF to suit 360{\textdegree} images - distortion-aware ray
sampling and content-aware ray sampling. We created an evaluation dataset
Synth360 using Replica and SceneCity models of indoor and outdoor scenes,
respectively. In experiments, we show that our proposal successfully builds
360{\textdegree} image NeRF in terms of both accuracy and efficiency. The
proposal is widely applicable to advanced variants of NeRF. DietNeRF, AugNeRF,
and NeRF++ combined with the proposed techniques further improve the
performance. Moreover, we show that our proposed method enhances the quality of
real-world scenes in 360{\textdegree} images. Synth360:
https://drive.google.com/drive/folders/1suL9B7DO2no21ggiIHkH3JF3OecasQLb.

Comments:
- Accepted at the 33rd British Machine Vision Conference (BMVC) 2022

---

## EditableNeRF: Editing Topologically Varying Neural Radiance Fields by  Key Points

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-07 | Chengwei Zheng, Wenbin Lin, Feng Xu | cs.CV | [PDF](http://arxiv.org/pdf/2212.04247v2){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) achieve highly photo-realistic novel-view
synthesis, but it's a challenging problem to edit the scenes modeled by
NeRF-based methods, especially for dynamic scenes. We propose editable neural
radiance fields that enable end-users to easily edit dynamic scenes and even
support topological changes. Input with an image sequence from a single camera,
our network is trained fully automatically and models topologically varying
dynamics using our picked-out surface key points. Then end-users can edit the
scene by easily dragging the key points to desired new positions. To achieve
this, we propose a scene analysis method to detect and initialize key points by
considering the dynamics in the scene, and a weighted key points strategy to
model topologically varying dynamics by joint key points and weights
optimization. Our method supports intuitive multi-dimensional (up to 3D)
editing and can generate novel scenes that are unseen in the input sequence.
Experiments demonstrate that our method achieves high-quality editing on
various dynamic scenes and outperforms the state-of-the-art. Our code and
captured data are available at https://chengwei-zheng.github.io/EditableNeRF/.

Comments:
- Accepted by CVPR 2023

---

## SSDNeRF: Semantic Soft Decomposition of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-07 | Siddhant Ranade, Christoph Lassner, Kai Li, Christian Haene, Shen-Chi Chen, Jean-Charles Bazin, Sofien Bouaziz | cs.CV | [PDF](http://arxiv.org/pdf/2212.03406v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) encode the radiance in a scene parameterized
by the scene's plenoptic function. This is achieved by using an MLP together
with a mapping to a higher-dimensional space, and has been proven to capture
scenes with a great level of detail. Naturally, the same parameterization can
be used to encode additional properties of the scene, beyond just its radiance.
A particularly interesting property in this regard is the semantic
decomposition of the scene. We introduce a novel technique for semantic soft
decomposition of neural radiance fields (named SSDNeRF) which jointly encodes
semantic signals in combination with radiance signals of a scene. Our approach
provides a soft decomposition of the scene into semantic parts, enabling us to
correctly encode multiple semantic classes blending along the same direction --
an impossible feat for existing methods. Not only does this lead to a detailed,
3D semantic representation of the scene, but we also show that the regularizing
effects of the MLP used for encoding help to improve the semantic
representation. We show state-of-the-art segmentation and reconstruction
results on a dataset of common objects and demonstrate how the proposed
approach can be applied for high quality temporally consistent video editing
and re-compositing on a dataset of casually captured selfie videos.

Comments:
- Project page:
  https://www.siddhantranade.com/research/2022/12/06/SSDNeRF-Semantic-Soft-Decomposition-of-Neural-Radiance-Fields.html

---

## NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as  General Image Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-06 | Congyue Deng, Chiyu "Max'' Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov | cs.CV | [PDF](http://arxiv.org/pdf/2212.03267v1){: .btn .btn-green } |

**Abstract**: 2D-to-3D reconstruction is an ill-posed problem, yet humans are good at
solving this problem due to their prior knowledge of the 3D world developed
over years. Driven by this observation, we propose NeRDi, a single-view NeRF
synthesis framework with general image priors from 2D diffusion models.
Formulating single-view reconstruction as an image-conditioned 3D generation
problem, we optimize the NeRF representations by minimizing a diffusion loss on
its arbitrary view renderings with a pretrained image diffusion model under the
input-view constraint. We leverage off-the-shelf vision-language models and
introduce a two-section language guidance as conditioning inputs to the
diffusion model. This is essentially helpful for improving multiview content
coherence as it narrows down the general image prior conditioned on the
semantic and visual features of the single-view input image. Additionally, we
introduce a geometric loss based on estimated depth maps to regularize the
underlying 3D geometry of the NeRF. Experimental results on the DTU MVS dataset
show that our method can synthesize novel views with higher quality even
compared to existing methods trained on this dataset. We also demonstrate our
generalizability in zero-shot NeRF synthesis for in-the-wild images.

---

## SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-05 | Anh-Quan Cao, Raoul de Charette | cs.CV | [PDF](http://arxiv.org/pdf/2212.02501v4){: .btn .btn-green } |

**Abstract**: 3D reconstruction from a single 2D image was extensively covered in the
literature but relies on depth supervision at training time, which limits its
applicability. To relax the dependence to depth we propose SceneRF, a
self-supervised monocular scene reconstruction method using only posed image
sequences for training. Fueled by the recent progress in neural radiance fields
(NeRF) we optimize a radiance field though with explicit depth optimization and
a novel probabilistic sampling strategy to efficiently handle large scenes. At
inference, a single input image suffices to hallucinate novel depth views which
are fused together to obtain 3D scene reconstruction. Thorough experiments
demonstrate that we outperform all baselines for novel depth views synthesis
and scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI.
Code is available at https://astra-vision.github.io/SceneRF .

Comments:
- ICCV 2023. Project page: https://astra-vision.github.io/SceneRF

---

## Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-05 | Rohith Agaram, Shaurya Dewan, Rahul Sajnani, Adrien Poulenard, Madhava Krishna, Srinath Sridhar | cs.CV | [PDF](http://arxiv.org/pdf/2212.02493v3){: .btn .btn-green } |

**Abstract**: Coordinate-based implicit neural networks, or neural fields, have emerged as
useful representations of shape and appearance in 3D computer vision. Despite
advances, however, it remains challenging to build neural fields for categories
of objects without datasets like ShapeNet that provide "canonicalized" object
instances that are consistently aligned for their 3D position and orientation
(pose). We present Canonical Field Network (CaFi-Net), a self-supervised method
to canonicalize the 3D pose of instances from an object category represented as
neural fields, specifically neural radiance fields (NeRFs). CaFi-Net directly
learns from continuous and noisy radiance fields using a Siamese network
architecture that is designed to extract equivariant field features for
category-level canonicalization. During inference, our method takes pre-trained
neural radiance fields of novel object instances at arbitrary 3D pose and
estimates a canonical field with consistent 3D pose across the entire category.
Extensive experiments on a new dataset of 1300 NeRF models across 13 object
categories show that our method matches or exceeds the performance of 3D point
cloud-based methods.

---

## One-shot Implicit Animatable Avatars with Model-based Priors



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-05 | Yangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang, Boxi Wu, Wenxiao Wang, Binbin Lin, Debing Zhang, Deng Cai | cs.CV | [PDF](http://arxiv.org/pdf/2212.02469v4){: .btn .btn-green } |

**Abstract**: Existing neural rendering methods for creating human avatars typically either
require dense input signals such as video or multi-view images, or leverage a
learned prior from large-scale specific 3D human datasets such that
reconstruction can be performed with sparse-view inputs. Most of these methods
fail to achieve realistic reconstruction when only a single image is available.
To enable the data-efficient creation of realistic animatable 3D humans, we
propose ELICIT, a novel method for learning human-specific neural radiance
fields from a single image. Inspired by the fact that humans can effortlessly
estimate the body geometry and imagine full-body clothing from a single image,
we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior.
Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned
vertex-based template model (i.e., SMPL) and implements the visual clothing
semantic prior with the CLIP-based pretrained models. Both priors are used to
jointly guide the optimization for creating plausible content in the invisible
areas. Taking advantage of the CLIP models, ELICIT can use text descriptions to
generate text-conditioned unseen regions. In order to further improve visual
details, we propose a segmentation-based sampling strategy that locally refines
different parts of the avatar. Comprehensive evaluations on multiple popular
benchmarks, including ZJU-MoCAP, Human3.6M, and DeepFashion, show that ELICIT
has outperformed strong baseline methods of avatar creation when only a single
image is available. The code is public for research purposes at
https://huangyangyi.github.io/ELICIT/.

Comments:
- To appear at ICCV 2023. Project website:
  https://huangyangyi.github.io/ELICIT/

---

## D-TensoRF: Tensorial Radiance Fields for Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-05 | Hankyu Jang, Daeyoung Kim | cs.CV | [PDF](http://arxiv.org/pdf/2212.02375v2){: .btn .btn-green } |

**Abstract**: Neural radiance field (NeRF) attracts attention as a promising approach to
reconstructing the 3D scene. As NeRF emerges, subsequent studies have been
conducted to model dynamic scenes, which include motions or topological
changes. However, most of them use an additional deformation network, slowing
down the training and rendering speed. Tensorial radiance field (TensoRF)
recently shows its potential for fast, high-quality reconstruction of static
scenes with compact model size. In this paper, we present D-TensoRF, a
tensorial radiance field for dynamic scenes, enabling novel view synthesis at a
specific time. We consider the radiance field of a dynamic scene as a 5D
tensor. The 5D tensor represents a 4D grid in which each axis corresponds to X,
Y, Z, and time and has 1D multi-channel features per element. Similar to
TensoRF, we decompose the grid either into rank-one vector components (CP
decomposition) or low-rank matrix components (newly proposed MM decomposition).
We also use smoothing regularization to reflect the relationship between
features at different times (temporal dependency). We conduct extensive
evaluations to analyze our models. We show that D-TensoRF with CP decomposition
and MM decomposition both have short training times and significantly low
memory footprints with quantitatively and qualitatively competitive rendering
results in comparison to the state-of-the-art methods in 3D dynamic scene
modeling.

Comments:
- 21 pages, 11 figures

---

## GARF:Geometry-Aware Generalized Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-05 | Yue Shi, Dingyi Rong, Bingbing Ni, Chang Chen, Wenjun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2212.02280v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has revolutionized free viewpoint rendering
tasks and achieved impressive results. However, the efficiency and accuracy
problems hinder its wide applications. To address these issues, we propose
Geometry-Aware Generalized Neural Radiance Field (GARF) with a geometry-aware
dynamic sampling (GADS) strategy to perform real-time novel view rendering and
unsupervised depth estimation on unseen scenes without per-scene optimization.
Distinct from most existing generalized NeRFs, our framework infers the unseen
scenes on both pixel-scale and geometry-scale with only a few input images.
More specifically, our method learns common attributes of novel-view synthesis
by an encoder-decoder structure and a point-level learnable multi-view feature
fusion module which helps avoid occlusion. To preserve scene characteristics in
the generalized model, we introduce an unsupervised depth estimation module to
derive the coarse geometry, narrow down the ray sampling interval to proximity
space of the estimated surface and sample in expectation maximum position,
constituting Geometry-Aware Dynamic Sampling strategy (GADS). Moreover, we
introduce a Multi-level Semantic Consistency loss (MSC) to assist more
informative representation learning. Extensive experiments on indoor and
outdoor datasets show that comparing with state-of-the-art generalized NeRF
methods, GARF reduces samples by more than 25\%, while improving rendering
quality and 3D geometry estimation.

---

## INGeo: Accelerating Instant Neural Scene Reconstruction with Noisy  Geometry Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-05 | Chaojian Li, Bichen Wu, Albert Pumarola, Peizhao Zhang, Yingyan Lin, Peter Vajda | cs.CV | [PDF](http://arxiv.org/pdf/2212.01959v1){: .btn .btn-green } |

**Abstract**: We present a method that accelerates reconstruction of 3D scenes and objects,
aiming to enable instant reconstruction on edge devices such as mobile phones
and AR/VR headsets. While recent works have accelerated scene reconstruction
training to minute/second-level on high-end GPUs, there is still a large gap to
the goal of instant training on edge devices which is yet highly desired in
many emerging applications such as immersive AR/VR. To this end, this work aims
to further accelerate training by leveraging geometry priors of the target
scene. Our method proposes strategies to alleviate the noise of the imperfect
geometry priors to accelerate the training speed on top of the highly optimized
Instant-NGP. On the NeRF Synthetic dataset, our work uses half of the training
iterations to reach an average test PSNR of >30.

Comments:
- Accepted by Computer Vision for Metaverse Workshop @ ECCV'22

---

## Neural Fourier Filter Bank



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-04 | Zhijie Wu, Yuhe Jin, Kwang Moo Yi | cs.CV | [PDF](http://arxiv.org/pdf/2212.01735v4){: .btn .btn-green } |

**Abstract**: We present a novel method to provide efficient and highly detailed
reconstructions. Inspired by wavelets, we learn a neural field that decompose
the signal both spatially and frequency-wise. We follow the recent grid-based
paradigm for spatial decomposition, but unlike existing work, encourage
specific frequencies to be stored in each grid via Fourier features encodings.
We then apply a multi-layer perceptron with sine activations, taking these
Fourier encoded features in at appropriate layers so that higher-frequency
components are accumulated on top of lower-frequency components sequentially,
which we sum up to form the final output. We demonstrate that our method
outperforms the state of the art regarding model compactness and convergence
speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural
radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.

---

## MaRF: Representing Mars as Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-03 | Lorenzo Giusti, Josue Garcia, Steven Cozine, Darrick Suen, Christina Nguyen, Ryan Alimo | cs.CV | [PDF](http://arxiv.org/pdf/2212.01672v1){: .btn .btn-green } |

**Abstract**: The aim of this work is to introduce MaRF, a novel framework able to
synthesize the Martian environment using several collections of images from
rover cameras. The idea is to generate a 3D scene of Mars' surface to address
key challenges in planetary surface exploration such as: planetary geology,
simulated navigation and shape analysis. Although there exist different methods
to enable a 3D reconstruction of Mars' surface, they rely on classical computer
graphics techniques that incur high amounts of computational resources during
the reconstruction process, and have limitations with generalizing
reconstructions to unseen scenes and adapting to new images coming from rover
cameras. The proposed framework solves the aforementioned limitations by
exploiting Neural Radiance Fields (NeRFs), a method that synthesize complex
scenes by optimizing a continuous volumetric scene function using a sparse set
of images. To speed up the learning process, we replaced the sparse set of
rover images with their neural graphics primitives (NGPs), a set of vectors of
fixed length that are learned to preserve the information of the original
images in a significantly smaller size. In the experimental section, we
demonstrate the environments created from actual Mars datasets captured by
Curiosity rover, Perseverance rover and Ingenuity helicopter, all of which are
available on the Planetary Data System (PDS).

Comments:
- ECCV 2022 (oral)

---

## StegaNeRF: Embedding Invisible Information within Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-03 | Chenxin Li, Brandon Y. Feng, Zhiwen Fan, Panwang Pan, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2212.01602v1){: .btn .btn-green } |

**Abstract**: Recent advances in neural rendering imply a future of widespread visual data
distributions through sharing NeRF model weights. However, while common visual
data (images and videos) have standard approaches to embed ownership or
copyright information explicitly or subtly, the problem remains unexplored for
the emerging NeRF format. We present StegaNeRF, a method for steganographic
information embedding in NeRF renderings. We design an optimization framework
allowing accurate hidden information extractions from images rendered by NeRF,
while preserving its original visual quality. We perform experimental
evaluations of our method under several potential deployment scenarios, and we
further discuss the insights discovered through our analysis. StegaNeRF
signifies an initial exploration into the novel problem of instilling
customizable, imperceptible, and recoverable information to NeRF renderings,
with minimal impact to rendered images. Project page:
https://xggnet.github.io/StegaNeRF/.

Comments:
- Project page: https://xggnet.github.io/StegaNeRF/

---

## Fast Non-Rigid Radiance Fields from Monocularized Data



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-02 | Moritz Kappel, Vladislav Golyanik, Susana Castillo, Christian Theobalt, Marcus Magnor | cs.CV | [PDF](http://arxiv.org/pdf/2212.01368v2){: .btn .btn-green } |

**Abstract**: The reconstruction and novel view synthesis of dynamic scenes recently gained
increased attention. As reconstruction from large-scale multi-view data
involves immense memory and computational requirements, recent benchmark
datasets provide collections of single monocular views per timestamp sampled
from multiple (virtual) cameras. We refer to this form of inputs as
"monocularized" data. Existing work shows impressive results for synthetic
setups and forward-facing real-world data, but is often limited in the training
speed and angular range for generating novel views. This paper addresses these
limitations and proposes a new method for full 360{\deg} inward-facing novel
view synthesis of non-rigidly deforming scenes. At the core of our method are:
1) An efficient deformation module that decouples the processing of spatial and
temporal information for accelerated training and inference; and 2) A static
module representing the canonical scene as a fast hash-encoded neural radiance
field. In addition to existing synthetic monocularized data, we systematically
analyze the performance on real-world inward-facing scenes using a newly
recorded challenging dataset sampled from a synchronized large-scale multi-view
rig. In both cases, our method is significantly faster than previous methods,
converging in less than 7 minutes and achieving real-time framerates at 1K
resolution, while obtaining a higher visual accuracy for generated novel views.
Our source code and data is available at our project page
https://graphics.tu-bs.de/publications/kappel2022fast.

Comments:
- 18 pages, 14 figures; project page:
  https://graphics.tu-bs.de/publications/kappel2022fast

---

## Surface Normal Clustering for Implicit Representation of Manhattan  Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-02 | Nikola Popovic, Danda Pani Paudel, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2212.01331v4){: .btn .btn-green } |

**Abstract**: Novel view synthesis and 3D modeling using implicit neural field
representation are shown to be very effective for calibrated multi-view
cameras. Such representations are known to benefit from additional geometric
and semantic supervision. Most existing methods that exploit additional
supervision require dense pixel-wise labels or localized scene priors. These
methods cannot benefit from high-level vague scene priors provided in terms of
scenes' descriptions. In this work, we aim to leverage the geometric prior of
Manhattan scenes to improve the implicit neural radiance field representations.
More precisely, we assume that only the knowledge of the indoor scene (under
investigation) being Manhattan is known -- with no additional information
whatsoever -- with an unknown Manhattan coordinate frame. Such high-level prior
is used to self-supervise the surface normals derived explicitly in the
implicit neural fields. Our modeling allows us to cluster the derived normals
and exploit their orthogonality constraints for self-supervision. Our
exhaustive experiments on datasets of diverse indoor scenes demonstrate the
significant benefit of the proposed method over the established baselines. The
source code is available at
https://github.com/nikola3794/normal-clustering-nerf.

Comments:
- Paper accepted to ICCV23

---

## RT-NeRF: Real-Time On-Device Neural Radiance Fields Towards Immersive  AR/VR Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-02 | Chaojian Li, Sixu Li, Yang Zhao, Wenbo Zhu, Yingyan Lin | cs.AR | [PDF](http://arxiv.org/pdf/2212.01120v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) based rendering has attracted growing attention
thanks to its state-of-the-art (SOTA) rendering quality and wide applications
in Augmented and Virtual Reality (AR/VR). However, immersive real-time (> 30
FPS) NeRF based rendering enabled interactions are still limited due to the low
achievable throughput on AR/VR devices. To this end, we first profile SOTA
efficient NeRF algorithms on commercial devices and identify two primary causes
of the aforementioned inefficiency: (1) the uniform point sampling and (2) the
dense accesses and computations of the required embeddings in NeRF.
Furthermore, we propose RT-NeRF, which to the best of our knowledge is the
first algorithm-hardware co-design acceleration of NeRF. Specifically, on the
algorithm level, RT-NeRF integrates an efficient rendering pipeline for largely
alleviating the inefficiency due to the commonly adopted uniform point sampling
method in NeRF by directly computing the geometry of pre-existing points.
Additionally, RT-NeRF leverages a coarse-grained view-dependent computing
ordering scheme for eliminating the (unnecessary) processing of invisible
points. On the hardware level, our proposed RT-NeRF accelerator (1) adopts a
hybrid encoding scheme to adaptively switch between a bitmap- or
coordinate-based sparsity encoding format for NeRF's sparse embeddings, aiming
to maximize the storage savings and thus reduce the required DRAM accesses
while supporting efficient NeRF decoding; and (2) integrates both a
dual-purpose bi-direction adder & search tree and a high-density sparse search
unit to coordinate the two aforementioned encoding formats. Extensive
experiments on eight datasets consistently validate the effectiveness of
RT-NeRF, achieving a large throughput improvement (e.g., 9.7x - 3,201x) while
maintaining the rendering quality as compared with SOTA efficient NeRF
solutions.

Comments:
- Accepted to ICCAD 2022

---

## 3D-TOGO: Towards Text-Guided Cross-Category 3D Object Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-02 | Zutao Jiang, Guansong Lu, Xiaodan Liang, Jihua Zhu, Wei Zhang, Xiaojun Chang, Hang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2212.01103v2){: .btn .btn-green } |

**Abstract**: Text-guided 3D object generation aims to generate 3D objects described by
user-defined captions, which paves a flexible way to visualize what we
imagined. Although some works have been devoted to solving this challenging
task, these works either utilize some explicit 3D representations (e.g., mesh),
which lack texture and require post-processing for rendering photo-realistic
views; or require individual time-consuming optimization for every single case.
Here, we make the first attempt to achieve generic text-guided cross-category
3D object generation via a new 3D-TOGO model, which integrates a text-to-views
generation module and a views-to-3D generation module. The text-to-views
generation module is designed to generate different views of the target 3D
object given an input caption. prior-guidance, caption-guidance and view
contrastive learning are proposed for achieving better view-consistency and
caption similarity. Meanwhile, a pixelNeRF model is adopted for the views-to-3D
generation module to obtain the implicit 3D neural representation from the
previously-generated views. Our 3D-TOGO model generates 3D objects in the form
of the neural radiance field with good texture and requires no time-cost
optimization for every single caption. Besides, 3D-TOGO can control the
category, color and shape of generated 3D objects with the input caption.
Extensive experiments on the largest 3D object dataset (i.e., ABO) are
conducted to verify that 3D-TOGO can better generate high-quality 3D objects
according to the input captions across 98 different categories, in terms of
PSNR, SSIM, LPIPS and CLIP-score, compared with text-NeRF and Dreamfields.

---

## QFF: Quantized Fourier Features for Neural Field Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-02 | Jae Yong Lee, Yuqun Wu, Chuhang Zou, Shenlong Wang, Derek Hoiem | cs.CV | [PDF](http://arxiv.org/pdf/2212.00914v1){: .btn .btn-green } |

**Abstract**: Multilayer perceptrons (MLPs) learn high frequencies slowly. Recent
approaches encode features in spatial bins to improve speed of learning
details, but at the cost of larger model size and loss of continuity. Instead,
we propose to encode features in bins of Fourier features that are commonly
used for positional encoding. We call these Quantized Fourier Features (QFF).
As a naturally multiresolution and periodic representation, our experiments
show that using QFF can result in smaller model size, faster training, and
better quality outputs for several applications, including Neural Image
Representations (NIR), Neural Radiance Field (NeRF) and Signed Distance
Function (SDF) modeling. QFF are easy to code, fast to compute, and serve as a
simple drop-in addition to many neural field representations.

---

## Mixed Neural Voxels for Fast Multi-view Video Synthesis



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-01 | Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei Song, Huaping Liu | cs.CV | [PDF](http://arxiv.org/pdf/2212.00190v2){: .btn .btn-green } |

**Abstract**: Synthesizing high-fidelity videos from real-world multi-view input is
challenging because of the complexities of real-world environments and highly
dynamic motions. Previous works based on neural radiance fields have
demonstrated high-quality reconstructions of dynamic scenes. However, training
such models on real-world scenes is time-consuming, usually taking days or
weeks. In this paper, we present a novel method named MixVoxels to better
represent the dynamic scenes with fast training speed and competitive rendering
qualities. The proposed MixVoxels represents the 4D dynamic scenes as a mixture
of static and dynamic voxels and processes them with different networks. In
this way, the computation of the required modalities for static voxels can be
processed by a lightweight model, which essentially reduces the amount of
computation, especially for many daily dynamic scenes dominated by the static
background. To separate the two kinds of voxels, we propose a novel variation
field to estimate the temporal variance of each voxel. For the dynamic voxels,
we design an inner-product time query method to efficiently query multiple time
steps, which is essential to recover the high-dynamic motions. As a result,
with 15 minutes of training for dynamic scenes with inputs of 300-frame videos,
MixVoxels achieves better PSNR than previous methods. Codes and trained models
are available at https://github.com/fengres/mixvoxels

Comments:
- ICCV 2023 (Oral)

---

## ViewNeRF: Unsupervised Viewpoint Estimation Using Category-Level Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-12-01 | Octave Mariotti, Oisin Mac Aodha, Hakan Bilen | cs.CV | [PDF](http://arxiv.org/pdf/2212.00436v1){: .btn .btn-green } |

**Abstract**: We introduce ViewNeRF, a Neural Radiance Field-based viewpoint estimation
method that learns to predict category-level viewpoints directly from images
during training. While NeRF is usually trained with ground-truth camera poses,
multiple extensions have been proposed to reduce the need for this expensive
supervision. Nonetheless, most of these methods still struggle in complex
settings with large camera movements, and are restricted to single scenes, i.e.
they cannot be trained on a collection of scenes depicting the same object
category. To address these issues, our method uses an analysis by synthesis
approach, combining a conditional NeRF with a viewpoint predictor and a scene
encoder in order to produce self-supervised reconstructions for whole object
categories. Rather than focusing on high fidelity reconstruction, we target
efficient and accurate viewpoint prediction in complex scenarios, e.g.
360{\deg} rotation on real data. Our model shows competitive results on
synthetic and real datasets, both for single scenes and multi-instance
collections.

---

## NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real  Image Animation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-30 | Yu Yin, Kamran Ghasedi, HsiangTao Wu, Jiaolong Yang, Xin Tong, Yun Fu | cs.CV | [PDF](http://arxiv.org/pdf/2211.17235v1){: .btn .btn-green } |

**Abstract**: Nerf-based Generative models have shown impressive capacity in generating
high-quality images with consistent 3D geometry. Despite successful synthesis
of fake identity images randomly sampled from latent space, adopting these
models for generating face images of real subjects is still a challenging task
due to its so-called inversion issue. In this paper, we propose a universal
method to surgically fine-tune these NeRF-GAN models in order to achieve
high-fidelity animation of real subjects only by a single image. Given the
optimized latent code for an out-of-domain real image, we employ 2D loss
functions on the rendered image to reduce the identity gap. Furthermore, our
method leverages explicit and implicit 3D regularizations using the in-domain
neighborhood samples around the optimized latent code to remove geometrical and
visual artifacts. Our experiments confirm the effectiveness of our method in
realistic, high-fidelity, and 3D consistent animation of real faces on multiple
NeRF-GAN models across different datasets.

---

## DINER: Depth-aware Image-based NEural Radiance fields



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-29 | Malte Prinzler, Otmar Hilliges, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2211.16630v2){: .btn .btn-green } |

**Abstract**: We present Depth-aware Image-based NEural Radiance fields (DINER). Given a
sparse set of RGB input views, we predict depth and feature maps to guide the
reconstruction of a volumetric scene representation that allows us to render 3D
objects under novel views. Specifically, we propose novel techniques to
incorporate depth information into feature fusion and efficient scene sampling.
In comparison to the previous state of the art, DINER achieves higher synthesis
quality and can process input views with greater disparity. This allows us to
capture scenes more completely without changing capturing hardware requirements
and ultimately enables larger viewpoint changes during novel view synthesis. We
evaluate our method by synthesizing novel views, both for human heads and for
general objects, and observe significantly improved qualitative results and
increased perceptual metrics compared to the previous state of the art. The
code is publicly available for research purposes.

Comments:
- Website: https://malteprinzler.github.io/projects/diner/diner.html ;
  Video: https://www.youtube.com/watch?v=iI_fpjY5k8Y&t=1s

---

## NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with  360° Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-29 | Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2211.16431v2){: .btn .btn-green } |

**Abstract**: Virtual reality and augmented reality (XR) bring increasing demand for 3D
content. However, creating high-quality 3D content requires tedious work that a
human expert must do. In this work, we study the challenging task of lifting a
single image to a 3D object and, for the first time, demonstrate the ability to
generate a plausible 3D object with 360{\deg} views that correspond well with
the given reference image. By conditioning on the reference image, our model
can fulfill the everlasting curiosity for synthesizing novel views of objects
from images. Our technique sheds light on a promising direction of easing the
workflows for 3D artists and XR designers. We propose a novel framework, dubbed
NeuralLift-360, that utilizes a depth-aware neural radiance representation
(NeRF) and learns to craft the scene guided by denoising diffusion models. By
introducing a ranking loss, our NeuralLift-360 can be guided with rough depth
estimation in the wild. We also adopt a CLIP-guided sampling strategy for the
diffusion prior to provide coherent guidance. Extensive experiments demonstrate
that our NeuralLift-360 significantly outperforms existing state-of-the-art
baselines. Project page: https://vita-group.github.io/NeuralLift-360/

Comments:
- Project page: https://vita-group.github.io/NeuralLift-360/

---

## Compressing Volumetric Radiance Fields to 1 MB

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-29 | Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, Liefeng Bo | cs.CV | [PDF](http://arxiv.org/pdf/2211.16386v1){: .btn .btn-green } |

**Abstract**: Approximating radiance fields with volumetric grids is one of promising
directions for improving NeRF, represented by methods like Plenoxels and DVGO,
which achieve super-fast training convergence and real-time rendering. However,
these methods typically require a tremendous storage overhead, costing up to
hundreds of megabytes of disk space and runtime memory for a single scene. We
address this issue in this paper by introducing a simple yet effective
framework, called vector quantized radiance fields (VQRF), for compressing
these volume-grid-based radiance fields. We first present a robust and adaptive
metric for estimating redundancy in grid models and performing voxel pruning by
better exploring intermediate outputs of volumetric rendering. A trainable
vector quantization is further proposed to improve the compactness of grid
models. In combination with an efficient joint tuning strategy and
post-processing, our method can achieve a compression ratio of 100$\times$ by
reducing the overall model size to 1 MB with negligible loss on visual quality.
Extensive experiments demonstrate that the proposed framework is capable of
achieving unrivaled performance and well generalization across multiple methods
with distinct volumetric structures, facilitating the wide use of volumetric
radiance fields methods in real-world applications. Code Available at
\url{https://github.com/AlgoHunt/VQRF}

---

## One is All: Bridging the Gap Between Neural Radiance Fields  Architectures with Progressive Volume Distillation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-29 | Shuangkang Fang, Weixin Xu, Heng Wang, Yi Yang, Yufeng Wang, Shuchang Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2211.15977v3){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) methods have proved effective as compact,
high-quality and versatile representations for 3D scenes, and enable downstream
tasks such as editing, retrieval, navigation, etc. Various neural architectures
are vying for the core structure of NeRF, including the plain Multi-Layer
Perceptron (MLP), sparse tensors, low-rank tensors, hashtables and their
compositions. Each of these representations has its particular set of
trade-offs. For example, the hashtable-based representations admit faster
training and rendering but their lack of clear geometric meaning hampers
downstream tasks like spatial-relation-aware editing. In this paper, we propose
Progressive Volume Distillation (PVD), a systematic distillation method that
allows any-to-any conversions between different architectures, including MLP,
sparse or low-rank tensors, hashtables and their compositions. PVD consequently
empowers downstream applications to optimally adapt the neural representations
for the task at hand in a post hoc fashion. The conversions are fast, as
distillation is progressively performed on different levels of volume
representations, from shallower to deeper. We also employ special treatment of
density to deal with its specific numerical instability problem. Empirical
evidence is presented to validate our method on the NeRF-Synthetic, LLFF and
TanksAndTemples datasets. For example, with PVD, an MLP-based NeRF model can be
distilled from a hashtable-based Instant-NGP model at a 10X~20X faster speed
than being trained the original NeRF from scratch, while achieving a superior
level of synthesis quality. Code is available at
https://github.com/megvii-research/AAAI2023-PVD.

Comments:
- Accepted by AAAI2023. Project Page: https://sk-fun.fun/PVD

---

## In-Hand 3D Object Scanning from an RGB Sequence

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-28 | Shreyas Hampali, Tomas Hodan, Luan Tran, Lingni Ma, Cem Keskin, Vincent Lepetit | cs.CV | [PDF](http://arxiv.org/pdf/2211.16193v2){: .btn .btn-green } |

**Abstract**: We propose a method for in-hand 3D scanning of an unknown object with a
monocular camera. Our method relies on a neural implicit surface representation
that captures both the geometry and the appearance of the object, however, by
contrast with most NeRF-based methods, we do not assume that the camera-object
relative poses are known. Instead, we simultaneously optimize both the object
shape and the pose trajectory. As direct optimization over all shape and pose
parameters is prone to fail without coarse-level initialization, we propose an
incremental approach that starts by splitting the sequence into carefully
selected overlapping segments within which the optimization is likely to
succeed. We reconstruct the object shape and track its poses independently
within each segment, then merge all the segments before performing a global
optimization. We show that our method is able to reconstruct the shape and
color of both textured and challenging texture-less objects, outperforms
classical methods that rely only on appearance features, and that its
performance is close to recent methods that assume known camera poses.

Comments:
- CVPR 2023

---

## High-fidelity Facial Avatar Reconstruction from Monocular Video with  Generative Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-28 | Yunpeng Bai, Yanbo Fan, Xuan Wang, Yong Zhang, Jingxiang Sun, Chun Yuan, Ying Shan | cs.CV | [PDF](http://arxiv.org/pdf/2211.15064v2){: .btn .btn-green } |

**Abstract**: High-fidelity facial avatar reconstruction from a monocular video is a
significant research problem in computer graphics and computer vision.
Recently, Neural Radiance Field (NeRF) has shown impressive novel view
rendering results and has been considered for facial avatar reconstruction.
However, the complex facial dynamics and missing 3D information in monocular
videos raise significant challenges for faithful facial reconstruction. In this
work, we propose a new method for NeRF-based facial avatar reconstruction that
utilizes 3D-aware generative prior. Different from existing works that depend
on a conditional deformation field for dynamic modeling, we propose to learn a
personalized generative prior, which is formulated as a local and low
dimensional subspace in the latent space of 3D-GAN. We propose an efficient
method to construct the personalized generative prior based on a small set of
facial images of a given individual. After learning, it allows for
photo-realistic rendering with novel views and the face reenactment can be
realized by performing navigation in the latent space. Our proposed method is
applicable for different driven signals, including RGB images, 3DMM
coefficients, and audios. Compared with existing works, we obtain superior
novel view synthesis results and faithfully face reenactment performance.

Comments:
- 8 pages, 7 figures

---

## 3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer  Avenue

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-27 | Yujie Li, Bowen Cai, Yuqin Liang, Rongfei Jia, Binqiang Zhao, Mingming Gong, Huan Fu | cs.CV | [PDF](http://arxiv.org/pdf/2211.14823v2){: .btn .btn-green } |

**Abstract**: This paper studies how to flexibly integrate reconstructed 3D models into
practical 3D modeling pipelines such as 3D scene creation and rendering. Due to
the technical difficulty, one can only obtain rough 3D models (R3DMs) for most
real objects using existing 3D reconstruction techniques. As a result,
physically-based rendering (PBR) would render low-quality images or videos for
scenes that are constructed by R3DMs. One promising solution would be
representing real-world objects as Neural Fields such as NeRFs, which are able
to generate photo-realistic renderings of an object under desired viewpoints.
However, a drawback is that the synthesized views through Neural Fields
Rendering (NFR) cannot reflect the simulated lighting details on R3DMs in PBR
pipelines, especially when object interactions in the 3D scene creation cause
local shadows. To solve this dilemma, we propose a lighting transfer network
(LighTNet) to bridge NFR and PBR, such that they can benefit from each other.
LighTNet reasons about a simplified image composition model, remedies the
uneven surface issue caused by R3DMs, and is empowered by several
perceptual-motivated constraints and a new Lab angle loss which enhances the
contrast between lighting strength and colors. Comparisons demonstrate that
LighTNet is superior in synthesizing impressive lighting, and is promising in
pushing NFR further in practical 3D modeling workflows. Project page:
https://3d-front-future.github.io/LighTNet .

---

## Sampling Neural Radiance Fields for Refractive Objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-27 | Jen-I Pan, Jheng-Wei Su, Kai-Wen Hsiao, Ting-Yu Yen, Hung-Kuo Chu | cs.CV | [PDF](http://arxiv.org/pdf/2211.14799v1){: .btn .btn-green } |

**Abstract**: Recently, differentiable volume rendering in neural radiance fields (NeRF)
has gained a lot of popularity, and its variants have attained many impressive
results. However, existing methods usually assume the scene is a homogeneous
volume so that a ray is cast along the straight path. In this work, the scene
is instead a heterogeneous volume with a piecewise-constant refractive index,
where the path will be curved if it intersects the different refractive
indices. For novel view synthesis of refractive objects, our NeRF-based
framework aims to optimize the radiance fields of bounded volume and boundary
from multi-view posed images with refractive object silhouettes. To tackle this
challenging problem, the refractive index of a scene is reconstructed from
silhouettes. Given the refractive index, we extend the stratified and
hierarchical sampling techniques in NeRF to allow drawing samples along a
curved path tracked by the Eikonal equation. The results indicate that our
framework outperforms the state-of-the-art method both quantitatively and
qualitatively, demonstrating better performance on the perceptual similarity
metric and an apparent improvement in the rendering quality on several
synthetic and real scenes.

Comments:
- SIGGRAPH Asia 2022 Technical Communications. 4 pages, 4 figures, 1
  table. Project: https://alexkeroro86.github.io/SampleNeRFRO/ Code:
  https://github.com/alexkeroro86/SampleNeRFRO

---

## SuNeRF: Validation of a 3D Global Reconstruction of the Solar Corona  Using Simulated EUV Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-27 | Kyriaki-Margarita Bintsi, Robert Jarolim, Benoit Tremblay, Miraflor Santos, Anna Jungbluth, James Paul Mason, Sairam Sundaresan, Angelos Vourlidas, Cooper Downs, Ronald M. Caplan, Andrés Muñoz Jaramillo | astro-ph.SR | [PDF](http://arxiv.org/pdf/2211.14879v1){: .btn .btn-green } |

**Abstract**: Extreme Ultraviolet (EUV) light emitted by the Sun impacts satellite
operations and communications and affects the habitability of planets.
Currently, EUV-observing instruments are constrained to viewing the Sun from
its equator (i.e., ecliptic), limiting our ability to forecast EUV emission for
other viewpoints (e.g. solar poles), and to generalize our knowledge of the
Sun-Earth system to other host stars. In this work, we adapt Neural Radiance
Fields (NeRFs) to the physical properties of the Sun and demonstrate that
non-ecliptic viewpoints could be reconstructed from observations limited to the
solar ecliptic. To validate our approach, we train on simulations of solar EUV
emission that provide a ground truth for all viewpoints. Our model accurately
reconstructs the simulated 3D structure of the Sun, achieving a peak
signal-to-noise ratio of 43.3 dB and a mean absolute relative error of 0.3\%
for non-ecliptic viewpoints. Our method provides a consistent 3D reconstruction
of the Sun from a limited number of viewpoints, thus highlighting the potential
to create a virtual instrument for satellite observations of the Sun. Its
extension to real observations will provide the missing link to compare the Sun
to other stars and to improve space-weather forecasting.

Comments:
- Accepted at Machine Learning and the Physical Sciences workshop,
  NeurIPS 2022

---

## ResNeRF: Geometry-Guided Residual Neural Radiance Field for Indoor Scene  Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-26 | Yuting Xiao, Yiqun Zhao, Yanyu Xu, Shenghua Gao | cs.CV | [PDF](http://arxiv.org/pdf/2211.16211v3){: .btn .btn-green } |

**Abstract**: We represent the ResNeRF, a novel geometry-guided two-stage framework for
indoor scene novel view synthesis. Be aware of that a good geometry would
greatly boost the performance of novel view synthesis, and to avoid the
geometry ambiguity issue, we propose to characterize the density distribution
of the scene based on a base density estimated from scene geometry and a
residual density parameterized by the geometry. In the first stage, we focus on
geometry reconstruction based on SDF representation, which would lead to a good
geometry surface of the scene and also a sharp density. In the second stage,
the residual density is learned based on the SDF learned in the first stage for
encoding more details about the appearance. In this way, our method can better
learn the density distribution with the geometry prior for high-fidelity novel
view synthesis while preserving the 3D structures. Experiments on large-scale
indoor scenes with many less-observed and textureless areas show that with the
good 3D surface, our method achieves state-of-the-art performance for novel
view synthesis.

Comments:
- This is an incomplete paper

---

## 3DDesigner: Towards Photorealistic 3D Object Generation and Editing with  Text-guided Diffusion Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-25 | Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, Dacheng Tao | cs.CV | [PDF](http://arxiv.org/pdf/2211.14108v3){: .btn .btn-green } |

**Abstract**: Text-guided diffusion models have shown superior performance in image/video
generation and editing. While few explorations have been performed in 3D
scenarios. In this paper, we discuss three fundamental and interesting problems
on this topic. First, we equip text-guided diffusion models to achieve
3D-consistent generation. Specifically, we integrate a NeRF-like neural field
to generate low-resolution coarse results for a given camera view. Such results
can provide 3D priors as condition information for the following diffusion
process. During denoising diffusion, we further enhance the 3D consistency by
modeling cross-view correspondences with a novel two-stream (corresponding to
two different views) asynchronous diffusion process. Second, we study 3D local
editing and propose a two-step solution that can generate 360-degree
manipulated results by editing an object from a single view. Step 1, we propose
to perform 2D local editing by blending the predicted noises. Step 2, we
conduct a noise-to-text inversion process that maps 2D blended noises into the
view-independent text embedding space. Once the corresponding text embedding is
obtained, 360-degree images can be generated. Last but not least, we extend our
model to perform one-shot novel view synthesis by fine-tuning on a single
image, firstly showing the potential of leveraging text guidance for novel view
synthesis. Extensive experiments and various applications show the prowess of
our 3DDesigner. The project page is available at
https://3ddesigner-diffusion.github.io/.

Comments:
- Submitted to IJCV

---

## ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-25 | Jingwang Ling, Zhibo Wang, Feng Xu | cs.CV | [PDF](http://arxiv.org/pdf/2211.14086v2){: .btn .btn-green } |

**Abstract**: By supervising camera rays between a scene and multi-view image planes, NeRF
reconstructs a neural scene representation for the task of novel view
synthesis. On the other hand, shadow rays between the light source and the
scene have yet to be considered. Therefore, we propose a novel shadow ray
supervision scheme that optimizes both the samples along the ray and the ray
location. By supervising shadow rays, we successfully reconstruct a neural SDF
of the scene from single-view images under multiple lighting conditions. Given
single-view binary shadows, we train a neural network to reconstruct a complete
scene not limited by the camera's line of sight. By further modeling the
correlation between the image colors and the shadow rays, our technique can
also be effectively extended to RGB inputs. We compare our method with previous
works on challenging tasks of shape reconstruction from single-view binary
shadow or RGB images and observe significant improvements. The code and data
are available at https://github.com/gerwang/ShadowNeuS.

Comments:
- CVPR 2023. Project page: https://gerwang.github.io/shadowneus/

---

## Dynamic Neural Portraits

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-25 | Michail Christos Doukas, Stylianos Ploumpis, Stefanos Zafeiriou | cs.CV | [PDF](http://arxiv.org/pdf/2211.13994v1){: .btn .btn-green } |

**Abstract**: We present Dynamic Neural Portraits, a novel approach to the problem of
full-head reenactment. Our method generates photo-realistic video portraits by
explicitly controlling head pose, facial expressions and eye gaze. Our proposed
architecture is different from existing methods that rely on GAN-based
image-to-image translation networks for transforming renderings of 3D faces
into photo-realistic images. Instead, we build our system upon a 2D
coordinate-based MLP with controllable dynamics. Our intuition to adopt a
2D-based representation, as opposed to recent 3D NeRF-like systems, stems from
the fact that video portraits are captured by monocular stationary cameras,
therefore, only a single viewpoint of the scene is available. Primarily, we
condition our generative model on expression blendshapes, nonetheless, we show
that our system can be successfully driven by audio features as well. Our
experiments demonstrate that the proposed method is 270 times faster than
recent NeRF-based reenactment methods, with our networks achieving speeds of 24
fps for resolutions up to 1024 x 1024, while outperforming prior works in terms
of visual quality.

Comments:
- In IEEE/CVF Winter Conference on Applications of Computer Vision
  (WACV) 2023

---

## Unsupervised Continual Semantic Adaptation through Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-25 | Zhizheng Liu, Francesco Milano, Jonas Frey, Roland Siegwart, Hermann Blum, Cesar Cadena | cs.CV | [PDF](http://arxiv.org/pdf/2211.13969v2){: .btn .btn-green } |

**Abstract**: An increasing amount of applications rely on data-driven models that are
deployed for perception tasks across a sequence of scenes. Due to the mismatch
between training and deployment data, adapting the model on the new scenes is
often crucial to obtain good performance. In this work, we study continual
multi-scene adaptation for the task of semantic segmentation, assuming that no
ground-truth labels are available during deployment and that performance on the
previous scenes should be maintained. We propose training a Semantic-NeRF
network for each scene by fusing the predictions of a segmentation model and
then using the view-consistent rendered semantic labels as pseudo-labels to
adapt the model. Through joint training with the segmentation model, the
Semantic-NeRF model effectively enables 2D-3D knowledge transfer. Furthermore,
due to its compact size, it can be stored in a long-term memory and
subsequently used to render data from arbitrary viewpoints to reduce
forgetting. We evaluate our approach on ScanNet, where we outperform both a
voxel-based baseline and a state-of-the-art unsupervised domain adaptation
method.

Comments:
- Accepted by the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2023. Zhizheng Liu and Francesco Milano share first
  authorship. Hermann Blum and Cesar Cadena share senior authorship. 18 pages,
  8 figures, 9 tables

---

## TPA-Net: Generate A Dataset for Text to Physics-based Animation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-25 | Yuxing Qiu, Feng Gao, Minchen Li, Govind Thattai, Yin Yang, Chenfanfu Jiang | cs.AI | [PDF](http://arxiv.org/pdf/2211.13887v1){: .btn .btn-green } |

**Abstract**: Recent breakthroughs in Vision-Language (V&L) joint research have achieved
remarkable results in various text-driven tasks. High-quality Text-to-video
(T2V), a task that has been long considered mission-impossible, was proven
feasible with reasonably good results in latest works. However, the resulting
videos often have undesired artifacts largely because the system is purely
data-driven and agnostic to the physical laws. To tackle this issue and further
push T2V towards high-level physical realism, we present an autonomous data
generation technique and a dataset, which intend to narrow the gap with a large
number of multi-modal, 3D Text-to-Video/Simulation (T2V/S) data. In the
dataset, we provide high-resolution 3D physical simulations for both solids and
fluids, along with textual descriptions of the physical phenomena. We take
advantage of state-of-the-art physical simulation methods (i) Incremental
Potential Contact (IPC) and (ii) Material Point Method (MPM) to simulate
diverse scenarios, including elastic deformations, material fractures,
collisions, turbulence, etc. Additionally, high-quality, multi-view rendering
videos are supplied for the benefit of T2V, Neural Radiance Fields (NeRF), and
other communities. This work is the first step towards fully automated
Text-to-Video/Simulation (T2V/S). Live examples and subsequent work are at
https://sites.google.com/view/tpa-net.

---

## ScanNeRF: a Scalable Benchmark for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-24 | Luca De Luigi, Damiano Bolognini, Federico Domeniconi, Daniele De Gregorio, Matteo Poggi, Luigi Di Stefano | cs.CV | [PDF](http://arxiv.org/pdf/2211.13762v2){: .btn .btn-green } |

**Abstract**: In this paper, we propose the first-ever real benchmark thought for
evaluating Neural Radiance Fields (NeRFs) and, in general, Neural Rendering
(NR) frameworks. We design and implement an effective pipeline for scanning
real objects in quantity and effortlessly. Our scan station is built with less
than 500$ hardware budget and can collect roughly 4000 images of a scanned
object in just 5 minutes. Such a platform is used to build ScanNeRF, a dataset
characterized by several train/val/test splits aimed at benchmarking the
performance of modern NeRF methods under different conditions. Accordingly, we
evaluate three cutting-edge NeRF variants on it to highlight their strengths
and weaknesses. The dataset is available on our project page, together with an
online benchmark to foster the development of better and better NeRFs.

Comments:
- WACV 2023. The first three authors contributed equally. Project page:
  https://eyecan-ai.github.io/scannerf/

---

## Immersive Neural Graphics Primitives

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-24 | Ke Li, Tim Rolff, Susanne Schmidt, Reinhard Bacher, Simone Frintrop, Wim Leemans, Frank Steinicke | cs.CV | [PDF](http://arxiv.org/pdf/2211.13494v1){: .btn .btn-green } |

**Abstract**: Neural radiance field (NeRF), in particular its extension by instant neural
graphics primitives, is a novel rendering method for view synthesis that uses
real-world images to build photo-realistic immersive virtual scenes. Despite
its potential, research on the combination of NeRF and virtual reality (VR)
remains sparse. Currently, there is no integration into typical VR systems
available, and the performance and suitability of NeRF implementations for VR
have not been evaluated, for instance, for different scene complexities or
screen resolutions. In this paper, we present and evaluate a NeRF-based
framework that is capable of rendering scenes in immersive VR allowing users to
freely move their heads to explore complex real-world scenes. We evaluate our
framework by benchmarking three different NeRF scenes concerning their
rendering performance at different scene complexities and resolutions.
Utilizing super-resolution, our approach can yield a frame rate of 30 frames
per second with a resolution of 1280x720 pixels per eye. We discuss potential
applications of our framework and provide an open source implementation online.

Comments:
- Submitted to IEEE VR, currently under review

---

## BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-23 | Peng Wang, Lingzhe Zhao, Ruijie Ma, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2211.12853v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have received considerable attention recently,
due to its impressive capability in photo-realistic 3D reconstruction and novel
view synthesis, given a set of posed camera images. Earlier work usually
assumes the input images are of good quality. However, image degradation (e.g.
image motion blur in low-light conditions) can easily happen in real-world
scenarios, which would further affect the rendering quality of NeRF. In this
paper, we present a novel bundle adjusted deblur Neural Radiance Fields
(BAD-NeRF), which can be robust to severe motion blurred images and inaccurate
camera poses. Our approach models the physical image formation process of a
motion blurred image, and jointly learns the parameters of NeRF and recovers
the camera motion trajectories during exposure time. In experiments, we show
that by directly modeling the real physical image formation process, BAD-NeRF
achieves superior performance over prior works on both synthetic and real
datasets. Code and data are available at https://github.com/WU-CVGL/BAD-NeRF.

Comments:
- Accepted to CVPR 2023, Project page:
  https://wangpeng000.github.io/BAD-NeRF/

---

## ActiveRMAP: Radiance Field for Active Mapping And Planning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-23 | Huangying Zhan, Jiyang Zheng, Yi Xu, Ian Reid, Hamid Rezatofighi | cs.CV | [PDF](http://arxiv.org/pdf/2211.12656v1){: .btn .btn-green } |

**Abstract**: A high-quality 3D reconstruction of a scene from a collection of 2D images
can be achieved through offline/online mapping methods. In this paper, we
explore active mapping from the perspective of implicit representations, which
have recently produced compelling results in a variety of applications. One of
the most popular implicit representations - Neural Radiance Field (NeRF), first
demonstrated photorealistic rendering results using multi-layer perceptrons,
with promising offline 3D reconstruction as a by-product of the radiance field.
More recently, researchers also applied this implicit representation for online
reconstruction and localization (i.e. implicit SLAM systems). However, the
study on using implicit representation for active vision tasks is still very
limited. In this paper, we are particularly interested in applying the neural
radiance field for active mapping and planning problems, which are closely
coupled tasks in an active system. We, for the first time, present an RGB-only
active vision framework using radiance field representation for active 3D
reconstruction and planning in an online manner. Specifically, we formulate
this joint task as an iterative dual-stage optimization problem, where we
alternatively optimize for the radiance field representation and path planning.
Experimental results suggest that the proposed method achieves competitive
results compared to other offline methods and outperforms active reconstruction
methods using NeRFs.

Comments:
- Under review

---

## PANeRF: Pseudo-view Augmentation for Improved Neural Radiance Fields  Based on Few-shot Inputs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-23 | Young Chun Ahn, Seokhwan Jang, Sungheon Park, Ji-Yeon Kim, Nahyup Kang | cs.CV | [PDF](http://arxiv.org/pdf/2211.12758v1){: .btn .btn-green } |

**Abstract**: The method of neural radiance fields (NeRF) has been developed in recent
years, and this technology has promising applications for synthesizing novel
views of complex scenes. However, NeRF requires dense input views, typically
numbering in the hundreds, for generating high-quality images. With a decrease
in the number of input views, the rendering quality of NeRF for unseen
viewpoints tends to degenerate drastically. To overcome this challenge, we
propose pseudo-view augmentation of NeRF, a scheme that expands a sufficient
amount of data by considering the geometry of few-shot inputs. We first
initialized the NeRF network by leveraging the expanded pseudo-views, which
efficiently minimizes uncertainty when rendering unseen views. Subsequently, we
fine-tuned the network by utilizing sparse-view inputs containing precise
geometry and color information. Through experiments under various settings, we
verified that our model faithfully synthesizes novel-view images of superior
quality and outperforms existing methods for multi-view datasets.

---

## CGOF++: Controllable 3D Face Synthesis with Conditional Generative  Occupancy Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-23 | Keqiang Sun, Shangzhe Wu, Ning Zhang, Zhaoyang Huang, Quan Wang, Hongsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2211.13251v2){: .btn .btn-green } |

**Abstract**: Capitalizing on the recent advances in image generation models, existing
controllable face image synthesis methods are able to generate high-fidelity
images with some levels of controllability, e.g., controlling the shapes,
expressions, textures, and poses of the generated face images. However,
previous methods focus on controllable 2D image generative models, which are
prone to producing inconsistent face images under large expression and pose
changes. In this paper, we propose a new NeRF-based conditional 3D face
synthesis framework, which enables 3D controllability over the generated face
images by imposing explicit 3D conditions from 3D face priors. At its core is a
conditional Generative Occupancy Field (cGOF++) that effectively enforces the
shape of the generated face to conform to a given 3D Morphable Model (3DMM)
mesh, built on top of EG3D [1], a recent tri-plane-based generative model. To
achieve accurate control over fine-grained 3D face shapes of the synthesized
images, we additionally incorporate a 3D landmark loss as well as a volume
warping loss into our synthesis framework. Experiments validate the
effectiveness of the proposed method, which is able to generate high-fidelity
face images and shows more precise 3D controllability than state-of-the-art
2D-based controllable face synthesis methods.

Comments:
- Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). This article is an extension of the NeurIPS'22 paper
  arXiv:2206.08361

---

## AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural  Voxels

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-23 | Yuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen Zhang, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2211.13206v3){: .btn .btn-green } |

**Abstract**: With NeRF widely used for facial reenactment, recent methods can recover
photo-realistic 3D head avatar from just a monocular video. Unfortunately, the
training process of the NeRF-based methods is quite time-consuming, as MLP used
in the NeRF-based methods is inefficient and requires too many iterations to
converge. To overcome this problem, we propose AvatarMAV, a fast 3D head avatar
reconstruction method using Motion-Aware Neural Voxels. AvatarMAV is the first
to model both the canonical appearance and the decoupled expression motion by
neural voxels for head avatar. In particular, the motion-aware neural voxels is
generated from the weighted concatenation of multiple 4D tensors. The 4D
tensors semantically correspond one-to-one with 3DMM expression basis and share
the same weights as 3DMM expression coefficients. Benefiting from our novel
representation, the proposed AvatarMAV can recover photo-realistic head avatars
in just 5 minutes (implemented with pure PyTorch), which is significantly
faster than the state-of-the-art facial reenactment methods. Project page:
https://www.liuyebin.com/avatarmav.

Comments:
- Accepted by SIGGRAPH 2023

---

## ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-23 | Yuan Li, Zhi-Hao Lin, David Forsyth, Jia-Bin Huang, Shenlong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2211.13226v3){: .btn .btn-green } |

**Abstract**: Physical simulations produce excellent predictions of weather effects. Neural
radiance fields produce SOTA scene models. We describe a novel NeRF-editing
procedure that can fuse physical simulations with NeRF models of scenes,
producing realistic movies of physical phenomena in those scenes. Our
application -- Climate NeRF -- allows people to visualize what climate change
outcomes will do to them. ClimateNeRF allows us to render realistic weather
effects, including smog, snow, and flood. Results can be controlled with
physically meaningful variables like water level. Qualitative and quantitative
studies show that our simulated results are significantly more realistic than
those from SOTA 2D image editing and SOTA 3D NeRF stylization.

Comments:
- project page: https://climatenerf.github.io/

---

## ONeRF: Unsupervised 3D Object Segmentation from Multiple Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-22 | Shengnan Liang, Yichen Liu, Shangzhe Wu, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2211.12038v1){: .btn .btn-green } |

**Abstract**: We present ONeRF, a method that automatically segments and reconstructs
object instances in 3D from multi-view RGB images without any additional manual
annotations. The segmented 3D objects are represented using separate Neural
Radiance Fields (NeRFs) which allow for various 3D scene editing and novel view
rendering. At the core of our method is an unsupervised approach using the
iterative Expectation-Maximization algorithm, which effectively aggregates 2D
visual features and the corresponding 3D cues from multi-views for joint 3D
object segmentation and reconstruction. Unlike existing approaches that can
only handle simple objects, our method produces segmented full 3D NeRFs of
individual objects with complex shapes, topologies and appearance. The
segmented ONeRfs enable a range of 3D scene editing, such as object
transformation, insertion and deletion.

---

## DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-22 | Dogyoon Lee, Minhyeok Lee, Chajin Shin, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2211.12046v4){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has exhibited outstanding three-dimensional (3D)
reconstruction quality via the novel view synthesis from multi-view images and
paired calibrated camera parameters. However, previous NeRF-based systems have
been demonstrated under strictly controlled settings, with little attention
paid to less ideal scenarios, including with the presence of noise such as
exposure, illumination changes, and blur. In particular, though blur frequently
occurs in real situations, NeRF that can handle blurred images has received
little attention. The few studies that have investigated NeRF for blurred
images have not considered geometric and appearance consistency in 3D space,
which is one of the most important factors in 3D reconstruction. This leads to
inconsistency and the degradation of the perceptual quality of the constructed
scene. Hence, this paper proposes a DP-NeRF, a novel clean NeRF framework for
blurred images, which is constrained with two physical priors. These priors are
derived from the actual blurring process during image acquisition by the
camera. DP-NeRF proposes rigid blurring kernel to impose 3D consistency
utilizing the physical priors and adaptive weight proposal to refine the color
composition error in consideration of the relationship between depth and blur.
We present extensive experimental results for synthetic and real scenes with
two types of blur: camera motion blur and defocus blur. The results demonstrate
that DP-NeRF successfully improves the perceptual quality of the constructed
NeRF ensuring 3D geometric and appearance consistency. We further demonstrate
the effectiveness of our model with comprehensive ablation analysis.

Comments:
- Accepted at CVPR 2023, Code: https://github.com/dogyoonlee/DP-NeRF,
  Project page: https://dogyoonlee.github.io/dpnerf/

---

## SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-22 | Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstantinos G. Derpanis, Jonathan Kelly, Marcus A. Brubaker, Igor Gilitschenski, Alex Levinshtein | cs.CV | [PDF](http://arxiv.org/pdf/2211.12254v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have emerged as a popular approach for novel
view synthesis. While NeRFs are quickly being adapted for a wider set of
applications, intuitively editing NeRF scenes is still an open challenge. One
important editing task is the removal of unwanted objects from a 3D scene, such
that the replaced region is visually plausible and consistent with its context.
We refer to this task as 3D inpainting. In 3D, solutions must be both
consistent across multiple views and geometrically valid. In this paper, we
propose a novel 3D inpainting method that addresses these challenges. Given a
small set of posed images and sparse annotations in a single input image, our
framework first rapidly obtains a 3D segmentation mask for a target object.
Using the mask, a perceptual optimizationbased approach is then introduced that
leverages learned 2D image inpainters, distilling their information into 3D
space, while ensuring view consistency. We also address the lack of a diverse
benchmark for evaluating 3D scene inpainting methods by introducing a dataset
comprised of challenging real-world scenes. In particular, our dataset contains
views of the same scene with and without a target object, enabling more
principled benchmarking of the 3D inpainting task. We first demonstrate the
superiority of our approach on multiview segmentation, comparing to NeRFbased
methods and 2D segmentation approaches. We then evaluate on the task of 3D
inpainting, establishing state-ofthe-art performance against other NeRF
manipulation algorithms, as well as a strong 2D image inpainter baseline.
Project Page: https://spinnerf3d.github.io

Comments:
- Project Page: https://spinnerf3d.github.io

---

## Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for  Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-22 | Brian K. S. Isaac-Medina, Chris G. Willcocks, Toby P. Breckon | cs.CV | [PDF](http://arxiv.org/pdf/2211.12285v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have attracted significant attention due to
their ability to synthesize novel scene views with great accuracy. However,
inherent to their underlying formulation, the sampling of points along a ray
with zero width may result in ambiguous representations that lead to further
rendering artifacts such as aliasing in the final scene. To address this issue,
the recent variant mip-NeRF proposes an Integrated Positional Encoding (IPE)
based on a conical view frustum. Although this is expressed with an integral
formulation, mip-NeRF instead approximates this integral as the expected value
of a multivariate Gaussian distribution. This approximation is reliable for
short frustums but degrades with highly elongated regions, which arises when
dealing with distant scene objects under a larger depth of field. In this
paper, we explore the use of an exact approach for calculating the IPE by using
a pyramid-based integral formulation instead of an approximated conical-based
one. We denote this formulation as Exact-NeRF and contribute the first approach
to offer a precise analytical solution to the IPE within the NeRF domain. Our
exploratory work illustrates that such an exact formulation Exact-NeRF matches
the accuracy of mip-NeRF and furthermore provides a natural extension to more
challenging scenarios without further modification, such as in the case of
unbounded scenes. Our contribution aims to both address the hitherto unexplored
issues of frustum approximation in earlier NeRF work and additionally provide
insight into the potential future consideration of analytical solutions in
future NeRF extensions.

Comments:
- 15 pages,10 figures

---

## Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial  Decomposition

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-22 | Jiaxiang Tang, Kaisiyuan Wang, Hang Zhou, Xiaokang Chen, Dongliang He, Tianshu Hu, Jingtuo Liu, Gang Zeng, Jingdong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2211.12368v1){: .btn .btn-green } |

**Abstract**: While dynamic Neural Radiance Fields (NeRF) have shown success in
high-fidelity 3D modeling of talking portraits, the slow training and inference
speed severely obstruct their potential usage. In this paper, we propose an
efficient NeRF-based framework that enables real-time synthesizing of talking
portraits and faster convergence by leveraging the recent success of grid-based
NeRF. Our key insight is to decompose the inherently high-dimensional talking
portrait representation into three low-dimensional feature grids. Specifically,
a Decomposed Audio-spatial Encoding Module models the dynamic head with a 3D
spatial grid and a 2D audio grid. The torso is handled with another 2D grid in
a lightweight Pseudo-3D Deformable Module. Both modules focus on efficiency
under the premise of good rendering quality. Extensive experiments demonstrate
that our method can generate realistic and audio-lips synchronized talking
portrait videos, while also being highly efficient compared to previous
methods.

Comments:
- Project page: https://me.kiui.moe/radnerf/

---

## Instant Volumetric Head Avatars



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-22 | Wojciech Zielonka, Timo Bolkart, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2211.12499v2){: .btn .btn-green } |

**Abstract**: We present Instant Volumetric Head Avatars (INSTA), a novel approach for
reconstructing photo-realistic digital avatars instantaneously. INSTA models a
dynamic neural radiance field based on neural graphics primitives embedded
around a parametric face model. Our pipeline is trained on a single monocular
RGB portrait video that observes the subject under different expressions and
views. While state-of-the-art methods take up to several days to train an
avatar, our method can reconstruct a digital avatar in less than 10 minutes on
modern GPU hardware, which is orders of magnitude faster than previous
solutions. In addition, it allows for the interactive rendering of novel poses
and expressions. By leveraging the geometry prior of the underlying parametric
face model, we demonstrate that INSTA extrapolates to unseen poses. In
quantitative and qualitative studies on various subjects, INSTA outperforms
state-of-the-art methods regarding rendering quality and training time.

Comments:
- Website: https://zielon.github.io/insta/ Video:
  https://youtu.be/HOgaeWTih7Q Accepted to CVPR2023

---

## Zero NeRF: Registration with Zero Overlap

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-22 | Casey Peat, Oliver Batchelor, Richard Green, James Atlas | cs.CV | [PDF](http://arxiv.org/pdf/2211.12544v1){: .btn .btn-green } |

**Abstract**: We present Zero-NeRF, a projective surface registration method that, to the
best of our knowledge, offers the first general solution capable of alignment
between scene representations with minimal or zero visual correspondence. To do
this, we enforce consistency between visible surfaces of partial and complete
reconstructions, which allows us to constrain occluded geometry. We use a NeRF
as our surface representation and the NeRF rendering pipeline to perform this
alignment. To demonstrate the efficacy of our method, we register real-world
scenes from opposite sides with infinitesimal overlaps that cannot be
accurately registered using prior methods, and we compare these results against
widely used registration methods.

---

## Dynamic Depth-Supervised NeRF for Multi-View RGB-D Operating Room Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-22 | Beerend G. A. Gerats, Jelmer M. Wolterink, Ivo A. M. J. Broeders | cs.CV | [PDF](http://arxiv.org/pdf/2211.12436v2){: .btn .btn-green } |

**Abstract**: The operating room (OR) is an environment of interest for the development of
sensing systems, enabling the detection of people, objects, and their semantic
relations. Due to frequent occlusions in the OR, these systems often rely on
input from multiple cameras. While increasing the number of cameras generally
increases algorithm performance, there are hard limitations to the number and
locations of cameras in the OR. Neural Radiance Fields (NeRF) can be used to
render synthetic views from arbitrary camera positions, virtually enlarging the
number of cameras in the dataset. In this work, we explore the use of NeRF for
view synthesis of dynamic scenes in the OR, and we show that regularisation
with depth supervision from RGB-D sensor data results in higher image quality.
We optimise a dynamic depth-supervised NeRF with up to six synchronised cameras
that capture the surgical field in five distinct phases before and during a
knee replacement surgery. We qualitatively inspect views rendered by a virtual
camera that moves 180 degrees around the surgical field at differing time
values. Quantitatively, we evaluate view synthesis from an unseen camera
position in terms of PSNR, SSIM and LPIPS for the colour channels and in MAE
and error percentage for the estimated depth. We find that NeRFs can be used to
generate geometrically consistent views, also from interpolated camera
positions and at interpolated time intervals. Views are generated from an
unseen camera pose with an average PSNR of 18.2 and a depth estimation error of
2.0%. Our results show the potential of a dynamic NeRF for view synthesis in
the OR and stress the relevance of depth supervision in a clinical setting.

Comments:
- Accepted to the Workshop on Ambient Intelligence for HealthCare 2023

---

## NeRF-RPN: A general framework for object detection in NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-21 | Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2211.11646v3){: .btn .btn-green } |

**Abstract**: This paper presents the first significant object detection framework,
NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model,
NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting
a novel voxel representation that incorporates multi-scale 3D neural volumetric
features, we demonstrate it is possible to regress the 3D bounding boxes of
objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN
is a general framework and can be applied to detect objects without class
labels. We experimented NeRF-RPN with various backbone architectures, RPN head
designs and loss functions. All of them can be trained in an end-to-end manner
to estimate high quality 3D bounding boxes. To facilitate future research in
object detection for NeRF, we built a new benchmark dataset which consists of
both synthetic and real-world data with careful labeling and clean up. Code and
dataset are available at https://github.com/lyclyc52/NeRF_RPN.

Comments:
- Accepted by CVPR 2023

---

## FLNeRF: 3D Facial Landmarks Estimation in Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-21 | Hao Zhang, Tianyuan Dai, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2211.11202v3){: .btn .btn-green } |

**Abstract**: This paper presents the first significant work on directly predicting 3D face
landmarks on neural radiance fields (NeRFs). Our 3D coarse-to-fine Face
Landmarks NeRF (FLNeRF) model efficiently samples from a given face NeRF with
individual facial features for accurate landmarks detection. Expression
augmentation is applied to facial features in a fine scale to simulate large
emotions range including exaggerated facial expressions (e.g., cheek blowing,
wide opening mouth, eye blinking) for training FLNeRF. Qualitative and
quantitative comparison with related state-of-the-art 3D facial landmark
estimation methods demonstrate the efficacy of FLNeRF, which contributes to
downstream tasks such as high-quality face editing and swapping with direct
control using our NeRF landmarks. Code and data will be available. Github link:
https://github.com/ZHANG1023/FLNeRF.

Comments:
- Hao Zhang and Tianyuan Dai contributed equally. Project website:
  https://github.com/ZHANG1023/FLNeRF

---

## SegNeRF: 3D Part Segmentation with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-21 | Jesus Zarzar, Sara Rojas, Silvio Giancola, Bernard Ghanem | cs.CV | [PDF](http://arxiv.org/pdf/2211.11215v2){: .btn .btn-green } |

**Abstract**: Recent advances in Neural Radiance Fields (NeRF) boast impressive
performances for generative tasks such as novel view synthesis and 3D
reconstruction. Methods based on neural radiance fields are able to represent
the 3D world implicitly by relying exclusively on posed images. Yet, they have
seldom been explored in the realm of discriminative tasks such as 3D part
segmentation. In this work, we attempt to bridge that gap by proposing SegNeRF:
a neural field representation that integrates a semantic field along with the
usual radiance field. SegNeRF inherits from previous works the ability to
perform novel view synthesis and 3D reconstruction, and enables 3D part
segmentation from a few images. Our extensive experiments on PartNet show that
SegNeRF is capable of simultaneously predicting geometry, appearance, and
semantic information from posed images, even for unseen objects. The predicted
semantic fields allow SegNeRF to achieve an average mIoU of $\textbf{30.30%}$
for 2D novel view segmentation, and $\textbf{37.46%}$ for 3D part segmentation,
boasting competitive performance against point-based methods by using only a
few posed images. Additionally, SegNeRF is able to generate an explicit 3D
model from a single image of an object taken in the wild, with its
corresponding part segmentation.

Comments:
- Fixed abstract typo

---

## Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-21 | Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo, Ying Shan, Fei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2211.11505v3){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have achieved photorealistic novel views
synthesis; however, the requirement of accurate camera poses limits its
application. Despite analysis-by-synthesis extensions for jointly learning
neural 3D representations and registering camera frames exist, they are
susceptible to suboptimal solutions if poorly initialized. We propose L2G-NeRF,
a Local-to-Global registration method for bundle-adjusting Neural Radiance
Fields: first, a pixel-wise flexible alignment, followed by a frame-wise
constrained parametric alignment. Pixel-wise local alignment is learned in an
unsupervised way via a deep network which optimizes photometric reconstruction
errors. Frame-wise global alignment is performed using differentiable parameter
estimation solvers on the pixel-wise correspondences to find a global
transformation. Experiments on synthetic and real-world data show that our
method outperforms the current state-of-the-art in terms of high-fidelity
reconstruction and resolving large camera pose misalignment. Our module is an
easy-to-use plugin that can be applied to NeRF variants and other neural field
applications. The Code and supplementary materials are available at
https://rover-xingyu.github.io/L2G-NeRF/.

Comments:
- Accepted to CVPR 2023

---

## Shape, Pose, and Appearance from a Single Image via Bootstrapped  Radiance Field Inversion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-21 | Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2211.11674v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) coupled with GANs represent a promising
direction in the area of 3D reconstruction from a single view, owing to their
ability to efficiently model arbitrary topologies. Recent work in this area,
however, has mostly focused on synthetic datasets where exact ground-truth
poses are known, and has overlooked pose estimation, which is important for
certain downstream applications such as augmented reality (AR) and robotics. We
introduce a principled end-to-end reconstruction framework for natural images,
where accurate ground-truth poses are not available. Our approach recovers an
SDF-parameterized 3D shape, pose, and appearance from a single image of an
object, without exploiting multiple views during training. More specifically,
we leverage an unconditional 3D-aware generator, to which we apply a hybrid
inversion scheme where a model produces a first guess of the solution which is
then refined via optimization. Our framework can de-render an image in as few
as 10 steps, enabling its use in practical scenarios. We demonstrate
state-of-the-art results on a variety of real and synthetic benchmarks.

Comments:
- CVPR 2023. Code and models are available at
  https://github.com/google-research/nerf-from-image

---

## ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of  Signed Distance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-21 | Mohammad Mahdi Johari, Camilla Carta, François Fleuret | cs.CV | [PDF](http://arxiv.org/pdf/2211.11704v2){: .btn .btn-green } |

**Abstract**: We present ESLAM, an efficient implicit neural representation method for
Simultaneous Localization and Mapping (SLAM). ESLAM reads RGB-D frames with
unknown camera poses in a sequential manner and incrementally reconstructs the
scene representation while estimating the current camera position in the scene.
We incorporate the latest advances in Neural Radiance Fields (NeRF) into a SLAM
system, resulting in an efficient and accurate dense visual SLAM method. Our
scene representation consists of multi-scale axis-aligned perpendicular feature
planes and shallow decoders that, for each point in the continuous space,
decode the interpolated features into Truncated Signed Distance Field (TSDF)
and RGB values. Our extensive experiments on three standard datasets, Replica,
ScanNet, and TUM RGB-D show that ESLAM improves the accuracy of 3D
reconstruction and camera localization of state-of-the-art dense visual SLAM
methods by more than 50%, while it runs up to 10 times faster and does not
require any pre-training.

Comments:
- CVPR 2023 Highlight. Project page: https://www.idiap.ch/paper/eslam/

---

## SPARF: Neural Radiance Fields from Sparse and Noisy Poses

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-21 | Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2211.11738v3){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has recently emerged as a powerful
representation to synthesize photorealistic novel views. While showing
impressive performance, it relies on the availability of dense input views with
highly accurate camera poses, thus limiting its application in real-world
scenarios. In this work, we introduce Sparse Pose Adjusting Radiance Field
(SPARF), to address the challenge of novel-view synthesis given only few
wide-baseline input images (as low as 3) with noisy camera poses. Our approach
exploits multi-view geometry constraints in order to jointly learn the NeRF and
refine the camera poses. By relying on pixel matches extracted between the
input views, our multi-view correspondence objective enforces the optimized
scene and camera poses to converge to a global and geometrically accurate
solution. Our depth consistency loss further encourages the reconstructed scene
to be consistent from any viewpoint. Our approach sets a new state of the art
in the sparse-view regime on multiple challenging datasets.

Comments:
- Code is released at https://github.com/google-research/sparf.
  Published at CVPR 2023 as a Highlight

---

## Towards Live 3D Reconstruction from Wearable Video: An Evaluation of  V-SLAM, NeRF, and Videogrammetry Techniques

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-21 | David Ramirez, Suren Jayasuriya, Andreas Spanias | eess.IV | [PDF](http://arxiv.org/pdf/2211.11836v1){: .btn .btn-green } |

**Abstract**: Mixed reality (MR) is a key technology which promises to change the future of
warfare. An MR hybrid of physical outdoor environments and virtual military
training will enable engagements with long distance enemies, both real and
simulated. To enable this technology, a large-scale 3D model of a physical
environment must be maintained based on live sensor observations. 3D
reconstruction algorithms should utilize the low cost and pervasiveness of
video camera sensors, from both overhead and soldier-level perspectives.
Mapping speed and 3D quality can be balanced to enable live MR training in
dynamic environments. Given these requirements, we survey several 3D
reconstruction algorithms for large-scale mapping for military applications
given only live video. We measure 3D reconstruction performance from common
structure from motion, visual-SLAM, and photogrammetry techniques. This
includes the open source algorithms COLMAP, ORB-SLAM3, and NeRF using
Instant-NGP. We utilize the autonomous driving academic benchmark KITTI, which
includes both dashboard camera video and lidar produced 3D ground truth. With
the KITTI data, our primary contribution is a quantitative evaluation of 3D
reconstruction computational speed when considering live video.

Comments:
- Accepted to 2022 Interservice/Industry Training, Simulation, and
  Education Conference (I/ITSEC), 13 pages

---

## DynIBaR: Neural Dynamic Image-Based Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-20 | Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, Noah Snavely | cs.CV | [PDF](http://arxiv.org/pdf/2211.11082v3){: .btn .btn-green } |

**Abstract**: We address the problem of synthesizing novel views from a monocular video
depicting a complex dynamic scene. State-of-the-art methods based on temporally
varying Neural Radiance Fields (aka dynamic NeRFs) have shown impressive
results on this task. However, for long videos with complex object motions and
uncontrolled camera trajectories, these methods can produce blurry or
inaccurate renderings, hampering their use in real-world applications. Instead
of encoding the entire dynamic scene within the weights of MLPs, we present a
new approach that addresses these limitations by adopting a volumetric
image-based rendering framework that synthesizes new viewpoints by aggregating
features from nearby views in a scene-motion-aware manner. Our system retains
the advantages of prior methods in its ability to model complex scenes and
view-dependent effects, but also enables synthesizing photo-realistic novel
views from long videos featuring complex scene dynamics with unconstrained
camera trajectories. We demonstrate significant improvements over
state-of-the-art methods on dynamic scene datasets, and also apply our approach
to in-the-wild videos with challenging camera and object motion, where prior
methods fail to produce high-quality renderings. Our project webpage is at
dynibar.github.io.

Comments:
- Award Candidate, CVPR 2023 Project page: dynibar.github.io

---

## Magic3D: High-Resolution Text-to-3D Content Creation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-18 | Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin | cs.CV | [PDF](http://arxiv.org/pdf/2211.10440v2){: .btn .btn-green } |

**Abstract**: DreamFusion has recently demonstrated the utility of a pre-trained
text-to-image diffusion model to optimize Neural Radiance Fields (NeRF),
achieving remarkable text-to-3D synthesis results. However, the method has two
inherent limitations: (a) extremely slow optimization of NeRF and (b)
low-resolution image space supervision on NeRF, leading to low-quality 3D
models with a long processing time. In this paper, we address these limitations
by utilizing a two-stage optimization framework. First, we obtain a coarse
model using a low-resolution diffusion prior and accelerate with a sparse 3D
hash grid structure. Using the coarse representation as the initialization, we
further optimize a textured 3D mesh model with an efficient differentiable
renderer interacting with a high-resolution latent diffusion model. Our method,
dubbed Magic3D, can create high quality 3D mesh models in 40 minutes, which is
2x faster than DreamFusion (reportedly taking 1.5 hours on average), while also
achieving higher resolution. User studies show 61.7% raters to prefer our
approach over DreamFusion. Together with the image-conditioned generation
capabilities, we provide users with new ways to control 3D synthesis, opening
up new avenues to various creative applications.

Comments:
- Accepted to CVPR 2023 as highlight. Project website:
  https://research.nvidia.com/labs/dir/magic3d

---

## Neural Fields for Fast and Scalable Interpolation of Geophysical Ocean  Variables

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-18 | J. Emmanuel Johnson, Redouane Lguensat, Ronan Fablet, Emmanuel Cosme, Julien Le Sommer | physics.ao-ph | [PDF](http://arxiv.org/pdf/2211.10444v1){: .btn .btn-green } |

**Abstract**: Optimal Interpolation (OI) is a widely used, highly trusted algorithm for
interpolation and reconstruction problems in geosciences. With the influx of
more satellite missions, we have access to more and more observations and it is
becoming more pertinent to take advantage of these observations in applications
such as forecasting and reanalysis. With the increase in the volume of
available data, scalability remains an issue for standard OI and it prevents
many practitioners from effectively and efficiently taking advantage of these
large sums of data to learn the model hyperparameters. In this work, we
leverage recent advances in Neural Fields (NerFs) as an alternative to the OI
framework where we show how they can be easily applied to standard
reconstruction problems in physical oceanography. We illustrate the relevance
of NerFs for gap-filling of sparse measurements of sea surface height (SSH) via
satellite altimetry and demonstrate how NerFs are scalable with comparable
results to the standard OI. We find that NerFs are a practical set of methods
that can be readily applied to geoscience interpolation problems and we
anticipate a wider adoption in the future.

Comments:
- Machine Learning and the Physical Sciences workshop, NeurIPS 2022

---

## AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware  Training

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-17 | Yifan Jiang, Peter Hedman, Ben Mildenhall, Dejia Xu, Jonathan T. Barron, Zhangyang Wang, Tianfan Xue | cs.CV | [PDF](http://arxiv.org/pdf/2211.09682v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) are a powerful representation for modeling a
3D scene as a continuous function. Though NeRF is able to render complex 3D
scenes with view-dependent effects, few efforts have been devoted to exploring
its limits in a high-resolution setting. Specifically, existing NeRF-based
methods face several limitations when reconstructing high-resolution real
scenes, including a very large number of parameters, misaligned input data, and
overly smooth details. In this work, we conduct the first pilot study on
training NeRF with high-resolution data and propose the corresponding
solutions: 1) marrying the multilayer perceptron (MLP) with convolutional
layers which can encode more neighborhood information while reducing the total
number of parameters; 2) a novel training strategy to address misalignment
caused by moving objects or small camera calibration errors; and 3) a
high-frequency aware loss. Our approach is nearly free without introducing
obvious training/testing costs, while experiments on different datasets
demonstrate that it can recover more high-frequency details compared with the
current state-of-the-art NeRF models. Project page:
\url{https://yifanjiang.net/alignerf.}

---

## CoNFies: Controllable Neural Face Avatars

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-16 | Heng Yu, Koichiro Niinuma, Laszlo A. Jeni | cs.CV | [PDF](http://arxiv.org/pdf/2211.08610v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) are compelling techniques for modeling dynamic
3D scenes from 2D image collections. These volumetric representations would be
well suited for synthesizing novel facial expressions but for two problems.
First, deformable NeRFs are object agnostic and model holistic movement of the
scene: they can replay how the motion changes over time, but they cannot alter
it in an interpretable way. Second, controllable volumetric representations
typically require either time-consuming manual annotations or 3D supervision to
provide semantic meaning to the scene. We propose a controllable neural
representation for face self-portraits (CoNFies), that solves both of these
problems within a common framework, and it can rely on automated processing. We
use automated facial action recognition (AFAR) to characterize facial
expressions as a combination of action units (AU) and their intensities. AUs
provide both the semantic locations and control labels for the system. CoNFies
outperformed competing methods for novel view and expression synthesis in terms
of visual and anatomic fidelity of expressions.

Comments:
- accepted by FG2023

---

## NeRFFaceEditing: Disentangled Face Editing in Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-15 | Kaiwen Jiang, Shu-Yu Chen, Feng-Lin Liu, Hongbo Fu, Lin Gao | cs.GR | [PDF](http://arxiv.org/pdf/2211.07968v1){: .btn .btn-green } |

**Abstract**: Recent methods for synthesizing 3D-aware face images have achieved rapid
development thanks to neural radiance fields, allowing for high quality and
fast inference speed. However, existing solutions for editing facial geometry
and appearance independently usually require retraining and are not optimized
for the recent work of generation, thus tending to lag behind the generation
process. To address these issues, we introduce NeRFFaceEditing, which enables
editing and decoupling geometry and appearance in the pretrained
tri-plane-based neural radiance field while retaining its high quality and fast
inference speed. Our key idea for disentanglement is to use the statistics of
the tri-plane to represent the high-level appearance of its corresponding
facial volume. Moreover, we leverage a generated 3D-continuous semantic mask as
an intermediary for geometry editing. We devise a geometry decoder (whose
output is unchanged when the appearance changes) and an appearance decoder. The
geometry decoder aligns the original facial volume with the semantic mask
volume. We also enhance the disentanglement by explicitly regularizing rendered
images with the same appearance but different geometry to be similar in terms
of color distribution for each facial component separately. Our method allows
users to edit via semantic masks with decoupled control of geometry and
appearance. Both qualitative and quantitative evaluations show the superior
geometry and appearance control abilities of our method compared to existing
and alternative solutions.

---

## Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-14 | Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or | cs.CV | [PDF](http://arxiv.org/pdf/2211.07600v1){: .btn .btn-green } |

**Abstract**: Text-guided image generation has progressed rapidly in recent years,
inspiring major breakthroughs in text-guided shape generation. Recently, it has
been shown that using score distillation, one can successfully text-guide a
NeRF model to generate a 3D object. We adapt the score distillation to the
publicly available, and computationally efficient, Latent Diffusion Models,
which apply the entire diffusion process in a compact latent space of a
pretrained autoencoder. As NeRFs operate in image space, a naive solution for
guiding them with latent score distillation would require encoding to the
latent space at each guidance step. Instead, we propose to bring the NeRF to
the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we
show that while Text-to-3D models can generate impressive results, they are
inherently unconstrained and may lack the ability to guide or enforce a
specific 3D structure. To assist and direct the 3D generation, we propose to
guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines
the coarse structure of the desired object. Then, we present means to integrate
such a constraint directly into a Latent-NeRF. This unique combination of text
and shape guidance allows for increased control over the generation process. We
also show that latent score distillation can be successfully applied directly
on 3D meshes. This allows for generating high-quality textures on a given
geometry. Our experiments validate the power of our different forms of guidance
and the efficiency of using latent rendering. Implementation is available at
https://github.com/eladrich/latent-nerf

---

## 3D-Aware Encoding for Style-based Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-12 | Yu-Jhe Li, Tao Xu, Bichen Wu, Ningyuan Zheng, Xiaoliang Dai, Albert Pumarola, Peizhao Zhang, Peter Vajda, Kris Kitani | cs.CV | [PDF](http://arxiv.org/pdf/2211.06583v1){: .btn .btn-green } |

**Abstract**: We tackle the task of NeRF inversion for style-based neural radiance fields,
(e.g., StyleNeRF). In the task, we aim to learn an inversion function to
project an input image to the latent space of a NeRF generator and then
synthesize novel views of the original image based on the latent code. Compared
with GAN inversion for 2D generative models, NeRF inversion not only needs to
1) preserve the identity of the input image, but also 2) ensure 3D consistency
in generated novel views. This requires the latent code obtained from the
single-view image to be invariant across multiple views. To address this new
challenge, we propose a two-stage encoder for style-based NeRF inversion. In
the first stage, we introduce a base encoder that converts the input image to a
latent code. To ensure the latent code is view-invariant and is able to
synthesize 3D consistent novel view images, we utilize identity contrastive
learning to train the base encoder. Second, to better preserve the identity of
the input image, we introduce a refining encoder to refine the latent code and
add finer details to the output image. Importantly note that the novelty of
this model lies in the design of its first-stage encoder which produces the
closest latent code lying on the latent manifold and thus the refinement in the
second stage would be close to the NeRF manifold. Through extensive
experiments, we demonstrate that our proposed two-stage encoder qualitatively
and quantitatively exhibits superiority over the existing encoders for
inversion in both image reconstruction and novel-view rendering.

Comments:
- 21 pages (under review)

---

## ParticleNeRF: A Particle-Based Encoding for Online Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-08 | Jad Abou-Chakra, Feras Dayoub, Niko Sünderhauf | cs.CV | [PDF](http://arxiv.org/pdf/2211.04041v4){: .btn .btn-green } |

**Abstract**: While existing Neural Radiance Fields (NeRFs) for dynamic scenes are offline
methods with an emphasis on visual fidelity, our paper addresses the online use
case that prioritises real-time adaptability. We present ParticleNeRF, a new
approach that dynamically adapts to changes in the scene geometry by learning
an up-to-date representation online, every 200ms. ParticleNeRF achieves this
using a novel particle-based parametric encoding. We couple features to
particles in space and backpropagate the photometric reconstruction loss into
the particles' position gradients, which are then interpreted as velocity
vectors. Governed by a lightweight physics system to handle collisions, this
lets the features move freely with the changing scene geometry. We demonstrate
ParticleNeRF on various dynamic scenes containing translating, rotating,
articulated, and deformable objects. ParticleNeRF is the first online dynamic
NeRF and achieves fast adaptability with better visual fidelity than
brute-force online InstantNGP and other baseline approaches on dynamic scenes
with online constraints. Videos of our system can be found at our project
website https://sites.google.com/view/particlenerf.

---

## Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable  Categories

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-07 | Samarth Sinha, Roman Shapovalov, Jeremy Reizenstein, Ignacio Rocco, Natalia Neverova, Andrea Vedaldi, David Novotny | cs.CV | [PDF](http://arxiv.org/pdf/2211.03889v1){: .btn .btn-green } |

**Abstract**: Obtaining photorealistic reconstructions of objects from sparse views is
inherently ambiguous and can only be achieved by learning suitable
reconstruction priors. Earlier works on sparse rigid object reconstruction
successfully learned such priors from large datasets such as CO3D. In this
paper, we extend this approach to dynamic objects. We use cats and dogs as a
representative example and introduce Common Pets in 3D (CoP3D), a collection of
crowd-sourced videos showing around 4,200 distinct pets. CoP3D is one of the
first large-scale datasets for benchmarking non-rigid 3D reconstruction "in the
wild". We also propose Tracker-NeRF, a method for learning 4D reconstruction
from our dataset. At test time, given a small number of video frames of an
unseen object, Tracker-NeRF predicts the trajectories of its 3D points and
generates new views, interpolating viewpoint and time. Results on CoP3D reveal
significantly better non-rigid new-view synthesis performance than existing
baselines.

---

## Learning-based Inverse Rendering of Complex Indoor Scenes with  Differentiable Monte Carlo Raytracing



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-06 | Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua Zhong, Dianbing Xi, Jiaxiang Zheng, Rui Tang, Hujun Bao, Rui Wang | cs.CV | [PDF](http://arxiv.org/pdf/2211.03017v2){: .btn .btn-green } |

**Abstract**: Indoor scenes typically exhibit complex, spatially-varying appearance from
global illumination, making inverse rendering a challenging ill-posed problem.
This work presents an end-to-end, learning-based inverse rendering framework
incorporating differentiable Monte Carlo raytracing with importance sampling.
The framework takes a single image as input to jointly recover the underlying
geometry, spatially-varying lighting, and photorealistic materials.
Specifically, we introduce a physically-based differentiable rendering layer
with screen-space ray tracing, resulting in more realistic specular reflections
that match the input photo. In addition, we create a large-scale,
photorealistic indoor scene dataset with significantly richer details like
complex furniture and dedicated decorations. Further, we design a novel
out-of-view lighting network with uncertainty-aware refinement leveraging
hypernetwork-based neural radiance fields to predict lighting outside the view
of the input photo. Through extensive evaluations on common benchmark datasets,
we demonstrate superior inverse rendering quality of our method compared to
state-of-the-art baselines, enabling various applications such as complex
object insertion and material editing with high fidelity. Code and data will be
made available at \url{https://jingsenzhu.github.io/invrend}.

---

## nerf2nerf: Pairwise Registration of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-11-03 | Lily Goli, Daniel Rebain, Sara Sabour, Animesh Garg, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2211.01600v1){: .btn .btn-green } |

**Abstract**: We introduce a technique for pairwise registration of neural fields that
extends classical optimization-based local registration (i.e. ICP) to operate
on Neural Radiance Fields (NeRF) -- neural 3D scene representations trained
from collections of calibrated images. NeRF does not decompose illumination and
color, so to make registration invariant to illumination, we introduce the
concept of a ''surface field'' -- a field distilled from a pre-trained NeRF
model that measures the likelihood of a point being on the surface of an
object. We then cast nerf2nerf registration as a robust optimization that
iteratively seeks a rigid transformation that aligns the surface fields of the
two scenes. We evaluate the effectiveness of our technique by introducing a
dataset of pre-trained NeRF scenes -- our synthetic scenes enable quantitative
evaluations and comparisons to classical registration techniques, while our
real scenes demonstrate the validity of our technique in real-world scenarios.
Additional results available at: https://nerf2nerf.github.io

---

## Mixed Reality Interface for Digital Twin of Plant Factory



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-29 | Byunghyun Ban | cs.HC | [PDF](http://arxiv.org/pdf/2211.00597v1){: .btn .btn-green } |

**Abstract**: An easier and intuitive interface architecture is necessary for digital twin
of plant factory. I suggest an immersive and interactive mixed reality
interface for digital twin models of smart farming, for remote work rather than
simulation of components. The environment is constructed with UI display and a
streaming background scene, which is a real time scene taken from camera device
located in the plant factory, processed with deformable neural radiance fields.
User can monitor and control the remote plant factory facilities with HMD or 2D
display based mixed reality environment. This paper also introduces detailed
concept and describes the system architecture to implement suggested mixed
reality interface.

Comments:
- 5 pages, 7 figures

---

## NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed  Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-28 | Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, Andreas Geiger | cs.CV | [PDF](http://arxiv.org/pdf/2210.15947v2){: .btn .btn-green } |

**Abstract**: Visually exploring in a real-world 4D spatiotemporal space freely in VR has
been a long-term quest. The task is especially appealing when only a few or
even single RGB cameras are used for capturing the dynamic scene. To this end,
we present an efficient framework capable of fast reconstruction, compact
modeling, and streamable rendering. First, we propose to decompose the 4D
spatiotemporal space according to temporal characteristics. Points in the 4D
space are associated with probabilities of belonging to three categories:
static, deforming, and new areas. Each area is represented and regularized by a
separate neural field. Second, we propose a hybrid representations based
feature streaming scheme for efficiently modeling the neural fields. Our
approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single
hand-held cameras and multi-camera arrays, achieving comparable or superior
rendering performance in terms of quality and speed comparable to recent
state-of-the-art methods, achieving reconstruction in 10 seconds per frame and
interactive rendering.

Comments:
- Project page: https://lsongx.github.io/projects/nerfplayer.html

---

## ProbNeRF: Uncertainty-Aware Inference of 3D Shapes from 2D Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-27 | Matthew D. Hoffman, Tuan Anh Le, Pavel Sountsov, Christopher Suter, Ben Lee, Vikash K. Mansinghka, Rif A. Saurous | cs.CV | [PDF](http://arxiv.org/pdf/2210.17415v1){: .btn .btn-green } |

**Abstract**: The problem of inferring object shape from a single 2D image is
underconstrained. Prior knowledge about what objects are plausible can help,
but even given such prior knowledge there may still be uncertainty about the
shapes of occluded parts of objects. Recently, conditional neural radiance
field (NeRF) models have been developed that can learn to infer good point
estimates of 3D models from single 2D images. The problem of inferring
uncertainty estimates for these models has received less attention. In this
work, we propose probabilistic NeRF (ProbNeRF), a model and inference strategy
for learning probabilistic generative models of 3D objects' shapes and
appearances, and for doing posterior inference to recover those properties from
2D images. ProbNeRF is trained as a variational autoencoder, but at test time
we use Hamiltonian Monte Carlo (HMC) for inference. Given one or a few 2D
images of an object (which may be partially occluded), ProbNeRF is able not
only to accurately model the parts it sees, but also to propose realistic and
diverse hypotheses about the parts it does not see. We show that key to the
success of ProbNeRF are (i) a deterministic rendering scheme, (ii) an
annealed-HMC strategy, (iii) a hypernetwork-based decoder architecture, and
(iv) doing inference over a full set of NeRF weights, rather than just a
low-dimensional code.

Comments:
- 18 pages, 18 figures, 1 table; submitted to the 26th International
  Conference on Artificial Intelligence and Statistics (AISTATS 2023)

---

## Boosting Point Clouds Rendering via Radiance Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-27 | Xiaoyang Huang, Yi Zhang, Bingbing Ni, Teng Li, Kai Chen, Wenjun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2210.15107v2){: .btn .btn-green } |

**Abstract**: Recent years we have witnessed rapid development in NeRF-based image
rendering due to its high quality. However, point clouds rendering is somehow
less explored. Compared to NeRF-based rendering which suffers from dense
spatial sampling, point clouds rendering is naturally less computation
intensive, which enables its deployment in mobile computing device. In this
work, we focus on boosting the image quality of point clouds rendering with a
compact model design. We first analyze the adaption of the volume rendering
formulation on point clouds. Based on the analysis, we simplify the NeRF
representation to a spatial mapping function which only requires single
evaluation per pixel. Further, motivated by ray marching, we rectify the the
noisy raw point clouds to the estimated intersection between rays and surfaces
as queried coordinates, which could avoid \textit{spatial frequency collapse}
and neighbor point disturbance. Composed of rasterization, spatial mapping and
the refinement stages, our method achieves the state-of-the-art performance on
point clouds rendering, outperforming prior works by notable margins, with a
smaller model size. We obtain a PSNR of 31.74 on NeRF-Synthetic, 25.88 on
ScanNet and 30.81 on DTU. Code and data are publicly available at
https://github.com/seanywang0408/RadianceMapping.

Comments:
- Accepted by Thirty-Seventh AAAI Conference on Artificial Intelligence
  (AAAI 2023)

---

## Learning Neural Radiance Fields from Multi-View Geometry

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-24 | Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi | cs.CV | [PDF](http://arxiv.org/pdf/2210.13041v1){: .btn .btn-green } |

**Abstract**: We present a framework, called MVG-NeRF, that combines classical Multi-View
Geometry algorithms and Neural Radiance Fields (NeRF) for image-based 3D
reconstruction. NeRF has revolutionized the field of implicit 3D
representations, mainly due to a differentiable volumetric rendering
formulation that enables high-quality and geometry-aware novel view synthesis.
However, the underlying geometry of the scene is not explicitly constrained
during training, thus leading to noisy and incorrect results when extracting a
mesh with marching cubes. To this end, we propose to leverage pixelwise depths
and normals from a classical 3D reconstruction pipeline as geometric priors to
guide NeRF optimization. Such priors are used as pseudo-ground truth during
training in order to improve the quality of the estimated underlying surface.
Moreover, each pixel is weighted by a confidence value based on the
forward-backward reprojection error for additional robustness. Experimental
results on real-world data demonstrate the effectiveness of this approach in
obtaining clean 3D meshes from images, while maintaining competitive
performances in novel view synthesis.

Comments:
- ECCV 2022 Workshop on "Learning to Generate 3D Shapes and Scenes"

---

## NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-24 | Antoni Rosinol, John J. Leonard, Luca Carlone | cs.CV | [PDF](http://arxiv.org/pdf/2210.13641v1){: .btn .btn-green } |

**Abstract**: We propose a novel geometric and photometric 3D mapping pipeline for accurate
and real-time scene reconstruction from monocular images. To achieve this, we
leverage recent advances in dense monocular SLAM and real-time hierarchical
volumetric neural radiance fields. Our insight is that dense monocular SLAM
provides the right information to fit a neural radiance field of the scene in
real-time, by providing accurate pose estimates and depth-maps with associated
uncertainty. With our proposed uncertainty-based depth loss, we achieve not
only good photometric accuracy, but also great geometric accuracy. In fact, our
proposed pipeline achieves better geometric and photometric accuracy than
competing approaches (up to 179% better PSNR and 86% better L1 depth), while
working in real-time and using only monocular images.

Comments:
- 10 pages, 6 figures

---

## Compressing Explicit Voxel Grid Representations: fast NeRFs become also  small

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-23 | Chenxi Lola Deng, Enzo Tartaglione | cs.CV | [PDF](http://arxiv.org/pdf/2210.12782v1){: .btn .btn-green } |

**Abstract**: NeRFs have revolutionized the world of per-scene radiance field
reconstruction because of their intrinsic compactness. One of the main
limitations of NeRFs is their slow rendering speed, both at training and
inference time. Recent research focuses on the optimization of an explicit
voxel grid (EVG) that represents the scene, which can be paired with neural
networks to learn radiance fields. This approach significantly enhances the
speed both at train and inference time, but at the cost of large memory
occupation. In this work we propose Re:NeRF, an approach that specifically
targets EVG-NeRFs compressibility, aiming to reduce memory storage of NeRF
models while maintaining comparable performance. We benchmark our approach with
three different EVG-NeRF architectures on four popular benchmarks, showing
Re:NeRF's broad usability and effectiveness.

---

## Joint Rigid Motion Correction and Sparse-View CT via Self-Calibrating  Neural Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-23 | Qing Wu, Xin Li, Hongjiang Wei, Jingyi Yu, Yuyao Zhang | eess.IV | [PDF](http://arxiv.org/pdf/2210.12731v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has widely received attention in Sparse-View
Computed Tomography (SVCT) reconstruction tasks as a self-supervised deep
learning framework. NeRF-based SVCT methods represent the desired CT image as a
continuous function of spatial coordinates and train a Multi-Layer Perceptron
(MLP) to learn the function by minimizing loss on the SV sinogram. Benefiting
from the continuous representation provided by NeRF, the high-quality CT image
can be reconstructed. However, existing NeRF-based SVCT methods strictly
suppose there is completely no relative motion during the CT acquisition
because they require \textit{accurate} projection poses to model the X-rays
that scan the SV sinogram. Therefore, these methods suffer from severe
performance drops for real SVCT imaging with motion. In this work, we propose a
self-calibrating neural field to recover the artifacts-free image from the
rigid motion-corrupted SV sinogram without using any external data.
Specifically, we parametrize the inaccurate projection poses caused by rigid
motion as trainable variables and then jointly optimize these pose variables
and the MLP. We conduct numerical experiments on a public CT image dataset. The
results indicate our model significantly outperforms two representative
NeRF-based methods for SVCT reconstruction tasks with four different levels of
rigid motion.

Comments:
- 5 pages

---

## An Exploration of Neural Radiance Field Scene Reconstruction: Synthetic,  Real-world and Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-21 | Benedict Quartey, Tuluhan Akbulut, Wasiwasi Mgonzo, Zheng Xin Yong | cs.CV | [PDF](http://arxiv.org/pdf/2210.12268v1){: .btn .btn-green } |

**Abstract**: This project presents an exploration into 3D scene reconstruction of
synthetic and real-world scenes using Neural Radiance Field (NeRF) approaches.
We primarily take advantage of the reduction in training and rendering time of
neural graphic primitives multi-resolution hash encoding, to reconstruct static
video game scenes and real-world scenes, comparing and observing reconstruction
detail and limitations. Additionally, we explore dynamic scene reconstruction
using Neural Radiance Fields for Dynamic Scenes(D-NeRF). Finally, we extend the
implementation of D-NeRF, originally constrained to handle synthetic scenes to
also handle real-world dynamic scenes.

---

## One-Shot Neural Fields for 3D Object Understanding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-21 | Valts Blukis, Taeyeop Lee, Jonathan Tremblay, Bowen Wen, In So Kweon, Kuk-Jin Yoon, Dieter Fox, Stan Birchfield | cs.RO | [PDF](http://arxiv.org/pdf/2210.12126v3){: .btn .btn-green } |

**Abstract**: We present a unified and compact scene representation for robotics, where
each object in the scene is depicted by a latent code capturing geometry and
appearance. This representation can be decoded for various tasks such as novel
view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or
voxel maps), collision checking, and stable grasp prediction. We build our
representation from a single RGB input image at test time by leveraging recent
advances in Neural Radiance Fields (NeRF) that learn category-level priors on
large multiview datasets, then fine-tune on novel objects from one or few
views. We expand the NeRF model for additional grasp outputs and explore ways
to leverage this representation for robotics. At test-time, we build the
representation from a single RGB input image observing the scene from only one
viewpoint. We find that the recovered representation allows rendering from
novel views, including of occluded object parts, and also for predicting
successful stable grasps. Grasp poses can be directly decoded from our latent
representation with an implicit grasp decoder. We experimented in both
simulation and real world and demonstrated the capability for robust robotic
grasping using such compact representation. Website:
https://nerfgrasp.github.io

Comments:
- IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshop (CVPRW) on XRNeRF: Advances in NeRF for the Metaverse 2023

---

## HDHumans: A Hybrid Approach for High-fidelity Digital Humans

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-21 | Marc Habermann, Lingjie Liu, Weipeng Xu, Gerard Pons-Moll, Michael Zollhoefer, Christian Theobalt | cs.CV | [PDF](http://arxiv.org/pdf/2210.12003v2){: .btn .btn-green } |

**Abstract**: Photo-real digital human avatars are of enormous importance in graphics, as
they enable immersive communication over the globe, improve gaming and
entertainment experiences, and can be particularly beneficial for AR and VR
settings. However, current avatar generation approaches either fall short in
high-fidelity novel view synthesis, generalization to novel motions,
reproduction of loose clothing, or they cannot render characters at the high
resolution offered by modern displays. To this end, we propose HDHumans, which
is the first method for HD human character synthesis that jointly produces an
accurate and temporally coherent 3D deforming surface and highly
photo-realistic images of arbitrary novel views and of motions not seen at
training time. At the technical core, our method tightly integrates a classical
deforming character template with neural radiance fields (NeRF). Our method is
carefully designed to achieve a synergy between classical surface deformation
and NeRF. First, the template guides the NeRF, which allows synthesizing novel
views of a highly dynamic and articulated character and even enables the
synthesis of novel motions. Second, we also leverage the dense pointclouds
resulting from NeRF to further improve the deforming surface via 3D-to-3D
supervision. We outperform the state of the art quantitatively and
qualitatively in terms of synthesis quality and resolution, as well as the
quality of 3D surface reconstruction.

---

## RGB-Only Reconstruction of Tabletop Scenes for Collision-Free  Manipulator Control

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-21 | Zhenggang Tang, Balakumar Sundaralingam, Jonathan Tremblay, Bowen Wen, Ye Yuan, Stephen Tyree, Charles Loop, Alexander Schwing, Stan Birchfield | cs.RO | [PDF](http://arxiv.org/pdf/2210.11668v2){: .btn .btn-green } |

**Abstract**: We present a system for collision-free control of a robot manipulator that
uses only RGB views of the world. Perceptual input of a tabletop scene is
provided by multiple images of an RGB camera (without depth) that is either
handheld or mounted on the robot end effector. A NeRF-like process is used to
reconstruct the 3D geometry of the scene, from which the Euclidean full signed
distance function (ESDF) is computed. A model predictive control algorithm is
then used to control the manipulator to reach a desired pose while avoiding
obstacles in the ESDF. We show results on a real dataset collected and
annotated in our lab.

Comments:
- ICRA 2023. Project page at https://ngp-mpc.github.io/

---

## Coordinates Are NOT Lonely -- Codebook Prior Helps Implicit Neural 3D  Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-20 | Fukun Yin, Wen Liu, Zilong Huang, Pei Cheng, Tao Chen, Gang YU | cs.CV | [PDF](http://arxiv.org/pdf/2210.11170v2){: .btn .btn-green } |

**Abstract**: Implicit neural 3D representation has achieved impressive results in surface
or scene reconstruction and novel view synthesis, which typically uses the
coordinate-based multi-layer perceptrons (MLPs) to learn a continuous scene
representation. However, existing approaches, such as Neural Radiance Field
(NeRF) and its variants, usually require dense input views (i.e. 50-150) to
obtain decent results. To relive the over-dependence on massive calibrated
images and enrich the coordinate-based feature representation, we explore
injecting the prior information into the coordinate-based network and introduce
a novel coordinate-based model, CoCo-INR, for implicit neural 3D
representation. The cores of our method are two attention modules: codebook
attention and coordinate attention. The former extracts the useful prototypes
containing rich geometry and appearance information from the prior codebook,
and the latter propagates such prior information into each coordinate and
enriches its feature representation for a scene or object surface. With the
help of the prior information, our method can render 3D views with more
photo-realistic appearance and geometries than the current methods using fewer
calibrated images available. Experiments on various scene reconstruction
datasets, including DTU and BlendedMVS, and the full 3D head reconstruction
dataset, H3DS, demonstrate the robustness under fewer input views and fine
detail-preserving capability of our proposed method.

Comments:
- NeurIPS 2022

---

## ARAH: Animatable Volume Rendering of Articulated Human SDFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-18 | Shaofei Wang, Katja Schwarz, Andreas Geiger, Siyu Tang | cs.CV | [PDF](http://arxiv.org/pdf/2210.10036v1){: .btn .btn-green } |

**Abstract**: Combining human body models with differentiable rendering has recently
enabled animatable avatars of clothed humans from sparse sets of multi-view RGB
videos. While state-of-the-art approaches achieve realistic appearance with
neural radiance fields (NeRF), the inferred geometry often lacks detail due to
missing geometric constraints. Further, animating avatars in
out-of-distribution poses is not yet possible because the mapping from
observation space to canonical space does not generalize faithfully to unseen
poses. In this work, we address these shortcomings and propose a model to
create animatable clothed human avatars with detailed geometry that generalize
well to out-of-distribution poses. To achieve detailed geometry, we combine an
articulated implicit surface representation with volume rendering. For
generalization, we propose a novel joint root-finding algorithm for
simultaneous ray-surface intersection search and correspondence search. Our
algorithm enables efficient point sampling and accurate point canonicalization
while generalizing well to unseen poses. We demonstrate that our proposed
pipeline can generate clothed avatars with high-quality pose-dependent geometry
and appearance from a sparse set of multi-view RGB videos. Our method achieves
state-of-the-art performance on geometry and appearance reconstruction while
creating animatable avatars that generalize well to out-of-distribution poses
beyond the small number of training poses.

Comments:
- Accepted to ECCV 2022. Project page:
  https://neuralbodies.github.io/arah/

---

## Parallel Inversion of Neural Radiance Fields for Robust Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-18 | Yunzhi Lin, Thomas Müller, Jonathan Tremblay, Bowen Wen, Stephen Tyree, Alex Evans, Patricio A. Vela, Stan Birchfield | cs.CV | [PDF](http://arxiv.org/pdf/2210.10108v2){: .btn .btn-green } |

**Abstract**: We present a parallelized optimization method based on fast Neural Radiance
Fields (NeRF) for estimating 6-DoF pose of a camera with respect to an object
or scene. Given a single observed RGB image of the target, we can predict the
translation and rotation of the camera by minimizing the residual between
pixels rendered from a fast NeRF model and pixels in the observed image. We
integrate a momentum-based camera extrinsic optimization procedure into Instant
Neural Graphics Primitives, a recent exceptionally fast NeRF implementation. By
introducing parallel Monte Carlo sampling into the pose estimation task, our
method overcomes local minima and improves efficiency in a more extensive
search space. We also show the importance of adopting a more robust pixel-based
loss function to reduce error. Experiments demonstrate that our method can
achieve improved generalization and robustness on both synthetic and real-world
benchmarks.

Comments:
- ICRA 2023. Project page at https://pnerfp.github.io/

---

## Differentiable Physics Simulation of Dynamics-Augmented Neural Objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-17 | Simon Le Cleac'h, Hong-Xing Yu, Michelle Guo, Taylor A. Howell, Ruohan Gao, Jiajun Wu, Zachary Manchester, Mac Schwager | cs.RO | [PDF](http://arxiv.org/pdf/2210.09420v3){: .btn .btn-green } |

**Abstract**: We present a differentiable pipeline for simulating the motion of objects
that represent their geometry as a continuous density field parameterized as a
deep network. This includes Neural Radiance Fields (NeRFs), and other related
models. From the density field, we estimate the dynamical properties of the
object, including its mass, center of mass, and inertia matrix. We then
introduce a differentiable contact model based on the density field for
computing normal and friction forces resulting from collisions. This allows a
robot to autonomously build object models that are visually and
\emph{dynamically} accurate from still images and videos of objects in motion.
The resulting Dynamics-Augmented Neural Objects (DANOs) are simulated with an
existing differentiable simulation engine, Dojo, interacting with other
standard simulation objects, such as spheres, planes, and robots specified as
URDFs. A robot can use this simulation to optimize grasps and manipulation
trajectories of neural objects, or to improve the neural object models through
gradient-based real-to-simulation transfer. We demonstrate the pipeline to
learn the coefficient of friction of a bar of soap from a real video of the
soap sliding on a table. We also learn the coefficient of friction and mass of
a Stanford bunny through interactions with a Panda robot arm from synthetic
data, and we optimize trajectories in simulation for the Panda arm to push the
bunny to a goal location.

---

## SPIDR: SDF-based Neural Point Fields for Illumination and Deformation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-15 | Ruofan Liang, Jiahao Zhang, Haoda Li, Chen Yang, Yushi Guan, Nandita Vijaykumar | cs.CV | [PDF](http://arxiv.org/pdf/2210.08398v3){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) have recently emerged as a promising approach
for 3D reconstruction and novel view synthesis. However, NeRF-based methods
encode shape, reflectance, and illumination implicitly and this makes it
challenging for users to manipulate these properties in the rendered images
explicitly. Existing approaches only enable limited editing of the scene and
deformation of the geometry. Furthermore, no existing work enables accurate
scene illumination after object deformation. In this work, we introduce SPIDR,
a new hybrid neural SDF representation. SPIDR combines point cloud and neural
implicit representations to enable the reconstruction of higher quality object
surfaces for geometry deformation and lighting estimation. meshes and surfaces
for object deformation and lighting estimation. To more accurately capture
environment illumination for scene relighting, we propose a novel neural
implicit model to learn environment light. To enable more accurate illumination
updates after deformation, we use the shadow mapping technique to approximate
the light visibility updates caused by geometry editing. We demonstrate the
effectiveness of SPIDR in enabling high quality geometry editing with more
accurate updates to the illumination of the scene.

Comments:
- Project page: https://nexuslrf.github.io/SPIDR_webpage/

---

## IBL-NeRF: Image-Based Lighting Formulation of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-15 | Changwoon Choi, Juhyeon Kim, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2210.08202v2){: .btn .btn-green } |

**Abstract**: We propose IBL-NeRF, which decomposes the neural radiance fields (NeRF) of
large-scale indoor scenes into intrinsic components. Recent approaches further
decompose the baked radiance of the implicit volume into intrinsic components
such that one can partially approximate the rendering equation. However, they
are limited to representing isolated objects with a shared environment
lighting, and suffer from computational burden to aggregate rays with Monte
Carlo integration. In contrast, our prefiltered radiance field extends the
original NeRF formulation to capture the spatial variation of lighting within
the scene volume, in addition to surface properties. Specifically, the scenes
of diverse materials are decomposed into intrinsic components for rendering,
namely, albedo, roughness, surface normal, irradiance, and prefiltered
radiance. All of the components are inferred as neural images from MLP, which
can model large-scale general scenes. Especially the prefiltered radiance
effectively models the volumetric light field, and captures spatial variation
beyond a single environment light. The prefiltering aggregates rays in a set of
predefined neighborhood sizes such that we can replace the costly Monte Carlo
integration of global illumination with a simple query from a neural image. By
adopting NeRF, our approach inherits superior visual quality and multi-view
consistency for synthesized images as well as the intrinsic components. We
demonstrate the performance on scenes with complex object layouts and light
configurations, which could not be processed in any of the previous works.

Comments:
- Computer Graphics Forum (Pacific Graphics 2023)

---

## 3D GAN Inversion with Pose Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-13 | Jaehoon Ko, Kyusun Cho, Daewon Choi, Kwangrok Ryoo, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2210.07301v2){: .btn .btn-green } |

**Abstract**: With the recent advances in NeRF-based 3D aware GANs quality, projecting an
image into the latent space of these 3D-aware GANs has a natural advantage over
2D GAN inversion: not only does it allow multi-view consistent editing of the
projected image, but it also enables 3D reconstruction and novel view synthesis
when given only a single image. However, the explicit viewpoint control acts as
a main hindrance in the 3D GAN inversion process, as both camera pose and
latent code have to be optimized simultaneously to reconstruct the given image.
Most works that explore the latent space of the 3D-aware GANs rely on
ground-truth camera viewpoint or deformable 3D model, thus limiting their
applicability. In this work, we introduce a generalizable 3D GAN inversion
method that infers camera viewpoint and latent code simultaneously to enable
multi-view consistent semantic image editing. The key to our approach is to
leverage pre-trained estimators for better initialization and utilize the
pixel-wise depth calculated from NeRF parameters to better reconstruct the
given image. We conduct extensive experiments on image reconstruction and
editing both quantitatively and qualitatively, and further compare our results
with 2D GAN-based editing to demonstrate the advantages of utilizing the latent
space of 3D GANs. Additional results and visualizations are available at
https://3dgan-inversion.github.io .

Comments:
- Project Page: https://3dgan-inversion.github.io

---

## MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without  Camera Pose

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-13 | Yang Fu, Ishan Misra, Xiaolong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2210.07181v2){: .btn .btn-green } |

**Abstract**: We propose a generalizable neural radiance fields - MonoNeRF, that can be
trained on large-scale monocular videos of moving in static scenes without any
ground-truth annotations of depth and camera poses. MonoNeRF follows an
Autoencoder-based architecture, where the encoder estimates the monocular depth
and the camera pose, and the decoder constructs a Multiplane NeRF
representation based on the depth encoder feature, and renders the input frames
with the estimated camera. The learning is supervised by the reconstruction
error. Once the model is learned, it can be applied to multiple applications
including depth estimation, camera pose estimation, and single-image novel view
synthesis. More qualitative results are available at:
https://oasisyang.github.io/mononerf .

Comments:
- ICML 2023 camera ready version. Project page:
  https://oasisyang.github.io/mononerf

---

## GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and  Specular Objects Using Generalizable NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-12 | Qiyu Dai, Yan Zhu, Yiran Geng, Ciyu Ruan, Jiazhao Zhang, He Wang | cs.RO | [PDF](http://arxiv.org/pdf/2210.06575v3){: .btn .btn-green } |

**Abstract**: In this work, we tackle 6-DoF grasp detection for transparent and specular
objects, which is an important yet challenging problem in vision-based robotic
systems, due to the failure of depth cameras in sensing their geometry. We, for
the first time, propose a multiview RGB-based 6-DoF grasp detection network,
GraspNeRF, that leverages the generalizable neural radiance field (NeRF) to
achieve material-agnostic object grasping in clutter. Compared to the existing
NeRF-based 3-DoF grasp detection methods that rely on densely captured input
images and time-consuming per-scene optimization, our system can perform
zero-shot NeRF construction with sparse RGB inputs and reliably detect 6-DoF
grasps, both in real-time. The proposed framework jointly learns generalizable
NeRF and grasp detection in an end-to-end manner, optimizing the scene
representation construction for the grasping. For training data, we generate a
large-scale photorealistic domain-randomized synthetic dataset of grasping in
cluttered tabletop scenes that enables direct transfer to the real world. Our
extensive experiments in synthetic and real-world environments demonstrate that
our method significantly outperforms all the baselines in all the experiments
while remaining in real-time. Project page can be found at
https://pku-epic.github.io/GraspNeRF

Comments:
- IEEE International Conference on Robotics and Automation (ICRA), 2023

---

## Reconstructing Personalized Semantic Facial NeRF Models From Monocular  Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-12 | Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, Juyong Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2210.06108v1){: .btn .btn-green } |

**Abstract**: We present a novel semantic model for human head defined with neural radiance
field. The 3D-consistent head model consist of a set of disentangled and
interpretable bases, and can be driven by low-dimensional expression
coefficients. Thanks to the powerful representation ability of neural radiance
field, the constructed model can represent complex facial attributes including
hair, wearings, which can not be represented by traditional mesh blendshape. To
construct the personalized semantic facial model, we propose to define the
bases as several multi-level voxel fields. With a short monocular RGB video as
input, our method can construct the subject's semantic facial NeRF model with
only ten to twenty minutes, and can render a photo-realistic human head image
in tens of miliseconds with a given expression coefficient and view direction.
With this novel representation, we apply it to many tasks like facial
retargeting and expression editing. Experimental results demonstrate its strong
representation ability and training/inference speed. Demo videos and released
code are provided in our project page:
https://ustc3dv.github.io/NeRFBlendShape/

Comments:
- Accepted by SIGGRAPH Asia 2022 (Journal Track). Project page:
  https://ustc3dv.github.io/NeRFBlendShape/

---

## X-NeRF: Explicit Neural Radiance Field for Multi-Scene 360$^{\circ} $  Insufficient RGB-D Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-11 | Haoyi Zhu, Hao-Shu Fang, Cewu Lu | cs.CV | [PDF](http://arxiv.org/pdf/2210.05135v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs), despite their outstanding performance on
novel view synthesis, often need dense input views. Many papers train one model
for each scene respectively and few of them explore incorporating multi-modal
data into this problem. In this paper, we focus on a rarely discussed but
important setting: can we train one model that can represent multiple scenes,
with 360$^\circ $ insufficient views and RGB-D images? We refer insufficient
views to few extremely sparse and almost non-overlapping views. To deal with
it, X-NeRF, a fully explicit approach which learns a general scene completion
process instead of a coordinate-based mapping, is proposed. Given a few
insufficient RGB-D input views, X-NeRF first transforms them to a sparse point
cloud tensor and then applies a 3D sparse generative Convolutional Neural
Network (CNN) to complete it to an explicit radiance field whose volumetric
rendering can be conducted fast without running networks during inference. To
avoid overfitting, besides common rendering loss, we apply perceptual loss as
well as view augmentation through random rotation on point clouds. The proposed
methodology significantly out-performs previous implicit methods in our
setting, indicating the great potential of proposed problem and approach. Codes
and data are available at https://github.com/HaoyiZhu/XNeRF.

---

## NerfAcc: A General NeRF Acceleration Toolbox

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-10 | Ruilong Li, Matthew Tancik, Angjoo Kanazawa | cs.CV | [PDF](http://arxiv.org/pdf/2210.04847v3){: .btn .btn-green } |

**Abstract**: We propose NerfAcc, a toolbox for efficient volumetric rendering of radiance
fields. We build on the techniques proposed in Instant-NGP, and extend these
techniques to not only support bounded static scenes, but also for dynamic
scenes and unbounded scenes. NerfAcc comes with a user-friendly Python API, and
is ready for plug-and-play acceleration of most NeRFs. Various examples are
provided to show how to use this toolbox. Code can be found here:
https://github.com/KAIR-BAIR/nerfacc. Note this write-up matches with NerfAcc
v0.3.5. For the latest features in NerfAcc, please check out our more recent
write-up at arXiv:2305.04966

Comments:
- Webpage: https://www.nerfacc.com/; Updated Write-up: arXiv:2305.04966

---

## SiNeRF: Sinusoidal Neural Radiance Fields for Joint Pose Estimation and  Scene Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-10 | Yitong Xia, Hao Tang, Radu Timofte, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2210.04553v1){: .btn .btn-green } |

**Abstract**: NeRFmm is the Neural Radiance Fields (NeRF) that deal with Joint Optimization
tasks, i.e., reconstructing real-world scenes and registering camera parameters
simultaneously. Despite NeRFmm producing precise scene synthesis and pose
estimations, it still struggles to outperform the full-annotated baseline on
challenging scenes. In this work, we identify that there exists a systematic
sub-optimality in joint optimization and further identify multiple potential
sources for it. To diminish the impacts of potential sources, we propose
Sinusoidal Neural Radiance Fields (SiNeRF) that leverage sinusoidal activations
for radiance mapping and a novel Mixed Region Sampling (MRS) for selecting ray
batch efficiently. Quantitative and qualitative results show that compared to
NeRFmm, SiNeRF achieves comprehensive significant improvements in image
synthesis quality and pose estimation accuracy. Codes are available at
https://github.com/yitongx/sinerf.

Comments:
- Accepted yet not published by BMVC2022

---

## NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills  using Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-10 | Arunkumar Byravan, Jan Humplik, Leonard Hasenclever, Arthur Brussee, Francesco Nori, Tuomas Haarnoja, Ben Moran, Steven Bohez, Fereshteh Sadeghi, Bojan Vujatovic, Nicolas Heess | cs.RO | [PDF](http://arxiv.org/pdf/2210.04932v1){: .btn .btn-green } |

**Abstract**: We present a system for applying sim2real approaches to "in the wild" scenes
with realistic visuals, and to policies which rely on active perception using
RGB cameras. Given a short video of a static scene collected using a generic
phone, we learn the scene's contact geometry and a function for novel view
synthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering
of the static scene by overlaying the rendering of other dynamic objects (e.g.
the robot's own body, a ball). A simulation is then created using the rendering
engine in a physics simulator which computes contact dynamics from the static
scene geometry (estimated from the NeRF volume density) and the dynamic
objects' geometry and physical properties (assumed known). We demonstrate that
we can use this simulation to learn vision-based whole body navigation and ball
pushing policies for a 20 degrees of freedom humanoid robot with an actuated
head-mounted RGB camera, and we successfully transfer these policies to a real
robot. Project video is available at
https://sites.google.com/view/nerf2real/home

---

## EVA3D: Compositional 3D Human Generation from 2D Image Collections

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-10 | Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2210.04888v1){: .btn .btn-green } |

**Abstract**: Inverse graphics aims to recover 3D models from 2D observations. Utilizing
differentiable rendering, recent 3D-aware generative models have shown
impressive results of rigid object generation using 2D images. However, it
remains challenging to generate articulated objects, like human bodies, due to
their complexity and diversity in poses and appearances. In this work, we
propose, EVA3D, an unconditional 3D human generative model learned from 2D
image collections only. EVA3D can sample 3D humans with detailed geometry and
render high-quality images (up to 512x256) without bells and whistles (e.g.
super resolution). At the core of EVA3D is a compositional human NeRF
representation, which divides the human body into local parts. Each part is
represented by an individual volume. This compositional representation enables
1) inherent human priors, 2) adaptive allocation of network parameters, 3)
efficient training and rendering. Moreover, to accommodate for the
characteristics of sparse 2D human image collections (e.g. imbalanced pose
distribution), we propose a pose-guided sampling strategy for better GAN
learning. Extensive experiments validate that EVA3D achieves state-of-the-art
3D human generation performance regarding both geometry and texture quality.
Notably, EVA3D demonstrates great potential and scalability to
"inverse-graphics" diverse human bodies with a clean framework.

Comments:
- Project Page at https://hongfz16.github.io/projects/EVA3D.html

---

## Robustifying the Multi-Scale Representation of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-09 | Nishant Jain, Suryansh Kumar, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2210.04233v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) recently emerged as a new paradigm for object
representation from multi-view (MV) images. Yet, it cannot handle multi-scale
(MS) images and camera pose estimation errors, which generally is the case with
multi-view images captured from a day-to-day commodity camera. Although
recently proposed Mip-NeRF could handle multi-scale imaging problems with NeRF,
it cannot handle camera pose estimation error. On the other hand, the newly
proposed BARF can solve the camera pose problem with NeRF but fails if the
images are multi-scale in nature. This paper presents a robust multi-scale
neural radiance fields representation approach to simultaneously overcome both
real-world imaging issues. Our method handles multi-scale imaging effects and
camera-pose estimation problems with NeRF-inspired approaches by leveraging the
fundamentals of scene rigidity. To reduce unpleasant aliasing artifacts due to
multi-scale images in the ray space, we leverage Mip-NeRF multi-scale
representation. For joint estimation of robust camera pose, we propose
graph-neural network-based multiple motion averaging in the neural volume
rendering framework. We demonstrate, with examples, that for an accurate neural
representation of an object from day-to-day acquired multi-view images, it is
crucial to have precise camera-pose estimates. Without considering robustness
measures in the camera pose estimation, modeling for multi-scale aliasing
artifacts via conical frustum can be counterproductive. We present extensive
experiments on the benchmark datasets to demonstrate that our approach provides
better results than the recent NeRF-inspired approaches for such realistic
settings.

Comments:
- Accepted for publication at British Machine Vision Conference (BMVC)
  2022. Draft info: 13 pages, 3 Figures, and 4 Tables

---

## Estimating Neural Reflectance Field from Radiance Field using Tree  Structures

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-09 | Xiu Li, Xiao Li, Yan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2210.04217v1){: .btn .btn-green } |

**Abstract**: We present a new method for estimating the Neural Reflectance Field (NReF) of
an object from a set of posed multi-view images under unknown lighting. NReF
represents 3D geometry and appearance of objects in a disentangled manner, and
are hard to be estimated from images only. Our method solves this problem by
exploiting the Neural Radiance Field (NeRF) as a proxy representation, from
which we perform further decomposition. A high-quality NeRF decomposition
relies on good geometry information extraction as well as good prior terms to
properly resolve ambiguities between different components. To extract
high-quality geometry information from radiance fields, we re-design a new
ray-casting based method for surface point extraction. To efficiently compute
and apply prior terms, we convert different prior terms into different type of
filter operations on the surface extracted from radiance field. We then employ
two type of auxiliary data structures, namely Gaussian KD-tree and octree, to
support fast querying of surface points and efficient computation of surface
filters during training. Based on this, we design a multi-stage decomposition
optimization pipeline for estimating neural reflectance field from neural
radiance fields. Extensive experiments show our method outperforms other
state-of-the-art methods on different data, and enable high-quality free-view
relighting as well as material editing tasks.

---

## VM-NeRF: Tackling Sparsity in NeRF with View Morphing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-09 | Matteo Bortolon, Alessio Del Bue, Fabio Poiesi | cs.CV | [PDF](http://arxiv.org/pdf/2210.04214v2){: .btn .btn-green } |

**Abstract**: NeRF aims to learn a continuous neural scene representation by using a finite
set of input images taken from various viewpoints. A well-known limitation of
NeRF methods is their reliance on data: the fewer the viewpoints, the higher
the likelihood of overfitting. This paper addresses this issue by introducing a
novel method to generate geometrically consistent image transitions between
viewpoints using View Morphing. Our VM-NeRF approach requires no prior
knowledge about the scene structure, as View Morphing is based on the
fundamental principles of projective geometry. VM-NeRF tightly integrates this
geometric view generation process during the training procedure of standard
NeRF approaches. Notably, our method significantly improves novel view
synthesis, particularly when only a few views are available. Experimental
evaluation reveals consistent improvement over current methods that handle
sparse viewpoints in NeRF models. We report an increase in PSNR of up to 1.8dB
and 1.0dB when training uses eight and four views, respectively. Source code:
\url{https://github.com/mbortolon97/VM-NeRF}

Comments:
- ICIAP 2023

---

## Towards Efficient Neural Scene Graphs by Learning Consistency Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-09 | Yeji Song, Chaerin Kong, Seoyoung Lee, Nojun Kwak, Joonseok Lee | cs.CV | [PDF](http://arxiv.org/pdf/2210.04127v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) achieves photo-realistic image rendering from
novel views, and the Neural Scene Graphs (NSG) \cite{ost2021neural} extends it
to dynamic scenes (video) with multiple objects. Nevertheless, computationally
heavy ray marching for every image frame becomes a huge burden. In this paper,
taking advantage of significant redundancy across adjacent frames in videos, we
propose a feature-reusing framework. From the first try of naively reusing the
NSG features, however, we learn that it is crucial to disentangle
object-intrinsic properties consistent across frames from transient ones. Our
proposed method, \textit{Consistency-Field-based NSG (CF-NSG)}, reformulates
neural radiance fields to additionally consider \textit{consistency fields}.
With disentangled representations, CF-NSG takes full advantage of the
feature-reusing scheme and performs an extended degree of scene manipulation in
a more controllable manner. We empirically verify that CF-NSG greatly improves
the inference efficiency by using 85\% less queries than NSG without notable
degradation in rendering quality. Code will be available at:
https://github.com/ldynx/CF-NSG

Comments:
- BMVC 2022, 22 pages

---

## ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial  Viewpoints

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-08 | Yinpeng Dong, Shouwei Ruan, Hang Su, Caixin Kang, Xingxing Wei, Jun Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2210.03895v1){: .btn .btn-green } |

**Abstract**: Recent studies have demonstrated that visual recognition models lack
robustness to distribution shift. However, current work mainly considers model
robustness to 2D image transformations, leaving viewpoint changes in the 3D
world less explored. In general, viewpoint changes are prevalent in various
real-world applications (e.g., autonomous driving), making it imperative to
evaluate viewpoint robustness. In this paper, we propose a novel method called
ViewFool to find adversarial viewpoints that mislead visual recognition models.
By encoding real-world objects as neural radiance fields (NeRF), ViewFool
characterizes a distribution of diverse adversarial viewpoints under an
entropic regularizer, which helps to handle the fluctuations of the real camera
pose and mitigate the reality gap between the real objects and their neural
representations. Experiments validate that the common image classifiers are
extremely vulnerable to the generated adversarial viewpoints, which also
exhibit high cross-model transferability. Based on ViewFool, we introduce
ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint
robustness of image classifiers. Evaluation results on 40 classifiers with
diverse architectures, objective functions, and data augmentations reveal a
significant drop in model performance when tested on ImageNet-V, which provides
a possibility to leverage ViewFool as an effective data augmentation strategy
to improve viewpoint robustness.

Comments:
- NeurIPS 2022

---

## SelfNeRF: Fast Training NeRF for Human from Monocular Self-rotating  Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-04 | Bo Peng, Jun Hu, Jingtao Zhou, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2210.01651v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose SelfNeRF, an efficient neural radiance field based
novel view synthesis method for human performance. Given monocular
self-rotating videos of human performers, SelfNeRF can train from scratch and
achieve high-fidelity results in about twenty minutes. Some recent works have
utilized the neural radiance field for dynamic human reconstruction. However,
most of these methods need multi-view inputs and require hours of training,
making it still difficult for practical use. To address this challenging
problem, we introduce a surface-relative representation based on
multi-resolution hash encoding that can greatly improve the training speed and
aggregate inter-frame information. Extensive experimental results on several
different datasets demonstrate the effectiveness and efficiency of SelfNeRF to
challenging monocular videos.

Comments:
- Project page: https://ustc3dv.github.io/SelfNeRF

---

## Capturing and Animation of Body and Clothing from Monocular Video



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-04 | Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J. Black, Timo Bolkart | cs.CV | [PDF](http://arxiv.org/pdf/2210.01868v1){: .btn .btn-green } |

**Abstract**: While recent work has shown progress on extracting clothed 3D human avatars
from a single image, video, or a set of 3D scans, several limitations remain.
Most methods use a holistic representation to jointly model the body and
clothing, which means that the clothing and body cannot be separated for
applications like virtual try-on. Other methods separately model the body and
clothing, but they require training from a large set of 3D clothed human meshes
obtained from 3D/4D scanners or physics simulations. Our insight is that the
body and clothing have different modeling requirements. While the body is well
represented by a mesh-based parametric 3D model, implicit representations and
neural radiance fields are better suited to capturing the large variety in
shape and appearance present in clothing. Building on this insight, we propose
SCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a
mesh-based body with a neural radiance field. Integrating the mesh into the
volumetric rendering in combination with a differentiable rasterizer enables us
to optimize SCARF directly from monocular videos, without any 3D supervision.
The hybrid modeling enables SCARF to (i) animate the clothed body avatar by
changing body poses (including hand articulation and facial expressions), (ii)
synthesize novel views of the avatar, and (iii) transfer clothing between
avatars in virtual try-on applications. We demonstrate that SCARF reconstructs
clothing with higher visual quality than existing methods, that the clothing
deforms with changing body pose and body shape, and that clothing can be
successfully transferred between avatars of different subjects. The code and
models are available at https://github.com/YadiraF/SCARF.

Comments:
- 7 pages main paper, 2 pages supp. mat

---

## NARF22: Neural Articulated Radiance Fields for Configuration-Aware  Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-03 | Stanley Lewis, Jana Pavlasek, Odest Chadwicke Jenkins | cs.RO | [PDF](http://arxiv.org/pdf/2210.01166v1){: .btn .btn-green } |

**Abstract**: Articulated objects pose a unique challenge for robotic perception and
manipulation. Their increased number of degrees-of-freedom makes tasks such as
localization computationally difficult, while also making the process of
real-world dataset collection unscalable. With the aim of addressing these
scalability issues, we propose Neural Articulated Radiance Fields (NARF22), a
pipeline which uses a fully-differentiable, configuration-parameterized Neural
Radiance Field (NeRF) as a means of providing high quality renderings of
articulated objects. NARF22 requires no explicit knowledge of the object
structure at inference time. We propose a two-stage parts-based training
mechanism which allows the object rendering models to generalize well across
the configuration space even if the underlying training data has as few as one
configuration represented. We demonstrate the efficacy of NARF22 by training
configurable renderers on a real-world articulated tool dataset collected via a
Fetch mobile manipulation robot. We show the applicability of the model to
gradient-based inference methods through a configuration estimation and 6
degree-of-freedom pose refinement task. The project webpage is available at:
https://progress.eecs.umich.edu/projects/narf/.

Comments:
- Accepted to the 2022 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS). Contact: Stanley Lewis, stanlew@umich.edu

---

## Unsupervised Multi-View Object Segmentation Using Radiance Field  Propagation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-02 | Xinhang Liu, Jiaben Chen, Huai Yu, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2210.00489v2){: .btn .btn-green } |

**Abstract**: We present radiance field propagation (RFP), a novel approach to segmenting
objects in 3D during reconstruction given only unlabeled multi-view images of a
scene. RFP is derived from emerging neural radiance field-based techniques,
which jointly encodes semantics with appearance and geometry. The core of our
method is a novel propagation strategy for individual objects' radiance fields
with a bidirectional photometric loss, enabling an unsupervised partitioning of
a scene into salient or meaningful regions corresponding to different object
instances. To better handle complex scenes with multiple objects and
occlusions, we further propose an iterative expectation-maximization algorithm
to refine object masks. RFP is one of the first unsupervised approach for
tackling 3D real scene object segmentation for neural radiance field (NeRF)
without any supervision, annotations, or other cues such as 3D bounding boxes
and prior knowledge of object class. Experiments demonstrate that RFP achieves
feasible segmentation results that are more accurate than previous unsupervised
image/scene segmentation approaches, and are comparable to existing supervised
NeRF-based methods. The segmented object representations enable individual 3D
object editing operations.

Comments:
- 23 pages, 14 figures, NeurIPS 2022

---

## IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable  Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-02 | Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2210.00647v3){: .btn .btn-green } |

**Abstract**: Existing inverse rendering combined with neural rendering methods can only
perform editable novel view synthesis on object-specific scenes, while we
present intrinsic neural radiance fields, dubbed IntrinsicNeRF, which introduce
intrinsic decomposition into the NeRF-based neural rendering method and can
extend its application to room-scale scenes. Since intrinsic decomposition is a
fundamentally under-constrained inverse problem, we propose a novel
distance-aware point sampling and adaptive reflectance iterative clustering
optimization method, which enables IntrinsicNeRF with traditional intrinsic
decomposition constraints to be trained in an unsupervised manner, resulting in
multi-view consistent intrinsic decomposition results. To cope with the problem
that different adjacent instances of similar reflectance in a scene are
incorrectly clustered together, we further propose a hierarchical clustering
method with coarse-to-fine optimization to obtain a fast hierarchical indexing
representation. It supports compelling real-time augmented applications such as
recoloring and illumination variation. Extensive experiments and editing
samples on both object-specific/room-scale scenes and synthetic/real-word data
demonstrate that we can obtain consistent intrinsic decomposition results and
high-fidelity novel view synthesis even for challenging sequences.

Comments:
- Accepted to ICCV2023, Project webpage:
  https://zju3dv.github.io/intrinsic_nerf/, code:
  https://github.com/zju3dv/IntrinsicNeRF

---

## Neural Implicit Surface Reconstruction from Noisy Camera Observations



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-02 | Sarthak Gupta, Patrik Huber | cs.CV | [PDF](http://arxiv.org/pdf/2210.01548v1){: .btn .btn-green } |

**Abstract**: Representing 3D objects and scenes with neural radiance fields has become
very popular over the last years. Recently, surface-based representations have
been proposed, that allow to reconstruct 3D objects from simple photographs.
However, most current techniques require an accurate camera calibration, i.e.
camera parameters corresponding to each image, which is often a difficult task
to do in real-life situations. To this end, we propose a method for learning 3D
surfaces from noisy camera parameters. We show that we can learn camera
parameters together with learning the surface representation, and demonstrate
good quality 3D surface reconstruction even with noisy camera observations.

Comments:
- 4 pages - 2 for paper, 2 for supplementary

---

## Structure-Aware NeRF without Posed Camera via Epipolar Constraint

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-01 | Shu Chen, Yang Zhang, Yaxin Xu, Beiji Zou | cs.CV | [PDF](http://arxiv.org/pdf/2210.00183v1){: .btn .btn-green } |

**Abstract**: The neural radiance field (NeRF) for realistic novel view synthesis requires
camera poses to be pre-acquired by a structure-from-motion (SfM) approach. This
two-stage strategy is not convenient to use and degrades the performance
because the error in the pose extraction can propagate to the view synthesis.
We integrate the pose extraction and view synthesis into a single end-to-end
procedure so they can benefit from each other. For training NeRF models, only
RGB images are given, without pre-known camera poses. The camera poses are
obtained by the epipolar constraint in which the identical feature in different
views has the same world coordinates transformed from the local camera
coordinates according to the extracted poses. The epipolar constraint is
jointly optimized with pixel color constraint. The poses are represented by a
CNN-based deep network, whose input is the related frames. This joint
optimization enables NeRF to be aware of the scene's structure that has an
improved generalization performance. Extensive experiments on a variety of
scenes demonstrate the effectiveness of the proposed approach. Code is
available at https://github.com/XTU-PR-LAB/SaNerf.

---

## NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-10-01 | Kyle Gao, Yina Gao, Hongjie He, Dening Lu, Linlin Xu, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2210.00379v5){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has recently become a significant development in
the field of Computer Vision, allowing for implicit, neural network-based scene
representation and novel view synthesis. NeRF models have found diverse
applications in robotics, urban mapping, autonomous navigation, virtual
reality/augmented reality, and more. Due to the growing popularity of NeRF and
its expanding research area, we present a comprehensive survey of NeRF papers
from the past two years. Our survey is organized into architecture and
application-based taxonomies and provides an introduction to the theory of NeRF
and its training via differentiable volume rendering. We also present a
benchmark comparison of the performance and speed of key NeRF models. By
creating this survey, we hope to introduce new researchers to NeRF, provide a
helpful reference for influential works in this field, as well as motivate
future research directions with our discussion section.

Comments:
- Fixed some typos from previous version

---

## Improving 3D-aware Image Synthesis with A Geometry-aware Discriminator

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-30 | Zifan Shi, Yinghao Xu, Yujun Shen, Deli Zhao, Qifeng Chen, Dit-Yan Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2209.15637v1){: .btn .btn-green } |

**Abstract**: 3D-aware image synthesis aims at learning a generative model that can render
photo-realistic 2D images while capturing decent underlying 3D shapes. A
popular solution is to adopt the generative adversarial network (GAN) and
replace the generator with a 3D renderer, where volume rendering with neural
radiance field (NeRF) is commonly used. Despite the advancement of synthesis
quality, existing methods fail to obtain moderate 3D shapes. We argue that,
considering the two-player game in the formulation of GANs, only making the
generator 3D-aware is not enough. In other words, displacing the generative
mechanism only offers the capability, but not the guarantee, of producing
3D-aware images, because the supervision of the generator primarily comes from
the discriminator. To address this issue, we propose GeoD through learning a
geometry-aware discriminator to improve 3D-aware GANs. Concretely, besides
differentiating real and fake samples from the 2D image space, the
discriminator is additionally asked to derive the geometry information from the
inputs, which is then applied as the guidance of the generator. Such a simple
yet effective design facilitates learning substantially more accurate 3D
shapes. Extensive experiments on various generator architectures and training
datasets verify the superiority of GeoD over state-of-the-art alternatives.
Moreover, our approach is registered as a general framework such that a more
capable discriminator (i.e., with a third task of novel view synthesis beyond
domain classification and geometry extraction) can further assist the generator
with a better multi-view consistency.

Comments:
- Accepted by NeurIPS 2022. Project page:
  https://vivianszf.github.io/geod

---

## TT-NF: Tensor Train Neural Fields



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-30 | Anton Obukhov, Mikhail Usvyatsov, Christos Sakaridis, Konrad Schindler, Luc Van Gool | cs.LG | [PDF](http://arxiv.org/pdf/2209.15529v1){: .btn .btn-green } |

**Abstract**: Learning neural fields has been an active topic in deep learning research,
focusing, among other issues, on finding more compact and easy-to-fit
representations. In this paper, we introduce a novel low-rank representation
termed Tensor Train Neural Fields (TT-NF) for learning neural fields on dense
regular grids and efficient methods for sampling from them. Our representation
is a TT parameterization of the neural field, trained with backpropagation to
minimize a non-convex objective. We analyze the effect of low-rank compression
on the downstream task quality metrics in two settings. First, we demonstrate
the efficiency of our method in a sandbox task of tensor denoising, which
admits comparison with SVD-based schemes designed to minimize reconstruction
error. Furthermore, we apply the proposed approach to Neural Radiance Fields,
where the low-rank structure of the field corresponding to the best quality can
be discovered only through learning.

Comments:
- Preprint, under review

---

## Understanding Pure CLIP Guidance for Voxel Grid NeRF Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-30 | Han-Hung Lee, Angel X. Chang | cs.CV | [PDF](http://arxiv.org/pdf/2209.15172v1){: .btn .btn-green } |

**Abstract**: We explore the task of text to 3D object generation using CLIP. Specifically,
we use CLIP for guidance without access to any datasets, a setting we refer to
as pure CLIP guidance. While prior work has adopted this setting, there is no
systematic study of mechanics for preventing adversarial generations within
CLIP. We illustrate how different image-based augmentations prevent the
adversarial generation problem, and how the generated results are impacted. We
test different CLIP model architectures and show that ensembling different
models for guidance can prevent adversarial generations within bigger models
and generate sharper results. Furthermore, we implement an implicit voxel grid
model to show how neural networks provide an additional layer of
regularization, resulting in better geometrical structure and coherency of
generated objects. Compared to prior work, we achieve more coherent results
with higher memory efficiency and faster training speeds.

---

## DreamFusion: Text-to-3D using 2D Diffusion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-29 | Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall | cs.CV | [PDF](http://arxiv.org/pdf/2209.14988v1){: .btn .btn-green } |

**Abstract**: Recent breakthroughs in text-to-image synthesis have been driven by diffusion
models trained on billions of image-text pairs. Adapting this approach to 3D
synthesis would require large-scale datasets of labeled 3D data and efficient
architectures for denoising 3D data, neither of which currently exist. In this
work, we circumvent these limitations by using a pretrained 2D text-to-image
diffusion model to perform text-to-3D synthesis. We introduce a loss based on
probability density distillation that enables the use of a 2D diffusion model
as a prior for optimization of a parametric image generator. Using this loss in
a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a
Neural Radiance Field, or NeRF) via gradient descent such that its 2D
renderings from random angles achieve a low loss. The resulting 3D model of the
given text can be viewed from any angle, relit by arbitrary illumination, or
composited into any 3D environment. Our approach requires no 3D training data
and no modifications to the image diffusion model, demonstrating the
effectiveness of pretrained image diffusion models as priors.

Comments:
- see project page at https://dreamfusion3d.github.io/

---

## SymmNeRF: Learning to Explore Symmetry Prior for Single-View View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-29 | Xingyi Li, Chaoyi Hong, Yiran Wang, Zhiguo Cao, Ke Xian, Guosheng Lin | cs.CV | [PDF](http://arxiv.org/pdf/2209.14819v2){: .btn .btn-green } |

**Abstract**: We study the problem of novel view synthesis of objects from a single image.
Existing methods have demonstrated the potential in single-view view synthesis.
However, they still fail to recover the fine appearance details, especially in
self-occluded areas. This is because a single view only provides limited
information. We observe that manmade objects usually exhibit symmetric
appearances, which introduce additional prior knowledge. Motivated by this, we
investigate the potential performance gains of explicitly embedding symmetry
into the scene representation. In this paper, we propose SymmNeRF, a neural
radiance field (NeRF) based framework that combines local and global
conditioning under the introduction of symmetry priors. In particular, SymmNeRF
takes the pixel-aligned image features and the corresponding symmetric features
as extra inputs to the NeRF, whose parameters are generated by a hypernetwork.
As the parameters are conditioned on the image-encoded latent codes, SymmNeRF
is thus scene-independent and can generalize to new scenes. Experiments on
synthetic and real-world datasets show that SymmNeRF synthesizes novel views
with more details regardless of the pose transformation, and demonstrates good
generalization when applied to unseen objects. Code is available at:
https://github.com/xingyi-li/SymmNeRF.

Comments:
- Accepted by ACCV 2022

---

## 360FusionNeRF: Panoramic Neural Radiance Fields with Joint Guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-28 | Shreyas Kulkarni, Peng Yin, Sebastian Scherer | cs.CV | [PDF](http://arxiv.org/pdf/2209.14265v2){: .btn .btn-green } |

**Abstract**: We present a method to synthesize novel views from a single $360^\circ$
panorama image based on the neural radiance field (NeRF). Prior studies in a
similar setting rely on the neighborhood interpolation capability of
multi-layer perceptions to complete missing regions caused by occlusion, which
leads to artifacts in their predictions. We propose 360FusionNeRF, a
semi-supervised learning framework where we introduce geometric supervision and
semantic consistency to guide the progressive training process. Firstly, the
input image is re-projected to $360^\circ$ images, and auxiliary depth maps are
extracted at other camera positions. The depth supervision, in addition to the
NeRF color guidance, improves the geometry of the synthesized views.
Additionally, we introduce a semantic consistency loss that encourages
realistic renderings of novel views. We extract these semantic features using a
pre-trained visual encoder such as CLIP, a Vision Transformer trained on
hundreds of millions of diverse 2D photographs mined from the web with natural
language supervision. Experiments indicate that our proposed method can produce
plausible completions of unobserved regions while preserving the features of
the scene. When trained across various scenes, 360FusionNeRF consistently
achieves the state-of-the-art performance when transferring to synthetic
Structured3D dataset (PSNR~5%, SSIM~3% LPIPS~13%), real-world Matterport3D
dataset (PSNR~3%, SSIM~3% LPIPS~9%) and Replica360 dataset (PSNR~8%, SSIM~2%
LPIPS~18%).

Comments:
- 8 pages, Fig 3, Submitted to IEEE RAL. arXiv admin note: text overlap
  with arXiv:2106.10859, arXiv:2104.00677, arXiv:2203.09957, arXiv:2204.00928
  by other authors

---

## OmniNeRF: Hybriding Omnidirectional Distance and Radiance fields for  Neural Surface Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-27 | Jiaming Shen, Bolin Song, Zirui Wu, Yi Xu | cs.CV | [PDF](http://arxiv.org/pdf/2209.13433v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction from images has wide applications in Virtual Reality and
Automatic Driving, where the precision requirement is very high.
Ground-breaking research in the neural radiance field (NeRF) by utilizing
Multi-Layer Perceptions has dramatically improved the representation quality of
3D objects. Some later studies improved NeRF by building truncated signed
distance fields (TSDFs) but still suffer from the problem of blurred surfaces
in 3D reconstruction. In this work, this surface ambiguity is addressed by
proposing a novel way of 3D shape representation, OmniNeRF. It is based on
training a hybrid implicit field of Omni-directional Distance Field (ODF) and
neural radiance field, replacing the apparent density in NeRF with
omnidirectional information. Moreover, we introduce additional supervision on
the depth map to further improve reconstruction quality. The proposed method
has been proven to effectively deal with NeRF defects at the edges of the
surface reconstruction, providing higher quality 3D scene reconstruction
results.

Comments:
- Accepted by CMSDA 2022

---

## Orbeez-SLAM: A Real-time Monocular Visual SLAM with ORB Features and  NeRF-realized Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-27 | Chi-Ming Chung, Yang-Che Tseng, Ya-Ching Hsu, Xiang-Qian Shi, Yun-Hung Hua, Jia-Fong Yeh, Wen-Chin Chen, Yi-Ting Chen, Winston H. Hsu | cs.RO | [PDF](http://arxiv.org/pdf/2209.13274v2){: .btn .btn-green } |

**Abstract**: A spatial AI that can perform complex tasks through visual signals and
cooperate with humans is highly anticipated. To achieve this, we need a visual
SLAM that easily adapts to new scenes without pre-training and generates dense
maps for downstream tasks in real-time. None of the previous learning-based and
non-learning-based visual SLAMs satisfy all needs due to the intrinsic
limitations of their components. In this work, we develop a visual SLAM named
Orbeez-SLAM, which successfully collaborates with implicit neural
representation and visual odometry to achieve our goals. Moreover, Orbeez-SLAM
can work with the monocular camera since it only needs RGB inputs, making it
widely applicable to the real world. Results show that our SLAM is up to 800x
faster than the strong baseline with superior rendering outcomes. Code link:
https://github.com/MarvinChung/Orbeez-SLAM.

---

## WaterNeRF: Neural Radiance Fields for Underwater Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-27 | Advaith Venkatramanan Sethuraman, Manikandasriram Srinivasan Ramanagopal, Katherine A. Skinner | cs.RO | [PDF](http://arxiv.org/pdf/2209.13091v2){: .btn .btn-green } |

**Abstract**: Underwater imaging is a critical task performed by marine robots for a wide
range of applications including aquaculture, marine infrastructure inspection,
and environmental monitoring. However, water column effects, such as
attenuation and backscattering, drastically change the color and quality of
imagery captured underwater. Due to varying water conditions and
range-dependency of these effects, restoring underwater imagery is a
challenging problem. This impacts downstream perception tasks including depth
estimation and 3D reconstruction. In this paper, we advance state-of-the-art in
neural radiance fields (NeRFs) to enable physics-informed dense depth
estimation and color correction. Our proposed method, WaterNeRF, estimates
parameters of a physics-based model for underwater image formation, leading to
a hybrid data-driven and model-based solution. After determining the scene
structure and radiance field, we can produce novel views of degraded as well as
corrected underwater images, along with dense depth of the scene. We evaluate
the proposed method qualitatively and quantitatively on a real underwater
dataset.

---

## Baking in the Feature: Accelerating Volumetric Segmentation by Rendering  Feature Maps

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-26 | Kenneth Blomqvist, Lionel Ott, Jen Jen Chung, Roland Siegwart | cs.CV | [PDF](http://arxiv.org/pdf/2209.12744v1){: .btn .btn-green } |

**Abstract**: Methods have recently been proposed that densely segment 3D volumes into
classes using only color images and expert supervision in the form of sparse
semantically annotated pixels. While impressive, these methods still require a
relatively large amount of supervision and segmenting an object can take
several minutes in practice. Such systems typically only optimize their
representation on the particular scene they are fitting, without leveraging any
prior information from previously seen images. In this paper, we propose to use
features extracted with models trained on large existing datasets to improve
segmentation performance. We bake this feature representation into a Neural
Radiance Field (NeRF) by volumetrically rendering feature maps and supervising
on features extracted from each input image. We show that by baking this
representation into the NeRF, we make the subsequent classification task much
easier. Our experiments show that our method achieves higher segmentation
accuracy with fewer semantic annotations than existing methods over a wide
range of scenes.

---

## Enforcing safety for vision-based controllers via Control Barrier  Functions and Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-25 | Mukun Tong, Charles Dawson, Chuchu Fan | cs.RO | [PDF](http://arxiv.org/pdf/2209.12266v3){: .btn .btn-green } |

**Abstract**: To navigate complex environments, robots must increasingly use
high-dimensional visual feedback (e.g. images) for control. However, relying on
high-dimensional image data to make control decisions raises important
questions; particularly, how might we prove the safety of a visual-feedback
controller? Control barrier functions (CBFs) are powerful tools for certifying
the safety of feedback controllers in the state-feedback setting, but CBFs have
traditionally been poorly-suited to visual feedback control due to the need to
predict future observations in order to evaluate the barrier function. In this
work, we solve this issue by leveraging recent advances in neural radiance
fields (NeRFs), which learn implicit representations of 3D scenes and can
render images from previously-unseen camera perspectives, to provide
single-step visual foresight for a CBF-based controller. This novel combination
is able to filter out unsafe actions and intervene to preserve safety. We
demonstrate the effect of our controller in real-time simulation experiments
where it successfully prevents the robot from taking dangerous actions.

Comments:
- Accepted to ICRA 2023

---

## NeRF-Loc: Transformer-Based Object Localization Within Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-24 | Jiankai Sun, Yan Xu, Mingyu Ding, Hongwei Yi, Chen Wang, Jingdong Wang, Liangjun Zhang, Mac Schwager | cs.CV | [PDF](http://arxiv.org/pdf/2209.12068v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become a widely-applied scene
representation technique in recent years, showing advantages for robot
navigation and manipulation tasks. To further advance the utility of NeRFs for
robotics, we propose a transformer-based framework, NeRF-Loc, to extract 3D
bounding boxes of objects in NeRF scenes. NeRF-Loc takes a pre-trained NeRF
model and camera view as input and produces labeled, oriented 3D bounding boxes
of objects as output. Using current NeRF training tools, a robot can train a
NeRF environment model in real-time and, using our algorithm, identify 3D
bounding boxes of objects of interest within the NeRF for downstream navigation
or manipulation tasks. Concretely, we design a pair of paralleled transformer
encoder branches, namely the coarse stream and the fine stream, to encode both
the context and details of target objects. The encoded features are then fused
together with attention layers to alleviate ambiguities for accurate object
localization. We have compared our method with conventional RGB(-D) based
methods that take rendered RGB images and depths from NeRFs as inputs. Our
method is better than the baselines.

---

## NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-19 | Zhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, Dejia Xu, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2209.08776v6){: .btn .btn-green } |

**Abstract**: Neural volumetric representations have shown the potential that Multi-layer
Perceptrons (MLPs) can be optimized with multi-view calibrated images to
represent scene geometry and appearance, without explicit 3D supervision.
Object segmentation can enrich many downstream applications based on the
learned radiance field. However, introducing hand-crafted segmentation to
define regions of interest in a complex real-world scene is non-trivial and
expensive as it acquires per view annotation. This paper carries out the
exploration of self-supervised learning for object segmentation using NeRF for
complex real-world scenes. Our framework, called NeRF with Self-supervised
Object Segmentation NeRF-SOS, couples object segmentation and neural radiance
field to segment objects in any view within a scene. By proposing a novel
collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS
encourages NeRF models to distill compact geometry-aware segmentation clusters
from their density fields and the self-supervised pre-trained 2D visual
features. The self-supervised object segmentation framework can be applied to
various NeRF models that both lead to photo-realistic rendering results and
convincing segmentation maps for both indoor and outdoor scenarios. Extensive
results on the LLFF, Tank & Temple, and BlendedMVS datasets validate the
effectiveness of NeRF-SOS. It consistently surpasses other 2D-based
self-supervised baselines and predicts finer semantics masks than existing
supervised counterparts. Please refer to the video on our project page for more
details:https://zhiwenfan.github.io/NeRF-SOS.

---

## Density-aware NeRF Ensembles: Quantifying Predictive Uncertainty in  Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-19 | Niko Sünderhauf, Jad Abou-Chakra, Dimity Miller | cs.CV | [PDF](http://arxiv.org/pdf/2209.08718v1){: .btn .btn-green } |

**Abstract**: We show that ensembling effectively quantifies model uncertainty in Neural
Radiance Fields (NeRFs) if a density-aware epistemic uncertainty term is
considered. The naive ensembles investigated in prior work simply average
rendered RGB images to quantify the model uncertainty caused by conflicting
explanations of the observed scene. In contrast, we additionally consider the
termination probabilities along individual rays to identify epistemic model
uncertainty due to a lack of knowledge about the parts of a scene unobserved
during training. We achieve new state-of-the-art performance across established
uncertainty quantification benchmarks for NeRFs, outperforming methods that
require complex changes to the NeRF architecture and training regime. We
furthermore demonstrate that NeRF uncertainty can be utilised for next-best
view selection and model refinement.

---

## Loc-NeRF: Monte Carlo Localization using Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-19 | Dominic Maggio, Marcus Abate, Jingnan Shi, Courtney Mario, Luca Carlone | cs.RO | [PDF](http://arxiv.org/pdf/2209.09050v1){: .btn .btn-green } |

**Abstract**: We present Loc-NeRF, a real-time vision-based robot localization approach
that combines Monte Carlo localization and Neural Radiance Fields (NeRF). Our
system uses a pre-trained NeRF model as the map of an environment and can
localize itself in real-time using an RGB camera as the only exteroceptive
sensor onboard the robot. While neural radiance fields have seen significant
applications for visual rendering in computer vision and graphics, they have
found limited use in robotics. Existing approaches for NeRF-based localization
require both a good initial pose guess and significant computation, making them
impractical for real-time robotics applications. By using Monte Carlo
localization as a workhorse to estimate poses using a NeRF map model, Loc-NeRF
is able to perform localization faster than the state of the art and without
relying on an initial pose estimate. In addition to testing on synthetic data,
we also run our system using real data collected by a Clearpath Jackal UGV and
demonstrate for the first time the ability to perform real-time global
localization with neural radiance fields. We make our code publicly available
at https://github.com/MIT-SPARK/Loc-NeRF.

---

## ActiveNeRF: Learning where to See with Uncertainty Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-18 | Xuran Pan, Zihang Lai, Shiji Song, Gao Huang | cs.CV | [PDF](http://arxiv.org/pdf/2209.08546v1){: .btn .btn-green } |

**Abstract**: Recently, Neural Radiance Fields (NeRF) has shown promising performances on
reconstructing 3D scenes and synthesizing novel views from a sparse set of 2D
images. Albeit effective, the performance of NeRF is highly influenced by the
quality of training samples. With limited posed images from the scene, NeRF
fails to generalize well to novel views and may collapse to trivial solutions
in unobserved regions. This makes NeRF impractical under resource-constrained
scenarios. In this paper, we present a novel learning framework, ActiveNeRF,
aiming to model a 3D scene with a constrained input budget. Specifically, we
first incorporate uncertainty estimation into a NeRF model, which ensures
robustness under few observations and provides an interpretation of how NeRF
understands the scene. On this basis, we propose to supplement the existing
training set with newly captured samples based on an active learning scheme. By
evaluating the reduction of uncertainty given new inputs, we select the samples
that bring the most information gain. In this way, the quality of novel view
synthesis can be improved with minimal additional resources. Extensive
experiments validate the performance of our model on both realistic and
synthetic scenes, especially with scarcer training data. Code will be released
at \url{https://github.com/LeapLabTHU/ActiveNeRF}.

Comments:
- Accepted by ECCV2022

---

## LATITUDE: Robotic Global Localization with Truncated Dynamic Low-pass  Filter in City-scale NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-18 | Zhenxin Zhu, Yuantao Chen, Zirui Wu, Chao Hou, Yongliang Shi, Chuxuan Li, Pengfei Li, Hao Zhao, Guyue Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2209.08498v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have made great success in representing
complex 3D scenes with high-resolution details and efficient memory.
Nevertheless, current NeRF-based pose estimators have no initial pose
prediction and are prone to local optima during optimization. In this paper, we
present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter,
which introduces a two-stage localization mechanism in city-scale NeRF. In
place recognition stage, we train a regressor through images generated from
trained NeRFs, which provides an initial value for global localization. In pose
optimization stage, we minimize the residual between the observed image and
rendered image by directly optimizing the pose on tangent plane. To avoid
convergence to local optimum, we introduce a Truncated Dynamic Low-pass Filter
(TDLF) for coarse-to-fine pose registration. We evaluate our method on both
synthetic and real-world data and show its potential applications for
high-precision navigation in large-scale city scenes. Codes and data will be
publicly available at https://github.com/jike5/LATITUDE.

Comments:
- 7 pages, 6 figures, ICRA 2023

---

## Uncertainty Guided Policy for Active Robotic 3D Reconstruction using  Neural Radiance Fields



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-17 | Soomin Lee, Le Chen, Jiahao Wang, Alexander Liniger, Suryansh Kumar, Fisher Yu | cs.CV | [PDF](http://arxiv.org/pdf/2209.08409v1){: .btn .btn-green } |

**Abstract**: In this paper, we tackle the problem of active robotic 3D reconstruction of
an object. In particular, we study how a mobile robot with an arm-held camera
can select a favorable number of views to recover an object's 3D shape
efficiently. Contrary to the existing solution to this problem, we leverage the
popular neural radiance fields-based object representation, which has recently
shown impressive results for various computer vision tasks. However, it is not
straightforward to directly reason about an object's explicit 3D geometric
details using such a representation, making the next-best-view selection
problem for dense 3D reconstruction challenging. This paper introduces a
ray-based volumetric uncertainty estimator, which computes the entropy of the
weight distribution of the color samples along each ray of the object's
implicit neural representation. We show that it is possible to infer the
uncertainty of the underlying 3D geometry given a novel view with the proposed
estimator. We then present a next-best-view selection policy guided by the
ray-based volumetric uncertainty in neural radiance fields-based
representations. Encouraging experimental results on synthetic and real-world
data suggest that the approach presented in this paper can enable a new
research direction of using an implicit 3D object representation for the
next-best-view problem in robot vision applications, distinguishing our
approach from the existing approaches that rely on explicit 3D geometric
modeling.

Comments:
- 8 pages, 9 figure; Accepted for publication at IEEE Robotics and
  Automation Letters (RA-L) 2022

---

## iDF-SLAM: End-to-End RGB-D SLAM with Neural Implicit Mapping and Deep  Feature Tracking

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-16 | Yuhang Ming, Weicai Ye, Andrew Calway | cs.RO | [PDF](http://arxiv.org/pdf/2209.07919v1){: .btn .btn-green } |

**Abstract**: We propose a novel end-to-end RGB-D SLAM, iDF-SLAM, which adopts a
feature-based deep neural tracker as the front-end and a NeRF-style neural
implicit mapper as the back-end. The neural implicit mapper is trained
on-the-fly, while though the neural tracker is pretrained on the ScanNet
dataset, it is also finetuned along with the training of the neural implicit
mapper. Under such a design, our iDF-SLAM is capable of learning to use
scene-specific features for camera tracking, thus enabling lifelong learning of
the SLAM system. Both the training for the tracker and the mapper are
self-supervised without introducing ground truth poses. We test the performance
of our iDF-SLAM on the Replica and ScanNet datasets and compare the results to
the two recent NeRF-based neural SLAM systems. The proposed iDF-SLAM
demonstrates state-of-the-art results in terms of scene reconstruction and
competitive performance in camera tracking.

Comments:
- 7 pages, 6 figures, 3 tables

---

## 3DMM-RF: Convolutional Radiance Fields for 3D Face Modeling



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-15 | Stathis Galanakis, Baris Gecer, Alexandros Lattas, Stefanos Zafeiriou | cs.CV | [PDF](http://arxiv.org/pdf/2209.07366v1){: .btn .btn-green } |

**Abstract**: Facial 3D Morphable Models are a main computer vision subject with countless
applications and have been highly optimized in the last two decades. The
tremendous improvements of deep generative networks have created various
possibilities for improving such models and have attracted wide interest.
Moreover, the recent advances in neural radiance fields, are revolutionising
novel-view synthesis of known scenes. In this work, we present a facial 3D
Morphable Model, which exploits both of the above, and can accurately model a
subject's identity, pose and expression and render it in arbitrary
illumination. This is achieved by utilizing a powerful deep style-based
generator to overcome two main weaknesses of neural radiance fields, their
rigidity and rendering speed. We introduce a style-based generative network
that synthesizes in one pass all and only the required rendering samples of a
neural radiance field. We create a vast labelled synthetic dataset of facial
renders, and train the network on these data, so that it can accurately model
and generalize on facial identity, pose and appearance. Finally, we show that
this model can accurately be fit to "in-the-wild" facial images of arbitrary
pose and illumination, extract the facial characteristics, and be used to
re-render the face in controllable conditions.

---

## StructNeRF: Neural Radiance Fields for Indoor Scenes with Structural  Hints

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-12 | Zheng Chen, Chen Wang, Yuan-Chen Guo, Song-Hai Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2209.05277v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) achieve photo-realistic view synthesis with
densely captured input images. However, the geometry of NeRF is extremely
under-constrained given sparse views, resulting in significant degradation of
novel view synthesis quality. Inspired by self-supervised depth estimation
methods, we propose StructNeRF, a solution to novel view synthesis for indoor
scenes with sparse inputs. StructNeRF leverages the structural hints naturally
embedded in multi-view inputs to handle the unconstrained geometry issue in
NeRF. Specifically, it tackles the texture and non-texture regions
respectively: a patch-based multi-view consistent photometric loss is proposed
to constrain the geometry of textured regions; for non-textured ones, we
explicitly restrict them to be 3D consistent planes. Through the dense
self-supervised depth constraints, our method improves both the geometry and
the view synthesis performance of NeRF without any additional training on
external data. Extensive experiments on several real-world datasets demonstrate
that StructNeRF surpasses state-of-the-art methods for indoor scenes with
sparse inputs both quantitatively and qualitatively.

---

## Generative Deformable Radiance Fields for Disentangled Image Synthesis  of Topology-Varying Objects

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-09 | Ziyu Wang, Yu Deng, Jiaolong Yang, Jingyi Yu, Xin Tong | cs.CV | [PDF](http://arxiv.org/pdf/2209.04183v1){: .btn .btn-green } |

**Abstract**: 3D-aware generative models have demonstrated their superb performance to
generate 3D neural radiance fields (NeRF) from a collection of monocular 2D
images even for topology-varying object categories. However, these methods
still lack the capability to separately control the shape and appearance of the
objects in the generated radiance fields. In this paper, we propose a
generative model for synthesizing radiance fields of topology-varying objects
with disentangled shape and appearance variations. Our method generates
deformable radiance fields, which builds the dense correspondence between the
density fields of the objects and encodes their appearances in a shared
template field. Our disentanglement is achieved in an unsupervised manner
without introducing extra labels to previous 3D-aware GAN training. We also
develop an effective image inversion scheme for reconstructing the radiance
field of an object in a real monocular image and manipulating its shape and
appearance. Experiments show that our method can successfully learn the
generative model from unstructured monocular images and well disentangle the
shape and appearance for objects (e.g., chairs) with large topological
variance. The model trained on synthetic data can faithfully reconstruct the
real object in a given single image and achieve high-quality texture and shape
editing results.

Comments:
- Accepted at Pacific Graphics 2022 & COMPUTER GRAPHICS Forum, Project
  Page: https://ziyuwang98.github.io/GDRF/

---

## PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and  Feature-metric Alignment

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-08 | Prajwal Chidananda, Saurabh Nair, Douglas Lee, Adrian Kaehler | cs.CV | [PDF](http://arxiv.org/pdf/2209.03910v1){: .btn .btn-green } |

**Abstract**: We present PixTrack, a vision based object pose tracking framework using
novel view synthesis and deep feature-metric alignment. Our evaluations
demonstrate that our method produces highly accurate, robust, and jitter-free
6DoF pose estimates of objects in RGB images without the need of any data
annotation or trajectory smoothing. Our method is also computationally
efficient making it easy to have multi-object tracking with no alteration to
our method and just using CPU multiprocessing.

---

## im2nerf: Image to Neural Radiance Field in the Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-08 | Lu Mi, Abhijit Kundu, David Ross, Frank Dellaert, Noah Snavely, Alireza Fathi | cs.CV | [PDF](http://arxiv.org/pdf/2209.04061v1){: .btn .btn-green } |

**Abstract**: We propose im2nerf, a learning framework that predicts a continuous neural
object representation given a single input image in the wild, supervised by
only segmentation output from off-the-shelf recognition methods. The standard
approach to constructing neural radiance fields takes advantage of multi-view
consistency and requires many calibrated views of a scene, a requirement that
cannot be satisfied when learning on large-scale image data in the wild. We
take a step towards addressing this shortcoming by introducing a model that
encodes the input image into a disentangled object representation that contains
a code for object shape, a code for object appearance, and an estimated camera
pose from which the object image is captured. Our model conditions a NeRF on
the predicted object representation and uses volume rendering to generate
images from novel views. We train the model end-to-end on a large collection of
input images. As the model is only provided with single-view images, the
problem is highly under-constrained. Therefore, in addition to using a
reconstruction loss on the synthesized input view, we use an auxiliary
adversarial loss on the novel rendered views. Furthermore, we leverage object
symmetry and cycle camera pose consistency. We conduct extensive quantitative
and qualitative experiments on the ShapeNet dataset as well as qualitative
experiments on Open Images dataset. We show that in all cases, im2nerf achieves
the state-of-the-art performance for novel view synthesis from a single-view
unposed image in the wild.

Comments:
- 12 pages, 8 figures, 4 tables

---

## Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D  Image Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-07 | Vadim Tschernezki, Iro Laina, Diane Larlus, Andrea Vedaldi | cs.CV | [PDF](http://arxiv.org/pdf/2209.03494v1){: .btn .btn-green } |

**Abstract**: We present Neural Feature Fusion Fields (N3F), a method that improves dense
2D image feature extractors when the latter are applied to the analysis of
multiple images reconstructible as a 3D scene. Given an image feature
extractor, for example pre-trained using self-supervision, N3F uses it as a
teacher to learn a student network defined in 3D space. The 3D student network
is similar to a neural radiance field that distills said features and can be
trained with the usual differentiable rendering machinery. As a consequence,
N3F is readily applicable to most neural rendering formulations, including
vanilla NeRF and its extensions to complex dynamic scenes. We show that our
method not only enables semantic understanding in the context of scene-specific
neural fields without the use of manual labels, but also consistently improves
over the self-supervised 2D baselines. This is demonstrated by considering
various tasks, such as 2D object retrieval, 3D segmentation, and scene editing,
in diverse sequences, including long egocentric videos in the EPIC-KITCHENS
benchmark.

Comments:
- 3DV2022, Oral. Project page: https://www.robots.ox.ac.uk/~vadim/n3f/

---

## CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural  Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-02 | Alexandra Carlson, Manikandasriram Srinivasan Ramanagopal, Nathan Tseng, Matthew Johnson-Roberson, Ram Vasudevan, Katherine A. Skinner | cs.CV | [PDF](http://arxiv.org/pdf/2209.01194v4){: .btn .btn-green } |

**Abstract**: Recent advances in neural radiance fields (NeRFs) achieve state-of-the-art
novel view synthesis and facilitate dense estimation of scene properties.
However, NeRFs often fail for large, unbounded scenes that are captured under
very sparse views with the scene content concentrated far away from the camera,
as is typical for field robotics applications. In particular, NeRF-style
algorithms perform poorly: (1) when there are insufficient views with little
pose diversity, (2) when scenes contain saturation and shadows, and (3) when
finely sampling large unbounded scenes with fine structures becomes
computationally intensive.
  This paper proposes CLONeR, which significantly improves upon NeRF by
allowing it to model large outdoor driving scenes that are observed from sparse
input sensor views. This is achieved by decoupling occupancy and color learning
within the NeRF framework into separate Multi-Layer Perceptrons (MLPs) trained
using LiDAR and camera data, respectively. In addition, this paper proposes a
novel method to build differentiable 3D Occupancy Grid Maps (OGM) alongside the
NeRF model, and leverage this occupancy grid for improved sampling of points
along a ray for volumetric rendering in metric space.
  Through extensive quantitative and qualitative experiments on scenes from the
KITTI dataset, this paper demonstrates that the proposed method outperforms
state-of-the-art NeRF models on both novel view synthesis and dense depth
prediction tasks when trained on sparse input data.

Comments:
- first two authors equally contributed

---

## Cross-Spectral Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-01 | Matteo Poggi, Pierluigi Zama Ramirez, Fabio Tosi, Samuele Salti, Stefano Mattoccia, Luigi Di Stefano | cs.CV | [PDF](http://arxiv.org/pdf/2209.00648v1){: .btn .btn-green } |

**Abstract**: We propose X-NeRF, a novel method to learn a Cross-Spectral scene
representation given images captured from cameras with different light spectrum
sensitivity, based on the Neural Radiance Fields formulation. X-NeRF optimizes
camera poses across spectra during training and exploits Normalized
Cross-Device Coordinates (NXDC) to render images of different modalities from
arbitrary viewpoints, which are aligned and at the same resolution. Experiments
on 16 forward-facing scenes, featuring color, multi-spectral and infrared
images, confirm the effectiveness of X-NeRF at modeling Cross-Spectral scene
representations.

Comments:
- 3DV 2022. Project page: https://cvlab-unibo.github.io/xnerf-web/

---

## On Quantizing Implicit Neural Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-09-01 | Cameron Gordon, Shin-Fang Chng, Lachlan MacDonald, Simon Lucey | cs.CV | [PDF](http://arxiv.org/pdf/2209.01019v1){: .btn .btn-green } |

**Abstract**: The role of quantization within implicit/coordinate neural networks is still
not fully understood. We note that using a canonical fixed quantization scheme
during training produces poor performance at low-rates due to the network
weight distributions changing over the course of training. In this work, we
show that a non-uniform quantization of neural weights can lead to significant
improvements. Specifically, we demonstrate that a clustered quantization
enables improved reconstruction. Finally, by characterising a trade-off between
quantization and network capacity, we demonstrate that it is possible (while
memory inefficient) to reconstruct signals using binary neural networks. We
demonstrate our findings experimentally on 2D image reconstruction and 3D
radiance fields; and show that simple quantization methods and architecture
search can achieve compression of NeRF to less than 16kb with minimal loss in
performance (323x smaller than the original NeRF).

Comments:
- 10 pages, 10 figures

---

## Dual-Space NeRF: Learning Animatable Avatars and Scene Lighting in  Separate Spaces

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-31 | Yihao Zhi, Shenhan Qian, Xinhao Yan, Shenghua Gao | cs.CV | [PDF](http://arxiv.org/pdf/2208.14851v1){: .btn .btn-green } |

**Abstract**: Modeling the human body in a canonical space is a common practice for
capturing and animation. But when involving the neural radiance field (NeRF),
learning a static NeRF in the canonical space is not enough because the
lighting of the body changes when the person moves even though the scene
lighting is constant. Previous methods alleviate the inconsistency of lighting
by learning a per-frame embedding, but this operation does not generalize to
unseen poses. Given that the lighting condition is static in the world space
while the human body is consistent in the canonical space, we propose a
dual-space NeRF that models the scene lighting and the human body with two MLPs
in two separate spaces. To bridge these two spaces, previous methods mostly
rely on the linear blend skinning (LBS) algorithm. However, the blending
weights for LBS of a dynamic neural field are intractable and thus are usually
memorized with another MLP, which does not generalize to novel poses. Although
it is possible to borrow the blending weights of a parametric mesh such as
SMPL, the interpolation operation introduces more artifacts. In this paper, we
propose to use the barycentric mapping, which can directly generalize to unseen
poses and surprisingly achieves superior results than LBS with neural blending
weights. Quantitative and qualitative results on the Human3.6M and the
ZJU-MoCap datasets show the effectiveness of our method.

Comments:
- Accepted by 3DV 2022

---

## A Portable Multiscopic Camera for Novel View and Time Synthesis in  Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-30 | Tianjia Zhang, Yuen-Fui Lau, Qifeng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2208.14433v1){: .btn .btn-green } |

**Abstract**: We present a portable multiscopic camera system with a dedicated model for
novel view and time synthesis in dynamic scenes. Our goal is to render
high-quality images for a dynamic scene from any viewpoint at any time using
our portable multiscopic camera. To achieve such novel view and time synthesis,
we develop a physical multiscopic camera equipped with five cameras to train a
neural radiance field (NeRF) in both time and spatial domains for dynamic
scenes. Our model maps a 6D coordinate (3D spatial position, 1D temporal
coordinate, and 2D viewing direction) to view-dependent and time-varying
emitted radiance and volume density. Volume rendering is applied to render a
photo-realistic image at a specified camera pose and time. To improve the
robustness of our physical camera, we propose a camera parameter optimization
module and a temporal frame interpolation module to promote information
propagation across time. We conduct experiments on both real-world and
synthetic datasets to evaluate our system, and the results show that our
approach outperforms alternative solutions qualitatively and quantitatively.
Our code and dataset are available at https://yuenfuilau.github.io.

Comments:
- To be presented at IROS2022

---

## Volume Rendering Digest (for NeRF)

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-29 | Andrea Tagliasacchi, Ben Mildenhall | cs.CV | [PDF](http://arxiv.org/pdf/2209.02417v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields employ simple volume rendering as a way to overcome
the challenges of differentiating through ray-triangle intersections by
leveraging a probabilistic notion of visibility. This is achieved by assuming
the scene is composed by a cloud of light-emitting particles whose density
changes in space. This technical report summarizes the derivations for
differentiable volume rendering. It is a condensed version of previous reports,
but rewritten in the context of NeRF, and adopting its commonly used notation.

Comments:
- Overleaf: https://www.overleaf.com/read/fkhpkzxhnyws

---

## Training and Tuning Generative Neural Radiance Fields for  Attribute-Conditional 3D-Aware Face Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-26 | Jichao Zhang, Aliaksandr Siarohin, Yahui Liu, Hao Tang, Nicu Sebe, Wei Wang | cs.CV | [PDF](http://arxiv.org/pdf/2208.12550v2){: .btn .btn-green } |

**Abstract**: Generative Neural Radiance Fields (GNeRF) based 3D-aware GANs have
demonstrated remarkable capabilities in generating high-quality images while
maintaining strong 3D consistency. Notably, significant advancements have been
made in the domain of face generation. However, most existing models prioritize
view consistency over disentanglement, resulting in limited semantic/attribute
control during generation. To address this limitation, we propose a conditional
GNeRF model incorporating specific attribute labels as input to enhance the
controllability and disentanglement abilities of 3D-aware generative models.
Our approach builds upon a pre-trained 3D-aware face model, and we introduce a
Training as Init and Optimizing for Tuning (TRIOT) method to train a
conditional normalized flow module to enable the facial attribute editing, then
optimize the latent vector to improve attribute-editing precision further. Our
extensive experiments demonstrate that our model produces high-quality edits
with superior view consistency while preserving non-target regions. Code is
available at https://github.com/zhangqianhui/TT-GNeRF.

Comments:
- 13 pages

---

## E-NeRF: Neural Radiance Fields from a Moving Event Camera

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-24 | Simon Klenk, Lukas Koestler, Davide Scaramuzza, Daniel Cremers | cs.CV | [PDF](http://arxiv.org/pdf/2208.11300v2){: .btn .btn-green } |

**Abstract**: Estimating neural radiance fields (NeRFs) from "ideal" images has been
extensively studied in the computer vision community. Most approaches assume
optimal illumination and slow camera motion. These assumptions are often
violated in robotic applications, where images may contain motion blur, and the
scene may not have suitable illumination. This can cause significant problems
for downstream tasks such as navigation, inspection, or visualization of the
scene. To alleviate these problems, we present E-NeRF, the first method which
estimates a volumetric scene representation in the form of a NeRF from a
fast-moving event camera. Our method can recover NeRFs during very fast motion
and in high-dynamic-range conditions where frame-based approaches fail. We show
that rendering high-quality frames is possible by only providing an event
stream as input. Furthermore, by combining events and frames, we can estimate
NeRFs of higher quality than state-of-the-art approaches under severe motion
blur. We also show that combining events and frames can overcome failure cases
of NeRF estimation in scenarios where only a few input views are available
without requiring additional regularization.

Comments:
- revised RAL version + added suppl. material

---

## PeRFception: Perception using Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-24 | Yoonwoo Jeong, Seungjoo Shin, Junha Lee, Christopher Choy, Animashree Anandkumar, Minsu Cho, Jaesik Park | cs.CV | [PDF](http://arxiv.org/pdf/2208.11537v1){: .btn .btn-green } |

**Abstract**: The recent progress in implicit 3D representation, i.e., Neural Radiance
Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible
in a differentiable manner. This new representation can effectively convey the
information of hundreds of high-resolution images in one compact format and
allows photorealistic synthesis of novel views. In this work, using the variant
of NeRF called Plenoxels, we create the first large-scale implicit
representation datasets for perception tasks, called the PeRFception, which
consists of two parts that incorporate both object-centric and scene-centric
scans for classification and segmentation. It shows a significant memory
compression rate (96.4\%) from the original dataset, while containing both 2D
and 3D information in a unified form. We construct the classification and
segmentation models that directly take as input this implicit format and also
propose a novel augmentation technique to avoid overfitting on backgrounds of
images. The code and data are publicly available in
https://postech-cvlab.github.io/PeRFception .

Comments:
- Project Page: https://postech-cvlab.github.io/PeRFception/

---

## Neural Capture of Animatable 3D Human from Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-18 | Gusi Te, Xiu Li, Xiao Li, Jinglu Wang, Wei Hu, Yan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2208.08728v1){: .btn .btn-green } |

**Abstract**: We present a novel paradigm of building an animatable 3D human representation
from a monocular video input, such that it can be rendered in any unseen poses
and views. Our method is based on a dynamic Neural Radiance Field (NeRF) rigged
by a mesh-based parametric 3D human model serving as a geometry proxy. Previous
methods usually rely on multi-view videos or accurate 3D geometry information
as additional inputs; besides, most methods suffer from degraded quality when
generalized to unseen poses. We identify that the key to generalization is a
good input embedding for querying dynamic NeRF: A good input embedding should
define an injective mapping in the full volumetric space, guided by surface
mesh deformation under pose variation. Based on this observation, we propose to
embed the input query with its relationship to local surface regions spanned by
a set of geodesic nearest neighbors on mesh vertices. By including both
position and relative distance information, our embedding defines a
distance-preserved deformation mapping and generalizes well to unseen poses. To
reduce the dependency on additional inputs, we first initialize per-frame 3D
meshes using off-the-shelf tools and then propose a pipeline to jointly
optimize NeRF and refine the initial mesh. Extensive experiments show our
method can synthesize plausible human rendering results under unseen poses and
views.

Comments:
- ECCV 2022

---

## Casual Indoor HDR Radiance Capture from Omnidirectional Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-16 | Pulkit Gera, Mohammad Reza Karimi Dastjerdi, Charles Renaud, P. J. Narayanan, Jean-François Lalonde | cs.CV | [PDF](http://arxiv.org/pdf/2208.07903v2){: .btn .btn-green } |

**Abstract**: We present PanoHDR-NeRF, a neural representation of the full HDR radiance
field of an indoor scene, and a pipeline to capture it casually, without
elaborate setups or complex capture protocols. First, a user captures a low
dynamic range (LDR) omnidirectional video of the scene by freely waving an
off-the-shelf camera around the scene. Then, an LDR2HDR network uplifts the
captured LDR frames to HDR, which are used to train a tailored NeRF++ model.
The resulting PanoHDR-NeRF can render full HDR images from any location of the
scene. Through experiments on a novel test dataset of real scenes with the
ground truth HDR radiance captured at locations not seen during training, we
show that PanoHDR-NeRF predicts plausible HDR radiance from any scene point. We
also show that the predicted radiance can synthesize correct lighting effects,
enabling the augmentation of indoor scenes with synthetic objects that are lit
correctly. Datasets and code are available at
https://lvsn.github.io/PanoHDR-NeRF/.

Comments:
- BMVC 2022

---

## DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-15 | Bing Wang, Lu Chen, Bo Yang | cs.CV | [PDF](http://arxiv.org/pdf/2208.07227v2){: .btn .btn-green } |

**Abstract**: In this paper, we study the problem of 3D scene geometry decomposition and
manipulation from 2D views. By leveraging the recent implicit neural
representation techniques, particularly the appealing neural radiance fields,
we introduce an object field component to learn unique codes for all individual
objects in 3D space only from 2D supervision. The key to this component is a
series of carefully designed loss functions to enable every 3D point,
especially in non-occupied space, to be effectively optimized even without 3D
labels. In addition, we introduce an inverse query algorithm to freely
manipulate any specified 3D object shape in the learned scene representation.
Notably, our manipulation algorithm can explicitly tackle key issues such as
object collisions and visual occlusions. Our method, called DM-NeRF, is among
the first to simultaneously reconstruct, decompose, manipulate and render
complex 3D scenes in a single pipeline. Extensive experiments on three datasets
clearly show that our method can accurately decompose all 3D objects from 2D
views, allowing any interested object to be freely manipulated in 3D space such
as translation, rotation, size adjustment, and deformation.

Comments:
- ICLR 2023. Our data and code are available at:
  https://github.com/vLAR-group/DM-NeRF

---

## UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance  Fields for 3D Scene

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-15 | Yaosen Chen, Qi Yuan, Zhiqiang Li, Yuegen Liu, Wei Wang, Chaoping Xie, Xuming Wen, Qien Yu | cs.CV | [PDF](http://arxiv.org/pdf/2208.07059v2){: .btn .btn-green } |

**Abstract**: 3D scenes photorealistic stylization aims to generate photorealistic images
from arbitrary novel views according to a given style image while ensuring
consistency when rendering from different viewpoints. Some existing stylization
methods with neural radiance fields can effectively predict stylized scenes by
combining the features of the style image with multi-view images to train 3D
scenes. However, these methods generate novel view images that contain
objectionable artifacts. Besides, they cannot achieve universal photorealistic
stylization for a 3D scene. Therefore, a styling image must retrain a 3D scene
representation network based on a neural radiation field. We propose a novel 3D
scene photorealistic style transfer framework to address these issues. It can
realize photorealistic 3D scene style transfer with a 2D style image. We first
pre-trained a 2D photorealistic style transfer network, which can meet the
photorealistic style transfer between any given content image and style image.
Then, we use voxel features to optimize a 3D scene and get the geometric
representation of the scene. Finally, we jointly optimize a hyper network to
realize the scene photorealistic style transfer of arbitrary style images. In
the transfer stage, we use a pre-trained 2D photorealistic network to constrain
the photorealistic style of different views and different style images in the
3D scene. The experimental results show that our method not only realizes the
3D photorealistic style transfer of arbitrary style images but also outperforms
the existing methods in terms of visual quality and consistency. Project
page:https://semchan.github.io/UPST_NeRF.

Comments:
- arXiv admin note: text overlap with arXiv:2205.12183 by other authors

---

## OmniVoxel: A Fast and Precise Reconstruction Method of Omnidirectional  Neural Radiance Field



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-12 | Qiaoge Li, Itsuki Ueda, Chun Xie, Hidehiko Shishido, Itaru Kitahara | cs.CV | [PDF](http://arxiv.org/pdf/2208.06335v1){: .btn .btn-green } |

**Abstract**: This paper proposes a method to reconstruct the neural radiance field with
equirectangular omnidirectional images. Implicit neural scene representation
with a radiance field can reconstruct the 3D shape of a scene continuously
within a limited spatial area. However, training a fully implicit
representation on commercial PC hardware requires a lot of time and computing
resources (15 $\sim$ 20 hours per scene). Therefore, we propose a method to
accelerate this process significantly (20 $\sim$ 40 minutes per scene). Instead
of using a fully implicit representation of rays for radiance field
reconstruction, we adopt feature voxels that contain density and color features
in tensors. Considering omnidirectional equirectangular input and the camera
layout, we use spherical voxelization for representation instead of cubic
representation. Our voxelization method could balance the reconstruction
quality of the inner scene and outer scene. In addition, we adopt the
axis-aligned positional encoding method on the color features to increase the
total image quality. Our method achieves satisfying empirical performance on
synthetic datasets with random camera poses. Moreover, we test our method with
real scenes which contain complex geometries and also achieve state-of-the-art
performance. Our code and complete dataset will be released at the same time as
the paper publication.

Comments:
- will be appeared in GCCE 2022

---

## RelPose: Predicting Probabilistic Relative Rotation for Single Objects  in the Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-11 | Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani | cs.CV | [PDF](http://arxiv.org/pdf/2208.05963v2){: .btn .btn-green } |

**Abstract**: We describe a data-driven method for inferring the camera viewpoints given
multiple images of an arbitrary object. This task is a core component of
classic geometric pipelines such as SfM and SLAM, and also serves as a vital
pre-processing requirement for contemporary neural approaches (e.g. NeRF) to
object reconstruction and view synthesis. In contrast to existing
correspondence-driven methods that do not perform well given sparse views, we
propose a top-down prediction based approach for estimating camera viewpoints.
Our key technical insight is the use of an energy-based formulation for
representing distributions over relative camera rotations, thus allowing us to
explicitly represent multiple camera modes arising from object symmetries or
views. Leveraging these relative predictions, we jointly estimate a consistent
set of camera rotations from multiple images. We show that our approach
outperforms state-of-the-art SfM and SLAM methods given sparse images on both
seen and unseen categories. Further, our probabilistic approach significantly
outperforms directly regressing relative poses, suggesting that modeling
multimodality is important for coherent joint reconstruction. We demonstrate
that our system can be a stepping stone toward in-the-wild reconstruction from
multi-view datasets. The project page with code and videos can be found at
https://jasonyzhang.com/relpose.

Comments:
- In ECCV 2022. V2: updated references

---

## FDNeRF: Few-shot Dynamic Neural Radiance Fields for Face Reconstruction  and Expression Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-11 | Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao | cs.CV | [PDF](http://arxiv.org/pdf/2208.05751v2){: .btn .btn-green } |

**Abstract**: We propose a Few-shot Dynamic Neural Radiance Field (FDNeRF), the first
NeRF-based method capable of reconstruction and expression editing of 3D faces
based on a small number of dynamic images. Unlike existing dynamic NeRFs that
require dense images as input and can only be modeled for a single identity,
our method enables face reconstruction across different persons with few-shot
inputs. Compared to state-of-the-art few-shot NeRFs designed for modeling
static scenes, the proposed FDNeRF accepts view-inconsistent dynamic inputs and
supports arbitrary facial expression editing, i.e., producing faces with novel
expressions beyond the input ones. To handle the inconsistencies between
dynamic inputs, we introduce a well-designed conditional feature warping (CFW)
module to perform expression conditioned warping in 2D feature space, which is
also identity adaptive and 3D constrained. As a result, features of different
expressions are transformed into the target ones. We then construct a radiance
field based on these view-consistent features and use volumetric rendering to
synthesize novel views of the modeled faces. Extensive experiments with
quantitative and qualitative evaluation demonstrate that our method outperforms
existing dynamic and few-shot NeRFs on both 3D face reconstruction and
expression editing tasks. Code is available at
https://github.com/FDNeRF/FDNeRF.

Comments:
- Accepted at SIGGRAPH Asia 2022. Project page:
  https://fdnerf.github.io

---

## Cascaded and Generalizable Neural Radiance Fields for Fast View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-09 | Phong Nguyen-Ha, Lam Huynh, Esa Rahtu, Jiri Matas, Janne Heikkila | cs.CV | [PDF](http://arxiv.org/pdf/2208.04717v2){: .btn .btn-green } |

**Abstract**: We present CG-NeRF, a cascade and generalizable neural radiance fields method
for view synthesis. Recent generalizing view synthesis methods can render
high-quality novel views using a set of nearby input views. However, the
rendering speed is still slow due to the nature of uniformly-point sampling of
neural radiance fields. Existing scene-specific methods can train and render
novel views efficiently but can not generalize to unseen data. Our approach
addresses the problems of fast and generalizing view synthesis by proposing two
novel modules: a coarse radiance fields predictor and a convolutional-based
neural renderer. This architecture infers consistent scene geometry based on
the implicit neural fields and renders new views efficiently using a single
GPU. We first train CG-NeRF on multiple 3D scenes of the DTU dataset, and the
network can produce high-quality and accurate novel views on unseen real and
synthetic data using only photometric losses. Moreover, our method can leverage
a denser set of reference images of a single scene to produce accurate novel
views without relying on additional explicit representations and still
maintains the high-speed rendering of the pre-trained model. Experimental
results show that CG-NeRF outperforms state-of-the-art generalizable neural
rendering methods on various synthetic and real datasets.

Comments:
- Accepted at IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)

---

## 360Roam: Real-Time Indoor Roaming Using Geometry-Aware 360$^\circ$  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-04 | Huajian Huang, Yingshu Chen, Tianjia Zhang, Sai-Kit Yeung | cs.CV | [PDF](http://arxiv.org/pdf/2208.02705v2){: .btn .btn-green } |

**Abstract**: Virtual tour among sparse 360$^\circ$ images is widely used while hindering
smooth and immersive roaming experiences. The emergence of Neural Radiance
Field (NeRF) has showcased significant progress in synthesizing novel views,
unlocking the potential for immersive scene exploration. Nevertheless, previous
NeRF works primarily focused on object-centric scenarios, resulting in
noticeable performance degradation when applied to outward-facing and
large-scale scenes due to limitations in scene parameterization. To achieve
seamless and real-time indoor roaming, we propose a novel approach using
geometry-aware radiance fields with adaptively assigned local radiance fields.
Initially, we employ multiple 360$^\circ$ images of an indoor scene to
progressively reconstruct explicit geometry in the form of a probabilistic
occupancy map, derived from a global omnidirectional radiance field.
Subsequently, we assign local radiance fields through an adaptive
divide-and-conquer strategy based on the recovered geometry. By incorporating
geometry-aware sampling and decomposition of the global radiance field, our
system effectively utilizes positional encoding and compact neural networks to
enhance rendering quality and speed. Additionally, the extracted floorplan of
the scene aids in providing visual guidance, contributing to a realistic
roaming experience. To demonstrate the effectiveness of our system, we curated
a diverse dataset of 360$^\circ$ images encompassing various real-life scenes,
on which we conducted extensive experiments. Quantitative and qualitative
comparisons against baseline approaches illustrated the superior performance of
our system in large-scale indoor scene roaming.

---

## T4DT: Tensorizing Time for Learning Temporal 3D Visual Data

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-02 | Mikhail Usvyatsov, Rafael Ballester-Rippoll, Lina Bashaeva, Konrad Schindler, Gonzalo Ferrer, Ivan Oseledets | cs.CV | [PDF](http://arxiv.org/pdf/2208.01421v2){: .btn .btn-green } |

**Abstract**: Unlike 2D raster images, there is no single dominant representation for 3D
visual data processing. Different formats like point clouds, meshes, or
implicit functions each have their strengths and weaknesses. Still, grid
representations such as signed distance functions have attractive properties
also in 3D. In particular, they offer constant-time random access and are
eminently suitable for modern machine learning. Unfortunately, the storage size
of a grid grows exponentially with its dimension. Hence they often exceed
memory limits even at moderate resolution. This work proposes using low-rank
tensor formats, including the Tucker, tensor train, and quantics tensor train
decompositions, to compress time-varying 3D data. Our method iteratively
computes, voxelizes, and compresses each frame's truncated signed distance
function and applies tensor rank truncation to condense all frames into a
single, compressed tensor that represents the entire 4D scene. We show that
low-rank tensor compression is extremely compact to store and query
time-varying signed distance functions. It significantly reduces the memory
footprint of 4D scenes while remarkably preserving their geometric quality.
Unlike existing, iterative learning-based approaches like DeepSDF and NeRF, our
method uses a closed-form algorithm with theoretical guarantees.

---

## DoF-NeRF: Depth-of-Field Meets Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-08-01 | Zijin Wu, Xingyi Li, Juewen Peng, Hao Lu, Zhiguo Cao, Weicai Zhong | cs.CV | [PDF](http://arxiv.org/pdf/2208.00945v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) and its variants have exhibited great success on
representing 3D scenes and synthesizing photo-realistic novel views. However,
they are generally based on the pinhole camera model and assume all-in-focus
inputs. This limits their applicability as images captured from the real world
often have finite depth-of-field (DoF). To mitigate this issue, we introduce
DoF-NeRF, a novel neural rendering approach that can deal with shallow DoF
inputs and can simulate DoF effect. In particular, it extends NeRF to simulate
the aperture of lens following the principles of geometric optics. Such a
physical guarantee allows DoF-NeRF to operate views with different focus
configurations. Benefiting from explicit aperture modeling, DoF-NeRF also
enables direct manipulation of DoF effect by adjusting virtual aperture and
focus parameters. It is plug-and-play and can be inserted into NeRF-based
frameworks. Experiments on synthetic and real-world datasets show that,
DoF-NeRF not only performs comparably with NeRF in the all-in-focus setting,
but also can synthesize all-in-focus novel views conditioned on shallow DoF
inputs. An interesting application of DoF-NeRF to DoF rendering is also
demonstrated. The source code will be made available at
https://github.com/zijinwuzijin/DoF-NeRF.

Comments:
- Accepted by ACMMM 2022

---

## MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient  Neural Field Rendering on Mobile Architectures

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-30 | Zhiqin Chen, Thomas Funkhouser, Peter Hedman, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2208.00277v5){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have demonstrated amazing ability to
synthesize images of 3D scenes from novel views. However, they rely upon
specialized volumetric rendering algorithms based on ray marching that are
mismatched to the capabilities of widely deployed graphics hardware. This paper
introduces a new NeRF representation based on textured polygons that can
synthesize novel images efficiently with standard rendering pipelines. The NeRF
is represented as a set of polygons with textures representing binary opacities
and feature vectors. Traditional rendering of the polygons with a z-buffer
yields an image with features at every pixel, which are interpreted by a small,
view-dependent MLP running in a fragment shader to produce a final pixel color.
This approach enables NeRFs to be rendered with the traditional polygon
rasterization pipeline, which provides massive pixel-level parallelism,
achieving interactive frame rates on a wide range of compute platforms,
including mobile phones.

Comments:
- CVPR 2023. Project page: https://mobile-nerf.github.io, code:
  https://github.com/google-research/jax3d/tree/main/jax3d/projects/mobilenerf

---

## Distilled Low Rank Neural Radiance Field with Quantization for Light  Field Compression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-30 | Jinglei Shi, Christine Guillemot | cs.CV | [PDF](http://arxiv.org/pdf/2208.00164v3){: .btn .btn-green } |

**Abstract**: We propose in this paper a Quantized Distilled Low-Rank Neural Radiance Field
(QDLR-NeRF) representation for the task of light field compression. While
existing compression methods encode the set of light field sub-aperture images,
our proposed method learns an implicit scene representation in the form of a
Neural Radiance Field (NeRF), which also enables view synthesis. To reduce its
size, the model is first learned under a Low-Rank (LR) constraint using a
Tensor Train (TT) decomposition within an Alternating Direction Method of
Multipliers (ADMM) optimization framework. To further reduce the model's size,
the components of the tensor train decomposition need to be quantized. However,
simultaneously considering the optimization of the NeRF model with both the
low-rank constraint and rate-constrained weight quantization is challenging. To
address this difficulty, we introduce a network distillation operation that
separates the low-rank approximation and the weight quantization during network
training. The information from the initial LR-constrained NeRF (LR-NeRF) is
distilled into a model of much smaller dimension (DLR-NeRF) based on the TT
decomposition of the LR-NeRF. We then learn an optimized global codebook to
quantize all TT components, producing the final QDLR-NeRF. Experimental results
show that our proposed method yields better compression efficiency compared to
state-of-the-art methods, and it additionally has the advantage of allowing the
synthesis of any light field view with high quality.

---

## End-to-end View Synthesis via NeRF Attention

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-29 | Zelin Zhao, Jiaya Jia | cs.CV | [PDF](http://arxiv.org/pdf/2207.14741v3){: .btn .btn-green } |

**Abstract**: In this paper, we present a simple seq2seq formulation for view synthesis
where we take a set of ray points as input and output colors corresponding to
the rays. Directly applying a standard transformer on this seq2seq formulation
has two limitations. First, the standard attention cannot successfully fit the
volumetric rendering procedure, and therefore high-frequency components are
missing in the synthesized views. Second, applying global attention to all rays
and pixels is extremely inefficient. Inspired by the neural radiance field
(NeRF), we propose the NeRF attention (NeRFA) to address the above problems. On
the one hand, NeRFA considers the volumetric rendering equation as a soft
feature modulation procedure. In this way, the feature modulation enhances the
transformers with the NeRF-like inductive bias. On the other hand, NeRFA
performs multi-stage attention to reduce the computational overhead.
Furthermore, the NeRFA model adopts the ray and pixel transformers to learn the
interactions between rays and pixels. NeRFA demonstrates superior performance
over NeRF and NerFormer on four datasets: DeepVoxels, Blender, LLFF, and CO3D.
Besides, NeRFA establishes a new state-of-the-art under two settings: the
single-scene view synthesis and the category-centric novel view synthesis.

Comments:
- Fixed reference formatting issues

---

## Neural Density-Distance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-29 | Itsuki Ueda, Yoshihiro Fukuhara, Hirokatsu Kataoka, Hiroaki Aizawa, Hidehiko Shishido, Itaru Kitahara | cs.CV | [PDF](http://arxiv.org/pdf/2207.14455v1){: .btn .btn-green } |

**Abstract**: The success of neural fields for 3D vision tasks is now indisputable.
Following this trend, several methods aiming for visual localization (e.g.,
SLAM) have been proposed to estimate distance or density fields using neural
fields. However, it is difficult to achieve high localization performance by
only density fields-based methods such as Neural Radiance Field (NeRF) since
they do not provide density gradient in most empty regions. On the other hand,
distance field-based methods such as Neural Implicit Surface (NeuS) have
limitations in objects' surface shapes. This paper proposes Neural
Density-Distance Field (NeDDF), a novel 3D representation that reciprocally
constrains the distance and density fields. We extend distance field
formulation to shapes with no explicit boundary surface, such as fur or smoke,
which enable explicit conversion from distance field to density field.
Consistent distance and density fields realized by explicit conversion enable
both robustness to initial values and high-quality registration. Furthermore,
the consistency between fields allows fast convergence from sparse point
clouds. Experiments show that NeDDF can achieve high localization performance
while providing comparable results to NeRF on novel view synthesis. The code is
available at https://github.com/ueda0319/neddf.

Comments:
- ECCV 2022 (poster). project page: https://ueda0319.github.io/neddf/

---

## Is Attention All That NeRF Needs?

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-27 | Mukund Varma T, Peihao Wang, Xuxi Chen, Tianlong Chen, Subhashini Venugopalan, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2207.13298v3){: .btn .btn-green } |

**Abstract**: We present Generalizable NeRF Transformer (GNT), a transformer-based
architecture that reconstructs Neural Radiance Fields (NeRFs) and learns to
renders novel views on the fly from source views. While prior works on NeRFs
optimize a scene representation by inverting a handcrafted rendering equation,
GNT achieves neural representation and rendering that generalizes across scenes
using transformers at two stages. (1) The view transformer leverages multi-view
geometry as an inductive bias for attention-based scene representation, and
predicts coordinate-aligned features by aggregating information from epipolar
lines on the neighboring views. (2) The ray transformer renders novel views
using attention to decode the features from the view transformer along the
sampled points during ray marching. Our experiments demonstrate that when
optimized on a single scene, GNT can successfully reconstruct NeRF without an
explicit rendering formula due to the learned ray renderer. When trained on
multiple scenes, GNT consistently achieves state-of-the-art performance when
transferring to unseen scenes and outperform all other methods by ~10% on
average. Our analysis of the learned attention maps to infer depth and
occlusion indicate that attention enables learning a physically-grounded
rendering. Our results show the promise of transformers as a universal modeling
tool for graphics. Please refer to our project page for video results:
https://vita-group.github.io/GNT/.

Comments:
- International Conference on Learning Representations (ICLR), 2023

---

## Deforming Radiance Fields with Cages



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-25 | Tianhan Xu, Tatsuya Harada | cs.CV | [PDF](http://arxiv.org/pdf/2207.12298v1){: .btn .btn-green } |

**Abstract**: Recent advances in radiance fields enable photorealistic rendering of static
or dynamic 3D scenes, but still do not support explicit deformation that is
used for scene manipulation or animation. In this paper, we propose a method
that enables a new type of deformation of the radiance field: free-form
radiance field deformation. We use a triangular mesh that encloses the
foreground object called cage as an interface, and by manipulating the cage
vertices, our approach enables the free-form deformation of the radiance field.
The core of our approach is cage-based deformation which is commonly used in
mesh deformation. We propose a novel formulation to extend it to the radiance
field, which maps the position and the view direction of the sampling points
from the deformed space to the canonical space, thus enabling the rendering of
the deformed scene. The deformation results of the synthetic datasets and the
real-world datasets demonstrate the effectiveness of our approach.

Comments:
- ECCV 2022. Project page: https://xth430.github.io/deforming-nerf/

---

## Learning Generalizable Light Field Networks from Few Images



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-24 | Qian Li, Franck Multon, Adnane Boukhayma | cs.CV | [PDF](http://arxiv.org/pdf/2207.11757v1){: .btn .btn-green } |

**Abstract**: We explore a new strategy for few-shot novel view synthesis based on a neural
light field representation. Given a target camera pose, an implicit neural
network maps each ray to its target pixel's color directly. The network is
conditioned on local ray features generated by coarse volumetric rendering from
an explicit 3D feature volume. This volume is built from the input images using
a 3D ConvNet. Our method achieves competitive performances on synthetic and
real MVS data with respect to state-of-the-art neural radiance field based
competition, while offering a 100 times faster rendering.

---

## Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-24 | Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou, Jiwen Lu | cs.CV | [PDF](http://arxiv.org/pdf/2207.11770v1){: .btn .btn-green } |

**Abstract**: Talking head synthesis is an emerging technology with wide applications in
film dubbing, virtual avatars and online education. Recent NeRF-based methods
generate more natural talking videos, as they better capture the 3D structural
information of faces. However, a specific model needs to be trained for each
identity with a large dataset. In this paper, we propose Dynamic Facial
Radiance Fields (DFRF) for few-shot talking head synthesis, which can rapidly
generalize to an unseen identity with few training data. Different from the
existing NeRF-based methods which directly encode the 3D geometry and
appearance of a specific person into the network, our DFRF conditions face
radiance field on 2D appearance images to learn the face prior. Thus the facial
radiance field can be flexibly adjusted to the new identity with few reference
images. Additionally, for better modeling of the facial deformations, we
propose a differentiable face warping module conditioned on audio signals to
deform all reference images to the query space. Extensive experiments show that
with only tens of seconds of training clip available, our proposed DFRF can
synthesize natural and high-quality audio-driven talking head videos for novel
identities with only 40k iterations. We highly recommend readers view our
supplementary video for intuitive comparisons. Code is available in
https://sstzal.github.io/DFRF/.

Comments:
- Accepted by ECCV 2022. Project page: https://sstzal.github.io/DFRF/

---

## PS-NeRF: Neural Inverse Rendering for Multi-view Photometric Stereo

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-23 | Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang Chen, Kwan-Yee K. Wong | cs.CV | [PDF](http://arxiv.org/pdf/2207.11406v2){: .btn .btn-green } |

**Abstract**: Traditional multi-view photometric stereo (MVPS) methods are often composed
of multiple disjoint stages, resulting in noticeable accumulated errors. In
this paper, we present a neural inverse rendering method for MVPS based on
implicit representation. Given multi-view images of a non-Lambertian object
illuminated by multiple unknown directional lights, our method jointly
estimates the geometry, materials, and lights. Our method first employs
multi-light images to estimate per-view surface normal maps, which are used to
regularize the normals derived from the neural radiance field. It then jointly
optimizes the surface normals, spatially-varying BRDFs, and lights based on a
shadow-aware differentiable rendering layer. After optimization, the
reconstructed object can be used for novel-view rendering, relighting, and
material editing. Experiments on both synthetic and real datasets demonstrate
that our method achieves far more accurate shape reconstruction than existing
MVPS and neural rendering methods. Our code and model can be found at
https://ywq.github.io/psnerf.

Comments:
- ECCV 2022, Project page: https://ywq.github.io/psnerf

---

## Neural-Sim: Learning to Generate Training Data with NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-22 | Yunhao Ge, Harkirat Behl, Jiashu Xu, Suriya Gunasekar, Neel Joshi, Yale Song, Xin Wang, Laurent Itti, Vibhav Vineet | cs.CV | [PDF](http://arxiv.org/pdf/2207.11368v1){: .btn .btn-green } |

**Abstract**: Training computer vision models usually requires collecting and labeling vast
amounts of imagery under a diverse set of scene configurations and properties.
This process is incredibly time-consuming, and it is challenging to ensure that
the captured data distribution maps well to the target domain of an application
scenario. Recently, synthetic data has emerged as a way to address both of
these issues. However, existing approaches either require human experts to
manually tune each scene property or use automatic methods that provide little
to no control; this requires rendering large amounts of random data variations,
which is slow and is often suboptimal for the target domain. We present the
first fully differentiable synthetic data pipeline that uses Neural Radiance
Fields (NeRFs) in a closed-loop with a target application's loss function. Our
approach generates data on-demand, with no human labor, to maximize accuracy
for a target task. We illustrate the effectiveness of our method on synthetic
and real-world object detection tasks. We also introduce a new
"YCB-in-the-Wild" dataset and benchmark that provides a test scenario for
object detection with varied poses in real-world environments.

Comments:
- ECCV 2022

---

## Generalizable Patch-Based Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-21 | Mohammed Suhail, Carlos Esteves, Leonid Sigal, Ameesh Makadia | cs.CV | [PDF](http://arxiv.org/pdf/2207.10662v2){: .btn .btn-green } |

**Abstract**: Neural rendering has received tremendous attention since the advent of Neural
Radiance Fields (NeRF), and has pushed the state-of-the-art on novel-view
synthesis considerably. The recent focus has been on models that overfit to a
single scene, and the few attempts to learn models that can synthesize novel
views of unseen scenes mostly consist of combining deep convolutional features
with a NeRF-like model. We propose a different paradigm, where no deep features
and no NeRF-like volume rendering are needed. Our method is capable of
predicting the color of a target ray in a novel scene directly, just from a
collection of patches sampled from the scene. We first leverage epipolar
geometry to extract patches along the epipolar lines of each reference view.
Each patch is linearly projected into a 1D feature vector and a sequence of
transformers process the collection. For positional encoding, we parameterize
rays as in a light field representation, with the crucial difference that the
coordinates are canonicalized with respect to the target ray, which makes our
method independent of the reference frame and improves generalization. We show
that our approach outperforms the state-of-the-art on novel view synthesis of
unseen scenes even when being trained with considerably less data than prior
work.

Comments:
- Project Page with code and results at
  https://mohammedsuhail.net/gen_patch_neural_rendering/

---

## AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-21 | Andreas Kurz, Thomas Neff, Zhaoyang Lv, Michael Zollhöfer, Markus Steinberger | cs.CV | [PDF](http://arxiv.org/pdf/2207.10312v2){: .btn .btn-green } |

**Abstract**: Novel view synthesis has recently been revolutionized by learning neural
radiance fields directly from sparse observations. However, rendering images
with this new paradigm is slow due to the fact that an accurate quadrature of
the volume rendering equation requires a large number of samples for each ray.
Previous work has mainly focused on speeding up the network evaluations that
are associated with each sample point, e.g., via caching of radiance values
into explicit spatial data structures, but this comes at the expense of model
compactness. In this paper, we propose a novel dual-network architecture that
takes an orthogonal direction by learning how to best reduce the number of
required sample points. To this end, we split our network into a sampling and
shading network that are jointly trained. Our training scheme employs fixed
sample positions along each ray, and incrementally introduces sparsity
throughout training to achieve high quality even at low sample counts. After
fine-tuning with the target number of samples, the resulting compact neural
representation can be rendered in real-time. Our experiments demonstrate that
our approach outperforms concurrent compact neural representations in terms of
quality and frame rate and performs on par with highly efficient hybrid
representations. Code and supplementary material is available at
https://thomasneff.github.io/adanerf.

Comments:
- ECCV 2022. Project page: https://thomasneff.github.io/adanerf

---

## Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for  Editable Portrait Image Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-21 | Jeong-gi Kwak, Yuanming Li, Dongsik Yoon, Donghyeon Kim, David Han, Hanseok Ko | cs.CV | [PDF](http://arxiv.org/pdf/2207.10257v2){: .btn .btn-green } |

**Abstract**: Over the years, 2D GANs have achieved great successes in photorealistic
portrait generation. However, they lack 3D understanding in the generation
process, thus they suffer from multi-view inconsistency problem. To alleviate
the issue, many 3D-aware GANs have been proposed and shown notable results, but
3D GANs struggle with editing semantic attributes. The controllability and
interpretability of 3D GANs have not been much explored. In this work, we
propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware
GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of
discovering semantic attributes during training and controlling them in an
unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN
to obtain a high-fidelity 3D-controllable generator. Unlike existing
latent-based methods allowing implicit pose control, the proposed
3D-controllable StyleGAN enables explicit pose control over portrait
generation. This distillation allows direct compatibility between 3D control
and many StyleGAN-based techniques (e.g., inversion and stylization), and also
brings an advantage in terms of computational resources. Our codes are
available at https://github.com/jgkwak95/SURF-GAN.

Comments:
- ECCV 2022, project page: https://jgkwak95.github.io/surfgan/

---

## NDF: Neural Deformable Fields for Dynamic Human Modelling



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-19 | Ruiqi Zhang, Jie Chen | cs.CV | [PDF](http://arxiv.org/pdf/2207.09193v1){: .btn .btn-green } |

**Abstract**: We propose Neural Deformable Fields (NDF), a new representation for dynamic
human digitization from a multi-view video. Recent works proposed to represent
a dynamic human body with shared canonical neural radiance fields which links
to the observation space with deformation fields estimations. However, the
learned canonical representation is static and the current design of the
deformation fields is not able to represent large movements or detailed
geometry changes. In this paper, we propose to learn a neural deformable field
wrapped around a fitted parametric body model to represent the dynamic human.
The NDF is spatially aligned by the underlying reference surface. A neural
network is then learned to map pose to the dynamics of NDF. The proposed NDF
representation can synthesize the digitized performer with novel views and
novel poses with a detailed and reasonable dynamic appearance. Experiments show
that our method significantly outperforms recent human synthesis methods.

Comments:
- 16 pages, 7 figures. Accepted by ECCV 2022

---

## Neural apparent BRDF fields for multiview photometric stereo

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-14 | Meghna Asthana, William A. P. Smith, Patrik Huber | cs.CV | [PDF](http://arxiv.org/pdf/2207.06793v1){: .btn .btn-green } |

**Abstract**: We propose to tackle the multiview photometric stereo problem using an
extension of Neural Radiance Fields (NeRFs), conditioned on light source
direction. The geometric part of our neural representation predicts surface
normal direction, allowing us to reason about local surface reflectance. The
appearance part of our neural representation is decomposed into a neural
bidirectional reflectance function (BRDF), learnt as part of the fitting
process, and a shadow prediction network (conditioned on light source
direction) allowing us to model the apparent BRDF. This balance of learnt
components with inductive biases based on physical image formation models
allows us to extrapolate far from the light source and viewer directions
observed during training. We demonstrate our approach on a multiview
photometric stereo benchmark and show that competitive performance can be
obtained with the neural density representation of a NeRF.

Comments:
- 9 pages, 6 figures, 1 table

---

## Vision Transformer for NeRF-Based View Synthesis from a Single Input  Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-12 | Kai-En Lin, Lin Yen-Chen, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, Ravi Ramamoorthi | cs.CV | [PDF](http://arxiv.org/pdf/2207.05736v2){: .btn .btn-green } |

**Abstract**: Although neural radiance fields (NeRF) have shown impressive advances for
novel view synthesis, most methods typically require multiple input images of
the same scene with accurate camera poses. In this work, we seek to
substantially reduce the inputs to a single unposed image. Existing approaches
condition on local image features to reconstruct a 3D object, but often render
blurry predictions at viewpoints that are far away from the source view. To
address this issue, we propose to leverage both the global and local features
to form an expressive 3D representation. The global features are learned from a
vision transformer, while the local features are extracted from a 2D
convolutional network. To synthesize a novel view, we train a multilayer
perceptron (MLP) network conditioned on the learned 3D representation to
perform volume rendering. This novel 3D representation allows the network to
reconstruct unseen regions without enforcing constraints like symmetry or
canonical coordinate systems. Our method can render novel views from only a
single input image and generalize across multiple object categories using a
single model. Quantitative and qualitative evaluations demonstrate that the
proposed method achieves state-of-the-art performance and renders richer
details than existing approaches.

Comments:
- WACV 2023 Project website:
  https://cseweb.ucsd.edu/~viscomp/projects/VisionNeRF/

---

## A Learned Radiance-Field Representation for Complex Luminaires

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-11 | Jorge Condor, Adrián Jarabo | cs.GR | [PDF](http://arxiv.org/pdf/2207.05009v1){: .btn .btn-green } |

**Abstract**: We propose an efficient method for rendering complex luminaires using a
high-quality octree-based representation of the luminaire emission. Complex
luminaires are a particularly challenging problem in rendering, due to their
caustic light paths inside the luminaire. We reduce the geometric complexity of
luminaires by using a simple proxy geometry and encode the visually-complex
emitted light field by using a neural radiance field. We tackle the multiple
challenges of using NeRFs for representing luminaires, including their high
dynamic range, high-frequency content and null-emission areas, by proposing a
specialized loss function. For rendering, we distill our luminaires' NeRF into
a Plenoctree, which we can be easily integrated into traditional rendering
systems. Our approach allows for speed-ups of up to 2 orders of magnitude in
scenes containing complex luminaires introducing minimal error.

Comments:
- 10 pages, 7 figures. Eurographics Proceedings (EGSR 2022,
  Symposium-only track) (https://diglib.eg.org/handle/10.2312/sr20221155)

---

## Progressively-connected Light Field Network for Efficient View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-10 | Peng Wang, Yuan Liu, Guying Lin, Jiatao Gu, Lingjie Liu, Taku Komura, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2207.04465v1){: .btn .btn-green } |

**Abstract**: This paper presents a Progressively-connected Light Field network (ProLiF),
for the novel view synthesis of complex forward-facing scenes. ProLiF encodes a
4D light field, which allows rendering a large batch of rays in one training
step for image- or patch-level losses. Directly learning a neural light field
from images has difficulty in rendering multi-view consistent images due to its
unawareness of the underlying 3D geometry. To address this problem, we propose
a progressive training scheme and regularization losses to infer the underlying
geometry during training, both of which enforce the multi-view consistency and
thus greatly improves the rendering quality. Experiments demonstrate that our
method is able to achieve significantly better rendering quality than the
vanilla neural light fields and comparable results to NeRF-like rendering
methods on the challenging LLFF dataset and Shiny Object dataset. Moreover, we
demonstrate better compatibility with LPIPS loss to achieve robustness to
varying light conditions and CLIP loss to control the rendering style of the
scene. Project page: https://totoro97.github.io/projects/prolif.

Comments:
- Project page: https://totoro97.github.io/projects/prolif

---

## VMRF: View Matching Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-06 | Jiahui Zhang, Fangneng Zhan, Rongliang Wu, Yingchen Yu, Wenqing Zhang, Bai Song, Xiaoqin Zhang, Shijian Lu | cs.CV | [PDF](http://arxiv.org/pdf/2207.02621v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have demonstrated very impressive performance
in novel view synthesis via implicitly modelling 3D representations from
multi-view 2D images. However, most existing studies train NeRF models with
either reasonable camera pose initialization or manually-crafted camera pose
distributions which are often unavailable or hard to acquire in various
real-world data. We design VMRF, an innovative view matching NeRF that enables
effective NeRF training without requiring prior knowledge in camera poses or
camera pose distributions. VMRF introduces a view matching scheme, which
exploits unbalanced optimal transport to produce a feature transport plan for
mapping a rendered image with randomly initialized camera pose to the
corresponding real image. With the feature transport plan as the guidance, a
novel pose calibration technique is designed which rectifies the initially
randomized camera poses by predicting relative pose transformations between the
pair of rendered and real images. Extensive experiments over a number of
synthetic and real datasets show that the proposed VMRF outperforms the
state-of-the-art qualitatively and quantitatively by large margins.

Comments:
- This paper has been accepted to ACM MM 2022

---

## SNeRF: Stylized Neural Implicit Representations for 3D Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-05 | Thu Nguyen-Phuoc, Feng Liu, Lei Xiao | cs.CV | [PDF](http://arxiv.org/pdf/2207.02363v1){: .btn .btn-green } |

**Abstract**: This paper presents a stylized novel view synthesis method. Applying
state-of-the-art stylization methods to novel views frame by frame often causes
jittering artifacts due to the lack of cross-view consistency. Therefore, this
paper investigates 3D scene stylization that provides a strong inductive bias
for consistent novel view synthesis. Specifically, we adopt the emerging neural
radiance fields (NeRF) as our choice of 3D scene representation for their
capability to render high-quality novel views for a variety of scenes. However,
as rendering a novel view from a NeRF requires a large number of samples,
training a stylized NeRF requires a large amount of GPU memory that goes beyond
an off-the-shelf GPU capacity. We introduce a new training method to address
this problem by alternating the NeRF and stylization optimization steps. Such a
method enables us to make full use of our hardware memory capacity to both
generate images at higher resolution and adopt more expressive image style
transfer methods. Our experiments show that our method produces stylized NeRFs
for a wide range of content, including indoor, outdoor and dynamic scenes, and
synthesizes high-quality novel views with cross-view consistency.

Comments:
- SIGGRAPH 2022 (Journal track). Project page:
  https://research.facebook.com/publications/snerf-stylized-neural-implicit-representations-for-3d-scenes/

---

## LaTeRF: Label and Text Driven Object Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-04 | Ashkan Mirzaei, Yash Kant, Jonathan Kelly, Igor Gilitschenski | cs.CV | [PDF](http://arxiv.org/pdf/2207.01583v3){: .btn .btn-green } |

**Abstract**: Obtaining 3D object representations is important for creating photo-realistic
simulations and for collecting AR and VR assets. Neural fields have shown their
effectiveness in learning a continuous volumetric representation of a scene
from 2D images, but acquiring object representations from these models with
weak supervision remains an open challenge. In this paper we introduce LaTeRF,
a method for extracting an object of interest from a scene given 2D images of
the entire scene, known camera poses, a natural language description of the
object, and a set of point-labels of object and non-object points in the input
images. To faithfully extract the object from the scene, LaTeRF extends the
NeRF formulation with an additional `objectness' probability at each 3D point.
Additionally, we leverage the rich latent space of a pre-trained CLIP model
combined with our differentiable object renderer, to inpaint the occluded parts
of the object. We demonstrate high-fidelity object extraction on both synthetic
and real-world datasets and justify our design choices through an extensive
ablation study.

---

## Aug-NeRF: Training Stronger Neural Radiance Fields with Triple-Level  Physically-Grounded Augmentations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-07-04 | Tianlong Chen, Peihao Wang, Zhiwen Fan, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2207.01164v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) regresses a neural parameterized scene by
differentially rendering multi-view images with ground-truth supervision.
However, when interpolating novel views, NeRF often yields inconsistent and
visually non-smooth geometric results, which we consider as a generalization
gap between seen and unseen views. Recent advances in convolutional neural
networks have demonstrated the promise of advanced robust data augmentations,
either random or learned, in enhancing both in-distribution and
out-of-distribution generalization. Inspired by that, we propose Augmented NeRF
(Aug-NeRF), which for the first time brings the power of robust data
augmentations into regularizing the NeRF training. Particularly, our proposal
learns to seamlessly blend worst-case perturbations into three distinct levels
of the NeRF pipeline with physical grounds, including (1) the input
coordinates, to simulate imprecise camera parameters at image capture; (2)
intermediate features, to smoothen the intrinsic feature manifold; and (3)
pre-rendering output, to account for the potential degradation factors in the
multi-view image supervision. Extensive results demonstrate that Aug-NeRF
effectively boosts NeRF performance in both novel view synthesis (up to 1.5dB
PSNR gain) and underlying geometry reconstruction. Furthermore, thanks to the
implicit smooth prior injected by the triple-level augmentations, Aug-NeRF can
even recover scenes from heavily corrupted images, a highly challenging setting
untackled before. Our codes are available in
https://github.com/VITA-Group/Aug-NeRF.

Comments:
- IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2022

---

## Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in  Robotic Surgery

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-30 | Yuehao Wang, Yonghao Long, Siu Hin Fan, Qi Dou | cs.CV | [PDF](http://arxiv.org/pdf/2206.15255v1){: .btn .btn-green } |

**Abstract**: Reconstruction of the soft tissues in robotic surgery from endoscopic stereo
videos is important for many applications such as intra-operative navigation
and image-guided robotic surgery automation. Previous works on this task mainly
rely on SLAM-based approaches, which struggle to handle complex surgical
scenes. Inspired by recent progress in neural rendering, we present a novel
framework for deformable tissue reconstruction from binocular captures in
robotic surgery under the single-viewpoint setting. Our framework adopts
dynamic neural radiance fields to represent deformable surgical scenes in MLPs
and optimize shapes and deformations in a learning-based manner. In addition to
non-rigid deformations, tool occlusion and poor 3D clues from a single
viewpoint are also particular challenges in soft tissue reconstruction. To
overcome these difficulties, we present a series of strategies of tool
mask-guided ray casting, stereo depth-cueing ray marching and stereo
depth-supervised optimization. With experiments on DaVinci robotic surgery
videos, our method significantly outperforms the current state-of-the-art
reconstruction method for handling various complex non-rigid deformations. To
our best knowledge, this is the first work leveraging neural rendering for
surgical scene 3D reconstruction with remarkable potential demonstrated. Code
is available at: https://github.com/med-air/EndoNeRF.

Comments:
- 11 pages, 4 figures, conference

---

## Regularization of NeRFs using differential geometry

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-29 | Thibaud Ehret, Roger Marí, Gabriele Facciolo | cs.CV | [PDF](http://arxiv.org/pdf/2206.14938v2){: .btn .btn-green } |

**Abstract**: Neural radiance fields, or NeRF, represent a breakthrough in the field of
novel view synthesis and 3D modeling of complex scenes from multi-view image
collections. Numerous recent works have shown the importance of making NeRF
models more robust, by means of regularization, in order to train with possibly
inconsistent and/or very sparse data. In this work, we explore how differential
geometry can provide elegant regularization tools for robustly training
NeRF-like models, which are modified so as to represent continuous and
infinitely differentiable functions. In particular, we present a generic
framework for regularizing different types of NeRFs observations to improve the
performance in challenging conditions. We also show how the same formalism can
also be used to natively encourage the regularity of surfaces by means of
Gaussian or mean curvatures.

---

## Ev-NeRF: Event Based Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-24 | Inwoo Hwang, Junho Kim, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2206.12455v2){: .btn .btn-green } |

**Abstract**: We present Ev-NeRF, a Neural Radiance Field derived from event data. While
event cameras can measure subtle brightness changes in high frame rates, the
measurements in low lighting or extreme motion suffer from significant domain
discrepancy with complex noise. As a result, the performance of event-based
vision tasks does not transfer to challenging environments, where the event
cameras are expected to thrive over normal cameras. We find that the multi-view
consistency of NeRF provides a powerful self-supervision signal for eliminating
the spurious measurements and extracting the consistent underlying structure
despite highly noisy input. Instead of posed images of the original NeRF, the
input to Ev-NeRF is the event measurements accompanied by the movements of the
sensors. Using the loss function that reflects the measurement model of the
sensor, Ev-NeRF creates an integrated neural volume that summarizes the
unstructured and sparse data points captured for about 2-4 seconds. The
generated neural volume can also produce intensity images from novel views with
reasonable depth estimates, which can serve as a high-quality input to various
vision-based tasks. Our results show that Ev-NeRF achieves competitive
performance for intensity image reconstruction under extreme noise conditions
and high-dynamic-range imaging.

Comments:
- Accepted to WACV 2023

---

## UNeRF: Time and Memory Conscious U-Shaped Network for Training Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-23 | Abiramy Kuganesan, Shih-yang Su, James J. Little, Helge Rhodin | cs.CV | [PDF](http://arxiv.org/pdf/2206.11952v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) increase reconstruction detail for novel view
synthesis and scene reconstruction, with applications ranging from large static
scenes to dynamic human motion. However, the increased resolution and
model-free nature of such neural fields come at the cost of high training times
and excessive memory requirements. Recent advances improve the inference time
by using complementary data structures yet these methods are ill-suited for
dynamic scenes and often increase memory consumption. Little has been done to
reduce the resources required at training time. We propose a method to exploit
the redundancy of NeRF's sample-based computations by partially sharing
evaluations across neighboring sample points. Our UNeRF architecture is
inspired by the UNet, where spatial resolution is reduced in the middle of the
network and information is shared between adjacent samples. Although this
change violates the strict and conscious separation of view-dependent
appearance and view-independent density estimation in the NeRF method, we show
that it improves novel view synthesis. We also introduce an alternative
subsampling strategy which shares computation while minimizing any violation of
view invariance. UNeRF is a plug-in module for the original NeRF network. Our
major contributions include reduction of the memory footprint, improved
accuracy, and reduced amortized processing time both during training and
inference. With only weak assumptions on locality, we achieve improved resource
utilization on a variety of neural radiance fields tasks. We demonstrate
applications to the novel view synthesis of static scenes as well as dynamic
human shape and motion.

---

## EventNeRF: Neural Radiance Fields from a Single Colour Event Camera

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-23 | Viktor Rudnev, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik | cs.CV | [PDF](http://arxiv.org/pdf/2206.11896v3){: .btn .btn-green } |

**Abstract**: Asynchronously operating event cameras find many applications due to their
high dynamic range, vanishingly low motion blur, low latency and low data
bandwidth. The field saw remarkable progress during the last few years, and
existing event-based 3D reconstruction approaches recover sparse point clouds
of the scene. However, such sparsity is a limiting factor in many cases,
especially in computer vision and graphics, that has not been addressed
satisfactorily so far. Accordingly, this paper proposes the first approach for
3D-consistent, dense and photorealistic novel view synthesis using just a
single colour event stream as input. At its core is a neural radiance field
trained entirely in a self-supervised manner from events while preserving the
original resolution of the colour event channels. Next, our ray sampling
strategy is tailored to events and allows for data-efficient training. At test,
our method produces results in the RGB space at unprecedented quality. We
evaluate our method qualitatively and numerically on several challenging
synthetic and real scenes and show that it produces significantly denser and
more visually appealing renderings than the existing methods. We also
demonstrate robustness in challenging scenarios with fast motion and under low
lighting conditions. We release the newly recorded dataset and our source code
to facilitate the research field, see https://4dqv.mpi-inf.mpg.de/EventNeRF.

Comments:
- 19 pages, 21 figures, 3 tables; CVPR 2023

---

## KiloNeuS: A Versatile Neural Implicit Surface Representation for  Real-Time Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-22 | Stefano Esposito, Daniele Baieri, Stefan Zellmann, André Hinkenjann, Emanuele Rodolà | cs.CV | [PDF](http://arxiv.org/pdf/2206.10885v2){: .btn .btn-green } |

**Abstract**: NeRF-based techniques fit wide and deep multi-layer perceptrons (MLPs) to a
continuous radiance field that can be rendered from any unseen viewpoint.
However, the lack of surface and normals definition and high rendering times
limit their usage in typical computer graphics applications. Such limitations
have recently been overcome separately, but solving them together remains an
open problem. We present KiloNeuS, a neural representation reconstructing an
implicit surface represented as a signed distance function (SDF) from
multi-view images and enabling real-time rendering by partitioning the space
into thousands of tiny MLPs fast to inference. As we learn the implicit surface
locally using independent models, resulting in a globally coherent geometry is
non-trivial and needs to be addressed during training. We evaluate rendering
performance on a GPU-accelerated ray-caster with in-shader neural network
inference, resulting in an average of 46 FPS at high resolution, proving a
satisfying tradeoff between storage costs and rendering quality. In fact, our
evaluation for rendering quality and surface recovery shows that KiloNeuS
outperforms its single-MLP counterpart. Finally, to exhibit the versatility of
KiloNeuS, we integrate it into an interactive path-tracer taking full advantage
of its surface normals. We consider our work a crucial first step toward
real-time rendering of implicit neural representations under global
illumination.

Comments:
- 9 pages, 8 figures

---

## FWD: Real-time Novel View Synthesis with Forward Warping and Depth

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-16 | Ang Cao, Chris Rockwell, Justin Johnson | cs.CV | [PDF](http://arxiv.org/pdf/2206.08355v3){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) is a challenging task requiring systems to
generate photorealistic images of scenes from new viewpoints, where both
quality and speed are important for applications. Previous image-based
rendering (IBR) methods are fast, but have poor quality when input views are
sparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give
impressive results but are not real-time. In our paper, we propose a
generalizable NVS method with sparse inputs, called FWD, which gives
high-quality synthesis in real-time. With explicit depth and differentiable
rendering, it achieves competitive results to the SOTA methods with 130-1000x
speedup and better perceptual quality. If available, we can seamlessly
integrate sensor depth during either training or inference to improve image
quality while retaining real-time speed. With the growing prevalence of depths
sensors, we hope that methods making use of depth will become increasingly
useful.

Comments:
- CVPR 2022. Project website https://caoang327.github.io/FWD/

---

## Controllable 3D Face Synthesis with Conditional Generative Occupancy  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-16 | Keqiang Sun, Shangzhe Wu, Zhaoyang Huang, Ning Zhang, Quan Wang, HongSheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2206.08361v2){: .btn .btn-green } |

**Abstract**: Capitalizing on the recent advances in image generation models, existing
controllable face image synthesis methods are able to generate high-fidelity
images with some levels of controllability, e.g., controlling the shapes,
expressions, textures, and poses of the generated face images. However, these
methods focus on 2D image generative models, which are prone to producing
inconsistent face images under large expression and pose changes. In this
paper, we propose a new NeRF-based conditional 3D face synthesis framework,
which enables 3D controllability over the generated face images by imposing
explicit 3D conditions from 3D face priors. At its core is a conditional
Generative Occupancy Field (cGOF) that effectively enforces the shape of the
generated face to commit to a given 3D Morphable Model (3DMM) mesh. To achieve
accurate control over fine-grained 3D face shapes of the synthesized image, we
additionally incorporate a 3D landmark loss as well as a volume warping loss
into our synthesis algorithm. Experiments validate the effectiveness of the
proposed method, which is able to generate high-fidelity face images and shows
more precise 3D controllability than state-of-the-art 2D-based controllable
face synthesis methods. Find code and demo at
https://keqiangsun.github.io/projects/cgof.

---

## Neural Deformable Voxel Grid for Fast Optimization of Dynamic View  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-15 | Xiang Guo, Guanying Chen, Yuchao Dai, Xiaoqing Ye, Jiadai Sun, Xiao Tan, Errui Ding | cs.CV | [PDF](http://arxiv.org/pdf/2206.07698v2){: .btn .btn-green } |

**Abstract**: Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel
view synthesis (NVS) for its superior performance. In this paper, we propose to
synthesize dynamic scenes. Extending the methods for static scenes to dynamic
scenes is not straightforward as both the scene geometry and appearance change
over time, especially under monocular setup. Also, the existing dynamic NeRF
methods generally require a lengthy per-scene training procedure, where
multi-layer perceptrons (MLP) are fitted to model both motions and radiance. In
this paper, built on top of the recent advances in voxel-grid optimization, we
propose a fast deformable radiance field method to handle dynamic scenes. Our
method consists of two modules. The first module adopts a deformation grid to
store 3D dynamic features, and a light-weight MLP for decoding the deformation
that maps a 3D point in the observation space to the canonical space using the
interpolated features. The second module contains a density and a color grid to
model the geometry and density of the scene. The occlusion is explicitly
modeled to further improve the rendering quality. Experimental results show
that our method achieves comparable performance to D-NeRF using only 20 minutes
for training, which is more than 70x faster than D-NeRF, clearly demonstrating
the efficiency of our proposed method.

Comments:
- Technical Report: 29 pages; project page:
  https://npucvr.github.io/NDVG

---

## Physics Informed Neural Fields for Smoke Reconstruction with Sparse Data



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-14 | Mengyu Chu, Lingjie Liu, Quan Zheng, Erik Franz, Hans-Peter Seidel, Christian Theobalt, Rhaleb Zayer | cs.GR | [PDF](http://arxiv.org/pdf/2206.06577v1){: .btn .btn-green } |

**Abstract**: High-fidelity reconstruction of fluids from sparse multiview RGB videos
remains a formidable challenge due to the complexity of the underlying physics
as well as complex occlusion and lighting in captures. Existing solutions
either assume knowledge of obstacles and lighting, or only focus on simple
fluid scenes without obstacles or complex lighting, and thus are unsuitable for
real-world scenes with unknown lighting or arbitrary obstacles. We present the
first method to reconstruct dynamic fluid by leveraging the governing physics
(ie, Navier -Stokes equations) in an end-to-end optimization from sparse videos
without taking lighting conditions, geometry information, or boundary
conditions as input. We provide a continuous spatio-temporal scene
representation using neural networks as the ansatz of density and velocity
solution functions for fluids as well as the radiance field for static objects.
With a hybrid architecture that separates static and dynamic contents, fluid
interactions with static obstacles are reconstructed for the first time without
additional geometry input or human labeling. By augmenting time-varying neural
radiance fields with physics-informed deep learning, our method benefits from
the supervision of images and physical priors. To achieve robust optimization
from sparse views, we introduced a layer-by-layer growing strategy to
progressively increase the network capacity. Using progressively growing models
with a new regularization term, we manage to disentangle density-color
ambiguity in radiance fields without overfitting. A pretrained
density-to-velocity fluid model is leveraged in addition as the data prior to
avoid suboptimal velocity which underestimates vorticity but trivially fulfills
physical equations. Our method exhibits high-quality results with relaxed
constraints and strong flexibility on a representative set of synthetic and
real flow captures.

Comments:
- accepted to ACM Transactions On Graphics (SIGGRAPH 2022), further
  info:\url{https://people.mpi-inf.mpg.de/~mchu/projects/PI-NeRF/}

---

## RigNeRF: Fully Controllable Neural 3D Portraits

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-13 | ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli Shechtman, Zhixin Shu | cs.CV | [PDF](http://arxiv.org/pdf/2206.06481v1){: .btn .btn-green } |

**Abstract**: Volumetric neural rendering methods, such as neural radiance fields (NeRFs),
have enabled photo-realistic novel view synthesis. However, in their standard
form, NeRFs do not support the editing of objects, such as a human head, within
a scene. In this work, we propose RigNeRF, a system that goes beyond just novel
view synthesis and enables full control of head pose and facial expressions
learned from a single portrait video. We model changes in head pose and facial
expressions using a deformation field that is guided by a 3D morphable face
model (3DMM). The 3DMM effectively acts as a prior for RigNeRF that learns to
predict only residuals to the 3DMM deformations and allows us to render novel
(rigid) poses and (non-rigid) expressions that were not present in the input
sequence. Using only a smartphone-captured short video of a subject for
training, we demonstrate the effectiveness of our method on free view synthesis
of a portrait scene with explicit head pose and expression controls. The
project page can be found here:
http://shahrukhathar.github.io/2022/06/06/RigNeRF.html

Comments:
- The project page can be found here:
  http://shahrukhathar.github.io/2022/06/06/RigNeRF.html

---

## SNeS: Learning Probably Symmetric Neural Surfaces from Incomplete Data

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-13 | Eldar Insafutdinov, Dylan Campbell, João F. Henriques, Andrea Vedaldi | cs.CV | [PDF](http://arxiv.org/pdf/2206.06340v1){: .btn .btn-green } |

**Abstract**: We present a method for the accurate 3D reconstruction of partly-symmetric
objects. We build on the strengths of recent advances in neural reconstruction
and rendering such as Neural Radiance Fields (NeRF). A major shortcoming of
such approaches is that they fail to reconstruct any part of the object which
is not clearly visible in the training image, which is often the case for
in-the-wild images and videos. When evidence is lacking, structural priors such
as symmetry can be used to complete the missing information. However,
exploiting such priors in neural rendering is highly non-trivial: while
geometry and non-reflective materials may be symmetric, shadows and reflections
from the ambient scene are not symmetric in general. To address this, we apply
a soft symmetry constraint to the 3D geometry and material properties, having
factored appearance into lighting, albedo colour and reflectivity. We evaluate
our method on the recently introduced CO3D dataset, focusing on the car
category due to the challenge of reconstructing highly-reflective materials. We
show that it can reconstruct unobserved regions with high fidelity and render
high-quality novel view images.

Comments:
- First two authors contributed equally

---

## AR-NeRF: Unsupervised Learning of Depth and Defocus Effects from Natural  Images with Aperture Rendering Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-13 | Takuhiro Kaneko | cs.CV | [PDF](http://arxiv.org/pdf/2206.06100v1){: .btn .btn-green } |

**Abstract**: Fully unsupervised 3D representation learning has gained attention owing to
its advantages in data collection. A successful approach involves a
viewpoint-aware approach that learns an image distribution based on generative
models (e.g., generative adversarial networks (GANs)) while generating various
view images based on 3D-aware models (e.g., neural radiance fields (NeRFs)).
However, they require images with various views for training, and consequently,
their application to datasets with few or limited viewpoints remains a
challenge. As a complementary approach, an aperture rendering GAN (AR-GAN) that
employs a defocus cue was proposed. However, an AR-GAN is a CNN-based model and
represents a defocus independently from a viewpoint change despite its high
correlation, which is one of the reasons for its performance. As an alternative
to an AR-GAN, we propose an aperture rendering NeRF (AR-NeRF), which can
utilize viewpoint and defocus cues in a unified manner by representing both
factors in a common ray-tracing framework. Moreover, to learn defocus-aware and
defocus-independent representations in a disentangled manner, we propose
aperture randomized training, for which we learn to generate images while
randomizing the aperture size and latent codes independently. During our
experiments, we applied AR-NeRF to various natural image datasets, including
flower, bird, and face images, the results of which demonstrate the utility of
AR-NeRF for unsupervised learning of the depth and defocus effects.

Comments:
- Accepted to CVPR 2022. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-nerf/

---

## Generalizable Neural Radiance Fields for Novel View Synthesis with  Transformer

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-10 | Dan Wang, Xinrui Cui, Septimiu Salcudean, Z. Jane Wang | cs.CV | [PDF](http://arxiv.org/pdf/2206.05375v1){: .btn .btn-green } |

**Abstract**: We propose a Transformer-based NeRF (TransNeRF) to learn a generic neural
radiance field conditioned on observed-view images for the novel view synthesis
task. By contrast, existing MLP-based NeRFs are not able to directly receive
observed views with an arbitrary number and require an auxiliary pooling-based
operation to fuse source-view information, resulting in the missing of
complicated relationships between source views and the target rendering view.
Furthermore, current approaches process each 3D point individually and ignore
the local consistency of a radiance field scene representation. These
limitations potentially can reduce their performance in challenging real-world
applications where large differences between source views and a novel rendering
view may exist. To address these challenges, our TransNeRF utilizes the
attention mechanism to naturally decode deep associations of an arbitrary
number of source views into a coordinate-based scene representation. Local
consistency of shape and appearance are considered in the ray-cast space and
the surrounding-view space within a unified Transformer network. Experiments
demonstrate that our TransNeRF, trained on a wide variety of scenes, can
achieve better performance in comparison to state-of-the-art image-based neural
rendering methods in both scene-agnostic and per-scene finetuning scenarios
especially when there is a considerable gap between source views and a
rendering view.

---

## NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-10 | Hao-Kang Liu, I-Chao Shen, Bing-Yu Chen | cs.CV | [PDF](http://arxiv.org/pdf/2206.04901v1){: .btn .btn-green } |

**Abstract**: Though Neural Radiance Field (NeRF) demonstrates compelling novel view
synthesis results, it is still unintuitive to edit a pre-trained NeRF because
the neural network's parameters and the scene geometry/appearance are often not
explicitly associated. In this paper, we introduce the first framework that
enables users to remove unwanted objects or retouch undesired regions in a 3D
scene represented by a pre-trained NeRF without any category-specific data and
training. The user first draws a free-form mask to specify a region containing
unwanted objects over a rendered view from the pre-trained NeRF. Our framework
first transfers the user-provided mask to other rendered views and estimates
guiding color and depth images within these transferred masked regions. Next,
we formulate an optimization problem that jointly inpaints the image content in
all masked regions across multiple views by updating the NeRF model's
parameters. We demonstrate our framework on diverse scenes and show it obtained
visual plausible and structurally consistent results across multiple views
using shorter time and less user manual efforts.

Comments:
- Hao-Kang Liu and I-Chao Shen contributed equally to the paper.
  Project page: https://jdily.github.io/proj_site/nerfin_proj.html

---

## Improved Direct Voxel Grid Optimization for Radiance Fields  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-10 | Cheng Sun, Min Sun, Hwann-Tzong Chen | cs.GR | [PDF](http://arxiv.org/pdf/2206.05085v4){: .btn .btn-green } |

**Abstract**: In this technical report, we improve the DVGO framework (called DVGOv2),
which is based on Pytorch and uses the simplest dense grid representation.
First, we re-implement part of the Pytorch operations with cuda, achieving 2-3x
speedup. The cuda extension is automatically compiled just in time. Second, we
extend DVGO to support Forward-facing and Unbounded Inward-facing capturing.
Third, we improve the space time complexity of the distortion loss proposed by
mip-NeRF 360 from O(N^2) to O(N). The distortion loss improves our quality and
training speed. Our efficient implementation could allow more future works to
benefit from the loss.

Comments:
- Project page https://sunset1995.github.io/dvgo/ ; Code
  https://github.com/sunset1995/DirectVoxGO ; Results updated

---

## Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-09 | Mingtong Zhang, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Yu-Xiong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2206.04669v1){: .btn .btn-green } |

**Abstract**: Comprehensive 3D scene understanding, both geometrically and semantically, is
important for real-world applications such as robot perception. Most of the
existing work has focused on developing data-driven discriminative models for
scene understanding. This paper provides a new approach to scene understanding,
from a synthesis model perspective, by leveraging the recent progress on
implicit 3D representation and neural rendering. Building upon the great
success of Neural Radiance Fields (NeRFs), we introduce Scene-Property
Synthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic
RGB images from novel viewpoints, but also render various accurate scene
properties (e.g., appearance, geometry, and semantics). By doing so, we
facilitate addressing a variety of scene understanding tasks under a unified
framework, including semantic segmentation, surface normal estimation,
reshading, keypoint detection, and edge detection. Our SS-NeRF framework can be
a powerful tool for bridging generative learning and discriminative learning,
and thus be beneficial to the investigation of a wide range of interesting
problems, such as studying task relationships within a synthesis paradigm,
transferring knowledge to novel tasks, facilitating downstream discriminative
tasks as ways of data augmentation, and serving as auto-labeller for data
creation.

---

## ObPose: Leveraging Pose for Object-Centric Scene Inference and  Generation in 3D

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-07 | Yizhe Wu, Oiwi Parker Jones, Ingmar Posner | cs.CV | [PDF](http://arxiv.org/pdf/2206.03591v3){: .btn .btn-green } |

**Abstract**: We present ObPose, an unsupervised object-centric inference and generation
model which learns 3D-structured latent representations from RGB-D scenes.
Inspired by prior art in 2D representation learning, ObPose considers a
factorised latent space, separately encoding object location (where) and
appearance (what). ObPose further leverages an object's pose (i.e. location and
orientation), defined via a minimum volume principle, as a novel inductive bias
for learning the where component. To achieve this, we propose an efficient,
voxelised approximation approach to recover the object shape directly from a
neural radiance field (NeRF). As a consequence, ObPose models each scene as a
composition of NeRFs, richly representing individual objects. To evaluate the
quality of the learned representations, ObPose is evaluated quantitatively on
the YCB, MultiShapeNet, and CLEVR datatasets for unsupervised scene
segmentation, outperforming the current state-of-the-art in 3D scene inference
(ObSuRF) by a significant margin. Generative results provide qualitative
demonstration that the same ObPose model can both generate novel scenes and
flexibly edit the objects in them. These capacities again reflect the quality
of the learned latents and the benefits of disentangling the where and what
components of a scene. Key design choices made in the ObPose encoder are
validated with ablations.

Comments:
- 14 pages, 4 figures

---

## Reinforcement Learning with Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-03 | Danny Driess, Ingmar Schubert, Pete Florence, Yunzhu Li, Marc Toussaint | cs.LG | [PDF](http://arxiv.org/pdf/2206.01634v1){: .btn .btn-green } |

**Abstract**: It is a long-standing problem to find effective representations for training
reinforcement learning (RL) agents. This paper demonstrates that learning state
representations with supervision from Neural Radiance Fields (NeRFs) can
improve the performance of RL compared to other learned representations or even
low-dimensional, hand-engineered state information. Specifically, we propose to
train an encoder that maps multiple image observations to a latent space
describing the objects in the scene. The decoder built from a
latent-conditioned NeRF serves as the supervision signal to learn the latent
space. An RL algorithm then operates on the learned latent space as its state
representation. We call this NeRF-RL. Our experiments indicate that NeRF as
supervision leads to a latent space better suited for the downstream RL tasks
involving robotic object manipulations like hanging mugs on hooks, pushing
objects, or opening doors. Video: https://dannydriess.github.io/nerf-rl

---

## Points2NeRF: Generating Neural Radiance Fields from 3D point cloud

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-02 | D. Zimny, T. Trzciński, P. Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2206.01290v2){: .btn .btn-green } |

**Abstract**: Contemporary registration devices for 3D visual information, such as LIDARs
and various depth cameras, capture data as 3D point clouds. In turn, such
clouds are challenging to be processed due to their size and complexity.
Existing methods address this problem by fitting a mesh to the point cloud and
rendering it instead. This approach, however, leads to the reduced fidelity of
the resulting visualization and misses color information of the objects crucial
in computer graphics applications. In this work, we propose to mitigate this
challenge by representing 3D objects as Neural Radiance Fields (NeRFs). We
leverage a hypernetwork paradigm and train the model to take a 3D point cloud
with the associated color values and return a NeRF network's weights that
reconstruct 3D objects from input 2D images. Our method provides efficient 3D
object representation and offers several advantages over the existing
approaches, including the ability to condition NeRFs and improved
generalization beyond objects seen in training. The latter we also confirmed in
the results of our empirical evaluation.

Comments:
- arXiv admin note: text overlap with arXiv:2003.08934 by other authors

---

## EfficientNeRF: Efficient Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-06-02 | Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, Jiaya Jia | cs.CV | [PDF](http://arxiv.org/pdf/2206.00878v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications.

---

## Novel View Synthesis for High-fidelity Headshot Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-31 | Satoshi Tsutsui, Weijia Mao, Sijing Lin, Yunyi Zhu, Murong Ma, Mike Zheng Shou | cs.CV | [PDF](http://arxiv.org/pdf/2205.15595v1){: .btn .btn-green } |

**Abstract**: Rendering scenes with a high-quality human face from arbitrary viewpoints is
a practical and useful technique for many real-world applications. Recently,
Neural Radiance Fields (NeRF), a rendering technique that uses neural networks
to approximate classical ray tracing, have been considered as one of the
promising approaches for synthesizing novel views from a sparse set of images.
We find that NeRF can render new views while maintaining geometric consistency,
but it does not properly maintain skin details, such as moles and pores. These
details are important particularly for faces because when we look at an image
of a face, we are much more sensitive to details than when we look at other
objects. On the other hand, 3D Morpable Models (3DMMs) based on traditional
meshes and textures can perform well in terms of skin detail despite that it
has less precise geometry and cannot cover the head and the entire scene with
background. Based on these observations, we propose a method to use both NeRF
and 3DMM to synthesize a high-fidelity novel view of a scene with a face. Our
method learns a Generative Adversarial Network (GAN) to mix a NeRF-synthesized
image and a 3DMM-rendered image and produces a photorealistic scene with a face
preserving the skin details. Experiments with various real-world scenes
demonstrate the effectiveness of our approach. The code will be available on
https://github.com/showlab/headshot .

---

## Decomposing NeRF for Editing via Feature Field Distillation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-31 | Sosuke Kobayashi, Eiichi Matsumoto, Vincent Sitzmann | cs.CV | [PDF](http://arxiv.org/pdf/2205.15585v2){: .btn .btn-green } |

**Abstract**: Emerging neural radiance fields (NeRF) are a promising scene representation
for computer graphics, enabling high-quality 3D reconstruction and novel view
synthesis from image observations. However, editing a scene represented by a
NeRF is challenging, as the underlying connectionist representations such as
MLPs or voxel grids are not object-centric or compositional. In particular, it
has been difficult to selectively edit specific regions or objects. In this
work, we tackle the problem of semantic scene decomposition of NeRFs to enable
query-based local editing of the represented 3D scenes. We propose to distill
the knowledge of off-the-shelf, self-supervised 2D image feature extractors
such as CLIP-LSeg or DINO into a 3D feature field optimized in parallel to the
radiance field. Given a user-specified query of various modalities such as
text, an image patch, or a point-and-click selection, 3D feature fields
semantically decompose 3D space without the need for re-training and enable us
to semantically select and edit regions in the radiance field. Our experiments
validate that the distilled feature fields (DFFs) can transfer recent progress
in 2D vision and language foundation models to 3D scene representations,
enabling convincing 3D segmentation and selective editing of emerging neural
graphics representations.

Comments:
- Accepted to NeurIPS 2022
  https://pfnet-research.github.io/distilled-feature-fields/

---

## SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary  Image collections

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-31 | Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T. Barron, Hendrik P. A. Lensch, Varun Jampani | cs.CV | [PDF](http://arxiv.org/pdf/2205.15768v1){: .btn .btn-green } |

**Abstract**: Inverse rendering of an object under entirely unknown capture conditions is a
fundamental challenge in computer vision and graphics. Neural approaches such
as NeRF have achieved photorealistic results on novel view synthesis, but they
require known camera poses. Solving this problem with unknown camera poses is
highly challenging as it requires joint optimization over shape, radiance, and
pose. This problem is exacerbated when the input images are captured in the
wild with varying backgrounds and illuminations. Standard pose estimation
techniques fail in such image collections in the wild due to very few estimated
correspondences across images. Furthermore, NeRF cannot relight a scene under
any illumination, as it operates on radiance (the product of reflectance and
illumination). We propose a joint optimization framework to estimate the shape,
BRDF, and per-image camera pose and illumination. Our method works on
in-the-wild online image collections of an object and produces relightable 3D
assets for several use-cases such as AR/VR. To our knowledge, our method is the
first to tackle this severely unconstrained task with minimal user interaction.
Project page: https://markboss.me/publication/2022-samurai/ Video:
https://youtu.be/LlYuGDjXp-8

---

## DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-31 | Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang, David Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu Qie, Mike Zheng Shou | cs.CV | [PDF](http://arxiv.org/pdf/2205.15723v2){: .btn .btn-green } |

**Abstract**: Modeling dynamic scenes is important for many applications such as virtual
reality and telepresence. Despite achieving unprecedented fidelity for novel
view synthesis in dynamic scenes, existing methods based on Neural Radiance
Fields (NeRF) suffer from slow convergence (i.e., model training time measured
in days). In this paper, we present DeVRF, a novel representation to accelerate
learning dynamic radiance fields. The core of DeVRF is to model both the 3D
canonical space and 4D deformation field of a dynamic, non-rigid scene with
explicit and discrete voxel-based representations. However, it is quite
challenging to train such a representation which has a large number of model
parameters, often resulting in overfitting issues. To overcome this challenge,
we devise a novel static-to-dynamic learning paradigm together with a new data
capture setup that is convenient to deploy in practice. This paradigm unlocks
efficient learning of deformable radiance fields via utilizing the 3D
volumetric canonical space learnt from multi-view static images to ease the
learning of 4D voxel deformation field with only few-view dynamic sequences. To
further improve the efficiency of our DeVRF and its synthesized novel view's
quality, we conduct thorough explorations and identify a set of strategies. We
evaluate DeVRF on both synthetic and real-world dynamic scenes with different
types of deformation. Experiments demonstrate that DeVRF achieves two orders of
magnitude speedup (100x faster) with on-par high-fidelity results compared to
the previous state-of-the-art approaches. The code and dataset will be released
in https://github.com/showlab/DeVRF.

Comments:
- Project page: https://jia-wei-liu.github.io/DeVRF/

---

## D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from  a Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-31 | Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, Cengiz Oztireli | cs.CV | [PDF](http://arxiv.org/pdf/2205.15838v4){: .btn .btn-green } |

**Abstract**: Given a monocular video, segmenting and decoupling dynamic objects while
recovering the static environment is a widely studied problem in machine
intelligence. Existing solutions usually approach this problem in the image
domain, limiting their performance and understanding of the environment. We
introduce Decoupled Dynamic Neural Radiance Field (D$^2$NeRF), a
self-supervised approach that takes a monocular video and learns a 3D scene
representation which decouples moving objects, including their shadows, from
the static background. Our method represents the moving objects and the static
background by two separate neural radiance fields with only one allowing for
temporal changes. A naive implementation of this approach leads to the dynamic
component taking over the static one as the representation of the former is
inherently more general and prone to overfitting. To this end, we propose a
novel loss to promote correct separation of phenomena. We further propose a
shadow field network to detect and decouple dynamically moving shadows. We
introduce a new dataset containing various dynamic objects and shadows and
demonstrate that our method can achieve better performance than
state-of-the-art approaches in decoupling dynamic and static 3D objects,
occlusion and shadow removal, and image segmentation for moving objects.

---

## Fast Dynamic Radiance Fields with Time-Aware Neural Voxels

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-30 | Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nießner, Qi Tian | cs.CV | [PDF](http://arxiv.org/pdf/2205.15285v2){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) have shown great success in modeling 3D scenes
and synthesizing novel-view images. However, most previous NeRF methods take
much time to optimize one single scene. Explicit data structures, e.g. voxel
features, show great potential to accelerate the training process. However,
voxel features face two big challenges to be applied to dynamic scenes, i.e.
modeling temporal information and capturing different scales of point motions.
We propose a radiance field framework by representing scenes with time-aware
voxel features, named as TiNeuVox. A tiny coordinate deformation network is
introduced to model coarse motion trajectories and temporal information is
further enhanced in the radiance network. A multi-distance interpolation method
is proposed and applied on voxel features to model both small and large
motions. Our framework significantly accelerates the optimization of dynamic
radiance fields while maintaining high rendering quality. Empirical evaluation
is performed on both synthetic and real scenes. Our TiNeuVox completes training
with only 8 minutes and 8-MB storage cost while showing similar or even better
rendering performance than previous dynamic NeRF methods.

Comments:
- SIGGRAPH Asia 2022. Project page: https://jaminfong.cn/tineuvox

---

## Neural Volumetric Object Selection

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-30 | Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G. Schwing, Oliver Wang | cs.CV | [PDF](http://arxiv.org/pdf/2205.14929v1){: .btn .btn-green } |

**Abstract**: We introduce an approach for selecting objects in neural volumetric 3D
representations, such as multi-plane images (MPI) and neural radiance fields
(NeRF). Our approach takes a set of foreground and background 2D user scribbles
in one view and automatically estimates a 3D segmentation of the desired
object, which can be rendered into novel views. To achieve this result, we
propose a novel voxel feature embedding that incorporates the neural volumetric
3D representation and multi-view image features from all input views. To
evaluate our approach, we introduce a new dataset of human-provided
segmentation masks for depicted objects in real-world multi-view scene
captures. We show that our approach out-performs strong baselines, including 2D
segmentation and 3D segmentation approaches adapted to our task.

Comments:
- CVPR 2022 camera ready

---

## Compressible-composable NeRF via Rank-residual Decomposition

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-30 | Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, Gang Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2205.14870v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) has emerged as a compelling method to represent
3D objects and scenes for photo-realistic rendering. However, its implicit
representation causes difficulty in manipulating the models like the explicit
mesh representation. Several recent advances in NeRF manipulation are usually
restricted by a shared renderer network, or suffer from large model size. To
circumvent the hurdle, in this paper, we present an explicit neural field
representation that enables efficient and convenient manipulation of models. To
achieve this goal, we learn a hybrid tensor rank decomposition of the scene
without neural networks. Motivated by the low-rank approximation property of
the SVD algorithm, we propose a rank-residual learning strategy to encourage
the preservation of primary information in lower ranks. The model size can then
be dynamically adjusted by rank truncation to control the levels of detail,
achieving near-optimal compression without extra optimization. Furthermore,
different models can be arbitrarily transformed and composed into one scene by
concatenating along the rank dimension. The growth of storage cost can also be
mitigated by compressing the unimportant objects in the composed scene. We
demonstrate that our method is able to achieve comparable rendering quality to
state-of-the-art methods, while enabling extra capability of compression and
composition. Code will be made available at https://github.com/ashawkey/CCNeRF.

Comments:
- NeurIPS 2022 camera-ready version

---

## V4D: Voxel for 4D Novel View Synthesis



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-28 | Wanshui Gan, Hongbin Xu, Yi Huang, Shifeng Chen, Naoto Yokoya | cs.CV | [PDF](http://arxiv.org/pdf/2205.14332v2){: .btn .btn-green } |

**Abstract**: Neural radiance fields have made a remarkable breakthrough in the novel view
synthesis task at the 3D static scene. However, for the 4D circumstance (e.g.,
dynamic scene), the performance of the existing method is still limited by the
capacity of the neural network, typically in a multilayer perceptron network
(MLP). In this paper, we utilize 3D Voxel to model the 4D neural radiance
field, short as V4D, where the 3D voxel has two formats. The first one is to
regularly model the 3D space and then use the sampled local 3D feature with the
time index to model the density field and the texture field by a tiny MLP. The
second one is in look-up tables (LUTs) format that is for the pixel-level
refinement, where the pseudo-surface produced by the volume rendering is
utilized as the guidance information to learn a 2D pixel-level refinement
mapping. The proposed LUTs-based refinement module achieves the performance
gain with little computational cost and could serve as the plug-and-play module
in the novel view synthesis task. Moreover, we propose a more effective
conditional positional encoding toward the 4D data that achieves performance
gain with negligible computational burdens. Extensive experiments demonstrate
that the proposed method achieves state-of-the-art performance at a low
computational cost.

---

## Differentiable Point-Based Radiance Fields for Efficient View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-28 | Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, Felix Heide | cs.CV | [PDF](http://arxiv.org/pdf/2205.14330v4){: .btn .btn-green } |

**Abstract**: We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.

---

## PREF: Phasorial Embedding Fields for Compact Neural Representations



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-26 | Binbin Huang, Xinhao Yan, Anpei Chen, Shenghua Gao, Jingyi Yu | cs.CV | [PDF](http://arxiv.org/pdf/2205.13524v3){: .btn .btn-green } |

**Abstract**: We present an efficient frequency-based neural representation termed PREF: a
shallow MLP augmented with a phasor volume that covers significant border
spectra than previous Fourier feature mapping or Positional Encoding. At the
core is our compact 3D phasor volume where frequencies distribute uniformly
along a 2D plane and dilate along a 1D axis. To this end, we develop a tailored
and efficient Fourier transform that combines both Fast Fourier transform and
local interpolation to accelerate na\"ive Fourier mapping. We also introduce a
Parsvel regularizer that stables frequency-based learning. In these ways, Our
PREF reduces the costly MLP in the frequency-based representation, thereby
significantly closing the efficiency gap between it and other hybrid
representations, and improving its interpretability. Comprehensive experiments
demonstrate that our PREF is able to capture high-frequency details while
remaining compact and robust, including 2D image generalization, 3D signed
distance function regression and 5D neural radiance field reconstruction.

---

## StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D  Mutual Learning

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-24 | Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, Lin Gao | cs.GR | [PDF](http://arxiv.org/pdf/2205.12183v2){: .btn .btn-green } |

**Abstract**: 3D scene stylization aims at generating stylized images of the scene from
arbitrary novel views following a given set of style examples, while ensuring
consistency when rendered from different views. Directly applying methods for
image or video stylization to 3D scenes cannot achieve such consistency. Thanks
to recently proposed neural radiance fields (NeRF), we are able to represent a
3D scene in a consistent way. Consistent 3D scene stylization can be
effectively achieved by stylizing the corresponding NeRF. However, there is a
significant domain gap between style examples which are 2D images and NeRF
which is an implicit volumetric representation. To address this problem, we
propose a novel mutual learning framework for 3D scene stylization that
combines a 2D image stylization network and NeRF to fuse the stylization
ability of 2D stylization network with the 3D consistency of NeRF. We first
pre-train a standard NeRF of the 3D scene to be stylized and replace its color
prediction module with a style network to obtain a stylized NeRF. It is
followed by distilling the prior knowledge of spatial consistency from NeRF to
the 2D stylization network through an introduced consistency loss. We also
introduce a mimic loss to supervise the mutual learning of the NeRF style
module and fine-tune the 2D stylization decoder. In order to further make our
model handle ambiguities of 2D stylization results, we introduce learnable
latent codes that obey the probability distributions conditioned on the style.
They are attached to training samples as conditional inputs to better learn the
style module in our novel stylized NeRF. Experimental results demonstrate that
our method is superior to existing approaches in both visual quality and
long-range consistency.

Comments:
- Accepted by CVPR 2022

---

## Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-19 | Arnab Dey, Yassine Ahmine, Andrew I. Comport | cs.CV | [PDF](http://arxiv.org/pdf/2205.09351v3){: .btn .btn-green } |

**Abstract**: Neural scene representations, such as Neural Radiance Fields (NeRF), are
based on training a multilayer perceptron (MLP) using a set of color images
with known poses. An increasing number of devices now produce RGB-D(color +
depth) information, which has been shown to be very important for a wide range
of tasks. Therefore, the aim of this paper is to investigate what improvements
can be made to these promising implicit representations by incorporating depth
information with the color images. In particular, the recently proposed
Mip-NeRF approach, which uses conical frustums instead of rays for volume
rendering, allows one to account for the varying area of a pixel with distance
from the camera center. The proposed method additionally models depth
uncertainty. This allows to address major limitations of NeRF-based approaches
including improving the accuracy of geometry, reduced artifacts, faster
training time, and shortened prediction time. Experiments are performed on
well-known benchmark scenes, and comparisons show improved accuracy in scene
geometry and photometric reconstruction, while reducing the training time by 3
- 5 times.

---

## Fast Neural Network based Solving of Partial Differential Equations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-18 | Jaroslaw Rzepecki, Daniel Bates, Chris Doran | cs.LG | [PDF](http://arxiv.org/pdf/2205.08978v2){: .btn .btn-green } |

**Abstract**: We present a novel method for using Neural Networks (NNs) for finding
solutions to a class of Partial Differential Equations (PDEs). Our method
builds on recent advances in Neural Radiance Field research (NeRFs) and allows
for a NN to converge to a PDE solution much faster than classic Physically
Informed Neural Network (PINNs) approaches.

---

## RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-14 | Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas Müller, Charles Loop, Nathan Morrical, Koki Nagano, Towaki Takikawa, Stan Birchfield | cs.CV | [PDF](http://arxiv.org/pdf/2205.07058v2){: .btn .btn-green } |

**Abstract**: We present a large-scale synthetic dataset for novel view synthesis
consisting of ~300k images rendered from nearly 2000 complex scenes using
high-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset
is orders of magnitude larger than existing synthetic datasets for novel view
synthesis, thus providing a large unified benchmark for both training and
evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of
our dataset exhibit challenging variations in camera views, lighting, shape,
materials, and textures. Because our dataset is too large for existing methods
to process, we propose Sparse Voxel Light Field (SVLF), an efficient
voxel-based light field approach for novel view synthesis that achieves
comparable performance to NeRF on synthetic data, while being an order of
magnitude faster to train and two orders of magnitude faster to render. SVLF
achieves this speed by relying on a sparse voxel octree, careful voxel sampling
(requiring only a handful of queries per ray), and reduced network structure;
as well as ground truth depth maps at training time. Our dataset is generated
by NViSII, a Python-based ray tracing renderer, which is designed to be simple
for non-experts to use and share, flexible and powerful through its use of
scripting, and able to create high-quality and physically-based rendered
images. Experiments with a subset of our dataset allow us to compare standard
methods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for
category-level modeling, pointing toward the need for future improvements in
this area.

Comments:
- ECCV 2022 Workshop on Learning to Generate 3D Shapes and Scenes.
  Project page at http://www.cs.umd.edu/~mmeshry/projects/rtmv

---

## Ray Priors through Reprojection: Improving Neural Radiance Fields for  Novel View Extrapolation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-12 | Jian Zhang, Yuanqing Zhang, Huan Fu, Xiaowei Zhou, Bowen Cai, Jinchi Huang, Rongfei Jia, Binqiang Zhao, Xing Tang | cs.CV | [PDF](http://arxiv.org/pdf/2205.05922v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have emerged as a potent paradigm for
representing scenes and synthesizing photo-realistic images. A main limitation
of conventional NeRFs is that they often fail to produce high-quality
renderings under novel viewpoints that are significantly different from the
training viewpoints. In this paper, instead of exploiting few-shot image
synthesis, we study the novel view extrapolation setting that (1) the training
images can well describe an object, and (2) there is a notable discrepancy
between the training and test viewpoints' distributions. We present RapNeRF
(RAy Priors) as a solution. Our insight is that the inherent appearances of a
3D surface's arbitrary visible projections should be consistent. We thus
propose a random ray casting policy that allows training unseen views using
seen views. Furthermore, we show that a ray atlas pre-computed from the
observed rays' viewing directions could further enhance the rendering quality
for extrapolated views. A main limitation is that RapNeRF would remove the
strong view-dependent effects because it leverages the multi-view consistency
property.

---

## View Synthesis with Sculpted Neural Points

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-12 | Yiming Zuo, Jia Deng | cs.CV | [PDF](http://arxiv.org/pdf/2205.05869v2){: .btn .btn-green } |

**Abstract**: We address the task of view synthesis, generating novel views of a scene
given a set of images as input. In many recent works such as NeRF (Mildenhall
et al., 2020), the scene geometry is parameterized using neural implicit
representations (i.e., MLPs). Implicit neural representations have achieved
impressive visual quality but have drawbacks in computational efficiency. In
this work, we propose a new approach that performs view synthesis using point
clouds. It is the first point-based method that achieves better visual quality
than NeRF while being 100x faster in rendering speed. Our approach builds on
existing works on differentiable point-based rendering but introduces a novel
technique we call "Sculpted Neural Points (SNP)", which significantly improves
the robustness to errors and holes in the reconstructed point cloud. We further
propose to use view-dependent point features based on spherical harmonics to
capture non-Lambertian surfaces, and new designs in the point-based rendering
pipeline that further boost the performance. Finally, we show that our system
supports fine-grained scene editing. Code is available at
https://github.com/princeton-vl/SNP.

---

## NeRF-Editing: Geometry Editing of Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-10 | Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, Lin Gao | cs.GR | [PDF](http://arxiv.org/pdf/2205.04978v1){: .btn .btn-green } |

**Abstract**: Implicit neural rendering, especially Neural Radiance Field (NeRF), has shown
great potential in novel view synthesis of a scene. However, current NeRF-based
methods cannot enable users to perform user-controlled shape deformation in the
scene. While existing works have proposed some approaches to modify the
radiance field according to the user's constraints, the modification is limited
to color editing or object translation and rotation. In this paper, we propose
a method that allows users to perform controllable shape deformation on the
implicit representation of the scene, and synthesizes the novel view images of
the edited scene without re-training the network. Specifically, we establish a
correspondence between the extracted explicit mesh representation and the
implicit neural representation of the target scene. Users can first utilize
well-developed mesh-based deformation methods to deform the mesh representation
of the scene. Our method then utilizes user edits from the mesh representation
to bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining
the rendering results of the edited scene. Extensive experiments demonstrate
that our framework can achieve ideal editing results not only on synthetic
data, but also on real scenes captured by users.

Comments:
- Accepted by CVPR 2022

---

## Sampling-free obstacle gradients and reactive planning in Neural  Radiance Fields (NeRF)

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-05-03 | Michael Pantic, Cesar Cadena, Roland Siegwart, Lionel Ott | cs.RO | [PDF](http://arxiv.org/pdf/2205.01389v1){: .btn .btn-green } |

**Abstract**: This work investigates the use of Neural implicit representations,
specifically Neural Radiance Fields (NeRF), for geometrical queries and motion
planning. We show that by adding the capacity to infer occupancy in a radius to
a pre-trained NeRF, we are effectively learning an approximation to a Euclidean
Signed Distance Field (ESDF). Using backward differentiation of the augmented
network, we obtain an obstacle gradient that is integrated into an obstacle
avoidance policy based on the Riemannian Motion Policies (RMP) framework. Thus,
our findings allow for very fast sampling-free obstacle avoidance planning in
the implicit representation.

Comments:
- Accepted to the "Motion Planning with Implicit Neural Representations
  of Geometry" Workshop at ICRA 2022

---

## AE-NeRF: Auto-Encoding Neural Radiance Fields for 3D-Aware Object  Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-28 | Mira Kim, Jaehoon Ko, Kyusun Cho, Junmyeong Choi, Daewon Choi, Seungryong Kim | cs.CV | [PDF](http://arxiv.org/pdf/2204.13426v1){: .btn .btn-green } |

**Abstract**: We propose a novel framework for 3D-aware object manipulation, called
Auto-Encoding Neural Radiance Fields (AE-NeRF). Our model, which is formulated
in an auto-encoder architecture, extracts disentangled 3D attributes such as 3D
shape, appearance, and camera pose from an image, and a high-quality image is
rendered from the attributes through disentangled generative Neural Radiance
Fields (NeRF). To improve the disentanglement ability, we present two losses,
global-local attribute consistency loss defined between input and output, and
swapped-attribute classification loss. Since training such auto-encoding
networks from scratch without ground-truth shape and appearance information is
non-trivial, we present a stage-wise training scheme, which dramatically helps
to boost the performance. We conduct experiments to demonstrate the
effectiveness of the proposed model over the latest methods and provide
extensive ablation studies.

---

## NeurMiPs: Neural Mixture of Planar Experts for View Synthesis



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-28 | Zhi-Hao Lin, Wei-Chiu Ma, Hao-Yu Hsu, Yu-Chiang Frank Wang, Shenlong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2204.13696v1){: .btn .btn-green } |

**Abstract**: We present Neural Mixtures of Planar Experts (NeurMiPs), a novel planar-based
scene representation for modeling geometry and appearance. NeurMiPs leverages a
collection of local planar experts in 3D space as the scene representation.
Each planar expert consists of the parameters of the local rectangular shape
representing geometry and a neural radiance field modeling the color and
opacity. We render novel views by calculating ray-plane intersections and
composite output colors and densities at intersected points to the image.
NeurMiPs blends the efficiency of explicit mesh rendering and flexibility of
the neural radiance field. Experiments demonstrate superior performance and
speed of our proposed method, compared to other 3D representations in novel
view synthesis.

Comments:
- CVPR 2022. Project page: https://zhihao-lin.github.io/neurmips/

---

## Generalizable Neural Performer: Learning Robust Radiance Fields for  Human Novel View Synthesis



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-25 | Wei Cheng, Su Xu, Jingtan Piao, Chen Qian, Wayne Wu, Kwan-Yee Lin, Hongsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2204.11798v1){: .btn .btn-green } |

**Abstract**: This work targets at using a general deep learning framework to synthesize
free-viewpoint images of arbitrary human performers, only requiring a sparse
number of camera views as inputs and skirting per-case fine-tuning. The large
variation of geometry and appearance, caused by articulated body poses, shapes
and clothing types, are the key bottlenecks of this task. To overcome these
challenges, we present a simple yet powerful framework, named Generalizable
Neural Performer (GNR), that learns a generalizable and robust neural body
representation over various geometry and appearance. Specifically, we compress
the light fields for novel view human rendering as conditional implicit neural
radiance fields from both geometry and appearance aspects. We first introduce
an Implicit Geometric Body Embedding strategy to enhance the robustness based
on both parametric 3D human body model and multi-view images hints. We further
propose a Screen-Space Occlusion-Aware Appearance Blending technique to
preserve the high-quality appearance, through interpolating source view
appearance to the radiance fields with a relax but approximate geometric
guidance.
  To evaluate our method, we present our ongoing effort of constructing a
dataset with remarkable complexity and diversity. The dataset GeneBody-1.0,
includes over 360M frames of 370 subjects under multi-view cameras capturing,
performing a large variety of pose actions, along with diverse body shapes,
clothing, accessories and hairdos. Experiments on GeneBody-1.0 and ZJU-Mocap
show better robustness of our methods than recent state-of-the-art
generalizable methods among all cross-dataset, unseen subjects and unseen poses
settings. We also demonstrate the competitiveness of our model compared with
cutting-edge case-specific ones. Dataset, code and model will be made publicly
available.

Comments:
- Project Page: https://generalizable-neural-performer.github.io/
  Dataset: https://generalizable-neural-performer.github.io/genebody.html/

---

## Control-NeRF: Editable Feature Volumes for Scene Rendering and  Manipulation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-22 | Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey Tulyakov, Gerard Pons-Moll | cs.CV | [PDF](http://arxiv.org/pdf/2204.10850v1){: .btn .btn-green } |

**Abstract**: We present a novel method for performing flexible, 3D-aware image content
manipulation while enabling high-quality novel view synthesis. While NeRF-based
approaches are effective for novel view synthesis, such models memorize the
radiance for every point in a scene within a neural network. Since these models
are scene-specific and lack a 3D scene representation, classical editing such
as shape manipulation, or combining scenes is not possible. Hence, editing and
combining NeRF-based scenes has not been demonstrated. With the aim of
obtaining interpretable and controllable scene representations, our model
couples learnt scene-specific feature volumes with a scene agnostic neural
rendering network. With this hybrid representation, we decouple neural
rendering from scene-specific geometry and appearance. We can generalize to
novel scenes by optimizing only the scene-specific 3D feature representation,
while keeping the parameters of the rendering network fixed. The rendering
function learnt during the initial training stage can thus be easily applied to
new scenes, making our approach more flexible. More importantly, since the
feature volumes are independent of the rendering model, we can manipulate and
combine scenes by editing their corresponding feature volumes. The edited
volume can then be plugged into the rendering model to synthesize high-quality
novel views. We demonstrate various scene manipulations, including mixing
scenes, deforming objects and inserting objects into scenes, while still
producing photo-realistic results.

---

## Implicit Object Mapping With Noisy Data

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-22 | Jad Abou-Chakra, Feras Dayoub, Niko Sünderhauf | cs.RO | [PDF](http://arxiv.org/pdf/2204.10516v2){: .btn .btn-green } |

**Abstract**: Modelling individual objects in a scene as Neural Radiance Fields (NeRFs)
provides an alternative geometric scene representation that may benefit
downstream robotics tasks such as scene understanding and object manipulation.
However, we identify three challenges to using real-world training data
collected by a robot to train a NeRF: (i) The camera trajectories are
constrained, and full visual coverage is not guaranteed - especially when
obstructions to the objects of interest are present; (ii) the poses associated
with the images are noisy due to odometry or localization noise; (iii) the
objects are not easily isolated from the background. This paper evaluates the
extent to which above factors degrade the quality of the learnt implicit object
representation. We introduce a pipeline that decomposes a scene into multiple
individual object-NeRFs, using noisy object instance masks and bounding boxes,
and evaluate the sensitivity of this pipeline with respect to noisy poses,
instance masks, and the number of training images. We uncover that the
sensitivity to noisy instance masks can be partially alleviated with depth
supervision and quantify the importance of including the camera extrinsics in
the NeRF optimisation process.

---

## SILVR: A Synthetic Immersive Large-Volume Plenoptic Dataset

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-20 | Martijn Courteaux, Julie Artois, Stijn De Pauw, Peter Lambert, Glenn Van Wallendael | cs.GR | [PDF](http://arxiv.org/pdf/2204.09523v1){: .btn .btn-green } |

**Abstract**: In six-degrees-of-freedom light-field (LF) experiences, the viewer's freedom
is limited by the extent to which the plenoptic function was sampled. Existing
LF datasets represent only small portions of the plenoptic function, such that
they either cover a small volume, or they have limited field of view.
Therefore, we propose a new LF image dataset "SILVR" that allows for
six-degrees-of-freedom navigation in much larger volumes while maintaining full
panoramic field of view. We rendered three different virtual scenes in various
configurations, where the number of views ranges from 642 to 2226. One of these
scenes (called Zen Garden) is a novel scene, and is made publicly available. We
chose to position the virtual cameras closely together in large cuboid and
spherical organisations ($2.2m^3$ to $48m^3$), equipped with 180{\deg} fish-eye
lenses. Every view is rendered to a color image and depth map of 2048px
$\times$ 2048px. Additionally, we present the software used to automate the
multi-view rendering process, as well as a lens-reprojection tool that converts
between images with panoramic or fish-eye projection to a standard rectilinear
(i.e., perspective) projection. Finally, we demonstrate how the proposed
dataset and software can be used to evaluate LF coding/rendering techniques(in
this case for training NeRFs with instant-ngp). As such, we provide the first
publicly-available LF dataset for large volumes of light with full panoramic
field of view

Comments:
- In 13th ACM Multimedia Systems Conference (MMSys '22), June 14-17,
  2022, Athlone, Ireland. ACM, New York, NY, USA, 6 pages

---

## Modeling Indirect Illumination for Inverse Rendering



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-14 | Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, Xiaowei Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2204.06837v1){: .btn .btn-green } |

**Abstract**: Recent advances in implicit neural representations and differentiable
rendering make it possible to simultaneously recover the geometry and materials
of an object from multi-view RGB images captured under unknown static
illumination. Despite the promising results achieved, indirect illumination is
rarely modeled in previous methods, as it requires expensive recursive path
tracing which makes the inverse rendering computationally intractable. In this
paper, we propose a novel approach to efficiently recovering spatially-varying
indirect illumination. The key insight is that indirect illumination can be
conveniently derived from the neural radiance field learned from input images
instead of being estimated jointly with direct illumination and materials. By
properly modeling the indirect illumination and visibility of direct
illumination, interreflection- and shadow-free albedo can be recovered. The
experiments on both synthetic and real data demonstrate the superior
performance of our approach compared to previous work and its capability to
synthesize realistic renderings under novel viewpoints and illumination. Our
code and data are available at https://zju3dv.github.io/invrender/.

---

## GARF: Gaussian Activated Radiance Fields for High Fidelity  Reconstruction and Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-12 | Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, Simon Lucey | cs.CV | [PDF](http://arxiv.org/pdf/2204.05735v1){: .btn .btn-green } |

**Abstract**: Despite Neural Radiance Fields (NeRF) showing compelling results in
photorealistic novel views synthesis of real-world scenes, most existing
approaches require accurate prior camera poses. Although approaches for jointly
recovering the radiance field and camera pose exist (BARF), they rely on a
cumbersome coarse-to-fine auxiliary positional embedding to ensure good
performance. We present Gaussian Activated neural Radiance Fields (GARF), a new
positional embedding-free neural radiance field architecture - employing
Gaussian activations - that outperforms the current state-of-the-art in terms
of high fidelity reconstruction and pose estimation.

Comments:
- Project page: https://sfchng.github.io/garf/

---

## NAN: Noise-Aware NeRFs for Burst-Denoising

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-10 | Naama Pearl, Tali Treibitz, Simon Korman | cs.CV | [PDF](http://arxiv.org/pdf/2204.04668v2){: .btn .btn-green } |

**Abstract**: Burst denoising is now more relevant than ever, as computational photography
helps overcome sensitivity issues inherent in mobile phones and small cameras.
A major challenge in burst-denoising is in coping with pixel misalignment,
which was so far handled with rather simplistic assumptions of simple motion,
or the ability to align in pre-processing. Such assumptions are not realistic
in the presence of large motion and high levels of noise. We show that Neural
Radiance Fields (NeRFs), originally suggested for physics-based novel-view
rendering, can serve as a powerful framework for burst denoising. NeRFs have an
inherent capability of handling noise as they integrate information from
multiple images, but they are limited in doing so, mainly since they build on
pixel-wise operations which are suitable to ideal imaging conditions. Our
approach, termed NAN, leverages inter-view and spatial information in NeRFs to
better deal with noise. It achieves state-of-the-art results in burst denoising
and is especially successful in coping with large movement and occlusions,
under very high levels of noise. With the rapid advances in accelerating NeRFs,
it could provide a powerful platform for denoising in challenging environments.

Comments:
- to appear at CVPR 2022

---

## Gravitationally Lensed Black Hole Emission Tomography

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-07 | Aviad Levis, Pratul P. Srinivasan, Andrew A. Chael, Ren Ng, Katherine L. Bouman | cs.CV | [PDF](http://arxiv.org/pdf/2204.03715v1){: .btn .btn-green } |

**Abstract**: Measurements from the Event Horizon Telescope enabled the visualization of
light emission around a black hole for the first time. So far, these
measurements have been used to recover a 2D image under the assumption that the
emission field is static over the period of acquisition. In this work, we
propose BH-NeRF, a novel tomography approach that leverages gravitational
lensing to recover the continuous 3D emission field near a black hole. Compared
to other 3D reconstruction or tomography settings, this task poses two
significant challenges: first, rays near black holes follow curved paths
dictated by general relativity, and second, we only observe measurements from a
single viewpoint. Our method captures the unknown emission field using a
continuous volumetric function parameterized by a coordinate-based neural
network, and uses knowledge of Keplerian orbital dynamics to establish
correspondence between 3D points over time. Together, these enable BH-NeRF to
recover accurate 3D emission fields, even in challenging situations with sparse
measurements and uncertain orbital dynamics. This work takes the first steps in
showing how future measurements from the Event Horizon Telescope could be used
to recover evolving 3D emission around the supermassive black hole in our
Galactic center.

Comments:
- To appear in the IEEE Proceedings of the Conference on Computer
  Vision and Pattern Recognition (CVPR), 2022. Supplemental material including
  accompanying pdf, code, and video highlight can be found in the project page:
  http://imaging.cms.caltech.edu/bhnerf/

---

## SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-06 | Krishna Wadhwani, Tamaki Kojima | cs.CV | [PDF](http://arxiv.org/pdf/2204.02585v3){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) has emerged as the state-of-the-art method for
novel view generation of complex scenes, but is very slow during inference.
Recently, there have been multiple works on speeding up NeRF inference, but the
state of the art methods for real-time NeRF inference rely on caching the
neural network output, which occupies several giga-bytes of disk space that
limits their real-world applicability. As caching the neural network of
original NeRF network is not feasible, Garbin et al. proposed "FastNeRF" which
factorizes the problem into 2 sub-networks - one which depends only on the 3D
coordinate of a sample point and one which depends only on the 2D camera
viewing direction. Although this factorization enables them to reduce the cache
size and perform inference at over 200 frames per second, the memory overhead
is still substantial. In this work, we propose SqueezeNeRF, which is more than
60 times memory-efficient than the sparse cache of FastNeRF and is still able
to render at more than 190 frames per second on a high spec GPU during
inference.

Comments:
- 9 pages, 3 figures, 5 tables. Presented in the "5th Efficient Deep
  Learning for Computer Vision" CVPR 2022 Workshop"

---

## Unified Implicit Neural Stylization



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-05 | Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia Xu, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2204.01943v3){: .btn .btn-green } |

**Abstract**: Representing visual signals by implicit representation (e.g., a coordinate
based deep network) has prevailed among many vision tasks. This work explores a
new intriguing direction: training a stylized implicit representation, using a
generalized approach that can apply to various 2D and 3D scenarios. We conduct
a pilot study on a variety of implicit functions, including 2D coordinate-based
representation, neural radiance field, and signed distance function. Our
solution is a Unified Implicit Neural Stylization framework, dubbed INS. In
contrary to vanilla implicit representation, INS decouples the ordinary
implicit function into a style implicit module and a content implicit module,
in order to separately encode the representations from the style image and
input scenes. An amalgamation module is then applied to aggregate these
information and synthesize the stylized output. To regularize the geometry in
3D scenes, we propose a novel self-distillation geometry consistency loss which
preserves the geometry fidelity of the stylized scenes. Comprehensive
experiments are conducted on multiple task settings, including novel view
synthesis of complex scenes, stylization for implicit surfaces, and fitting
images using MLPs. We further demonstrate that the learned representation is
continuous not only spatially but also style-wise, leading to effortlessly
interpolating between different styles and generating images with new mixed
styles. Please refer to the video on our project page for more view synthesis
results: https://zhiwenfan.github.io/INS.

---

## Neural Rendering of Humans in Novel View and Pose from Monocular Video



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-04 | Tiantian Wang, Nikolaos Sarafianos, Ming-Hsuan Yang, Tony Tung | cs.CV | [PDF](http://arxiv.org/pdf/2204.01218v2){: .btn .btn-green } |

**Abstract**: We introduce a new method that generates photo-realistic humans under novel
views and poses given a monocular video as input. Despite the significant
progress recently on this topic, with several methods exploring shared
canonical neural radiance fields in dynamic scene scenarios, learning a
user-controlled model for unseen poses remains a challenging task. To tackle
this problem, we introduce an effective method to a) integrate observations
across several frames and b) encode the appearance at each individual frame. We
accomplish this by utilizing both the human pose that models the body shape as
well as point clouds that partially cover the human as input. Our approach
simultaneously learns a shared set of latent codes anchored to the human pose
among several frames, and an appearance-dependent code anchored to incomplete
point clouds generated by each frame and its predicted depth. The former human
pose-based code models the shape of the performer whereas the latter point
cloud-based code predicts fine-level details and reasons about missing
structures at the unseen poses. To further recover non-visible regions in query
frames, we employ a temporal transformer to integrate features of points in
query frames and tracked body points from automatically-selected key frames.
Experiments on various sequences of dynamic humans from different datasets
including ZJU-MoCap show that our method significantly outperforms existing
approaches under unseen poses and novel views given monocular videos as input.

Comments:
- 10 pages

---

## SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single  Image

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-04-02 | Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, Zhangyang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2204.00928v2){: .btn .btn-green } |

**Abstract**: Despite the rapid development of Neural Radiance Field (NeRF), the necessity
of dense covers largely prohibits its wider applications. While several recent
works have attempted to address this issue, they either operate with sparse
views (yet still, a few of them) or on simple objects/scenes. In this work, we
consider a more ambitious task: training neural radiance field, over
realistically complex visual scenes, by "looking only once", i.e., using only a
single view. To attain this goal, we present a Single View NeRF (SinNeRF)
framework consisting of thoughtfully designed semantic and geometry
regularizations. Specifically, SinNeRF constructs a semi-supervised learning
process, where we introduce and propagate geometry pseudo labels and semantic
pseudo labels to guide the progressive training process. Extensive experiments
are conducted on complex scene benchmarks, including NeRF synthetic dataset,
Local Light Field Fusion dataset, and DTU dataset. We show that even without
pre-training on multi-view datasets, SinNeRF can yield photo-realistic
novel-view synthesis results. Under the single image setting, SinNeRF
significantly outperforms the current state-of-the-art NeRF baselines in all
cases. Project page: https://vita-group.github.io/SinNeRF/

Comments:
- Project page: https://vita-group.github.io/SinNeRF/

---

## R2L: Distilling Neural Radiance Field to Neural Light Field for  Efficient Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-31 | Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, Sergey Tulyakov | cs.CV | [PDF](http://arxiv.org/pdf/2203.17261v2){: .btn .btn-green } |

**Abstract**: Recent research explosion on Neural Radiance Field (NeRF) shows the
encouraging potential to represent complex scenes with neural networks. One
major drawback of NeRF is its prohibitive inference time: Rendering a single
pixel requires querying the NeRF network hundreds of times. To resolve it,
existing efforts mainly attempt to reduce the number of required sampled
points. However, the problem of iterative sampling still exists. On the other
hand, Neural Light Field (NeLF) presents a more straightforward representation
over NeRF in novel view synthesis -- the rendering of a pixel amounts to one
single forward pass without ray-marching. In this work, we present a deep
residual MLP network (88 layers) to effectively learn the light field. We show
the key to successfully learning such a deep NeLF network is to have sufficient
data, for which we transfer the knowledge from a pre-trained NeRF model via
data distillation. Extensive experiments on both synthetic and real-world
scenes show the merits of our method over other counterpart algorithms. On the
synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x
runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average
PSNR improvement) rendering quality than NeRF without any customized
parallelism requirement.

Comments:
- Accepted by ECCV 2022. Code: https://github.com/snap-research/R2L

---

## MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-31 | Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng, Zicheng Liu, Xin Tong | cs.CV | [PDF](http://arxiv.org/pdf/2203.16875v2){: .btn .btn-green } |

**Abstract**: There has been rapid progress recently on 3D human rendering, including novel
view synthesis and pose animation, based on the advances of neural radiance
fields (NeRF). However, most existing methods focus on person-specific training
and their training typically requires multi-view videos. This paper deals with
a new challenging task -- rendering novel views and novel poses for a person
unseen in training, using only multiview images as input. For this task, we
propose a simple yet effective method to train a generalizable NeRF with
multiview images as conditional input. The key ingredient is a dedicated
representation combining a canonical NeRF and a volume deformation scheme.
Using a canonical space enables our method to learn shared properties of human
and easily generalize to different people. Volume deformation is used to
connect the canonical space with input and target images and query image
features for radiance and density prediction. We leverage the parametric 3D
human model fitted on the input images to derive the deformation, which works
quite well in practice when combined with our canonical NeRF. The experiments
on both real and synthetic data with the novel view synthesis and pose
animation tasks collectively demonstrate the efficacy of our method.

---

## DDNeRF: Depth Distribution Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-30 | David Dadon, Ohad Fried, Yacov Hel-Or | cs.CV | [PDF](http://arxiv.org/pdf/2203.16626v1){: .btn .btn-green } |

**Abstract**: In recent years, the field of implicit neural representation has progressed
significantly. Models such as neural radiance fields (NeRF), which uses
relatively small neural networks, can represent high-quality scenes and achieve
state-of-the-art results for novel view synthesis. Training these types of
networks, however, is still computationally very expensive. We present depth
distribution neural radiance field (DDNeRF), a new method that significantly
increases sampling efficiency along rays during training while achieving
superior results for a given sampling budget. DDNeRF achieves this by learning
a more accurate representation of the density distribution along rays. More
specifically, we train a coarse model to predict the internal distribution of
the transparency of an input volume in addition to the volume's total density.
This finer distribution then guides the sampling procedure of the fine model.
This method allows us to use fewer samples during training while reducing
computational resources.

---

## DRaCoN -- Differentiable Rasterization Conditioned Neural Radiance  Fields for Articulated Avatars

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-29 | Amit Raj, Umar Iqbal, Koki Nagano, Sameh Khamis, Pavlo Molchanov, James Hays, Jan Kautz | cs.CV | [PDF](http://arxiv.org/pdf/2203.15798v1){: .btn .btn-green } |

**Abstract**: Acquisition and creation of digital human avatars is an important problem
with applications to virtual telepresence, gaming, and human modeling. Most
contemporary approaches for avatar generation can be viewed either as 3D-based
methods, which use multi-view data to learn a 3D representation with appearance
(such as a mesh, implicit surface, or volume), or 2D-based methods which learn
photo-realistic renderings of avatars but lack accurate 3D representations. In
this work, we present, DRaCoN, a framework for learning full-body volumetric
avatars which exploits the advantages of both the 2D and 3D neural rendering
techniques. It consists of a Differentiable Rasterization module, DiffRas, that
synthesizes a low-resolution version of the target image along with additional
latent features guided by a parametric body model. The output of DiffRas is
then used as conditioning to our conditional neural 3D representation module
(c-NeRF) which generates the final high-res image along with body geometry
using volumetric rendering. While DiffRas helps in obtaining photo-realistic
image quality, c-NeRF, which employs signed distance fields (SDF) for 3D
representations, helps to obtain fine 3D geometric details. Experiments on the
challenging ZJU-MoCap and Human3.6M datasets indicate that DRaCoN outperforms
state-of-the-art methods both in terms of error metrics and visual quality.

Comments:
- Project page at https://dracon-avatars.github.io/

---

## Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene  Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-29 | Xiao Fu, Shangzhan Zhang, Tianrun Chen, Yichong Lu, Lanyun Zhu, Xiaowei Zhou, Andreas Geiger, Yiyi Liao | cs.CV | [PDF](http://arxiv.org/pdf/2203.15224v2){: .btn .btn-green } |

**Abstract**: Large-scale training data with high-quality annotations is critical for
training semantic and instance segmentation models. Unfortunately, pixel-wise
annotation is labor-intensive and costly, raising the demand for more efficient
labeling strategies. In this work, we present a novel 3D-to-2D label transfer
method, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and
instance labels from easy-to-obtain coarse 3D bounding primitives. Our method
utilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D
semantic cues transferred from existing datasets. We demonstrate that this
combination allows for improved geometry guided by semantic information,
enabling rendering of accurate semantic maps across multiple views.
Furthermore, this fusion process resolves label ambiguity of the coarse 3D
annotations and filters noise in the 2D predictions. By inferring in 3D space
and rendering to 2D labels, our 2D semantic and instance labels are multi-view
consistent by design. Experimental results show that Panoptic NeRF outperforms
existing label transfer methods in terms of accuracy and multi-view consistency
on challenging urban scenes of the KITTI-360 dataset.

Comments:
- Project page: https://fuxiao0719.github.io/projects/panopticnerf/

---

## Towards Learning Neural Representations from Shadows

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-29 | Kushagra Tiwary, Tzofi Klinghoffer, Ramesh Raskar | cs.CV | [PDF](http://arxiv.org/pdf/2203.15946v2){: .btn .btn-green } |

**Abstract**: We present a method that learns neural shadow fields which are neural scene
representations that are only learnt from the shadows present in the scene.
While traditional shape-from-shadow (SfS) algorithms reconstruct geometry from
shadows, they assume a fixed scanning setup and fail to generalize to complex
scenes. Neural rendering algorithms, on the other hand, rely on photometric
consistency between RGB images, but largely ignore physical cues such as
shadows, which have been shown to provide valuable information about the scene.
We observe that shadows are a powerful cue that can constrain neural scene
representations to learn SfS, and even outperform NeRF to reconstruct otherwise
hidden geometry. We propose a graphics-inspired differentiable approach to
render accurate shadows with volumetric rendering, predicting a shadow map that
can be compared to the ground truth shadow. Even with just binary shadow maps,
we show that neural rendering can localize the object and estimate coarse
geometry. Our approach reveals that sparse cues in images can be used to
estimate geometry using differentiable volumetric rendering. Moreover, our
framework is highly generalizable and can work alongside existing 3D
reconstruction techniques that otherwise only use photometric consistency.

---

## RGB-D Neural Radiance Fields: Local Sampling for Faster Training

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-26 | Arnab Dey, Andrew I. Comport | cs.CV | [PDF](http://arxiv.org/pdf/2203.15587v2){: .btn .btn-green } |

**Abstract**: Learning a 3D representation of a scene has been a challenging problem for
decades in computer vision. Recent advances in implicit neural representation
from images using neural radiance fields(NeRF) have shown promising results.
Some of the limitations of previous NeRF based methods include longer training
time, and inaccurate underlying geometry. The proposed method takes advantage
of RGB-D data to reduce training time by leveraging depth sensing to improve
local sampling. This paper proposes a depth-guided local sampling strategy and
a smaller neural network architecture to achieve faster training time without
compromising quality.

---

## Continuous Dynamic-NeRF: Spline-NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-25 | Julian Knodt | cs.CV | [PDF](http://arxiv.org/pdf/2203.13800v1){: .btn .btn-green } |

**Abstract**: The problem of reconstructing continuous functions over time is important for
problems such as reconstructing moving scenes, and interpolating between time
steps. Previous approaches that use deep-learning rely on regularization to
ensure that reconstructions are approximately continuous, which works well on
short sequences. As sequence length grows, though, it becomes more difficult to
regularize, and it becomes less feasible to learn only through regularization.
We propose a new architecture for function reconstruction based on classical
Bezier splines, which ensures $C^0$ and $C^1$-continuity, where $C^0$
continuity is that $\forall c:\lim\limits_{x\to c} f(x)
  = f(c)$, or more intuitively that there are no breaks at any point in the
function. In order to demonstrate our architecture, we reconstruct dynamic
scenes using Neural Radiance Fields, but hope it is clear that our approach is
general and can be applied to a variety of problems. We recover a Bezier spline
$B(\beta, t\in[0,1])$, parametrized by the control points $\beta$. Using Bezier
splines ensures reconstructions have $C^0$ and $C^1$ continuity, allowing for
guaranteed interpolation over time. We reconstruct $\beta$ with a multi-layer
perceptron (MLP), blending machine learning with classical animation
techniques. All code is available at https://github.com/JulianKnodt/nerf_atlas,
and datasets are from prior work.

---

## NeuMan: Neural Human Radiance Field from a Single Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-23 | Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, Anurag Ranjan | cs.CV | [PDF](http://arxiv.org/pdf/2203.12575v2){: .btn .btn-green } |

**Abstract**: Photorealistic rendering and reposing of humans is important for enabling
augmented reality experiences. We propose a novel framework to reconstruct the
human and the scene that can be rendered with novel human poses and views from
just a single in-the-wild video. Given a video captured by a moving camera, we
train two NeRF models: a human NeRF model and a scene NeRF model. To train
these models, we rely on existing methods to estimate the rough geometry of the
human and the scene. Those rough geometry estimates allow us to create a
warping field from the observation space to the canonical pose-independent
space, where we train the human model in. Our method is able to learn subject
specific details, including cloth wrinkles and accessories, from just a 10
seconds video clip, and to provide high quality renderings of the human under
novel poses, from novel views, together with the background.

---

## NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-21 | Xiaoshuai Zhang, Sai Bi, Kalyan Sunkavalli, Hao Su, Zexiang Xu | cs.CV | [PDF](http://arxiv.org/pdf/2203.11283v1){: .btn .btn-green } |

**Abstract**: While NeRF has shown great success for neural reconstruction and rendering,
its limited MLP capacity and long per-scene optimization times make it
challenging to model large-scale indoor scenes. In contrast, classical 3D
reconstruction methods can handle large-scale scenes but do not produce
realistic renderings. We propose NeRFusion, a method that combines the
advantages of NeRF and TSDF-based fusion techniques to achieve efficient
large-scale reconstruction and photo-realistic rendering. We process the input
image sequence to predict per-frame local radiance fields via direct network
inference. These are then fused using a novel recurrent neural network that
incrementally reconstructs a global, sparse scene representation in real-time
at 22 fps. This global volume can be further fine-tuned to boost rendering
quality. We demonstrate that NeRFusion achieves state-of-the-art quality on
both large-scale indoor and small-scale object scenes, with substantially
faster reconstruction than NeRF and other recent methods.

Comments:
- CVPR 2022

---

## Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-21 | Yuedong Chen, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2203.10821v2){: .btn .btn-green } |

**Abstract**: Image translation and manipulation have gain increasing attention along with
the rapid development of deep generative models. Although existing approaches
have brought impressive results, they mainly operated in 2D space. In light of
recent advances in NeRF-based 3D-aware generative models, we introduce a new
task, Semantic-to-NeRF translation, that aims to reconstruct a 3D scene
modelled by NeRF, conditioned on one single-view semantic mask as input. To
kick-off this novel task, we propose the Sem2NeRF framework. In particular,
Sem2NeRF addresses the highly challenging task by encoding the semantic mask
into the latent code that controls the 3D scene representation of a pre-trained
decoder. To further improve the accuracy of the mapping, we integrate a new
region-aware learning strategy into the design of both the encoder and the
decoder. We verify the efficacy of the proposed Sem2NeRF and demonstrate that
it outperforms several strong baselines on two benchmark datasets. Code and
video are available at https://donydchen.github.io/sem2nerf/

Comments:
- ECCV2022, Code: https://github.com/donydchen/sem2nerf Project Page:
  https://donydchen.github.io/sem2nerf/

---

## Conditional-Flow NeRF: Accurate 3D Modelling with Reliable Uncertainty  Quantification

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-18 | Jianxiong Shen, Antonio Agudo, Francesc Moreno-Noguer, Adria Ruiz | cs.CV | [PDF](http://arxiv.org/pdf/2203.10192v1){: .btn .btn-green } |

**Abstract**: A critical limitation of current methods based on Neural Radiance Fields
(NeRF) is that they are unable to quantify the uncertainty associated with the
learned appearance and geometry of the scene. This information is paramount in
real applications such as medical diagnosis or autonomous driving where, to
reduce potentially catastrophic failures, the confidence on the model outputs
must be included into the decision-making process. In this context, we
introduce Conditional-Flow NeRF (CF-NeRF), a novel probabilistic framework to
incorporate uncertainty quantification into NeRF-based approaches. For this
purpose, our method learns a distribution over all possible radiance fields
modelling which is used to quantify the uncertainty associated with the
modelled scene. In contrast to previous approaches enforcing strong constraints
over the radiance field distribution, CF-NeRF learns it in a flexible and fully
data-driven manner by coupling Latent Variable Modelling and Conditional
Normalizing Flows. This strategy allows to obtain reliable uncertainty
estimation while preserving model expressivity. Compared to previous
state-of-the-art methods proposed for uncertainty quantification in NeRF, our
experiments show that the proposed method achieves significantly lower
prediction errors and more reliable uncertainty values for synthetic novel view
and depth-map estimation.

---

## ViewFormer: NeRF-free Neural Rendering from Few Images Using  Transformers

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-18 | Jonáš Kulhánek, Erik Derner, Torsten Sattler, Robert Babuška | cs.CV | [PDF](http://arxiv.org/pdf/2203.10157v2){: .btn .btn-green } |

**Abstract**: Novel view synthesis is a long-standing problem. In this work, we consider a
variant of the problem where we are given only a few context views sparsely
covering a scene or an object. The goal is to predict novel viewpoints in the
scene, which requires learning priors. The current state of the art is based on
Neural Radiance Field (NeRF), and while achieving impressive results, the
methods suffer from long training times as they require evaluating millions of
3D point samples via a neural network for each image. We propose a 2D-only
method that maps multiple context views and a query pose to a new image in a
single pass of a neural network. Our model uses a two-stage architecture
consisting of a codebook and a transformer model. The codebook is used to embed
individual images into a smaller latent space, and the transformer solves the
view synthesis task in this more compact space. To train our model efficiently,
we introduce a novel branching attention mechanism that allows us to use the
same model not only for neural rendering but also for camera pose estimation.
Experimental results on real-world scenes show that our approach is competitive
compared to NeRF-based methods while not reasoning explicitly in 3D, and it is
faster to train.

Comments:
- ECCV 2022 poster

---

## Enhancement of Novel View Synthesis Using Omnidirectional Image  Completion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-18 | Takayuki Hara, Tatsuya Harada | cs.CV | [PDF](http://arxiv.org/pdf/2203.09957v4){: .btn .btn-green } |

**Abstract**: In this study, we present a method for synthesizing novel views from a single
360-degree RGB-D image based on the neural radiance field (NeRF) . Prior
studies relied on the neighborhood interpolation capability of multi-layer
perceptrons to complete missing regions caused by occlusion and zooming, which
leads to artifacts. In the method proposed in this study, the input image is
reprojected to 360-degree RGB images at other camera positions, the missing
regions of the reprojected images are completed by a 2D image generative model,
and the completed images are utilized to train the NeRF. Because multiple
completed images contain inconsistencies in 3D, we introduce a method to learn
the NeRF model using a subset of completed images that cover the target scene
with less overlap of completed regions. The selection of such a subset of
images can be attributed to the maximum weight independent set problem, which
is solved through simulated annealing. Experiments demonstrated that the
proposed method can synthesize plausible novel views while preserving the
features of the scene for both artificial and real-world data.

Comments:
- 20 pages, 19 figures

---

## TensoRF: Tensorial Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-17 | Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su | cs.CV | [PDF](http://arxiv.org/pdf/2203.09517v2){: .btn .btn-green } |

**Abstract**: We present TensoRF, a novel approach to model and reconstruct radiance
fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a
scene as a 4D tensor, which represents a 3D voxel grid with per-voxel
multi-channel features. Our central idea is to factorize the 4D scene tensor
into multiple compact low-rank tensor components. We demonstrate that applying
traditional CP decomposition -- that factorizes tensors into rank-one
components with compact vectors -- in our framework leads to improvements over
vanilla NeRF. To further boost performance, we introduce a novel vector-matrix
(VM) decomposition that relaxes the low-rank constraints for two modes of a
tensor and factorizes tensors into compact vector and matrix factors. Beyond
superior rendering quality, our models with CP and VM decompositions lead to a
significantly lower memory footprint in comparison to previous and concurrent
works that directly optimize per-voxel features. Experimentally, we demonstrate
that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with
better rendering quality and even a smaller model size (<4 MB) compared to
NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality
and outperforms previous state-of-the-art methods, while reducing the
reconstruction time (<10 min) and retaining a compact model size (<75 MB).

Comments:
- Project Page: https://apchenstu.github.io/TensoRF/

---

## Sat-NeRF: Learning Multi-View Satellite Photogrammetry With Transient  Objects and Shadow Modeling Using RPC Cameras

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-16 | Roger Marí, Gabriele Facciolo, Thibaud Ehret | cs.CV | [PDF](http://arxiv.org/pdf/2203.08896v2){: .btn .btn-green } |

**Abstract**: We introduce the Satellite Neural Radiance Field (Sat-NeRF), a new end-to-end
model for learning multi-view satellite photogrammetry in the wild. Sat-NeRF
combines some of the latest trends in neural rendering with native satellite
camera models, represented by rational polynomial coefficient (RPC) functions.
The proposed method renders new views and infers surface models of similar
quality to those obtained with traditional state-of-the-art stereo pipelines.
Multi-date images exhibit significant changes in appearance, mainly due to
varying shadows and transient objects (cars, vegetation). Robustness to these
challenges is achieved by a shadow-aware irradiance model and uncertainty
weighting to deal with transient phenomena that cannot be explained by the
position of the sun. We evaluate Sat-NeRF using WorldView-3 images from
different locations and stress the advantages of applying a bundle adjustment
to the satellite camera models prior to training. This boosts the network
performance and can optionally be used to extract additional cues for depth
supervision.

Comments:
- Accepted at CVPR EarthVision Workshop 2022

---

## Animatable Implicit Neural Representations for Creating Realistic  Avatars from Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-15 | Sida Peng, Zhen Xu, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, Xiaowei Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2203.08133v4){: .btn .btn-green } |

**Abstract**: This paper addresses the challenge of reconstructing an animatable human
model from a multi-view video. Some recent works have proposed to decompose a
non-rigidly deforming scene into a canonical neural radiance field and a set of
deformation fields that map observation-space points to the canonical space,
thereby enabling them to learn the dynamic scene from images. However, they
represent the deformation field as translational vector field or SE(3) field,
which makes the optimization highly under-constrained. Moreover, these
representations cannot be explicitly controlled by input motions. Instead, we
introduce a pose-driven deformation field based on the linear blend skinning
algorithm, which combines the blend weight field and the 3D human skeleton to
produce observation-to-canonical correspondences. Since 3D human skeletons are
more observable, they can regularize the learning of the deformation field.
Moreover, the pose-driven deformation field can be controlled by input skeletal
motions to generate new deformation fields to animate the canonical human
model. Experiments show that our approach significantly outperforms recent
human modeling methods. The code is available at
https://zju3dv.github.io/animatable_nerf/.

Comments:
- Project page: https://zju3dv.github.io/animatable_nerf/. arXiv admin
  note: substantial text overlap with arXiv:2105.02872

---

## DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video  Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-15 | Yichao Yan, Zanwei Zhou, Zi Wang, Jingnan Gao, Xiaokang Yang | cs.CV | [PDF](http://arxiv.org/pdf/2203.07931v2){: .btn .btn-green } |

**Abstract**: Conversation is an essential component of virtual avatar activities in the
metaverse. With the development of natural language processing, textual and
vocal conversation generation has achieved a significant breakthrough. However,
face-to-face conversations account for the vast majority of daily
conversations, while most existing methods focused on single-person talking
head generation. In this work, we take a step further and consider generating
realistic face-to-face conversation videos. Conversation generation is more
challenging than single-person talking head generation, since it not only
requires generating photo-realistic individual talking heads but also demands
the listener to respond to the speaker. In this paper, we propose a novel
unified framework based on neural radiance field (NeRF) to address this task.
Specifically, we model both the speaker and listener with a NeRF framework,
with different conditions to control individual expressions. The speaker is
driven by the audio signal, while the response of the listener depends on both
visual and acoustic information. In this way, face-to-face conversation videos
are generated between human avatars, with all the interlocutors modeled within
the same network. Moreover, to facilitate future research on this task, we
collect a new human conversation dataset containing 34 clips of videos.
Quantitative and qualitative experiments evaluate our method in different
aspects, e.g., image quality, pose sequence trend, and naturalness of the
rendering videos. Experimental results demonstrate that the avatars in the
resulting videos are able to perform a realistic conversation, and maintain
individual styles. All the code, data, and models will be made publicly
available.

---

## 3D-GIF: 3D-Controllable Object Generation via Implicit Factorized  Representations

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-12 | Minsoo Lee, Chaeyeon Chung, Hojun Cho, Minjung Kim, Sanghun Jung, Jaegul Choo, Minhyuk Sung | cs.CV | [PDF](http://arxiv.org/pdf/2203.06457v1){: .btn .btn-green } |

**Abstract**: While NeRF-based 3D-aware image generation methods enable viewpoint control,
limitations still remain to be adopted to various 3D applications. Due to their
view-dependent and light-entangled volume representation, the 3D geometry
presents unrealistic quality and the color should be re-rendered for every
desired viewpoint. To broaden the 3D applicability from 3D-aware image
generation to 3D-controllable object generation, we propose the factorized
representations which are view-independent and light-disentangled, and training
schemes with randomly sampled light conditions. We demonstrate the superiority
of our method by visualizing factorized representations, re-lighted images, and
albedo-textured meshes. In addition, we show that our approach improves the
quality of the generated geometry via visualization and quantitative
comparison. To the best of our knowledge, this is the first work that extracts
albedo-textured meshes with unposed 2D images without any additional labels or
assumptions.

---

## NeRFocus: Neural Radiance Field for 3D Synthetic Defocus

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-10 | Yinhuai Wang, Shuzhou Yang, Yujie Hu, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2203.05189v2){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) bring a new wave for 3D interactive
experiences. However, as an important part of the immersive experiences, the
defocus effects have not been fully explored within NeRF. Some recent
NeRF-based methods generate 3D defocus effects in a post-process fashion by
utilizing multiplane technology. Still, they are either time-consuming or
memory-consuming. This paper proposes a novel thin-lens-imaging-based NeRF
framework that can directly render various 3D defocus effects, dubbed NeRFocus.
Unlike the pinhole, the thin lens refracts rays of a scene point, so its
imaging on the sensor plane is scattered as a circle of confusion (CoC). A
direct solution sampling enough rays to approximate this process is
computationally expensive. Instead, we propose to inverse the thin lens imaging
to explicitly model the beam path for each point on the sensor plane and
generalize this paradigm to the beam path of each pixel, then use the
frustum-based volume rendering to render each pixel's beam path. We further
design an efficient probabilistic training (p-training) strategy to simplify
the training process vastly. Extensive experiments demonstrate that our
NeRFocus can achieve various 3D defocus effects with adjustable camera pose,
focus distance, and aperture size. Existing NeRF can be regarded as our special
case by setting aperture size as zero to render large depth-of-field images.
Despite such merits, NeRFocus does not sacrifice NeRF's original performance
(e.g., training and inference time, parameter consumption, rendering quality),
which implies its great potential for broader application and further
improvement. Code and video are available at
https://github.com/wyhuai/NeRFocus.

---

## NeRF-Pose: A First-Reconstruct-Then-Regress Approach for  Weakly-supervised 6D Object Pose Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-09 | Fu Li, Hao Yu, Ivan Shugurov, Benjamin Busam, Shaowu Yang, Slobodan Ilic | cs.CV | [PDF](http://arxiv.org/pdf/2203.04802v2){: .btn .btn-green } |

**Abstract**: Pose estimation of 3D objects in monocular images is a fundamental and
long-standing problem in computer vision. Existing deep learning approaches for
6D pose estimation typically rely on the assumption of availability of 3D
object models and 6D pose annotations. However, precise annotation of 6D poses
in real data is intricate, time-consuming and not scalable, while synthetic
data scales well but lacks realism. To avoid these problems, we present a
weakly-supervised reconstruction-based pipeline, named NeRF-Pose, which needs
only 2D object segmentation and known relative camera poses during training.
Following the first-reconstruct-then-regress idea, we first reconstruct the
objects from multiple views in the form of an implicit neural representation.
Then, we train a pose regression network to predict pixel-wise 2D-3D
correspondences between images and the reconstructed model. At inference, the
approach only needs a single image as input. A NeRF-enabled PnP+RANSAC
algorithm is used to estimate stable and accurate pose from the predicted
correspondences. Experiments on LineMod and LineMod-Occlusion show that the
proposed method has state-of-the-art accuracy in comparison to the best 6D pose
estimation methods in spite of being trained only with weak labels. Besides, we
extend the Homebrewed DB dataset with more real training images to support the
weakly supervised task and achieve compelling results on this dataset. The
extended dataset and code will be released soon.

---

## NeReF: Neural Refractive Field for Fluid Surface Reconstruction and  Implicit Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-08 | Ziyu Wang, Wei Yang, Junming Cao, Lan Xu, Junqing Yu, Jingyi Yu | cs.CV | [PDF](http://arxiv.org/pdf/2203.04130v1){: .btn .btn-green } |

**Abstract**: Existing neural reconstruction schemes such as Neural Radiance Field (NeRF)
are largely focused on modeling opaque objects. We present a novel neural
refractive field(NeReF) to recover wavefront of transparent fluids by
simultaneously estimating the surface position and normal of the fluid front.
Unlike prior arts that treat the reconstruction target as a single layer of the
surface, NeReF is specifically formulated to recover a volumetric normal field
with its corresponding density field. A query ray will be refracted by NeReF
according to its accumulated refractive point and normal, and we employ the
correspondences and uniqueness of refracted ray for NeReF optimization. We show
NeReF, as a global optimization scheme, can more robustly tackle refraction
distortions detrimental to traditional methods for correspondence matching.
Furthermore, the continuous NeReF representation of wavefront enables view
synthesis as well as normal integration. We validate our approach on both
synthetic and real data and show it is particularly suitable for sparse
multi-view acquisition. We hence build a small light field array and experiment
on various surface shapes to demonstrate high fidelity NeReF reconstruction.

---

## Kubric: A scalable dataset generator

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-07 | Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J. Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji,  Hsueh-Ti,  Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, Andrea Tagliasacchi | cs.CV | [PDF](http://arxiv.org/pdf/2203.03570v1){: .btn .btn-green } |

**Abstract**: Data is the driving force of machine learning, with the amount and quality of
training data often being more important for the performance of a system than
architecture and training details. But collecting, processing and annotating
real data at scale is difficult, expensive, and frequently raises additional
privacy, fairness and legal concerns. Synthetic data is a powerful tool with
the potential to address these shortcomings: 1) it is cheap 2) supports rich
ground-truth annotations 3) offers full control over data and 4) can circumvent
or mitigate problems regarding bias, privacy and licensing. Unfortunately,
software tools for effective data generation are less mature than those for
architecture design and training, which leads to fragmented generation efforts.
To address these problems we introduce Kubric, an open-source Python framework
that interfaces with PyBullet and Blender to generate photo-realistic scenes,
with rich annotations, and seamlessly scales to large jobs distributed over
thousands of machines, and generating TBs of data. We demonstrate the
effectiveness of Kubric by presenting a series of 13 different generated
datasets for tasks ranging from studying 3D NeRF models to optical flow
estimation. We release Kubric, the used assets, all of the generation code, as
well as the rendered datasets for reuse and modification.

Comments:
- 21 pages, CVPR2022

---

## NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-03 | Lin Yen-Chen, Pete Florence, Jonathan T. Barron, Tsung-Yi Lin, Alberto Rodriguez, Phillip Isola | cs.RO | [PDF](http://arxiv.org/pdf/2203.01913v2){: .btn .btn-green } |

**Abstract**: Thin, reflective objects such as forks and whisks are common in our daily
lives, but they are particularly challenging for robot perception because it is
hard to reconstruct them using commodity RGB-D cameras or multi-view stereo
techniques. While traditional pipelines struggle with objects like these,
Neural Radiance Fields (NeRFs) have recently been shown to be remarkably
effective for performing view synthesis on objects with thin structures or
reflective materials. In this paper we explore the use of NeRF as a new source
of supervision for robust robot vision systems. In particular, we demonstrate
that a NeRF representation of a scene can be used to train dense object
descriptors. We use an optimized NeRF to extract dense correspondences between
multiple views of an object, and then use these correspondences as training
data for learning a view-invariant representation of the object. NeRF's usage
of a density field allows us to reformulate the correspondence problem with a
novel distribution-of-depths formulation, as opposed to the conventional
approach of using a depth map. Dense correspondence models supervised with our
method significantly outperform off-the-shelf learned descriptors by 106%
(PCK@3px metric, more than doubling performance) and outperform our baseline
supervised with multi-view stereo by 29%. Furthermore, we demonstrate the
learned dense descriptors enable robots to perform accurate 6-degree of freedom
(6-DoF) pick and place of thin and reflective objects.

Comments:
- ICRA 2022, Website: https://yenchenlin.me/nerf-supervision/

---

## NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural  Radiance Fields



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-03 | Shanyan Guan, Huayu Deng, Yunbo Wang, Xiaokang Yang | cs.LG | [PDF](http://arxiv.org/pdf/2203.01762v2){: .btn .btn-green } |

**Abstract**: Deep learning has shown great potential for modeling the physical dynamics of
complex particle systems such as fluids. Existing approaches, however, require
the supervision of consecutive particle properties, including positions and
velocities. In this paper, we consider a partially observable scenario known as
fluid dynamics grounding, that is, inferring the state transitions and
interactions within the fluid particle systems from sequential visual
observations of the fluid surface. We propose a differentiable two-stage
network named NeuroFluid. Our approach consists of (i) a particle-driven neural
renderer, which involves fluid physical properties into the volume rendering
function, and (ii) a particle transition model optimized to reduce the
differences between the rendered and the observed images. NeuroFluid provides
the first solution to unsupervised learning of particle-based fluid dynamics by
training these two models jointly. It is shown to reasonably estimate the
underlying physics of fluids with different initial shapes, viscosity, and
densities.

Comments:
- ICML 2022, the project page: https://syguan96.github.io/NeuroFluid/

---

## Playable Environments: Video Manipulation in Space and Time



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-03 | Willi Menapace, Stéphane Lathuilière, Aliaksandr Siarohin, Christian Theobalt, Sergey Tulyakov, Vladislav Golyanik, Elisa Ricci | cs.CV | [PDF](http://arxiv.org/pdf/2203.01914v2){: .btn .btn-green } |

**Abstract**: We present Playable Environments - a new representation for interactive video
generation and manipulation in space and time. With a single image at inference
time, our novel framework allows the user to move objects in 3D while
generating a video by providing a sequence of desired actions. The actions are
learnt in an unsupervised manner. The camera can be controlled to get the
desired viewpoint. Our method builds an environment state for each frame, which
can be manipulated by our proposed action module and decoded back to the image
space with volumetric rendering. To support diverse appearances of objects, we
extend neural radiance fields with style-based modulation. Our method trains on
a collection of various monocular videos requiring only the estimated camera
parameters and 2D object locations. To set a challenging benchmark, we
introduce two large scale video datasets with significant camera movements. As
evidenced by our experiments, playable environments enable several creative
applications not attainable by prior video synthesis works, including playable
3D video generation, stylization and manipulation. Further details, code and
examples are available at
https://willi-menapace.github.io/playable-environments-website

Comments:
- CVPR 2022

---

## ICARUS: A Specialized Architecture for Neural Radiance Fields Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-03-01 | Chaolin Rao, Huangjie Yu, Haochuan Wan, Jindong Zhou, Yueyang Zheng, Yu Ma, Anpei Chen, Minye Wu, Binzhe Yuan, Pingqiang Zhou, Xin Lou, Jingyi Yu | cs.AR | [PDF](http://arxiv.org/pdf/2203.01414v3){: .btn .btn-green } |

**Abstract**: The practical deployment of Neural Radiance Fields (NeRF) in rendering
applications faces several challenges, with the most critical one being low
rendering speed on even high-end graphic processing units (GPUs). In this
paper, we present ICARUS, a specialized accelerator architecture tailored for
NeRF rendering. Unlike GPUs using general purpose computing and memory
architectures for NeRF, ICARUS executes the complete NeRF pipeline using
dedicated plenoptic cores (PLCore) consisting of a positional encoding unit
(PEU), a multi-layer perceptron (MLP) engine, and a volume rendering unit
(VRU). A PLCore takes in positions \& directions and renders the corresponding
pixel colors without any intermediate data going off-chip for temporary storage
and exchange, which can be time and power consuming. To implement the most
expensive component of NeRF, i.e., the MLP, we transform the fully connected
operations to approximated reconfigurable multiple constant multiplications
(MCMs), where common subexpressions are shared across different multiplications
to improve the computation efficiency. We build a prototype ICARUS using
Synopsys HAPS-80 S104, a field programmable gate array (FPGA)-based prototyping
system for large-scale integrated circuits and systems design. We evaluate the
power-performance-area (PPA) of a PLCore using 40nm LP CMOS technology. Working
at 400 MHz, a single PLCore occupies 16.5 $mm^2$ and consumes 282.8 mW,
translating to 0.105 uJ/sample. The results are compared with those of GPU and
tensor processing unit (TPU) implementations.

---

## Pix2NeRF: Unsupervised Conditional $π$-GAN for Single Image to Neural  Radiance Fields Translation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-02-26 | Shengqu Cai, Anton Obukhov, Dengxin Dai, Luc Van Gool | cs.CV | [PDF](http://arxiv.org/pdf/2202.13162v1){: .btn .btn-green } |

**Abstract**: We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object
or a scene of a specific class, conditioned on a single input image. This is a
challenging task, as training NeRF requires multiple views of the same scene,
coupled with corresponding poses, which are hard to obtain. Our method is based
on $\pi$-GAN, a generative model for unconditional 3D-aware image synthesis,
which maps random latent codes to radiance fields of a class of objects. We
jointly optimize (1) the $\pi$-GAN objective to utilize its high-fidelity
3D-aware generation and (2) a carefully designed reconstruction objective. The
latter includes an encoder coupled with $\pi$-GAN generator to form an
auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is
unsupervised, capable of being trained with independent images without 3D,
multi-view, or pose supervision. Applications of our pipeline include 3d avatar
generation, object-centric novel view synthesis with a single input image, and
3d-aware super-resolution, to name a few.

Comments:
- 16 pages, 10 figures

---

## Learning Multi-Object Dynamics with Compositional Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-02-24 | Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, Marc Toussaint | cs.CV | [PDF](http://arxiv.org/pdf/2202.11855v3){: .btn .btn-green } |

**Abstract**: We present a method to learn compositional multi-object dynamics models from
image observations based on implicit object encoders, Neural Radiance Fields
(NeRFs), and graph neural networks. NeRFs have become a popular choice for
representing scenes due to their strong 3D prior. However, most NeRF approaches
are trained on a single scene, representing the whole scene with a global
model, making generalization to novel scenes, containing different numbers of
objects, challenging. Instead, we present a compositional, object-centric
auto-encoder framework that maps multiple views of the scene to a set of latent
vectors representing each object separately. The latent vectors parameterize
individual NeRFs from which the scene can be reconstructed. Based on those
latent vectors, we train a graph neural network dynamics model in the latent
space to achieve compositionality for dynamics prediction. A key feature of our
approach is that the latent vectors are forced to encode 3D information through
the NeRF decoder, which enables us to incorporate structural priors in learning
the dynamics models, making long-term predictions more stable compared to
several baselines. Simulated and real world experiments show that our method
can model and learn the dynamics of compositional scenes including rigid and
deformable objects. Video: https://dannydriess.github.io/compnerfdyn/

Comments:
- v3: real robot exp

---

## Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-02-17 | Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Lan Xu, Jingyi Yu | cs.CV | [PDF](http://arxiv.org/pdf/2202.08614v2){: .btn .btn-green } |

**Abstract**: Implicit neural representations such as Neural Radiance Field (NeRF) have
focused mainly on modeling static objects captured under multi-view settings
where real-time rendering can be achieved with smart data structures, e.g.,
PlenOctree. In this paper, we present a novel Fourier PlenOctree (FPO)
technique to tackle efficient neural modeling and real-time rendering of
dynamic scenes captured under the free-view video (FVV) setting. The key idea
in our FPO is a novel combination of generalized NeRF, PlenOctree
representation, volumetric fusion and Fourier transform. To accelerate FPO
construction, we present a novel coarse-to-fine fusion scheme that leverages
the generalizable NeRF technique to generate the tree via spatial blending. To
tackle dynamic scenes, we tailor the implicit network to model the Fourier
coefficients of timevarying density and color attributes. Finally, we construct
the FPO and train the Fourier coefficients directly on the leaves of a union
PlenOctree structure of the dynamic sequence. We show that the resulting FPO
enables compact memory overload to handle dynamic objects and supports
efficient fine-tuning. Extensive experiments show that the proposed method is
3000 times faster than the original NeRF and achieves over an order of
magnitude acceleration over SOTA while preserving high visual quality for the
free-viewpoint rendering of unseen dynamic scenes.

Comments:
- Project page: https://aoliao12138.github.io/FPO/

---

## NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-02-12 | Jiakai Zhang, Liao Wang, Xinhang Liu, Fuqiang Zhao, Minzhang Li, Haizhao Dai, Boyuan Zhang, Wei Yang, Lan Xu, Jingyi Yu | cs.CV | [PDF](http://arxiv.org/pdf/2202.06088v1){: .btn .btn-green } |

**Abstract**: Some of the most exciting experiences that Metaverse promises to offer, for
instance, live interactions with virtual characters in virtual environments,
require real-time photo-realistic rendering. 3D reconstruction approaches to
rendering, active or passive, still require extensive cleanup work to fix the
meshes or point clouds. In this paper, we present a neural volumography
technique called neural volumetric video or NeuVV to support immersive,
interactive, and spatial-temporal rendering of volumetric video contents with
photo-realism and in real-time. The core of NeuVV is to efficiently encode a
dynamic neural radiance field (NeRF) into renderable and editable primitives.
We introduce two types of factorization schemes: a hyper-spherical harmonics
(HH) decomposition for modeling smooth color variations over space and time and
a learnable basis representation for modeling abrupt density and color changes
caused by motion. NeuVV factorization can be integrated into a Video Octree
(VOctree) analogous to PlenOctree to significantly accelerate training while
reducing memory overhead. Real-time NeuVV rendering further enables a class of
immersive content editing tools. Specifically, NeuVV treats each VOctree as a
primitive and implements volume-based depth ordering and alpha blending to
realize spatial-temporal compositions for content re-purposing. For example, we
demonstrate positioning varied manifestations of the same performance at
different 3D locations with different timing, adjusting color/texture of the
performer's clothing, casting spotlight shadows and synthesizing distance
falloff lighting, etc, all at an interactive speed. We further develop a hybrid
neural-rasterization rendering framework to support consumer-level VR headsets
so that the aforementioned volumetric video viewing and editing, for the first
time, can be conducted immersively in virtual 3D space.

---

## Block-NeRF: Scalable Large Scene Neural View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-02-10 | Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P. Srinivasan, Jonathan T. Barron, Henrik Kretzschmar | cs.CV | [PDF](http://arxiv.org/pdf/2202.05263v1){: .btn .btn-green } |

**Abstract**: We present Block-NeRF, a variant of Neural Radiance Fields that can represent
large-scale environments. Specifically, we demonstrate that when scaling NeRF
to render city-scale scenes spanning multiple blocks, it is vital to decompose
the scene into individually trained NeRFs. This decomposition decouples
rendering time from scene size, enables rendering to scale to arbitrarily large
environments, and allows per-block updates of the environment. We adopt several
architectural changes to make NeRF robust to data captured over months under
different environmental conditions. We add appearance embeddings, learned pose
refinement, and controllable exposure to each individual NeRF, and introduce a
procedure for aligning appearance between adjacent NeRFs so that they can be
seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to
create the largest neural scene representation to date, capable of rendering an
entire neighborhood of San Francisco.

Comments:
- Project page: https://waymo.com/research/block-nerf/

---

## PVSeRF: Joint Pixel-, Voxel- and Surface-Aligned Radiance Field for  Single-Image Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-02-10 | Xianggang Yu, Jiapeng Tang, Yipeng Qin, Chenghong Li, Linchao Bao, Xiaoguang Han, Shuguang Cui | cs.CV | [PDF](http://arxiv.org/pdf/2202.04879v1){: .btn .btn-green } |

**Abstract**: We present PVSeRF, a learning framework that reconstructs neural radiance
fields from single-view RGB images, for novel view synthesis. Previous
solutions, such as pixelNeRF, rely only on pixel-aligned features and suffer
from feature ambiguity issues. As a result, they struggle with the
disentanglement of geometry and appearance, leading to implausible geometries
and blurry results. To address this challenge, we propose to incorporate
explicit geometry reasoning and combine it with pixel-aligned features for
radiance field prediction. Specifically, in addition to pixel-aligned features,
we further constrain the radiance field learning to be conditioned on i)
voxel-aligned features learned from a coarse volumetric grid and ii) fine
surface-aligned features extracted from a regressed point cloud. We show that
the introduction of such geometry-aware features helps to achieve a better
disentanglement between appearance and geometry, i.e. recovering more accurate
geometries and synthesizing higher quality images of novel views. Extensive
experiments against state-of-the-art methods on ShapeNet benchmarks demonstrate
the superiority of our approach for single-image novel view synthesis.

---

## MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware  CT-Projections from a Single X-ray

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-02-02 | Abril Corona-Figueroa, Jonathan Frawley, Sam Bond-Taylor, Sarath Bethapudi, Hubert P. H. Shum, Chris G. Willcocks | eess.IV | [PDF](http://arxiv.org/pdf/2202.01020v3){: .btn .btn-green } |

**Abstract**: Computed tomography (CT) is an effective medical imaging modality, widely
used in the field of clinical medicine for the diagnosis of various
pathologies. Advances in Multidetector CT imaging technology have enabled
additional functionalities, including generation of thin slice multiplanar
cross-sectional body imaging and 3D reconstructions. However, this involves
patients being exposed to a considerable dose of ionising radiation. Excessive
ionising radiation can lead to deterministic and harmful effects on the body.
This paper proposes a Deep Learning model that learns to reconstruct CT
projections from a few or even a single-view X-ray. This is based on a novel
architecture that builds from neural radiance fields, which learns a continuous
representation of CT scans by disentangling the shape and volumetric depth of
surface and internal anatomical structures from 2D images. Our model is trained
on chest and knee datasets, and we demonstrate qualitative and quantitative
high-fidelity renderings and compare our approach to other recent radiance
field-based methods. Our code and link to our datasets are available at
https://github.com/abrilcf/mednerf

Comments:
- 6 pages, 4 figures, accepted at IEEE EMBC 2022

---

## CLA-NeRF: Category-Level Articulated Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-02-01 | Wei-Cheng Tseng, Hung-Ju Liao, Lin Yen-Chen, Min Sun | cs.CV | [PDF](http://arxiv.org/pdf/2202.00181v3){: .btn .btn-green } |

**Abstract**: We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field
that can perform view synthesis, part segmentation, and articulated pose
estimation. CLA-NeRF is trained at the object category level using no CAD
models and no depth, but a set of RGB images with ground truth camera poses and
part segments. During inference, it only takes a few RGB views (i.e., few-shot)
of an unseen 3D object instance within the known category to infer the object
part segmentation and the neural radiance field. Given an articulated pose as
input, CLA-NeRF can perform articulation-aware volume rendering to generate the
corresponding RGB image at any camera pose. Moreover, the articulated pose of
an object can be estimated via inverse rendering. In our experiments, we
evaluate the framework across five categories on both synthetic and real-world
data. In all cases, our method shows realistic deformation results and accurate
articulated pose estimation. We believe that both few-shot articulated object
rendering and articulated pose estimation open doors for robots to perceive and
interact with unseen articulated objects.

Comments:
- accepted by ICRA 2022

---

## From data to functa: Your data point is a function and you can treat it  like one

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-01-28 | Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Rezende, Dan Rosenbaum | cs.LG | [PDF](http://arxiv.org/pdf/2201.12204v3){: .btn .btn-green } |

**Abstract**: It is common practice in deep learning to represent a measurement of the
world on a discrete grid, e.g. a 2D grid of pixels. However, the underlying
signal represented by these measurements is often continuous, e.g. the scene
depicted in an image. A powerful continuous alternative is then to represent
these measurements using an implicit neural representation, a neural function
trained to output the appropriate measurement value for any input spatial
location. In this paper, we take this idea to its next level: what would it
take to perform deep learning on these functions instead, treating them as
data? In this context we refer to the data as functa, and propose a framework
for deep learning on functa. This view presents a number of challenges around
efficient conversion from data to functa, compact representation of functa, and
effectively solving downstream tasks on functa. We outline a recipe to overcome
these challenges and apply it to a wide range of data modalities including
images, 3D shapes, neural radiance fields (NeRF) and data on manifolds. We
demonstrate that this approach has various compelling properties across data
modalities, in particular on the canonical tasks of generative modeling, data
imputation, novel view synthesis and classification. Code:
https://github.com/deepmind/functa

---

## Point-NeRF: Point-based Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-01-21 | Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, Ulrich Neumann | cs.CV | [PDF](http://arxiv.org/pdf/2201.08845v7){: .btn .btn-green } |

**Abstract**: Volumetric neural rendering methods like NeRF generate high-quality view
synthesis results but are optimized per-scene leading to prohibitive
reconstruction time. On the other hand, deep multi-view stereo methods can
quickly reconstruct scene geometry via direct network inference. Point-NeRF
combines the advantages of these two approaches by using neural 3D point
clouds, with associated neural features, to model a radiance field. Point-NeRF
can be rendered efficiently by aggregating neural point features near scene
surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can
be initialized via direct inference of a pre-trained deep network to produce a
neural point cloud; this point cloud can be finetuned to surpass the visual
quality of NeRF with 30X faster training time. Point-NeRF can be combined with
other 3D reconstruction methods and handles the errors and outliers in such
methods via a novel pruning and growing mechanism. The experiments on the DTU,
the NeRF Synthetics , the ScanNet and the Tanks and Temples datasets
demonstrate Point-NeRF can surpass the existing methods and achieve the
state-of-the-art results.

Comments:
- Accepted to CVPR 2022 (Oral)

---

## Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-01-19 | Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, Bolei Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2201.07786v1){: .btn .btn-green } |

**Abstract**: Animating high-fidelity video portrait with speech audio is crucial for
virtual reality and digital entertainment. While most previous studies rely on
accurate explicit structural information, recent works explore the implicit
scene representation of Neural Radiance Fields (NeRF) for realistic generation.
In order to capture the inconsistent motions as well as the semantic difference
between human head and torso, some work models them via two individual sets of
NeRF, leading to unnatural results. In this work, we propose Semantic-aware
Speaking Portrait NeRF (SSP-NeRF), which creates delicate audio-driven
portraits using one unified set of NeRF. The proposed model can handle the
detailed local facial semantics and the global head-torso relationship through
two semantic-aware modules. Specifically, we first propose a Semantic-Aware
Dynamic Ray Sampling module with an additional parsing branch that facilitates
audio-driven volume rendering. Moreover, to enable portrait rendering in one
unified neural radiance field, a Torso Deformation module is designed to
stabilize the large-scale non-rigid torso motions. Extensive evaluations
demonstrate that our proposed approach renders more realistic video portraits
compared to previous methods. Project page:
https://alvinliu0.github.io/projects/SSP-NeRF

Comments:
- 12 pages, 3 figures. Project page:
  https://alvinliu0.github.io/projects/SSP-NeRF

---

## Virtual Elastic Objects



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-01-12 | Hsiao-yu Chen, Edgar Tretschk, Tuur Stuyck, Petr Kadlecek, Ladislav Kavan, Etienne Vouga, Christoph Lassner | cs.CV | [PDF](http://arxiv.org/pdf/2201.04623v1){: .btn .btn-green } |

**Abstract**: We present Virtual Elastic Objects (VEOs): virtual objects that not only look
like their real-world counterparts but also behave like them, even when subject
to novel interactions. Achieving this presents multiple challenges: not only do
objects have to be captured including the physical forces acting on them, then
faithfully reconstructed and rendered, but also plausible material parameters
found and simulated. To create VEOs, we built a multi-view capture system that
captures objects under the influence of a compressed air stream. Building on
recent advances in model-free, dynamic Neural Radiance Fields, we reconstruct
the objects and corresponding deformation fields. We propose to use a
differentiable, particle-based simulator to use these deformation fields to
find representative material parameters, which enable us to run new
simulations. To render simulated objects, we devise a method for integrating
the simulation results with Neural Radiance Fields. The resulting method is
applicable to a wide range of scenarios: it can handle objects composed of
inhomogeneous material, with very different shapes, and it can simulate
interactions with other virtual objects. We present our results using a newly
collected dataset of 12 objects under a variety of force fields, which will be
shared with the community.

---

## NeROIC: Neural Rendering of Objects from Online Image Collections



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-01-07 | Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, Sergey Tulyakov | cs.CV | [PDF](http://arxiv.org/pdf/2201.02533v2){: .btn .btn-green } |

**Abstract**: We present a novel method to acquire object representations from online image
collections, capturing high-quality geometry and material properties of
arbitrary objects from photographs with varying cameras, illumination, and
backgrounds. This enables various object-centric rendering applications such as
novel-view synthesis, relighting, and harmonized background composition from
challenging in-the-wild input. Using a multi-stage approach extending neural
radiance fields, we first infer the surface geometry and refine the coarsely
estimated initial camera parameters, while leveraging coarse foreground object
masks to improve the training efficiency and geometry quality. We also
introduce a robust normal estimation technique which eliminates the effect of
geometric noise while retaining crucial details. Lastly, we extract surface
material properties and ambient illumination, represented in spherical
harmonics with extensions that handle transient elements, e.g. sharp shadows.
The union of these components results in a highly modular and efficient object
acquisition framework. Extensive evaluations and comparisons demonstrate the
advantages of our approach in capturing high-quality geometry and appearance
properties useful for rendering applications.

Comments:
- SIGGRAPH 2022 (Journal Track). Project page:
  https://formyfamily.github.io/NeROIC/ Code repository:
  https://github.com/snap-research/NeROIC/

---

## Surface-Aligned Neural Radiance Fields for Controllable 3D Human  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-01-05 | Tianhan Xu, Yasuhiro Fujita, Eiichi Matsumoto | cs.CV | [PDF](http://arxiv.org/pdf/2201.01683v2){: .btn .btn-green } |

**Abstract**: We propose a new method for reconstructing controllable implicit 3D human
models from sparse multi-view RGB videos. Our method defines the neural scene
representation on the mesh surface points and signed distances from the surface
of a human body mesh. We identify an indistinguishability issue that arises
when a point in 3D space is mapped to its nearest surface point on a mesh for
learning surface-aligned neural scene representation. To address this issue, we
propose projecting a point onto a mesh surface using a barycentric
interpolation with modified vertex normals. Experiments with the ZJU-MoCap and
Human3.6M datasets show that our approach achieves a higher quality in a
novel-view and novel-pose synthesis than existing methods. We also demonstrate
that our method easily supports the control of body shape and clothes. Project
page: https://pfnet-research.github.io/surface-aligned-nerf/.

Comments:
- CVPR 2022. Project page:
  https://pfnet-research.github.io/surface-aligned-nerf/

---

## DFA-NeRF: Personalized Talking Head Generation via Disentangled Face  Attributes Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2022-01-03 | Shunyu Yao, RuiZhe Zhong, Yichao Yan, Guangtao Zhai, Xiaokang Yang | cs.CV | [PDF](http://arxiv.org/pdf/2201.00791v1){: .btn .btn-green } |

**Abstract**: While recent advances in deep neural networks have made it possible to render
high-quality images, generating photo-realistic and personalized talking head
remains challenging. With given audio, the key to tackling this task is
synchronizing lip movement and simultaneously generating personalized
attributes like head movement and eye blink. In this work, we observe that the
input audio is highly correlated to lip motion while less correlated to other
personalized attributes (e.g., head movements). Inspired by this, we propose a
novel framework based on neural radiance field to pursue high-fidelity and
personalized talking head generation. Specifically, neural radiance field takes
lip movements features and personalized attributes as two disentangled
conditions, where lip movements are directly predicted from the audio inputs to
achieve lip-synchronized generation. In the meanwhile, personalized attributes
are sampled from a probabilistic model, where we design a Transformer-based
variational autoencoder sampled from Gaussian Process to learn plausible and
natural-looking head pose and eye blink. Experiments on several benchmarks
demonstrate that our method achieves significantly better results than
state-of-the-art methods.