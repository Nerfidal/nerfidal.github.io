---
layout: default
title: August 2025
parent: Papers
nav_order: 202508
---

<!---metadata--->


## Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach  to Weakly-Supervised Video Anomaly Detection

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | Giacomo D'Amicantonio, Snehashis Majhi, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, Fran√ßois Bremond, Egor Bondarev | cs.CV | [PDF](http://arxiv.org/pdf/2508.06318v1){: .btn .btn-green } |

**Abstract**: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.



---

## A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a  Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | Yanxing Liang, Yinghui Wang, Jinlong Yang, Wei Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.05950v1){: .btn .btn-green } |

**Abstract**: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.



---

## UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | Wenpeng Xing, Jie Chen, Zaifeng Yang, Changting Lin, Jianfeng Dong, Chaochao Chen, Xun Zhou, Meng Han | cs.CV | [PDF](http://arxiv.org/pdf/2508.06169v1){: .btn .btn-green } |

**Abstract**: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.



---

## Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | YoungChan Choi, HengFei Wang, YiHua Cheng, Boeun Kim, Hyung Jin Chang, YoungGeun Choi, Sang-Il Choi | cs.CV | [PDF](http://arxiv.org/pdf/2508.06136v1){: .btn .btn-green } |

**Abstract**: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

Comments:
- 9 pages, 5 figures, ACM Multimeida 2025 accepted

---

## ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera  Samplings and Diffusion Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | Minsu Kim, Subin Jeon, In Cho, Mijin Yoo, Seon Joo Kim | cs.CV | [PDF](http://arxiv.org/pdf/2508.06014v1){: .btn .btn-green } |

**Abstract**: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

Comments:
- 10 pages, 6 Figures, ICCV 2025

---

## Optimization-Free Style Transfer for 3D Gaussian Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Raphael Du Sablon, David Hart | cs.CV | [PDF](http://arxiv.org/pdf/2508.05813v1){: .btn .btn-green } |

**Abstract**: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.



---

## MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown  Camera Poses

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Jong-Ik Park, Carlee Joe-Wong, Gary K. Fedder | cs.CV | [PDF](http://arxiv.org/pdf/2508.05819v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.



---

## GAP: Gaussianize Any Point Clouds with Text Guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Weiqi Zhang, Junsheng Zhou, Haotian Geng, Wenyuan Zhang, Yu-Shen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2508.05631v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving
fast and high-quality rendering. As point clouds serve as a widely-used and
easily accessible form of 3D representation, bridging the gap between point
clouds and Gaussians becomes increasingly important. Recent studies have
explored how to convert the colored points into Gaussians, but directly
generating Gaussians from colorless 3D point clouds remains an unsolved
challenge. In this paper, we propose GAP, a novel approach that gaussianizes
raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key
idea is to design a multi-view optimization framework that leverages a
depth-aware image diffusion model to synthesize consistent appearances across
different viewpoints. To ensure geometric accuracy, we introduce a
surface-anchoring mechanism that effectively constrains Gaussians to lie on the
surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a
diffuse-based inpainting strategy that specifically targets at completing
hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation
task across varying complexity levels, from synthetic point clouds to
challenging real-world scans, and even large-scale scenes. Project Page:
https://weiqi-zhang.github.io/GAP.

Comments:
- ICCV 2025. Project page: https://weiqi-zhang.github.io/GAP

---

## Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Yifan Zhou, Beizhen Zhao, Pengcheng Wu, Hao Wang | cs.GR | [PDF](http://arxiv.org/pdf/2508.04966v1){: .btn .btn-green } |

**Abstract**: While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its
extension to dynamic scenes introduces significant challenges. Existing dynamic
3DGS methods suffer from either over-smoothing due to low-rank decomposition or
feature collision from high-dimensional grid sampling. This is because of the
inherent spectral conflicts between preserving motion details and maintaining
deformation consistency at different frequency. To address these challenges, we
propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions.
Our approach contains three key innovations: a spectral-aware Laplacian
encoding architecture which merges Hash encoding and Laplacian-based module for
flexible frequency motion control, an enhanced Gaussian dynamics attribute that
compensates for photometric distortions caused by geometric deformation, and an
adaptive Gaussian split strategy guided by KDTree-based primitive control to
efficiently query and optimize dynamic areas. Through extensive experiments,
our method demonstrates state-of-the-art performance in reconstructing complex
dynamic scenes, achieving better reconstruction fidelity.



---

## 3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Junyu Zhou, Yuyang Huang, Wenrui Dai, Junni Zou, Ziyang Zheng, Nuowen Kan, Chenglin Li, Hongkai Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2508.05343v1){: .btn .btn-green } |

**Abstract**: Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time
rendering while maintaining high-fidelity novel view synthesis. However, 3DGS
resorts to the Gaussian function that is low-pass by nature and is restricted
in representing high-frequency details in 3D scenes. Moreover, it causes
redundant primitives with degraded training and rendering efficiency and
excessive memory overhead. To overcome these limitations, we propose 3D Gabor
Splatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with
multiple directional 3D frequency responses for radiance field representation
supervised by multi-view images. The proposed 3D Gabor-based primitive forms a
filter bank incorporating multiple 3D Gabor kernels at different frequencies to
enhance flexibility and efficiency in capturing fine 3D details. Furthermore,
to achieve novel view rendering, an efficient CUDA-based rasterizer is
developed to project the multiple directional 3D frequency components
characterized by 3D Gabor-based primitives onto the 2D image plane, and a
frequency-adaptive mechanism is presented for adaptive joint optimization of
primitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless
integration into existing 3DGS paradigms to enhance both efficiency and quality
of novel view synthesis. Extensive experiments demonstrate that 3DGabSplat
outperforms 3DGS and its variants using alternative primitives, and achieves
state-of-the-art rendering quality across both real-world and synthetic scenes.
Remarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously
reduced number of primitives and memory consumption.

Comments:
- Accepted by ACM MM'25

---

## Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Zijian Wang, Beizhen Zhao, Hao Wang | cs.GR | [PDF](http://arxiv.org/pdf/2508.04965v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable
capabilities in real-time and photorealistic novel view synthesis. However,
traditional 3DGS representations often struggle with large-scale scene
management and efficient storage, particularly when dealing with complex
environments or limited computational resources. To address these limitations,
we introduce a novel perceive-sample-compress framework for 3D Gaussian
Splatting. Specifically, we propose a scene perception compensation algorithm
that intelligently refines Gaussian parameters at each level. This algorithm
intelligently prioritizes visual importance for higher fidelity rendering in
critical areas, while optimizing resource usage and improving overall visible
quality. Furthermore, we propose a pyramid sampling representation to manage
Gaussian primitives across hierarchical levels. Finally, to facilitate
efficient storage of proposed hierarchical pyramid representations, we develop
a Generalized Gaussian Mixed model compression algorithm to achieve significant
compression ratios without sacrificing visual fidelity. The extensive
experiments demonstrate that our method significantly improves memory
efficiency and high visual quality while maintaining real-time rendering speed.



---

## UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for  Enhanced Sparse-View 3DGS

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Zhihao Guo, Peng Wang, Zidong Chen, Xiangyu Kong, Yan Lyu, Guanyu Gao, Liangxiu Han | cs.CV | [PDF](http://arxiv.org/pdf/2508.04968v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a competitive approach for novel view
synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian
projection and blending. However, Gaussians are treated equally weighted for
rendering in most 3DGS methods, making them prone to overfitting, which is
particularly the case in sparse-view scenarios. To address this, we investigate
how adaptive weighting of Gaussians affects rendering quality, which is
characterised by learned uncertainties proposed. This learned uncertainty
serves two key purposes: first, it guides the differentiable update of Gaussian
opacity while preserving the 3DGS pipeline integrity; second, the uncertainty
undergoes soft differentiable dropout regularisation, which strategically
transforms the original uncertainty into continuous drop probabilities that
govern the final Gaussian projection and blending process for rendering.
Extensive experimental results over widely adopted datasets demonstrate that
our method outperforms rivals in sparse-view 3D synthesis, achieving higher
quality reconstruction with fewer Gaussians in most datasets compared to
existing sparse-view approaches, e.g., compared to DropGaussian, our method
achieves 3.27\% PSNR improvements on the MipNeRF 360 dataset.

Comments:
- 11 pages, 5 figures

---

## Refining Gaussian Splatting: A Volumetric Densification Approach

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Mohamed Abdul Gafoor, Marius Preda, Titus Zaharia | cs.GR | [PDF](http://arxiv.org/pdf/2508.05187v1){: .btn .btn-green } |

**Abstract**: Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS)
often depends on effective point primitive management. The underlying Adaptive
Density Control (ADC) process addresses this issue by automating densification
and pruning. Yet, the vanilla 3DGS densification strategy shows key
shortcomings. To address this issue, in this paper we introduce a novel density
control method, which exploits the volumes of inertia associated to each
Gaussian function to guide the refinement process. Furthermore, we study the
effect of both traditional Structure from Motion (SfM) and Deep Image Matching
(DIM) methods for point cloud initialization. Extensive experimental
evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses
3DGS in reconstruction quality, delivering encouraging performance across
diverse scenes.



---

## A Study of the Framework and Real-World Applications of Language  Embedding for 3D Scene Understanding

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Mahmoud Chick Zaouali, Todd Charter, Yehor Karpichev, Brandon Haworth, Homayoun Najjjaran | cs.GR | [PDF](http://arxiv.org/pdf/2508.05064v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has rapidly emerged as a transformative technique for
real-time 3D scene representation, offering a highly efficient and expressive
alternative to Neural Radiance Fields (NeRF). Its ability to render complex
scenes with high fidelity has enabled progress across domains such as scene
reconstruction, robotics, and interactive content creation. More recently, the
integration of Large Language Models (LLMs) and language embeddings into
Gaussian Splatting pipelines has opened new possibilities for text-conditioned
generation, editing, and semantic scene understanding. Despite these advances,
a comprehensive overview of this emerging intersection has been lacking. This
survey presents a structured review of current research efforts that combine
language guidance with 3D Gaussian Splatting, detailing theoretical
foundations, integration strategies, and real-world use cases. We highlight key
limitations such as computational bottlenecks, generalizability, and the
scarcity of semantically annotated 3D Gaussian data and outline open challenges
and future directions for advancing language-guided 3D scene understanding
using Gaussian Splatting.



---

## CF3: Compact and Fast 3D Feature Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Hyunjoon Lee, Joonkyu Min, Jaesik Park | cs.CV | [PDF](http://arxiv.org/pdf/2508.05254v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D
foundation models. However, most approaches rely on a bottom-up optimization
process that treats raw 2D features as ground truth, incurring increased
computational costs. We propose a top-down pipeline for constructing compact
and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast
weighted fusion of multi-view 2D features with pre-trained Gaussians. This
approach enables training a per-Gaussian autoencoder directly on the lifted
features, instead of training autoencoders in the 2D domain. As a result, the
autoencoder better aligns with the feature distribution. More importantly, we
introduce an adaptive sparsification method that optimizes the Gaussian
attributes of the feature field while pruning and merging the redundant
Gaussians, constructing an efficient representation with preserved geometric
details. Our approach achieves a competitive 3D feature field using as little
as 5% of the Gaussians compared to Feature-3DGS.

Comments:
- ICCV 2025

---

## Bridging Diffusion Models and 3D Representations: A 3D Consistent  Super-Resolution Framework

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Yi-Ting Chen, Ting-Hsuan Liao, Pengsheng Guo, Alexander Schwing, Jia-Bin Huang | cs.CV | [PDF](http://arxiv.org/pdf/2508.04090v1){: .btn .btn-green } |

**Abstract**: We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based
super-resolution framework that leverages off-the-shelf diffusion-based 2D
super-resolution models. 3DSR encourages 3D consistency across views via the
use of an explicit 3D Gaussian-splatting-based scene representation. This makes
the proposed 3DSR different from prior work, such as image upsampling or the
use of video super-resolution, which either don't consider 3D consistency or
aim to incorporate 3D consistency implicitly. Notably, our method enhances
visual quality without additional fine-tuning, ensuring spatial coherence
within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data,
demonstrating that it produces high-resolution results that are visually
compelling, while maintaining structural consistency in 3D reconstructions.
Code will be released.

Comments:
- Accepted to ICCV 2025

---

## CryoGS: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Suyi Chen, Haibin Ling | eess.IV | [PDF](http://arxiv.org/pdf/2508.04929v1){: .btn .btn-green } |

**Abstract**: As a critical modality for structural biology, cryogenic electron microscopy
(cryo-EM) facilitates the determination of macromolecular structures at
near-atomic resolution. The core computational task in single-particle cryo-EM
is to reconstruct the 3D electrostatic potential of a molecule from a large
collection of noisy 2D projections acquired at unknown orientations. Gaussian
mixture models (GMMs) provide a continuous, compact, and physically
interpretable representation for molecular density and have recently gained
interest in cryo-EM reconstruction. However, existing methods rely on external
consensus maps or atomic models for initialization, limiting their use in
self-contained pipelines. Addressing this issue, we introduce cryoGS, a
GMM-based method that integrates Gaussian splatting with the physics of cryo-EM
image formation. In particular, we develop an orthogonal projection-aware
Gaussian splatting, with adaptations such as a normalization term and
FFT-aligned coordinate system tailored for cryo-EM imaging. All these
innovations enable stable and efficient homogeneous reconstruction directly
from raw cryo-EM particle images using random initialization. Experimental
results on real datasets validate the effectiveness and robustness of cryoGS
over representative baselines. The code will be released upon publication.



---

## DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Zexu Huang, Min Xu, Stuart Perry | cs.CV | [PDF](http://arxiv.org/pdf/2508.04099v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) represents a significant advancement in the
field of efficient and high-fidelity novel view synthesis. Despite recent
progress, achieving accurate geometric reconstruction under sparse-view
conditions remains a fundamental challenge. Existing methods often rely on
non-local depth regularization, which fails to capture fine-grained structures
and is highly sensitive to depth estimation noise. Furthermore, traditional
smoothing methods neglect semantic boundaries and indiscriminately degrade
essential edges and textures, consequently limiting the overall quality of
reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware
regularization framework for 3D Gaussian Splatting. DET-GS introduces a
hierarchical geometric depth supervision framework that adaptively enforces
multi-level geometric consistency, significantly enhancing structural fidelity
and robustness against depth estimation noise. To preserve scene boundaries, we
design an edge-aware depth regularization guided by semantic masks derived from
Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving
Total Variation loss that selectively smooths homogeneous regions while
rigorously retaining high-frequency details and textures. Extensive experiments
demonstrate that DET-GS achieves substantial improvements in both geometric
accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on
sparse-view novel view synthesis benchmarks.



---

## SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry  Decomposition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Jiahui Li, Shengeng Tang, Jingxuan He, Gang Huang, Zhangye Wang, Yantao Pan, Lechao Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2508.04224v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic 3D scenes from monocular video remains fundamentally
challenging due to the need to jointly infer motion, structure, and appearance
from limited observations. Existing dynamic scene reconstruction methods based
on Gaussian Splatting often entangle static and dynamic elements in a shared
representation, leading to motion leakage, geometric distortions, and temporal
flickering. We identify that the root cause lies in the coupled modeling of
geometry and appearance across time, which hampers both stability and
interpretability. To address this, we propose \textbf{SplitGaussian}, a novel
framework that explicitly decomposes scene representations into static and
dynamic components. By decoupling motion modeling from background geometry and
allowing only the dynamic branch to deform over time, our method prevents
motion artifacts in static regions while supporting view- and time-dependent
appearance refinement. This disentangled design not only enhances temporal
consistency and reconstruction fidelity but also accelerates convergence.
Extensive experiments demonstrate that SplitGaussian outperforms prior
state-of-the-art methods in rendering quality, geometric stability, and motion
separation.



---

## MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Yaopeng Lou, Liao Shen, Tianqi Liu, Jiaqi Li, Zihao Huang, Huiqiang Sun, Zhiguo Cao | cs.CV | [PDF](http://arxiv.org/pdf/2508.04297v1){: .btn .btn-green } |

**Abstract**: We present Multi-Baseline Gaussian Splatting (MuRF), a generalized
feed-forward approach for novel view synthesis that effectively handles diverse
baseline settings, including sparse input views with both small and large
baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and
Monocular Depth Estimation (MDE) to enhance feature representations for
generalizable reconstruction. Next, We propose a projection-and-sampling
mechanism for deep depth fusion, which constructs a fine probability volume to
guide the regression of the feature map. Furthermore, We introduce a
reference-view loss to improve geometry and optimization efficiency. We
leverage 3D Gaussian representations to accelerate training and inference time
while enhancing rendering quality. MuRF achieves state-of-the-art performance
across multiple baseline settings and diverse scenarios ranging from simple
objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also
demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360
datasets.

Comments:
- This work is accepted by ICCV 2025

---

## RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Zhan Li, Huangying Zhan, Changyang Li, Qingan Yan, Yi Xu | cs.GR | [PDF](http://arxiv.org/pdf/2508.04078v1){: .btn .btn-green } |

**Abstract**: Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive
and expert-driven process, often resulting in inconsistent reconstructions and
suboptimal results. We propose RLGS, a plug-and-play reinforcement learning
framework for adaptive hyperparameter tuning in 3DGS through lightweight policy
modules, dynamically adjusting critical hyperparameters such as learning rates
and densification thresholds. The framework is model-agnostic and seamlessly
integrates into existing 3DGS pipelines without architectural modifications. We
demonstrate its generalization ability across multiple state-of-the-art 3DGS
variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness
across diverse datasets. RLGS consistently enhances rendering quality. For
example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)
dataset, under a fixed Gaussian budget, and continues to yield gains even when
baseline performance saturates. Our results suggest that RLGS provides an
effective and general solution for automating hyperparameter tuning in 3DGS
training, bridging a gap in applying reinforcement learning to 3DGS.

Comments:
- 14 pages, 9 figures

---

## Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned  and Addressed for XR Research

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Ke Li, Mana Masuda, Susanne Schmidt, Shohei Mori | cs.GR | [PDF](http://arxiv.org/pdf/2508.04326v2){: .btn .btn-green } |

**Abstract**: The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)
and Neural Radiance Fields (NeRF), has revolutionized interactive
photorealistic view synthesis and presents enormous opportunities for XR
research and applications. However, despite the exponential growth of RF
research, RF-related contributions to the XR community remain sparse. To better
understand this research gap, we performed a systematic survey of current RF
literature to analyze (i) how RF is envisioned for XR applications, (ii) how
they have already been implemented, and (iii) the remaining research gaps. We
collected 365 RF contributions related to XR from computer vision, computer
graphics, robotics, multimedia, human-computer interaction, and XR communities,
seeking to answer the above research questions. Among the 365 papers, we
performed an analysis of 66 papers that already addressed a detailed aspect of
RF research for XR. With this survey, we extended and positioned XR-specific RF
research topics in the broader RF research field and provide a helpful resource
for the XR community to navigate within the rapid development of RF research.

Comments:
- This work is a pre-print version of a paper that has been accepted to
  the IEEE TVCG journal for future publication

---

## Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Hongyu Shen, Junfeng Ni, Yixin Chen, Weishuo Li, Mingtao Pei, Siyuan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2508.03227v1){: .btn .btn-green } |

**Abstract**: We address the challenge of lifting 2D visual segmentation to 3D in Gaussian
Splatting. Existing methods often suffer from inconsistent 2D masks across
viewpoints and produce noisy segmentation boundaries as they neglect these
semantic cues to refine the learned Gaussians. To overcome this, we introduce
Gaussian Instance Tracing (GIT), which augments the standard Gaussian
representation with an instance weight matrix across input views. Leveraging
the inherent consistency of Gaussians in 3D, we use this matrix to identify and
correct 2D segmentation inconsistencies. Furthermore, since each Gaussian
ideally corresponds to a single object, we propose a GIT-guided adaptive
density control mechanism to split and prune ambiguous Gaussians during
training, resulting in sharper and more coherent 2D and 3D segmentation
boundaries. Experimental results show that our method extracts clean 3D assets
and consistently improves 3D segmentation in both online (e.g., self-prompting)
and offline (e.g., contrastive lifting) settings, enabling applications such as
hierarchical segmentation, object extraction, and scene editing.



---

## Duplex-GS: Proxy-Guided Weighted Blending for Real-Time  Order-Independent Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Weihang Liu, Yuke Li, Yuxuan Li, Jingyi Yu, Xin Lou | cs.CV | [PDF](http://arxiv.org/pdf/2508.03180v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable
rendering fidelity and efficiency. However, these methods still rely on
computationally expensive sequential alpha-blending operations, resulting in
significant overhead, particularly on resource-constrained platforms. In this
paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy
Gaussian representations with order-independent rendering techniques to achieve
photorealistic results while sustaining real-time performance. To mitigate the
overhead caused by view-adaptive radix sort, we introduce cell proxies for
local Gaussians management and propose cell search rasterization for further
acceleration. By seamlessly combining our framework with Order-Independent
Transparency (OIT), we develop a physically inspired weighted sum rendering
technique that simultaneously eliminates "popping" and "transparency"
artifacts, yielding substantial improvements in both accuracy and efficiency.
Extensive experiments on a variety of real-world datasets demonstrate the
robustness of our method across diverse scenarios, including multi-scale
training views and large-scale environments. Our results validate the
advantages of the OIT rendering paradigm in Gaussian Splatting, achieving
high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT
based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix
sort overhead without quality degradation.



---

## H3R: Hybrid Multi-view Correspondence for Generalizable 3D  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Heng Jia, Linchao Zhu, Na Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2508.03118v1){: .btn .btn-green } |

**Abstract**: Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable
3D reconstruction remains challenging, particularly in multi-view
correspondence modeling. Existing approaches face a fundamental trade-off:
explicit methods achieve geometric precision but struggle with ambiguous
regions, while implicit methods provide robustness but suffer from slow
convergence. We present H3R, a hybrid framework that addresses this limitation
by integrating volumetric latent fusion with attention-based feature
aggregation. Our framework consists of two complementary components: an
efficient latent volume that enforces geometric consistency through epipolar
constraints, and a camera-aware Transformer that leverages Pl\"ucker
coordinates for adaptive correspondence refinement. By integrating both
paradigms, our approach enhances generalization while converging 2$\times$
faster than existing methods. Furthermore, we show that spatial-aligned
foundation models (e.g., SD-VAE) substantially outperform semantic-aligned
models (e.g., DINOv2), resolving the mismatch between semantic representations
and spatial reconstruction requirements. Our method supports variable-number
and high-resolution input views while demonstrating robust cross-dataset
generalization. Extensive experiments show that our method achieves
state-of-the-art performance across multiple benchmarks, with significant PSNR
improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and
DTU datasets, respectively. Code is available at
https://github.com/JiaHeng-DLUT/H3R.

Comments:
- ICCV 2025

---

## SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Liheng Zhang, Weihao Yu, Zubo Lu, Haozhi Gu, Jin Huang | cs.CV | [PDF](http://arxiv.org/pdf/2508.03017v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have enhanced efficient and
high-quality novel view synthesis. However, representing scenes requires a
large number of Gaussian points, leading to high storage demands and limiting
practical deployment. The latest methods facilitate the compression of Gaussian
models but struggle to identify truly insignificant Gaussian points in the
scene, leading to a decline in subsequent Gaussian pruning, compression
quality, and rendering performance. To address this issue, we propose SA-3DGS,
a method that significantly reduces storage costs while maintaining rendering
quality. SA-3DGS learns an importance score to automatically identify the least
significant Gaussians in scene reconstruction, thereby enabling effective
pruning and redundancy reduction. Next, the importance-aware clustering module
compresses Gaussians attributes more accurately into the codebook, improving
the codebook's expressive capability while reducing model size. Finally, the
codebook repair module leverages contextual scene information to repair the
codebook, thereby recovering the original Gaussian point attributes and
mitigating the degradation in rendering quality caused by information loss.
Experimental results on several benchmark datasets show that our method
achieves up to 66x compression while maintaining or even improving rendering
quality. The proposed Gaussian pruning approach is not only adaptable to but
also improves other pruning-based methods (e.g., LightGaussian), showcasing
excellent performance and strong generalization ability.

Comments:
- 9 pages, 7 figures. Under review at AAAI 2026

---

## Uni3R: Unified 3D Reconstruction and Semantic Understanding via  Generalizable Gaussian Splatting from Unposed Multi-View Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Xiangyu Sun, Haoyi jiang, Liu Liu, Seungtae Nam, Gyeongjin Kang, Xinjie wang, Wei Sui, Zhizhong Su, Wenyu Liu, Xinggang Wang, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2508.03643v2){: .btn .btn-green } |

**Abstract**: Reconstructing and semantically interpreting 3D scenes from sparse 2D views
remains a fundamental challenge in computer vision. Conventional methods often
decouple semantic understanding from reconstruction or necessitate costly
per-scene optimization, thereby restricting their scalability and
generalizability. In this paper, we introduce Uni3R, a novel feed-forward
framework that jointly reconstructs a unified 3D scene representation enriched
with open-vocabulary semantics, directly from unposed multi-view images. Our
approach leverages a Cross-View Transformer to robustly integrate information
across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian
primitives endowed with semantic feature fields. This unified representation
facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic
segmentation, and depth prediction, all within a single, feed-forward pass.
Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art
across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on
ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D
scene reconstruction and understanding. The code is available at
https://github.com/HorizonRobotics/Uni3R.

Comments:
- The code is available at https://github.com/HorizonRobotics/Uni3R

---

## RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under  Low-Quality Conditions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Anran Wu, Long Peng, Xin Di, Xueyuan Dai, Chen Wu, Yang Wang, Xueyang Fu, Yang Cao, Zheng-Jun Zha | cs.CV | [PDF](http://arxiv.org/pdf/2508.03077v1){: .btn .btn-green } |

**Abstract**: Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of
optimization-based 3DGS by enabling fast and high-quality reconstruction
without the need for per-scene optimization. However, existing feedforward
approaches typically assume that input multi-view images are clean and
high-quality. In real-world scenarios, images are often captured under
challenging conditions such as noise, low light, or rain, resulting in
inaccurate geometry and degraded 3D reconstruction. To address these
challenges, we propose a general and efficient multi-view feature enhancement
module, RobustGS, which substantially improves the robustness of feedforward
3DGS methods under various adverse imaging conditions, enabling high-quality 3D
reconstruction. The RobustGS module can be seamlessly integrated into existing
pretrained pipelines in a plug-and-play manner to enhance reconstruction
robustness. Specifically, we introduce a novel component, Generalized
Degradation Learner, designed to extract generic representations and
distributions of multiple degradations from multi-view inputs, thereby
enhancing degradation-awareness and improving the overall quality of 3D
reconstruction. In addition, we propose a novel semantic-aware state-space
model. It first leverages the extracted degradation representations to enhance
corrupted inputs in the feature space. Then, it employs a semantic-aware
strategy to aggregate semantically similar information across different views,
enabling the extraction of fine-grained cross-view correspondences and further
improving the quality of 3D representations. Extensive experiments demonstrate
that our approach, when integrated into existing methods in a plug-and-play
manner, consistently achieves state-of-the-art reconstruction quality across
various types of degradations.



---

## ScrewSplat: An End-to-End Method for Articulated Object Recognition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Seungyeon Kim, Junsu Ha, Young Hun Kim, Yonghyeon Lee, Frank C. Park | cs.RO | [PDF](http://arxiv.org/pdf/2508.02146v1){: .btn .btn-green } |

**Abstract**: Articulated object recognition -- the task of identifying both the geometry
and kinematic joints of objects with movable parts -- is essential for enabling
robots to interact with everyday objects such as doors and laptops. However,
existing approaches often rely on strong assumptions, such as a known number of
articulated parts; require additional inputs, such as depth images; or involve
complex intermediate steps that can introduce potential errors -- limiting
their practicality in real-world settings. In this paper, we introduce
ScrewSplat, a simple end-to-end method that operates solely on RGB
observations. Our approach begins by randomly initializing screw axes, which
are then iteratively optimized to recover the object's underlying kinematic
structure. By integrating with Gaussian Splatting, we simultaneously
reconstruct the 3D geometry and segment the object into rigid, movable parts.
We demonstrate that our method achieves state-of-the-art recognition accuracy
across a diverse set of articulated objects, and further enables zero-shot,
text-guided manipulation using the recovered kinematic model.

Comments:
- 26 pages, 12 figures, Conference on Robot Learning (CoRL) 2025

---

## Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Jianchao Wang, Peng Zhou, Cen Li, Rong Quan, Jie Qin | cs.CV | [PDF](http://arxiv.org/pdf/2508.02493v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a powerful and computationally efficient
representation for 3D reconstruction. Despite its strengths, 3DGS often
produces floating artifacts, which are erroneous structures detached from the
actual geometry and significantly degrade visual fidelity. The underlying
mechanisms causing these artifacts, particularly in low-quality initialization
scenarios, have not been fully explored. In this paper, we investigate the
origins of floating artifacts from a frequency-domain perspective and identify
under-optimized Gaussians as the primary source. Based on our analysis, we
propose \textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),
which selectively expands under-optimized Gaussians to prioritize accurate
low-frequency learning. Additionally, we introduce complementary depth-based
and scale-based strategies to dynamically refine Gaussian expansion,
effectively mitigating detail erosion. Extensive experiments on both synthetic
and real-world datasets demonstrate that EFA-GS substantially reduces floating
artifacts while preserving high-frequency details, achieving an improvement of
1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we
validate the effectiveness of our approach in downstream 3D editing tasks. We
provide our implementation in https://jcwang-gh.github.io/EFA-GS.

Comments:
- Project Website: https://jcwang-gh.github.io/EFA-GS

---

## SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene  Completion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Rui Qian, Haozhi Cao, Tianchen Deng, Shenghai Yuan, Lihua Xie | cs.CV | [PDF](http://arxiv.org/pdf/2508.02261v1){: .btn .btn-green } |

**Abstract**: Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising
task that aims to infer dense geometric and semantic descriptions of a scene
from a single image. While recent object-centric paradigms significantly
improve efficiency by leveraging flexible 3D Gaussian primitives, they still
rely heavily on a large number of randomly initialized primitives, which
inevitably leads to 1) inefficient primitive initialization and 2) outlier
primitives that introduce erroneous artifacts. In this paper, we propose
SplatSSC, a novel framework that resolves these limitations with a depth-guided
initialization strategy and a principled Gaussian aggregator. Instead of random
initialization, SplatSSC utilizes a dedicated depth branch composed of a
Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image
and depth features to generate a sparse yet representative set of initial
Gaussian primitives. To mitigate noise from outlier primitives, we develop the
Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing
geometric and semantic predictions during the Gaussian-to-voxel splatting
process. Complemented with a specialized Probability Scale Loss, our method
achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming
prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both
latency and memory consumption by more than 9.3%. The code will be released
upon acceptance.



---

## From Photons to Physics: Autonomous Indoor Drones and the Future of  Objective Property Assessment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Petteri Teikari, Mike Jarrell, Irene Bandera Moreno, Harri Pesola | cs.RO | [PDF](http://arxiv.org/pdf/2508.01965v1){: .btn .btn-green } |

**Abstract**: The convergence of autonomous indoor drones with physics-aware sensing
technologies promises to transform property assessment from subjective visual
inspection to objective, quantitative measurement. This comprehensive review
examines the technical foundations enabling this paradigm shift across four
critical domains: (1) platform architectures optimized for indoor navigation,
where weight constraints drive innovations in heterogeneous computing,
collision-tolerant design, and hierarchical control systems; (2) advanced
sensing modalities that extend perception beyond human vision, including
hyperspectral imaging for material identification, polarimetric sensing for
surface characterization, and computational imaging with metaphotonics enabling
radical miniaturization; (3) intelligent autonomy through active reconstruction
algorithms, where drones equipped with 3D Gaussian Splatting make strategic
decisions about viewpoint selection to maximize information gain within battery
constraints; and (4) integration pathways with existing property workflows,
including Building Information Modeling (BIM) systems and industry standards
like Uniform Appraisal Dataset (UAD) 3.6.

Comments:
- 63 pages, 5 figures

---

## VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic  Urban Scenes Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yuru Xiao, Zihan Lin, Chao Lu, Deming Zhai, Kui Jiang, Wenbo Zhao, Wei Zhang, Junjun Jiang, Huanran Wang, Xianming Liu | cs.CV | [PDF](http://arxiv.org/pdf/2508.02129v1){: .btn .btn-green } |

**Abstract**: Dynamic urban scene modeling is a rapidly evolving area with broad
applications. While current approaches leveraging neural radiance fields or
Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity
novel view synthesis, they still face significant limitations. These often stem
from a dependence on pre-calibrated object tracks or difficulties in accurately
modeling fast-moving objects from undersampled capture, particularly due to
challenges in handling temporal discontinuities. To overcome these issues, we
propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our
key insight is to distill robust, temporally consistent priors from a test-time
adapted video diffusion model. To ensure precise pose alignment and effective
integration of this denoised content, we introduce two core innovations: a
joint timestamp optimization strategy that refines interpolated frame poses,
and an uncertainty distillation method that adaptively extracts target content
while preserving well-reconstructed regions. Extensive experiments demonstrate
that our method significantly enhances dynamic modeling, especially for
fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view
synthesis over baseline approaches.



---

## GaussianCross: Cross-modal Self-supervised 3D Representation Learning  via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau | cs.CV | [PDF](http://arxiv.org/pdf/2508.02172v1){: .btn .btn-green } |

**Abstract**: The significance of informative and robust point representations has been
widely acknowledged for 3D scene understanding. Despite existing
self-supervised pre-training counterparts demonstrating promising performance,
the model collapse and structural information deficiency remain prevalent due
to insufficient point discrimination difficulty, yielding unreliable
expressions and suboptimal performance. In this paper, we present
GaussianCross, a novel cross-modal self-supervised 3D representation learning
architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques
to address current challenges. GaussianCross seamlessly converts
scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian
representation without missing details, enabling stable and generalizable
pre-training. Subsequently, a tri-attribute adaptive distillation splatting
module is incorporated to construct a 3D feature field, facilitating synergetic
feature capturing of appearance, geometry, and semantic cues to maintain
cross-modal consistency. To validate GaussianCross, we perform extensive
evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In
particular, GaussianCross shows a prominent parameter and data efficiency,
achieving superior performance through linear probing (<0.1% parameters) and
limited data training (1% of scenes) compared to state-of-the-art methods.
Furthermore, GaussianCross demonstrates strong generalization capabilities,
improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on
ScanNet200 semantic and instance segmentation tasks, respectively, supporting
the effectiveness of our approach. The code, weights, and visualizations are
publicly available at
\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.

Comments:
- 14 pages, 8 figures, accepted by MM'25

---

## Uncertainty Estimation for Novel Views in Gaussian Splatting from  Primitive-Based Representations of Error and Visibility

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Thomas Gottwald, Edgar Heinert, Matthias Rottmann | cs.GR | [PDF](http://arxiv.org/pdf/2508.02443v1){: .btn .btn-green } |

**Abstract**: In this work, we present a novel method for uncertainty estimation (UE) in
Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical
applications such as robotics and medicine. Previous methods typically estimate
the variance of Gaussian primitives and use the rendering process to obtain
pixel-wise uncertainties. Our method establishes primitive representations of
error and visibility of trainings views, which carries meaningful uncertainty
information. This representation is obtained by projection of training error
and visibility onto the primitives. Uncertainties of novel views are obtained
by rendering the primitive representations of uncertainty for those novel
views, yielding uncertainty feature maps. To aggregate these uncertainty
feature maps of novel views, we perform a pixel-wise regression on holdout
data. In our experiments, we analyze the different components of our method,
investigating various combinations of uncertainty feature maps and regression
models. Furthermore, we considered the effect of separating splatting into
foreground and background. Our UEs show high correlations to true errors,
outperforming state-of-the-art methods, especially on foreground objects. The
trained regression models show generalization capabilities to new scenes,
allowing uncertainty estimation without the need for holdout data.



---

## ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant  Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Fangxin Liu, Haomin Li, Bowen Zhu, Zongwu Wang, Zhuoran Song, Habing Guan, Li Jiang | cs.AR | [PDF](http://arxiv.org/pdf/2508.02304v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) offer significant promise for generating
photorealistic images and videos. However, existing mainstream neural rendering
models often fall short in meeting the demands for immediacy and power
efficiency in practical applications. Specifically, these models frequently
exhibit irregular access patterns and substantial computational overhead,
leading to undesirable inference latency and high power consumption.
Computing-in-memory (CIM), an emerging computational paradigm, has the
potential to address these access bottlenecks and reduce the power consumption
associated with model execution.
  To bridge the gap between model performance and real-world scene
requirements, we propose an algorithm-architecture co-design approach,
abbreviated as ASDR, a CIM-based accelerator supporting efficient neural
rendering. At the algorithmic level, we propose two rendering optimization
schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of
different pixels, thus reducing access memory and computational overhead. (2)
Reducing MLP overhead by decoupling and approximating the volume rendering of
color and density. At the architecture level, we design an efficient
ReRAM-based CIM architecture with efficient data mapping and reuse
microarchitecture. Experiments demonstrate that our design can achieve up to
$9.55\times$ and $69.75\times$ speedup over state-of-the-art NeRF accelerators
and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.

Comments:
- Accepted by the 2025 International Conference on Architectural
  Support for Programming Languages and Operating Systems (ASPLOS 2025). The
  paper will be presented at ASPLOS 2026

---

## PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal  Spans via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yijun Xu, Jingrui Zhang, Yuhan Chen, Dingwen Wang, Lei Yu, Chu He | cs.CV | [PDF](http://arxiv.org/pdf/2508.02660v1){: .btn .btn-green } |

**Abstract**: Modeling complex rigid motion across large spatiotemporal spans remains an
unresolved challenge in dynamic reconstruction. Existing paradigms are mainly
confined to short-term, small-scale deformation and offer limited consideration
for physical consistency. This study proposes PMGS, focusing on reconstructing
Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:
1) Target Modeling: achieving object-centralized reconstruction through dynamic
scene decomposition and an improved point density control; 2) Motion Recovery:
restoring full motion sequences by learning per-frame SE(3) poses. We introduce
an acceleration consistency constraint to bridge Newtonian mechanics and pose
estimation, and design a dynamic simulated annealing strategy that adaptively
schedules learning rates based on motion states. Futhermore, we devise a Kalman
fusion scheme to optimize error accumulation from multi-source observations to
mitigate disturbances. Experiments show PMGS's superior performance in
reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic
methods.



---

## GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yikuang Yuluo, Yue Ma, Kuan Shen, Tongtong Jin, Wang Liao, Yangpu Ma, Fuquan Wang | eess.IV | [PDF](http://arxiv.org/pdf/2508.02408v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT
reconstruction. However, existing methods rely on the average gradient
magnitude of points within the view, often leading to severe needle-like
artifacts under sparse-view conditions. To address this challenge, we propose
GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses
needle-like artifacts and improves reconstruction accuracy under sparse-view
conditions. Our framework introduces two key innovations: (1) a Denoised Point
Cloud Initialization Strategy that reduces initialization errors and
accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that
refines gradient computation using graph-based density differences, improving
splitting accuracy and density representation. Experiments on X-3D and
real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR
improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These
results highlight the applicability of GR-Gaussian for accurate CT
reconstruction under challenging sparse-view conditions.

Comments:
- 10

---

## GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Miko≈Çaj Zieli≈Ñski, Krzysztof Byrski, Tomasz Szczepanik, Przemys≈Çaw Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2508.02831v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently
transformed 3D scene representation and rendering. NeRF achieves high-fidelity
novel view synthesis by learning volumetric representations through neural
networks, but its implicit encoding makes editing and physical interaction
challenging. In contrast, GS represents scenes as explicit collections of
Gaussian primitives, enabling real-time rendering, faster training, and more
intuitive manipulation. This explicit structure has made GS particularly
well-suited for interactive editing and integration with physics-based
simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural
Radiance Fields Interactive Editing), a hybrid model that combines the
photorealistic rendering quality of NeRF with the editable and structured
representation of GS. Instead of using spherical harmonics for appearance
modeling, we assign each Gaussian a trainable feature embedding. These
embeddings are used to condition a NeRF network based on the k nearest
Gaussians to each query point. To make this conditioning efficient, we
introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest
Gaussian search based on a modified ray-tracing pipeline. We also integrate a
multi-resolution hash grid to initialize and update Gaussian features.
Together, these components enable real-time, locality-aware editing: as
Gaussian primitives are repositioned or modified, their interpolated influence
is immediately reflected in the rendered output. By combining the strengths of
implicit and explicit representations, GENIE supports intuitive scene
manipulation, dynamic interaction, and compatibility with physical simulation,
bridging the gap between geometry-based editing and neural rendering. The code
can be found under (https://github.com/MikolajZielinski/genie)



---

## LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Luqi Cheng, Zhangshuo Qi, Zijie Zhou, Chao Lu, Guangming Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2508.01704v1){: .btn .btn-green } |

**Abstract**: Maps play an important role in autonomous driving systems. The recently
proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit
scene reconstruction results, demonstrating the potential for map construction
in autonomous driving scenarios. However, because of the time and computational
costs involved in generating Gaussian scenes, how to update the map becomes a
significant challenge. In this paper, we propose LT-Gaussian, a map update
method for 3D-GS-based maps. LT-Gaussian consists of three main components:
Multimodal Gaussian Splatting, Structural Change Detection Module, and
Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is
generated using our proposed Multimodal Gaussian Splatting. Subsequently,
during the map update process, we compare the outdated Gaussian map with the
current LiDAR data stream to identify structural changes. Finally, we perform
targeted updates to the Gaussian-map to generate an up-to-date map. We
establish a benchmark for map updating on the nuScenes dataset to
quantitatively evaluate our method. The experimental results show that
LT-Gaussian can effectively and efficiently update the Gaussian-map, handling
common environmental changes in autonomous driving scenarios. Furthermore, by
taking full advantage of information from both new and old scenes, LT-Gaussian
is able to produce higher quality reconstruction results compared to map update
strategies that reconstruct maps from scratch. Our open-source code is
available at https://github.com/ChengLuqi/LT-gaussian.

Comments:
- Accepted by IV 2025

---

## DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Yufeng Chi, Huimin Ma, Kafeng Wang, Jianmin Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.01684v1){: .btn .btn-green } |

**Abstract**: While diffusion models have demonstrated remarkable progress in 2D image
generation and editing, extending these capabilities to 3D editing remains
challenging, particularly in maintaining multi-view consistency. Classical
approaches typically update 3D representations through iterative refinement
based on a single editing view. However, these methods often suffer from slow
convergence and blurry artifacts caused by cross-view inconsistencies. Recent
methods improve efficiency by propagating 2D editing attention features, yet
still exhibit fine-grained inconsistencies and failure modes in complex scenes
due to insufficient constraints. To address this, we propose \textbf{DisCo3D},
a novel framework that distills 3D consistency priors into a 2D editor. Our
method first fine-tunes a 3D generator using multi-view inputs for scene
adaptation, then trains a 2D editor through consistency distillation. The
edited multi-view outputs are finally optimized into 3D representations via
Gaussian Splatting. Experimental results show DisCo3D achieves stable
multi-view consistency and outperforms state-of-the-art methods in editing
quality.

Comments:
- 17 pages, 7 figures

---

## AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for  Instance-Level 3D Scene Understanding and Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Zhaonan Wang, Manyi Li, Changhe Tu | cs.CV | [PDF](http://arxiv.org/pdf/2508.01740v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has witnessed exponential adoption across
diverse applications, driving a critical need for semantic-aware 3D Gaussian
representations to enable scene understanding and editing tasks. Existing
approaches typically attach semantic features to a collection of free Gaussians
and distill the features via differentiable rendering, leading to noisy
segmentation and a messy selection of Gaussians. In this paper, we introduce
AG$^2$aussian, a novel framework that leverages an anchor-graph structure to
organize semantic features and regulate Gaussian primitives. Our anchor-graph
structure not only promotes compact and instance-aware Gaussian distributions,
but also facilitates graph-based propagation, achieving a clean and accurate
instance-level Gaussian selection. Extensive validation across four
applications, i.e. interactive click-based query, open-vocabulary text-driven
query, object removal editing, and physics simulation, demonstrates the
advantages of our approach and its benefits to various applications. The
experiments and ablation studies further evaluate the effectiveness of the key
designs of our approach.



---

## No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from  Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Ranran Huang, Krystian Mikolajczyk | cs.CV | [PDF](http://arxiv.org/pdf/2508.01171v1){: .btn .btn-green } |

**Abstract**: We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from
sparse multi-view images, requiring no ground-truth poses during training or
inference. It employs a shared feature extraction backbone, enabling
simultaneous prediction of 3D Gaussian primitives and camera poses in a
canonical space from unposed inputs within a single feed-forward step.
Alongside the rendering loss based on estimated novel-view poses, a
reprojection loss is integrated to enforce the learning of pixel-aligned
Gaussian primitives for enhanced geometric constraints. This pose-free training
paradigm and efficient one-step feed-forward design make SPFSplat well-suited
for practical applications. Remarkably, despite the absence of pose
supervision, SPFSplat achieves state-of-the-art performance in novel view
synthesis even under significant viewpoint changes and limited image overlap.
It also surpasses recent methods trained with geometry priors in relative pose
estimation. Code and trained models are available on our project page:
https://ranrhuang.github.io/spfsplat/.

Comments:
- Project Page: https://ranrhuang.github.io/spfsplat/

---

## OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian  Splatting for Refined Object-Level Understanding


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Dianyi Yang, Xihan Wang, Yu Gao, Shiyang Liu, Bohan Ren, Yufeng Yue, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2508.01150v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D scene understanding have made significant strides
in enabling interaction with scenes using open-vocabulary queries, particularly
for VR/AR and robotic applications. Nevertheless, existing methods are hindered
by rigid offline pipelines and the inability to provide precise 3D object-level
understanding given open-ended queries. In this paper, we present
OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that
improves semantic modeling and refines object-level understanding.
OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed
Distance Field to facilitate lossless fusion of semantic features on-the-fly.
Furthermore, we introduce a novel multimodal language-guided approach named
MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D
objects by adaptively adjusting similarity thresholds, achieving an improvement
17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments
demonstrate that our method outperforms existing methods in 3D object
understanding and scene reconstruction quality, as well as showcasing its
effectiveness in language-guided scene interaction. The code is available at
https://young-bit.github.io/opengs-fusion.github.io/ .

Comments:
- IROS2025

---

## OCSplats: Observation Completeness Quantification and Label Noise  Separation in 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Han Ling, Xian Xu, Yinghui Sun, Quansen Sun | cs.CV | [PDF](http://arxiv.org/pdf/2508.01239v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become one of the most promising 3D
reconstruction technologies. However, label noise in real-world scenarios-such
as moving objects, non-Lambertian surfaces, and shadows-often leads to
reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods
either fail to separate noise effectively or require scene-specific fine-tuning
of hyperparameters, making them difficult to apply in practice. This paper
re-examines the problem of anti-noise reconstruction from the perspective of
epistemic uncertainty, proposing a novel framework, OCSplats. By combining key
technologies such as hybrid noise assessment and observation-based cognitive
correction, the accuracy of noise classification in areas with cognitive
differences has been significantly improved. Moreover, to address the issue of
varying noise proportions in different scenarios, we have designed a label
noise classification pipeline based on dynamic anchor points. This pipeline
enables OCSplats to be applied simultaneously to scenarios with vastly
different noise proportions without adjusting parameters. Extensive experiments
demonstrate that OCSplats always achieve leading reconstruction performance and
precise label noise classification in scenes of different complexity levels.



---

## Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Quankai Gao, Iliyan Georgiev, Tuanfeng Y. Wang, Krishna Kumar Singh, Ulrich Neumann, Jae Shin Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2508.01464v1){: .btn .btn-green } |

**Abstract**: 3D generation has made significant progress, however, it still largely
remains at the object-level. Feedforward 3D scene-level generation has been
rarely explored due to the lack of models capable of scaling-up latent
representation learning on 3D scene-level data. Unlike object-level generative
models, which are trained on well-labeled 3D data in a bounded canonical space,
scene-level generations with 3D scenes represented by 3D Gaussian Splatting
(3DGS) are unbounded and exhibit scale inconsistency across different scenes,
making unified latent representation learning for generative purposes extremely
challenging. In this paper, we introduce Can3Tok, the first 3D scene-level
variational autoencoder (VAE) capable of encoding a large number of Gaussian
primitives into a low-dimensional latent embedding, which effectively captures
both semantic and spatial information of the inputs. Beyond model design, we
propose a general pipeline for 3D scene data processing to address scale
inconsistency issue. We validate our method on the recent scene-level 3D
dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to
novel 3D scenes, while compared methods fail to converge on even a few hundred
scene inputs during training and exhibit zero generalization ability during
inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as
our applications to demonstrate its ability to facilitate downstream generation
tasks.



---

## PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Wentao Sun, Hanqing Xu, Quanyun Wu, Dedong Zhang, Yiping Chen, Lingfei Ma, John S. Zelek, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.00259v1){: .btn .btn-green } |

**Abstract**: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

Comments:
- 22 pages, 9 figures

---

## Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a  Bimanual Robot with Handover and Gaussian Splat Merging


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Tianshuang Qiu, Zehan Ma, Karim El-Refai, Hiya Shah, Chung Min Kim, Justin Kerr, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2508.00354v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/



---

## Cooperative Perception: A Resource-Efficient Framework for Multi-Drone  3D Scene Reconstruction Using Federated Diffusion and NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Massoud Pourmandi | cs.AI | [PDF](http://arxiv.org/pdf/2508.00967v1){: .btn .btn-green } |

**Abstract**: The proposal introduces an innovative drone swarm perception system that aims
to solve problems related to computational limitations and low-bandwidth
communication, and real-time scene reconstruction. The framework enables
efficient multi-agent 3D/4D scene synthesis through federated learning of
shared diffusion model and YOLOv12 lightweight semantic extraction and local
NeRF updates while maintaining privacy and scalability. The framework redesigns
generative diffusion models for joint scene reconstruction, and improves
cooperative scene understanding, while adding semantic-aware compression
protocols. The approach can be validated through simulations and potential
real-world deployment on drone testbeds, positioning it as a disruptive
advancement in multi-agent AI for autonomous systems.

Comments:
- 15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS
  2024 template
