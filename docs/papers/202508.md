---
layout: default
title: August 2025
parent: Papers
nav_order: 202508
---

<!---metadata--->


## Uni3R: Unified 3D Reconstruction and Semantic Understanding via  Generalizable Gaussian Splatting from Unposed Multi-View Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Xiangyu Sun, Haoyi jiang, Liu Liu, Seungtae Nam, Gyeongjin Kang, Xinjie wang, Wei Sui, Zhizhong Su, Wenyu Liu, Xinggang Wang, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2508.03643v1){: .btn .btn-green } |

**Abstract**: Reconstructing and semantically interpreting 3D scenes from sparse 2D views
remains a fundamental challenge in computer vision. Conventional methods often
decouple semantic understanding from reconstruction or necessitate costly
per-scene optimization, thereby restricting their scalability and
generalizability. In this paper, we introduce Uni3R, a novel feed-forward
framework that jointly reconstructs a unified 3D scene representation enriched
with open-vocabulary semantics, directly from unposed multi-view images. Our
approach leverages a Cross-View Transformer to robustly integrate information
across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian
primitives endowed with semantic feature fields. This unified representation
facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic
segmentation, and depth prediction, all within a single, feed-forward pass.
Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art
across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on
ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D
scene reconstruction and understanding. The code is available at
https://github.com/HorizonRobotics/Uni3R.

Comments:
- The code is available at https://github.com/HorizonRobotics/Uni3R

---

## SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Liheng Zhang, Weihao Yu, Zubo Lu, Haozhi Gu, Jin Huang | cs.CV | [PDF](http://arxiv.org/pdf/2508.03017v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have enhanced efficient and
high-quality novel view synthesis. However, representing scenes requires a
large number of Gaussian points, leading to high storage demands and limiting
practical deployment. The latest methods facilitate the compression of Gaussian
models but struggle to identify truly insignificant Gaussian points in the
scene, leading to a decline in subsequent Gaussian pruning, compression
quality, and rendering performance. To address this issue, we propose SA-3DGS,
a method that significantly reduces storage costs while maintaining rendering
quality. SA-3DGS learns an importance score to automatically identify the least
significant Gaussians in scene reconstruction, thereby enabling effective
pruning and redundancy reduction. Next, the importance-aware clustering module
compresses Gaussians attributes more accurately into the codebook, improving
the codebook's expressive capability while reducing model size. Finally, the
codebook repair module leverages contextual scene information to repair the
codebook, thereby recovering the original Gaussian point attributes and
mitigating the degradation in rendering quality caused by information loss.
Experimental results on several benchmark datasets show that our method
achieves up to 66x compression while maintaining or even improving rendering
quality. The proposed Gaussian pruning approach is not only adaptable to but
also improves other pruning-based methods (e.g., LightGaussian), showcasing
excellent performance and strong generalization ability.

Comments:
- 9 pages, 7 figures. Under review at AAAI 2026

---

## Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Hongyu Shen, Junfeng Ni, Yixin Chen, Weishuo Li, Mingtao Pei, Siyuan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2508.03227v1){: .btn .btn-green } |

**Abstract**: We address the challenge of lifting 2D visual segmentation to 3D in Gaussian
Splatting. Existing methods often suffer from inconsistent 2D masks across
viewpoints and produce noisy segmentation boundaries as they neglect these
semantic cues to refine the learned Gaussians. To overcome this, we introduce
Gaussian Instance Tracing (GIT), which augments the standard Gaussian
representation with an instance weight matrix across input views. Leveraging
the inherent consistency of Gaussians in 3D, we use this matrix to identify and
correct 2D segmentation inconsistencies. Furthermore, since each Gaussian
ideally corresponds to a single object, we propose a GIT-guided adaptive
density control mechanism to split and prune ambiguous Gaussians during
training, resulting in sharper and more coherent 2D and 3D segmentation
boundaries. Experimental results show that our method extracts clean 3D assets
and consistently improves 3D segmentation in both online (e.g., self-prompting)
and offline (e.g., contrastive lifting) settings, enabling applications such as
hierarchical segmentation, object extraction, and scene editing.



---

## Duplex-GS: Proxy-Guided Weighted Blending for Real-Time  Order-Independent Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Weihang Liu, Yuke Li, Yuxuan Li, Jingyi Yu, Xin Lou | cs.CV | [PDF](http://arxiv.org/pdf/2508.03180v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable
rendering fidelity and efficiency. However, these methods still rely on
computationally expensive sequential alpha-blending operations, resulting in
significant overhead, particularly on resource-constrained platforms. In this
paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy
Gaussian representations with order-independent rendering techniques to achieve
photorealistic results while sustaining real-time performance. To mitigate the
overhead caused by view-adaptive radix sort, we introduce cell proxies for
local Gaussians management and propose cell search rasterization for further
acceleration. By seamlessly combining our framework with Order-Independent
Transparency (OIT), we develop a physically inspired weighted sum rendering
technique that simultaneously eliminates "popping" and "transparency"
artifacts, yielding substantial improvements in both accuracy and efficiency.
Extensive experiments on a variety of real-world datasets demonstrate the
robustness of our method across diverse scenarios, including multi-scale
training views and large-scale environments. Our results validate the
advantages of the OIT rendering paradigm in Gaussian Splatting, achieving
high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT
based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix
sort overhead without quality degradation.



---

## RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under  Low-Quality Conditions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Anran Wu, Long Peng, Xin Di, Xueyuan Dai, Chen Wu, Yang Wang, Xueyang Fu, Yang Cao, Zheng-Jun Zha | cs.CV | [PDF](http://arxiv.org/pdf/2508.03077v1){: .btn .btn-green } |

**Abstract**: Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of
optimization-based 3DGS by enabling fast and high-quality reconstruction
without the need for per-scene optimization. However, existing feedforward
approaches typically assume that input multi-view images are clean and
high-quality. In real-world scenarios, images are often captured under
challenging conditions such as noise, low light, or rain, resulting in
inaccurate geometry and degraded 3D reconstruction. To address these
challenges, we propose a general and efficient multi-view feature enhancement
module, RobustGS, which substantially improves the robustness of feedforward
3DGS methods under various adverse imaging conditions, enabling high-quality 3D
reconstruction. The RobustGS module can be seamlessly integrated into existing
pretrained pipelines in a plug-and-play manner to enhance reconstruction
robustness. Specifically, we introduce a novel component, Generalized
Degradation Learner, designed to extract generic representations and
distributions of multiple degradations from multi-view inputs, thereby
enhancing degradation-awareness and improving the overall quality of 3D
reconstruction. In addition, we propose a novel semantic-aware state-space
model. It first leverages the extracted degradation representations to enhance
corrupted inputs in the feature space. Then, it employs a semantic-aware
strategy to aggregate semantically similar information across different views,
enabling the extraction of fine-grained cross-view correspondences and further
improving the quality of 3D representations. Extensive experiments demonstrate
that our approach, when integrated into existing methods in a plug-and-play
manner, consistently achieves state-of-the-art reconstruction quality across
various types of degradations.



---

## H3R: Hybrid Multi-view Correspondence for Generalizable 3D  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Heng Jia, Linchao Zhu, Na Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2508.03118v1){: .btn .btn-green } |

**Abstract**: Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable
3D reconstruction remains challenging, particularly in multi-view
correspondence modeling. Existing approaches face a fundamental trade-off:
explicit methods achieve geometric precision but struggle with ambiguous
regions, while implicit methods provide robustness but suffer from slow
convergence. We present H3R, a hybrid framework that addresses this limitation
by integrating volumetric latent fusion with attention-based feature
aggregation. Our framework consists of two complementary components: an
efficient latent volume that enforces geometric consistency through epipolar
constraints, and a camera-aware Transformer that leverages Pl\"ucker
coordinates for adaptive correspondence refinement. By integrating both
paradigms, our approach enhances generalization while converging 2$\times$
faster than existing methods. Furthermore, we show that spatial-aligned
foundation models (e.g., SD-VAE) substantially outperform semantic-aligned
models (e.g., DINOv2), resolving the mismatch between semantic representations
and spatial reconstruction requirements. Our method supports variable-number
and high-resolution input views while demonstrating robust cross-dataset
generalization. Extensive experiments show that our method achieves
state-of-the-art performance across multiple benchmarks, with significant PSNR
improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and
DTU datasets, respectively. Code is available at
https://github.com/JiaHeng-DLUT/H3R.

Comments:
- ICCV 2025

---

## Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Jianchao Wang, Peng Zhou, Cen Li, Rong Quan, Jie Qin | cs.CV | [PDF](http://arxiv.org/pdf/2508.02493v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a powerful and computationally efficient
representation for 3D reconstruction. Despite its strengths, 3DGS often
produces floating artifacts, which are erroneous structures detached from the
actual geometry and significantly degrade visual fidelity. The underlying
mechanisms causing these artifacts, particularly in low-quality initialization
scenarios, have not been fully explored. In this paper, we investigate the
origins of floating artifacts from a frequency-domain perspective and identify
under-optimized Gaussians as the primary source. Based on our analysis, we
propose \textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),
which selectively expands under-optimized Gaussians to prioritize accurate
low-frequency learning. Additionally, we introduce complementary depth-based
and scale-based strategies to dynamically refine Gaussian expansion,
effectively mitigating detail erosion. Extensive experiments on both synthetic
and real-world datasets demonstrate that EFA-GS substantially reduces floating
artifacts while preserving high-frequency details, achieving an improvement of
1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we
validate the effectiveness of our approach in downstream 3D editing tasks. We
provide our implementation in https://jcwang-gh.github.io/EFA-GS.

Comments:
- Project Website: https://jcwang-gh.github.io/EFA-GS

---

## SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene  Completion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Rui Qian, Haozhi Cao, Tianchen Deng, Shenghai Yuan, Lihua Xie | cs.CV | [PDF](http://arxiv.org/pdf/2508.02261v1){: .btn .btn-green } |

**Abstract**: Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising
task that aims to infer dense geometric and semantic descriptions of a scene
from a single image. While recent object-centric paradigms significantly
improve efficiency by leveraging flexible 3D Gaussian primitives, they still
rely heavily on a large number of randomly initialized primitives, which
inevitably leads to 1) inefficient primitive initialization and 2) outlier
primitives that introduce erroneous artifacts. In this paper, we propose
SplatSSC, a novel framework that resolves these limitations with a depth-guided
initialization strategy and a principled Gaussian aggregator. Instead of random
initialization, SplatSSC utilizes a dedicated depth branch composed of a
Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image
and depth features to generate a sparse yet representative set of initial
Gaussian primitives. To mitigate noise from outlier primitives, we develop the
Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing
geometric and semantic predictions during the Gaussian-to-voxel splatting
process. Complemented with a specialized Probability Scale Loss, our method
achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming
prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both
latency and memory consumption by more than 9.3%. The code will be released
upon acceptance.



---

## From Photons to Physics: Autonomous Indoor Drones and the Future of  Objective Property Assessment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Petteri Teikari, Mike Jarrell, Irene Bandera Moreno, Harri Pesola | cs.RO | [PDF](http://arxiv.org/pdf/2508.01965v1){: .btn .btn-green } |

**Abstract**: The convergence of autonomous indoor drones with physics-aware sensing
technologies promises to transform property assessment from subjective visual
inspection to objective, quantitative measurement. This comprehensive review
examines the technical foundations enabling this paradigm shift across four
critical domains: (1) platform architectures optimized for indoor navigation,
where weight constraints drive innovations in heterogeneous computing,
collision-tolerant design, and hierarchical control systems; (2) advanced
sensing modalities that extend perception beyond human vision, including
hyperspectral imaging for material identification, polarimetric sensing for
surface characterization, and computational imaging with metaphotonics enabling
radical miniaturization; (3) intelligent autonomy through active reconstruction
algorithms, where drones equipped with 3D Gaussian Splatting make strategic
decisions about viewpoint selection to maximize information gain within battery
constraints; and (4) integration pathways with existing property workflows,
including Building Information Modeling (BIM) systems and industry standards
like Uniform Appraisal Dataset (UAD) 3.6.

Comments:
- 63 pages, 5 figures

---

## VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic  Urban Scenes Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yuru Xiao, Zihan Lin, Chao Lu, Deming Zhai, Kui Jiang, Wenbo Zhao, Wei Zhang, Junjun Jiang, Huanran Wang, Xianming Liu | cs.CV | [PDF](http://arxiv.org/pdf/2508.02129v1){: .btn .btn-green } |

**Abstract**: Dynamic urban scene modeling is a rapidly evolving area with broad
applications. While current approaches leveraging neural radiance fields or
Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity
novel view synthesis, they still face significant limitations. These often stem
from a dependence on pre-calibrated object tracks or difficulties in accurately
modeling fast-moving objects from undersampled capture, particularly due to
challenges in handling temporal discontinuities. To overcome these issues, we
propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our
key insight is to distill robust, temporally consistent priors from a test-time
adapted video diffusion model. To ensure precise pose alignment and effective
integration of this denoised content, we introduce two core innovations: a
joint timestamp optimization strategy that refines interpolated frame poses,
and an uncertainty distillation method that adaptively extracts target content
while preserving well-reconstructed regions. Extensive experiments demonstrate
that our method significantly enhances dynamic modeling, especially for
fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view
synthesis over baseline approaches.



---

## ScrewSplat: An End-to-End Method for Articulated Object Recognition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Seungyeon Kim, Junsu Ha, Young Hun Kim, Yonghyeon Lee, Frank C. Park | cs.RO | [PDF](http://arxiv.org/pdf/2508.02146v1){: .btn .btn-green } |

**Abstract**: Articulated object recognition -- the task of identifying both the geometry
and kinematic joints of objects with movable parts -- is essential for enabling
robots to interact with everyday objects such as doors and laptops. However,
existing approaches often rely on strong assumptions, such as a known number of
articulated parts; require additional inputs, such as depth images; or involve
complex intermediate steps that can introduce potential errors -- limiting
their practicality in real-world settings. In this paper, we introduce
ScrewSplat, a simple end-to-end method that operates solely on RGB
observations. Our approach begins by randomly initializing screw axes, which
are then iteratively optimized to recover the object's underlying kinematic
structure. By integrating with Gaussian Splatting, we simultaneously
reconstruct the 3D geometry and segment the object into rigid, movable parts.
We demonstrate that our method achieves state-of-the-art recognition accuracy
across a diverse set of articulated objects, and further enables zero-shot,
text-guided manipulation using the recovered kinematic model.

Comments:
- 26 pages, 12 figures, Conference on Robot Learning (CoRL) 2025

---

## GaussianCross: Cross-modal Self-supervised 3D Representation Learning  via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau | cs.CV | [PDF](http://arxiv.org/pdf/2508.02172v1){: .btn .btn-green } |

**Abstract**: The significance of informative and robust point representations has been
widely acknowledged for 3D scene understanding. Despite existing
self-supervised pre-training counterparts demonstrating promising performance,
the model collapse and structural information deficiency remain prevalent due
to insufficient point discrimination difficulty, yielding unreliable
expressions and suboptimal performance. In this paper, we present
GaussianCross, a novel cross-modal self-supervised 3D representation learning
architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques
to address current challenges. GaussianCross seamlessly converts
scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian
representation without missing details, enabling stable and generalizable
pre-training. Subsequently, a tri-attribute adaptive distillation splatting
module is incorporated to construct a 3D feature field, facilitating synergetic
feature capturing of appearance, geometry, and semantic cues to maintain
cross-modal consistency. To validate GaussianCross, we perform extensive
evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In
particular, GaussianCross shows a prominent parameter and data efficiency,
achieving superior performance through linear probing (<0.1% parameters) and
limited data training (1% of scenes) compared to state-of-the-art methods.
Furthermore, GaussianCross demonstrates strong generalization capabilities,
improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on
ScanNet200 semantic and instance segmentation tasks, respectively, supporting
the effectiveness of our approach. The code, weights, and visualizations are
publicly available at
\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.

Comments:
- 14 pages, 8 figures, accepted by MM'25

---

## GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Mikołaj Zieliński, Krzysztof Byrski, Tomasz Szczepanik, Przemysław Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2508.02831v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently
transformed 3D scene representation and rendering. NeRF achieves high-fidelity
novel view synthesis by learning volumetric representations through neural
networks, but its implicit encoding makes editing and physical interaction
challenging. In contrast, GS represents scenes as explicit collections of
Gaussian primitives, enabling real-time rendering, faster training, and more
intuitive manipulation. This explicit structure has made GS particularly
well-suited for interactive editing and integration with physics-based
simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural
Radiance Fields Interactive Editing), a hybrid model that combines the
photorealistic rendering quality of NeRF with the editable and structured
representation of GS. Instead of using spherical harmonics for appearance
modeling, we assign each Gaussian a trainable feature embedding. These
embeddings are used to condition a NeRF network based on the k nearest
Gaussians to each query point. To make this conditioning efficient, we
introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest
Gaussian search based on a modified ray-tracing pipeline. We also integrate a
multi-resolution hash grid to initialize and update Gaussian features.
Together, these components enable real-time, locality-aware editing: as
Gaussian primitives are repositioned or modified, their interpolated influence
is immediately reflected in the rendered output. By combining the strengths of
implicit and explicit representations, GENIE supports intuitive scene
manipulation, dynamic interaction, and compatibility with physical simulation,
bridging the gap between geometry-based editing and neural rendering. The code
can be found under (https://github.com/MikolajZielinski/genie)



---

## ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant  Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Fangxin Liu, Haomin Li, Bowen Zhu, Zongwu Wang, Zhuoran Song, Habing Guan, Li Jiang | cs.AR | [PDF](http://arxiv.org/pdf/2508.02304v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) offer significant promise for generating
photorealistic images and videos. However, existing mainstream neural rendering
models often fall short in meeting the demands for immediacy and power
efficiency in practical applications. Specifically, these models frequently
exhibit irregular access patterns and substantial computational overhead,
leading to undesirable inference latency and high power consumption.
Computing-in-memory (CIM), an emerging computational paradigm, has the
potential to address these access bottlenecks and reduce the power consumption
associated with model execution.
  To bridge the gap between model performance and real-world scene
requirements, we propose an algorithm-architecture co-design approach,
abbreviated as ASDR, a CIM-based accelerator supporting efficient neural
rendering. At the algorithmic level, we propose two rendering optimization
schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of
different pixels, thus reducing access memory and computational overhead. (2)
Reducing MLP overhead by decoupling and approximating the volume rendering of
color and density. At the architecture level, we design an efficient
ReRAM-based CIM architecture with efficient data mapping and reuse
microarchitecture. Experiments demonstrate that our design can achieve up to
$9.55\times$ and $69.75\times$ speedup over state-of-the-art NeRF accelerators
and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.

Comments:
- Accepted by the 2025 International Conference on Architectural
  Support for Programming Languages and Operating Systems (ASPLOS 2025). The
  paper will be presented at ASPLOS 2026

---

## PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal  Spans via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yijun Xu, Jingrui Zhang, Yuhan Chen, Dingwen Wang, Lei Yu, Chu He | cs.CV | [PDF](http://arxiv.org/pdf/2508.02660v1){: .btn .btn-green } |

**Abstract**: Modeling complex rigid motion across large spatiotemporal spans remains an
unresolved challenge in dynamic reconstruction. Existing paradigms are mainly
confined to short-term, small-scale deformation and offer limited consideration
for physical consistency. This study proposes PMGS, focusing on reconstructing
Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:
1) Target Modeling: achieving object-centralized reconstruction through dynamic
scene decomposition and an improved point density control; 2) Motion Recovery:
restoring full motion sequences by learning per-frame SE(3) poses. We introduce
an acceleration consistency constraint to bridge Newtonian mechanics and pose
estimation, and design a dynamic simulated annealing strategy that adaptively
schedules learning rates based on motion states. Futhermore, we devise a Kalman
fusion scheme to optimize error accumulation from multi-source observations to
mitigate disturbances. Experiments show PMGS's superior performance in
reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic
methods.



---

## GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yikuang Yuluo, Yue Ma, Kuan Shen, Tongtong Jin, Wang Liao, Yangpu Ma, Fuquan Wang | eess.IV | [PDF](http://arxiv.org/pdf/2508.02408v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT
reconstruction. However, existing methods rely on the average gradient
magnitude of points within the view, often leading to severe needle-like
artifacts under sparse-view conditions. To address this challenge, we propose
GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses
needle-like artifacts and improves reconstruction accuracy under sparse-view
conditions. Our framework introduces two key innovations: (1) a Denoised Point
Cloud Initialization Strategy that reduces initialization errors and
accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that
refines gradient computation using graph-based density differences, improving
splitting accuracy and density representation. Experiments on X-3D and
real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR
improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These
results highlight the applicability of GR-Gaussian for accurate CT
reconstruction under challenging sparse-view conditions.

Comments:
- 10

---

## Uncertainty Estimation for Novel Views in Gaussian Splatting from  Primitive-Based Representations of Error and Visibility

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Thomas Gottwald, Edgar Heinert, Matthias Rottmann | cs.GR | [PDF](http://arxiv.org/pdf/2508.02443v1){: .btn .btn-green } |

**Abstract**: In this work, we present a novel method for uncertainty estimation (UE) in
Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical
applications such as robotics and medicine. Previous methods typically estimate
the variance of Gaussian primitives and use the rendering process to obtain
pixel-wise uncertainties. Our method establishes primitive representations of
error and visibility of trainings views, which carries meaningful uncertainty
information. This representation is obtained by projection of training error
and visibility onto the primitives. Uncertainties of novel views are obtained
by rendering the primitive representations of uncertainty for those novel
views, yielding uncertainty feature maps. To aggregate these uncertainty
feature maps of novel views, we perform a pixel-wise regression on holdout
data. In our experiments, we analyze the different components of our method,
investigating various combinations of uncertainty feature maps and regression
models. Furthermore, we considered the effect of separating splatting into
foreground and background. Our UEs show high correlations to true errors,
outperforming state-of-the-art methods, especially on foreground objects. The
trained regression models show generalization capabilities to new scenes,
allowing uncertainty estimation without the need for holdout data.



---

## DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Yufeng Chi, Huimin Ma, Kafeng Wang, Jianmin Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.01684v1){: .btn .btn-green } |

**Abstract**: While diffusion models have demonstrated remarkable progress in 2D image
generation and editing, extending these capabilities to 3D editing remains
challenging, particularly in maintaining multi-view consistency. Classical
approaches typically update 3D representations through iterative refinement
based on a single editing view. However, these methods often suffer from slow
convergence and blurry artifacts caused by cross-view inconsistencies. Recent
methods improve efficiency by propagating 2D editing attention features, yet
still exhibit fine-grained inconsistencies and failure modes in complex scenes
due to insufficient constraints. To address this, we propose \textbf{DisCo3D},
a novel framework that distills 3D consistency priors into a 2D editor. Our
method first fine-tunes a 3D generator using multi-view inputs for scene
adaptation, then trains a 2D editor through consistency distillation. The
edited multi-view outputs are finally optimized into 3D representations via
Gaussian Splatting. Experimental results show DisCo3D achieves stable
multi-view consistency and outperforms state-of-the-art methods in editing
quality.

Comments:
- 17 pages, 7 figures

---

## AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for  Instance-Level 3D Scene Understanding and Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Zhaonan Wang, Manyi Li, Changhe Tu | cs.CV | [PDF](http://arxiv.org/pdf/2508.01740v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has witnessed exponential adoption across
diverse applications, driving a critical need for semantic-aware 3D Gaussian
representations to enable scene understanding and editing tasks. Existing
approaches typically attach semantic features to a collection of free Gaussians
and distill the features via differentiable rendering, leading to noisy
segmentation and a messy selection of Gaussians. In this paper, we introduce
AG$^2$aussian, a novel framework that leverages an anchor-graph structure to
organize semantic features and regulate Gaussian primitives. Our anchor-graph
structure not only promotes compact and instance-aware Gaussian distributions,
but also facilitates graph-based propagation, achieving a clean and accurate
instance-level Gaussian selection. Extensive validation across four
applications, i.e. interactive click-based query, open-vocabulary text-driven
query, object removal editing, and physics simulation, demonstrates the
advantages of our approach and its benefits to various applications. The
experiments and ablation studies further evaluate the effectiveness of the key
designs of our approach.



---

## LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Luqi Cheng, Zhangshuo Qi, Zijie Zhou, Chao Lu, Guangming Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2508.01704v1){: .btn .btn-green } |

**Abstract**: Maps play an important role in autonomous driving systems. The recently
proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit
scene reconstruction results, demonstrating the potential for map construction
in autonomous driving scenarios. However, because of the time and computational
costs involved in generating Gaussian scenes, how to update the map becomes a
significant challenge. In this paper, we propose LT-Gaussian, a map update
method for 3D-GS-based maps. LT-Gaussian consists of three main components:
Multimodal Gaussian Splatting, Structural Change Detection Module, and
Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is
generated using our proposed Multimodal Gaussian Splatting. Subsequently,
during the map update process, we compare the outdated Gaussian map with the
current LiDAR data stream to identify structural changes. Finally, we perform
targeted updates to the Gaussian-map to generate an up-to-date map. We
establish a benchmark for map updating on the nuScenes dataset to
quantitatively evaluate our method. The experimental results show that
LT-Gaussian can effectively and efficiently update the Gaussian-map, handling
common environmental changes in autonomous driving scenarios. Furthermore, by
taking full advantage of information from both new and old scenes, LT-Gaussian
is able to produce higher quality reconstruction results compared to map update
strategies that reconstruct maps from scratch. Our open-source code is
available at https://github.com/ChengLuqi/LT-gaussian.

Comments:
- Accepted by IV 2025

---

## OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian  Splatting for Refined Object-Level Understanding


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Dianyi Yang, Xihan Wang, Yu Gao, Shiyang Liu, Bohan Ren, Yufeng Yue, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2508.01150v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D scene understanding have made significant strides
in enabling interaction with scenes using open-vocabulary queries, particularly
for VR/AR and robotic applications. Nevertheless, existing methods are hindered
by rigid offline pipelines and the inability to provide precise 3D object-level
understanding given open-ended queries. In this paper, we present
OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that
improves semantic modeling and refines object-level understanding.
OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed
Distance Field to facilitate lossless fusion of semantic features on-the-fly.
Furthermore, we introduce a novel multimodal language-guided approach named
MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D
objects by adaptively adjusting similarity thresholds, achieving an improvement
17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments
demonstrate that our method outperforms existing methods in 3D object
understanding and scene reconstruction quality, as well as showcasing its
effectiveness in language-guided scene interaction. The code is available at
https://young-bit.github.io/opengs-fusion.github.io/ .

Comments:
- IROS2025

---

## No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from  Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Ranran Huang, Krystian Mikolajczyk | cs.CV | [PDF](http://arxiv.org/pdf/2508.01171v1){: .btn .btn-green } |

**Abstract**: We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from
sparse multi-view images, requiring no ground-truth poses during training or
inference. It employs a shared feature extraction backbone, enabling
simultaneous prediction of 3D Gaussian primitives and camera poses in a
canonical space from unposed inputs within a single feed-forward step.
Alongside the rendering loss based on estimated novel-view poses, a
reprojection loss is integrated to enforce the learning of pixel-aligned
Gaussian primitives for enhanced geometric constraints. This pose-free training
paradigm and efficient one-step feed-forward design make SPFSplat well-suited
for practical applications. Remarkably, despite the absence of pose
supervision, SPFSplat achieves state-of-the-art performance in novel view
synthesis even under significant viewpoint changes and limited image overlap.
It also surpasses recent methods trained with geometry priors in relative pose
estimation. Code and trained models are available on our project page:
https://ranrhuang.github.io/spfsplat/.

Comments:
- Project Page: https://ranrhuang.github.io/spfsplat/

---

## OCSplats: Observation Completeness Quantification and Label Noise  Separation in 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Han Ling, Xian Xu, Yinghui Sun, Quansen Sun | cs.CV | [PDF](http://arxiv.org/pdf/2508.01239v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become one of the most promising 3D
reconstruction technologies. However, label noise in real-world scenarios-such
as moving objects, non-Lambertian surfaces, and shadows-often leads to
reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods
either fail to separate noise effectively or require scene-specific fine-tuning
of hyperparameters, making them difficult to apply in practice. This paper
re-examines the problem of anti-noise reconstruction from the perspective of
epistemic uncertainty, proposing a novel framework, OCSplats. By combining key
technologies such as hybrid noise assessment and observation-based cognitive
correction, the accuracy of noise classification in areas with cognitive
differences has been significantly improved. Moreover, to address the issue of
varying noise proportions in different scenarios, we have designed a label
noise classification pipeline based on dynamic anchor points. This pipeline
enables OCSplats to be applied simultaneously to scenarios with vastly
different noise proportions without adjusting parameters. Extensive experiments
demonstrate that OCSplats always achieve leading reconstruction performance and
precise label noise classification in scenes of different complexity levels.



---

## Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Quankai Gao, Iliyan Georgiev, Tuanfeng Y. Wang, Krishna Kumar Singh, Ulrich Neumann, Jae Shin Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2508.01464v1){: .btn .btn-green } |

**Abstract**: 3D generation has made significant progress, however, it still largely
remains at the object-level. Feedforward 3D scene-level generation has been
rarely explored due to the lack of models capable of scaling-up latent
representation learning on 3D scene-level data. Unlike object-level generative
models, which are trained on well-labeled 3D data in a bounded canonical space,
scene-level generations with 3D scenes represented by 3D Gaussian Splatting
(3DGS) are unbounded and exhibit scale inconsistency across different scenes,
making unified latent representation learning for generative purposes extremely
challenging. In this paper, we introduce Can3Tok, the first 3D scene-level
variational autoencoder (VAE) capable of encoding a large number of Gaussian
primitives into a low-dimensional latent embedding, which effectively captures
both semantic and spatial information of the inputs. Beyond model design, we
propose a general pipeline for 3D scene data processing to address scale
inconsistency issue. We validate our method on the recent scene-level 3D
dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to
novel 3D scenes, while compared methods fail to converge on even a few hundred
scene inputs during training and exhibit zero generalization ability during
inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as
our applications to demonstrate its ability to facilitate downstream generation
tasks.



---

## PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Wentao Sun, Hanqing Xu, Quanyun Wu, Dedong Zhang, Yiping Chen, Lingfei Ma, John S. Zelek, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.00259v1){: .btn .btn-green } |

**Abstract**: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

Comments:
- 22 pages, 9 figures

---

## Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a  Bimanual Robot with Handover and Gaussian Splat Merging


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Tianshuang Qiu, Zehan Ma, Karim El-Refai, Hiya Shah, Chung Min Kim, Justin Kerr, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2508.00354v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/



---

## Cooperative Perception: A Resource-Efficient Framework for Multi-Drone  3D Scene Reconstruction Using Federated Diffusion and NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Massoud Pourmandi | cs.AI | [PDF](http://arxiv.org/pdf/2508.00967v1){: .btn .btn-green } |

**Abstract**: The proposal introduces an innovative drone swarm perception system that aims
to solve problems related to computational limitations and low-bandwidth
communication, and real-time scene reconstruction. The framework enables
efficient multi-agent 3D/4D scene synthesis through federated learning of
shared diffusion model and YOLOv12 lightweight semantic extraction and local
NeRF updates while maintaining privacy and scalability. The framework redesigns
generative diffusion models for joint scene reconstruction, and improves
cooperative scene understanding, while adding semantic-aware compression
protocols. The approach can be validated through simulations and potential
real-world deployment on drone testbeds, positioning it as a disruptive
advancement in multi-agent AI for autonomous systems.

Comments:
- 15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS
  2024 template
