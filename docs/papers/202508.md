---
layout: default
title: August 2025
parent: Papers
nav_order: 202508
---

<!---metadata--->


## PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal  Spans via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yijun Xu, Jingrui Zhang, Yuhan Chen, Dingwen Wang, Lei Yu, Chu He | cs.CV | [PDF](http://arxiv.org/pdf/2508.02660v1){: .btn .btn-green } |

**Abstract**: Modeling complex rigid motion across large spatiotemporal spans remains an
unresolved challenge in dynamic reconstruction. Existing paradigms are mainly
confined to short-term, small-scale deformation and offer limited consideration
for physical consistency. This study proposes PMGS, focusing on reconstructing
Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:
1) Target Modeling: achieving object-centralized reconstruction through dynamic
scene decomposition and an improved point density control; 2) Motion Recovery:
restoring full motion sequences by learning per-frame SE(3) poses. We introduce
an acceleration consistency constraint to bridge Newtonian mechanics and pose
estimation, and design a dynamic simulated annealing strategy that adaptively
schedules learning rates based on motion states. Futhermore, we devise a Kalman
fusion scheme to optimize error accumulation from multi-source observations to
mitigate disturbances. Experiments show PMGS's superior performance in
reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic
methods.



---

## Uncertainty Estimation for Novel Views in Gaussian Splatting from  Primitive-Based Representations of Error and Visibility

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Thomas Gottwald, Edgar Heinert, Matthias Rottmann | cs.GR | [PDF](http://arxiv.org/pdf/2508.02443v1){: .btn .btn-green } |

**Abstract**: In this work, we present a novel method for uncertainty estimation (UE) in
Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical
applications such as robotics and medicine. Previous methods typically estimate
the variance of Gaussian primitives and use the rendering process to obtain
pixel-wise uncertainties. Our method establishes primitive representations of
error and visibility of trainings views, which carries meaningful uncertainty
information. This representation is obtained by projection of training error
and visibility onto the primitives. Uncertainties of novel views are obtained
by rendering the primitive representations of uncertainty for those novel
views, yielding uncertainty feature maps. To aggregate these uncertainty
feature maps of novel views, we perform a pixel-wise regression on holdout
data. In our experiments, we analyze the different components of our method,
investigating various combinations of uncertainty feature maps and regression
models. Furthermore, we considered the effect of separating splatting into
foreground and background. Our UEs show high correlations to true errors,
outperforming state-of-the-art methods, especially on foreground objects. The
trained regression models show generalization capabilities to new scenes,
allowing uncertainty estimation without the need for holdout data.



---

## GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yikuang Yuluo, Yue Ma, Kuan Shen, Tongtong Jin, Wang Liao, Yangpu Ma, Fuquan Wang | eess.IV | [PDF](http://arxiv.org/pdf/2508.02408v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT
reconstruction. However, existing methods rely on the average gradient
magnitude of points within the view, often leading to severe needle-like
artifacts under sparse-view conditions. To address this challenge, we propose
GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses
needle-like artifacts and improves reconstruction accuracy under sparse-view
conditions. Our framework introduces two key innovations: (1) a Denoised Point
Cloud Initialization Strategy that reduces initialization errors and
accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that
refines gradient computation using graph-based density differences, improving
splitting accuracy and density representation. Experiments on X-3D and
real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR
improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These
results highlight the applicability of GR-Gaussian for accurate CT
reconstruction under challenging sparse-view conditions.

Comments:
- 10

---

## ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant  Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Fangxin Liu, Haomin Li, Bowen Zhu, Zongwu Wang, Zhuoran Song, Habing Guan, Li Jiang | cs.AR | [PDF](http://arxiv.org/pdf/2508.02304v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) offer significant promise for generating
photorealistic images and videos. However, existing mainstream neural rendering
models often fall short in meeting the demands for immediacy and power
efficiency in practical applications. Specifically, these models frequently
exhibit irregular access patterns and substantial computational overhead,
leading to undesirable inference latency and high power consumption.
Computing-in-memory (CIM), an emerging computational paradigm, has the
potential to address these access bottlenecks and reduce the power consumption
associated with model execution.
  To bridge the gap between model performance and real-world scene
requirements, we propose an algorithm-architecture co-design approach,
abbreviated as ASDR, a CIM-based accelerator supporting efficient neural
rendering. At the algorithmic level, we propose two rendering optimization
schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of
different pixels, thus reducing access memory and computational overhead. (2)
Reducing MLP overhead by decoupling and approximating the volume rendering of
color and density. At the architecture level, we design an efficient
ReRAM-based CIM architecture with efficient data mapping and reuse
microarchitecture. Experiments demonstrate that our design can achieve up to
$9.55\times$ and $69.75\times$ speedup over state-of-the-art NeRF accelerators
and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.

Comments:
- Accepted by the 2025 International Conference on Architectural
  Support for Programming Languages and Operating Systems (ASPLOS 2025). The
  paper will be presented at ASPLOS 2026

---

## SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene  Completion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Rui Qian, Haozhi Cao, Tianchen Deng, Shenghai Yuan, Lihua Xie | cs.CV | [PDF](http://arxiv.org/pdf/2508.02261v1){: .btn .btn-green } |

**Abstract**: Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising
task that aims to infer dense geometric and semantic descriptions of a scene
from a single image. While recent object-centric paradigms significantly
improve efficiency by leveraging flexible 3D Gaussian primitives, they still
rely heavily on a large number of randomly initialized primitives, which
inevitably leads to 1) inefficient primitive initialization and 2) outlier
primitives that introduce erroneous artifacts. In this paper, we propose
SplatSSC, a novel framework that resolves these limitations with a depth-guided
initialization strategy and a principled Gaussian aggregator. Instead of random
initialization, SplatSSC utilizes a dedicated depth branch composed of a
Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image
and depth features to generate a sparse yet representative set of initial
Gaussian primitives. To mitigate noise from outlier primitives, we develop the
Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing
geometric and semantic predictions during the Gaussian-to-voxel splatting
process. Complemented with a specialized Probability Scale Loss, our method
achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming
prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both
latency and memory consumption by more than 9.3%. The code will be released
upon acceptance.



---

## GaussianCross: Cross-modal Self-supervised 3D Representation Learning  via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau | cs.CV | [PDF](http://arxiv.org/pdf/2508.02172v1){: .btn .btn-green } |

**Abstract**: The significance of informative and robust point representations has been
widely acknowledged for 3D scene understanding. Despite existing
self-supervised pre-training counterparts demonstrating promising performance,
the model collapse and structural information deficiency remain prevalent due
to insufficient point discrimination difficulty, yielding unreliable
expressions and suboptimal performance. In this paper, we present
GaussianCross, a novel cross-modal self-supervised 3D representation learning
architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques
to address current challenges. GaussianCross seamlessly converts
scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian
representation without missing details, enabling stable and generalizable
pre-training. Subsequently, a tri-attribute adaptive distillation splatting
module is incorporated to construct a 3D feature field, facilitating synergetic
feature capturing of appearance, geometry, and semantic cues to maintain
cross-modal consistency. To validate GaussianCross, we perform extensive
evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In
particular, GaussianCross shows a prominent parameter and data efficiency,
achieving superior performance through linear probing (<0.1% parameters) and
limited data training (1% of scenes) compared to state-of-the-art methods.
Furthermore, GaussianCross demonstrates strong generalization capabilities,
improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on
ScanNet200 semantic and instance segmentation tasks, respectively, supporting
the effectiveness of our approach. The code, weights, and visualizations are
publicly available at
\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.

Comments:
- 14 pages, 8 figures, accepted by MM'25

---

## ScrewSplat: An End-to-End Method for Articulated Object Recognition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Seungyeon Kim, Junsu Ha, Young Hun Kim, Yonghyeon Lee, Frank C. Park | cs.RO | [PDF](http://arxiv.org/pdf/2508.02146v1){: .btn .btn-green } |

**Abstract**: Articulated object recognition -- the task of identifying both the geometry
and kinematic joints of objects with movable parts -- is essential for enabling
robots to interact with everyday objects such as doors and laptops. However,
existing approaches often rely on strong assumptions, such as a known number of
articulated parts; require additional inputs, such as depth images; or involve
complex intermediate steps that can introduce potential errors -- limiting
their practicality in real-world settings. In this paper, we introduce
ScrewSplat, a simple end-to-end method that operates solely on RGB
observations. Our approach begins by randomly initializing screw axes, which
are then iteratively optimized to recover the object's underlying kinematic
structure. By integrating with Gaussian Splatting, we simultaneously
reconstruct the 3D geometry and segment the object into rigid, movable parts.
We demonstrate that our method achieves state-of-the-art recognition accuracy
across a diverse set of articulated objects, and further enables zero-shot,
text-guided manipulation using the recovered kinematic model.

Comments:
- 26 pages, 12 figures, Conference on Robot Learning (CoRL) 2025

---

## VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic  Urban Scenes Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yuru Xiao, Zihan Lin, Chao Lu, Deming Zhai, Kui Jiang, Wenbo Zhao, Wei Zhang, Junjun Jiang, Huanran Wang, Xianming Liu | cs.CV | [PDF](http://arxiv.org/pdf/2508.02129v1){: .btn .btn-green } |

**Abstract**: Dynamic urban scene modeling is a rapidly evolving area with broad
applications. While current approaches leveraging neural radiance fields or
Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity
novel view synthesis, they still face significant limitations. These often stem
from a dependence on pre-calibrated object tracks or difficulties in accurately
modeling fast-moving objects from undersampled capture, particularly due to
challenges in handling temporal discontinuities. To overcome these issues, we
propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our
key insight is to distill robust, temporally consistent priors from a test-time
adapted video diffusion model. To ensure precise pose alignment and effective
integration of this denoised content, we introduce two core innovations: a
joint timestamp optimization strategy that refines interpolated frame poses,
and an uncertainty distillation method that adaptively extracts target content
while preserving well-reconstructed regions. Extensive experiments demonstrate
that our method significantly enhances dynamic modeling, especially for
fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view
synthesis over baseline approaches.



---

## From Photons to Physics: Autonomous Indoor Drones and the Future of  Objective Property Assessment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Petteri Teikari, Mike Jarrell, Irene Bandera Moreno, Harri Pesola | cs.RO | [PDF](http://arxiv.org/pdf/2508.01965v1){: .btn .btn-green } |

**Abstract**: The convergence of autonomous indoor drones with physics-aware sensing
technologies promises to transform property assessment from subjective visual
inspection to objective, quantitative measurement. This comprehensive review
examines the technical foundations enabling this paradigm shift across four
critical domains: (1) platform architectures optimized for indoor navigation,
where weight constraints drive innovations in heterogeneous computing,
collision-tolerant design, and hierarchical control systems; (2) advanced
sensing modalities that extend perception beyond human vision, including
hyperspectral imaging for material identification, polarimetric sensing for
surface characterization, and computational imaging with metaphotonics enabling
radical miniaturization; (3) intelligent autonomy through active reconstruction
algorithms, where drones equipped with 3D Gaussian Splatting make strategic
decisions about viewpoint selection to maximize information gain within battery
constraints; and (4) integration pathways with existing property workflows,
including Building Information Modeling (BIM) systems and industry standards
like Uniform Appraisal Dataset (UAD) 3.6.

Comments:
- 63 pages, 5 figures

---

## Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Jianchao Wang, Peng Zhou, Cen Li, Rong Quan, Jie Qin | cs.CV | [PDF](http://arxiv.org/pdf/2508.02493v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a powerful and computationally efficient
representation for 3D reconstruction. Despite its strengths, 3DGS often
produces floating artifacts, which are erroneous structures detached from the
actual geometry and significantly degrade visual fidelity. The underlying
mechanisms causing these artifacts, particularly in low-quality initialization
scenarios, have not been fully explored. In this paper, we investigate the
origins of floating artifacts from a frequency-domain perspective and identify
under-optimized Gaussians as the primary source. Based on our analysis, we
propose \textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),
which selectively expands under-optimized Gaussians to prioritize accurate
low-frequency learning. Additionally, we introduce complementary depth-based
and scale-based strategies to dynamically refine Gaussian expansion,
effectively mitigating detail erosion. Extensive experiments on both synthetic
and real-world datasets demonstrate that EFA-GS substantially reduces floating
artifacts while preserving high-frequency details, achieving an improvement of
1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we
validate the effectiveness of our approach in downstream 3D editing tasks. Our
implementation will be released on GitHub.



---

## LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Luqi Cheng, Zhangshuo Qi, Zijie Zhou, Chao Lu, Guangming Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2508.01704v1){: .btn .btn-green } |

**Abstract**: Maps play an important role in autonomous driving systems. The recently
proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit
scene reconstruction results, demonstrating the potential for map construction
in autonomous driving scenarios. However, because of the time and computational
costs involved in generating Gaussian scenes, how to update the map becomes a
significant challenge. In this paper, we propose LT-Gaussian, a map update
method for 3D-GS-based maps. LT-Gaussian consists of three main components:
Multimodal Gaussian Splatting, Structural Change Detection Module, and
Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is
generated using our proposed Multimodal Gaussian Splatting. Subsequently,
during the map update process, we compare the outdated Gaussian map with the
current LiDAR data stream to identify structural changes. Finally, we perform
targeted updates to the Gaussian-map to generate an up-to-date map. We
establish a benchmark for map updating on the nuScenes dataset to
quantitatively evaluate our method. The experimental results show that
LT-Gaussian can effectively and efficiently update the Gaussian-map, handling
common environmental changes in autonomous driving scenarios. Furthermore, by
taking full advantage of information from both new and old scenes, LT-Gaussian
is able to produce higher quality reconstruction results compared to map update
strategies that reconstruct maps from scratch. Our open-source code is
available at https://github.com/ChengLuqi/LT-gaussian.

Comments:
- Accepted by IV 2025

---

## DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Yufeng Chi, Huimin Ma, Kafeng Wang, Jianmin Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.01684v1){: .btn .btn-green } |

**Abstract**: While diffusion models have demonstrated remarkable progress in 2D image
generation and editing, extending these capabilities to 3D editing remains
challenging, particularly in maintaining multi-view consistency. Classical
approaches typically update 3D representations through iterative refinement
based on a single editing view. However, these methods often suffer from slow
convergence and blurry artifacts caused by cross-view inconsistencies. Recent
methods improve efficiency by propagating 2D editing attention features, yet
still exhibit fine-grained inconsistencies and failure modes in complex scenes
due to insufficient constraints. To address this, we propose \textbf{DisCo3D},
a novel framework that distills 3D consistency priors into a 2D editor. Our
method first fine-tunes a 3D generator using multi-view inputs for scene
adaptation, then trains a 2D editor through consistency distillation. The
edited multi-view outputs are finally optimized into 3D representations via
Gaussian Splatting. Experimental results show DisCo3D achieves stable
multi-view consistency and outperforms state-of-the-art methods in editing
quality.

Comments:
- 17 pages, 7 figures

---

## AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for  Instance-Level 3D Scene Understanding and Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Zhaonan Wang, Manyi Li, Changhe Tu | cs.CV | [PDF](http://arxiv.org/pdf/2508.01740v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has witnessed exponential adoption across
diverse applications, driving a critical need for semantic-aware 3D Gaussian
representations to enable scene understanding and editing tasks. Existing
approaches typically attach semantic features to a collection of free Gaussians
and distill the features via differentiable rendering, leading to noisy
segmentation and a messy selection of Gaussians. In this paper, we introduce
AG$^2$aussian, a novel framework that leverages an anchor-graph structure to
organize semantic features and regulate Gaussian primitives. Our anchor-graph
structure not only promotes compact and instance-aware Gaussian distributions,
but also facilitates graph-based propagation, achieving a clean and accurate
instance-level Gaussian selection. Extensive validation across four
applications, i.e. interactive click-based query, open-vocabulary text-driven
query, object removal editing, and physics simulation, demonstrates the
advantages of our approach and its benefits to various applications. The
experiments and ablation studies further evaluate the effectiveness of the key
designs of our approach.



---

## OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian  Splatting for Refined Object-Level Understanding


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Dianyi Yang, Xihan Wang, Yu Gao, Shiyang Liu, Bohan Ren, Yufeng Yue, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2508.01150v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D scene understanding have made significant strides
in enabling interaction with scenes using open-vocabulary queries, particularly
for VR/AR and robotic applications. Nevertheless, existing methods are hindered
by rigid offline pipelines and the inability to provide precise 3D object-level
understanding given open-ended queries. In this paper, we present
OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that
improves semantic modeling and refines object-level understanding.
OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed
Distance Field to facilitate lossless fusion of semantic features on-the-fly.
Furthermore, we introduce a novel multimodal language-guided approach named
MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D
objects by adaptively adjusting similarity thresholds, achieving an improvement
17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments
demonstrate that our method outperforms existing methods in 3D object
understanding and scene reconstruction quality, as well as showcasing its
effectiveness in language-guided scene interaction. The code is available at
https://young-bit.github.io/opengs-fusion.github.io/ .

Comments:
- IROS2025

---

## No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from  Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Ranran Huang, Krystian Mikolajczyk | cs.CV | [PDF](http://arxiv.org/pdf/2508.01171v1){: .btn .btn-green } |

**Abstract**: We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from
sparse multi-view images, requiring no ground-truth poses during training or
inference. It employs a shared feature extraction backbone, enabling
simultaneous prediction of 3D Gaussian primitives and camera poses in a
canonical space from unposed inputs within a single feed-forward step.
Alongside the rendering loss based on estimated novel-view poses, a
reprojection loss is integrated to enforce the learning of pixel-aligned
Gaussian primitives for enhanced geometric constraints. This pose-free training
paradigm and efficient one-step feed-forward design make SPFSplat well-suited
for practical applications. Remarkably, despite the absence of pose
supervision, SPFSplat achieves state-of-the-art performance in novel view
synthesis even under significant viewpoint changes and limited image overlap.
It also surpasses recent methods trained with geometry priors in relative pose
estimation. Code and trained models are available on our project page:
https://ranrhuang.github.io/spfsplat/.

Comments:
- Project Page: https://ranrhuang.github.io/spfsplat/

---

## OCSplats: Observation Completeness Quantification and Label Noise  Separation in 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Han Ling, Xian Xu, Yinghui Sun, Quansen Sun | cs.CV | [PDF](http://arxiv.org/pdf/2508.01239v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become one of the most promising 3D
reconstruction technologies. However, label noise in real-world scenarios-such
as moving objects, non-Lambertian surfaces, and shadows-often leads to
reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods
either fail to separate noise effectively or require scene-specific fine-tuning
of hyperparameters, making them difficult to apply in practice. This paper
re-examines the problem of anti-noise reconstruction from the perspective of
epistemic uncertainty, proposing a novel framework, OCSplats. By combining key
technologies such as hybrid noise assessment and observation-based cognitive
correction, the accuracy of noise classification in areas with cognitive
differences has been significantly improved. Moreover, to address the issue of
varying noise proportions in different scenarios, we have designed a label
noise classification pipeline based on dynamic anchor points. This pipeline
enables OCSplats to be applied simultaneously to scenarios with vastly
different noise proportions without adjusting parameters. Extensive experiments
demonstrate that OCSplats always achieve leading reconstruction performance and
precise label noise classification in scenes of different complexity levels.



---

## Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Quankai Gao, Iliyan Georgiev, Tuanfeng Y. Wang, Krishna Kumar Singh, Ulrich Neumann, Jae Shin Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2508.01464v1){: .btn .btn-green } |

**Abstract**: 3D generation has made significant progress, however, it still largely
remains at the object-level. Feedforward 3D scene-level generation has been
rarely explored due to the lack of models capable of scaling-up latent
representation learning on 3D scene-level data. Unlike object-level generative
models, which are trained on well-labeled 3D data in a bounded canonical space,
scene-level generations with 3D scenes represented by 3D Gaussian Splatting
(3DGS) are unbounded and exhibit scale inconsistency across different scenes,
making unified latent representation learning for generative purposes extremely
challenging. In this paper, we introduce Can3Tok, the first 3D scene-level
variational autoencoder (VAE) capable of encoding a large number of Gaussian
primitives into a low-dimensional latent embedding, which effectively captures
both semantic and spatial information of the inputs. Beyond model design, we
propose a general pipeline for 3D scene data processing to address scale
inconsistency issue. We validate our method on the recent scene-level 3D
dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to
novel 3D scenes, while compared methods fail to converge on even a few hundred
scene inputs during training and exhibit zero generalization ability during
inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as
our applications to demonstrate its ability to facilitate downstream generation
tasks.



---

## Cooperative Perception: A Resource-Efficient Framework for Multi-Drone  3D Scene Reconstruction Using Federated Diffusion and NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Massoud Pourmandi | cs.AI | [PDF](http://arxiv.org/pdf/2508.00967v1){: .btn .btn-green } |

**Abstract**: The proposal introduces an innovative drone swarm perception system that aims
to solve problems related to computational limitations and low-bandwidth
communication, and real-time scene reconstruction. The framework enables
efficient multi-agent 3D/4D scene synthesis through federated learning of
shared diffusion model and YOLOv12 lightweight semantic extraction and local
NeRF updates while maintaining privacy and scalability. The framework redesigns
generative diffusion models for joint scene reconstruction, and improves
cooperative scene understanding, while adding semantic-aware compression
protocols. The approach can be validated through simulations and potential
real-world deployment on drone testbeds, positioning it as a disruptive
advancement in multi-agent AI for autonomous systems.

Comments:
- 15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS
  2024 template

---

## Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a  Bimanual Robot with Handover and Gaussian Splat Merging


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Tianshuang Qiu, Zehan Ma, Karim El-Refai, Hiya Shah, Chung Min Kim, Justin Kerr, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2508.00354v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/



---

## PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Wentao Sun, Hanqing Xu, Quanyun Wu, Dedong Zhang, Yiping Chen, Lingfei Ma, John S. Zelek, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.00259v1){: .btn .btn-green } |

**Abstract**: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

Comments:
- 22 pages, 9 figures
