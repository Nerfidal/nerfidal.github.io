---
layout: default
title: August 2025
parent: Papers
nav_order: 202508
---

<!---metadata--->


## Seam360GS: Seamless 360Â° Gaussian Splatting from Real-World  Omnidirectional Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-27 | Changha Shin, Woong Oh Cho, Seon Joo Kim | cs.CV | [PDF](http://arxiv.org/pdf/2508.20080v1){: .btn .btn-green } |

**Abstract**: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

Comments:
- Accepted to ICCV 2025. 10 pages main text, 4 figures, 4 tables,
  supplementary material included

---

## MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for  High-Fidelity Dynamic Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-27 | Han Jiao, Jiakai Sun, Yexing Xu, Lei Zhao, Wei Xing, Huaizhong Lin | cs.CV | [PDF](http://arxiv.org/pdf/2508.19786v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

Comments:
- 8 pages, 9 figures, Anonymous AAAI Submission

---

## FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction  with Large Gaussian Reconstruction Transformers

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-27 | Yue Wu, Yufan Wu, Wen Li, Yuxi Lu, Kairui Feng, Xuanhong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2508.19754v1){: .btn .btn-green } |

**Abstract**: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.



---

## LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-27 | Yupeng Zhang, Dezhi Zheng, Ping Lu, Han Zhang, Lei Wang, Liping xiang, Cheng Luo, Kaijun Deng, Xiaowen Fu, Linlin Shen, Jinbao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2508.19699v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

Comments:
- PRCV 2025

---

## Style4D-Bench: A Benchmark Suite for 4D Stylization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-26 | Beiqi Chen, Shuai Shao, Haitang Feng, Jianhuang Lai, Jianlou Si, Guangcong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2508.19243v1){: .btn .btn-green } |

**Abstract**: We introduce Style4D-Bench, the first benchmark suite specifically designed
for 4D stylization, with the goal of standardizing evaluation and facilitating
progress in this emerging area. Style4D-Bench comprises: 1) a comprehensive
evaluation protocol measuring spatial fidelity, temporal coherence, and
multi-view consistency through both perceptual and quantitative metrics, 2) a
strong baseline that make an initial attempt for 4D stylization, and 3) a
curated collection of high-resolution dynamic 4D scenes with diverse motions
and complex backgrounds. To establish a strong baseline, we present Style4D, a
novel framework built upon 4D Gaussian Splatting. It consists of three key
components: a basic 4DGS scene representation to capture reliable geometry, a
Style Gaussian Representation that leverages lightweight per-Gaussian MLPs for
temporally and spatially aware appearance control, and a Holistic
Geometry-Preserved Style Transfer module designed to enhance spatio-temporal
consistency via contrastive coherence learning and structural content
preservation. Extensive experiments on Style4D-Bench demonstrate that Style4D
achieves state-of-the-art performance in 4D stylization, producing fine-grained
stylistic details with stable temporal dynamics and consistent multi-view
rendering. We expect Style4D-Bench to become a valuable resource for
benchmarking and advancing research in stylized rendering of dynamic 3D scenes.
Project page: https://becky-catherine.github.io/Style4D . Code:
https://github.com/Becky-catherine/Style4D-Bench .

Comments:
- Project page: https://becky-catherine.github.io/Style4D . Code:
  https://github.com/Becky-catherine/Style4D-Bench

---

## ColorGS: High-fidelity Surgical Scene Reconstruction with Colored  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-26 | Qun Ji, Peng Li, Mingqiang Wei | cs.CV | [PDF](http://arxiv.org/pdf/2508.18696v1){: .btn .btn-green } |

**Abstract**: High-fidelity reconstruction of deformable tissues from endoscopic videos
remains challenging due to the limitations of existing methods in capturing
subtle color variations and modeling global deformations. While 3D Gaussian
Splatting (3DGS) enables efficient dynamic reconstruction, its fixed
per-Gaussian color assignment struggles with intricate textures, and linear
deformation modeling fails to model consistent global deformation. To address
these issues, we propose ColorGS, a novel framework that integrates spatially
adaptive color encoding and enhanced deformation modeling for surgical scene
reconstruction. First, we introduce Colored Gaussian Primitives, which employ
dynamic anchors with learnable color parameters to adaptively encode spatially
varying textures, significantly improving color expressiveness under complex
lighting and tissue similarity. Second, we design an Enhanced Deformation Model
(EDM) that combines time-aware Gaussian basis functions with learnable
time-independent deformations, enabling precise capture of both localized
tissue deformations and global motion consistency caused by surgical
interactions. Extensive experiments on DaVinci robotic surgery videos and
benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves
state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior
3DGS-based methods) and superior SSIM (97.25\%) while maintaining real-time
rendering efficiency. Our work advances surgical scene reconstruction by
balancing high fidelity with computational practicality, critical for
intraoperative guidance and AR/VR applications.



---

## Can we make NeRF-based visual localization privacy-preserving?

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-26 | Maxime Pietrantoni, Martin Humenberger, Torsten Sattler, Gabriela Csurka | cs.CV | [PDF](http://arxiv.org/pdf/2508.18971v1){: .btn .btn-green } |

**Abstract**: Visual localization (VL) is the task of estimating the camera pose in a known
scene. VL methods, a.o., can be distinguished based on how they represent the
scene, e.g., explicitly through a (sparse) point cloud or a collection of
images or implicitly through the weights of a neural network. Recently,
NeRF-based methods have become popular for VL. While NeRFs offer high-quality
novel view synthesis, they inadvertently encode fine scene details, raising
privacy concerns when deployed in cloud-based localization services as
sensitive information could be recovered. In this paper, we tackle this
challenge on two ends. We first propose a new protocol to assess
privacy-preservation of NeRF-based representations. We show that NeRFs trained
with photometric losses store fine-grained details in their geometry
representations, making them vulnerable to privacy attacks, even if the head
that predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving
Neural Segmentation Field), a NeRF variant trained with segmentation
supervision instead of RGB images. These segmentation labels are learned in a
self-supervised manner, ensuring they are coarse enough to obscure identifiable
scene details while remaining discriminativeness in 3D. The segmentation space
of ppNeSF can be used for accurate visual localization, yielding
state-of-the-art results.



---

## PseudoMapTrainer: Learning Online Mapping without HD Maps

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-26 | Christian LÃ¶wens, Thorben Funke, Jingchao Xie, Alexandru Paul Condurache | cs.CV | [PDF](http://arxiv.org/pdf/2508.18788v1){: .btn .btn-green } |

**Abstract**: Online mapping models show remarkable results in predicting vectorized maps
from multi-view camera images only. However, all existing approaches still rely
on ground-truth high-definition maps during training, which are expensive to
obtain and often not geographically diverse enough for reliable generalization.
In this work, we propose PseudoMapTrainer, a novel approach to online mapping
that uses pseudo-labels generated from unlabeled sensor data. We derive those
pseudo-labels by reconstructing the road surface from multi-camera imagery
using Gaussian splatting and semantics of a pre-trained 2D segmentation
network. In addition, we introduce a mask-aware assignment algorithm and loss
function to handle partially masked pseudo-labels, allowing for the first time
the training of online mapping models without any ground-truth maps.
Furthermore, our pseudo-labels can be effectively used to pre-train an online
model in a semi-supervised manner to leverage large-scale unlabeled
crowdsourced data. The code is available at
github.com/boschresearch/PseudoMapTrainer.

Comments:
- Accepted at ICCV 2025

---

## Camera Pose Refinement via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-25 | Lulu Hao, Lipu Zhou, Zhenzhong Wei, Xu Wang | cs.CV | [PDF](http://arxiv.org/pdf/2508.17876v1){: .btn .btn-green } |

**Abstract**: Camera pose refinement aims at improving the accuracy of initial pose
estimation for applications in 3D computer vision. Most refinement approaches
rely on 2D-3D correspondences with specific descriptors or dedicated networks,
requiring reconstructing the scene again for a different descriptor or fully
retraining the network for each scene. Some recent methods instead infer pose
from feature similarity, but their lack of geometry constraints results in less
accuracy. To overcome these limitations, we propose a novel camera pose
refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as
GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing
3DGS model to render novel views, providing a lightweight solution that can be
directly applied to diverse scenes without additional training or fine-tuning.
Specifically, we introduce an iterative optimization approach, which refines
the camera pose using epipolar geometric constraints among the query and
multiple rendered images. Our method allows flexibly choosing feature
extractors and matchers to establish these constraints. Extensive empirical
evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate
that our method outperforms state-of-the-art camera pose refinement approaches,
achieving 53.3% and 56.9% reductions in median translation and rotation errors
on 7-Scenes, and 40.7% and 53.2% on Cambridge.



---

## Generating Human-AI Collaborative Design Sequence for 3D Assets via  Differentiable Operation Graph

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-25 | Xiaoyang Huang, Bingbing Ni, Wenjun Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2508.17645v1){: .btn .btn-green } |

**Abstract**: The emergence of 3D artificial intelligence-generated content (3D-AIGC) has
enabled rapid synthesis of intricate geometries. However, a fundamental
disconnect persists between AI-generated content and human-centric design
paradigms, rooted in representational incompatibilities: conventional AI
frameworks predominantly manipulate meshes or neural representations
(\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate within
parametric modeling tools. This disconnection diminishes the practical value of
AI for 3D industry, undermining the efficiency of human-AI collaboration. To
resolve this disparity, we focus on generating design operation sequences,
which are structured modeling histories that comprehensively capture the
step-by-step construction process of 3D assets and align with designers'
typical workflows in modern 3D software. We first reformulate fundamental
modeling operations (\emph{e.g.}, \emph{Extrude}, \emph{Boolean}) into
differentiable units, enabling joint optimization of continuous (\emph{e.g.},
\emph{Extrude} height) and discrete (\emph{e.g.}, \emph{Boolean} type)
parameters via gradient-based learning. Based on these differentiable
operations, a hierarchical graph with gating mechanism is constructed and
optimized end-to-end by minimizing Chamfer Distance to target geometries.
Multi-stage sequence length constraint and domain rule penalties enable
unsupervised learning of compact design sequences without ground-truth sequence
supervision. Extensive validation demonstrates that the generated operation
sequences achieve high geometric fidelity, smooth mesh wiring, rational step
composition and flexible editing capacity, with full compatibility within
design industry.



---

## GWM: Towards Scalable Gaussian World Models for Robotic Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-25 | Guanxing Lu, Baoxiong Jia, Puhao Li, Yixin Chen, Ziwei Wang, Yansong Tang, Siyuan Huang | cs.RO | [PDF](http://arxiv.org/pdf/2508.17600v1){: .btn .btn-green } |

**Abstract**: Training robot policies within a learned world model is trending due to the
inefficiency of real-world interactions. The established image-based world
models and policies have shown prior success, but lack robust geometric
information that requires consistent spatial and physical understanding of the
three-dimensional world, even pre-trained on internet-scale video sources. To
this end, we propose a novel branch of world model named Gaussian World Model
(GWM) for robotic manipulation, which reconstructs the future state by
inferring the propagation of Gaussian primitives under the effect of robot
actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D
variational autoencoder, enabling fine-grained scene-level future state
reconstruction with Gaussian Splatting. GWM can not only enhance the visual
representation for imitation learning agent by self-supervised future
prediction training, but can serve as a neural simulator that supports
model-based reinforcement learning. Both simulated and real-world experiments
depict that GWM can precisely predict future scenes conditioned on diverse
robot actions, and can be further utilized to train policies that outperform
the state-of-the-art by impressive margins, showcasing the initial data scaling
potential of 3D world model.

Comments:
- Published at ICCV 2025. Project page:
  https://gaussian-world-model.github.io/

---

## MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-25 | Hanzhi Chang, Ruijie Zhu, Wenjie Chang, Mulin Yu, Yanzhe Liang, Jiahao Lu, Zhuoyuan Li, Tianzhu Zhang | cs.GR | [PDF](http://arxiv.org/pdf/2508.17811v1){: .btn .btn-green } |

**Abstract**: Surface reconstruction has been widely studied in computer vision and
graphics. However, existing surface reconstruction works struggle to recover
accurate scene geometry when the input views are extremely sparse. To address
this issue, we propose MeshSplat, a generalizable sparse-view surface
reconstruction framework via Gaussian Splatting. Our key idea is to leverage
2DGS as a bridge, which connects novel view synthesis to learned geometric
priors and then transfers these priors to achieve surface reconstruction.
Specifically, we incorporate a feed-forward network to predict per-view
pixel-aligned 2DGS, which enables the network to synthesize novel view images
and thus eliminates the need for direct 3D ground-truth supervision. To improve
the accuracy of 2DGS position and orientation prediction, we propose a Weighted
Chamfer Distance Loss to regularize the depth maps, especially in overlapping
areas of input views, and also a normal prediction network to align the
orientation of 2DGS with normal vectors predicted by a monocular normal
estimator. Extensive experiments validate the effectiveness of our proposed
improvement, demonstrating that our method achieves state-of-the-art
performance in generalizable sparse-view mesh reconstruction tasks. Project
Page: https://hanzhichang.github.io/meshsplat_web

Comments:
- 17 pages, 15 figures, 5 tables

---

## Real-time 3D Visualization of Radiance Fields on Light Field Displays

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-25 | Jonghyun Kim, Cheng Sun, Michael Stengel, Matthew Chan, Andrew Russell, Jaehyun Jung, Wil Braithwaite, Shalini De Mello, David Luebke | cs.GR | [PDF](http://arxiv.org/pdf/2508.18540v1){: .btn .btn-green } |

**Abstract**: Radiance fields have revolutionized photo-realistic 3D scene visualization by
enabling high-fidelity reconstruction of complex environments, making them an
ideal match for light field displays. However, integrating these technologies
presents significant computational challenges, as light field displays require
multiple high-resolution renderings from slightly shifted viewpoints, while
radiance fields rely on computationally intensive volume rendering. In this
paper, we propose a unified and efficient framework for real-time radiance
field rendering on light field displays. Our method supports a wide range of
radiance field representations, including NeRFs, 3D Gaussian Splatting, and
Sparse Voxels, within a shared architecture based on a single-pass plane
sweeping strategy and caching of shared, non-directional components. The
framework generalizes across different scene formats without retraining, and
avoids redundant computation across views. We further demonstrate a real-time
interactive application on a Looking Glass display, achieving 200+ FPS at 512p
across 45 views, enabling seamless, immersive 3D interaction. On standard
benchmarks, our method achieves up to 22x speedup compared to independently
rendering each view, while preserving image quality.

Comments:
- 10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed
  equally

---

## IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with  New Imagery Data

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-25 | Meida Chen, Luis Leal, Yue Hu, Rong Liu, Butian Xiong, Andrew Feng, Jiuyi Xu, Yangming Shi | cs.CV | [PDF](http://arxiv.org/pdf/2508.17579v1){: .btn .btn-green } |

**Abstract**: For simulation and training purposes, military organizations have made
substantial investments in developing high-resolution 3D virtual environments
through extensive imaging and 3D scanning. However, the dynamic nature of
battlefield conditions-where objects may appear or vanish over time-makes
frequent full-scale updates both time-consuming and costly. In response, we
introduce the Incremental Dynamic Update (IDU) pipeline, which efficiently
updates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with
only a small set of newly acquired images. Our approach starts with camera pose
estimation to align new images with the existing 3D model, followed by change
detection to pinpoint modifications in the scene. A 3D generative AI model is
then used to create high-quality 3D assets of the new elements, which are
seamlessly integrated into the existing 3D model. The IDU pipeline incorporates
human guidance to ensure high accuracy in object identification and placement,
with each update focusing on a single new object at a time. Experimental
results confirm that our proposed IDU pipeline significantly reduces update
time and labor, offering a cost-effective and targeted solution for maintaining
up-to-date 3D models in rapidly evolving military scenarios.



---

## GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene  Representations

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-25 | Fadi Khatib, Dror Moran, Guy Trostianetsky, Yoni Kasten, Meirav Galun, Ronen Basri | cs.CV | [PDF](http://arxiv.org/pdf/2508.18242v1){: .btn .btn-green } |

**Abstract**: We introduce GSVisLoc, a visual localization method designed for 3D Gaussian
Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a
query image, our goal is to estimate the camera's position and orientation. We
accomplish this by robustly matching scene features to image features. Scene
features are produced by downsampling and encoding the 3D Gaussians while image
features are obtained by encoding image patches. Our algorithm proceeds in
three steps, starting with coarse matching, then fine matching, and finally by
applying pose refinement for an accurate final estimate. Importantly, our
method leverages the explicit 3DGS scene representation for visual localization
without requiring modifications, retraining, or additional reference images. We
evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive
localization performance on standard benchmarks while outperforming existing
3DGS-based baselines. Moreover, our approach generalizes effectively to novel
scenes without additional training.

Comments:
- Accepted to ICCV 2025 Workshops (CALIPOSE). Project page:
  https://gsvisloc.github.io/

---

## FastAvatar: Instant 3D Gaussian Splatting for Faces from Single  Unconstrained Poses

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-25 | Hao Liang, Zhixuan Ge, Ashish Tiwari, Soumendu Majee, G. M. Dilshan Godaliyadda, Ashok Veeraraghavan, Guha Balakrishnan | cs.CV | [PDF](http://arxiv.org/pdf/2508.18389v1){: .btn .btn-green } |

**Abstract**: We present FastAvatar, a pose-invariant, feed-forward framework that can
generate a 3D Gaussian Splatting (3DGS) model from a single face image from an
arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel
encoder-decoder neural network design to achieve both fast fitting and identity
preservation regardless of input pose. First, FastAvatar constructs a 3DGS face
``template'' model from a training dataset of faces with multi-view captures.
Second, FastAvatar encodes the input face image into an identity-specific and
pose-invariant latent embedding, and decodes this embedding to predict
residuals to the structural and appearance parameters of each Gaussian in the
template 3DGS model. By only inferring residuals in a feed-forward fashion,
model inference is fast and robust. FastAvatar significantly outperforms
existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction
quality, and runs 1000x faster than per-face optimization methods (e.g.,
FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent
space design supports real-time identity interpolation and attribute editing
which is not possible with any existing feed-forward 3DGS face generation
framework. FastAvatar's combination of excellent reconstruction quality and
speed expands the scope of 3DGS for photorealistic avatar applications in
consumer and interactive systems.

Comments:
- 11 pages, 5 figures

---

## Fiducial Marker Splatting for High-Fidelity Robotics Simulations

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-23 | Diram Tabaa, Gianni Di Caro | cs.CV | [PDF](http://arxiv.org/pdf/2508.17012v1){: .btn .btn-green } |

**Abstract**: High-fidelity 3D simulation is critical for training mobile robots, but its
traditional reliance on mesh-based representations often struggle in complex
environments, such as densely packed greenhouses featuring occlusions and
repetitive structures. Recent neural rendering methods, like Gaussian Splatting
(GS), achieve remarkable visual realism but lack flexibility to incorporate
fiducial markers, which are essential for robotic localization and control. We
propose a hybrid framework that combines the photorealism of GS with structured
marker representations. Our core contribution is a novel algorithm for
efficiently generating GS-based fiducial markers (e.g., AprilTags) within
cluttered scenes. Experiments show that our approach outperforms traditional
image-fitting techniques in both efficiency and pose-estimation accuracy. We
further demonstrate the framework's potential in a greenhouse simulation. This
agricultural setting serves as a challenging testbed, as its combination of
dense foliage, similar-looking elements, and occlusions pushes the limits of
perception, thereby highlighting the framework's value for real-world
applications.



---

## RF-PGS: Fully-structured Spatial Wireless Channel Representation with  Planar Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-23 | Lihao Zhang, Zongtan Li, Haijian Sun | cs.CV | [PDF](http://arxiv.org/pdf/2508.16849v1){: .btn .btn-green } |

**Abstract**: In the 6G era, the demand for higher system throughput and the implementation
of emerging 6G technologies require large-scale antenna arrays and accurate
spatial channel state information (Spatial-CSI). Traditional channel modeling
approaches, such as empirical models, ray tracing, and measurement-based
methods, face challenges in spatial resolution, efficiency, and scalability.
Radiance field-based methods have emerged as promising alternatives but still
suffer from geometric inaccuracy and costly supervision. This paper proposes
RF-PGS, a novel framework that reconstructs high-fidelity radio propagation
paths from only sparse path loss spectra. By introducing Planar Gaussians as
geometry primitives with certain RF-specific optimizations, RF-PGS achieves
dense, surface-aligned scene reconstruction in the first geometry training
stage. In the subsequent Radio Frequency (RF) training stage, the proposed
fully-structured radio radiance, combined with a tailored multi-view loss,
accurately models radio propagation behavior. Compared to prior radiance field
methods, RF-PGS significantly improves reconstruction accuracy, reduces
training costs, and enables efficient representation of wireless channels,
offering a practical solution for scalable 6G Spatial-CSI modeling.

Comments:
- 13 pages, 16 figures, in submission to IEEE journal

---

## Align 3D Representation and Text Embedding for 3D Content  Personalization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-23 | Qi Song, Ziyuan Luo, Ka Chun Cheung, Simon See, Renjie Wan | cs.CV | [PDF](http://arxiv.org/pdf/2508.16932v1){: .btn .btn-green } |

**Abstract**: Recent advances in NeRF and 3DGS have significantly enhanced the efficiency
and quality of 3D content synthesis. However, efficient personalization of
generated 3D content remains a critical challenge. Current 3D personalization
approaches predominantly rely on knowledge distillation-based methods, which
require computationally expensive retraining procedures. To address this
challenge, we propose \textbf{Invert3D}, a novel framework for convenient 3D
content personalization. Nowadays, vision-language models such as CLIP enable
direct image personalization through aligned vision-text embedding spaces.
However, the inherent structural differences between 3D content and 2D images
preclude direct application of these techniques to 3D personalization. Our
approach bridges this gap by establishing alignment between 3D representations
and text embedding spaces. Specifically, we develop a camera-conditioned
3D-to-text inverse mechanism that projects 3D contents into a 3D embedding
aligned with text embeddings. This alignment enables efficient manipulation and
personalization of 3D content through natural language prompts, eliminating the
need for computationally retraining procedures. Extensive experiments
demonstrate that Invert3D achieves effective personalization of 3D content. Our
work is available at: https://github.com/qsong2001/Invert3D.



---

## Arbitrary-Scale 3D Gaussian Super-Resolution

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-22 | Huimin Zeng, Yue Bai, Yun Fu | cs.CV | [PDF](http://arxiv.org/pdf/2508.16467v1){: .btn .btn-green } |

**Abstract**: Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically
perform high-resolution (HR) rendering of fixed scale factors, making them
impractical for resource-limited scenarios. Directly rendering arbitrary-scale
HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of
scale-aware rendering ability, while adding a post-processing upsampler for
3DGS complicates the framework and reduces rendering efficiency. To tackle
these issues, we build an integrated framework that incorporates scale-aware
rendering, generative prior-guided optimization, and progressive
super-resolving to enable 3D Gaussian super-resolution of arbitrary scale
factors with a single 3D model. Notably, our approach supports both integer and
non-integer scale rendering to provide more flexibility. Extensive experiments
demonstrate the effectiveness of our model in rendering high-quality
arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It
preserves structural consistency with LR views and across different scales,
while maintaining real-time rendering speed (85 FPS at 1080p).



---

## Enhancing Novel View Synthesis from extremely sparse views with SfM-free  3D Gaussian Splatting Framework

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-21 | Zongqi He, Hanmin Li, Kin-Chung Chan, Yushen Zuo, Hao Xie, Zhe Xiao, Jun Xiao, Kin-Man Lam | cs.CV | [PDF](http://arxiv.org/pdf/2508.15457v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time
performance in novel view synthesis, yet its effectiveness relies heavily on
dense multi-view inputs with precisely known camera poses, which are rarely
available in real-world scenarios. When input views become extremely sparse,
the Structure-from-Motion (SfM) method that 3DGS depends on for initialization
fails to accurately reconstruct the 3D geometric structures of scenes,
resulting in degraded rendering quality. In this paper, we propose a novel
SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs
3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we
propose a dense stereo module to progressively estimates camera pose
information and reconstructs a global dense point cloud for initialization. To
address the inherent problem of information scarcity in extremely sparse-view
settings, we propose a coherent view interpolation module that interpolates
camera poses based on training view pairs and generates viewpoint-consistent
content as additional supervision signals for training. Furthermore, we
introduce multi-scale Laplacian consistent regularization and adaptive
spatial-aware multi-scale geometry regularization to enhance the quality of
geometrical structures and rendered content. Experiments show that our method
significantly outperforms other state-of-the-art 3DGS-based approaches,
achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view
conditions (using only 2 training views). The images synthesized by our method
exhibit minimal distortion while preserving rich high-frequency details,
resulting in superior visual quality compared to existing techniques.

Comments:
- 13 pages, 4 figures

---

## Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting  with Upsampled 2D X-ray Projection Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-21 | Jeonghyun Noh, Hyun-Jic Oh, Byungju Chae, Won-Ki Jeong | eess.IV | [PDF](http://arxiv.org/pdf/2508.15151v1){: .btn .btn-green } |

**Abstract**: Computed tomography (CT) is widely used in clinical diagnosis, but acquiring
high-resolution (HR) CT is limited by radiation exposure risks. Deep
learning-based super-resolution (SR) methods have been studied to reconstruct
HR from low-resolution (LR) inputs. While supervised SR approaches have shown
promising results, they require large-scale paired LR-HR volume datasets that
are often unavailable. In contrast, zero-shot methods alleviate the need for
paired data by using only a single LR input, but typically struggle to recover
fine anatomical details due to limited internal information. To overcome these,
we propose a novel zero-shot 3D CT SR framework that leverages upsampled 2D
X-ray projection priors generated by a diffusion model. Exploiting the
abundance of HR 2D X-ray data, we train a diffusion model on large-scale 2D
X-ray projection and introduce a per-projection adaptive sampling strategy. It
selects the generative process for each projection, thus providing HR
projections as strong external priors for 3D CT reconstruction. These
projections serve as inputs to 3D Gaussian splatting for reconstructing a 3D CT
volume. Furthermore, we propose negative alpha blending (NAB-GS) that allows
negative values in Gaussian density representation. NAB-GS enables residual
learning between LR and diffusion-based projections, thereby enhancing
high-frequency structure reconstruction. Experiments on two datasets show that
our method achieves superior quantitative and qualitative results for 3D CT SR.



---

## DriveSplat: Decoupled Driving Scene Reconstruction with  Geometry-enhanced Partitioned Neural Gaussians


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-21 | Cong Wang, Xianda Guo, Wenbo Xu, Wei Tian, Ruiqi Song, Chenming Zhang, Lingxi Li, Long Chen | cs.CV | [PDF](http://arxiv.org/pdf/2508.15376v2){: .btn .btn-green } |

**Abstract**: In the realm of driving scenarios, the presence of rapidly moving vehicles,
pedestrians in motion, and large-scale static backgrounds poses significant
challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian
Splatting address the motion blur problem by decoupling dynamic and static
components within the scene. However, these decoupling strategies overlook
background optimization with adequate geometry relationships and rely solely on
fitting each training view by adding Gaussians. Therefore, these models exhibit
limited robustness in rendering novel views and lack an accurate geometric
representation. To address the above issues, we introduce DriveSplat, a
high-quality reconstruction method for driving scenarios based on neural
Gaussian representations with dynamic-static decoupling. To better accommodate
the predominantly linear motion patterns of driving viewpoints, a region-wise
voxel initialization scheme is employed, which partitions the scene into near,
middle, and far regions to enhance close-range detail representation.
Deformable neural Gaussians are introduced to model non-rigid dynamic actors,
whose parameters are temporally adjusted by a learnable deformation network.
The entire framework is further supervised by depth and normal priors from
pre-trained models, improving the accuracy of geometric structures. Our method
has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating
state-of-the-art performance in novel-view synthesis for driving scenarios.



---

## UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose  Estimation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-21 | Zhaodong Jiang, Ashish Sinha, Tongtong Cao, Yuan Ren, Bingbing Liu, Binbin Xu | cs.RO | [PDF](http://arxiv.org/pdf/2508.15972v1){: .btn .btn-green } |

**Abstract**: Estimating the 6D pose of novel objects is a fundamental yet challenging
problem in robotics, often relying on access to object CAD models. However,
acquiring such models can be costly and impractical. Recent approaches aim to
bypass this requirement by leveraging strong priors from foundation models to
reconstruct objects from single or multi-view images, but typically require
additional training or produce hallucinated geometry. To this end, we propose
UnPose, a novel framework for zero-shot, model-free 6D object pose estimation
and reconstruction that exploits 3D priors and uncertainty estimates from a
pre-trained diffusion model. Specifically, starting from a single-view RGB-D
frame, UnPose uses a multi-view diffusion model to estimate an initial 3D model
using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise
epistemic uncertainty estimates. As additional observations become available,
we incrementally refine the 3DGS model by fusing new views guided by the
diffusion model's uncertainty, thereby continuously improving the pose
estimation accuracy and 3D reconstruction quality. To ensure global
consistency, the diffusion prior-generated views and subsequent observations
are further integrated in a pose graph and jointly optimized into a coherent
3DGS field. Extensive experiments demonstrate that UnPose significantly
outperforms existing approaches in both 6D pose estimation accuracy and 3D
reconstruction quality. We further showcase its practical applicability in
real-world robotic manipulation tasks.

Comments:
- Published at the Conference on Robot Learning (CoRL) 2025. For more
  details please visit https://frankzhaodong.github.io/UnPose

---

## Image-Conditioned 3D Gaussian Splat Quantization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-21 | Xinshuang Liu, Runfa Blark Li, Keito Suzuki, Truong Nguyen | cs.CV | [PDF](http://arxiv.org/pdf/2508.15372v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has attracted considerable attention for
enabling high-quality real-time rendering. Although 3DGS compression methods
have been proposed for deployment on storage-constrained devices, two
limitations hinder archival use: (1) they compress medium-scale scenes only to
the megabyte range, which remains impractical for large-scale scenes or
extensive scene collections; and (2) they lack mechanisms to accommodate scene
changes after long-term archival. To address these limitations, we propose an
Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially
enhances compression efficiency and provides adaptability to scene changes
after archiving. ICGS-Quantizer improves quantization efficiency by jointly
exploiting inter-Gaussian and inter-attribute correlations and by using shared
codebooks across all training scenes, which are then fixed and applied to
previously unseen test scenes, eliminating the overhead of per-scene codebooks.
This approach effectively reduces the storage requirements for 3DGS to the
kilobyte range while preserving visual fidelity. To enable adaptability to
post-archival scene changes, ICGS-Quantizer conditions scene decoding on images
captured at decoding time. The encoding, quantization, and decoding processes
are trained jointly, ensuring that the codes, which are quantized
representations of the scene, are effective for conditional decoding. We
evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating.
Experimental results show that ICGS-Quantizer consistently outperforms
state-of-the-art methods in compression efficiency and adaptability to scene
changes. Our code, model, and data will be publicly available on GitHub.



---

## MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View  Consistent Diffusion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-21 | Xuyang Chen, Zhijun Zhai, Kaixuan Zhou, Zengmao Wang, Jianan He, Dong Wang, Yanfeng Zhang, mingwei Sun, RÃ¼diger Westermann, Konrad Schindler, Liqiu Meng | cs.CV | [PDF](http://arxiv.org/pdf/2508.15169v2){: .btn .btn-green } |

**Abstract**: Mesh models have become increasingly accessible for numerous cities; however,
the lack of realistic textures restricts their application in virtual urban
navigation and autonomous driving. To address this, this paper proposes MeSS
(Meshbased Scene Synthesis) for generating high-quality, styleconsistent
outdoor scenes with city mesh models serving as the geometric prior. While
image and video diffusion models can leverage spatial layouts (such as depth
maps or HD maps) as control conditions to generate street-level perspective
views, they are not directly applicable to 3D scene generation. Video diffusion
models excel at synthesizing consistent view sequences that depict scenes but
often struggle to adhere to predefined camera paths or align accurately with
rendered control videos. In contrast, image diffusion models, though unable to
guarantee cross-view visual consistency, can produce more geometry-aligned
results when combined with ControlNet. Building on this insight, our approach
enhances image diffusion models by improving cross-view consistency. The
pipeline comprises three key stages: first, we generate geometrically
consistent sparse views using Cascaded Outpainting ControlNets; second, we
propagate denser intermediate views via a component dubbed AGInpaint; and
third, we globally eliminate visual inconsistencies (e.g., varying exposure)
using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting
(3DGS) scene is reconstructed by initializing Gaussian balls on the mesh
surface. Our method outperforms existing approaches in both geometric alignment
and generation quality. Once synthesized, the scene can be rendered in diverse
styles through relighting and style transfer techniques.



---

## GeMS: Efficient Gaussian Splatting for Extreme Motion Blur

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-20 | Gopi Raju Matta, Trisha Reddypalli, Vemunuri Divya Madhuri, Kaushik Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2508.14682v1){: .btn .btn-green } |

**Abstract**: We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to
handle severely motion-blurred images. State-of-the-art deblurring methods for
extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches
like Deblur-GS, typically assume access to sharp images for camera pose
estimation and point cloud generation, an unrealistic assumption. Methods
relying on COLMAP initialization, such as BAD-Gaussians, also fail due to
unreliable feature correspondences under severe blur. To address these
challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly
from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep
learning-based Structure-from-Motion pipeline that estimates poses and
generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which
enables robust scene initialization by treating Gaussians as samples from a
probability distribution, eliminating heuristic densification and pruning; and
(3) joint optimization of camera trajectories and Gaussian parameters for
stable reconstruction. While this pipeline produces strong results,
inaccuracies may remain when all inputs are severely blurred. To mitigate this,
we propose GeMS-E, which integrates a progressive refinement step using events:
(4) Event-based Double Integral (EDI) deblurring restores sharper images that
are then fed into GeMS, improving pose estimation, point cloud generation, and
overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art
performance on synthetic and real-world datasets. To our knowledge, this is the
first framework to address extreme motion blur within 3DGS directly from
severely blurred inputs.



---

## GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-20 | Jiaxin Wei, Stefan Leutenegger, Simon Schaefer | cs.CV | [PDF](http://arxiv.org/pdf/2508.14717v1){: .btn .btn-green } |

**Abstract**: Recent developments in 3D Gaussian Splatting have significantly enhanced
novel view synthesis, yet generating high-quality renderings from extreme novel
viewpoints or partially observed regions remains challenging. Meanwhile,
diffusion models exhibit strong generative capabilities, but their reliance on
text prompts and lack of awareness of specific scene information hinder
accurate 3D reconstruction tasks. To address these limitations, we introduce
GSFix3D, a novel framework that improves the visual fidelity in
under-constrained regions by distilling prior knowledge from diffusion models
into 3D representations, while preserving consistency with observed scene
details. At its core is GSFixer, a latent diffusion model obtained via our
customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to
adapt pretrained generative models to a variety of environments and artifact
types from different reconstruction methods, enabling robust novel view repair
for unseen camera poses. Moreover, we propose a random mask augmentation
strategy that empowers GSFixer to plausibly inpaint missing regions.
Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer
achieve state-of-the-art performance, requiring only minimal scene-specific
fine-tuning on captured data. Real-world test further confirms its resilience
to potential pose errors. Our code and data will be made publicly available.
Project page: https://gsfix3d.github.io.



---

## Reconstruction Using the Invisible: Intuition from NIR and Metadata for  Enhanced 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-20 | Gyusam Chang, Tuan-Anh Vu, Vivek Alumootil, Harris Song, Deanna Pham, Sangpil Kim, M. Khalid Jawed | cs.CV | [PDF](http://arxiv.org/pdf/2508.14443v1){: .btn .btn-green } |

**Abstract**: While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in
agriculture remains underexplored. Agricultural scenes present unique
challenges for 3D reconstruction methods, particularly due to uneven
illumination, occlusions, and a limited field of view. To address these
limitations, we introduce \textbf{NIRPlant}, a novel multimodal dataset
encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth,
and LiDAR data collected under varied indoor and outdoor lighting conditions.
By integrating NIR data, our approach enhances robustness and provides crucial
botanical insights that extend beyond the visible spectrum. Additionally, we
leverage text-based metadata derived from vegetation indices, such as NDVI,
NDWI, and the chlorophyll index, which significantly enriches the contextual
understanding of complex agricultural environments. To fully exploit these
modalities, we propose \textbf{NIRSplat}, an effective multimodal Gaussian
splatting architecture employing a cross-attention mechanism combined with 3D
point-based positional encoding, providing robust geometric priors.
Comprehensive experiments demonstrate that \textbf{NIRSplat} outperforms
existing landmark methods, including 3DGS, CoR-GS, and InstantSplat,
highlighting its effectiveness in challenging agricultural scenarios. The code
and dataset are publicly available at:
https://github.com/StructuresComp/3D-Reconstruction-NIR



---

## GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via  Gaussian Surfels

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-20 | Xingyuan Yang, Min Wei | cs.CV | [PDF](http://arxiv.org/pdf/2508.14563v1){: .btn .btn-green } |

**Abstract**: Inverse rendering of glossy objects from RGB imagery remains fundamentally
limited by inherent ambiguity. Although NeRF-based methods achieve
high-fidelity reconstruction via dense-ray sampling, their computational cost
is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction
efficiency but exhibits limitations under specular reflections. Multi-view
inconsistencies introduce high-frequency surface noise and structural
artifacts, while simplified rendering equations obscure material properties,
leading to implausible relighting results. To address these issues, we propose
GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we
establish robust surface reconstruction through physics-based rendering with
split-sum approximation, enhanced by geometric priors from foundation models.
Second, we perform material decomposition by leveraging Monte Carlo importance
sampling of the full rendering equation, modeling indirect illumination via
differentiable 2D Gaussian ray tracing and refining high-frequency specular
details through spherical mipmap-based directional encoding that captures
anisotropic highlights. Extensive experiments demonstrate state-of-the-art
performance in geometry reconstruction, material separation, and photorealistic
relighting under novel illuminations, outperforming existing inverse rendering
approaches.

Comments:
- 13 pages, 13 figures

---

## From Slices to Structures: Unsupervised 3D Reconstruction of Female  Pelvic Anatomy from Freehand Transvaginal Ultrasound

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-20 | Max KrÃ¤henmann, Sergio Tascon-Morales, Fabian Laumer, Julia E. Vogt, Ece Ozkan | eess.IV | [PDF](http://arxiv.org/pdf/2508.14552v1){: .btn .btn-green } |

**Abstract**: Volumetric ultrasound has the potential to significantly improve diagnostic
accuracy and clinical decision-making, yet its widespread adoption remains
limited by dependence on specialized hardware and restrictive acquisition
protocols. In this work, we present a novel unsupervised framework for
reconstructing 3D anatomical structures from freehand 2D transvaginal
ultrasound (TVS) sweeps, without requiring external tracking or learned pose
estimators. Our method adapts the principles of Gaussian Splatting to the
domain of ultrasound, introducing a slice-aware, differentiable rasterizer
tailored to the unique physics and geometry of ultrasound imaging. We model
anatomy as a collection of anisotropic 3D Gaussians and optimize their
parameters directly from image-level supervision, leveraging sensorless probe
motion estimation and domain-specific geometric priors. The result is a
compact, flexible, and memory-efficient volumetric representation that captures
anatomical detail with high spatial fidelity. This work demonstrates that
accurate 3D reconstruction from 2D ultrasound images can be achieved through
purely computational means, offering a scalable alternative to conventional 3D
systems and enabling new opportunities for AI-assisted analysis and diagnosis.



---

## Pixie: Fast and Generalizable Supervised Learning of 3D Physics from  Pixels

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-20 | Long Le, Ryan Lucas, Chen Wang, Chuhao Chen, Dinesh Jayaraman, Eric Eaton, Lingjie Liu | cs.CV | [PDF](http://arxiv.org/pdf/2508.17437v2){: .btn .btn-green } |

**Abstract**: Inferring the physical properties of 3D scenes from visual information is a
critical yet challenging task for creating interactive and realistic virtual
worlds. While humans intuitively grasp material characteristics such as
elasticity or stiffness, existing methods often rely on slow, per-scene
optimization, limiting their generalizability and application. To address this
problem, we introduce PIXIE, a novel method that trains a generalizable neural
network to predict physical properties across multiple scenes from 3D visual
features purely using supervised losses. Once trained, our feed-forward network
can perform fast inference of plausible material fields, which coupled with a
learned static scene representation like Gaussian Splatting enables realistic
physics simulation under external forces. To facilitate this research, we also
collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and
physic material annotations. Extensive evaluations demonstrate that PIXIE is
about 1.46-4.39x better and orders of magnitude faster than test-time
optimization methods. By leveraging pretrained visual features like CLIP, our
method can also zero-shot generalize to real-world scenes despite only ever
been trained on synthetic data. https://pixie-3d.github.io/

Comments:
- Website: https://pixie-3d.github.io/

---

## Distilled-3DGS:Distilled 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-19 | Lintao Xiang, Xinkai Chen, Jianhuang Lai, Guangcong Wang | cs.CV | [PDF](http://arxiv.org/pdf/2508.14037v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view
synthesis (NVS). However, it suffers from a significant drawback: achieving
high-fidelity rendering typically necessitates a large number of 3D Gaussians,
resulting in substantial memory consumption and storage requirements. To
address this challenge, we propose the first knowledge distillation framework
for 3DGS, featuring various teacher models, including vanilla 3DGS,
noise-augmented variants, and dropout-regularized versions. The outputs of
these teachers are aggregated to guide the optimization of a lightweight
student model. To distill the hidden geometric structure, we propose a
structural similarity loss to boost the consistency of spatial geometric
distributions between the student and teacher model. Through comprehensive
quantitative and qualitative evaluations across diverse datasets, the proposed
Distilled-3DGS, a simple yet effective framework without bells and whistles,
achieves promising rendering results in both rendering quality and storage
efficiency compared to state-of-the-art methods. Project page:
https://distilled3dgs.github.io . Code:
https://github.com/lt-xiang/Distilled-3DGS .

Comments:
- Project page: https://distilled3dgs.github.io Code:
  https://github.com/lt-xiang/Distilled-3DGS

---

## EAvatar: Expression-Aware Head Avatar Reconstruction with Generative  Geometry Priors


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-19 | Shikun Zhang, Cunjian Chen, Yiqun Wang, Qiuhong Ke, Yong Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.13537v1){: .btn .btn-green } |

**Abstract**: High-fidelity head avatar reconstruction plays a crucial role in AR/VR,
gaming, and multimedia content creation. Recent advances in 3D Gaussian
Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry
with real-time rendering capability and are now widely used in high-fidelity
head avatar reconstruction tasks. However, existing 3DGS-based methods still
face significant challenges in capturing fine-grained facial expressions and
preserving local texture continuity, especially in highly deformable regions.
To mitigate these limitations, we propose a novel 3DGS-based framework termed
EAvatar for head reconstruction that is both expression-aware and
deformation-aware. Our method introduces a sparse expression control mechanism,
where a small number of key Gaussians are used to influence the deformation of
their neighboring Gaussians, enabling accurate modeling of local deformations
and fine-scale texture transitions. Furthermore, we leverage high-quality 3D
priors from pretrained generative models to provide a more reliable facial
geometry, offering structural guidance that improves convergence stability and
shape accuracy during training. Experimental results demonstrate that our
method produces more accurate and visually coherent head reconstructions with
improved expression controllability and detail fidelity.

Comments:
- 20 pages, 11 figures

---

## LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-19 | Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2508.14041v1){: .btn .btn-green } |

**Abstract**: LongSplat addresses critical challenges in novel view synthesis (NVS) from
casually captured long videos characterized by irregular camera motion, unknown
camera poses, and expansive scenes. Current methods often suffer from pose
drift, inaccurate geometry initialization, and severe memory limitations. To
address these issues, we introduce LongSplat, a robust unposed 3D Gaussian
Splatting framework featuring: (1) Incremental Joint Optimization that
concurrently optimizes camera poses and 3D Gaussians to avoid local minima and
ensure global consistency; (2) a robust Pose Estimation Module leveraging
learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that
converts dense point clouds into anchors based on spatial density. Extensive
experiments on challenging benchmarks demonstrate that LongSplat achieves
state-of-the-art results, substantially improving rendering quality, pose
accuracy, and computational efficiency compared to prior approaches. Project
page: https://linjohnss.github.io/longsplat/

Comments:
- ICCV 2025. Project page: https://linjohnss.github.io/longsplat/

---

## GALA: Guided Attention with Language Alignment for Open Vocabulary  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-19 | Elena Alegret, Kunyi Li, Sen Wang, Siyun Liang, Michael Niemeyer, Stefano Gasperini, Nassir Navab, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2508.14278v2){: .btn .btn-green } |

**Abstract**: 3D scene reconstruction and understanding have gained increasing popularity,
yet existing methods still struggle to capture fine-grained, language-aware 3D
representations from 2D images. In this paper, we present GALA, a novel
framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting
(3DGS). GALA distills a scene-specific 3D instance feature field via
self-supervised contrastive learning. To extend to generalized language feature
fields, we introduce the core contribution of GALA, a cross-attention module
with two learnable codebooks that encode view-independent semantic embeddings.
This design not only ensures intra-instance feature similarity but also
supports seamless 2D and 3D open-vocabulary queries. It reduces memory
consumption by avoiding per-Gaussian high-dimensional feature learning.
Extensive experiments on real-world datasets demonstrate GALA's remarkable
open-vocabulary performance on both 2D and 3D.



---

## Online 3D Gaussian Splatting Modeling with Novel View Selection

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-19 | Byeonggwon Lee, Junkyu Park, Khang Truong Giang, Soohwan Song | cs.CV | [PDF](http://arxiv.org/pdf/2508.14014v1){: .btn .btn-green } |

**Abstract**: This study addresses the challenge of generating online 3D Gaussian Splatting
(3DGS) models from RGB-only frames. Previous studies have employed dense SLAM
techniques to estimate 3D scenes from keyframes for 3DGS model construction.
However, these methods are limited by their reliance solely on keyframes, which
are insufficient to capture an entire scene, resulting in incomplete
reconstructions. Moreover, building a generalizable model requires
incorporating frames from diverse viewpoints to achieve broader scene coverage.
However, online processing restricts the use of many frames or extensive
training iterations. Therefore, we propose a novel method for high-quality 3DGS
modeling that improves model completeness through adaptive view selection. By
analyzing reconstruction quality online, our approach selects optimal
non-keyframes for additional training. By integrating both keyframes and
selected non-keyframes, the method refines incomplete regions from diverse
viewpoints, significantly enhancing completeness. We also present a framework
that incorporates an online multi-view stereo approach, ensuring consistency in
3D information throughout the 3DGS modeling process. Experimental results
demonstrate that our method outperforms state-of-the-art methods, delivering
exceptional performance in complex outdoor scenes.



---

## PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-19 | Chunji Lv, Zequn Chen, Donglin Di, Weinan Zhang, Hao Li, Wei Chen, Changsheng Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.13911v1){: .btn .btn-green } |

**Abstract**: While physics-grounded 3D motion synthesis has seen significant progress,
current methods face critical limitations. They typically rely on
pre-reconstructed 3D Gaussian Splatting (3DGS) representations, while physics
integration depends on either inflexible, manually defined physical attributes
or unstable, optimization-heavy guidance from video models. To overcome these
challenges, we introduce PhysGM, a feed-forward framework that jointly predicts
a 3D Gaussian representation and its physical properties from a single image,
enabling immediate, physical simulation and high-fidelity 4D rendering. We
first establish a base model by jointly optimizing for Gaussian reconstruction
and probabilistic physics prediction. The model is then refined with physically
plausible reference videos to enhance both rendering fidelity and physics
prediction accuracy. We adopt the Direct Preference Optimization (DPO) to align
its simulations with reference videos, circumventing Score Distillation
Sampling (SDS) optimization which needs back-propagating gradients through the
complex differentiable simulation and rasterization. To facilitate the
training, we introduce a new dataset PhysAssets of over 24,000 3D assets,
annotated with physical properties and corresponding guiding videos.
Experimental results demonstrate that our method effectively generates
high-fidelity 4D simulations from a single image in one minute. This represents
a significant speedup over prior works while delivering realistic rendering
results. Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/



---

## Is-NeRF: In-scattering Neural Radiance Field for Blurred Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-19 | Nan Luo, Chenglin Ye, Jiaxu Li, Gang Liu, Bo Wan, Di Wang, Lupeng Liu, Jun Xiao | cs.GR | [PDF](http://arxiv.org/pdf/2508.13808v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) has gained significant attention for its
prominent implicit 3D representation and realistic novel view synthesis
capabilities. Available works unexceptionally employ straight-line volume
rendering, which struggles to handle sophisticated lightpath scenarios and
introduces geometric ambiguities during training, particularly evident when
processing motion-blurred images. To address these challenges, this work
proposes a novel deblur neural radiance field, Is-NeRF, featuring explicit
lightpath modeling in real-world environments. By unifying six common light
propagation phenomena through an in-scattering representation, we establish a
new scattering-aware volume rendering pipeline adaptable to complex lightpaths.
Additionally, we introduce an adaptive learning strategy that enables
autonomous determining of scattering directions and sampling intervals to
capture finer object details. The proposed network jointly optimizes NeRF
parameters, scattering parameters, and camera motions to recover fine-grained
scene representations from blurry images. Comprehensive evaluations demonstrate
that it effectively handles complex real-world scenarios, outperforming
state-of-the-art approaches in generating high-fidelity images with accurate
geometric details.



---

## InnerGS: Internal Scenes Rendering via Factorized 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-18 | Shuxin Liang, Yihan Xiao, Wenlu Tang | eess.IV | [PDF](http://arxiv.org/pdf/2508.13287v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently gained popularity for efficient
scene rendering by representing scenes as explicit sets of anisotropic 3D
Gaussians. However, most existing work focuses primarily on modeling external
surfaces. In this work, we target the reconstruction of internal scenes, which
is crucial for applications that require a deep understanding of an object's
interior. By directly modeling a continuous volumetric density through the
inner 3D Gaussian distribution, our model effectively reconstructs smooth and
detailed internal structures from sparse sliced data. Our approach eliminates
the need for camera poses, is plug-and-play, and is inherently compatible with
any data modalities. We provide cuda implementation at:
https://github.com/Shuxin-Liang/InnerGS.



---

## Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-18 | Kangjie Chen, Yingji Zhong, Zhihao Li, Jiaqi Lin, Youyu Chen, Minghan Qin, Haoqian Wang | cs.CV | [PDF](http://arxiv.org/pdf/2508.12720v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

Comments:
- Under review. Project page:
  https://chenkangjie1123.github.io/Co-Adaptation-3DGS/

---

## IntelliCap: Intelligent Guidance for Consistent View Sampling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-18 | Ayaka Yasunaga, Hideo Saito, Dieter Schmalstieg, Shohei Mori | cs.CV | [PDF](http://arxiv.org/pdf/2508.13043v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

Comments:
- This work is a pre-print version of a paper that has been accepted to
  the IEEE International Symposium on Mixed and Augmented Reality for future
  publication. Project Page:
  https://mediated-reality.github.io/projects/yasunaga_ismar25/

---

## Improving Densification in 3D Gaussian Splatting for High-Fidelity  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-17 | Xiaobin Deng, Changyu Diao, Min Li, Ruohan Yu, Duanqing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2508.12313v1){: .btn .btn-green } |

**Abstract**: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

Comments:
- Project page: https://xiaobin2001.github.io/improved-gs-web

---

## TiP4GEN: Text to Immersive Panorama 4D Scene Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-17 | Ke Xing, Hanwen Liang, Dejia Xu, Yuyang Yin, Konstantinos N. Plataniotis, Yao Zhao, Yunchao Wei | cs.CV | [PDF](http://arxiv.org/pdf/2508.12415v2){: .btn .btn-green } |

**Abstract**: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

Comments:
- Accepted In Proceedings of the 33rd ACM International Conference on
  Multimedia (MM' 25)

---

## PreSem-Surf: RGB-D Surface Reconstruction with Progressive Semantic  Modeling and SG-MLP Pre-Rendering Mechanism

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-17 | Yuyan Ye, Hang Xu, Yanghang Huang, Jiali Huang, Qian Weng | cs.GR | [PDF](http://arxiv.org/pdf/2508.13228v1){: .btn .btn-green } |

**Abstract**: This paper proposes PreSem-Surf, an optimized method based on the Neural
Radiance Field (NeRF) framework, capable of reconstructing high-quality scene
surfaces from RGB-D sequences in a short time. The method integrates RGB,
depth, and semantic information to improve reconstruction performance.
Specifically, a novel SG-MLP sampling structure combined with PR-MLP
(Preconditioning Multilayer Perceptron) is introduced for voxel pre-rendering,
allowing the model to capture scene-related information earlier and better
distinguish noise from local details. Furthermore, progressive semantic
modeling is adopted to extract semantic information at increasing levels of
precision, reducing training time while enhancing scene understanding.
Experiments on seven synthetic scenes with six evaluation metrics show that
PreSem-Surf achieves the best performance in C-L1, F-score, and IoU, while
maintaining competitive results in NC, Accuracy, and Completeness,
demonstrating its effectiveness and practical applicability.

Comments:
- 2025 International Joint Conference on Neural Networks (IJCNN 2025)

---

## InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-16 | Hongyuan Liu, Haochen Yu, Jianfei Jiang, Qiankun Liu, Jiansheng Chen, Huimin Ma | cs.CV | [PDF](http://arxiv.org/pdf/2508.12015v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.



---

## ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by  3D Gaussian Splat Camouflages

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-16 | Matthew Hull, Haoyang Yang, Pratham Mehta, Mansi Phute, Aeree Cho, Haorang Wang, Matthew Lau, Wenke Lee, Wilian Lunardi, Martin Andreoni, Polo Chau | cs.CV | [PDF](http://arxiv.org/pdf/2508.11854v1){: .btn .btn-green } |

**Abstract**: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

Comments:
- 7 pages, 6 figures

---

## RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-16 | Wenqing Wang, Yun Fu | cs.CV | [PDF](http://arxiv.org/pdf/2508.12163v1){: .btn .btn-green } |

**Abstract**: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

Comments:
- Accepted to the ICCV 2025 Workshop on Artificial Social Intelligence

---

## Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-15 | Simona Kocour, Assia Benbihi, Torsten Sattler | cs.CV | [PDF](http://arxiv.org/pdf/2508.11431v1){: .btn .btn-green } |

**Abstract**: Understanding what semantic information persists after object removal is
critical for privacy-preserving 3D reconstruction and editable scene
representations. In this work, we introduce a novel benchmark and evaluation
framework to measure semantic residuals, the unintended semantic traces left
behind, after object removal in 3D Gaussian Splatting. We conduct experiments
across a diverse set of indoor and outdoor scenes, showing that current methods
can preserve semantic information despite the absence of visual geometry. We
also release Remove360, a dataset of pre/post-removal RGB images and
object-level masks captured in real-world environments. While prior datasets
have focused on isolated object instances, Remove360 covers a broader and more
complex range of indoor and outdoor scenes, enabling evaluation of object
removal in the context of full-scene representations. Given ground truth images
of a scene before and after object removal, we assess whether we can truly
eliminate semantic presence, and if downstream models can still infer what was
removed. Our findings reveal critical limitations in current 3D object removal
techniques and underscore the need for more robust solutions capable of
handling real-world complexity. The evaluation framework is available at
github.com/spatial-intelligence-ai/Remove360.git. Data are available at
huggingface.co/datasets/simkoc/Remove360.

Comments:
- arXiv admin note: substantial text overlap with arXiv:2503.17574

---

## Versatile Video Tokenization with Generative 2D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-15 | Zhenghao Chen, Zicong Chen, Lei Liu, Yiming Wu, Dong Xu | cs.CV | [PDF](http://arxiv.org/pdf/2508.11183v1){: .btn .btn-green } |

**Abstract**: Video tokenization procedure is critical for a wide range of video processing
tasks. Most existing approaches directly transform video into fixed-grid and
patch-wise tokens, which exhibit limited versatility. Spatially, uniformly
allocating a fixed number of tokens often leads to over-encoding in
low-information regions. Temporally, reducing redundancy remains challenging
without explicitly distinguishing between static and dynamic content. In this
work, we propose the Gaussian Video Transformer (GVT), a versatile video
tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We
first extract latent rigid features from a video clip and represent them with a
set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian
Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D
Gaussians not only enhance spatial adaptability by assigning higher (resp.,
lower) rendering weights to regions with higher (resp., lower) information
content during rasterization, but also improve generalization by avoiding
per-video optimization.To enhance the temporal versatility, we introduce a
Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into
static and dynamic sets, which explicitly model static content shared across
different time-steps and dynamic content specific to each time-step, enabling a
compact representation.We primarily evaluate GVT on the video reconstruction,
while also assessing its performance on action recognition and compression
using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments
demonstrate that GVT achieves a state-of-the-art video reconstruction quality,
outperforms the baseline MAGVIT-v2 in action recognition, and delivers
comparable compression performance.



---

## Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-14 | Zheng Zhou, Jia-Chen Zhang, Yu-Jie Xiong, Chun-Ming Xia | cs.CV | [PDF](http://arxiv.org/pdf/2508.10507v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian splatting have significantly improved
real-time novel view synthesis, yet insufficient geometric constraints during
scene optimization often result in blurred reconstructions of fine-grained
details, particularly in regions with high-frequency textures and sharp
discontinuities. To address this, we propose a comprehensive optimization
framework integrating multisample anti-aliasing (MSAA) with dual geometric
constraints. Our system computes pixel colors through adaptive blending of
quadruple subsamples, effectively reducing aliasing artifacts in high-frequency
components. The framework introduces two constraints: (a) an adaptive weighting
strategy that prioritizes under-reconstructed regions through dynamic gradient
analysis, and (b) gradient differential constraints enforcing geometric
regularization at object boundaries. This targeted optimization enables the
model to allocate computational resources preferentially to critical regions
requiring refinement while maintaining global consistency. Extensive
experimental evaluations across multiple benchmarks demonstrate that our method
achieves state-of-the-art performance in detail preservation, particularly in
preserving high-frequency textures and sharp discontinuities, while maintaining
real-time rendering efficiency. Quantitative metrics and perceptual studies
confirm statistically significant improvements over baseline approaches in both
structural similarity (SSIM) and perceptual quality (LPIPS).



---

## SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse  Satellite Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-13 | Xuejun Huang, Xinyi Liu, Yi Wan, Zhi Zheng, Bin Zhang, Mingtao Xiong, Yingying Pei, Yongjun Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2508.09479v1){: .btn .btn-green } |

**Abstract**: Three-dimensional scene reconstruction from sparse-view satellite images is a
long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its
variants have recently attracted attention for its high efficiency, existing
methods remain unsuitable for satellite images due to incompatibility with
rational polynomial coefficient (RPC) models and limited generalization
capability. Recent advances in generalizable 3DGS approaches show potential,
but they perform poorly on multi-temporal sparse satellite images due to
limited geometric constraints, transient objects, and radiometric
inconsistencies. To address these limitations, we propose SkySplat, a novel
self-supervised framework that integrates the RPC model into the generalizable
3DGS pipeline, enabling more effective use of sparse geometric cues for
improved reconstruction. SkySplat relies only on RGB images and
radiometric-robust relative height supervision, thereby eliminating the need
for ground-truth height maps. Key components include a Cross-Self Consistency
Module (CSCM), which mitigates transient object interference via
consistency-based masking, and a multi-view consistency aggregation strategy
that refines reconstruction results. Compared to per-scene optimization
methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.
It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to
1.80 m on the DFC19 dataset significantly, and demonstrates strong
cross-dataset generalization on the MVS3D benchmark.



---

## E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event  Cameras

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-13 | Chaoran Feng, Zhenyu Tang, Wangbo Yu, Yatian Pang, Yian Zhao, Jianbin Zhao, Li Yuan, Yonghong Tian | cs.CV | [PDF](http://arxiv.org/pdf/2508.09912v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis and 4D reconstruction techniques predominantly rely on
RGB cameras, thereby inheriting inherent limitations such as the dependence on
adequate lighting, susceptibility to motion blur, and a limited dynamic range.
Event cameras, offering advantages of low power, high temporal resolution and
high dynamic range, have brought a new perspective to addressing the scene
reconstruction challenges in high-speed motion and low-light scenes. To this
end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting
approach, for novel view synthesis from multi-view event streams with
fast-moving cameras. Specifically, we introduce an event-based initialization
scheme to ensure stable training and propose event-adaptive slicing splatting
for time-aware reconstruction. Additionally, we employ intensity importance
pruning to eliminate floating artifacts and enhance 3D consistency, while
incorporating an adaptive contrast threshold for more precise optimization. We
design a synthetic multi-view camera setup with six moving event cameras
surrounding the object in a 360-degree configuration and provide a benchmark
multi-view event stream dataset that captures challenging motion scenarios. Our
approach outperforms both event-only and event-RGB fusion baselines and paves
the way for the exploration of multi-view event-based reconstruction as a novel
approach for rapid scene capture.

Comments:
- 16 pages, 10 figures, 5 Tables, accepted by ACMMM 2025

---

## DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-13 | Jiachen Li, Guangzhi Han, Jin Wan, Yuan Gao, Delong Han | cs.GR | [PDF](http://arxiv.org/pdf/2508.09610v1){: .btn .btn-green } |

**Abstract**: In 3D reconstruction of underwater scenes, traditional methods based on
atmospheric optical models cannot effectively deal with the selective
attenuation of light wavelengths and the effect of suspended particle
scattering, which are unique to the water medium, and lead to color distortion,
geometric artifacts, and collapsing phenomena at long distances. We propose the
DualPhys-GS framework to achieve high-quality underwater reconstruction through
a dual-path optimization mechanism. Our approach further develops a dual
feature-guided attenuation-scattering modeling mechanism, the RGB-guided
attenuation optimization model combines RGB features and depth information and
can handle edge and structural details. In contrast, the multi-scale
depth-aware scattering model captures scattering effects at different scales
using a feature pyramid network and an attention mechanism. Meanwhile, we
design several special loss functions. The attenuation scattering consistency
loss ensures physical consistency. The water body type adaptive loss
dynamically adjusts the weighting coefficients. The edge-aware scattering loss
is used to maintain the sharpness of structural edges. The multi-scale feature
loss helps to capture global and local structural information. In addition, we
design a scene adaptive mechanism that can automatically identify the
water-body-type characteristics (e.g., clear coral reef waters or turbid
coastal waters) and dynamically adjust the scattering and attenuation
parameters and optimization strategies. Experimental results show that our
method outperforms existing methods in several metrics, especially in suspended
matter-dense regions and long-distance scenes, and the reconstruction quality
is significantly improved.

Comments:
- 12 pages, 4 figures

---

## EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-13 | Yuning Huang, Jiahao Pang, Fengqing Zhu, Dong Tian | cs.CV | [PDF](http://arxiv.org/pdf/2508.10227v1){: .btn .btn-green } |

**Abstract**: As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)
demonstrates fast training/rendering with superior visual quality. The two
tasks of 3DGS, Gaussian creation and view rendering, are typically separated
over time or devices, and thus storage/transmission and finally compression of
3DGS Gaussians become necessary. We begin with a correlation and statistical
analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals
that spherical harmonic AC attributes precisely follow Laplace distributions,
while mixtures of Gaussian distributions can approximate rotation, scaling, and
opacity. Additionally, harmonic AC attributes manifest weak correlations with
other attributes except for inherited correlations from a color space. A
factorized and parameterized entropy coding method, EntropyGS, is hereinafter
proposed. During encoding, distribution parameters of each Gaussian attribute
are estimated to assist their entropy coding. The quantization for entropy
coding is adaptively performed according to Gaussian attribute types. EntropyGS
demonstrates about 30x rate reduction on benchmark datasets while maintaining
similar rendering quality compared to input 3DGS data, with a fast encoding and
decoding time.



---

## A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing,  and Generation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-13 | Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding | cs.CV | [PDF](http://arxiv.org/pdf/2508.09977v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative
to Neural Radiance Fields (NeRF) for 3D scene representation, offering
high-fidelity photorealistic rendering with real-time performance. Beyond novel
view synthesis, the explicit and compact nature of 3DGS enables a wide range of
downstream applications that require geometric and semantic understanding. This
survey provides a comprehensive overview of recent progress in 3DGS
applications. It first introduces 2D foundation models that support semantic
understanding and control in 3DGS applications, followed by a review of
NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS
applications into segmentation, editing, generation, and other functional
tasks. For each, we summarize representative methods, supervision strategies,
and learning paradigms, highlighting shared design principles and emerging
trends. Commonly used datasets and evaluation protocols are also summarized,
along with comparative analyses of recent methods across public benchmarks. To
support ongoing research and development, a continually updated repository of
papers, code, and resources is maintained at
https://github.com/heshuting555/Awesome-3DGS-Applications.

Comments:
- GitHub Repo:
  https://github.com/heshuting555/Awesome-3DGS-Applications

---

## HumanGenesis: Agent-Based Geometric and Generative Modeling for  Synthetic Human Dynamics

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-13 | Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang | cs.CV | [PDF](http://arxiv.org/pdf/2508.09858v1){: .btn .btn-green } |

**Abstract**: \textbf{Synthetic human dynamics} aims to generate photorealistic videos of
human subjects performing expressive, intention-driven motions. However,
current approaches face two core challenges: (1) \emph{geometric inconsistency}
and \emph{coarse reconstruction}, due to limited 3D modeling and detail
preservation; and (2) \emph{motion generalization limitations} and \emph{scene
inharmonization}, stemming from weak generative capabilities. To address these,
we present \textbf{HumanGenesis}, a framework that integrates geometric and
generative modeling through four collaborative agents: (1)
\textbf{Reconstructor} builds 3D-consistent human-scene representations from
monocular video using 3D Gaussian Splatting and deformation decomposition. (2)
\textbf{Critique Agent} enhances reconstruction fidelity by identifying and
refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose
Guider} enables motion generalization by generating expressive pose sequences
using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes
photorealistic, coherent video via a hybrid rendering pipeline with diffusion,
refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis
achieves state-of-the-art performance on tasks including text-guided synthesis,
video reenactment, and novel-pose generalization, significantly improving
expressiveness, geometric fidelity, and scene integration.



---

## Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in  surgical vision

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-13 | Gerardo Loza, Junlei Hu, Dominic Jones, Sharib Ali, Pietro Valdastri | cs.CV | [PDF](http://arxiv.org/pdf/2508.09681v1){: .btn .btn-green } |

**Abstract**: We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.

Comments:
- 10 pages

---

## Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-13 | Yuekun Wu, Yik Lung Pang, Andrea Cavallaro, Changjae Oh | cs.RO | [PDF](http://arxiv.org/pdf/2508.09855v1){: .btn .btn-green } |

**Abstract**: Human-robot teaming (HRT) systems often rely on large-scale datasets of human
and robot interactions, especially for close-proximity collaboration tasks such
as human-robot handovers. Learning robot manipulation policies from raw,
real-world image data requires a large number of robot-action trials in the
physical environment. Although simulation training offers a cost-effective
alternative, the visual domain gap between simulation and robot workspace
remains a major limitation. We introduce a method for training HRT policies,
focusing on human-to-robot handovers, solely from RGB images without the need
for real-robot training or real-robot data collection. The goal is to enable
the robot to reliably receive objects from a human with stable grasping while
avoiding collisions with the human hand. The proposed policy learner leverages
sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes
to generate robot demonstrations containing image-action pairs captured with a
camera mounted on the robot gripper. As a result, the simulated camera pose
changes in the reconstructed scene can be directly translated into gripper pose
changes. Experiments in both Gaussian Splatting reconstructed scene and
real-world human-to-robot handover experiments demonstrate that our method
serves as a new and effective representation for the human-to-robot handover
task, contributing to more seamless and robust HRT.

Comments:
- 3 pages, 3 figures

---

## GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video  Diffusion Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-13 | Xingyilang Yin, Qi Zhang, Jiahao Chang, Ying Feng, Qingnan Fan, Xi Yang, Chi-Man Pun, Huaqi Zhang, Xiaodong Cun | cs.CV | [PDF](http://arxiv.org/pdf/2508.09667v1){: .btn .btn-green } |

**Abstract**: Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views
is an ill-posed problem due to insufficient information, often resulting in
noticeable artifacts. While recent approaches have sought to leverage
generative priors to complete information for under-constrained regions, they
struggle to generate content that remains consistent with input observations.
To address this challenge, we propose GSFixer, a novel framework designed to
improve the quality of 3DGS representations reconstructed from sparse inputs.
The core of our approach is the reference-guided video restoration model, built
upon a DiT-based video diffusion model trained on paired artifact 3DGS renders
and clean frames with additional reference-based conditions. Considering the
input sparse views as references, our model integrates both 2D semantic
features and 3D geometric features of reference views extracted from the visual
geometry foundation model, enhancing the semantic coherence and 3D consistency
when fixing artifact novel views. Furthermore, considering the lack of suitable
benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which
contains artifact frames rendered using low-quality 3DGS. Extensive experiments
demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS
artifact restoration and sparse-view 3D reconstruction. Project page:
https://github.com/GVCLab/GSFixer.



---

## Gradient-Direction-Aware Density Control for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-12 | Zheng Zhou, Yu-Jie Xiong, Chun-Ming Xia, Jia-Chen Zhang, Hong-Jian Zhan | cs.CV | [PDF](http://arxiv.org/pdf/2508.09239v1){: .btn .btn-green } |

**Abstract**: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.



---

## A new dataset and comparison for multi-camera frame synthesis

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-12 | Conall Daly, Anil Kokaram | eess.IV | [PDF](http://arxiv.org/pdf/2508.09068v1){: .btn .btn-green } |

**Abstract**: Many methods exist for frame synthesis in image sequences but can be broadly
categorised into frame interpolation and view synthesis techniques.
Fundamentally, both frame interpolation and view synthesis tackle the same
task, interpolating a frame given surrounding frames in time or space. However,
most frame interpolation datasets focus on temporal aspects with single cameras
moving through time and space, while view synthesis datasets are typically
biased toward stereoscopic depth estimation use cases. This makes direct
comparison between view synthesis and frame interpolation methods challenging.
In this paper, we develop a novel multi-camera dataset using a custom-built
dense linear camera array to enable fair comparison between these approaches.
We evaluate classical and deep learning frame interpolators against a view
synthesis method (3D Gaussian Splatting) for the task of view in-betweening.
Our results reveal that deep learning methods do not significantly outperform
classical methods on real image data, with 3D Gaussian Splatting actually
underperforming frame interpolators by as much as 3.5 dB PSNR. However, in
synthetic scenes, the situation reverses -- 3D Gaussian Splatting outperforms
frame interpolation algorithms by almost 5 dB PSNR at a 95% confidence level.

Comments:
- SPIE2025 - Applications of Digital Image Processing XLVIII accepted
  manuscript

---

## Communication Efficient Robotic Mixed Reality with Gaussian Splatting  Cross-Layer Optimization

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-12 | Chenxuan Liu, He Li, Zongze Li, Shuai Wang, Wei Xu, Kejiang Ye, Derrick Wing Kwan Ng, Chengzhong Xu | cs.RO | [PDF](http://arxiv.org/pdf/2508.08624v1){: .btn .btn-green } |

**Abstract**: Realizing low-cost communication in robotic mixed reality (RoboMR) systems
presents a challenge, due to the necessity of uploading high-resolution images
through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR
(GSMR), which enables the simulator to opportunistically render a
photo-realistic view from the robot's pose by calling ``memory'' from a GS
model, thus reducing the need for excessive image uploads. However, the GS
model may involve discrepancies compared to the actual environments. To this
end, a GS cross-layer optimization (GSCLO) framework is further proposed, which
jointly optimizes content switching (i.e., deciding whether to upload image or
not) and power allocation (i.e., adjusting to content profiles) across
different frames by minimizing a newly derived GSMR loss function. The GSCLO
problem is addressed by an accelerated penalty optimization (APO) algorithm
that reduces computational complexity by over $10$x compared to traditional
branch-and-bound and search algorithms. Moreover, variants of GSCLO are
presented to achieve robust, low-power, and multi-robot GSMR. Extensive
experiments demonstrate that the proposed GSMR paradigm and GSCLO method
achieve significant improvements over existing benchmarks on both wheeled and
legged robots in terms of diverse metrics in various scenarios. For the first
time, it is found that RoboMR can be achieved with ultra-low communication
costs, and mixture of data is useful for enhancing GS performance in dynamic
scenarios.

Comments:
- 14 pages, 18 figures, to appear in IEEE Transactions on Cognitive
  Communications and Networking

---

## GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing  Environments

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-12 | Lin Zeng, Boming Zhao, Jiarui Hu, Xujie Shen, Ziqiang Dang, Hujun Bao, Zhaopeng Cui | cs.CV | [PDF](http://arxiv.org/pdf/2508.08867v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis with neural models has advanced rapidly in recent years,
yet adapting these models to scene changes remains an open problem. Existing
methods are either labor-intensive, requiring extensive model retraining, or
fail to capture detailed types of changes over time. In this paper, we present
GaussianUpdate, a novel approach that combines 3D Gaussian representation with
continual learning to address these challenges. Our method effectively updates
the Gaussian radiance fields with current data while preserving information
from past scenes. Unlike existing methods, GaussianUpdate explicitly models
different types of changes through a novel multi-stage update strategy.
Additionally, we introduce a visibility-aware continual learning approach with
generative replay, enabling self-aware updating without the need to store
images. The experiments on the benchmark dataset demonstrate our method
achieves superior and real-time rendering with the capability of visualizing
changes over different times

Comments:
- Accepted to ICCV 2025

---

## MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based  Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-12 | Yao Lu, Jiawei Li, Ming Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2508.08798v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) have achieved remarkable
progress in dynamic human reconstruction and rendering. Part-based rendering
paradigms, guided by human segmentation, allow for flexible parameter
allocation based on structural complexity, thereby enhancing representational
efficiency. However, existing methods still struggle with complex pose
variations, often producing unnatural transitions at part boundaries and
failing to reconstruct occluded regions accurately in monocular settings. We
propose MonoPartNeRF, a novel framework for monocular dynamic human rendering
that ensures smooth transitions and robust occlusion recovery. First, we build
a bidirectional deformation model that combines rigid and non-rigid
transformations to establish a continuous, reversible mapping between
observation and canonical spaces. Sampling points are projected into a
parameterized surface-time space (u, v, t) to better capture non-rigid motion.
A consistency loss further suppresses deformation-induced artifacts and
discontinuities. We introduce a part-based pose embedding mechanism that
decomposes global pose vectors into local joint embeddings based on body
regions. This is combined with keyframe pose retrieval and interpolation, along
three orthogonal directions, to guide pose-aware feature sampling. A learnable
appearance code is integrated via attention to model dynamic texture changes
effectively. Experiments on the ZJU-MoCap and MonoCap datasets demonstrate that
our method significantly outperforms prior approaches under complex pose and
occlusion conditions, achieving superior joint alignment, texture fidelity, and
structural continuity.



---

## Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy  Prediction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-12 | Cheng Chen, Hao Huang, Saurabh Bagchi | cs.CV | [PDF](http://arxiv.org/pdf/2508.10936v1){: .btn .btn-green } |

**Abstract**: Collaborative perception enables connected vehicles to share information,
overcoming occlusions and extending the limited sensing range inherent in
single-agent (non-collaborative) systems. Existing vision-only methods for 3D
semantic occupancy prediction commonly rely on dense 3D voxels, which incur
high communication costs, or 2D planar features, which require accurate depth
estimation or additional supervision, limiting their applicability to
collaborative scenarios. To address these challenges, we propose the first
approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D
semantic occupancy prediction. By sharing and fusing intermediate Gaussian
primitives, our method provides three benefits: a neighborhood-based
cross-agent fusion that removes duplicates and suppresses noisy or inconsistent
Gaussians; a joint encoding of geometry and semantics in each primitive, which
reduces reliance on depth supervision and allows simple rigid alignment; and
sparse, object-centric messages that preserve structural information while
reducing communication volume. Extensive experiments demonstrate that our
approach outperforms single-agent perception and baseline collaborative methods
by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,
respectively. When further reducing the number of transmitted Gaussians, our
method still achieves a +1.9 improvement in mIoU, using only 34.6%
communication volume, highlighting robust performance under limited
communication budgets.



---

## SAGOnline: Segment Any Gaussians Online

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-11 | Wentao Sun, Quanyun Wu, Hanqing Xu, Kyle Gao, Zhengsen Xu, Yiping Chen, Dedong Zhang, Lingfei Ma, John S. Zelek, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.08219v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit
3D scene representation, yet achieving efficient and consistent 3D segmentation
remains challenging. Current methods suffer from prohibitive computational
costs, limited 3D spatial reasoning, and an inability to track multiple objects
simultaneously. We present Segment Any Gaussians Online (SAGOnline), a
lightweight and zero-shot framework for real-time 3D segmentation in Gaussian
scenes that addresses these limitations through two key innovations: (1) a
decoupled strategy that integrates video foundation models (e.g., SAM2) for
view-consistent 2D mask propagation across synthesized views; and (2) a
GPU-accelerated 3D mask generation and Gaussian-level instance labeling
algorithm that assigns unique identifiers to 3D primitives, enabling lossless
multi-object tracking and segmentation across views. SAGOnline achieves
state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)
benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times
in inference speed (27 ms/frame). Qualitative results demonstrate robust
multi-object segmentation and tracking in complex scenes. Our contributions
include: (i) a lightweight and zero-shot framework for 3D segmentation in
Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling
simultaneous segmentation and tracking, and (iii) the effective adaptation of
2D video foundation models to the 3D domain. This work allows real-time
rendering and 3D scene understanding, paving the way for practical AR/VR and
robotic applications.

Comments:
- 19 pages, 10 figures

---

## ReferSplat: Referring Segmentation in 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-11 | Shuting He, Guangquan Jie, Changshuo Wang, Yun Zhou, Shuming Hu, Guanbin Li, Henghui Ding | cs.CV | [PDF](http://arxiv.org/pdf/2508.08252v1){: .btn .btn-green } |

**Abstract**: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task
that aims to segment target objects in a 3D Gaussian scene based on natural
language descriptions, which often contain spatial relationships or object
attributes. This task requires the model to identify newly described objects
that may be occluded or not directly visible in a novel view, posing a
significant challenge for 3D multi-modal understanding. Developing this
capability is crucial for advancing embodied AI. To support research in this
area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that
3D multi-modal understanding and spatial relationship modeling are key
challenges for R3DGS. To address these challenges, we propose ReferSplat, a
framework that explicitly models 3D Gaussian points with natural language
expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art
performance on both the newly proposed R3DGS task and 3D open-vocabulary
segmentation benchmarks. Dataset and code are available at
https://github.com/heshuting555/ReferSplat.

Comments:
- ICML 2025 Oral, Code: https://github.com/heshuting555/ReferSplat

---

## Touch-Augmented Gaussian Splatting for Enhanced 3D Scene Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-11 | Yuchen Gao, Xiao Xu, Eckehard Steinbach, Daniel E. Lucani, Qi Zhang | eess.SP | [PDF](http://arxiv.org/pdf/2508.07717v1){: .btn .btn-green } |

**Abstract**: This paper presents a multimodal framework that integrates touch signals
(contact points and surface normals) into 3D Gaussian Splatting (3DGS). Our
approach enhances scene reconstruction, particularly under challenging
conditions like low lighting, limited camera viewpoints, and occlusions.
Different from the visual-only method, the proposed approach incorporates
spatially selective touch measurements to refine both the geometry and
appearance of the 3D Gaussian representation. To guide the touch exploration,
we introduce a two-stage sampling scheme that initially probes sparse regions
and then concentrates on high-uncertainty boundaries identified from the
reconstructed mesh. A geometric loss is proposed to ensure surface smoothness,
resulting in improved geometry. Experimental results across diverse scenarios
show consistent improvements in geometric accuracy. In the most challenging
case with severe occlusion, the Chamfer Distance is reduced by over 15x,
demonstrating the effectiveness of integrating touch cues into 3D Gaussian
Splatting. Furthermore, our approach maintains a fully online pipeline,
underscoring its feasibility in visually degraded environments.



---

## Multi-view Normal and Distance Guidance Gaussian Splatting for Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-11 | Bo Jia, Yanan Guo, Ying Chang, Benkui Zhang, Ying Xie, Kangning Du, Lin Cao | cs.CV | [PDF](http://arxiv.org/pdf/2508.07701v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) achieves remarkable results in the field of
surface reconstruction. However, when Gaussian normal vectors are aligned
within the single-view projection plane, while the geometry appears reasonable
in the current view, biases may emerge upon switching to nearby views. To
address the distance and global matching challenges in multi-view scenes, we
design multi-view normal and distance-guided Gaussian splatting. This method
achieves geometric depth unification and high-accuracy reconstruction by
constraining nearby depth maps and aligning 3D normals. Specifically, for the
reconstruction of small indoor and outdoor scenes, we propose a multi-view
distance reprojection regularization module that achieves multi-view Gaussian
alignment by computing the distance loss between two nearby views and the same
Gaussian surface. Additionally, we develop a multi-view normal enhancement
module, which ensures consistency across views by matching the normals of pixel
points in nearby views and calculating the loss. Extensive experimental results
demonstrate that our method outperforms the baseline in both quantitative and
qualitative evaluations, significantly enhancing the surface reconstruction
capability of 3DGS. Our code will be made publicly available at
(https://github.com/Bistu3DV/MND-GS/).

Comments:
- This paper has been accepted by IROS 2025. Code:
  https://github.com/Bistu3DV/MND-GS/

---

## Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and  Spatially Consistent Content Creation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-11 | Minghao Yin, Yukang Cao, Songyou Peng, Kai Han | cs.CV | [PDF](http://arxiv.org/pdf/2508.07557v1){: .btn .btn-green } |

**Abstract**: Generating high-quality 4D content from monocular videos for applications
such as digital humans and AR/VR poses challenges in ensuring temporal and
spatial consistency, preserving intricate details, and incorporating user
guidance effectively. To overcome these challenges, we introduce Splat4D, a
novel framework enabling high-fidelity 4D content generation from a monocular
video. Splat4D achieves superior performance while maintaining faithful
spatial-temporal coherence by leveraging multi-view rendering, inconsistency
identification, a video diffusion model, and an asymmetric U-Net for
refinement. Through extensive evaluations on public benchmarks, Splat4D
consistently demonstrates state-of-the-art performance across various metrics,
underscoring the efficacy of our approach. Additionally, the versatility of
Splat4D is validated in various applications such as text/image conditioned 4D
generation, 4D human generation, and text-guided content editing, producing
coherent outcomes following user instructions.



---

## NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and  Deformable 3D Gaussian Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-11 | Tianle Zeng, Junlei Hu, Gerardo Loza Galindo, Sharib Ali, Duygu Sarikaya, Pietro Valdastri, Dominic Jones | cs.CV | [PDF](http://arxiv.org/pdf/2508.07897v1){: .btn .btn-green } |

**Abstract**: Computer vision-based technologies significantly enhance surgical automation
by advancing tool tracking, detection, and localization. However, Current
data-driven approaches are data-voracious, requiring large, high-quality
labeled image datasets, which limits their application in surgical data
science. Our Work introduces a novel dynamic Gaussian Splatting technique to
address the data scarcity in surgical image datasets. We propose a dynamic
Gaussian model to represent dynamic surgical scenes, enabling the rendering of
surgical instruments from unseen viewpoints and deformations with real tissue
backgrounds. We utilize a dynamic training adjustment strategy to address
challenges posed by poorly calibrated camera poses from real-world scenarios.
Additionally, we propose a method based on dynamic Gaussians for automatically
generating annotations for our synthetic data. For evaluation, we constructed a
new dataset featuring seven scenes with 14,000 frames of tool and camera motion
and tool jaw articulation, with a background of an ex-vivo porcine model. Using
this dataset, we synthetically replicate the scene deformation from the ground
truth data, allowing direct comparisons of synthetic image quality.
Experimental results illustrate that our method generates photo-realistic
labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio
(29.87). We further evaluate the performance of medical-specific neural
networks trained on real and synthetic images using an unseen real-world image
dataset. Our results show that the performance of models trained on synthetic
images generated by the proposed method outperforms those trained with
state-of-the-art standard data augmentation by 10%, leading to an overall
improvement in model performances by nearly 15%.

Comments:
- 13 pages, 9 figures

---

## FantasyStyle: Controllable Stylized Distillation for 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-11 | Yitong Yang, Yinglin Wang, Changshuo Wang, Huajie Wang, Shuting He | cs.CV | [PDF](http://arxiv.org/pdf/2508.08136v1){: .btn .btn-green } |

**Abstract**: The success of 3DGS in generative and editing applications has sparked
growing interest in 3DGS-based style transfer. However, current methods still
face two major challenges: (1) multi-view inconsistency often leads to style
conflicts, resulting in appearance smoothing and distortion; and (2) heavy
reliance on VGG features, which struggle to disentangle style and content from
style images, often causing content leakage and excessive stylization. To
tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style
transfer framework, and the first to rely entirely on diffusion model
distillation. It comprises two key components: (1) \textbf{Multi-View Frequency
Consistency}. We enhance cross-view consistency by applying a 3D filter to
multi-view noisy latent, selectively reducing low-frequency components to
mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized
Distillation}. To suppress content leakage from style images, we introduce
negative guidance to exclude undesired content. In addition, we identify the
limitations of Score Distillation Sampling and Delta Denoising Score in 3D
style transfer and remove the reconstruction term accordingly. Building on
these insights, we propose a controllable stylized distillation that leverages
negative guidance to more effectively optimize the 3D Gaussians. Extensive
experiments demonstrate that our method consistently outperforms
state-of-the-art approaches, achieving higher stylization quality and visual
realism across various scenes and styles.



---

## GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-10 | Qilin Zhang, Olaf Wysocki, Boris Jutzi | cs.CV | [PDF](http://arxiv.org/pdf/2508.07355v1){: .btn .btn-green } |

**Abstract**: Recent advances in Gaussian Splatting (GS) have demonstrated its
effectiveness in photo-realistic rendering and 3D reconstruction. Among these,
2D Gaussian Splatting (2DGS) is particularly suitable for surface
reconstruction due to its flattened Gaussian representation and integrated
normal regularization. However, its performance often degrades in large-scale
and complex urban scenes with frequent occlusions, leading to incomplete
building reconstructions. We propose GS4Buildings, a novel prior-guided
Gaussian Splatting method leveraging the ubiquity of semantic 3D building
models for robust and scalable building surface reconstruction. Instead of
relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings
initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic
3D building models. Moreover, we generate prior depth and normal maps from the
planar building geometry and incorporate them into the optimization process,
providing strong geometric guidance for surface consistency and structural
accuracy. We also introduce an optional building-focused mode that limits
reconstruction to building regions, achieving a 71.8% reduction in Gaussian
primitives and enabling a more efficient and compact representation.
Experiments on urban datasets demonstrate that GS4Buildings improves
reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These
results highlight the potential of semantic building model integration to
advance GS-based reconstruction toward real-world urban applications such as
smart cities and digital twins. Our project is available:
https://github.com/zqlin0521/GS4Buildings.

Comments:
- Accepted for presentation at ISPRS 3D GeoInfo & Smart Data, Smart
  Cities 2025, Kashiwa, Japan. To appear in the ISPRS Annals of the
  Photogrammetry, Remote Sensing and Spatial Information Sciences

---

## CharacterShot: Controllable and Consistent 4D Character Animation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-10 | Junyao Gao, Jiaxing Li, Wenran Liu, Yanhong Zeng, Fei Shen, Kai Chen, Yanan Sun, Cairong Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2508.07409v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose \textbf{CharacterShot}, a controllable and
consistent 4D character animation framework that enables any individual
designer to create dynamic 3D characters (i.e., 4D character animation) from a
single reference character image and a 2D pose sequence. We begin by
pretraining a powerful 2D character animation model based on a cutting-edge
DiT-based image-to-video model, which allows for any 2D pose sequnce as
controllable signal. We then lift the animation model from 2D to 3D through
introducing dual-attention module together with camera prior to generate
multi-view videos with spatial-temporal and spatial-view consistency. Finally,
we employ a novel neighbor-constrained 4D gaussian splatting optimization on
these multi-view videos, resulting in continuous and stable 4D character
representations. Moreover, to improve character-centric performance, we
construct a large-scale dataset Character4D, containing 13,115 unique
characters with diverse appearances and motions, rendered from multiple
viewpoints. Extensive experiments on our newly constructed benchmark,
CharacterBench, demonstrate that our approach outperforms current
state-of-the-art methods. Code, models, and datasets will be publicly available
at https://github.com/Jeoyal/CharacterShot.

Comments:
- 13 pages, 10 figures. Code at https://github.com/Jeoyal/CharacterShot

---

## DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-10 | Rajaei Khatib, Raja Giryes | cs.CV | [PDF](http://arxiv.org/pdf/2508.07372v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method,
obtaining high-quality reconstruction with real-time rendering runtime
performance. The main idea behind 3DGS is to represent the scene as a
collection of 3D gaussians, while learning their parameters to fit the given
views of the scene. While achieving superior performance in the presence of
many views, 3DGS struggles with sparse view reconstruction, where the input
views are sparse and do not fully cover the scene and have low overlaps. In
this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By
using the DIP prior, which utilizes internal structure and patterns, with
coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla
3DGS fails, such as sparse view recovery. Note that our approach does not use
any pre-trained models such as generative models and depth estimation, but
rather relies only on the input frames. Among such methods, DIP-GS obtains
state-of-the-art (SOTA) competitive results on various sparse-view
reconstruction tasks, demonstrating its capabilities.



---

## Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry  Model Accuracy and Resolution

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-10 | Pranav Chougule | cs.CV | [PDF](http://arxiv.org/pdf/2508.07483v1){: .btn .btn-green } |

**Abstract**: In this paper, I present a comprehensive study comparing Photogrammetry and
Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I
created a dataset of images from a real-world scene and constructed 3D models
using both methods. To evaluate the performance, I compared the models using
structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned
perceptual image patch similarity (LPIPS), and lp/mm resolution based on the
USAF resolution chart. A significant contribution of this work is the
development of a modified Gaussian Splatting repository, which I forked and
enhanced to enable rendering images from novel camera poses generated in the
Blender environment. This innovation allows for the synthesis of high-quality
novel views, showcasing the flexibility and potential of Gaussian Splatting. My
investigation extends to an augmented dataset that includes both original
ground images and novel views synthesized via Gaussian Splatting. This
augmented dataset was employed to generate a new photogrammetry model, which
was then compared against the original photogrammetry model created using only
the original images. The results demonstrate the efficacy of using Gaussian
Splatting to generate novel high-quality views and its potential to improve
photogrammetry-based 3D reconstructions. The comparative analysis highlights
the strengths and limitations of both approaches, providing valuable
information for applications in extended reality (XR), photogrammetry, and
autonomous vehicle simulations. Code is available at
https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.



---

## Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS  Watermarking Systems

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-10 | Qingyuan Zeng, Shu Jiang, Jiajing Lin, Zhenzhong Wang, Kay Chen Tan, Min Jiang | cs.CR | [PDF](http://arxiv.org/pdf/2508.07263v1){: .btn .btn-green } |

**Abstract**: With the rise of 3D Gaussian Splatting (3DGS), a variety of digital
watermarking techniques, embedding either 1D bitstreams or 2D images, are used
for copyright protection. However, the robustness of these watermarking
techniques against potential attacks remains underexplored. This paper
introduces the first universal black-box attack framework, the Group-based
Multi-objective Evolutionary Attack (GMEA), designed to challenge these
watermarking systems. We formulate the attack as a large-scale multi-objective
optimization problem, balancing watermark removal with visual quality. In a
black-box setting, we introduce an indirect objective function that blinds the
watermark detector by minimizing the standard deviation of features extracted
by a convolutional network, thus rendering the feature maps uninformative. To
manage the vast search space of 3DGS models, we employ a group-based
optimization strategy to partition the model into multiple, independent
sub-optimization problems. Experiments demonstrate that our framework
effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking
methods while maintaining high visual fidelity. This work reveals critical
vulnerabilities in existing 3DGS copyright protection schemes and calls for the
development of more robust watermarking systems.



---

## 3D Gaussian Representations with Motion Trajectory Field for Dynamic  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-10 | Xuesong Li, Lars Petersson, Vivien Rolland | cs.RO | [PDF](http://arxiv.org/pdf/2508.07182v1){: .btn .btn-green } |

**Abstract**: This paper addresses the challenge of novel-view synthesis and motion
reconstruction of dynamic scenes from monocular video, which is critical for
many robotic applications. Although Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have demonstrated remarkable success in rendering
static scenes, extending them to reconstruct dynamic scenes remains
challenging. In this work, we introduce a novel approach that combines 3DGS
with a motion trajectory field, enabling precise handling of complex object
motions and achieving physically plausible motion trajectories. By decoupling
dynamic objects from static background, our method compactly optimizes the
motion trajectory field. The approach incorporates time-invariant motion
coefficients and shared motion trajectory bases to capture intricate motion
patterns while minimizing optimization complexity. Extensive experiments
demonstrate that our approach achieves state-of-the-art results in both
novel-view synthesis and motion trajectory recovery from monocular video,
advancing the capabilities of dynamic scene reconstruction.



---

## 3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS  Compression

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-09 | Yuke Xing, William Gordon, Qi Yang, Kaifa Yang, Jiarui Wang, Yiling Xu | cs.CV | [PDF](http://arxiv.org/pdf/2508.07038v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high
visual fidelity, but its substantial storage requirements hinder practical
deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate
compression modules. However, these 3DGS generative compression techniques
introduce unique distortions lacking systematic quality assessment research. To
this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment
(VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences
generated from 11 scenes across 6 SOTA 3DGS compression algorithms with
systematically designed parameter levels. With annotations from 50
participants, we obtained MOS scores with outlier removal and validated dataset
reliability. We benchmark 6 3DGS compression algorithms on storage efficiency
and visual quality, and evaluate 15 quality assessment metrics across multiple
paradigms. Our work enables specialized VQA model training for 3DGS, serving as
a catalyst for compression and quality assessment research. The dataset is
available at https://github.com/YukeXing/3DGS-VBench.



---

## DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of  Fruit

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-09 | Aiden Swann, Alex Qiu, Matthew Strong, Angelina Zhang, Samuel Morstein, Kai Rayle, Monroe Kennedy III | cs.RO | [PDF](http://arxiv.org/pdf/2508.07118v1){: .btn .btn-green } |

**Abstract**: DexFruit is a robotic manipulation framework that enables gentle, autonomous
handling of fragile fruit and precise evaluation of damage. Many fruits are
fragile and prone to bruising, thus requiring humans to manually harvest them
with care. In this work, we demonstrate by using optical tactile sensing,
autonomous manipulation of fruit with minimal damage can be achieved. We show
that our tactile informed diffusion policies outperform baselines in both
reduced bruising and pick-and-place success rate across three fruits:
strawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,
a novel technique to represent and quantify visual damage in high-resolution 3D
representation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring
damage lack quantitative rigor or require expensive equipment. With FruitSplat,
we distill a 2D strawberry mask as well as a 2D bruise segmentation mask into
the 3DGS representation. Furthermore, this representation is modular and
general, compatible with any relevant 2D model. Overall, we demonstrate a 92%
grasping policy success rate, up to a 20% reduction in visual bruising, and up
to an 31% improvement in grasp success rate on challenging fruit compared to
our baselines across our three tested fruits. We rigorously evaluate this
result with over 630 trials. Please checkout our website at
https://dex-fruit.github.io .

Comments:
- 8 pages, 5 figures

---

## Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real  Images Beyond 180 Degree Field of View

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-09 | Ulas Gunes, Matias Turkulainen, Juho Kannala, Esa Rahtu | cs.CV | [PDF](http://arxiv.org/pdf/2508.06968v1){: .btn .btn-green } |

**Abstract**: We present the first evaluation of fisheye-based 3D Gaussian Splatting
methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180
degree. Our study covers both indoor and outdoor scenes captured with 200
degree fisheye cameras and analyzes how each method handles extreme distortion
in real world settings. We evaluate performance under varying fields of view
(200 degree, 160 degree, and 120 degree) to study the tradeoff between
peripheral distortion and spatial coverage. Fisheye-GS benefits from field of
view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable
across all settings and maintains high perceptual quality at the full 200
degree view. To address the limitations of SfM-based initialization, which
often fails under strong distortion, we also propose a depth-based strategy
using UniK3D predictions from only 2-3 fisheye images per scene. Although
UniK3D is not trained on real fisheye data, it produces dense point clouds that
enable reconstruction quality on par with SfM, even in difficult scenes with
fog, glare, or sky. Our results highlight the practical viability of
fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and
distortion-heavy image inputs.



---

## EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-09 | Siyu Chen, Shenghai Yuan, Thien-Minh Nguyen, Zhuyu Huang, Chenyang Shi, Jin Jing, Lihua Xie | cs.RO | [PDF](http://arxiv.org/pdf/2508.07003v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over
traditional SLAM methods, enabling photorealistic 3D reconstruction that
conventional approaches often struggle to achieve. However, existing GS-SLAM
systems perform poorly under persistent and severe motion blur commonly
encountered in real-world scenarios, leading to significantly degraded tracking
accuracy and compromised 3D reconstruction quality. To address this limitation,
we propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D
inputs to simultaneously reduce motion blur in images and compensate for the
sparse and discrete nature of event streams, enabling robust tracking and
high-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system
explicitly models the camera's continuous trajectory during exposure,
supporting event- and blur-aware tracking and mapping on a unified 3D Gaussian
Splatting scene. Furthermore, we introduce a learnable camera response function
to align the dynamic ranges of events and images, along with a no-event loss to
suppress ringing artifacts during reconstruction. We validate our approach on a
new dataset comprising synthetic and real-world sequences with significant
motion blur. Extensive experimental results demonstrate that EGS-SLAM
consistently outperforms existing GS-SLAM systems in both trajectory accuracy
and photorealistic 3D Gaussian Splatting reconstruction. The source code will
be available at https://github.com/Chensiyu00/EGS-SLAM.

Comments:
- Accepted by IEEE RAL

---

## Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | YoungChan Choi, HengFei Wang, YiHua Cheng, Boeun Kim, Hyung Jin Chang, YoungGeun Choi, Sang-Il Choi | cs.CV | [PDF](http://arxiv.org/pdf/2508.06136v1){: .btn .btn-green } |

**Abstract**: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

Comments:
- 9 pages, 5 figures, ACM Multimeida 2025 accepted

---

## A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a  Single Image

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | Yanxing Liang, Yinghui Wang, Jinlong Yang, Wei Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.05950v1){: .btn .btn-green } |

**Abstract**: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.



---

## CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | Wenpeng Xing, Jie Chen, Zaifeng Yang, Tiancheng Zhao, Gaolei Li, Changting Lin, Yike Guo, Meng Han | cs.CV | [PDF](http://arxiv.org/pdf/2508.06632v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown impressive performance in novel view
synthesis, but challenges remain in rendering scenes with complex specular
reflections and highlights. Existing approaches may produce blurry reflections
due to entanglement between lighting and material properties, or encounter
optimization instability when relying on physically-based inverse rendering. In
this work, we present a neural rendering framework based on dynamic coefficient
decomposition, aiming to improve the modeling of view-dependent appearance. Our
approach decomposes complex appearance into a shared, static neural basis that
encodes intrinsic material properties, and a set of dynamic coefficients
generated by a Coefficient Network conditioned on view and illumination. A
Dynamic Radiance Integrator then combines these components to synthesize the
final radiance. Experimental results on several challenging benchmarks suggest
that our method can produce sharper and more realistic specular highlights
compared to existing techniques. We hope that this decomposition paradigm can
provide a flexible and effective direction for modeling complex appearance in
neural scene representations.



---

## ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera  Samplings and Diffusion Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | Minsu Kim, Subin Jeon, In Cho, Mijin Yoo, Seon Joo Kim | cs.CV | [PDF](http://arxiv.org/pdf/2508.06014v1){: .btn .btn-green } |

**Abstract**: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

Comments:
- 10 pages, 6 Figures, ICCV 2025

---

## Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach  to Weakly-Supervised Video Anomaly Detection

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | Giacomo D'Amicantonio, Snehashis Majhi, Quan Kong, Lorenzo Garattoni, Gianpiero Francesca, FranÃ§ois Bremond, Egor Bondarev | cs.CV | [PDF](http://arxiv.org/pdf/2508.06318v1){: .btn .btn-green } |

**Abstract**: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.



---

## UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-08 | Wenpeng Xing, Jie Chen, Zaifeng Yang, Changting Lin, Jianfeng Dong, Chaochao Chen, Xun Zhou, Meng Han | cs.CV | [PDF](http://arxiv.org/pdf/2508.06169v1){: .btn .btn-green } |

**Abstract**: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.



---

## UGOD: Uncertainty-Guided Differentiable Opacity and Soft Dropout for  Enhanced Sparse-View 3DGS

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Zhihao Guo, Peng Wang, Zidong Chen, Xiangyu Kong, Yan Lyu, Guanyu Gao, Liangxiu Han | cs.CV | [PDF](http://arxiv.org/pdf/2508.04968v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become a competitive approach for novel view
synthesis (NVS) due to its advanced rendering efficiency through 3D Gaussian
projection and blending. However, Gaussians are treated equally weighted for
rendering in most 3DGS methods, making them prone to overfitting, which is
particularly the case in sparse-view scenarios. To address this, we investigate
how adaptive weighting of Gaussians affects rendering quality, which is
characterised by learned uncertainties proposed. This learned uncertainty
serves two key purposes: first, it guides the differentiable update of Gaussian
opacity while preserving the 3DGS pipeline integrity; second, the uncertainty
undergoes soft differentiable dropout regularisation, which strategically
transforms the original uncertainty into continuous drop probabilities that
govern the final Gaussian projection and blending process for rendering.
Extensive experimental results over widely adopted datasets demonstrate that
our method outperforms rivals in sparse-view 3D synthesis, achieving higher
quality reconstruction with fewer Gaussians in most datasets compared to
existing sparse-view approaches, e.g., compared to DropGaussian, our method
achieves 3.27\% PSNR improvements on the MipNeRF 360 dataset.

Comments:
- 11 pages, 5 figures

---

## CF3: Compact and Fast 3D Feature Fields

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Hyunjoon Lee, Joonkyu Min, Jaesik Park | cs.CV | [PDF](http://arxiv.org/pdf/2508.05254v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2D
foundation models. However, most approaches rely on a bottom-up optimization
process that treats raw 2D features as ground truth, incurring increased
computational costs. We propose a top-down pipeline for constructing compact
and fast 3D Gaussian feature fields, namely, CF3. We first perform a fast
weighted fusion of multi-view 2D features with pre-trained Gaussians. This
approach enables training a per-Gaussian autoencoder directly on the lifted
features, instead of training autoencoders in the 2D domain. As a result, the
autoencoder better aligns with the feature distribution. More importantly, we
introduce an adaptive sparsification method that optimizes the Gaussian
attributes of the feature field while pruning and merging the redundant
Gaussians, constructing an efficient representation with preserved geometric
details. Our approach achieves a competitive 3D feature field using as little
as 5% of the Gaussians compared to Feature-3DGS.

Comments:
- ICCV 2025

---

## Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Zijian Wang, Beizhen Zhao, Hao Wang | cs.GR | [PDF](http://arxiv.org/pdf/2508.04965v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable
capabilities in real-time and photorealistic novel view synthesis. However,
traditional 3DGS representations often struggle with large-scale scene
management and efficient storage, particularly when dealing with complex
environments or limited computational resources. To address these limitations,
we introduce a novel perceive-sample-compress framework for 3D Gaussian
Splatting. Specifically, we propose a scene perception compensation algorithm
that intelligently refines Gaussian parameters at each level. This algorithm
intelligently prioritizes visual importance for higher fidelity rendering in
critical areas, while optimizing resource usage and improving overall visible
quality. Furthermore, we propose a pyramid sampling representation to manage
Gaussian primitives across hierarchical levels. Finally, to facilitate
efficient storage of proposed hierarchical pyramid representations, we develop
a Generalized Gaussian Mixed model compression algorithm to achieve significant
compression ratios without sacrificing visual fidelity. The extensive
experiments demonstrate that our method significantly improves memory
efficiency and high visual quality while maintaining real-time rendering speed.



---

## A Study of the Framework and Real-World Applications of Language  Embedding for 3D Scene Understanding

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Mahmoud Chick Zaouali, Todd Charter, Yehor Karpichev, Brandon Haworth, Homayoun Najjjaran | cs.GR | [PDF](http://arxiv.org/pdf/2508.05064v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting has rapidly emerged as a transformative technique for
real-time 3D scene representation, offering a highly efficient and expressive
alternative to Neural Radiance Fields (NeRF). Its ability to render complex
scenes with high fidelity has enabled progress across domains such as scene
reconstruction, robotics, and interactive content creation. More recently, the
integration of Large Language Models (LLMs) and language embeddings into
Gaussian Splatting pipelines has opened new possibilities for text-conditioned
generation, editing, and semantic scene understanding. Despite these advances,
a comprehensive overview of this emerging intersection has been lacking. This
survey presents a structured review of current research efforts that combine
language guidance with 3D Gaussian Splatting, detailing theoretical
foundations, integration strategies, and real-world use cases. We highlight key
limitations such as computational bottlenecks, generalizability, and the
scarcity of semantically annotated 3D Gaussian data and outline open challenges
and future directions for advancing language-guided 3D scene understanding
using Gaussian Splatting.



---

## Refining Gaussian Splatting: A Volumetric Densification Approach

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Mohamed Abdul Gafoor, Marius Preda, Titus Zaharia | cs.GR | [PDF](http://arxiv.org/pdf/2508.05187v1){: .btn .btn-green } |

**Abstract**: Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS)
often depends on effective point primitive management. The underlying Adaptive
Density Control (ADC) process addresses this issue by automating densification
and pruning. Yet, the vanilla 3DGS densification strategy shows key
shortcomings. To address this issue, in this paper we introduce a novel density
control method, which exploits the volumes of inertia associated to each
Gaussian function to guide the refinement process. Furthermore, we study the
effect of both traditional Structure from Motion (SfM) and Deep Image Matching
(DIM) methods for point cloud initialization. Extensive experimental
evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses
3DGS in reconstruction quality, delivering encouraging performance across
diverse scenes.



---

## Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Yifan Zhou, Beizhen Zhao, Pengcheng Wu, Hao Wang | cs.GR | [PDF](http://arxiv.org/pdf/2508.04966v1){: .btn .btn-green } |

**Abstract**: While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its
extension to dynamic scenes introduces significant challenges. Existing dynamic
3DGS methods suffer from either over-smoothing due to low-rank decomposition or
feature collision from high-dimensional grid sampling. This is because of the
inherent spectral conflicts between preserving motion details and maintaining
deformation consistency at different frequency. To address these challenges, we
propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions.
Our approach contains three key innovations: a spectral-aware Laplacian
encoding architecture which merges Hash encoding and Laplacian-based module for
flexible frequency motion control, an enhanced Gaussian dynamics attribute that
compensates for photometric distortions caused by geometric deformation, and an
adaptive Gaussian split strategy guided by KDTree-based primitive control to
efficiently query and optimize dynamic areas. Through extensive experiments,
our method demonstrates state-of-the-art performance in reconstructing complex
dynamic scenes, achieving better reconstruction fidelity.



---

## Optimization-Free Style Transfer for 3D Gaussian Splats


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Raphael Du Sablon, David Hart | cs.CV | [PDF](http://arxiv.org/pdf/2508.05813v1){: .btn .btn-green } |

**Abstract**: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.



---

## 3DGabSplat: 3D Gabor Splatting for Frequency-adaptive Radiance Field  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Junyu Zhou, Yuyang Huang, Wenrui Dai, Junni Zou, Ziyang Zheng, Nuowen Kan, Chenglin Li, Hongkai Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2508.05343v1){: .btn .btn-green } |

**Abstract**: Recent prominence in 3D Gaussian Splatting (3DGS) has enabled real-time
rendering while maintaining high-fidelity novel view synthesis. However, 3DGS
resorts to the Gaussian function that is low-pass by nature and is restricted
in representing high-frequency details in 3D scenes. Moreover, it causes
redundant primitives with degraded training and rendering efficiency and
excessive memory overhead. To overcome these limitations, we propose 3D Gabor
Splatting (3DGabSplat) that leverages a novel 3D Gabor-based primitive with
multiple directional 3D frequency responses for radiance field representation
supervised by multi-view images. The proposed 3D Gabor-based primitive forms a
filter bank incorporating multiple 3D Gabor kernels at different frequencies to
enhance flexibility and efficiency in capturing fine 3D details. Furthermore,
to achieve novel view rendering, an efficient CUDA-based rasterizer is
developed to project the multiple directional 3D frequency components
characterized by 3D Gabor-based primitives onto the 2D image plane, and a
frequency-adaptive mechanism is presented for adaptive joint optimization of
primitives. 3DGabSplat is scalable to be a plug-and-play kernel for seamless
integration into existing 3DGS paradigms to enhance both efficiency and quality
of novel view synthesis. Extensive experiments demonstrate that 3DGabSplat
outperforms 3DGS and its variants using alternative primitives, and achieves
state-of-the-art rendering quality across both real-world and synthetic scenes.
Remarkably, we achieve up to 1.35 dB PSNR gain over 3DGS with simultaneously
reduced number of primitives and memory consumption.

Comments:
- Accepted by ACM MM'25

---

## MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown  Camera Poses

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Jong-Ik Park, Carlee Joe-Wong, Gary K. Fedder | cs.CV | [PDF](http://arxiv.org/pdf/2508.05819v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.



---

## GAP: Gaussianize Any Point Clouds with Text Guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-07 | Weiqi Zhang, Junsheng Zhou, Haotian Geng, Wenyuan Zhang, Yu-Shen Liu | cs.CV | [PDF](http://arxiv.org/pdf/2508.05631v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving
fast and high-quality rendering. As point clouds serve as a widely-used and
easily accessible form of 3D representation, bridging the gap between point
clouds and Gaussians becomes increasingly important. Recent studies have
explored how to convert the colored points into Gaussians, but directly
generating Gaussians from colorless 3D point clouds remains an unsolved
challenge. In this paper, we propose GAP, a novel approach that gaussianizes
raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key
idea is to design a multi-view optimization framework that leverages a
depth-aware image diffusion model to synthesize consistent appearances across
different viewpoints. To ensure geometric accuracy, we introduce a
surface-anchoring mechanism that effectively constrains Gaussians to lie on the
surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a
diffuse-based inpainting strategy that specifically targets at completing
hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation
task across varying complexity levels, from synthetic point clouds to
challenging real-world scans, and even large-scale scenes. Project Page:
https://weiqi-zhang.github.io/GAP.

Comments:
- ICCV 2025. Project page: https://weiqi-zhang.github.io/GAP

---

## Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned  and Addressed for XR Research

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Ke Li, Mana Masuda, Susanne Schmidt, Shohei Mori | cs.GR | [PDF](http://arxiv.org/pdf/2508.04326v2){: .btn .btn-green } |

**Abstract**: The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)
and Neural Radiance Fields (NeRF), has revolutionized interactive
photorealistic view synthesis and presents enormous opportunities for XR
research and applications. However, despite the exponential growth of RF
research, RF-related contributions to the XR community remain sparse. To better
understand this research gap, we performed a systematic survey of current RF
literature to analyze (i) how RF is envisioned for XR applications, (ii) how
they have already been implemented, and (iii) the remaining research gaps. We
collected 365 RF contributions related to XR from computer vision, computer
graphics, robotics, multimedia, human-computer interaction, and XR communities,
seeking to answer the above research questions. Among the 365 papers, we
performed an analysis of 66 papers that already addressed a detailed aspect of
RF research for XR. With this survey, we extended and positioned XR-specific RF
research topics in the broader RF research field and provide a helpful resource
for the XR community to navigate within the rapid development of RF research.

Comments:
- This work is a pre-print version of a paper that has been accepted to
  the IEEE TVCG journal for future publication

---

## MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Yaopeng Lou, Liao Shen, Tianqi Liu, Jiaqi Li, Zihao Huang, Huiqiang Sun, Zhiguo Cao | cs.CV | [PDF](http://arxiv.org/pdf/2508.04297v1){: .btn .btn-green } |

**Abstract**: We present Multi-Baseline Gaussian Splatting (MuRF), a generalized
feed-forward approach for novel view synthesis that effectively handles diverse
baseline settings, including sparse input views with both small and large
baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and
Monocular Depth Estimation (MDE) to enhance feature representations for
generalizable reconstruction. Next, We propose a projection-and-sampling
mechanism for deep depth fusion, which constructs a fine probability volume to
guide the regression of the feature map. Furthermore, We introduce a
reference-view loss to improve geometry and optimization efficiency. We
leverage 3D Gaussian representations to accelerate training and inference time
while enhancing rendering quality. MuRF achieves state-of-the-art performance
across multiple baseline settings and diverse scenarios ranging from simple
objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also
demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360
datasets.

Comments:
- This work is accepted by ICCV 2025

---

## Bridging Diffusion Models and 3D Representations: A 3D Consistent  Super-Resolution Framework

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Yi-Ting Chen, Ting-Hsuan Liao, Pengsheng Guo, Alexander Schwing, Jia-Bin Huang | cs.CV | [PDF](http://arxiv.org/pdf/2508.04090v1){: .btn .btn-green } |

**Abstract**: We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based
super-resolution framework that leverages off-the-shelf diffusion-based 2D
super-resolution models. 3DSR encourages 3D consistency across views via the
use of an explicit 3D Gaussian-splatting-based scene representation. This makes
the proposed 3DSR different from prior work, such as image upsampling or the
use of video super-resolution, which either don't consider 3D consistency or
aim to incorporate 3D consistency implicitly. Notably, our method enhances
visual quality without additional fine-tuning, ensuring spatial coherence
within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data,
demonstrating that it produces high-resolution results that are visually
compelling, while maintaining structural consistency in 3D reconstructions.
Code will be released.

Comments:
- Accepted to ICCV 2025

---

## SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry  Decomposition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Jiahui Li, Shengeng Tang, Jingxuan He, Gang Huang, Zhangye Wang, Yantao Pan, Lechao Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2508.04224v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic 3D scenes from monocular video remains fundamentally
challenging due to the need to jointly infer motion, structure, and appearance
from limited observations. Existing dynamic scene reconstruction methods based
on Gaussian Splatting often entangle static and dynamic elements in a shared
representation, leading to motion leakage, geometric distortions, and temporal
flickering. We identify that the root cause lies in the coupled modeling of
geometry and appearance across time, which hampers both stability and
interpretability. To address this, we propose \textbf{SplitGaussian}, a novel
framework that explicitly decomposes scene representations into static and
dynamic components. By decoupling motion modeling from background geometry and
allowing only the dynamic branch to deform over time, our method prevents
motion artifacts in static regions while supporting view- and time-dependent
appearance refinement. This disentangled design not only enhances temporal
consistency and reconstruction fidelity but also accelerates convergence.
Extensive experiments demonstrate that SplitGaussian outperforms prior
state-of-the-art methods in rendering quality, geometric stability, and motion
separation.



---

## DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Zexu Huang, Min Xu, Stuart Perry | cs.CV | [PDF](http://arxiv.org/pdf/2508.04099v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) represents a significant advancement in the
field of efficient and high-fidelity novel view synthesis. Despite recent
progress, achieving accurate geometric reconstruction under sparse-view
conditions remains a fundamental challenge. Existing methods often rely on
non-local depth regularization, which fails to capture fine-grained structures
and is highly sensitive to depth estimation noise. Furthermore, traditional
smoothing methods neglect semantic boundaries and indiscriminately degrade
essential edges and textures, consequently limiting the overall quality of
reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware
regularization framework for 3D Gaussian Splatting. DET-GS introduces a
hierarchical geometric depth supervision framework that adaptively enforces
multi-level geometric consistency, significantly enhancing structural fidelity
and robustness against depth estimation noise. To preserve scene boundaries, we
design an edge-aware depth regularization guided by semantic masks derived from
Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving
Total Variation loss that selectively smooths homogeneous regions while
rigorously retaining high-frequency details and textures. Extensive experiments
demonstrate that DET-GS achieves substantial improvements in both geometric
accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on
sparse-view novel view synthesis benchmarks.



---

## CryoGS: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Suyi Chen, Haibin Ling | eess.IV | [PDF](http://arxiv.org/pdf/2508.04929v1){: .btn .btn-green } |

**Abstract**: As a critical modality for structural biology, cryogenic electron microscopy
(cryo-EM) facilitates the determination of macromolecular structures at
near-atomic resolution. The core computational task in single-particle cryo-EM
is to reconstruct the 3D electrostatic potential of a molecule from a large
collection of noisy 2D projections acquired at unknown orientations. Gaussian
mixture models (GMMs) provide a continuous, compact, and physically
interpretable representation for molecular density and have recently gained
interest in cryo-EM reconstruction. However, existing methods rely on external
consensus maps or atomic models for initialization, limiting their use in
self-contained pipelines. Addressing this issue, we introduce cryoGS, a
GMM-based method that integrates Gaussian splatting with the physics of cryo-EM
image formation. In particular, we develop an orthogonal projection-aware
Gaussian splatting, with adaptations such as a normalization term and
FFT-aligned coordinate system tailored for cryo-EM imaging. All these
innovations enable stable and efficient homogeneous reconstruction directly
from raw cryo-EM particle images using random initialization. Experimental
results on real datasets validate the effectiveness and robustness of cryoGS
over representative baselines. The code will be released upon publication.



---

## RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-06 | Zhan Li, Huangying Zhan, Changyang Li, Qingan Yan, Yi Xu | cs.GR | [PDF](http://arxiv.org/pdf/2508.04078v1){: .btn .btn-green } |

**Abstract**: Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive
and expert-driven process, often resulting in inconsistent reconstructions and
suboptimal results. We propose RLGS, a plug-and-play reinforcement learning
framework for adaptive hyperparameter tuning in 3DGS through lightweight policy
modules, dynamically adjusting critical hyperparameters such as learning rates
and densification thresholds. The framework is model-agnostic and seamlessly
integrates into existing 3DGS pipelines without architectural modifications. We
demonstrate its generalization ability across multiple state-of-the-art 3DGS
variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness
across diverse datasets. RLGS consistently enhances rendering quality. For
example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)
dataset, under a fixed Gaussian budget, and continues to yield gains even when
baseline performance saturates. Our results suggest that RLGS provides an
effective and general solution for automating hyperparameter tuning in 3DGS
training, bridging a gap in applying reinforcement learning to 3DGS.

Comments:
- 14 pages, 9 figures

---

## Uni3R: Unified 3D Reconstruction and Semantic Understanding via  Generalizable Gaussian Splatting from Unposed Multi-View Images

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Xiangyu Sun, Haoyi jiang, Liu Liu, Seungtae Nam, Gyeongjin Kang, Xinjie wang, Wei Sui, Zhizhong Su, Wenyu Liu, Xinggang Wang, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2508.03643v2){: .btn .btn-green } |

**Abstract**: Reconstructing and semantically interpreting 3D scenes from sparse 2D views
remains a fundamental challenge in computer vision. Conventional methods often
decouple semantic understanding from reconstruction or necessitate costly
per-scene optimization, thereby restricting their scalability and
generalizability. In this paper, we introduce Uni3R, a novel feed-forward
framework that jointly reconstructs a unified 3D scene representation enriched
with open-vocabulary semantics, directly from unposed multi-view images. Our
approach leverages a Cross-View Transformer to robustly integrate information
across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian
primitives endowed with semantic feature fields. This unified representation
facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic
segmentation, and depth prediction, all within a single, feed-forward pass.
Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art
across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on
ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D
scene reconstruction and understanding. The code is available at
https://github.com/HorizonRobotics/Uni3R.

Comments:
- The code is available at https://github.com/HorizonRobotics/Uni3R

---

## Duplex-GS: Proxy-Guided Weighted Blending for Real-Time  Order-Independent Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Weihang Liu, Yuke Li, Yuxuan Li, Jingyi Yu, Xin Lou | cs.CV | [PDF](http://arxiv.org/pdf/2508.03180v1){: .btn .btn-green } |

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable
rendering fidelity and efficiency. However, these methods still rely on
computationally expensive sequential alpha-blending operations, resulting in
significant overhead, particularly on resource-constrained platforms. In this
paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy
Gaussian representations with order-independent rendering techniques to achieve
photorealistic results while sustaining real-time performance. To mitigate the
overhead caused by view-adaptive radix sort, we introduce cell proxies for
local Gaussians management and propose cell search rasterization for further
acceleration. By seamlessly combining our framework with Order-Independent
Transparency (OIT), we develop a physically inspired weighted sum rendering
technique that simultaneously eliminates "popping" and "transparency"
artifacts, yielding substantial improvements in both accuracy and efficiency.
Extensive experiments on a variety of real-world datasets demonstrate the
robustness of our method across diverse scenarios, including multi-scale
training views and large-scale environments. Our results validate the
advantages of the OIT rendering paradigm in Gaussian Splatting, achieving
high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT
based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix
sort overhead without quality degradation.



---

## RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under  Low-Quality Conditions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Anran Wu, Long Peng, Xin Di, Xueyuan Dai, Chen Wu, Yang Wang, Xueyang Fu, Yang Cao, Zheng-Jun Zha | cs.CV | [PDF](http://arxiv.org/pdf/2508.03077v1){: .btn .btn-green } |

**Abstract**: Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of
optimization-based 3DGS by enabling fast and high-quality reconstruction
without the need for per-scene optimization. However, existing feedforward
approaches typically assume that input multi-view images are clean and
high-quality. In real-world scenarios, images are often captured under
challenging conditions such as noise, low light, or rain, resulting in
inaccurate geometry and degraded 3D reconstruction. To address these
challenges, we propose a general and efficient multi-view feature enhancement
module, RobustGS, which substantially improves the robustness of feedforward
3DGS methods under various adverse imaging conditions, enabling high-quality 3D
reconstruction. The RobustGS module can be seamlessly integrated into existing
pretrained pipelines in a plug-and-play manner to enhance reconstruction
robustness. Specifically, we introduce a novel component, Generalized
Degradation Learner, designed to extract generic representations and
distributions of multiple degradations from multi-view inputs, thereby
enhancing degradation-awareness and improving the overall quality of 3D
reconstruction. In addition, we propose a novel semantic-aware state-space
model. It first leverages the extracted degradation representations to enhance
corrupted inputs in the feature space. Then, it employs a semantic-aware
strategy to aggregate semantically similar information across different views,
enabling the extraction of fine-grained cross-view correspondences and further
improving the quality of 3D representations. Extensive experiments demonstrate
that our approach, when integrated into existing methods in a plug-and-play
manner, consistently achieves state-of-the-art reconstruction quality across
various types of degradations.



---

## H3R: Hybrid Multi-view Correspondence for Generalizable 3D  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Heng Jia, Linchao Zhu, Na Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2508.03118v1){: .btn .btn-green } |

**Abstract**: Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable
3D reconstruction remains challenging, particularly in multi-view
correspondence modeling. Existing approaches face a fundamental trade-off:
explicit methods achieve geometric precision but struggle with ambiguous
regions, while implicit methods provide robustness but suffer from slow
convergence. We present H3R, a hybrid framework that addresses this limitation
by integrating volumetric latent fusion with attention-based feature
aggregation. Our framework consists of two complementary components: an
efficient latent volume that enforces geometric consistency through epipolar
constraints, and a camera-aware Transformer that leverages Pl\"ucker
coordinates for adaptive correspondence refinement. By integrating both
paradigms, our approach enhances generalization while converging 2$\times$
faster than existing methods. Furthermore, we show that spatial-aligned
foundation models (e.g., SD-VAE) substantially outperform semantic-aligned
models (e.g., DINOv2), resolving the mismatch between semantic representations
and spatial reconstruction requirements. Our method supports variable-number
and high-resolution input views while demonstrating robust cross-dataset
generalization. Extensive experiments show that our method achieves
state-of-the-art performance across multiple benchmarks, with significant PSNR
improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and
DTU datasets, respectively. Code is available at
https://github.com/JiaHeng-DLUT/H3R.

Comments:
- ICCV 2025

---

## SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Liheng Zhang, Weihao Yu, Zubo Lu, Haozhi Gu, Jin Huang | cs.CV | [PDF](http://arxiv.org/pdf/2508.03017v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D Gaussian Splatting have enhanced efficient and
high-quality novel view synthesis. However, representing scenes requires a
large number of Gaussian points, leading to high storage demands and limiting
practical deployment. The latest methods facilitate the compression of Gaussian
models but struggle to identify truly insignificant Gaussian points in the
scene, leading to a decline in subsequent Gaussian pruning, compression
quality, and rendering performance. To address this issue, we propose SA-3DGS,
a method that significantly reduces storage costs while maintaining rendering
quality. SA-3DGS learns an importance score to automatically identify the least
significant Gaussians in scene reconstruction, thereby enabling effective
pruning and redundancy reduction. Next, the importance-aware clustering module
compresses Gaussians attributes more accurately into the codebook, improving
the codebook's expressive capability while reducing model size. Finally, the
codebook repair module leverages contextual scene information to repair the
codebook, thereby recovering the original Gaussian point attributes and
mitigating the degradation in rendering quality caused by information loss.
Experimental results on several benchmark datasets show that our method
achieves up to 66x compression while maintaining or even improving rendering
quality. The proposed Gaussian pruning approach is not only adaptable to but
also improves other pruning-based methods (e.g., LightGaussian), showcasing
excellent performance and strong generalization ability.

Comments:
- 9 pages, 7 figures. Under review at AAAI 2026

---

## Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-05 | Hongyu Shen, Junfeng Ni, Yixin Chen, Weishuo Li, Mingtao Pei, Siyuan Huang | cs.CV | [PDF](http://arxiv.org/pdf/2508.03227v1){: .btn .btn-green } |

**Abstract**: We address the challenge of lifting 2D visual segmentation to 3D in Gaussian
Splatting. Existing methods often suffer from inconsistent 2D masks across
viewpoints and produce noisy segmentation boundaries as they neglect these
semantic cues to refine the learned Gaussians. To overcome this, we introduce
Gaussian Instance Tracing (GIT), which augments the standard Gaussian
representation with an instance weight matrix across input views. Leveraging
the inherent consistency of Gaussians in 3D, we use this matrix to identify and
correct 2D segmentation inconsistencies. Furthermore, since each Gaussian
ideally corresponds to a single object, we propose a GIT-guided adaptive
density control mechanism to split and prune ambiguous Gaussians during
training, resulting in sharper and more coherent 2D and 3D segmentation
boundaries. Experimental results show that our method extracts clean 3D assets
and consistently improves 3D segmentation in both online (e.g., self-prompting)
and offline (e.g., contrastive lifting) settings, enabling applications such as
hierarchical segmentation, object extraction, and scene editing.



---

## GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | MikoÅaj ZieliÅski, Krzysztof Byrski, Tomasz Szczepanik, PrzemysÅaw Spurek | cs.CV | [PDF](http://arxiv.org/pdf/2508.02831v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently
transformed 3D scene representation and rendering. NeRF achieves high-fidelity
novel view synthesis by learning volumetric representations through neural
networks, but its implicit encoding makes editing and physical interaction
challenging. In contrast, GS represents scenes as explicit collections of
Gaussian primitives, enabling real-time rendering, faster training, and more
intuitive manipulation. This explicit structure has made GS particularly
well-suited for interactive editing and integration with physics-based
simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural
Radiance Fields Interactive Editing), a hybrid model that combines the
photorealistic rendering quality of NeRF with the editable and structured
representation of GS. Instead of using spherical harmonics for appearance
modeling, we assign each Gaussian a trainable feature embedding. These
embeddings are used to condition a NeRF network based on the k nearest
Gaussians to each query point. To make this conditioning efficient, we
introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest
Gaussian search based on a modified ray-tracing pipeline. We also integrate a
multi-resolution hash grid to initialize and update Gaussian features.
Together, these components enable real-time, locality-aware editing: as
Gaussian primitives are repositioned or modified, their interpolated influence
is immediately reflected in the rendered output. By combining the strengths of
implicit and explicit representations, GENIE supports intuitive scene
manipulation, dynamic interaction, and compatibility with physical simulation,
bridging the gap between geometry-based editing and neural rendering. The code
can be found under (https://github.com/MikolajZielinski/genie)



---

## ScrewSplat: An End-to-End Method for Articulated Object Recognition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Seungyeon Kim, Junsu Ha, Young Hun Kim, Yonghyeon Lee, Frank C. Park | cs.RO | [PDF](http://arxiv.org/pdf/2508.02146v1){: .btn .btn-green } |

**Abstract**: Articulated object recognition -- the task of identifying both the geometry
and kinematic joints of objects with movable parts -- is essential for enabling
robots to interact with everyday objects such as doors and laptops. However,
existing approaches often rely on strong assumptions, such as a known number of
articulated parts; require additional inputs, such as depth images; or involve
complex intermediate steps that can introduce potential errors -- limiting
their practicality in real-world settings. In this paper, we introduce
ScrewSplat, a simple end-to-end method that operates solely on RGB
observations. Our approach begins by randomly initializing screw axes, which
are then iteratively optimized to recover the object's underlying kinematic
structure. By integrating with Gaussian Splatting, we simultaneously
reconstruct the 3D geometry and segment the object into rigid, movable parts.
We demonstrate that our method achieves state-of-the-art recognition accuracy
across a diverse set of articulated objects, and further enables zero-shot,
text-guided manipulation using the recovered kinematic model.

Comments:
- 26 pages, 12 figures, Conference on Robot Learning (CoRL) 2025

---

## VDEGaussian: Video Diffusion Enhanced 4D Gaussian Splatting for Dynamic  Urban Scenes Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yuru Xiao, Zihan Lin, Chao Lu, Deming Zhai, Kui Jiang, Wenbo Zhao, Wei Zhang, Junjun Jiang, Huanran Wang, Xianming Liu | cs.CV | [PDF](http://arxiv.org/pdf/2508.02129v1){: .btn .btn-green } |

**Abstract**: Dynamic urban scene modeling is a rapidly evolving area with broad
applications. While current approaches leveraging neural radiance fields or
Gaussian Splatting have achieved fine-grained reconstruction and high-fidelity
novel view synthesis, they still face significant limitations. These often stem
from a dependence on pre-calibrated object tracks or difficulties in accurately
modeling fast-moving objects from undersampled capture, particularly due to
challenges in handling temporal discontinuities. To overcome these issues, we
propose a novel video diffusion-enhanced 4D Gaussian Splatting framework. Our
key insight is to distill robust, temporally consistent priors from a test-time
adapted video diffusion model. To ensure precise pose alignment and effective
integration of this denoised content, we introduce two core innovations: a
joint timestamp optimization strategy that refines interpolated frame poses,
and an uncertainty distillation method that adaptively extracts target content
while preserving well-reconstructed regions. Extensive experiments demonstrate
that our method significantly enhances dynamic modeling, especially for
fast-moving objects, achieving an approximate PSNR gain of 2 dB for novel view
synthesis over baseline approaches.



---

## Low-Frequency First: Eliminating Floating Artifacts in 3D Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Jianchao Wang, Peng Zhou, Cen Li, Rong Quan, Jie Qin | cs.CV | [PDF](http://arxiv.org/pdf/2508.02493v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is a powerful and computationally efficient
representation for 3D reconstruction. Despite its strengths, 3DGS often
produces floating artifacts, which are erroneous structures detached from the
actual geometry and significantly degrade visual fidelity. The underlying
mechanisms causing these artifacts, particularly in low-quality initialization
scenarios, have not been fully explored. In this paper, we investigate the
origins of floating artifacts from a frequency-domain perspective and identify
under-optimized Gaussians as the primary source. Based on our analysis, we
propose \textit{Eliminating-Floating-Artifacts} Gaussian Splatting (EFA-GS),
which selectively expands under-optimized Gaussians to prioritize accurate
low-frequency learning. Additionally, we introduce complementary depth-based
and scale-based strategies to dynamically refine Gaussian expansion,
effectively mitigating detail erosion. Extensive experiments on both synthetic
and real-world datasets demonstrate that EFA-GS substantially reduces floating
artifacts while preserving high-frequency details, achieving an improvement of
1.68 dB in PSNR over baseline method on our RWLQ dataset. Furthermore, we
validate the effectiveness of our approach in downstream 3D editing tasks. We
provide our implementation in https://jcwang-gh.github.io/EFA-GS.

Comments:
- Project Website: https://jcwang-gh.github.io/EFA-GS

---

## SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene  Completion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Rui Qian, Haozhi Cao, Tianchen Deng, Shenghai Yuan, Lihua Xie | cs.CV | [PDF](http://arxiv.org/pdf/2508.02261v1){: .btn .btn-green } |

**Abstract**: Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising
task that aims to infer dense geometric and semantic descriptions of a scene
from a single image. While recent object-centric paradigms significantly
improve efficiency by leveraging flexible 3D Gaussian primitives, they still
rely heavily on a large number of randomly initialized primitives, which
inevitably leads to 1) inefficient primitive initialization and 2) outlier
primitives that introduce erroneous artifacts. In this paper, we propose
SplatSSC, a novel framework that resolves these limitations with a depth-guided
initialization strategy and a principled Gaussian aggregator. Instead of random
initialization, SplatSSC utilizes a dedicated depth branch composed of a
Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image
and depth features to generate a sparse yet representative set of initial
Gaussian primitives. To mitigate noise from outlier primitives, we develop the
Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing
geometric and semantic predictions during the Gaussian-to-voxel splatting
process. Complemented with a specialized Probability Scale Loss, our method
achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming
prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both
latency and memory consumption by more than 9.3%. The code will be released
upon acceptance.



---

## GaussianCross: Cross-modal Self-supervised 3D Representation Learning  via Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau | cs.CV | [PDF](http://arxiv.org/pdf/2508.02172v1){: .btn .btn-green } |

**Abstract**: The significance of informative and robust point representations has been
widely acknowledged for 3D scene understanding. Despite existing
self-supervised pre-training counterparts demonstrating promising performance,
the model collapse and structural information deficiency remain prevalent due
to insufficient point discrimination difficulty, yielding unreliable
expressions and suboptimal performance. In this paper, we present
GaussianCross, a novel cross-modal self-supervised 3D representation learning
architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques
to address current challenges. GaussianCross seamlessly converts
scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian
representation without missing details, enabling stable and generalizable
pre-training. Subsequently, a tri-attribute adaptive distillation splatting
module is incorporated to construct a 3D feature field, facilitating synergetic
feature capturing of appearance, geometry, and semantic cues to maintain
cross-modal consistency. To validate GaussianCross, we perform extensive
evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In
particular, GaussianCross shows a prominent parameter and data efficiency,
achieving superior performance through linear probing (<0.1% parameters) and
limited data training (1% of scenes) compared to state-of-the-art methods.
Furthermore, GaussianCross demonstrates strong generalization capabilities,
improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on
ScanNet200 semantic and instance segmentation tasks, respectively, supporting
the effectiveness of our approach. The code, weights, and visualizations are
publicly available at
\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.

Comments:
- 14 pages, 8 figures, accepted by MM'25

---

## GR-Gaussian: Graph-Based Radiative Gaussian Splatting for Sparse-View CT  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yikuang Yuluo, Yue Ma, Kuan Shen, Tongtong Jin, Wang Liao, Yangpu Ma, Fuquan Wang | eess.IV | [PDF](http://arxiv.org/pdf/2508.02408v2){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for CT
reconstruction. However, existing methods rely on the average gradient
magnitude of points within the view, often leading to severe needle-like
artifacts under sparse-view conditions. To address this challenge, we propose
GR-Gaussian, a graph-based 3D Gaussian Splatting framework that suppresses
needle-like artifacts and improves reconstruction accuracy under sparse-view
conditions. Our framework introduces two key innovations: (1) a Denoised Point
Cloud Initialization Strategy that reduces initialization errors and
accelerates convergence; and (2) a Pixel-Graph-Aware Gradient Strategy that
refines gradient computation using graph-based density differences, improving
splitting accuracy and density representation. Experiments on X-3D and
real-world datasets validate the effectiveness of GR-Gaussian, achieving PSNR
improvements of 0.67 dB and 0.92 dB, and SSIM gains of 0.011 and 0.021. These
results highlight the applicability of GR-Gaussian for accurate CT
reconstruction under challenging sparse-view conditions.

Comments:
- 10

---

## From Photons to Physics: Autonomous Indoor Drones and the Future of  Objective Property Assessment

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Petteri Teikari, Mike Jarrell, Irene Bandera Moreno, Harri Pesola | cs.RO | [PDF](http://arxiv.org/pdf/2508.01965v1){: .btn .btn-green } |

**Abstract**: The convergence of autonomous indoor drones with physics-aware sensing
technologies promises to transform property assessment from subjective visual
inspection to objective, quantitative measurement. This comprehensive review
examines the technical foundations enabling this paradigm shift across four
critical domains: (1) platform architectures optimized for indoor navigation,
where weight constraints drive innovations in heterogeneous computing,
collision-tolerant design, and hierarchical control systems; (2) advanced
sensing modalities that extend perception beyond human vision, including
hyperspectral imaging for material identification, polarimetric sensing for
surface characterization, and computational imaging with metaphotonics enabling
radical miniaturization; (3) intelligent autonomy through active reconstruction
algorithms, where drones equipped with 3D Gaussian Splatting make strategic
decisions about viewpoint selection to maximize information gain within battery
constraints; and (4) integration pathways with existing property workflows,
including Building Information Modeling (BIM) systems and industry standards
like Uniform Appraisal Dataset (UAD) 3.6.

Comments:
- 63 pages, 5 figures

---

## ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant  Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Fangxin Liu, Haomin Li, Bowen Zhu, Zongwu Wang, Zhuoran Song, Habing Guan, Li Jiang | cs.AR | [PDF](http://arxiv.org/pdf/2508.02304v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) offer significant promise for generating
photorealistic images and videos. However, existing mainstream neural rendering
models often fall short in meeting the demands for immediacy and power
efficiency in practical applications. Specifically, these models frequently
exhibit irregular access patterns and substantial computational overhead,
leading to undesirable inference latency and high power consumption.
Computing-in-memory (CIM), an emerging computational paradigm, has the
potential to address these access bottlenecks and reduce the power consumption
associated with model execution.
  To bridge the gap between model performance and real-world scene
requirements, we propose an algorithm-architecture co-design approach,
abbreviated as ASDR, a CIM-based accelerator supporting efficient neural
rendering. At the algorithmic level, we propose two rendering optimization
schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of
different pixels, thus reducing access memory and computational overhead. (2)
Reducing MLP overhead by decoupling and approximating the volume rendering of
color and density. At the architecture level, we design an efficient
ReRAM-based CIM architecture with efficient data mapping and reuse
microarchitecture. Experiments demonstrate that our design can achieve up to
$9.55\times$ and $69.75\times$ speedup over state-of-the-art NeRF accelerators
and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.

Comments:
- Accepted by the 2025 International Conference on Architectural
  Support for Programming Languages and Operating Systems (ASPLOS 2025). The
  paper will be presented at ASPLOS 2026

---

## PMGS: Reconstruction of Projectile Motion across Large Spatiotemporal  Spans via 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Yijun Xu, Jingrui Zhang, Yuhan Chen, Dingwen Wang, Lei Yu, Chu He | cs.CV | [PDF](http://arxiv.org/pdf/2508.02660v1){: .btn .btn-green } |

**Abstract**: Modeling complex rigid motion across large spatiotemporal spans remains an
unresolved challenge in dynamic reconstruction. Existing paradigms are mainly
confined to short-term, small-scale deformation and offer limited consideration
for physical consistency. This study proposes PMGS, focusing on reconstructing
Projectile Motion via 3D Gaussian Splatting. The workflow comprises two stages:
1) Target Modeling: achieving object-centralized reconstruction through dynamic
scene decomposition and an improved point density control; 2) Motion Recovery:
restoring full motion sequences by learning per-frame SE(3) poses. We introduce
an acceleration consistency constraint to bridge Newtonian mechanics and pose
estimation, and design a dynamic simulated annealing strategy that adaptively
schedules learning rates based on motion states. Futhermore, we devise a Kalman
fusion scheme to optimize error accumulation from multi-source observations to
mitigate disturbances. Experiments show PMGS's superior performance in
reconstructing high-speed nonlinear rigid motion compared to mainstream dynamic
methods.



---

## Uncertainty Estimation for Novel Views in Gaussian Splatting from  Primitive-Based Representations of Error and Visibility

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-04 | Thomas Gottwald, Edgar Heinert, Matthias Rottmann | cs.GR | [PDF](http://arxiv.org/pdf/2508.02443v1){: .btn .btn-green } |

**Abstract**: In this work, we present a novel method for uncertainty estimation (UE) in
Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical
applications such as robotics and medicine. Previous methods typically estimate
the variance of Gaussian primitives and use the rendering process to obtain
pixel-wise uncertainties. Our method establishes primitive representations of
error and visibility of trainings views, which carries meaningful uncertainty
information. This representation is obtained by projection of training error
and visibility onto the primitives. Uncertainties of novel views are obtained
by rendering the primitive representations of uncertainty for those novel
views, yielding uncertainty feature maps. To aggregate these uncertainty
feature maps of novel views, we perform a pixel-wise regression on holdout
data. In our experiments, we analyze the different components of our method,
investigating various combinations of uncertainty feature maps and regression
models. Furthermore, we considered the effect of separating splatting into
foreground and background. Our UEs show high correlations to true errors,
outperforming state-of-the-art methods, especially on foreground objects. The
trained regression models show generalization capabilities to new scenes,
allowing uncertainty estimation without the need for holdout data.



---

## AG$^2$aussian: Anchor-Graph Structured Gaussian Splatting for  Instance-Level 3D Scene Understanding and Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Zhaonan Wang, Manyi Li, Changhe Tu | cs.CV | [PDF](http://arxiv.org/pdf/2508.01740v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has witnessed exponential adoption across
diverse applications, driving a critical need for semantic-aware 3D Gaussian
representations to enable scene understanding and editing tasks. Existing
approaches typically attach semantic features to a collection of free Gaussians
and distill the features via differentiable rendering, leading to noisy
segmentation and a messy selection of Gaussians. In this paper, we introduce
AG$^2$aussian, a novel framework that leverages an anchor-graph structure to
organize semantic features and regulate Gaussian primitives. Our anchor-graph
structure not only promotes compact and instance-aware Gaussian distributions,
but also facilitates graph-based propagation, achieving a clean and accurate
instance-level Gaussian selection. Extensive validation across four
applications, i.e. interactive click-based query, open-vocabulary text-driven
query, object removal editing, and physics simulation, demonstrates the
advantages of our approach and its benefits to various applications. The
experiments and ablation studies further evaluate the effectiveness of the key
designs of our approach.



---

## DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Yufeng Chi, Huimin Ma, Kafeng Wang, Jianmin Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.01684v1){: .btn .btn-green } |

**Abstract**: While diffusion models have demonstrated remarkable progress in 2D image
generation and editing, extending these capabilities to 3D editing remains
challenging, particularly in maintaining multi-view consistency. Classical
approaches typically update 3D representations through iterative refinement
based on a single editing view. However, these methods often suffer from slow
convergence and blurry artifacts caused by cross-view inconsistencies. Recent
methods improve efficiency by propagating 2D editing attention features, yet
still exhibit fine-grained inconsistencies and failure modes in complex scenes
due to insufficient constraints. To address this, we propose \textbf{DisCo3D},
a novel framework that distills 3D consistency priors into a 2D editor. Our
method first fine-tunes a 3D generator using multi-view inputs for scene
adaptation, then trains a 2D editor through consistency distillation. The
edited multi-view outputs are finally optimized into 3D representations via
Gaussian Splatting. Experimental results show DisCo3D achieves stable
multi-view consistency and outperforms state-of-the-art methods in editing
quality.

Comments:
- 17 pages, 7 figures

---

## LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for  Autonomous Driving

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-03 | Luqi Cheng, Zhangshuo Qi, Zijie Zhou, Chao Lu, Guangming Xiong | cs.CV | [PDF](http://arxiv.org/pdf/2508.01704v1){: .btn .btn-green } |

**Abstract**: Maps play an important role in autonomous driving systems. The recently
proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit
scene reconstruction results, demonstrating the potential for map construction
in autonomous driving scenarios. However, because of the time and computational
costs involved in generating Gaussian scenes, how to update the map becomes a
significant challenge. In this paper, we propose LT-Gaussian, a map update
method for 3D-GS-based maps. LT-Gaussian consists of three main components:
Multimodal Gaussian Splatting, Structural Change Detection Module, and
Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is
generated using our proposed Multimodal Gaussian Splatting. Subsequently,
during the map update process, we compare the outdated Gaussian map with the
current LiDAR data stream to identify structural changes. Finally, we perform
targeted updates to the Gaussian-map to generate an up-to-date map. We
establish a benchmark for map updating on the nuScenes dataset to
quantitatively evaluate our method. The experimental results show that
LT-Gaussian can effectively and efficiently update the Gaussian-map, handling
common environmental changes in autonomous driving scenarios. Furthermore, by
taking full advantage of information from both new and old scenes, LT-Gaussian
is able to produce higher quality reconstruction results compared to map update
strategies that reconstruct maps from scratch. Our open-source code is
available at https://github.com/ChengLuqi/LT-gaussian.

Comments:
- Accepted by IV 2025

---

## Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D  Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Quankai Gao, Iliyan Georgiev, Tuanfeng Y. Wang, Krishna Kumar Singh, Ulrich Neumann, Jae Shin Yoon | cs.CV | [PDF](http://arxiv.org/pdf/2508.01464v1){: .btn .btn-green } |

**Abstract**: 3D generation has made significant progress, however, it still largely
remains at the object-level. Feedforward 3D scene-level generation has been
rarely explored due to the lack of models capable of scaling-up latent
representation learning on 3D scene-level data. Unlike object-level generative
models, which are trained on well-labeled 3D data in a bounded canonical space,
scene-level generations with 3D scenes represented by 3D Gaussian Splatting
(3DGS) are unbounded and exhibit scale inconsistency across different scenes,
making unified latent representation learning for generative purposes extremely
challenging. In this paper, we introduce Can3Tok, the first 3D scene-level
variational autoencoder (VAE) capable of encoding a large number of Gaussian
primitives into a low-dimensional latent embedding, which effectively captures
both semantic and spatial information of the inputs. Beyond model design, we
propose a general pipeline for 3D scene data processing to address scale
inconsistency issue. We validate our method on the recent scene-level 3D
dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to
novel 3D scenes, while compared methods fail to converge on even a few hundred
scene inputs during training and exhibit zero generalization ability during
inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as
our applications to demonstrate its ability to facilitate downstream generation
tasks.



---

## OCSplats: Observation Completeness Quantification and Label Noise  Separation in 3DGS

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Han Ling, Xian Xu, Yinghui Sun, Quansen Sun | cs.CV | [PDF](http://arxiv.org/pdf/2508.01239v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has become one of the most promising 3D
reconstruction technologies. However, label noise in real-world scenarios-such
as moving objects, non-Lambertian surfaces, and shadows-often leads to
reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods
either fail to separate noise effectively or require scene-specific fine-tuning
of hyperparameters, making them difficult to apply in practice. This paper
re-examines the problem of anti-noise reconstruction from the perspective of
epistemic uncertainty, proposing a novel framework, OCSplats. By combining key
technologies such as hybrid noise assessment and observation-based cognitive
correction, the accuracy of noise classification in areas with cognitive
differences has been significantly improved. Moreover, to address the issue of
varying noise proportions in different scenarios, we have designed a label
noise classification pipeline based on dynamic anchor points. This pipeline
enables OCSplats to be applied simultaneously to scenarios with vastly
different noise proportions without adjusting parameters. Extensive experiments
demonstrate that OCSplats always achieve leading reconstruction performance and
precise label noise classification in scenes of different complexity levels.



---

## OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian  Splatting for Refined Object-Level Understanding


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Dianyi Yang, Xihan Wang, Yu Gao, Shiyang Liu, Bohan Ren, Yufeng Yue, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2508.01150v1){: .btn .btn-green } |

**Abstract**: Recent advancements in 3D scene understanding have made significant strides
in enabling interaction with scenes using open-vocabulary queries, particularly
for VR/AR and robotic applications. Nevertheless, existing methods are hindered
by rigid offline pipelines and the inability to provide precise 3D object-level
understanding given open-ended queries. In this paper, we present
OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that
improves semantic modeling and refines object-level understanding.
OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed
Distance Field to facilitate lossless fusion of semantic features on-the-fly.
Furthermore, we introduce a novel multimodal language-guided approach named
MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D
objects by adaptively adjusting similarity thresholds, achieving an improvement
17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments
demonstrate that our method outperforms existing methods in 3D object
understanding and scene reconstruction quality, as well as showcasing its
effectiveness in language-guided scene interaction. The code is available at
https://young-bit.github.io/opengs-fusion.github.io/ .

Comments:
- IROS2025

---

## No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from  Sparse Views

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-02 | Ranran Huang, Krystian Mikolajczyk | cs.CV | [PDF](http://arxiv.org/pdf/2508.01171v1){: .btn .btn-green } |

**Abstract**: We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from
sparse multi-view images, requiring no ground-truth poses during training or
inference. It employs a shared feature extraction backbone, enabling
simultaneous prediction of 3D Gaussian primitives and camera poses in a
canonical space from unposed inputs within a single feed-forward step.
Alongside the rendering loss based on estimated novel-view poses, a
reprojection loss is integrated to enforce the learning of pixel-aligned
Gaussian primitives for enhanced geometric constraints. This pose-free training
paradigm and efficient one-step feed-forward design make SPFSplat well-suited
for practical applications. Remarkably, despite the absence of pose
supervision, SPFSplat achieves state-of-the-art performance in novel view
synthesis even under significant viewpoint changes and limited image overlap.
It also surpasses recent methods trained with geometry priors in relative pose
estimation. Code and trained models are available on our project page:
https://ranrhuang.github.io/spfsplat/.

Comments:
- Project Page: https://ranrhuang.github.io/spfsplat/

---

## PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Wentao Sun, Hanqing Xu, Quanyun Wu, Dedong Zhang, Yiping Chen, Lingfei Ma, John S. Zelek, Jonathan Li | cs.CV | [PDF](http://arxiv.org/pdf/2508.00259v1){: .btn .btn-green } |

**Abstract**: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

Comments:
- 22 pages, 9 figures

---

## Cooperative Perception: A Resource-Efficient Framework for Multi-Drone  3D Scene Reconstruction Using Federated Diffusion and NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Massoud Pourmandi | cs.AI | [PDF](http://arxiv.org/pdf/2508.00967v1){: .btn .btn-green } |

**Abstract**: The proposal introduces an innovative drone swarm perception system that aims
to solve problems related to computational limitations and low-bandwidth
communication, and real-time scene reconstruction. The framework enables
efficient multi-agent 3D/4D scene synthesis through federated learning of
shared diffusion model and YOLOv12 lightweight semantic extraction and local
NeRF updates while maintaining privacy and scalability. The framework redesigns
generative diffusion models for joint scene reconstruction, and improves
cooperative scene understanding, while adding semantic-aware compression
protocols. The approach can be validated through simulations and potential
real-world deployment on drone testbeds, positioning it as a disruptive
advancement in multi-agent AI for autonomous systems.

Comments:
- 15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS
  2024 template

---

## Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a  Bimanual Robot with Handover and Gaussian Splat Merging


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-08-01 | Tianshuang Qiu, Zehan Ma, Karim El-Refai, Hiya Shah, Chung Min Kim, Justin Kerr, Ken Goldberg | cs.RO | [PDF](http://arxiv.org/pdf/2508.00354v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/


