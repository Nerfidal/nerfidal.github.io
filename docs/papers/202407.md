---
layout: default
title: July 2024
parent: Papers
nav_order: 202407
---

<!---metadata--->


## Motion-Oriented Compositional Neural Radiance Fields for Monocular  Dynamic Human Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Jaehyeok Kim, Dongyoon Wee, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2407.11962v1){: .btn .btn-green } |

**Abstract**: This paper introduces Motion-oriented Compositional Neural Radiance Fields
(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of
monocular human videos via novel non-rigid motion modeling approach. In the
context of dynamic clothed humans, complex cloth dynamics generate non-rigid
motions that are intrinsically distinct from skeletal articulations and
critically important for the rendering quality. The conventional approach
models non-rigid motions as spatial (3D) deviations in addition to skeletal
transformations. However, it is either time-consuming or challenging to achieve
optimal quality due to its high learning complexity without a direct
supervision. To target this problem, we propose a novel approach of modeling
non-rigid motions as radiance residual fields to benefit from more direct color
supervision in the rendering and utilize the rigid radiance fields as a prior
to reduce the complexity of the learning process. Our approach utilizes a
single multiresolution hash encoding (MHE) to concurrently learn the canonical
T-pose representation from rigid skeletal motions and the radiance residual
field for non-rigid motions. Additionally, to further improve both training
efficiency and usability, we extend MoCo-NeRF to support simultaneous training
of multiple subjects within a single framework, thanks to our effective design
for modeling non-rigid motions. This scalability is achieved through the
integration of a global MHE and learnable identity codes in addition to
multiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,
clearly demonstrating state-of-the-art performance in both single- and
multi-subject settings. The code and model will be made publicly available at
the project page: https://stevejaehyeok.github.io/publications/moco-nerf.

Comments:
- Accepted by ECCV2024

---

## I$^2$-SLAM: Inverting Imaging Process for Robust Photorealistic Dense  SLAM


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Gwangtak Bae, Changwoon Choi, Hyeongjun Heo, Sang Min Kim, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2407.11347v1){: .btn .btn-green } |

**Abstract**: We present an inverse image-formation module that can enhance the robustness
of existing visual SLAM pipelines for casually captured scenarios. Casual video
captures often suffer from motion blur and varying appearances, which degrade
the final quality of coherent 3D visual representation. We propose integrating
the physical imaging into the SLAM system, which employs linear HDR radiance
maps to collect measurements. Specifically, individual frames aggregate images
of multiple poses along the camera trajectory to explain prevalent motion blur
in hand-held videos. Additionally, we accommodate per-frame appearance
variation by dedicating explicit variables for image formation steps, namely
white balance, exposure time, and camera response function. Through joint
optimization of additional variables, the SLAM pipeline produces high-quality
images with more accurate trajectories. Extensive experiments demonstrate that
our approach can be incorporated into recent visual SLAM pipelines using
various scene representations, such as neural radiance fields or Gaussian
splatting.

Comments:
- ECCV 2024

---

## IPA-NeRF: Illusory Poisoning Attack Against Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Wenxiang Jiang, Hanwei Zhang, Shuo Zhao, Zhongwen Guo, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2407.11921v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) represents a significant advancement in computer
vision, offering implicit neural network-based scene representation and novel
view synthesis capabilities. Its applications span diverse fields including
robotics, urban mapping, autonomous navigation, virtual reality/augmented
reality, etc., some of which are considered high-risk AI applications. However,
despite its widespread adoption, the robustness and security of NeRF remain
largely unexplored. In this study, we contribute to this area by introducing
the Illusory Poisoning Attack against Neural Radiance Fields (IPA-NeRF). This
attack involves embedding a hidden backdoor view into NeRF, allowing it to
produce predetermined outputs, i.e. illusory, when presented with the specified
backdoor view while maintaining normal performance with standard inputs. Our
attack is specifically designed to deceive users or downstream models at a
particular position while ensuring that any abnormalities in NeRF remain
undetectable from other viewpoints. Experimental results demonstrate the
effectiveness of our Illusory Poisoning Attack, successfully presenting the
desired illusory on the specified viewpoint without impacting other views.
Notably, we achieve this attack by introducing small perturbations solely to
the training set. The code can be found at
https://github.com/jiang-wenxiang/IPA-NeRF.



---

## Gaussian Splatting LK

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Liuyue Xie, Joel Julin, Koichiro Niinuma, Laszlo A. Jeni | cs.CV | [PDF](http://arxiv.org/pdf/2407.11309v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic 3D scenes from 2D images and generating diverse views
over time presents a significant challenge due to the inherent complexity and
temporal dynamics involved. While recent advancements in neural implicit models
and dynamic Gaussian Splatting have shown promise, limitations persist,
particularly in accurately capturing the underlying geometry of highly dynamic
scenes. Some approaches address this by incorporating strong semantic and
geometric priors through diffusion models. However, we explore a different
avenue by investigating the potential of regularizing the native warp field
within the dynamic Gaussian Splatting framework. Our method is grounded on the
key intuition that an accurate warp field should produce continuous space-time
motions. While enforcing the motion constraints on warp fields is non-trivial,
we show that we can exploit knowledge innate to the forward warp field network
to derive an analytical velocity field, then time integrate for scene flows to
effectively constrain both the 2D motion and 3D positions of the Gaussians.
This derived Lucas-Kanade style analytical regularization enables our method to
achieve superior performance in reconstructing highly dynamic scenes, even
under minimal camera movement, extending the boundaries of what existing
dynamic Gaussian Splatting frameworks can achieve.

Comments:
- 15 pages, 10 figures

---

## Ev-GS: Event-based Gaussian splatting for Efficient and Accurate  Radiance Field Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Jingqian Wu, Shuo Zhu, Chutian Wang, Edmund Y. Lam | cs.CV | [PDF](http://arxiv.org/pdf/2407.11343v1){: .btn .btn-green } |

**Abstract**: Computational neuromorphic imaging (CNI) with event cameras offers advantages
such as minimal motion blur and enhanced dynamic range, compared to
conventional frame-based methods. Existing event-based radiance field rendering
methods are built on neural radiance field, which is computationally heavy and
slow in reconstruction speed. Motivated by the two aspects, we introduce Ev-GS,
the first CNI-informed scheme to infer 3D Gaussian splatting from a monocular
event camera, enabling efficient novel view synthesis. Leveraging 3D Gaussians
with pure event-based supervision, Ev-GS overcomes challenges such as the
detection of fast-moving objects and insufficient lighting. Experimental
results show that Ev-GS outperforms the method that takes frame-based signals
as input by rendering realistic views with reduced blurring and improved visual
quality. Moreover, it demonstrates competitive reconstruction quality and
reduced computing occupancy compared to existing methods, which paves the way
to a highly efficient CNI approach for signal processing.



---

## MVG-Splatting: Multi-View Guided Gaussian Splatting with Adaptive  Quantile-Based Geometric Consistency Densification

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Zhuoxiao Li, Shanliang Yao, Yijie Chu, Angel F. Garcia-Fernandez, Yong Yue, Eng Gee Lim, Xiaohui Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2407.11840v1){: .btn .btn-green } |

**Abstract**: In the rapidly evolving field of 3D reconstruction, 3D Gaussian Splatting
(3DGS) and 2D Gaussian Splatting (2DGS) represent significant advancements.
Although 2DGS compresses 3D Gaussian primitives into 2D Gaussian surfels to
effectively enhance mesh extraction quality, this compression can potentially
lead to a decrease in rendering quality. Additionally, unreliable densification
processes and the calculation of depth through the accumulation of opacity can
compromise the detail of mesh extraction. To address this issue, we introduce
MVG-Splatting, a solution guided by Multi-View considerations. Specifically, we
integrate an optimized method for calculating normals, which, combined with
image gradients, helps rectify inconsistencies in the original depth
computations. Additionally, utilizing projection strategies akin to those in
Multi-View Stereo (MVS), we propose an adaptive quantile-based method that
dynamically determines the level of additional densification guided by depth
maps, from coarse to fine detail. Experimental evidence demonstrates that our
method not only resolves the issues of rendering quality degradation caused by
depth discrepancies but also facilitates direct mesh extraction from dense
Gaussian point clouds using the Marching Cubes algorithm. This approach
significantly enhances the overall fidelity and accuracy of the 3D
reconstruction process, ensuring that both the geometric details and visual
quality.

Comments:
- https://mvgsplatting.github.io

---

## DreamCatalyst: Fast and High-Quality 3D Editing via Controlling  Editability and Identity Preservation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Jiwook Kim, Seonho Lee, Jaeyo Shin, Jiho Choi, Hyunjung Shim | cs.CV | [PDF](http://arxiv.org/pdf/2407.11394v1){: .btn .btn-green } |

**Abstract**: Score distillation sampling (SDS) has emerged as an effective framework in
text-driven 3D editing tasks due to its inherent 3D consistency. However,
existing SDS-based 3D editing methods suffer from extensive training time and
lead to low-quality results, primarily because these methods deviate from the
sampling dynamics of diffusion models. In this paper, we propose DreamCatalyst,
a novel framework that interprets SDS-based editing as a diffusion reverse
process. Our objective function considers the sampling dynamics, thereby making
the optimization process of DreamCatalyst an approximation of the diffusion
reverse process in editing tasks. DreamCatalyst aims to reduce training time
and improve editing quality. DreamCatalyst presents two modes: (1) a faster
mode, which edits the NeRF scene in only about 25 minutes, and (2) a
high-quality mode, which produces superior results in less than 70 minutes.
Specifically, our high-quality mode outperforms current state-of-the-art NeRF
editing methods both in terms of speed and quality. See more extensive results
on our project page: https://dream-catalyst.github.io.



---

## Click-Gaussian: Interactive Segmentation to Any 3D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Seokhun Choi, Hyeonseop Song, Jaechul Kim, Taehyeong Kim, Hoseok Do | cs.CV | [PDF](http://arxiv.org/pdf/2407.11793v1){: .btn .btn-green } |

**Abstract**: Interactive segmentation of 3D Gaussians opens a great opportunity for
real-time manipulation of 3D scenes thanks to the real-time rendering
capability of 3D Gaussian Splatting. However, the current methods suffer from
time-consuming post-processing to deal with noisy segmentation output. Also,
they struggle to provide detailed segmentation, which is important for
fine-grained manipulation of 3D scenes. In this study, we propose
Click-Gaussian, which learns distinguishable feature fields of two-level
granularity, facilitating segmentation without time-consuming post-processing.
We delve into challenges stemming from inconsistently learned feature fields
resulting from 2D segmentation obtained independently from a 3D scene. 3D
segmentation accuracy deteriorates when 2D segmentation results across the
views, primary cues for 3D segmentation, are in conflict. To overcome these
issues, we propose Global Feature-guided Learning (GFL). GFL constructs the
clusters of global feature candidates from noisy 2D segments across the views,
which smooths out noises when training the features of 3D Gaussians. Our method
runs in 10 ms per click, 15 to 130 times as fast as the previous methods, while
also significantly improving segmentation accuracy. Our project page is
available at https://seokhunchoi.github.io/Click-Gaussian

Comments:
- Accepted to ECCV 2024. The first two authors contributed equally to
  this work

---

## SlingBAG: Sliding ball adaptive growth algorithm with differentiable  radiation enables super-efficient iterative 3D photoacoustic image  reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li | cs.CV | [PDF](http://arxiv.org/pdf/2407.11781v1){: .btn .btn-green } |

**Abstract**: High-quality 3D photoacoustic imaging (PAI) reconstruction under sparse view
or limited view has long been challenging. Traditional 3D iterative-based
reconstruction methods suffer from both slow speed and high memory consumption.
Recently, in computer graphics, the differentiable rendering has made
significant progress, particularly with the rise of 3D Gaussian Splatting.
Inspired by these, we introduce differentiable radiation into PAI, developing a
novel reconstruction algorithm: the Sliding Ball Adaptive Growth algorithm
(SlingBAG) for 3D PAI, which shows ability in high-quality 3D PAI
reconstruction both under extremely sparse view and limited view.
  We established the point cloud dataset in PAI, and used unique differentiable
rapid radiator based on the spherical decomposition strategy and the randomly
initialized point cloud adaptively optimized according to sparse sensor data.
Each point undergoes updates in 3D coordinates, initial pressure, and
resolution (denoted by the radius of ball). Points undergo adaptive growth
during iterative process, including point destroying, splitting and duplicating
along the gradient of their positions, manifesting the sliding ball effect.
  Finally, our point cloud to voxel grid shader renders the final
reconstruction results. Simulation and in vivo experiments demonstrate that our
SlingBAG reconstruction result's SNR can be more than 40 dB under extremely
sparse view, while the SNR of traditional back-projection algorithm's result is
less than 20 dB. Moreover, the result of SlingBAG's structural similarity to
the ground truth is significantly higher, with an SSIM value of 95.6%.
  Notably, our differentiable rapid radiator can conduct forward PA simulation
in homogeneous, non-viscous media substantially faster than current methods
that numerically simulate the wave propagation, such as k-Wave. The dataset and
all code will be open source.



---

## iHuman: Instant Animatable Digital Humans From Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Pramish Paudel, Anubhav Khanal, Ajad Chhatkuli, Danda Pani Paudel, Jyoti Tandukar | cs.CV | [PDF](http://arxiv.org/pdf/2407.11174v1){: .btn .btn-green } |

**Abstract**: Personalized 3D avatars require an animatable representation of digital
humans. Doing so instantly from monocular videos offers scalability to broad
class of users and wide-scale applications. In this paper, we present a fast,
simple, yet effective method for creating animatable 3D digital humans from
monocular videos. Our method utilizes the efficiency of Gaussian splatting to
model both 3D geometry and appearance. However, we observed that naively
optimizing Gaussian splats results in inaccurate geometry, thereby leading to
poor animations. This work achieves and illustrates the need of accurate 3D
mesh-type modelling of the human body for animatable digitization through
Gaussian splats. This is achieved by developing a novel pipeline that benefits
from three key aspects: (a) implicit modelling of surface's displacements and
the color's spherical harmonics; (b) binding of 3D Gaussians to the respective
triangular faces of the body template; (c) a novel technique to render normals
followed by their auxiliary supervision. Our exhaustive experiments on three
different benchmark datasets demonstrates the state-of-the-art results of our
method, in limited time settings. In fact, our method is faster by an order of
magnitude (in terms of training time) than its closest competitor. At the same
time, we achieve superior rendering and 3D reconstruction performance under the
change of poses.

Comments:
- 15 pages, eccv, 2024

---

## Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High  Quality and Efficient Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Francesco Di Sario, Riccardo Renzulli, Enzo Tartaglione, Marco Grangetto | cs.CV | [PDF](http://arxiv.org/pdf/2407.10389v1){: .btn .btn-green } |

**Abstract**: Since the introduction of NeRFs, considerable attention has been focused on
improving their training and inference times, leading to the development of
Fast-NeRFs models. Despite demonstrating impressive rendering speed and
quality, the rapid convergence of such models poses challenges for further
improving reconstruction quality. Common strategies to improve rendering
quality involves augmenting model parameters or increasing the number of
sampled points. However, these computationally intensive approaches encounter
limitations in achieving significant quality enhancements. This study
introduces a model-agnostic framework inspired by Sparsely-Gated Mixture of
Experts to enhance rendering quality without escalating computational
complexity. Our approach enables specialization in rendering different scene
components by employing a mixture of experts with varying resolutions. We
present a novel gate formulation designed to maximize expert capabilities and
propose a resolution-based routing technique to effectively induce sparsity and
decompose scenes. Our work significantly improves reconstruction quality while
maintaining competitive performance.



---

## Interactive Rendering of Relightable and Animatable Gaussian Avatars


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Youyi Zhan, Tianjia Shao, He Wang, Yin Yang, Kun Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2407.10707v1){: .btn .btn-green } |

**Abstract**: Creating relightable and animatable avatars from multi-view or monocular
videos is a challenging task for digital human creation and virtual reality
applications. Previous methods rely on neural radiance fields or ray tracing,
resulting in slow training and rendering processes. By utilizing Gaussian
Splatting, we propose a simple and efficient method to decouple body materials
and lighting from sparse-view or monocular avatar videos, so that the avatar
can be rendered simultaneously under novel viewpoints, poses, and lightings at
interactive frame rates (6.9 fps). Specifically, we first obtain the canonical
body mesh using a signed distance function and assign attributes to each mesh
vertex. The Gaussians in the canonical space then interpolate from nearby body
mesh vertices to obtain the attributes. We subsequently deform the Gaussians to
the posed space using forward skinning, and combine the learnable environment
light with the Gaussian attributes for shading computation. To achieve fast
shadow modeling, we rasterize the posed body mesh from dense viewpoints to
obtain the visibility. Our approach is not only simple but also fast enough to
allow interactive rendering of avatar animation under environmental light
changes. Experiments demonstrate that, compared to previous works, our method
can render higher quality results at a faster speed on both synthetic and real
datasets.



---

## IE-NeRF: Inpainting Enhanced Neural Radiance Fields in the Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Shuaixian Wang, Haoran Xu, Yaokun Li, Jiwei Chen, Guang Tan | cs.CV | [PDF](http://arxiv.org/pdf/2407.10695v1){: .btn .btn-green } |

**Abstract**: We present a novel approach for synthesizing realistic novel views using
Neural Radiance Fields (NeRF) with uncontrolled photos in the wild. While NeRF
has shown impressive results in controlled settings, it struggles with
transient objects commonly found in dynamic and time-varying scenes. Our
framework called \textit{Inpainting Enhanced NeRF}, or \ours, enhances the
conventional NeRF by drawing inspiration from the technique of image
inpainting. Specifically, our approach extends the Multi-Layer Perceptrons
(MLP) of NeRF, enabling it to simultaneously generate intrinsic properties
(static color, density) and extrinsic transient masks. We introduce an
inpainting module that leverages the transient masks to effectively exclude
occlusions, resulting in improved volume rendering quality. Additionally, we
propose a new training strategy with frequency regularization to address the
sparsity issue of low-frequency transient components. We evaluate our approach
on internet photo collections of landmarks, demonstrating its ability to
generate high-quality novel views and achieve state-of-the-art performance.



---

## Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using  Datagraphs


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | W. J. Meijer, A. C. Kemmeren, E. H. J. Riemens, J. E. Fransman, M. van Bekkum, G. J. Burghouts, J. D. van Mil | cs.RO | [PDF](http://arxiv.org/pdf/2407.10743v1){: .btn .btn-green } |

**Abstract**: This paper addresses the challenge of scaling Large Multimodal Models (LMMs)
to expansive 3D environments. Solving this open problem is especially relevant
for robot deployment in many first-responder scenarios, such as
search-and-rescue missions that cover vast spaces. The use of LMMs in these
settings is currently hampered by the strict context windows that limit the
LMM's input size. We therefore introduce a novel approach that utilizes a
datagraph structure, which allows the LMM to iteratively query smaller sections
of a large environment. Using the datagraph in conjunction with graph traversal
algorithms, we can prioritize the most relevant locations to the query, thereby
improving the scalability of 3D scene language tasks. We illustrate the
datagraph using 3D scenes, but these can be easily substituted by other dense
modalities that represent the environment, such as pointclouds or Gaussian
splats. We demonstrate the potential to use the datagraph for two 3D scene
language task use cases, in a search-and-rescue mission example.

Comments:
- Accepted to the RSS Workshop on Semantics for Robotics: From
  Environment Understanding and Reasoning to Safe Interaction 2024

---

## Evaluating geometric accuracy of NeRF reconstructions compared to SLAM  method

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Adam Korycki, Colleen Josephson, Steve McGuire | cs.CV | [PDF](http://arxiv.org/pdf/2407.11238v1){: .btn .btn-green } |

**Abstract**: As Neural Radiance Field (NeRF) implementations become faster, more efficient
and accurate, their applicability to real world mapping tasks becomes more
accessible. Traditionally, 3D mapping, or scene reconstruction, has relied on
expensive LiDAR sensing. Photogrammetry can perform image-based 3D
reconstruction but is computationally expensive and requires extremely dense
image representation to recover complex geometry and photorealism. NeRFs
perform 3D scene reconstruction by training a neural network on sparse image
and pose data, achieving superior results to photogrammetry with less input
data. This paper presents an evaluation of two NeRF scene reconstructions for
the purpose of estimating the diameter of a vertical PVC cylinder. One of these
are trained on commodity iPhone data and the other is trained on robot-sourced
imagery and poses. This neural-geometry is compared to state-of-the-art
lidar-inertial SLAM in terms of scene noise and metric-accuracy.



---

## Domain Generalization for 6D Pose Estimation Through NeRF-based Image  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer | cs.CV | [PDF](http://arxiv.org/pdf/2407.10762v1){: .btn .btn-green } |

**Abstract**: This work introduces a novel augmentation method that increases the diversity
of a train set to improve the generalization abilities of a 6D pose estimation
network. For this purpose, a Neural Radiance Field is trained from synthetic
images and exploited to generate an augmented set. Our method enriches the
initial set by enabling the synthesis of images with (i) unseen viewpoints,
(ii) rich illumination conditions through appearance extrapolation, and (iii)
randomized textures. We validate our augmentation method on the challenging
use-case of spacecraft pose estimation and show that it significantly improves
the pose estimation generalization capabilities. On the SPEED+ dataset, our
method reduces the error on the pose by 50% on both target domains.



---

## AirNeRF: 3D Reconstruction of Human with Drone and NeRF for Future  Communication Systems

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Alexey Kotcov, Maria Dronova, Vladislav Cheremnykh, Sausar Karaf, Dzmitry Tsetserukou | cs.RO | [PDF](http://arxiv.org/pdf/2407.10865v1){: .btn .btn-green } |

**Abstract**: In the rapidly evolving landscape of digital content creation, the demand for
fast, convenient, and autonomous methods of crafting detailed 3D
reconstructions of humans has grown significantly. Addressing this pressing
need, our AirNeRF system presents an innovative pathway to the creation of a
realistic 3D human avatar. Our approach leverages Neural Radiance Fields (NeRF)
with an automated drone-based video capturing method. The acquired data
provides a swift and precise way to create high-quality human body
reconstructions following several stages of our system. The rigged mesh derived
from our system proves to be an excellent foundation for free-view synthesis of
dynamic humans, particularly well-suited for the immersive experiences within
gaming and virtual reality.



---

## NGP-RT: Fusing Multi-Level Hash Features with Lightweight Attention for  Real-Time Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Yubin Hu, Xiaoyang Guo, Yang Xiao, Jingwei Huang, Yong-Jin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.10482v1){: .btn .btn-green } |

**Abstract**: This paper presents NGP-RT, a novel approach for enhancing the rendering
speed of Instant-NGP to achieve real-time novel view synthesis. As a classic
NeRF-based method, Instant-NGP stores implicit features in multi-level grids or
hash tables and applies a shallow MLP to convert the implicit features into
explicit colors and densities. Although it achieves fast training speed, there
is still a lot of room for improvement in its rendering speed due to the
per-point MLP executions for implicit multi-level feature aggregation,
especially for real-time applications. To address this challenge, our proposed
NGP-RT explicitly stores colors and densities as hash features, and leverages a
lightweight attention mechanism to disambiguate the hash collisions instead of
using computationally intensive MLP. At the rendering stage, NGP-RT
incorporates a pre-computed occupancy distance grid into the ray marching
strategy to inform the distance to the nearest occupied voxel, thereby reducing
the number of marching points and global memory access. Experimental results
show that on the challenging Mip-NeRF360 dataset, NGP-RT achieves better
rendering quality than previous NeRF-based methods, achieving 108 fps at 1080p
resolution on a single Nvidia RTX 3090 GPU. Our approach is promising for
NeRF-based real-time applications that require efficient and high-quality
rendering.

Comments:
- ECCV 2024

---

## RecGS: Removing Water Caustic with Recurrent Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-14 | Tianyi Zhang, Weiming Zhi, Kaining Huang, Joshua Mangelson, Corina Barbalata, Matthew Johnson-Roberson | cs.CV | [PDF](http://arxiv.org/pdf/2407.10318v1){: .btn .btn-green } |

**Abstract**: Water caustics are commonly observed in seafloor imaging data from
shallow-water areas. Traditional methods that remove caustic patterns from
images often rely on 2D filtering or pre-training on an annotated dataset,
hindering the performance when generalizing to real-world seafloor data with 3D
structures. In this paper, we present a novel method Recurrent Gaussian
Splatting, which takes advantage of today's photorealistic 3D reconstruction
technology, 3DGS, to separate caustics from seafloor imagery. With a sequence
of images taken by an underwater robot, we build 3DGS recursively and decompose
the caustic with low-pass filtering in each iteration. In the experiments, we
analyze and compare with different methods, including joint optimization, 2D
filtering, and deep learning approaches. The results show that our method can
effectively separate the caustic from the seafloor, improving the visual
appearance.

Comments:
- 8 pages, 9 figures

---

## SpikeGS: 3D Gaussian Splatting from Spike Streams with High-Speed Camera  Motion

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-14 | Jiyuan Zhang, Kang Chen, Shiyan Chen, Yajing Zheng, Tiejun Huang, Zhaofei Yu | cs.CV | [PDF](http://arxiv.org/pdf/2407.10062v1){: .btn .btn-green } |

**Abstract**: Novel View Synthesis plays a crucial role by generating new 2D renderings
from multi-view images of 3D scenes. However, capturing high-speed scenes with
conventional cameras often leads to motion blur, hindering the effectiveness of
3D reconstruction. To address this challenge, high-frame-rate dense 3D
reconstruction emerges as a vital technique, enabling detailed and accurate
modeling of real-world objects or scenes in various fields, including Virtual
Reality or embodied AI. Spike cameras, a novel type of neuromorphic sensor,
continuously record scenes with an ultra-high temporal resolution, showing
potential for accurate 3D reconstruction. Despite their promise, existing
approaches, such as applying Neural Radiance Fields (NeRF) to spike cameras,
encounter challenges due to the time-consuming rendering process. To address
this issue, we make the first attempt to introduce the 3D Gaussian Splatting
(3DGS) into spike cameras in high-speed capture, providing 3DGS as dense and
continuous clues of views, then constructing SpikeGS. Specifically, to train
SpikeGS, we establish computational equations between the rendering process of
3DGS and the processes of instantaneous imaging and exposing-like imaging of
the continuous spike stream. Besides, we build a very lightweight but effective
mapping process from spikes to instant images to support training. Furthermore,
we introduced a new spike-based 3D rendering dataset for validation. Extensive
experiments have demonstrated our method possesses the high quality of novel
view rendering, proving the tremendous potential of spike cameras in modeling
3D scenes.



---

## RS-NeRF: Neural Radiance Fields from Rolling Shutter Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-14 | Muyao Niu, Tong Chen, Yifan Zhan, Zhuoxiao Li, Xiang Ji, Yinqiang Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2407.10267v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become increasingly popular because of
their impressive ability for novel view synthesis. However, their effectiveness
is hindered by the Rolling Shutter (RS) effects commonly found in most camera
systems. To solve this, we present RS-NeRF, a method designed to synthesize
normal images from novel views using input with RS distortions. This involves a
physical model that replicates the image formation process under RS conditions
and jointly optimizes NeRF parameters and camera extrinsic for each image row.
We further address the inherent shortcomings of the basic RS-NeRF model by
delving into the RS characteristics and developing algorithms to enhance its
functionality. First, we impose a smoothness regularization to better estimate
trajectories and improve the synthesis quality, in line with the camera
movement prior. We also identify and address a fundamental flaw in the vanilla
RS model by introducing a multi-sampling algorithm. This new approach improves
the model's performance by comprehensively exploiting the RGB data across
different rows for each intermediate camera pose. Through rigorous
experimentation, we demonstrate that RS-NeRF surpasses previous methods in both
synthetic and real-world scenarios, proving its ability to correct RS-related
distortions effectively. Codes and data available:
https://github.com/MyNiuuu/RS-NeRF

Comments:
- ECCV 2024 ; Codes and data: https://github.com/MyNiuuu/RS-NeRF

---

## 3DEgo: 3D Editing on the Go!

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-14 | Umar Khalid, Hasan Iqbal, Azib Farooq, Jing Hua, Chen Chen | cs.CV | [PDF](http://arxiv.org/pdf/2407.10102v1){: .btn .btn-green } |

**Abstract**: We introduce 3DEgo to address a novel problem of directly synthesizing
photorealistic 3D scenes from monocular videos guided by textual prompts.
Conventional methods construct a text-conditioned 3D scene through a
three-stage process, involving pose estimation using Structure-from-Motion
(SfM) libraries like COLMAP, initializing the 3D model with unedited images,
and iteratively updating the dataset with edited images to achieve a 3D scene
with text fidelity. Our framework streamlines the conventional multi-stage 3D
editing process into a single-stage workflow by overcoming the reliance on
COLMAP and eliminating the cost of model initialization. We apply a diffusion
model to edit video frames prior to 3D scene creation by incorporating our
designed noise blender module for enhancing multi-view editing consistency, a
step that does not require additional training or fine-tuning of T2I diffusion
models. 3DEgo utilizes 3D Gaussian Splatting to create 3D scenes from the
multi-view consistent edited frames, capitalizing on the inherent temporal
continuity and explicit point cloud data. 3DEgo demonstrates remarkable editing
precision, speed, and adaptability across a variety of video sources, as
validated by extensive evaluations on six datasets, including our own prepared
GS25 dataset. Project Page: https://3dego.github.io/

Comments:
- ECCV 2024 Accepted Paper

---

## Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-13 | Zhentao Huang, Minglun Gong | cs.CV | [PDF](http://arxiv.org/pdf/2407.09733v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Textured-GS, an innovative method for rendering
Gaussian splatting that incorporates spatially defined color and opacity
variations using Spherical Harmonics (SH). This approach enables each Gaussian
to exhibit a richer representation by accommodating varying colors and
opacities across its surface, significantly enhancing rendering quality
compared to traditional methods. To demonstrate the merits of our approach, we
have adapted the Mini-Splatting architecture to integrate textured Gaussians
without increasing the number of Gaussians. Our experiments across multiple
real-world datasets show that Textured-GS consistently outperforms both the
baseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The
results highlight the potential of Textured-GS to advance Gaussian-based
rendering technologies, promising more efficient and high-quality scene
reconstructions.

Comments:
- 9 pages

---

## Radiance Fields from Photons

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-12 | Sacha Jungerman, Mohit Gupta | cs.CV | [PDF](http://arxiv.org/pdf/2407.09386v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields, or NeRFs, have become the de facto approach for
high-quality view synthesis from a collection of images captured from multiple
viewpoints. However, many issues remain when capturing images in-the-wild under
challenging conditions, such as low light, high dynamic range, or rapid motion
leading to smeared reconstructions with noticeable artifacts. In this work, we
introduce quanta radiance fields, a novel class of neural radiance fields that
are trained at the granularity of individual photons using single-photon
cameras (SPCs). We develop theory and practical computational techniques for
building radiance fields and estimating dense camera poses from unconventional,
stochastic, and high-speed binary frame sequences captured by SPCs. We
demonstrate, both via simulations and a SPC hardware prototype, high-fidelity
reconstructions under high-speed motion, in low light, and for extreme dynamic
range settings.



---

## HPC: Hierarchical Progressive Coding Framework for Volumetric Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-12 | Zihan Zheng, Houqiang Zhong, Qiang Hu, Xiaoyun Zhang, Li Song, Ya Zhang, Yanfeng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2407.09026v1){: .btn .btn-green } |

**Abstract**: Volumetric video based on Neural Radiance Field (NeRF) holds vast potential
for various 3D applications, but its substantial data volume poses significant
challenges for compression and transmission. Current NeRF compression lacks the
flexibility to adjust video quality and bitrate within a single model for
various network and device capacities. To address these issues, we propose HPC,
a novel hierarchical progressive volumetric video coding framework achieving
variable bitrate using a single model. Specifically, HPC introduces a
hierarchical representation with a multi-resolution residual radiance field to
reduce temporal redundancy in long-duration sequences while simultaneously
generating various levels of detail. Then, we propose an end-to-end progressive
learning approach with a multi-rate-distortion loss function to jointly
optimize both hierarchical representation and compression. Our HPC trained only
once can realize multiple compression levels, while the current methods need to
train multiple fixed-bitrate models for different rate-distortion (RD)
tradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality
levels with variable bitrate by a single model and exhibits competitive RD
performance, even outperforming fixed-bitrate models across various datasets.

Comments:
- 11 pages, 7 figures

---

## StyleSplat: 3D Object Style Transfer with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-12 | Sahil Jain, Avik Kuthiala, Prabhdeep Singh Sethi, Prakanshul Saxena | cs.CV | [PDF](http://arxiv.org/pdf/2407.09473v1){: .btn .btn-green } |

**Abstract**: Recent advancements in radiance fields have opened new avenues for creating
high-quality 3D assets and scenes. Style transfer can enhance these 3D assets
with diverse artistic styles, transforming creative expression. However,
existing techniques are often slow or unable to localize style transfer to
specific objects. We introduce StyleSplat, a lightweight method for stylizing
3D objects in scenes represented by 3D Gaussians from reference style images.
Our approach first learns a photorealistic representation of the scene using 3D
Gaussian splatting while jointly segmenting individual 3D objects. We then use
a nearest-neighbor feature matching loss to finetune the Gaussians of the
selected objects, aligning their spherical harmonic coefficients with the style
image to ensure consistency and visual appeal. StyleSplat allows for quick,
customizable style transfer and localized stylization of multiple objects
within a scene, each with a different style. We demonstrate its effectiveness
across various 3D scenes and styles, showcasing enhanced control and
customization in 3D creation.

Comments:
- for code and results, see http://bernard0047.github.io/stylesplat

---

## Physics-Informed Learning of Characteristic Trajectories for Smoke  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-12 | Yiming Wang, Siyu Tang, Mengyu Chu | cs.CV | [PDF](http://arxiv.org/pdf/2407.09679v1){: .btn .btn-green } |

**Abstract**: We delve into the physics-informed neural reconstruction of smoke and
obstacles through sparse-view RGB videos, tackling challenges arising from
limited observation of complex dynamics. Existing physics-informed neural
networks often emphasize short-term physics constraints, leaving the proper
preservation of long-term conservation less explored. We introduce Neural
Characteristic Trajectory Fields, a novel representation utilizing Eulerian
neural fields to implicitly model Lagrangian fluid trajectories. This
topology-free, auto-differentiable representation facilitates efficient flow
map calculations between arbitrary frames as well as efficient velocity
extraction via auto-differentiation. Consequently, it enables end-to-end
supervision covering long-term conservation and short-term physics priors.
Building on the representation, we propose physics-informed trajectory learning
and integration into NeRF-based scene reconstruction. We enable advanced
obstacle handling through self-supervised scene decomposition and seamless
integrated boundary constraints. Our results showcase the ability to overcome
challenges like occlusion uncertainty, density-color ambiguity, and
static-dynamic entanglements. Code and sample tests are at
\url{https://github.com/19reborn/PICT_smoke}.

Comments:
- SIGGRAPH 2024 (conference track), Project Website:
  \url{https://19reborn.github.io/PICT_Smoke.github.io/}

---

## Bayesian uncertainty analysis for underwater 3D reconstruction with  neural radiance fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Haojie Lian, Xinhao Li, Yilin Qu, Jing Du, Zhuxuan Meng, Jie Liu, Leilei Chen | cs.CE | [PDF](http://arxiv.org/pdf/2407.08154v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) are a deep learning technique that can
generate novel views of 3D scenes using sparse 2D images from different viewing
directions and camera poses. As an extension of conventional NeRFs in
underwater environment, where light can get absorbed and scattered by water,
SeaThru-NeRF was proposed to separate the clean appearance and geometric
structure of underwater scene from the effects of the scattering medium. Since
the quality of the appearance and structure of underwater scenes is crucial for
downstream tasks such as underwater infrastructure inspection, the reliability
of the 3D reconstruction model should be considered and evaluated. Nonetheless,
owing to the lack of ability to quantify uncertainty in 3D reconstruction of
underwater scenes under natural ambient illumination, the practical deployment
of NeRFs in unmanned autonomous underwater navigation is limited. To address
this issue, we introduce a spatial perturbation field D_omega based on Bayes'
rays in SeaThru-NeRF and perform Laplace approximation to obtain a Gaussian
distribution N(0,Sigma) of the parameters omega, where the diagonal elements of
Sigma correspond to the uncertainty at each spatial location. We also employ a
simple thresholding method to remove artifacts from the rendered results of
underwater scenes. Numerical experiments are provided to demonstrate the
effectiveness of this approach.



---

## Explicit_NeRF_QA: A Quality Assessment Database for Explicit NeRF Model  Compression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Yuke Xing, Qi Yang, Kaifa Yang, Yilin Xu, Zhu Li | eess.IV | [PDF](http://arxiv.org/pdf/2407.08165v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) have demonstrated significant
advantages in representing and synthesizing 3D scenes. Explicit NeRF models
facilitate the practical NeRF applications with faster rendering speed, and
also attract considerable attention in NeRF compression due to its huge storage
cost. To address the challenge of the NeRF compression study, in this paper, we
construct a new dataset, called Explicit_NeRF_QA. We use 22 3D objects with
diverse geometries, textures, and material complexities to train four typical
explicit NeRF models across five parameter levels. Lossy compression is
introduced during the model generation, pivoting the selection of key
parameters such as hash table size for InstantNGP and voxel grid resolution for
Plenoxels. By rendering NeRF samples to processed video sequences (PVS), a
large scale subjective experiment with lab environment is conducted to collect
subjective scores from 21 viewers. The diversity of content, accuracy of mean
opinion scores (MOS), and characteristics of NeRF distortion are
comprehensively presented, establishing the heterogeneity of the proposed
dataset. The state-of-the-art objective metrics are tested in the new dataset.
Best Person correlation, which is around 0.85, is collected from the
full-reference objective metric. All tested no-reference metrics report very
poor results with 0.4 to 0.6 correlations, demonstrating the need for further
development of more robust no-reference metrics. The dataset, including NeRF
samples, source 3D objects, multiview images for NeRF generation, PVSs, MOS, is
made publicly available at the following location:
https://github.com/LittlericeChloe/Explicit_NeRF_QA.

Comments:
- 5 pages, 4 figures, 2 tables, conference

---

## Survey on Fundamental Deep Learning 3D Reconstruction Techniques

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Yonge Bai, LikHang Wong, TszYin Twan | cs.CV | [PDF](http://arxiv.org/pdf/2407.08137v1){: .btn .btn-green } |

**Abstract**: This survey aims to investigate fundamental deep learning (DL) based 3D
reconstruction techniques that produce photo-realistic 3D models and scenes,
highlighting Neural Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and
3D Gaussian Splatting. We dissect the underlying algorithms, evaluate their
strengths and tradeoffs, and project future research trajectories in this
rapidly evolving field. We provide a comprehensive overview of the fundamental
in DL-driven 3D scene reconstruction, offering insights into their potential
applications and limitations.



---

## Feasibility of Neural Radiance Fields for Crime Scene Video  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Shariq Nadeem Malik, Min Hao Chee, Dayan Mario Anthony Perera, Chern Hong Lim | cs.CV | [PDF](http://arxiv.org/pdf/2407.08795v1){: .btn .btn-green } |

**Abstract**: This paper aims to review and determine the feasibility of using variations
of NeRF models in order to reconstruct crime scenes given input videos of the
scene. We focus on three main innovations of NeRF when it comes to
reconstructing crime scenes: Multi-object Synthesis, Deformable Synthesis, and
Lighting. From there, we analyse its innovation progress against the
requirements to be met in order to be able to reconstruct crime scenes with
given videos of such scenes.

Comments:
- 4 pages, 1 table

---

## WildGaussians: 3D Gaussian Splatting in the Wild

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, Torsten Sattler | cs.CV | [PDF](http://arxiv.org/pdf/2407.08447v1){: .btn .btn-green } |

**Abstract**: While the field of 3D scene reconstruction is dominated by NeRFs due to their
photorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,
offering similar quality with real-time rendering speeds. However, both methods
primarily excel with well-controlled 3D scenes, while in-the-wild data -
characterized by occlusions, dynamic objects, and varying illumination -
remains challenging. NeRFs can adapt to such conditions easily through
per-image embedding vectors, but 3DGS struggles due to its explicit
representation and lack of shared parameters. To address this, we introduce
WildGaussians, a novel approach to handle occlusions and appearance changes
with 3DGS. By leveraging robust DINO features and integrating an appearance
modeling module within 3DGS, our method achieves state-of-the-art results. We
demonstrate that WildGaussians matches the real-time rendering speed of 3DGS
while surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all
within a simple architectural framework.

Comments:
- https://wild-gaussians.github.io/

---

## MeshAvatar: Learning High-quality Triangular Human Avatars from  Multi-view Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Yushuo Chen, Zerong Zheng, Zhe Li, Chao Xu, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.08414v1){: .btn .btn-green } |

**Abstract**: We present a novel pipeline for learning high-quality triangular human
avatars from multi-view videos. Recent methods for avatar learning are
typically based on neural radiance fields (NeRF), which is not compatible with
traditional graphics pipeline and poses great challenges for operations like
editing or synthesizing under different environments. To overcome these
limitations, our method represents the avatar with an explicit triangular mesh
extracted from an implicit SDF field, complemented by an implicit material
field conditioned on given poses. Leveraging this triangular avatar
representation, we incorporate physics-based rendering to accurately decompose
geometry and texture. To enhance both the geometric and appearance details, we
further employ a 2D UNet as the network backbone and introduce pseudo normal
ground-truth as additional supervision. Experiments show that our method can
learn triangular avatars with high-quality geometry reconstruction and
plausible material decomposition, inherently supporting editing, manipulation
or relighting operations.

Comments:
- Project Page: https://shad0wta9.github.io/meshavatar-page/

---

## MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-10 | Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras | cs.CV | [PDF](http://arxiv.org/pdf/2407.07284v1){: .btn .btn-green } |

**Abstract**: We introduce MIGS (Multi-Identity Gaussian Splatting), a novel method that
learns a single neural representation for multiple identities, using only
monocular videos. Recent 3D Gaussian Splatting (3DGS) approaches for human
avatars require per-identity optimization. However, learning a multi-identity
representation presents advantages in robustly animating humans under arbitrary
poses. We propose to construct a high-order tensor that combines all the
learnable 3DGS parameters for all the training identities. By assuming a
low-rank structure and factorizing the tensor, we model the complex rigid and
non-rigid deformations of multiple subjects in a unified network, significantly
reducing the total number of parameters. Our proposed approach leverages
information from all the training identities, enabling robust animation under
challenging unseen poses, outperforming existing approaches. We also
demonstrate how it can be extended to learn unseen identities.

Comments:
- Accepted by ECCV 2024. Project page:
  https://aggelinacha.github.io/MIGS/

---

## Drantal-NeRF: Diffusion-Based Restoration for Anti-aliasing Neural  Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-10 | Ganlin Yang, Kaidong Zhang, Jingjing Fu, Dong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.07461v1){: .btn .btn-green } |

**Abstract**: Aliasing artifacts in renderings produced by Neural Radiance Field (NeRF) is
a long-standing but complex issue in the field of 3D implicit representation,
which arises from a multitude of intricate causes and was mitigated by
designing more advanced but complex scene parameterization methods before. In
this paper, we present a Diffusion-based restoration method for anti-aliasing
Neural Radiance Field (Drantal-NeRF). We consider the anti-aliasing issue from
a low-level restoration perspective by viewing aliasing artifacts as a kind of
degradation model added to clean ground truths. By leveraging the powerful
prior knowledge encapsulated in diffusion model, we could restore the
high-realism anti-aliasing renderings conditioned on aliased low-quality
counterparts. We further employ a feature-wrapping operation to ensure
multi-view restoration consistency and finetune the VAE decoder to better adapt
to the scene-specific data distribution. Our proposed method is easy to
implement and agnostic to various NeRF backbones. We conduct extensive
experiments on challenging large-scale urban scenes as well as unbounded
360-degree scenes and achieve substantial qualitative and quantitative
improvements.



---

## Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-10 | Qi Song, Ziyuan Luo, Ka Chun Cheung, Simon See, Renjie Wan | cs.CV | [PDF](http://arxiv.org/pdf/2407.07735v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become a key method for 3D scene
representation. With the rising prominence and influence of NeRF, safeguarding
its intellectual property has become increasingly important. In this paper, we
propose \textbf{NeRFProtector}, which adopts a plug-and-play strategy to
protect NeRF's copyright during its creation. NeRFProtector utilizes a
pre-trained watermarking base model, enabling NeRF creators to embed binary
messages directly while creating their NeRF. Our plug-and-play property ensures
NeRF creators can flexibly choose NeRF variants without excessive
modifications. Leveraging our newly designed progressive distillation, we
demonstrate performance on par with several leading-edge neural rendering
methods. Our project is available at:
\url{https://qsong2001.github.io/NeRFProtector}.

Comments:
- Accepted by ECCV2024

---

## Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-09 | Dogyoon Lee, Donghyeong Kim, Jungho Lee, Minhyeok Lee, Seunghoon Lee, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2407.06613v1){: .btn .btn-green } |

**Abstract**: Recent studies construct deblurred neural radiance fields (DeRF) using dozens
of blurry images, which are not practical scenarios if only a limited number of
blurry images are available. This paper focuses on constructing DeRF from
sparse-view for more pragmatic real-world scenarios. As observed in our
experiments, establishing DeRF from sparse views proves to be a more
challenging problem due to the inherent complexity arising from the
simultaneous optimization of blur kernels and NeRF from sparse view.
Sparse-DeRF successfully regularizes the complicated joint optimization,
presenting alleviated overfitting artifacts and enhanced quality on radiance
fields. The regularization consists of three key components: Surface
smoothness, helps the model accurately predict the scene structure utilizing
unseen and additional hidden rays derived from the blur kernel based on
statistical tendencies of real-world; Modulated gradient scaling, helps the
model adjust the amount of the backpropagated gradient according to the
arrangements of scene objects; Perceptual distillation improves the perceptual
quality by overcoming the ill-posed multi-view inconsistency of image
deblurring and distilling the pre-filtered information, compensating for the
lack of clean information in blurry images. We demonstrate the effectiveness of
the Sparse-DeRF with extensive quantitative and qualitative experimental
results by training DeRF from 2-view, 4-view, and 6-view blurry images.

Comments:
- Project page: https://dogyoonlee.github.io/sparsederf/

---

## Reference-based Controllable Scene Stylization with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-09 | Yiqun Mei, Jiacong Xu, Vishal M. Patel | cs.CV | [PDF](http://arxiv.org/pdf/2407.07220v1){: .btn .btn-green } |

**Abstract**: Referenced-based scene stylization that edits the appearance based on a
content-aligned reference image is an emerging research area. Starting with a
pretrained neural radiance field (NeRF), existing methods typically learn a
novel appearance that matches the given style. Despite their effectiveness,
they inherently suffer from time-consuming volume rendering, and thus are
impractical for many real-time applications. In this work, we propose ReGS,
which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to
enable real-time stylized view synthesis. Editing the appearance of a
pretrained 3DGS is challenging as it uses discrete Gaussians as 3D
representation, which tightly bind appearance with geometry. Simply optimizing
the appearance as prior methods do is often insufficient for modeling
continuous textures in the given reference image. To address this challenge, we
propose a novel texture-guided control mechanism that adaptively adjusts local
responsible Gaussians to a new geometric arrangement, serving for desired
texture details. The proposed process is guided by texture clues for effective
appearance editing, and regularized by scene depth for preserving original
geometric structure. With these novel designs, we show ReGs can produce
state-of-the-art stylization results that respect the reference texture while
embracing real-time rendering speed for free-view navigation.



---

## 3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-09 | Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, Zan Gojcic | cs.GR | [PDF](http://arxiv.org/pdf/2407.07090v2){: .btn .btn-green } |

**Abstract**: Particle-based representations of radiance fields such as 3D Gaussian
Splatting have found great success for reconstructing and re-rendering of
complex scenes. Most existing methods render particles via rasterization,
projecting them to screen space tiles for processing in a sorted order. This
work instead considers ray tracing the particles, building a bounding volume
hierarchy and casting a ray for each pixel using high-performance GPU ray
tracing hardware. To efficiently handle large numbers of semi-transparent
particles, we describe a specialized rendering algorithm which encapsulates
particles with bounding meshes to leverage fast ray-triangle intersections, and
shades batches of intersections in depth-order. The benefits of ray tracing are
well-known in computer graphics: processing incoherent rays for secondary
lighting effects such as shadows and reflections, rendering from
highly-distorted cameras common in robotics, stochastically sampling rays, and
more. With our renderer, this flexibility comes at little cost compared to
rasterization. Experiments demonstrate the speed and accuracy of our approach,
as well as several applications in computer graphics and vision. We further
propose related improvements to the basic Gaussian representation, including a
simple use of generalized kernel functions which significantly reduces particle
hit counts.

Comments:
- Project page: https://gaussiantracer.github.io/

---

## PanDORA: Casual HDR Radiance Acquisition for Indoor Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Mohammad Reza Karimi Dastjerdi, Frédéric Fortier-Chouinard, Yannick Hold-Geoffroy, Marc Hébert, Claude Demers, Nima Kalantari, Jean-François Lalonde | cs.CV | [PDF](http://arxiv.org/pdf/2407.06150v1){: .btn .btn-green } |

**Abstract**: Most novel view synthesis methods such as NeRF are unable to capture the true
high dynamic range (HDR) radiance of scenes since they are typically trained on
photos captured with standard low dynamic range (LDR) cameras. While the
traditional exposure bracketing approach which captures several images at
different exposures has recently been adapted to the multi-view case, we find
such methods to fall short of capturing the full dynamic range of indoor
scenes, which includes very bright light sources. In this paper, we present
PanDORA: a PANoramic Dual-Observer Radiance Acquisition system for the casual
capture of indoor scenes in high dynamic range. Our proposed system comprises
two 360{\deg} cameras rigidly attached to a portable tripod. The cameras
simultaneously acquire two 360{\deg} videos: one at a regular exposure and the
other at a very fast exposure, allowing a user to simply wave the apparatus
casually around the scene in a matter of minutes. The resulting images are fed
to a NeRF-based algorithm that reconstructs the scene's full high dynamic
range. Compared to HDR baselines from previous work, our approach reconstructs
the full HDR radiance of indoor scenes without sacrificing the visual quality
while retaining the ease of capture from recent NeRF-like approaches.

Comments:
- 10 pages, 8 figures

---

## RRM: Relightable assets using Radiance guided Material extraction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Diego Gomez, Julien Philip, Adrien Kaiser, Élie Michel | cs.CV | [PDF](http://arxiv.org/pdf/2407.06397v1){: .btn .btn-green } |

**Abstract**: Synthesizing NeRFs under arbitrary lighting has become a seminal problem in
the last few years. Recent efforts tackle the problem via the extraction of
physically-based parameters that can then be rendered under arbitrary lighting,
but they are limited in the range of scenes they can handle, usually
mishandling glossy scenes. We propose RRM, a method that can extract the
materials, geometry, and environment lighting of a scene even in the presence
of highly reflective objects. Our method consists of a physically-aware
radiance field representation that informs physically-based parameters, and an
expressive environment light structure based on a Laplacian Pyramid. We
demonstrate that our contributions outperform the state-of-the-art on parameter
retrieval tasks, leading to high-fidelity relighting and novel view synthesis
on surfacic scenes.

Comments:
- Paper accepted and presented at CGI 2024

---

## Dynamic Neural Radiance Field From Defocused Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Xianrui Luo, Huiqiang Sun, Juewen Peng, Zhiguo Cao | cs.CV | [PDF](http://arxiv.org/pdf/2407.05586v1){: .btn .btn-green } |

**Abstract**: Dynamic Neural Radiance Field (NeRF) from monocular videos has recently been
explored for space-time novel view synthesis and achieved excellent results.
However, defocus blur caused by depth variation often occurs in video capture,
compromising the quality of dynamic reconstruction because the lack of sharp
details interferes with modeling temporal consistency between input views. To
tackle this issue, we propose D2RF, the first dynamic NeRF method designed to
restore sharp novel views from defocused monocular videos. We introduce layered
Depth-of-Field (DoF) volume rendering to model the defocus blur and reconstruct
a sharp NeRF supervised by defocused views. The blur model is inspired by the
connection between DoF rendering and volume rendering. The opacity in volume
rendering aligns with the layer visibility in DoF rendering.To execute the
blurring, we modify the layered blur kernel to the ray-based kernel and employ
an optimized sparse kernel to gather the input rays efficiently and render the
optimized rays with our layered DoF volume rendering. We synthesize a dataset
with defocused dynamic scenes for our task, and extensive experiments on our
dataset show that our method outperforms existing approaches in synthesizing
all-in-focus novel views from defocus blur while maintaining spatial-temporal
consistency in the scene.

Comments:
- Accepted by ECCV 2024

---

## Enhancing Neural Radiance Fields with Depth and Normal Completion Priors  from Sparse Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Jiawei Guo, HungChyun Chou, Ning Ding | cs.CV | [PDF](http://arxiv.org/pdf/2407.05666v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) are an advanced technology that creates highly
realistic images by learning about scenes through a neural network model.
However, NeRF often encounters issues when there are not enough images to work
with, leading to problems in accurately rendering views. The main issue is that
NeRF lacks sufficient structural details to guide the rendering process
accurately. To address this, we proposed a Depth and Normal Dense Completion
Priors for NeRF (CP\_NeRF) framework. This framework enhances view rendering by
adding depth and normal dense completion priors to the NeRF optimization
process. Before optimizing NeRF, we obtain sparse depth maps using the
Structure from Motion (SfM) technique used to get camera poses. Based on the
sparse depth maps and a normal estimator, we generate sparse normal maps for
training a normal completion prior with precise standard deviations. During
optimization, we apply depth and normal completion priors to transform sparse
data into dense depth and normal maps with their standard deviations. We use
these dense maps to guide ray sampling, assist distance sampling and construct
a normal loss function for better training accuracy. To improve the rendering
of NeRF's normal outputs, we incorporate an optical centre position embedder
that helps synthesize more accurate normals through volume rendering.
Additionally, we employ a normal patch matching technique to choose accurate
rendered normal maps, ensuring more precise supervision for the model. Our
method is superior to leading techniques in rendering detailed indoor scenes,
even with limited input views.



---

## GeoNLF: Geometry guided Pose-Free Neural LiDAR Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Weiyi Xue, Zehan Zheng, Fan Lu, Haiyun Wei, Guang Chen, Changjun Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2407.05597v1){: .btn .btn-green } |

**Abstract**: Although recent efforts have extended Neural Radiance Fields (NeRF) into
LiDAR point cloud synthesis, the majority of existing works exhibit a strong
dependence on precomputed poses. However, point cloud registration methods
struggle to achieve precise global pose estimation, whereas previous pose-free
NeRFs overlook geometric consistency in global reconstruction. In light of
this, we explore the geometric insights of point clouds, which provide explicit
registration priors for reconstruction. Based on this, we propose Geometry
guided Neural LiDAR Fields(GeoNLF), a hybrid framework performing alternately
global neural reconstruction and pure geometric pose optimization. Furthermore,
NeRFs tend to overfit individual frames and easily get stuck in local minima
under sparse-view inputs. To tackle this issue, we develop a
selective-reweighting strategy and introduce geometric constraints for robust
optimization. Extensive experiments on NuScenes and KITTI-360 datasets
demonstrate the superiority of GeoNLF in both novel view synthesis and
multi-view registration of low-frequency large-scale point clouds.



---

## PICA: Physics-Integrated Clothed Avatar

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-07 | Bo Peng, Yunfan Tao, Haoyu Zhan, Yudong Guo, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2407.05324v1){: .btn .btn-green } |

**Abstract**: We introduce PICA, a novel representation for high-fidelity animatable
clothed human avatars with physics-accurate dynamics, even for loose clothing.
Previous neural rendering-based representations of animatable clothed humans
typically employ a single model to represent both the clothing and the
underlying body. While efficient, these approaches often fail to accurately
represent complex garment dynamics, leading to incorrect deformations and
noticeable rendering artifacts, especially for sliding or loose garments.
Furthermore, previous works represent garment dynamics as pose-dependent
deformations and facilitate novel pose animations in a data-driven manner. This
often results in outcomes that do not faithfully represent the mechanics of
motion and are prone to generating artifacts in out-of-distribution poses. To
address these issues, we adopt two individual 3D Gaussian Splatting (3DGS)
models with different deformation characteristics, modeling the human body and
clothing separately. This distinction allows for better handling of their
respective motion characteristics. With this representation, we integrate a
graph neural network (GNN)-based clothed body physics simulation module to
ensure an accurate representation of clothing dynamics. Our method, through its
carefully designed features, achieves high-fidelity rendering of clothed human
bodies in complex and novel driving poses, significantly outperforming previous
methods under the same settings.

Comments:
- Project page: https://ustc3dv.github.io/PICA/

---

## GaussReg: Fast 3D Registration with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-07 | Jiahao Chang, Yinglin Xu, Yihao Li, Yuantao Chen, Xiaoguang Han | cs.CV | [PDF](http://arxiv.org/pdf/2407.05254v1){: .btn .btn-green } |

**Abstract**: Point cloud registration is a fundamental problem for large-scale 3D scene
scanning and reconstruction. With the help of deep learning, registration
methods have evolved significantly, reaching a nearly-mature stage. As the
introduction of Neural Radiance Fields (NeRF), it has become the most popular
3D scene representation as its powerful view synthesis capabilities. Regarding
NeRF representation, its registration is also required for large-scale scene
reconstruction. However, this topic extremly lacks exploration. This is due to
the inherent challenge to model the geometric relationship among two scenes
with implicit representations. The existing methods usually convert the
implicit representation to explicit representation for further registration.
Most recently, Gaussian Splatting (GS) is introduced, employing explicit 3D
Gaussian. This method significantly enhances rendering speed while maintaining
high rendering quality. Given two scenes with explicit GS representations, in
this work, we explore the 3D registration task between them. To this end, we
propose GaussReg, a novel coarse-to-fine framework, both fast and accurate. The
coarse stage follows existing point cloud registration methods and estimates a
rough alignment for point clouds from GS. We further newly present an
image-guided fine registration approach, which renders images from GS to
provide more detailed geometric information for precise alignment. To support
comprehensive evaluation, we carefully build a scene-level dataset called
ScanNet-GSReg with 1379 scenes obtained from the ScanNet dataset and collect an
in-the-wild dataset called GSReg. Experimental results demonstrate our method
achieves state-of-the-art performance on multiple datasets. Our GaussReg is 44
times faster than HLoc (SuperPoint as the feature extractor and SuperGlue as
the matcher) with comparable accuracy.

Comments:
- ECCV 2024

---

## SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-06 | Weixing Xie, Junfeng Yao, Xianpeng Cao, Qiqin Lin, Zerui Tang, Xiao Dong, Xiaohu Guo | cs.CV | [PDF](http://arxiv.org/pdf/2407.05023v1){: .btn .btn-green } |

**Abstract**: Dynamic reconstruction of deformable tissues in endoscopic video is a key
technology for robot-assisted surgery. Recent reconstruction methods based on
neural radiance fields (NeRFs) have achieved remarkable results in the
reconstruction of surgical scenes. However, based on implicit representation,
NeRFs struggle to capture the intricate details of objects in the scene and
cannot achieve real-time rendering. In addition, restricted single view
perception and occluded instruments also propose special challenges in surgical
scene reconstruction. To address these issues, we develop SurgicalGaussian, a
deformable 3D Gaussian Splatting method to model dynamic surgical scenes. Our
approach models the spatio-temporal features of soft tissues at each time stamp
via a forward-mapping deformation MLP and regularization to constrain local 3D
Gaussians to comply with consistent movement. With the depth initialization
strategy and tool mask-guided training, our method can remove surgical
instruments and reconstruct high-fidelity surgical scenes. Through experiments
on various surgical videos, our network outperforms existing method on many
aspects, including rendering quality, rendering speed and GPU usage. The
project page can be found at https://surgicalgaussian.github.io.



---

## GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-05 | Yuxuan Mu, Xinxin Zuo, Chuan Guo, Yilin Wang, Juwei Lu, Xiaofeng Wu, Songcen Xu, Peng Dai, Youliang Yan, Li Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2407.04237v2){: .btn .btn-green } |

**Abstract**: We present GSD, a diffusion model approach based on Gaussian Splatting (GS)
representation for 3D object reconstruction from a single view. Prior works
suffer from inconsistent 3D geometry or mediocre rendering quality due to
improper representations. We take a step towards resolving these shortcomings
by utilizing the recent state-of-the-art 3D explicit representation, Gaussian
Splatting, and an unconditional diffusion model. This model learns to generate
3D objects represented by sets of GS ellipsoids. With these strong generative
3D priors, though learning unconditionally, the diffusion model is ready for
view-guided reconstruction without further model fine-tuning. This is achieved
by propagating fine-grained 2D features through the efficient yet flexible
splatting function and the guided denoising sampling process. In addition, a 2D
diffusion model is further employed to enhance rendering fidelity, and improve
reconstructed GS quality by polishing and re-using the rendered images. The
final reconstructed objects explicitly come with high-quality 3D structure and
texture, and can be efficiently rendered in arbitrary views. Experiments on the
challenging real-world CO3D dataset demonstrate the superiority of our
approach. Project page: $\href{https://yxmu.foo/GSD/}{\text{this https URL}}$

Comments:
- Accepted for ECCV 2024

---

## Segment Any 4D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-05 | Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, Xinggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2407.04504v2){: .btn .btn-green } |

**Abstract**: Modeling, understanding, and reconstructing the real world are crucial in
XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable
success in modeling and understanding 3D scenes. Similarly, various 4D
representations have demonstrated the ability to capture the dynamics of the 4D
world. However, there is a dearth of research focusing on segmentation within
4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D),
one of the first frameworks to segment anything in the 4D digital world based
on 4D Gaussians. In SA4D, an efficient temporal identity feature field is
introduced to handle Gaussian drifting, with the potential to learn precise
identity features from noisy and sparse input. Additionally, a 4D segmentation
refinement process is proposed to remove artifacts. Our SA4D achieves precise,
high-quality segmentation within seconds in 4D Gaussians and shows the ability
to remove, recolor, compose, and render high-quality anything masks. More demos
are available at: https://jsxzs.github.io/sa4d/.

Comments:
- 22 pages

---

## Gaussian Eigen Models for Human Heads

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-05 | Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2407.04545v1){: .btn .btn-green } |

**Abstract**: We present personalized Gaussian Eigen Models (GEMs) for human heads, a novel
method that compresses dynamic 3D Gaussians into low-dimensional linear spaces.
Our approach is inspired by the seminal work of Blanz and Vetter, where a
mesh-based 3D morphable model (3DMM) is constructed from registered meshes.
Based on dynamic 3D Gaussians, we create a lower-dimensional representation of
primitives that applies to most 3DGS head avatars. Specifically, we propose a
universal method to distill the appearance of a mesh-controlled UNet Gaussian
avatar using an ensemble of linear eigenbasis. We replace heavy CNN-based
architectures with a single linear layer improving speed and enabling a range
of real-time downstream applications. To create a particular facial expression,
one simply needs to perform a dot product between the eigen coefficients and
the distilled basis. This efficient method removes the requirement for an input
mesh during testing, enhancing simplicity and speed in expression generation.
This process is highly efficient and supports real-time rendering on everyday
devices, leveraging the effectiveness of standard Gaussian Splatting. In
addition, we demonstrate how the GEM can be controlled using a ResNet-based
regression architecture. We show and compare self-reenactment and cross-person
reenactment to state-of-the-art 3D avatar methods, demonstrating higher quality
and better control. A real-time demo showcases the applicability of the GEM
representation.

Comments:
- https://zielon.github.io/gem/

---

## SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-04 | Yijia Guo, Liwen Hu, Lei Ma, Tiejun Huang | cs.CV | [PDF](http://arxiv.org/pdf/2407.03771v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance
in 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.
Fulfilling this requirement can be challenging in real-world scenarios
especially when the camera moves fast, which severely limits the application of
3DGS. To address these challenges, we proposed Spike Gausian Splatting
(SpikeGS), the first framework that integrates the spike streams into 3DGS
pipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With
accumulation rasterization, interval supervision, and a specially designed
pipeline, SpikeGS extracts detailed geometry and texture from high temporal
resolution but texture lacking spike stream, reconstructs 3D scenes captured in
1 second. Extensive experiments on multiple synthetic and real-world datasets
demonstrate the superiority of SpikeGS compared with existing spike-based and
deblur 3D scene reconstruction methods. Codes and data will be released soon.



---

## PFGS: High Fidelity Point Cloud Rendering via Feature Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-04 | Jiaxu Wang, Ziyi Zhang, Junhao He, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2407.03857v1){: .btn .btn-green } |

**Abstract**: Rendering high-fidelity images from sparse point clouds is still challenging.
Existing learning-based approaches suffer from either hole artifacts, missing
details, or expensive computations. In this paper, we propose a novel framework
to render high-quality images from sparse points. This method first attempts to
bridge the 3D Gaussian Splatting and point cloud rendering, which includes
several cascaded modules. We first use a regressor to estimate Gaussian
properties in a point-wise manner, the estimated properties are used to
rasterize neural feature descriptors into 2D planes which are extracted from a
multiscale extractor. The projected feature volume is gradually decoded toward
the final prediction via a multiscale and progressive decoder. The whole
pipeline experiences a two-stage training and is driven by our well-designed
progressive and multiscale reconstruction loss. Experiments on different
benchmarks show the superiority of our method in terms of rendering qualities
and the necessities of our main components.



---

## CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion  Blur Images

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-04 | Junghe Lee, Donghyeong Kim, Dogyoon Lee, Suhwan Cho, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2407.03923v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) have received significant attention due to
their high-quality novel view rendering ability, prompting research to address
various real-world cases. One critical challenge is the camera motion blur
caused by camera movement during exposure time, which prevents accurate 3D
scene reconstruction. In this study, we propose continuous rigid motion-aware
gaussian splatting (CRiM-GS) to reconstruct accurate 3D scene from blurry
images with real-time rendering speed. Considering the actual camera motion
blurring process, which consists of complex motion patterns, we predict the
continuous movement of the camera based on neural ordinary differential
equations (ODEs). Specifically, we leverage rigid body transformations to model
the camera motion with proper regularization, preserving the shape and size of
the object. Furthermore, we introduce a continuous deformable 3D transformation
in the \textit{SE(3)} field to adapt the rigid body transformation to
real-world problems by ensuring a higher degree of freedom. By revisiting
fundamental camera theory and employing advanced neural network training
techniques, we achieve accurate modeling of continuous camera trajectories. We
conduct extensive experiments, demonstrating state-of-the-art performance both
quantitatively and qualitatively on benchmark datasets.

Comments:
- Project Page : https://jho-yonsei.github.io/CRiM-Gaussian/

---

## VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using  Learned Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-03 | Sungwon Hwang, Min-Jung Kim, Taewoong Kang, Jayeon Kang, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2407.02945v2){: .btn .btn-green } |

**Abstract**: Neural rendering-based urban scene reconstruction methods commonly rely on
images collected from driving vehicles with cameras facing and moving forward.
Although these methods can successfully synthesize from views similar to
training camera trajectory, directing the novel view outside the training
camera distribution does not guarantee on-par performance. In this paper, we
tackle the Extrapolated View Synthesis (EVS) problem by evaluating the
reconstructions on views such as looking left, right or downwards with respect
to training camera distributions. To improve rendering quality for EVS, we
initialize our model by constructing dense LiDAR map, and propose to leverage
prior scene knowledge such as surface normal estimator and large-scale
diffusion model. Qualitative and quantitative comparisons demonstrate the
effectiveness of our methods on EVS. To the best of our knowledge, we are the
first to address the EVS problem in urban scene reconstruction. Link to our
project page: https://vegs3d.github.io/.

Comments:
- The first two authors contributed equally. Project Page:
  https://vegs3d.github.io/

---

## Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-03 | Jiaxin Guo, Jiangliu Wang, Di Kang, Wenzhen Dong, Wenting Wang, Yun-hui Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02918v1){: .btn .btn-green } |

**Abstract**: Real-time 3D reconstruction of surgical scenes plays a vital role in
computer-assisted surgery, holding a promise to enhance surgeons' visibility.
Recent advancements in 3D Gaussian Splatting (3DGS) have shown great potential
for real-time novel view synthesis of general scenes, which relies on accurate
poses and point clouds generated by Structure-from-Motion (SfM) for
initialization. However, 3DGS with SfM fails to recover accurate camera poses
and geometry in surgical scenes due to the challenges of minimal textures and
photometric inconsistencies. To tackle this problem, in this paper, we propose
the first SfM-free 3DGS-based method for surgical scene reconstruction by
jointly optimizing the camera poses and scene representation. Based on the
video continuity, the key of our method is to exploit the immediate optical
flow priors to guide the projection flow derived from 3D Gaussians. Unlike most
previous methods relying on photometric loss only, we formulate the pose
estimation problem as minimizing the flow loss between the projection flow and
optical flow. A consistency check is further introduced to filter the flow
outliers by detecting the rigid and reliable points that satisfy the epipolar
geometry. During 3D Gaussian optimization, we randomly sample frames to
optimize the scene representations to grow the 3D Gaussian progressively.
Experiments on the SCARED dataset demonstrate our superior performance over
existing methods in novel view synthesis and pose estimation with high
efficiency. Code is available at https://github.com/wrld/Free-SurGS.

Comments:
- Accepted to MICCAI 2024

---

## BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event  Stream

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Wenpu Li, Pian Wan, Peng Wang, Jinghang Li, Yi Zhou, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02174v2){: .btn .btn-green } |

**Abstract**: Neural implicit representation of visual scenes has attracted a lot of
attention in recent research of computer vision and graphics. Most prior
methods focus on how to reconstruct 3D scene representation from a set of
images. In this work, we demonstrate the possibility to recover the neural
radiance fields (NeRF) from a single blurry image and its corresponding event
stream. We model the camera motion with a cubic B-Spline in SE(3) space. Both
the blurry image and the brightness change within a time interval, can then be
synthesized from the 3D scene representation given the 6-DoF poses interpolated
from the cubic B-Spline. Our method can jointly learn both the implicit neural
scene representation and recover the camera motion by minimizing the
differences between the synthesized data and the real measurements without
pre-computed camera poses from COLMAP. We evaluate the proposed method with
both synthetic and real datasets. The experimental results demonstrate that we
are able to render view-consistent latent sharp images from the learned NeRF
and bring a blurry image alive in high quality. Code and data are available at
https://github.com/WU-CVGL/BeNeRF.

Comments:
- Accepted to ECCV 2024

---

## TrAME: Trajectory-Anchored Multi-View Editing for Text-Guided 3D  Gaussian Splatting Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Chaofan Luo, Donglin Di, Yongjia Ma, Zhou Xue, Chen Wei, Xun Yang, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02034v1){: .btn .btn-green } |

**Abstract**: Despite significant strides in the field of 3D scene editing, current methods
encounter substantial challenge, particularly in preserving 3D consistency in
multi-view editing process. To tackle this challenge, we propose a progressive
3D editing strategy that ensures multi-view consistency via a
Trajectory-Anchored Scheme (TAS) with a dual-branch editing mechanism.
Specifically, TAS facilitates a tightly coupled iterative process between 2D
view editing and 3D updating, preventing error accumulation yielded from
text-to-image process. Additionally, we explore the relationship between
optimization-based methods and reconstruction-based methods, offering a unified
perspective for selecting superior design choice, supporting the rationale
behind the designed TAS. We further present a tuning-free View-Consistent
Attention Control (VCAC) module that leverages cross-view semantic and
geometric reference from the source branch to yield aligned views from the
target branch during the editing of 2D views. To validate the effectiveness of
our method, we analyze 2D examples to demonstrate the improved consistency with
the VCAC module. Further extensive quantitative and qualitative results in
text-guided 3D scene editing indicate that our method achieves superior editing
quality compared to state-of-the-art methods. We will make the complete
codebase publicly available following the conclusion of the double-blind review
process.



---

## AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Mustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02598v2){: .btn .btn-green } |

**Abstract**: Realistic scene reconstruction and view synthesis are essential for advancing
autonomous driving systems by simulating safety-critical scenarios. 3D Gaussian
Splatting excels in real-time rendering and static scene reconstructions but
struggles with modeling driving scenarios due to complex backgrounds, dynamic
objects, and sparse views. We propose AutoSplat, a framework employing Gaussian
splatting to achieve highly realistic reconstructions of autonomous driving
scenes. By imposing geometric constraints on Gaussians representing the road
and sky regions, our method enables multi-view consistent simulation of
challenging scenarios including lane changes. Leveraging 3D templates, we
introduce a reflected Gaussian consistency constraint to supervise both the
visible and unseen side of foreground objects. Moreover, to model the dynamic
appearance of foreground objects, we estimate residual spherical harmonics for
each foreground Gaussian. Extensive experiments on Pandaset and KITTI
demonstrate that AutoSplat outperforms state-of-the-art methods in scene
reconstruction and novel view synthesis across diverse driving scenarios. Visit
our project page at https://autosplat.github.io/.



---

## MomentsNeRF: Leveraging Orthogonal Moments for Few-Shot Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Ahmad AlMughrabi, Ricardo Marques, Petia Radeva | cs.CV | [PDF](http://arxiv.org/pdf/2407.02668v1){: .btn .btn-green } |

**Abstract**: We propose MomentsNeRF, a novel framework for one- and few-shot neural
rendering that predicts a neural representation of a 3D scene using Orthogonal
Moments. Our architecture offers a new transfer learning method to train on
multi-scenes and incorporate a per-scene optimization using one or a few images
at test time. Our approach is the first to successfully harness features
extracted from Gabor and Zernike moments, seamlessly integrating them into the
NeRF architecture. We show that MomentsNeRF performs better in synthesizing
images with complex textures and shapes, achieving a significant noise
reduction, artifact elimination, and completing the missing parts compared to
the recent one- and few-shot neural rendering frameworks. Extensive experiments
on the DTU and Shapenet datasets show that MomentsNeRF improves the
state-of-the-art by {3.39\;dB\;PSNR}, 11.1% SSIM, 17.9% LPIPS, and 8.3% DISTS
metrics. Moreover, it outperforms state-of-the-art performance for both novel
view synthesis and single-image 3D view reconstruction. The source code is
accessible at: https://amughrabi.github.io/momentsnerf/.



---

## Active Human Pose Estimation via an Autonomous UAV Agent

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Jingxi Chen, Botao He, Chahat Deep Singh, Cornelia Fermuller, Yiannis Aloimonos | cs.RO | [PDF](http://arxiv.org/pdf/2407.01811v1){: .btn .btn-green } |

**Abstract**: One of the core activities of an active observer involves moving to secure a
"better" view of the scene, where the definition of "better" is task-dependent.
This paper focuses on the task of human pose estimation from videos capturing a
person's activity. Self-occlusions within the scene can complicate or even
prevent accurate human pose estimation. To address this, relocating the camera
to a new vantage point is necessary to clarify the view, thereby improving 2D
human pose estimation. This paper formalizes the process of achieving an
improved viewpoint. Our proposed solution to this challenge comprises three
main components: a NeRF-based Drone-View Data Generation Framework, an On-Drone
Network for Camera View Error Estimation, and a Combined Planner for devising a
feasible motion plan to reposition the camera based on the predicted errors for
camera views. The Data Generation Framework utilizes NeRF-based methods to
generate a comprehensive dataset of human poses and activities, enhancing the
drone's adaptability in various scenarios. The Camera View Error Estimation
Network is designed to evaluate the current human pose and identify the most
promising next viewing angles for the drone, ensuring a reliable and precise
pose estimation from those angles. Finally, the combined planner incorporates
these angles while considering the drone's physical and environmental
limitations, employing efficient algorithms to navigate safe and effective
flight paths. This system represents a significant advancement in active 2D
human pose estimation for an autonomous UAV agent, offering substantial
potential for applications in aerial cinematography by improving the
performance of autonomous human pose estimation and maintaining the operational
safety and efficiency of UAVs.



---

## Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Zihan Gao, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Yuwei Guo, Shuyuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2407.01220v1){: .btn .btn-green } |

**Abstract**: Understanding 3D scenes is a crucial challenge in computer vision research
with applications spanning multiple domains. Recent advancements in distilling
2D vision-language foundation models into neural fields, like NeRF and 3DGS,
enables open-vocabulary segmentation of 3D scenes from 2D multi-view images
without the need for precise 3D annotations. While effective, however, the
per-pixel distillation of high-dimensional CLIP features introduces ambiguity
and necessitates complex regularization strategies, adding inefficiencies
during training. This paper presents MaskField, which enables fast and
efficient 3D open-vocabulary segmentation with neural fields under weak
supervision. Unlike previous methods, MaskField distills masks rather than
dense high-dimensional CLIP features. MaskFields employ neural fields as binary
mask generators and supervise them with masks generated by SAM and classified
by coarse CLIP features. MaskField overcomes the ambiguous object boundaries by
naturally introducing SAM segmented object shapes without extra regularization
during training. By circumventing the direct handling of high-dimensional CLIP
features during training, MaskField is particularly compatible with explicit
scene representations like 3DGS. Our extensive experiments show that MaskField
not only surpasses prior state-of-the-art methods but also achieves remarkably
fast convergence, outperforming previous methods with just 5 minutes of
training. We hope that MaskField will inspire further exploration into how
neural fields can be trained to comprehend 3D scenes from 2D models.

Comments:
- 16 pages, 7 figures

---

## GaussianStego: A Generalizable Stenography Pipeline for Generative 3D  Gaussians Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Chenxin Li, Hengyu Liu, Zhiwen Fan, Wuyang Li, Yifan Liu, Panwang Pan, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01301v1){: .btn .btn-green } |

**Abstract**: Recent advancements in large generative models and real-time neural rendering
using point-based techniques pave the way for a future of widespread visual
data distribution through sharing synthesized 3D assets. However, while
standardized methods for embedding proprietary or copyright information, either
overtly or subtly, exist for conventional visual content such as images and
videos, this issue remains unexplored for emerging generative 3D formats like
Gaussian Splatting. We present GaussianStego, a method for embedding
steganographic information in the rendering of generated 3D assets. Our
approach employs an optimization framework that enables the accurate extraction
of hidden information from images rendered using Gaussian assets derived from
large models, while maintaining their original visual quality. We conduct
preliminary evaluations of our method across several potential deployment
scenarios and discuss issues identified through analysis. GaussianStego
represents an initial exploration into the novel challenge of embedding
customizable, imperceptible, and recoverable information within the renders
produced by current 3D generative models, while ensuring minimal impact on the
rendered content's quality.

Comments:
- Project website: https://gaussian-stego.github.io/

---

## EndoSparse: Real-Time Sparse View Synthesis of Endoscopic Scenes using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Chenxin Li, Brandon Y. Feng, Yifan Liu, Hengyu Liu, Cheng Wang, Weihao Yu, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01029v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction of biological tissues from a collection of endoscopic
images is a key to unlock various important downstream surgical applications
with 3D capabilities. Existing methods employ various advanced neural rendering
techniques for photorealistic view synthesis, but they often struggle to
recover accurate 3D representations when only sparse observations are
available, which is usually the case in real-world clinical scenarios. To
tackle this {sparsity} challenge, we propose a framework leveraging the prior
knowledge from multiple foundation models during the reconstruction process,
dubbed as \textit{EndoSparse}. Experimental results indicate that our proposed
strategy significantly improves the geometric and appearance quality under
challenging sparse-view conditions, including using only three views. In
rigorous benchmarking experiments against state-of-the-art methods,
\textit{EndoSparse} achieves superior results in terms of accurate geometry,
realistic appearance, and rendering efficiency, confirming the robustness to
sparse-view limitations in endoscopic reconstruction. \textit{EndoSparse}
signifies a steady step towards the practical deployment of neural 3D
reconstruction in real-world clinical scenarios. Project page:
https://endo-sparse.github.io/.

Comments:
- Accpeted by MICCAI2024

---

## DRAGON: Drone and Ground Gaussian Splatting for 3D Building  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Yujin Ham, Mateusz Michalkiewicz, Guha Balakrishnan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01761v1){: .btn .btn-green } |

**Abstract**: 3D building reconstruction from imaging data is an important task for many
applications ranging from urban planning to reconnaissance. Modern Novel View
synthesis (NVS) methods like NeRF and Gaussian Splatting offer powerful
techniques for developing 3D models from natural 2D imagery in an unsupervised
fashion. These algorithms generally require input training views surrounding
the scene of interest, which, in the case of large buildings, is typically not
available across all camera elevations. In particular, the most readily
available camera viewpoints at scale across most buildings are at near-ground
(e.g., with mobile phones) and aerial (drones) elevations. However, due to the
significant difference in viewpoint between drone and ground image sets, camera
registration - a necessary step for NVS algorithms - fails. In this work we
propose a method, DRAGON, that can take drone and ground building imagery as
input and produce a 3D NVS model. The key insight of DRAGON is that
intermediate elevation imagery may be extrapolated by an NVS algorithm itself
in an iterative procedure with perceptual regularization, thereby bridging the
visual feature gap between the two elevations and enabling registration. We
compiled a semi-synthetic dataset of 9 large building scenes using Google Earth
Studio, and quantitatively and qualitatively demonstrate that DRAGON can
generate compelling renderings on this dataset compared to baseline strategies.

Comments:
- 12 pages, 9 figures, accepted to ICCP 2024
