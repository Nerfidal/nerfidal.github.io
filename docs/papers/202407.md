---
layout: default
title: July 2024
parent: Papers
nav_order: 202407
---

<!---metadata--->


## VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using  Learned Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-03 | Sungwon Hwang, Min-Jung Kim, Taewoong Kang, Jayeon Kang, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2407.02945v1){: .btn .btn-green } |

**Abstract**: Neural rendering-based urban scene reconstruction methods commonly rely on
images collected from driving vehicles with cameras facing and moving forward.
Although these methods can successfully synthesize from views similar to
training camera trajectory, directing the novel view outside the training
camera distribution does not guarantee on-par performance. In this paper, we
tackle the Extrapolated View Synthesis (EVS) problem by evaluating the
reconstructions on views such as looking left, right or downwards with respect
to training camera distributions. To improve rendering quality for EVS, we
initialize our model by constructing dense LiDAR map, and propose to leverage
prior scene knowledge such as surface normal estimator and large-scale
diffusion model. Qualitative and quantitative comparisons demonstrate the
effectiveness of our methods on EVS. To the best of our knowledge, we are the
first to address the EVS problem in urban scene reconstruction. Link to our
project page: https://vegs3d.github.io/.



---

## Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-03 | Jiaxin Guo, Jiangliu Wang, Di Kang, Wenzhen Dong, Wenting Wang, Yun-hui Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02918v1){: .btn .btn-green } |

**Abstract**: Real-time 3D reconstruction of surgical scenes plays a vital role in
computer-assisted surgery, holding a promise to enhance surgeons' visibility.
Recent advancements in 3D Gaussian Splatting (3DGS) have shown great potential
for real-time novel view synthesis of general scenes, which relies on accurate
poses and point clouds generated by Structure-from-Motion (SfM) for
initialization. However, 3DGS with SfM fails to recover accurate camera poses
and geometry in surgical scenes due to the challenges of minimal textures and
photometric inconsistencies. To tackle this problem, in this paper, we propose
the first SfM-free 3DGS-based method for surgical scene reconstruction by
jointly optimizing the camera poses and scene representation. Based on the
video continuity, the key of our method is to exploit the immediate optical
flow priors to guide the projection flow derived from 3D Gaussians. Unlike most
previous methods relying on photometric loss only, we formulate the pose
estimation problem as minimizing the flow loss between the projection flow and
optical flow. A consistency check is further introduced to filter the flow
outliers by detecting the rigid and reliable points that satisfy the epipolar
geometry. During 3D Gaussian optimization, we randomly sample frames to
optimize the scene representations to grow the 3D Gaussian progressively.
Experiments on the SCARED dataset demonstrate our superior performance over
existing methods in novel view synthesis and pose estimation with high
efficiency. Code is available at https://github.com/wrld/Free-SurGS.

Comments:
- Accepted to MICCAI 2024

---

## MomentsNeRF: Leveraging Orthogonal Moments for Few-Shot Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Ahmad AlMughrabi, Ricardo Marques, Petia Radeva | cs.CV | [PDF](http://arxiv.org/pdf/2407.02668v1){: .btn .btn-green } |

**Abstract**: We propose MomentsNeRF, a novel framework for one- and few-shot neural
rendering that predicts a neural representation of a 3D scene using Orthogonal
Moments. Our architecture offers a new transfer learning method to train on
multi-scenes and incorporate a per-scene optimization using one or a few images
at test time. Our approach is the first to successfully harness features
extracted from Gabor and Zernike moments, seamlessly integrating them into the
NeRF architecture. We show that MomentsNeRF performs better in synthesizing
images with complex textures and shapes, achieving a significant noise
reduction, artifact elimination, and completing the missing parts compared to
the recent one- and few-shot neural rendering frameworks. Extensive experiments
on the DTU and Shapenet datasets show that MomentsNeRF improves the
state-of-the-art by {3.39\;dB\;PSNR}, 11.1% SSIM, 17.9% LPIPS, and 8.3% DISTS
metrics. Moreover, it outperforms state-of-the-art performance for both novel
view synthesis and single-image 3D view reconstruction. The source code is
accessible at: https://amughrabi.github.io/momentsnerf/.



---

## TrAME: Trajectory-Anchored Multi-View Editing for Text-Guided 3D  Gaussian Splatting Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Chaofan Luo, Donglin Di, Yongjia Ma, Zhou Xue, Chen Wei, Xun Yang, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02034v1){: .btn .btn-green } |

**Abstract**: Despite significant strides in the field of 3D scene editing, current methods
encounter substantial challenge, particularly in preserving 3D consistency in
multi-view editing process. To tackle this challenge, we propose a progressive
3D editing strategy that ensures multi-view consistency via a
Trajectory-Anchored Scheme (TAS) with a dual-branch editing mechanism.
Specifically, TAS facilitates a tightly coupled iterative process between 2D
view editing and 3D updating, preventing error accumulation yielded from
text-to-image process. Additionally, we explore the relationship between
optimization-based methods and reconstruction-based methods, offering a unified
perspective for selecting superior design choice, supporting the rationale
behind the designed TAS. We further present a tuning-free View-Consistent
Attention Control (VCAC) module that leverages cross-view semantic and
geometric reference from the source branch to yield aligned views from the
target branch during the editing of 2D views. To validate the effectiveness of
our method, we analyze 2D examples to demonstrate the improved consistency with
the VCAC module. Further extensive quantitative and qualitative results in
text-guided 3D scene editing indicate that our method achieves superior editing
quality compared to state-of-the-art methods. We will make the complete
codebase publicly available following the conclusion of the double-blind review
process.



---

## AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Mustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02598v1){: .btn .btn-green } |

**Abstract**: Realistic scene reconstruction and view synthesis are essential for advancing
autonomous driving systems by simulating safety-critical scenarios. 3D Gaussian
Splatting excels in real-time rendering and static scene reconstructions but
struggles with modeling driving scenarios due to complex backgrounds, dynamic
objects, and sparse views. We propose AutoSplat, a framework employing Gaussian
splatting to achieve highly realistic reconstructions of autonomous driving
scenes. By imposing geometric constraints on Gaussians representing the road
and sky regions, our method enables multi-view consistent simulation of
challenging scenarios including lane changes. Leveraging 3D templates, we
introduce a reflected Gaussian consistency constraint to supervise both the
visible and unseen side of foreground objects. Moreover, to model the dynamic
appearance of foreground objects, we estimate residual spherical harmonics for
each foreground Gaussian. Extensive experiments on Pandaset and KITTI
demonstrate that AutoSplat outperforms state-of-the-art methods in scene
reconstruction and novel view synthesis across diverse driving scenarios. Visit
our $\href{https://autosplat.github.io/}{\text{project page}}$.



---

## BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event  Stream

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Wenpu Li, Pian Wan, Peng Wang, Jinghang Li, Yi Zhou, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02174v2){: .btn .btn-green } |

**Abstract**: Neural implicit representation of visual scenes has attracted a lot of
attention in recent research of computer vision and graphics. Most prior
methods focus on how to reconstruct 3D scene representation from a set of
images. In this work, we demonstrate the possibility to recover the neural
radiance fields (NeRF) from a single blurry image and its corresponding event
stream. We model the camera motion with a cubic B-Spline in SE(3) space. Both
the blurry image and the brightness change within a time interval, can then be
synthesized from the 3D scene representation given the 6-DoF poses interpolated
from the cubic B-Spline. Our method can jointly learn both the implicit neural
scene representation and recover the camera motion by minimizing the
differences between the synthesized data and the real measurements without
pre-computed camera poses from COLMAP. We evaluate the proposed method with
both synthetic and real datasets. The experimental results demonstrate that we
are able to render view-consistent latent sharp images from the learned NeRF
and bring a blurry image alive in high quality. Code and data are available at
https://github.com/WU-CVGL/BeNeRF.

Comments:
- Accepted to ECCV 2024

---

## DRAGON: Drone and Ground Gaussian Splatting for 3D Building  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Yujin Ham, Mateusz Michalkiewicz, Guha Balakrishnan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01761v1){: .btn .btn-green } |

**Abstract**: 3D building reconstruction from imaging data is an important task for many
applications ranging from urban planning to reconnaissance. Modern Novel View
synthesis (NVS) methods like NeRF and Gaussian Splatting offer powerful
techniques for developing 3D models from natural 2D imagery in an unsupervised
fashion. These algorithms generally require input training views surrounding
the scene of interest, which, in the case of large buildings, is typically not
available across all camera elevations. In particular, the most readily
available camera viewpoints at scale across most buildings are at near-ground
(e.g., with mobile phones) and aerial (drones) elevations. However, due to the
significant difference in viewpoint between drone and ground image sets, camera
registration - a necessary step for NVS algorithms - fails. In this work we
propose a method, DRAGON, that can take drone and ground building imagery as
input and produce a 3D NVS model. The key insight of DRAGON is that
intermediate elevation imagery may be extrapolated by an NVS algorithm itself
in an iterative procedure with perceptual regularization, thereby bridging the
visual feature gap between the two elevations and enabling registration. We
compiled a semi-synthetic dataset of 9 large building scenes using Google Earth
Studio, and quantitatively and qualitatively demonstrate that DRAGON can
generate compelling renderings on this dataset compared to baseline strategies.

Comments:
- 12 pages, 9 figures, accepted to ICCP 2024

---

## GaussianStego: A Generalizable Stenography Pipeline for Generative 3D  Gaussians Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Chenxin Li, Hengyu Liu, Zhiwen Fan, Wuyang Li, Yifan Liu, Panwang Pan, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01301v1){: .btn .btn-green } |

**Abstract**: Recent advancements in large generative models and real-time neural rendering
using point-based techniques pave the way for a future of widespread visual
data distribution through sharing synthesized 3D assets. However, while
standardized methods for embedding proprietary or copyright information, either
overtly or subtly, exist for conventional visual content such as images and
videos, this issue remains unexplored for emerging generative 3D formats like
Gaussian Splatting. We present GaussianStego, a method for embedding
steganographic information in the rendering of generated 3D assets. Our
approach employs an optimization framework that enables the accurate extraction
of hidden information from images rendered using Gaussian assets derived from
large models, while maintaining their original visual quality. We conduct
preliminary evaluations of our method across several potential deployment
scenarios and discuss issues identified through analysis. GaussianStego
represents an initial exploration into the novel challenge of embedding
customizable, imperceptible, and recoverable information within the renders
produced by current 3D generative models, while ensuring minimal impact on the
rendered content's quality.

Comments:
- Project website: https://gaussian-stego.github.io/

---

## Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Zihan Gao, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Yuwei Guo, Shuyuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2407.01220v1){: .btn .btn-green } |

**Abstract**: Understanding 3D scenes is a crucial challenge in computer vision research
with applications spanning multiple domains. Recent advancements in distilling
2D vision-language foundation models into neural fields, like NeRF and 3DGS,
enables open-vocabulary segmentation of 3D scenes from 2D multi-view images
without the need for precise 3D annotations. While effective, however, the
per-pixel distillation of high-dimensional CLIP features introduces ambiguity
and necessitates complex regularization strategies, adding inefficiencies
during training. This paper presents MaskField, which enables fast and
efficient 3D open-vocabulary segmentation with neural fields under weak
supervision. Unlike previous methods, MaskField distills masks rather than
dense high-dimensional CLIP features. MaskFields employ neural fields as binary
mask generators and supervise them with masks generated by SAM and classified
by coarse CLIP features. MaskField overcomes the ambiguous object boundaries by
naturally introducing SAM segmented object shapes without extra regularization
during training. By circumventing the direct handling of high-dimensional CLIP
features during training, MaskField is particularly compatible with explicit
scene representations like 3DGS. Our extensive experiments show that MaskField
not only surpasses prior state-of-the-art methods but also achieves remarkably
fast convergence, outperforming previous methods with just 5 minutes of
training. We hope that MaskField will inspire further exploration into how
neural fields can be trained to comprehend 3D scenes from 2D models.

Comments:
- 16 pages, 7 figures

---

## EndoSparse: Real-Time Sparse View Synthesis of Endoscopic Scenes using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Chenxin Li, Brandon Y. Feng, Yifan Liu, Hengyu Liu, Cheng Wang, Weihao Yu, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01029v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction of biological tissues from a collection of endoscopic
images is a key to unlock various important downstream surgical applications
with 3D capabilities. Existing methods employ various advanced neural rendering
techniques for photorealistic view synthesis, but they often struggle to
recover accurate 3D representations when only sparse observations are
available, which is usually the case in real-world clinical scenarios. To
tackle this {sparsity} challenge, we propose a framework leveraging the prior
knowledge from multiple foundation models during the reconstruction process,
dubbed as \textit{EndoSparse}. Experimental results indicate that our proposed
strategy significantly improves the geometric and appearance quality under
challenging sparse-view conditions, including using only three views. In
rigorous benchmarking experiments against state-of-the-art methods,
\textit{EndoSparse} achieves superior results in terms of accurate geometry,
realistic appearance, and rendering efficiency, confirming the robustness to
sparse-view limitations in endoscopic reconstruction. \textit{EndoSparse}
signifies a steady step towards the practical deployment of neural 3D
reconstruction in real-world clinical scenarios. Project page:
https://endo-sparse.github.io/.

Comments:
- Accpeted by MICCAI2024

---

## Active Human Pose Estimation via an Autonomous UAV Agent

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Jingxi Chen, Botao He, Chahat Deep Singh, Cornelia Fermuller, Yiannis Aloimonos | cs.RO | [PDF](http://arxiv.org/pdf/2407.01811v1){: .btn .btn-green } |

**Abstract**: One of the core activities of an active observer involves moving to secure a
"better" view of the scene, where the definition of "better" is task-dependent.
This paper focuses on the task of human pose estimation from videos capturing a
person's activity. Self-occlusions within the scene can complicate or even
prevent accurate human pose estimation. To address this, relocating the camera
to a new vantage point is necessary to clarify the view, thereby improving 2D
human pose estimation. This paper formalizes the process of achieving an
improved viewpoint. Our proposed solution to this challenge comprises three
main components: a NeRF-based Drone-View Data Generation Framework, an On-Drone
Network for Camera View Error Estimation, and a Combined Planner for devising a
feasible motion plan to reposition the camera based on the predicted errors for
camera views. The Data Generation Framework utilizes NeRF-based methods to
generate a comprehensive dataset of human poses and activities, enhancing the
drone's adaptability in various scenarios. The Camera View Error Estimation
Network is designed to evaluate the current human pose and identify the most
promising next viewing angles for the drone, ensuring a reliable and precise
pose estimation from those angles. Finally, the combined planner incorporates
these angles while considering the drone's physical and environmental
limitations, employing efficient algorithms to navigate safe and effective
flight paths. This system represents a significant advancement in active 2D
human pose estimation for an autonomous UAV agent, offering substantial
potential for applications in aerial cinematography by improving the
performance of autonomous human pose estimation and maintaining the operational
safety and efficiency of UAVs.


