---
layout: default
title: July 2024
parent: Papers
nav_order: 202407
---

<!---metadata--->


## Expressive Whole-Body 3D Gaussian Avatar

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-31 | Gyeongsik Moon, Takaaki Shiratori, Shunsuke Saito | cs.CV | [PDF](http://arxiv.org/pdf/2407.21686v1){: .btn .btn-green } |

**Abstract**: Facial expression and hand motions are necessary to express our emotions and
interact with the world. Nevertheless, most of the 3D human avatars modeled
from a casually captured video only support body motions without facial
expressions and hand motions.In this work, we present ExAvatar, an expressive
whole-body 3D human avatar learned from a short monocular video. We design
ExAvatar as a combination of the whole-body parametric mesh model (SMPL-X) and
3D Gaussian Splatting (3DGS). The main challenges are 1) a limited diversity of
facial expressions and poses in the video and 2) the absence of 3D
observations, such as 3D scans and RGBD images. The limited diversity in the
video makes animations with novel facial expressions and poses non-trivial. In
addition, the absence of 3D observations could cause significant ambiguity in
human parts that are not observed in the video, which can result in noticeable
artifacts under novel motions. To address them, we introduce our hybrid
representation of the mesh and 3D Gaussians. Our hybrid representation treats
each 3D Gaussian as a vertex on the surface with pre-defined connectivity
information (i.e., triangle faces) between them following the mesh topology of
SMPL-X. It makes our ExAvatar animatable with novel facial expressions by
driven by the facial expression space of SMPL-X. In addition, by using
connectivity-based regularizers, we significantly reduce artifacts in novel
facial expressions and poses.

Comments:
- Accepted to ECCV 2024. Project page:
  https://mks0601.github.io/ExAvatar/

---

## Localized Gaussian Splatting Editing with Contextual Awareness

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-31 | Hanyuan Xiao, Yingshu Chen, Huajian Huang, Haolin Xiong, Jing Yang, Pratusha Prasad, Yajie Zhao | cs.CV | [PDF](http://arxiv.org/pdf/2408.00083v1){: .btn .btn-green } |

**Abstract**: Recent text-guided generation of individual 3D object has achieved great
success using diffusion priors. However, these methods are not suitable for
object insertion and replacement tasks as they do not consider the background,
leading to illumination mismatches within the environment. To bridge the gap,
we introduce an illumination-aware 3D scene editing pipeline for 3D Gaussian
Splatting (3DGS) representation. Our key observation is that inpainting by the
state-of-the-art conditional 2D diffusion model is consistent with background
in lighting. To leverage the prior knowledge from the well-trained diffusion
models for 3D object generation, our approach employs a coarse-to-fine
objection optimization pipeline with inpainted views. In the first coarse step,
we achieve image-to-3D lifting given an ideal inpainted view. The process
employs 3D-aware diffusion prior from a view-conditioned diffusion model, which
preserves illumination present in the conditioning image. To acquire an ideal
inpainted image, we introduce an Anchor View Proposal (AVP) algorithm to find a
single view that best represents the scene illumination in target region. In
the second Texture Enhancement step, we introduce a novel Depth-guided
Inpainting Score Distillation Sampling (DI-SDS), which enhances geometry and
texture details with the inpainting diffusion prior, beyond the scope of the
3D-aware diffusion prior knowledge in the first coarse step. DI-SDS not only
provides fine-grained texture enhancement, but also urges optimization to
respect scene lighting. Our approach efficiently achieves local editing with
global illumination consistency without explicitly modeling light transport. We
demonstrate robustness of our method by evaluating editing in real scenes
containing explicit highlight and shadows, and compare against the
state-of-the-art text-to-3D editing methods.



---

## StyleRF-VolVis: Style Transfer of Neural Radiance Fields for Expressive  Volume Visualization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-31 | Kaiyuan Tang, Chaoli Wang | cs.GR | [PDF](http://arxiv.org/pdf/2408.00150v1){: .btn .btn-green } |

**Abstract**: In volume visualization, visualization synthesis has attracted much attention
due to its ability to generate novel visualizations without following the
conventional rendering pipeline. However, existing solutions based on
generative adversarial networks often require many training images and take
significant training time. Still, issues such as low quality, consistency, and
flexibility persist. This paper introduces StyleRF-VolVis, an innovative style
transfer framework for expressive volume visualization (VolVis) via neural
radiance field (NeRF). The expressiveness of StyleRF-VolVis is upheld by its
ability to accurately separate the underlying scene geometry (i.e., content)
and color appearance (i.e., style), conveniently modify color, opacity, and
lighting of the original rendering while maintaining visual content consistency
across the views, and effectively transfer arbitrary styles from reference
images to the reconstructed 3D scene. To achieve these, we design a base NeRF
model for scene geometry extraction, a palette color network to classify
regions of the radiance field for photorealistic editing, and an unrestricted
color network to lift the color palette constraint via knowledge distillation
for non-photorealistic editing. We demonstrate the superior quality,
consistency, and flexibility of StyleRF-VolVis by experimenting with various
volume rendering scenes and reference images and comparing StyleRF-VolVis
against other image-based (AdaIN), video-based (ReReVST), and NeRF-based (ARF
and SNeRF) style rendering solutions.

Comments:
- Accepted by IEEE VIS 2024

---

## SceneTeller: Language-to-3D Scene Generation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-30 | Başak Melis Öcal, Maxim Tatarchenko, Sezer Karaoglu, Theo Gevers | cs.CV | [PDF](http://arxiv.org/pdf/2407.20727v1){: .btn .btn-green } |

**Abstract**: Designing high-quality indoor 3D scenes is important in many practical
applications, such as room planning or game development. Conventionally, this
has been a time-consuming process which requires both artistic skill and
familiarity with professional software, making it hardly accessible for layman
users. However, recent advances in generative AI have established solid
foundation for democratizing 3D design. In this paper, we propose a pioneering
approach for text-based 3D room design. Given a prompt in natural language
describing the object placement in the room, our method produces a high-quality
3D scene corresponding to it. With an additional text prompt the users can
change the appearance of the entire scene or of individual objects in it. Built
using in-context learning, CAD model retrieval and 3D-Gaussian-Splatting-based
stylization, our turnkey pipeline produces state-of-the-art 3D scenes, while
being easy to use even for novices. Our project page is available at
https://sceneteller.github.io/.

Comments:
- ECCV'24 camera-ready version

---

## Dynamic Scene Understanding through Object-Centric Voxelization and  Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-30 | Yanpeng Zhao, Yiwei Hao, Siyu Gao, Yunbo Wang, Xiaokang Yang | cs.CV | [PDF](http://arxiv.org/pdf/2407.20908v1){: .btn .btn-green } |

**Abstract**: Learning object-centric representations from unsupervised videos is
challenging. Unlike most previous approaches that focus on decomposing 2D
images, we present a 3D generative model named DynaVol-S for dynamic scenes
that enables object-centric learning within a differentiable volume rendering
framework. The key idea is to perform object-centric voxelization to capture
the 3D nature of the scene, which infers per-object occupancy probabilities at
individual spatial locations. These voxel features evolve through a
canonical-space deformation function and are optimized in an inverse rendering
pipeline with a compositional NeRF. Additionally, our approach integrates 2D
semantic features to create 3D semantic grids, representing the scene through
multiple disentangled voxel grids. DynaVol-S significantly outperforms existing
models in both novel view synthesis and unsupervised decomposition tasks for
dynamic scenes. By jointly considering geometric structures and semantic
features, it effectively addresses challenging real-world scenarios involving
complex object interactions. Furthermore, once trained, the explicitly
meaningful voxel features enable additional capabilities that 2D scene
decomposition methods cannot achieve, such as novel scene generation through
editing geometric shapes or manipulating the motion trajectories of objects.



---

## Registering Neural 4D Gaussians for Endoscopic Surgery

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-29 | Yiming Huang, Beilei Cui, Ikemura Kei, Jiekai Zhang, Long Bai, Hongliang Ren | cs.RO | [PDF](http://arxiv.org/pdf/2407.20213v1){: .btn .btn-green } |

**Abstract**: The recent advance in neural rendering has enabled the ability to reconstruct
high-quality 4D scenes using neural networks. Although 4D neural reconstruction
is popular, registration for such representations remains a challenging task,
especially for dynamic scene registration in surgical planning and simulation.
In this paper, we propose a novel strategy for dynamic surgical neural scene
registration. We first utilize 4D Gaussian Splatting to represent the surgical
scene and capture both static and dynamic scenes effectively. Then, a spatial
aware feature aggregation method, Spatially Weight Cluttering (SWC) is proposed
to accurately align the feature between surgical scenes, enabling precise and
realistic surgical simulations. Lastly, we present a novel strategy of
deformable scene registration to register two dynamic scenes. By incorporating
both spatial and temporal information for correspondence matching, our approach
achieves superior performance compared to existing registration methods for
implicit neural representation. The proposed method has the potential to
improve surgical planning and training, ultimately leading to better patient
outcomes.



---

## Radiance Fields for Robotic Teleoperation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-29 | Maximum Wilder-Smith, Vaishakh Patil, Marco Hutter | cs.RO | [PDF](http://arxiv.org/pdf/2407.20194v1){: .btn .btn-green } |

**Abstract**: Radiance field methods such as Neural Radiance Fields (NeRFs) or 3D Gaussian
Splatting (3DGS), have revolutionized graphics and novel view synthesis. Their
ability to synthesize new viewpoints with photo-realistic quality, as well as
capture complex volumetric and specular scenes, makes them an ideal
visualization for robotic teleoperation setups. Direct camera teleoperation
provides high-fidelity operation at the cost of maneuverability, while
reconstruction-based approaches offer controllable scenes with lower fidelity.
With this in mind, we propose replacing the traditional
reconstruction-visualization components of the robotic teleoperation pipeline
with online Radiance Fields, offering highly maneuverable scenes with
photorealistic quality. As such, there are three main contributions to state of
the art: (1) online training of Radiance Fields using live data from multiple
cameras, (2) support for a variety of radiance methods including NeRF and 3DGS,
(3) visualization suite for these methods including a virtual reality scene. To
enable seamless integration with existing setups, these components were tested
with multiple robots in multiple configurations and were displayed using
traditional tools as well as the VR headset. The results across methods and
robots were compared quantitatively to a baseline of mesh reconstruction, and a
user study was conducted to compare the different visualization methods. For
videos and code, check out https://leggedrobotics.github.io/rffr.github.io/.

Comments:
- 8 pages, 10 figures, Accepted to IROS 2024

---

## Garment Animation NeRF with Color Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-29 | Renke Wang, Meng Zhang, Jun Li, Jian Yan | cs.CV | [PDF](http://arxiv.org/pdf/2407.19774v1){: .btn .btn-green } |

**Abstract**: Generating high-fidelity garment animations through traditional workflows,
from modeling to rendering, is both tedious and expensive. These workflows
often require repetitive steps in response to updates in character motion,
rendering viewpoint changes, or appearance edits. Although recent neural
rendering offers an efficient solution for computationally intensive processes,
it struggles with rendering complex garment animations containing fine wrinkle
details and realistic garment-and-body occlusions, while maintaining structural
consistency across frames and dense view rendering. In this paper, we propose a
novel approach to directly synthesize garment animations from body motion
sequences without the need for an explicit garment proxy. Our approach infers
garment dynamic features from body motion, providing a preliminary overview of
garment structure. Simultaneously, we capture detailed features from
synthesized reference images of the garment's front and back, generated by a
pre-trained image model. These features are then used to construct a neural
radiance field that renders the garment animation video. Additionally, our
technique enables garment recoloring by decomposing its visual elements. We
demonstrate the generalizability of our method across unseen body motions and
camera views, ensuring detailed structural consistency. Furthermore, we
showcase its applicability to color editing on both real and synthetic garment
data. Compared to existing neural rendering techniques, our method exhibits
qualitative and quantitative improvements in garment dynamics and wrinkle
detail modeling. Code is available at
\url{https://github.com/wrk226/GarmentAnimationNeRF}.



---

## Revisit Self-supervised Depth Estimation with Local  Structure-from-Motion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-27 | Shengjie Zhu, Xiaoming Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.19166v1){: .btn .btn-green } |

**Abstract**: Both self-supervised depth estimation and Structure-from-Motion (SfM) recover
scene depth from RGB videos. Despite sharing a similar objective, the two
approaches are disconnected. Prior works of self-supervision backpropagate
losses defined within immediate neighboring frames. Instead of
learning-through-loss, this work proposes an alternative scheme by performing
local SfM. First, with calibrated RGB or RGB-D images, we employ a depth and
correspondence estimator to infer depthmaps and pair-wise correspondence maps.
Then, a novel bundle-RANSAC-adjustment algorithm jointly optimizes camera poses
and one depth adjustment for each depthmap. Finally, we fix camera poses and
employ a NeRF, however, without a neural network, for dense triangulation and
geometric verification. Poses, depth adjustments, and triangulated sparse
depths are our outputs. For the first time, we show self-supervision within $5$
frames already benefits SoTA supervised depth and correspondence models.



---

## IOVS4NeRF:Incremental Optimal View Selection for Large-Scale NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-26 | Jingpeng Xie, Shiyu Tan, Yuanlei Wang, Yizhen Lao | cs.CV | [PDF](http://arxiv.org/pdf/2407.18611v1){: .btn .btn-green } |

**Abstract**: Urban-level three-dimensional reconstruction for modern applications demands
high rendering fidelity while minimizing computational costs. The advent of
Neural Radiance Fields (NeRF) has enhanced 3D reconstruction, yet it exhibits
artifacts under multiple viewpoints. In this paper, we propose a new NeRF
framework method to address these issues. Our method uses image content and
pose data to iteratively plan the next best view. A crucial aspect of this
method involves uncertainty estimation, guiding the selection of views with
maximum information gain from a candidate set. This iterative process enhances
rendering quality over time. Simultaneously, we introduce the Vonoroi diagram
and threshold sampling together with flight classifier to boost the efficiency,
while keep the original NeRF network intact. It can serve as a plug-in tool to
assist in better rendering, outperforming baselines and similar prior works.



---

## ScalingGaussian: Enhancing 3D Content Creation with Generative Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-26 | Shen Chen, Jiale Zhou, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Lei Li | cs.CV | [PDF](http://arxiv.org/pdf/2407.19035v1){: .btn .btn-green } |

**Abstract**: The creation of high-quality 3D assets is paramount for applications in
digital heritage preservation, entertainment, and robotics. Traditionally, this
process necessitates skilled professionals and specialized software for the
modeling, texturing, and rendering of 3D objects. However, the rising demand
for 3D assets in gaming and virtual reality (VR) has led to the creation of
accessible image-to-3D technologies, allowing non-professionals to produce 3D
content and decreasing dependence on expert input. Existing methods for 3D
content generation struggle to simultaneously achieve detailed textures and
strong geometric consistency. We introduce a novel 3D content creation
framework, ScalingGaussian, which combines 3D and 2D diffusion models to
achieve detailed textures and geometric consistency in generated 3D assets.
Initially, a 3D diffusion model generates point clouds, which are then
densified through a process of selecting local regions, introducing Gaussian
noise, followed by using local density-weighted selection. To refine the 3D
gaussians, we utilize a 2D diffusion model with Score Distillation Sampling
(SDS) loss, guiding the 3D Gaussians to clone and split. Finally, the 3D
Gaussians are converted into meshes, and the surface textures are optimized
using Mean Square Error(MSE) and Gradient Profile Prior(GPP) losses. Our method
addresses the common issue of sparse point clouds in 3D diffusion, resulting in
improved geometric structure and detailed textures. Experiments on image-to-3D
tasks demonstrate that our approach efficiently generates high-quality 3D
assets.

Comments:
- 14 pages

---

## GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale  Image Super-Resolution

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-25 | Jintong Hu, Bin Xia, Bin Chen, Wenming Yang, Lei Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2407.18046v1){: .btn .btn-green } |

**Abstract**: Implicit neural representations (INRs) have significantly advanced the field
of arbitrary-scale super-resolution (ASSR) of images. Most existing INR-based
ASSR networks first extract features from the given low-resolution image using
an encoder, and then render the super-resolved result via a multi-layer
perceptron decoder. Although these approaches have shown promising results,
their performance is constrained by the limited representation ability of
discrete latent codes in the encoded features. In this paper, we propose a
novel ASSR method named GaussianSR that overcomes this limitation through 2D
Gaussian Splatting (2DGS). Unlike traditional methods that treat pixels as
discrete points, GaussianSR represents each pixel as a continuous Gaussian
field. The encoded features are simultaneously refined and upsampled by
rendering the mutually stacked Gaussian fields. As a result, long-range
dependencies are established to enhance representation ability. In addition, a
classifier is developed to dynamically assign Gaussian kernels to all pixels to
further improve flexibility. All components of GaussianSR (i.e., encoder,
classifier, Gaussian kernels, and decoder) are jointly learned end-to-end.
Experiments demonstrate that GaussianSR achieves superior ASSR performance with
fewer parameters than existing methods while enjoying interpretable and
content-aware feature aggregations.

Comments:
- 13 pages, 12 figures

---

## 3D Gaussian Splatting: Survey, Technologies, Challenges, and  Opportunities

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-24 | Yanqi Bao, Tianyu Ding, Jing Huo, Yaoli Liu, Yuxin Li, Wenbin Li, Yang Gao, Jiebo Luo | cs.CV | [PDF](http://arxiv.org/pdf/2407.17418v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the
potential to become a mainstream method for 3D representations. It can
effectively transform multi-view images into explicit 3D Gaussian
representations through efficient training, and achieve real-time rendering of
novel views. This survey aims to analyze existing 3DGS-related works from
multiple intersecting perspectives, including related tasks, technologies,
challenges, and opportunities. The primary objective is to provide newcomers
with a rapid understanding of the field and to assist researchers in
methodically organizing existing technologies and challenges. Specifically, we
delve into the optimization, application, and extension of 3DGS, categorizing
them based on their focuses or motivations. Additionally, we summarize and
classify nine types of technical modules and corresponding improvements
identified in existing works. Based on these analyses, we further examine the
common challenges and technologies across various tasks, proposing potential
research opportunities.



---

## SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View  Consistency

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-24 | Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, Varun Jampani | cs.CV | [PDF](http://arxiv.org/pdf/2407.17470v1){: .btn .btn-green } |

**Abstract**: We present Stable Video 4D (SV4D), a latent video diffusion model for
multi-frame and multi-view consistent dynamic 3D content generation. Unlike
previous methods that rely on separately trained generative models for video
generation and novel view synthesis, we design a unified diffusion model to
generate novel view videos of dynamic 3D objects. Specifically, given a
monocular reference video, SV4D generates novel views for each video frame that
are temporally consistent. We then use the generated novel view videos to
optimize an implicit 4D representation (dynamic NeRF) efficiently, without the
need for cumbersome SDS-based optimization used in most prior works. To train
our unified novel view video generation model, we curated a dynamic 3D object
dataset from the existing Objaverse dataset. Extensive experimental results on
multiple datasets and user studies demonstrate SV4D's state-of-the-art
performance on novel-view video synthesis as well as 4D generation compared to
prior works.

Comments:
- Project page: https://sv4d.github.io/

---

## DHGS: Decoupled Hybrid Gaussian Splatting for Driving Scene

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-23 | Xi Shi, Lingli Chen, Peng Wei, Xi Wu, Tian Jiang, Yonggang Luo, Lecheng Xie | cs.CV | [PDF](http://arxiv.org/pdf/2407.16600v2){: .btn .btn-green } |

**Abstract**: Existing Gaussian splatting methods often fall short in achieving
satisfactory novel view synthesis in driving scenes, primarily due to the
absence of crafty design and geometric constraints for the involved elements.
This paper introduces a novel neural rendering method termed Decoupled Hybrid
Gaussian Splatting (DHGS), targeting at promoting the rendering quality of
novel view synthesis for static driving scenes. The novelty of this work lies
in the decoupled and hybrid pixel-level blender for road and non-road layers,
without the conventional unified differentiable rendering logic for the entire
scene, while still maintaining consistent and continuous superimposition
through the proposed depth-ordered hybrid rendering strategy. Additionally, an
implicit road representation comprised of a Signed Distance Field (SDF) is
trained to supervise the road surface with subtle geometric attributes.
Accompanied by the use of auxiliary transmittance loss and consistency loss,
novel images with imperceptible boundary and elevated fidelity are ultimately
obtained. Substantial experiments on the Waymo dataset prove that DHGS
outperforms the state-of-the-art methods. The project page where more video
evidences are given is: https://ironbrotherstyle.github.io/dhgs_web.

Comments:
- 13 pages, 14 figures, conference

---

## DreamDissector: Learning Disentangled Text-to-3D Generation from 2D  Diffusion Priors

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-23 | Zizheng Yan, Jiapeng Zhou, Fanpeng Meng, Yushuang Wu, Lingteng Qiu, Zisheng Ye, Shuguang Cui, Guanying Chen, Xiaoguang Han | cs.CV | [PDF](http://arxiv.org/pdf/2407.16260v1){: .btn .btn-green } |

**Abstract**: Text-to-3D generation has recently seen significant progress. To enhance its
practicality in real-world applications, it is crucial to generate multiple
independent objects with interactions, similar to layer-compositing in 2D image
editing. However, existing text-to-3D methods struggle with this task, as they
are designed to generate either non-independent objects or independent objects
lacking spatially plausible interactions. Addressing this, we propose
DreamDissector, a text-to-3D method capable of generating multiple independent
objects with interactions. DreamDissector accepts a multi-object text-to-3D
NeRF as input and produces independent textured meshes. To achieve this, we
introduce the Neural Category Field (NeCF) for disentangling the input NeRF.
Additionally, we present the Category Score Distillation Sampling (CSDS),
facilitated by a Deep Concept Mining (DCM) module, to tackle the concept gap
issue in diffusion models. By leveraging NeCF and CSDS, we can effectively
derive sub-NeRFs from the original scene. Further refinement enhances geometry
and texture. Our experimental results validate the effectiveness of
DreamDissector, providing users with novel means to control 3D synthesis at the
object level and potentially opening avenues for various creative applications
in the future.

Comments:
- ECCV 2024. Project page: https://chester256.github.io/dreamdissector

---

## Integrating Meshes and 3D Gaussians for Indoor Scene Reconstruction with  SAM Mask Guidance

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-23 | Jiyeop Kim, Jongwoo Lim | cs.CV | [PDF](http://arxiv.org/pdf/2407.16173v1){: .btn .btn-green } |

**Abstract**: We present a novel approach for 3D indoor scene reconstruction that combines
3D Gaussian Splatting (3DGS) with mesh representations. We use meshes for the
room layout of the indoor scene, such as walls, ceilings, and floors, while
employing 3D Gaussians for other objects. This hybrid approach leverages the
strengths of both representations, offering enhanced flexibility and ease of
editing. However, joint training of meshes and 3D Gaussians is challenging
because it is not clear which primitive should affect which part of the
rendered image. Objects close to the room layout often struggle during
training, particularly when the room layout is textureless, which can lead to
incorrect optimizations and unnecessary 3D Gaussians. To overcome these
challenges, we employ Segment Anything Model (SAM) to guide the selection of
primitives. The SAM mask loss enforces each instance to be represented by
either Gaussians or meshes, ensuring clear separation and stable training.
Furthermore, we introduce an additional densification stage without resetting
the opacity after the standard densification. This stage mitigates the
degradation of image quality caused by a limited number of 3D Gaussians after
the standard densification.



---

## HDRSplat: Gaussian Splatting for High Dynamic Range 3D Scene  Reconstruction from Raw Images

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-23 | Shreyas Singh, Aryan Garg, Kaushik Mitra | cs.CV | [PDF](http://arxiv.org/pdf/2407.16503v1){: .btn .btn-green } |

**Abstract**: The recent advent of 3D Gaussian Splatting (3DGS) has revolutionized the 3D
scene reconstruction space enabling high-fidelity novel view synthesis in
real-time. However, with the exception of RawNeRF, all prior 3DGS and
NeRF-based methods rely on 8-bit tone-mapped Low Dynamic Range (LDR) images for
scene reconstruction. Such methods struggle to achieve accurate reconstructions
in scenes that require a higher dynamic range. Examples include scenes captured
in nighttime or poorly lit indoor spaces having a low signal-to-noise ratio, as
well as daylight scenes with shadow regions exhibiting extreme contrast. Our
proposed method HDRSplat tailors 3DGS to train directly on 14-bit linear raw
images in near darkness which preserves the scenes' full dynamic range and
content. Our key contributions are two-fold: Firstly, we propose a linear HDR
space-suited loss that effectively extracts scene information from noisy dark
regions and nearly saturated bright regions simultaneously, while also handling
view-dependent colors without increasing the degree of spherical harmonics.
Secondly, through careful rasterization tuning, we implicitly overcome the
heavy reliance and sensitivity of 3DGS on point cloud initialization. This is
critical for accurate reconstruction in regions of low texture, high depth of
field, and low illumination. HDRSplat is the fastest method to date that does
14-bit (HDR) 3D scene reconstruction in $\le$15 minutes/scene ($\sim$30x faster
than prior state-of-the-art RawNeRF). It also boasts the fastest inference
speed at $\ge$120fps. We further demonstrate the applicability of our HDR scene
reconstruction by showcasing various applications like synthetic defocus, dense
depth map extraction, and post-capture control of exposure, tone-mapping and
view-point.



---

## PAV: Personalized Head Avatar from Unstructured Video Collection

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-22 | Akin Caliskan, Berkay Kicanaoglu, Hyeongwoo Kim | cs.CV | [PDF](http://arxiv.org/pdf/2407.21047v1){: .btn .btn-green } |

**Abstract**: We propose PAV, Personalized Head Avatar for the synthesis of human faces
under arbitrary viewpoints and facial expressions. PAV introduces a method that
learns a dynamic deformable neural radiance field (NeRF), in particular from a
collection of monocular talking face videos of the same character under various
appearance and shape changes. Unlike existing head NeRF methods that are
limited to modeling such input videos on a per-appearance basis, our method
allows for learning multi-appearance NeRFs, introducing appearance embedding
for each input video via learnable latent neural features attached to the
underlying geometry. Furthermore, the proposed appearance-conditioned density
formulation facilitates the shape variation of the character, such as facial
hair and soft tissues, in the radiance field prediction. To the best of our
knowledge, our approach is the first dynamic deformable NeRF framework to model
appearance and shape variations in a single unified network for
multi-appearances of the same subject. We demonstrate experimentally that PAV
outperforms the baseline method in terms of visual rendering quality in our
quantitative and qualitative studies on various subjects.

Comments:
- Accepted to ECCV24. Project page:
  https://akincaliskan3d.github.io/PAV

---

## BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis  in Large-scale Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-22 | Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, Yu-Lun Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.15848v1){: .btn .btn-green } |

**Abstract**: While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality,
their protracted training duration remains a limitation. Generalizable and
MVS-based NeRFs, although capable of mitigating training time, often incur
tradeoffs in quality. This paper presents a novel approach called BoostMVSNeRFs
to enhance the rendering quality of MVS-based NeRFs in large-scale scenes. We
first identify limitations in MVS-based NeRF methods, such as restricted
viewport coverage and artifacts due to limited input views. Then, we address
these limitations by proposing a new method that selects and combines multiple
cost volumes during volume rendering. Our method does not require training and
can adapt to any MVS-based NeRF methods in a feed-forward fashion to improve
rendering quality. Furthermore, our approach is also end-to-end trainable,
allowing fine-tuning on specific scenes. We demonstrate the effectiveness of
our method through experiments on large-scale datasets, showing significant
rendering quality improvements in large-scale scenes and unbounded outdoor
scenarios. We release the source code of BoostMVSNeRFs at
https://su-terry.github.io/BoostMVSNeRFs/.

Comments:
- SIGGRAPH 2024 Conference Papers. Project page:
  https://su-terry.github.io/BoostMVSNeRFs/

---

## Enhancement of 3D Gaussian Splatting using Raw Mesh for Photorealistic  Recreation of Architectures

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-22 | Ruizhe Wang, Chunliang Hua, Tomakayev Shingys, Mengyuan Niu, Qingxin Yang, Lizhong Gao, Yi Zheng, Junyan Yang, Qiao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2407.15435v1){: .btn .btn-green } |

**Abstract**: The photorealistic reconstruction and rendering of architectural scenes have
extensive applications in industries such as film, games, and transportation.
It also plays an important role in urban planning, architectural design, and
the city's promotion, especially in protecting historical and cultural relics.
The 3D Gaussian Splatting, due to better performance over NeRF, has become a
mainstream technology in 3D reconstruction. Its only input is a set of images
but it relies heavily on geometric parameters computed by the SfM process. At
the same time, there is an existing abundance of raw 3D models, that could
inform the structural perception of certain buildings but cannot be applied. In
this paper, we propose a straightforward method to harness these raw 3D models
to guide 3D Gaussians in capturing the basic shape of the building and improve
the visual quality of textures and details when photos are captured
non-systematically. This exploration opens up new possibilities for improving
the effectiveness of 3D reconstruction techniques in the field of architectural
design.



---

## 6DGS: 6D Pose Estimation from a Single Image and a 3D Gaussian Splatting  Model

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-22 | Matteo Bortolon, Theodore Tsesmelis, Stuart James, Fabio Poiesi, Alessio Del Bue | cs.CV | [PDF](http://arxiv.org/pdf/2407.15484v1){: .btn .btn-green } |

**Abstract**: We propose 6DGS to estimate the camera pose of a target RGB image given a 3D
Gaussian Splatting (3DGS) model representing the scene. 6DGS avoids the
iterative process typical of analysis-by-synthesis methods (e.g. iNeRF) that
also require an initialization of the camera pose in order to converge.
Instead, our method estimates a 6DoF pose by inverting the 3DGS rendering
process. Starting from the object surface, we define a radiant Ellicell that
uniformly generates rays departing from each ellipsoid that parameterize the
3DGS model. Each Ellicell ray is associated with the rendering parameters of
each ellipsoid, which in turn is used to obtain the best bindings between the
target image pixels and the cast rays. These pixel-ray bindings are then ranked
to select the best scoring bundle of rays, which their intersection provides
the camera center and, in turn, the camera rotation. The proposed solution
obviates the necessity of an "a priori" pose for initialization, and it solves
6DoF pose estimation in closed form, without the need for iterations. Moreover,
compared to the existing Novel View Synthesis (NVS) baselines for pose
estimation, 6DGS can improve the overall average rotational accuracy by 12% and
translation accuracy by 22% on real scenes, despite not requiring any
initialization pose. At the same time, our method operates near real-time,
reaching 15fps on consumer hardware.

Comments:
- Project page: https://mbortolon97.github.io/6dgs/ Accepted to ECCV
  2024

---

## HoloDreamer: Holistic 3D Panoramic World Generation from Text  Descriptions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-21 | Haiyang Zhou, Xinhua Cheng, Wangbo Yu, Yonghong Tian, Li Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2407.15187v1){: .btn .btn-green } |

**Abstract**: 3D scene generation is in high demand across various domains, including
virtual reality, gaming, and the film industry. Owing to the powerful
generative capabilities of text-to-image diffusion models that provide reliable
priors, the creation of 3D scenes using only text prompts has become viable,
thereby significantly advancing researches in text-driven 3D scene generation.
In order to obtain multiple-view supervision from 2D diffusion models,
prevailing methods typically employ the diffusion model to generate an initial
local image, followed by iteratively outpainting the local image using
diffusion models to gradually generate scenes. Nevertheless, these
outpainting-based approaches prone to produce global inconsistent scene
generation results without high degree of completeness, restricting their
broader applications. To tackle these problems, we introduce HoloDreamer, a
framework that first generates high-definition panorama as a holistic
initialization of the full 3D scene, then leverage 3D Gaussian Splatting
(3D-GS) to quickly reconstruct the 3D scene, thereby facilitating the creation
of view-consistent and fully enclosed 3D scenes. Specifically, we propose
Stylized Equirectangular Panorama Generation, a pipeline that combines multiple
diffusion models to enable stylized and detailed equirectangular panorama
generation from complex text prompts. Subsequently, Enhanced Two-Stage Panorama
Reconstruction is introduced, conducting a two-stage optimization of 3D-GS to
inpaint the missing region and enhance the integrity of the scene.
Comprehensive experiments demonstrated that our method outperforms prior works
in terms of overall visual consistency and harmony as well as reconstruction
quality and rendering robustness when generating fully enclosed scenes.

Comments:
- Homepage: https://zhouhyocean.github.io/holodreamer

---

## Realistic Surgical Image Dataset Generation Based On 3D Gaussian  Splatting


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-20 | Tianle Zeng, Gerardo Loza Galindo, Junlei Hu, Pietro Valdastri, Dominic Jones | cs.CV | [PDF](http://arxiv.org/pdf/2407.14846v1){: .btn .btn-green } |

**Abstract**: Computer vision technologies markedly enhance the automation capabilities of
robotic-assisted minimally invasive surgery (RAMIS) through advanced tool
tracking, detection, and localization. However, the limited availability of
comprehensive surgical datasets for training represents a significant challenge
in this field. This research introduces a novel method that employs 3D Gaussian
Splatting to generate synthetic surgical datasets. We propose a method for
extracting and combining 3D Gaussian representations of surgical instruments
and background operating environments, transforming and combining them to
generate high-fidelity synthetic surgical scenarios. We developed a data
recording system capable of acquiring images alongside tool and camera poses in
a surgical scene. Using this pose data, we synthetically replicate the scene,
thereby enabling direct comparisons of the synthetic image quality (29.592
PSNR). As a further validation, we compared two YOLOv5 models trained on the
synthetic and real data, respectively, and assessed their performance in an
unseen real-world test dataset. Comparing the performances, we observe an
improvement in neural network performance, with the synthetic-trained model
outperforming the real-world trained model by 12%, testing both on real-world
data.

Comments:
- This paper has already been accepted by INTERNATIONAL CONFERENCE ON
  MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION (MICCAI 2024)

---

## A Benchmark for Gaussian Splatting Compression and Quality Assessment  Study

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-19 | Qi Yang, Kaifa Yang, Yuke Xing, Yiling Xu, Zhu Li | cs.CV | [PDF](http://arxiv.org/pdf/2407.14197v1){: .btn .btn-green } |

**Abstract**: To fill the gap of traditional GS compression method, in this paper, we first
propose a simple and effective GS data compression anchor called Graph-based GS
Compression (GGSC). GGSC is inspired by graph signal processing theory and uses
two branches to compress the primitive center and attributes. We split the
whole GS sample via KDTree and clip the high-frequency components after the
graph Fourier transform. Followed by quantization, G-PCC and adaptive
arithmetic coding are used to compress the primitive center and attribute
residual matrix to generate the bitrate file. GGSS is the first work to explore
traditional GS compression, with advantages that can reveal the GS distortion
characteristics corresponding to typical compression operation, such as
high-frequency clipping and quantization. Second, based on GGSC, we create a GS
Quality Assessment dataset (GSQA) with 120 samples. A subjective experiment is
conducted in a laboratory environment to collect subjective scores after
rendering GS into Processed Video Sequences (PVS). We analyze the
characteristics of different GS distortions based on Mean Opinion Scores (MOS),
demonstrating the sensitivity of different attributes distortion to visual
quality. The GGSC code and the dataset, including GS samples, MOS, and PVS, are
made publicly available at https://github.com/Qi-Yangsjtu/GGSC.



---

## DirectL: Efficient Radiance Fields Rendering for 3D Light Field Displays

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-19 | Zongyuan Yang, Baolin Liu, Yingde Song, Yongping Xiong, Lan Yi, Zhaohe Zhang, Xunbo Yu | cs.GR | [PDF](http://arxiv.org/pdf/2407.14053v1){: .btn .btn-green } |

**Abstract**: Autostereoscopic display, despite decades of development, has not achieved
extensive application, primarily due to the daunting challenge of 3D content
creation for non-specialists. The emergence of Radiance Field as an innovative
3D representation has markedly revolutionized the domains of 3D reconstruction
and generation. This technology greatly simplifies 3D content creation for
common users, broadening the applicability of Light Field Displays (LFDs).
However, the combination of these two fields remains largely unexplored. The
standard paradigm to create optimal content for parallax-based light field
displays demands rendering at least 45 slightly shifted views preferably at
high resolution per frame, a substantial hurdle for real-time rendering. We
introduce DirectL, a novel rendering paradigm for Radiance Fields on 3D
displays. We thoroughly analyze the interweaved mapping of spatial rays to
screen subpixels, precisely determine the light rays entering the human eye,
and propose subpixel repurposing to significantly reduce the pixel count
required for rendering. Tailored for the two predominant radiance
fields--Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS), we
propose corresponding optimized rendering pipelines that directly render the
light field images instead of multi-view images. Extensive experiments across
various displays and user study demonstrate that DirectL accelerates rendering
by up to 40 times compared to the standard paradigm without sacrificing visual
quality. Its rendering process-only modification allows seamless integration
into subsequent radiance field tasks. Finally, we integrate DirectL into
diverse applications, showcasing the stunning visual experiences and the
synergy between LFDs and Radiance Fields, which unveils tremendous potential
for commercialization applications. \href{direct-l.github.io}{\textbf{Project
Homepage}



---

## GaussianBeV: 3D Gaussian Representation meets Perception Models for BeV  Segmentation


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-19 | Florian Chabot, Nicolas Granger, Guillaume Lapouge | cs.CV | [PDF](http://arxiv.org/pdf/2407.14108v1){: .btn .btn-green } |

**Abstract**: The Bird's-eye View (BeV) representation is widely used for 3D perception
from multi-view camera images. It allows to merge features from different
cameras into a common space, providing a unified representation of the 3D
scene. The key component is the view transformer, which transforms image views
into the BeV. However, actual view transformer methods based on geometry or
cross-attention do not provide a sufficiently detailed representation of the
scene, as they use a sub-sampling of the 3D space that is non-optimal for
modeling the fine structures of the environment. In this paper, we propose
GaussianBeV, a novel method for transforming image features to BeV by finely
representing the scene using a set of 3D gaussians located and oriented in 3D
space. This representation is then splattered to produce the BeV feature map by
adapting recent advances in 3D representation rendering based on gaussian
splatting. GaussianBeV is the first approach to use this 3D gaussian modeling
and 3D scene rendering process online, i.e. without optimizing it on a specific
scene and directly integrated into a single stage model for BeV scene
understanding. Experiments show that the proposed representation is highly
effective and place GaussianBeV as the new state-of-the-art on the BeV semantic
segmentation task on the nuScenes dataset.



---

## Semantic Communications for 3D Human Face Transmission with Neural  Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-19 | Guanlin Wu, Zhonghao Lyu, Juyong Zhang, Jie Xu | eess.IV | [PDF](http://arxiv.org/pdf/2407.13992v1){: .btn .btn-green } |

**Abstract**: This paper investigates the transmission of three-dimensional (3D) human face
content for immersive communication over a rate-constrained
transmitter-receiver link. We propose a new framework named NeRF-SeCom, which
leverages neural radiance fields (NeRF) and semantic communications to improve
the quality of 3D visualizations while minimizing the communication overhead.
In the NeRF-SeCom framework, we first train a NeRF face model based on the
NeRFBlendShape method, which is pre-shared between the transmitter and receiver
as the semantic knowledge base to facilitate the real-time transmission. Next,
with knowledge base, the transmitter extracts and sends only the essential
semantic features for the receiver to reconstruct 3D face in real time. To
optimize the transmission efficiency, we classify the expression features into
static and dynamic types. Over each video chunk, static features are
transmitted once for all frames, whereas dynamic features are transmitted over
a portion of frames to adhere to rate constraints. Additionally, we propose a
feature prediction mechanism, which allows the receiver to predict the dynamic
features for frames that are not transmitted. Experiments show that our
proposed NeRF-SeCom framework significantly outperforms benchmark methods in
delivering high-quality 3D visualizations of human faces.

Comments:
- 6 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:2405.12155

---

## HOTS3D: Hyper-Spherical Optimal Transport for Semantic Alignment of  Text-to-3D Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-19 | Zezeng Li, Weimin Wang, WenHai Li, Na Lei, Xianfeng Gu | cs.CV | [PDF](http://arxiv.org/pdf/2407.14419v1){: .btn .btn-green } |

**Abstract**: Recent CLIP-guided 3D generation methods have achieved promising results but
struggle with generating faithful 3D shapes that conform with input text due to
the gap between text and image embeddings. To this end, this paper proposes
HOTS3D which makes the first attempt to effectively bridge this gap by aligning
text features to the image features with spherical optimal transport (SOT).
However, in high-dimensional situations, solving the SOT remains a challenge.
To obtain the SOT map for high-dimensional features obtained from CLIP encoding
of two modalities, we mathematically formulate and derive the solution based on
Villani's theorem, which can directly align two hyper-sphere distributions
without manifold exponential maps. Furthermore, we implement it by leveraging
input convex neural networks (ICNNs) for the optimal Kantorovich potential.
With the optimally mapped features, a diffusion-based generator and a
Nerf-based decoder are subsequently utilized to transform them into 3D shapes.
Extensive qualitative and qualitative comparisons with state-of-the-arts
demonstrate the superiority of the proposed HOTS3D for 3D shape generation,
especially on the consistency with text semantics.



---

## GeometrySticker: Enabling Ownership Claim of Recolorized Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-18 | Xiufeng Huang, Ka Chun Cheung, Simon See, Renjie Wan | cs.CV | [PDF](http://arxiv.org/pdf/2407.13390v1){: .btn .btn-green } |

**Abstract**: Remarkable advancements in the recolorization of Neural Radiance Fields
(NeRF) have simplified the process of modifying NeRF's color attributes. Yet,
with the potential of NeRF to serve as shareable digital assets, there's a
concern that malicious users might alter the color of NeRF models and falsely
claim the recolorized version as their own. To safeguard against such breaches
of ownership, enabling original NeRF creators to establish rights over
recolorized NeRF is crucial. While approaches like CopyRNeRF have been
introduced to embed binary messages into NeRF models as digital signatures for
copyright protection, the process of recolorization can remove these binary
messages. In our paper, we present GeometrySticker, a method for seamlessly
integrating binary messages into the geometry components of radiance fields,
akin to applying a sticker. GeometrySticker can embed binary messages into NeRF
models while preserving the effectiveness of these messages against
recolorization. Our comprehensive studies demonstrate that GeometrySticker is
adaptable to prevalent NeRF architectures and maintains a commendable level of
robustness against various distortions. Project page:
https://kevinhuangxf.github.io/GeometrySticker/.



---

## Connecting Consistency Distillation to Score Distillation for Text-to-3D  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-18 | Zongrui Li, Minghui Hu, Qian Zheng, Xudong Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2407.13584v2){: .btn .btn-green } |

**Abstract**: Although recent advancements in text-to-3D generation have significantly
improved generation quality, issues like limited level of detail and low
fidelity still persist, which requires further improvement. To understand the
essence of those issues, we thoroughly analyze current score distillation
methods by connecting theories of consistency distillation to score
distillation. Based on the insights acquired through analysis, we propose an
optimization framework, Guided Consistency Sampling (GCS), integrated with 3D
Gaussian Splatting (3DGS) to alleviate those issues. Additionally, we have
observed the persistent oversaturation in the rendered views of generated 3D
assets. From experiments, we find that it is caused by unwanted accumulated
brightness in 3DGS during optimization. To mitigate this issue, we introduce a
Brightness-Equalized Generation (BEG) scheme in 3DGS rendering. Experimental
results demonstrate that our approach generates 3D assets with more details and
higher fidelity than state-of-the-art methods. The codes are released at
https://github.com/LMozart/ECCV2024-GCS-BEG.

Comments:
- Paper accepted by ECCV2024

---

## EaDeblur-GS: Event assisted 3D Deblur Reconstruction with Gaussian  Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-18 | Yuchen Weng, Zhengwen Shen, Ruofan Chen, Qi Wang, Jun Wang | cs.CV | [PDF](http://arxiv.org/pdf/2407.13520v1){: .btn .btn-green } |

**Abstract**: 3D deblurring reconstruction techniques have recently seen significant
advancements with the development of Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS). Although these techniques can recover relatively
clear 3D reconstructions from blurry image inputs, they still face limitations
in handling severe blurring and complex camera motion. To address these issues,
we propose Event-assisted 3D Deblur Reconstruction with Gaussian Splatting
(EaDeblur-GS), which integrates event camera data to enhance the robustness of
3DGS against motion blur. By employing an Adaptive Deviation Estimator (ADE)
network to estimate Gaussian center deviations and using novel loss functions,
EaDeblur-GS achieves sharp 3D reconstructions in real-time, demonstrating
performance comparable to state-of-the-art methods.



---

## KFD-NeRF: Rethinking Dynamic NeRF with Kalman Filter

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-18 | Yifan Zhan, Zhuoxiao Li, Muyao Niu, Zhihang Zhong, Shohei Nobuhara, Ko Nishino, Yinqiang Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2407.13185v1){: .btn .btn-green } |

**Abstract**: We introduce KFD-NeRF, a novel dynamic neural radiance field integrated with
an efficient and high-quality motion reconstruction framework based on Kalman
filtering. Our key idea is to model the dynamic radiance field as a dynamic
system whose temporally varying states are estimated based on two sources of
knowledge: observations and predictions. We introduce a novel plug-in Kalman
filter guided deformation field that enables accurate deformation estimation
from scene observations and predictions. We use a shallow Multi-Layer
Perceptron (MLP) for observations and model the motion as locally linear to
calculate predictions with motion equations. To further enhance the performance
of the observation MLP, we introduce regularization in the canonical space to
facilitate the network's ability to learn warping for different frames.
Additionally, we employ an efficient tri-plane representation for encoding the
canonical space, which has been experimentally demonstrated to converge quickly
with high quality. This enables us to use a shallower observation MLP,
consisting of just two layers in our implementation. We conduct experiments on
synthetic and real data and compare with past dynamic NeRF methods. Our
KFD-NeRF demonstrates similar or even superior rendering performance within
comparable computational time and achieves state-of-the-art view synthesis
performance with thorough training.

Comments:
- accepted to eccv2024

---

## Generalizable Human Gaussians for Sparse View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-17 | Youngjoong Kwon, Baole Fang, Yixing Lu, Haoye Dong, Cheng Zhang, Francisco Vicente Carrasco, Albert Mosella-Montoro, Jianjin Xu, Shingo Takagi, Daeil Kim, Aayush Prakash, Fernando De la Torre | cs.CV | [PDF](http://arxiv.org/pdf/2407.12777v1){: .btn .btn-green } |

**Abstract**: Recent progress in neural rendering has brought forth pioneering methods,
such as NeRF and Gaussian Splatting, which revolutionize view rendering across
various domains like AR/VR, gaming, and content creation. While these methods
excel at interpolating {\em within the training data}, the challenge of
generalizing to new scenes and objects from very sparse views persists.
Specifically, modeling 3D humans from sparse views presents formidable hurdles
due to the inherent complexity of human geometry, resulting in inaccurate
reconstructions of geometry and textures. To tackle this challenge, this paper
leverages recent advancements in Gaussian Splatting and introduces a new method
to learn generalizable human Gaussians that allows photorealistic and accurate
view-rendering of a new human subject from a limited set of sparse views in a
feed-forward manner. A pivotal innovation of our approach involves
reformulating the learning of 3D Gaussian parameters into a regression process
defined on the 2D UV space of a human template, which allows leveraging the
strong geometry prior and the advantages of 2D convolutions. In addition, a
multi-scaffold is proposed to effectively represent the offset details. Our
method outperforms recent methods on both within-dataset generalization as well
as cross-dataset generalization settings.



---

## Invertible Neural Warp for NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-17 | Shin-Fang Chng, Ravi Garg, Hemanth Saratchandran, Simon Lucey | cs.CV | [PDF](http://arxiv.org/pdf/2407.12354v1){: .btn .btn-green } |

**Abstract**: This paper tackles the simultaneous optimization of pose and Neural Radiance
Fields (NeRF). Departing from the conventional practice of using explicit
global representations for camera pose, we propose a novel overparameterized
representation that models camera poses as learnable rigid warp functions. We
establish that modeling the rigid warps must be tightly coupled with
constraints and regularization imposed. Specifically, we highlight the critical
importance of enforcing invertibility when learning rigid warp functions via
neural network and propose the use of an Invertible Neural Network (INN)
coupled with a geometry-informed constraint for this purpose. We present
results on synthetic and real-world datasets, and demonstrate that our approach
outperforms existing baselines in terms of pose estimation and high-fidelity
reconstruction due to enhanced optimization convergence.

Comments:
- Accepted to ECCV 2024. Project page:
  https://sfchng.github.io/ineurowarping-github.io/

---

## SG-NeRF: Neural Surface Reconstruction with Scene Graph Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-17 | Yiyang Chen, Siyan Dong, Xulong Wang, Lulu Cai, Youyi Zheng, Yanchao Yang | cs.CV | [PDF](http://arxiv.org/pdf/2407.12667v1){: .btn .btn-green } |

**Abstract**: 3D surface reconstruction from images is essential for numerous applications.
Recently, Neural Radiance Fields (NeRFs) have emerged as a promising framework
for 3D modeling. However, NeRFs require accurate camera poses as input, and
existing methods struggle to handle significantly noisy pose estimates (i.e.,
outliers), which are commonly encountered in real-world scenarios. To tackle
this challenge, we present a novel approach that optimizes radiance fields with
scene graphs to mitigate the influence of outlier poses. Our method
incorporates an adaptive inlier-outlier confidence estimation scheme based on
scene graphs, emphasizing images of high compatibility with the neighborhood
and consistency in the rendering quality. We also introduce an effective
intersection-over-union (IoU) loss to optimize the camera pose and surface
geometry, together with a coarse-to-fine strategy to facilitate the training.
Furthermore, we propose a new dataset containing typical outlier poses for a
detailed evaluation. Experimental results on various datasets consistently
demonstrate the effectiveness and superiority of our method over existing
approaches, showcasing its robustness in handling outliers and producing
high-quality 3D reconstructions. Our code and data are available at:
\url{https://github.com/Iris-cyy/SG-NeRF}.

Comments:
- ECCV 2024

---

## Splatfacto-W: A Nerfstudio Implementation of Gaussian Splatting for  Unconstrained Photo Collections

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-17 | Congrong Xu, Justin Kerr, Angjoo Kanazawa | cs.CV | [PDF](http://arxiv.org/pdf/2407.12306v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis from unconstrained in-the-wild image collections remains
a significant yet challenging task due to photometric variations and transient
occluders that complicate accurate scene reconstruction. Previous methods have
approached these issues by integrating per-image appearance features embeddings
in Neural Radiance Fields (NeRFs). Although 3D Gaussian Splatting (3DGS) offers
faster training and real-time rendering, adapting it for unconstrained image
collections is non-trivial due to the substantially different architecture. In
this paper, we introduce Splatfacto-W, an approach that integrates per-Gaussian
neural color features and per-image appearance embeddings into the
rasterization process, along with a spherical harmonics-based background model
to represent varying photometric appearances and better depict backgrounds. Our
key contributions include latent appearance modeling, efficient transient
object handling, and precise background modeling. Splatfacto-W delivers
high-quality, real-time novel view synthesis with improved scene consistency in
in-the-wild scenarios. Our method improves the Peak Signal-to-Noise Ratio
(PSNR) by an average of 5.3 dB compared to 3DGS, enhances training speed by 150
times compared to NeRF-based methods, and achieves a similar rendering speed to
3DGS. Additional video results and code integrated into Nerfstudio are
available at https://kevinxu02.github.io/splatfactow/.

Comments:
- 9 pages

---

## InfoNorm: Mutual Information Shaping of Normals for Sparse-View  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-17 | Xulong Wang, Siyan Dong, Youyi Zheng, Yanchao Yang | cs.CV | [PDF](http://arxiv.org/pdf/2407.12661v1){: .btn .btn-green } |

**Abstract**: 3D surface reconstruction from multi-view images is essential for scene
understanding and interaction. However, complex indoor scenes pose challenges
such as ambiguity due to limited observations. Recent implicit surface
representations, such as Neural Radiance Fields (NeRFs) and signed distance
functions (SDFs), employ various geometric priors to resolve the lack of
observed information. Nevertheless, their performance heavily depends on the
quality of the pre-trained geometry estimation models. To ease such dependence,
we propose regularizing the geometric modeling by explicitly encouraging the
mutual information among surface normals of highly correlated scene points. In
this way, the geometry learning process is modulated by the second-order
correlations from noisy (first-order) geometric priors, thus eliminating the
bias due to poor generalization. Additionally, we introduce a simple yet
effective scheme that utilizes semantic and geometric features to identify
correlated points, enhancing their mutual information accordingly. The proposed
technique can serve as a plugin for SDF-based neural surface representations.
Our experiments demonstrate the effectiveness of the proposed in improving the
surface reconstruction quality of major states of the arts. Our code is
available at: \url{https://github.com/Muliphein/InfoNorm}.

Comments:
- ECCV 2024

---

## DreamCatalyst: Fast and High-Quality 3D Editing via Controlling  Editability and Identity Preservation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Jiwook Kim, Seonho Lee, Jaeyo Shin, Jiho Choi, Hyunjung Shim | cs.CV | [PDF](http://arxiv.org/pdf/2407.11394v1){: .btn .btn-green } |

**Abstract**: Score distillation sampling (SDS) has emerged as an effective framework in
text-driven 3D editing tasks due to its inherent 3D consistency. However,
existing SDS-based 3D editing methods suffer from extensive training time and
lead to low-quality results, primarily because these methods deviate from the
sampling dynamics of diffusion models. In this paper, we propose DreamCatalyst,
a novel framework that interprets SDS-based editing as a diffusion reverse
process. Our objective function considers the sampling dynamics, thereby making
the optimization process of DreamCatalyst an approximation of the diffusion
reverse process in editing tasks. DreamCatalyst aims to reduce training time
and improve editing quality. DreamCatalyst presents two modes: (1) a faster
mode, which edits the NeRF scene in only about 25 minutes, and (2) a
high-quality mode, which produces superior results in less than 70 minutes.
Specifically, our high-quality mode outperforms current state-of-the-art NeRF
editing methods both in terms of speed and quality. See more extensive results
on our project page: https://dream-catalyst.github.io.



---

## IPA-NeRF: Illusory Poisoning Attack Against Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Wenxiang Jiang, Hanwei Zhang, Shuo Zhao, Zhongwen Guo, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2407.11921v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) represents a significant advancement in computer
vision, offering implicit neural network-based scene representation and novel
view synthesis capabilities. Its applications span diverse fields including
robotics, urban mapping, autonomous navigation, virtual reality/augmented
reality, etc., some of which are considered high-risk AI applications. However,
despite its widespread adoption, the robustness and security of NeRF remain
largely unexplored. In this study, we contribute to this area by introducing
the Illusory Poisoning Attack against Neural Radiance Fields (IPA-NeRF). This
attack involves embedding a hidden backdoor view into NeRF, allowing it to
produce predetermined outputs, i.e. illusory, when presented with the specified
backdoor view while maintaining normal performance with standard inputs. Our
attack is specifically designed to deceive users or downstream models at a
particular position while ensuring that any abnormalities in NeRF remain
undetectable from other viewpoints. Experimental results demonstrate the
effectiveness of our Illusory Poisoning Attack, successfully presenting the
desired illusory on the specified viewpoint without impacting other views.
Notably, we achieve this attack by introducing small perturbations solely to
the training set. The code can be found at
https://github.com/jiang-wenxiang/IPA-NeRF.



---

## I$^2$-SLAM: Inverting Imaging Process for Robust Photorealistic Dense  SLAM


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Gwangtak Bae, Changwoon Choi, Hyeongjun Heo, Sang Min Kim, Young Min Kim | cs.CV | [PDF](http://arxiv.org/pdf/2407.11347v1){: .btn .btn-green } |

**Abstract**: We present an inverse image-formation module that can enhance the robustness
of existing visual SLAM pipelines for casually captured scenarios. Casual video
captures often suffer from motion blur and varying appearances, which degrade
the final quality of coherent 3D visual representation. We propose integrating
the physical imaging into the SLAM system, which employs linear HDR radiance
maps to collect measurements. Specifically, individual frames aggregate images
of multiple poses along the camera trajectory to explain prevalent motion blur
in hand-held videos. Additionally, we accommodate per-frame appearance
variation by dedicating explicit variables for image formation steps, namely
white balance, exposure time, and camera response function. Through joint
optimization of additional variables, the SLAM pipeline produces high-quality
images with more accurate trajectories. Extensive experiments demonstrate that
our approach can be incorporated into recent visual SLAM pipelines using
various scene representations, such as neural radiance fields or Gaussian
splatting.

Comments:
- ECCV 2024

---

## SlingBAG: Sliding ball adaptive growth algorithm with differentiable  radiation enables super-efficient iterative 3D photoacoustic image  reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li | cs.CV | [PDF](http://arxiv.org/pdf/2407.11781v1){: .btn .btn-green } |

**Abstract**: High-quality 3D photoacoustic imaging (PAI) reconstruction under sparse view
or limited view has long been challenging. Traditional 3D iterative-based
reconstruction methods suffer from both slow speed and high memory consumption.
Recently, in computer graphics, the differentiable rendering has made
significant progress, particularly with the rise of 3D Gaussian Splatting.
Inspired by these, we introduce differentiable radiation into PAI, developing a
novel reconstruction algorithm: the Sliding Ball Adaptive Growth algorithm
(SlingBAG) for 3D PAI, which shows ability in high-quality 3D PAI
reconstruction both under extremely sparse view and limited view.
  We established the point cloud dataset in PAI, and used unique differentiable
rapid radiator based on the spherical decomposition strategy and the randomly
initialized point cloud adaptively optimized according to sparse sensor data.
Each point undergoes updates in 3D coordinates, initial pressure, and
resolution (denoted by the radius of ball). Points undergo adaptive growth
during iterative process, including point destroying, splitting and duplicating
along the gradient of their positions, manifesting the sliding ball effect.
  Finally, our point cloud to voxel grid shader renders the final
reconstruction results. Simulation and in vivo experiments demonstrate that our
SlingBAG reconstruction result's SNR can be more than 40 dB under extremely
sparse view, while the SNR of traditional back-projection algorithm's result is
less than 20 dB. Moreover, the result of SlingBAG's structural similarity to
the ground truth is significantly higher, with an SSIM value of 95.6%.
  Notably, our differentiable rapid radiator can conduct forward PA simulation
in homogeneous, non-viscous media substantially faster than current methods
that numerically simulate the wave propagation, such as k-Wave. The dataset and
all code will be open source.



---

## Gaussian Splatting LK

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Liuyue Xie, Joel Julin, Koichiro Niinuma, Laszlo A. Jeni | cs.CV | [PDF](http://arxiv.org/pdf/2407.11309v1){: .btn .btn-green } |

**Abstract**: Reconstructing dynamic 3D scenes from 2D images and generating diverse views
over time presents a significant challenge due to the inherent complexity and
temporal dynamics involved. While recent advancements in neural implicit models
and dynamic Gaussian Splatting have shown promise, limitations persist,
particularly in accurately capturing the underlying geometry of highly dynamic
scenes. Some approaches address this by incorporating strong semantic and
geometric priors through diffusion models. However, we explore a different
avenue by investigating the potential of regularizing the native warp field
within the dynamic Gaussian Splatting framework. Our method is grounded on the
key intuition that an accurate warp field should produce continuous space-time
motions. While enforcing the motion constraints on warp fields is non-trivial,
we show that we can exploit knowledge innate to the forward warp field network
to derive an analytical velocity field, then time integrate for scene flows to
effectively constrain both the 2D motion and 3D positions of the Gaussians.
This derived Lucas-Kanade style analytical regularization enables our method to
achieve superior performance in reconstructing highly dynamic scenes, even
under minimal camera movement, extending the boundaries of what existing
dynamic Gaussian Splatting frameworks can achieve.

Comments:
- 15 pages, 10 figures

---

## Ev-GS: Event-based Gaussian splatting for Efficient and Accurate  Radiance Field Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Jingqian Wu, Shuo Zhu, Chutian Wang, Edmund Y. Lam | cs.CV | [PDF](http://arxiv.org/pdf/2407.11343v1){: .btn .btn-green } |

**Abstract**: Computational neuromorphic imaging (CNI) with event cameras offers advantages
such as minimal motion blur and enhanced dynamic range, compared to
conventional frame-based methods. Existing event-based radiance field rendering
methods are built on neural radiance field, which is computationally heavy and
slow in reconstruction speed. Motivated by the two aspects, we introduce Ev-GS,
the first CNI-informed scheme to infer 3D Gaussian splatting from a monocular
event camera, enabling efficient novel view synthesis. Leveraging 3D Gaussians
with pure event-based supervision, Ev-GS overcomes challenges such as the
detection of fast-moving objects and insufficient lighting. Experimental
results show that Ev-GS outperforms the method that takes frame-based signals
as input by rendering realistic views with reduced blurring and improved visual
quality. Moreover, it demonstrates competitive reconstruction quality and
reduced computing occupancy compared to existing methods, which paves the way
to a highly efficient CNI approach for signal processing.



---

## MVG-Splatting: Multi-View Guided Gaussian Splatting with Adaptive  Quantile-Based Geometric Consistency Densification

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Zhuoxiao Li, Shanliang Yao, Yijie Chu, Angel F. Garcia-Fernandez, Yong Yue, Eng Gee Lim, Xiaohui Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2407.11840v1){: .btn .btn-green } |

**Abstract**: In the rapidly evolving field of 3D reconstruction, 3D Gaussian Splatting
(3DGS) and 2D Gaussian Splatting (2DGS) represent significant advancements.
Although 2DGS compresses 3D Gaussian primitives into 2D Gaussian surfels to
effectively enhance mesh extraction quality, this compression can potentially
lead to a decrease in rendering quality. Additionally, unreliable densification
processes and the calculation of depth through the accumulation of opacity can
compromise the detail of mesh extraction. To address this issue, we introduce
MVG-Splatting, a solution guided by Multi-View considerations. Specifically, we
integrate an optimized method for calculating normals, which, combined with
image gradients, helps rectify inconsistencies in the original depth
computations. Additionally, utilizing projection strategies akin to those in
Multi-View Stereo (MVS), we propose an adaptive quantile-based method that
dynamically determines the level of additional densification guided by depth
maps, from coarse to fine detail. Experimental evidence demonstrates that our
method not only resolves the issues of rendering quality degradation caused by
depth discrepancies but also facilitates direct mesh extraction from dense
Gaussian point clouds using the Marching Cubes algorithm. This approach
significantly enhances the overall fidelity and accuracy of the 3D
reconstruction process, ensuring that both the geometric details and visual
quality.

Comments:
- https://mvgsplatting.github.io

---

## Click-Gaussian: Interactive Segmentation to Any 3D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Seokhun Choi, Hyeonseop Song, Jaechul Kim, Taehyeong Kim, Hoseok Do | cs.CV | [PDF](http://arxiv.org/pdf/2407.11793v1){: .btn .btn-green } |

**Abstract**: Interactive segmentation of 3D Gaussians opens a great opportunity for
real-time manipulation of 3D scenes thanks to the real-time rendering
capability of 3D Gaussian Splatting. However, the current methods suffer from
time-consuming post-processing to deal with noisy segmentation output. Also,
they struggle to provide detailed segmentation, which is important for
fine-grained manipulation of 3D scenes. In this study, we propose
Click-Gaussian, which learns distinguishable feature fields of two-level
granularity, facilitating segmentation without time-consuming post-processing.
We delve into challenges stemming from inconsistently learned feature fields
resulting from 2D segmentation obtained independently from a 3D scene. 3D
segmentation accuracy deteriorates when 2D segmentation results across the
views, primary cues for 3D segmentation, are in conflict. To overcome these
issues, we propose Global Feature-guided Learning (GFL). GFL constructs the
clusters of global feature candidates from noisy 2D segments across the views,
which smooths out noises when training the features of 3D Gaussians. Our method
runs in 10 ms per click, 15 to 130 times as fast as the previous methods, while
also significantly improving segmentation accuracy. Our project page is
available at https://seokhunchoi.github.io/Click-Gaussian

Comments:
- Accepted to ECCV 2024. The first two authors contributed equally to
  this work

---

## Motion-Oriented Compositional Neural Radiance Fields for Monocular  Dynamic Human Modeling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-16 | Jaehyeok Kim, Dongyoon Wee, Dan Xu | cs.CV | [PDF](http://arxiv.org/pdf/2407.11962v2){: .btn .btn-green } |

**Abstract**: This paper introduces Motion-oriented Compositional Neural Radiance Fields
(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of
monocular human videos via novel non-rigid motion modeling approach. In the
context of dynamic clothed humans, complex cloth dynamics generate non-rigid
motions that are intrinsically distinct from skeletal articulations and
critically important for the rendering quality. The conventional approach
models non-rigid motions as spatial (3D) deviations in addition to skeletal
transformations. However, it is either time-consuming or challenging to achieve
optimal quality due to its high learning complexity without a direct
supervision. To target this problem, we propose a novel approach of modeling
non-rigid motions as radiance residual fields to benefit from more direct color
supervision in the rendering and utilize the rigid radiance fields as a prior
to reduce the complexity of the learning process. Our approach utilizes a
single multiresolution hash encoding (MHE) to concurrently learn the canonical
T-pose representation from rigid skeletal motions and the radiance residual
field for non-rigid motions. Additionally, to further improve both training
efficiency and usability, we extend MoCo-NeRF to support simultaneous training
of multiple subjects within a single framework, thanks to our effective design
for modeling non-rigid motions. This scalability is achieved through the
integration of a global MHE and learnable identity codes in addition to
multiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,
clearly demonstrating state-of-the-art performance in both single- and
multi-subject settings. The code and model will be made publicly available at
the project page: https://stevejaehyeok.github.io/publications/moco-nerf.

Comments:
- Accepted by ECCV2024

---

## Scaling 3D Reasoning with LMMs to Large Robot Mission Environments Using  Datagraphs


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | W. J. Meijer, A. C. Kemmeren, E. H. J. Riemens, J. E. Fransman, M. van Bekkum, G. J. Burghouts, J. D. van Mil | cs.RO | [PDF](http://arxiv.org/pdf/2407.10743v1){: .btn .btn-green } |

**Abstract**: This paper addresses the challenge of scaling Large Multimodal Models (LMMs)
to expansive 3D environments. Solving this open problem is especially relevant
for robot deployment in many first-responder scenarios, such as
search-and-rescue missions that cover vast spaces. The use of LMMs in these
settings is currently hampered by the strict context windows that limit the
LMM's input size. We therefore introduce a novel approach that utilizes a
datagraph structure, which allows the LMM to iteratively query smaller sections
of a large environment. Using the datagraph in conjunction with graph traversal
algorithms, we can prioritize the most relevant locations to the query, thereby
improving the scalability of 3D scene language tasks. We illustrate the
datagraph using 3D scenes, but these can be easily substituted by other dense
modalities that represent the environment, such as pointclouds or Gaussian
splats. We demonstrate the potential to use the datagraph for two 3D scene
language task use cases, in a search-and-rescue mission example.

Comments:
- Accepted to the RSS Workshop on Semantics for Robotics: From
  Environment Understanding and Reasoning to Safe Interaction 2024

---

## Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High  Quality and Efficient Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Francesco Di Sario, Riccardo Renzulli, Enzo Tartaglione, Marco Grangetto | cs.CV | [PDF](http://arxiv.org/pdf/2407.10389v1){: .btn .btn-green } |

**Abstract**: Since the introduction of NeRFs, considerable attention has been focused on
improving their training and inference times, leading to the development of
Fast-NeRFs models. Despite demonstrating impressive rendering speed and
quality, the rapid convergence of such models poses challenges for further
improving reconstruction quality. Common strategies to improve rendering
quality involves augmenting model parameters or increasing the number of
sampled points. However, these computationally intensive approaches encounter
limitations in achieving significant quality enhancements. This study
introduces a model-agnostic framework inspired by Sparsely-Gated Mixture of
Experts to enhance rendering quality without escalating computational
complexity. Our approach enables specialization in rendering different scene
components by employing a mixture of experts with varying resolutions. We
present a novel gate formulation designed to maximize expert capabilities and
propose a resolution-based routing technique to effectively induce sparsity and
decompose scenes. Our work significantly improves reconstruction quality while
maintaining competitive performance.



---

## AirNeRF: 3D Reconstruction of Human with Drone and NeRF for Future  Communication Systems

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Alexey Kotcov, Maria Dronova, Vladislav Cheremnykh, Sausar Karaf, Dzmitry Tsetserukou | cs.RO | [PDF](http://arxiv.org/pdf/2407.10865v1){: .btn .btn-green } |

**Abstract**: In the rapidly evolving landscape of digital content creation, the demand for
fast, convenient, and autonomous methods of crafting detailed 3D
reconstructions of humans has grown significantly. Addressing this pressing
need, our AirNeRF system presents an innovative pathway to the creation of a
realistic 3D human avatar. Our approach leverages Neural Radiance Fields (NeRF)
with an automated drone-based video capturing method. The acquired data
provides a swift and precise way to create high-quality human body
reconstructions following several stages of our system. The rigged mesh derived
from our system proves to be an excellent foundation for free-view synthesis of
dynamic humans, particularly well-suited for the immersive experiences within
gaming and virtual reality.



---

## NGP-RT: Fusing Multi-Level Hash Features with Lightweight Attention for  Real-Time Novel View Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Yubin Hu, Xiaoyang Guo, Yang Xiao, Jingwei Huang, Yong-Jin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.10482v1){: .btn .btn-green } |

**Abstract**: This paper presents NGP-RT, a novel approach for enhancing the rendering
speed of Instant-NGP to achieve real-time novel view synthesis. As a classic
NeRF-based method, Instant-NGP stores implicit features in multi-level grids or
hash tables and applies a shallow MLP to convert the implicit features into
explicit colors and densities. Although it achieves fast training speed, there
is still a lot of room for improvement in its rendering speed due to the
per-point MLP executions for implicit multi-level feature aggregation,
especially for real-time applications. To address this challenge, our proposed
NGP-RT explicitly stores colors and densities as hash features, and leverages a
lightweight attention mechanism to disambiguate the hash collisions instead of
using computationally intensive MLP. At the rendering stage, NGP-RT
incorporates a pre-computed occupancy distance grid into the ray marching
strategy to inform the distance to the nearest occupied voxel, thereby reducing
the number of marching points and global memory access. Experimental results
show that on the challenging Mip-NeRF360 dataset, NGP-RT achieves better
rendering quality than previous NeRF-based methods, achieving 108 fps at 1080p
resolution on a single Nvidia RTX 3090 GPU. Our approach is promising for
NeRF-based real-time applications that require efficient and high-quality
rendering.

Comments:
- ECCV 2024

---

## IE-NeRF: Inpainting Enhanced Neural Radiance Fields in the Wild

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Shuaixian Wang, Haoran Xu, Yaokun Li, Jiwei Chen, Guang Tan | cs.CV | [PDF](http://arxiv.org/pdf/2407.10695v1){: .btn .btn-green } |

**Abstract**: We present a novel approach for synthesizing realistic novel views using
Neural Radiance Fields (NeRF) with uncontrolled photos in the wild. While NeRF
has shown impressive results in controlled settings, it struggles with
transient objects commonly found in dynamic and time-varying scenes. Our
framework called \textit{Inpainting Enhanced NeRF}, or \ours, enhances the
conventional NeRF by drawing inspiration from the technique of image
inpainting. Specifically, our approach extends the Multi-Layer Perceptrons
(MLP) of NeRF, enabling it to simultaneously generate intrinsic properties
(static color, density) and extrinsic transient masks. We introduce an
inpainting module that leverages the transient masks to effectively exclude
occlusions, resulting in improved volume rendering quality. Additionally, we
propose a new training strategy with frequency regularization to address the
sparsity issue of low-frequency transient components. We evaluate our approach
on internet photo collections of landmarks, demonstrating its ability to
generate high-quality novel views and achieve state-of-the-art performance.



---

## Domain Generalization for 6D Pose Estimation Through NeRF-based Image  Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer | cs.CV | [PDF](http://arxiv.org/pdf/2407.10762v1){: .btn .btn-green } |

**Abstract**: This work introduces a novel augmentation method that increases the diversity
of a train set to improve the generalization abilities of a 6D pose estimation
network. For this purpose, a Neural Radiance Field is trained from synthetic
images and exploited to generate an augmented set. Our method enriches the
initial set by enabling the synthesis of images with (i) unseen viewpoints,
(ii) rich illumination conditions through appearance extrapolation, and (iii)
randomized textures. We validate our augmentation method on the challenging
use-case of spacecraft pose estimation and show that it significantly improves
the pose estimation generalization capabilities. On the SPEED+ dataset, our
method reduces the error on the pose by 50% on both target domains.



---

## iHuman: Instant Animatable Digital Humans From Monocular Videos

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Pramish Paudel, Anubhav Khanal, Ajad Chhatkuli, Danda Pani Paudel, Jyoti Tandukar | cs.CV | [PDF](http://arxiv.org/pdf/2407.11174v1){: .btn .btn-green } |

**Abstract**: Personalized 3D avatars require an animatable representation of digital
humans. Doing so instantly from monocular videos offers scalability to broad
class of users and wide-scale applications. In this paper, we present a fast,
simple, yet effective method for creating animatable 3D digital humans from
monocular videos. Our method utilizes the efficiency of Gaussian splatting to
model both 3D geometry and appearance. However, we observed that naively
optimizing Gaussian splats results in inaccurate geometry, thereby leading to
poor animations. This work achieves and illustrates the need of accurate 3D
mesh-type modelling of the human body for animatable digitization through
Gaussian splats. This is achieved by developing a novel pipeline that benefits
from three key aspects: (a) implicit modelling of surface's displacements and
the color's spherical harmonics; (b) binding of 3D Gaussians to the respective
triangular faces of the body template; (c) a novel technique to render normals
followed by their auxiliary supervision. Our exhaustive experiments on three
different benchmark datasets demonstrates the state-of-the-art results of our
method, in limited time settings. In fact, our method is faster by an order of
magnitude (in terms of training time) than its closest competitor. At the same
time, we achieve superior rendering and 3D reconstruction performance under the
change of poses.

Comments:
- 15 pages, eccv, 2024

---

## Interactive Rendering of Relightable and Animatable Gaussian Avatars


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Youyi Zhan, Tianjia Shao, He Wang, Yin Yang, Kun Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2407.10707v1){: .btn .btn-green } |

**Abstract**: Creating relightable and animatable avatars from multi-view or monocular
videos is a challenging task for digital human creation and virtual reality
applications. Previous methods rely on neural radiance fields or ray tracing,
resulting in slow training and rendering processes. By utilizing Gaussian
Splatting, we propose a simple and efficient method to decouple body materials
and lighting from sparse-view or monocular avatar videos, so that the avatar
can be rendered simultaneously under novel viewpoints, poses, and lightings at
interactive frame rates (6.9 fps). Specifically, we first obtain the canonical
body mesh using a signed distance function and assign attributes to each mesh
vertex. The Gaussians in the canonical space then interpolate from nearby body
mesh vertices to obtain the attributes. We subsequently deform the Gaussians to
the posed space using forward skinning, and combine the learnable environment
light with the Gaussian attributes for shading computation. To achieve fast
shadow modeling, we rasterize the posed body mesh from dense viewpoints to
obtain the visibility. Our approach is not only simple but also fast enough to
allow interactive rendering of avatar animation under environmental light
changes. Experiments demonstrate that, compared to previous works, our method
can render higher quality results at a faster speed on both synthetic and real
datasets.



---

## Evaluating geometric accuracy of NeRF reconstructions compared to SLAM  method

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-15 | Adam Korycki, Colleen Josephson, Steve McGuire | cs.CV | [PDF](http://arxiv.org/pdf/2407.11238v1){: .btn .btn-green } |

**Abstract**: As Neural Radiance Field (NeRF) implementations become faster, more efficient
and accurate, their applicability to real world mapping tasks becomes more
accessible. Traditionally, 3D mapping, or scene reconstruction, has relied on
expensive LiDAR sensing. Photogrammetry can perform image-based 3D
reconstruction but is computationally expensive and requires extremely dense
image representation to recover complex geometry and photorealism. NeRFs
perform 3D scene reconstruction by training a neural network on sparse image
and pose data, achieving superior results to photogrammetry with less input
data. This paper presents an evaluation of two NeRF scene reconstructions for
the purpose of estimating the diameter of a vertical PVC cylinder. One of these
are trained on commodity iPhone data and the other is trained on robot-sourced
imagery and poses. This neural-geometry is compared to state-of-the-art
lidar-inertial SLAM in terms of scene noise and metric-accuracy.



---

## RecGS: Removing Water Caustic with Recurrent Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-14 | Tianyi Zhang, Weiming Zhi, Kaining Huang, Joshua Mangelson, Corina Barbalata, Matthew Johnson-Roberson | cs.CV | [PDF](http://arxiv.org/pdf/2407.10318v2){: .btn .btn-green } |

**Abstract**: Water caustics are commonly observed in seafloor imaging data from
shallow-water areas. Traditional methods that remove caustic patterns from
images often rely on 2D filtering or pre-training on an annotated dataset,
hindering the performance when generalizing to real-world seafloor data with 3D
structures. In this paper, we present a novel method Recurrent Gaussian
Splatting (RecGS), which takes advantage of today's photorealistic 3D
reconstruction technology, 3DGS, to separate caustics from seafloor imagery.
With a sequence of images taken by an underwater robot, we build 3DGS
recurrently and decompose the caustic with low-pass filtering in each
iteration. In the experiments, we analyze and compare with different methods,
including joint optimization, 2D filtering, and deep learning approaches. The
results show that our method can effectively separate the caustic from the
seafloor, improving the visual appearance, and can be potentially applied on
more problems with inconsistent illumination.

Comments:
- 8 pages, 9 figures

---

## SpikeGS: 3D Gaussian Splatting from Spike Streams with High-Speed Camera  Motion

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-14 | Jiyuan Zhang, Kang Chen, Shiyan Chen, Yajing Zheng, Tiejun Huang, Zhaofei Yu | cs.CV | [PDF](http://arxiv.org/pdf/2407.10062v1){: .btn .btn-green } |

**Abstract**: Novel View Synthesis plays a crucial role by generating new 2D renderings
from multi-view images of 3D scenes. However, capturing high-speed scenes with
conventional cameras often leads to motion blur, hindering the effectiveness of
3D reconstruction. To address this challenge, high-frame-rate dense 3D
reconstruction emerges as a vital technique, enabling detailed and accurate
modeling of real-world objects or scenes in various fields, including Virtual
Reality or embodied AI. Spike cameras, a novel type of neuromorphic sensor,
continuously record scenes with an ultra-high temporal resolution, showing
potential for accurate 3D reconstruction. Despite their promise, existing
approaches, such as applying Neural Radiance Fields (NeRF) to spike cameras,
encounter challenges due to the time-consuming rendering process. To address
this issue, we make the first attempt to introduce the 3D Gaussian Splatting
(3DGS) into spike cameras in high-speed capture, providing 3DGS as dense and
continuous clues of views, then constructing SpikeGS. Specifically, to train
SpikeGS, we establish computational equations between the rendering process of
3DGS and the processes of instantaneous imaging and exposing-like imaging of
the continuous spike stream. Besides, we build a very lightweight but effective
mapping process from spikes to instant images to support training. Furthermore,
we introduced a new spike-based 3D rendering dataset for validation. Extensive
experiments have demonstrated our method possesses the high quality of novel
view rendering, proving the tremendous potential of spike cameras in modeling
3D scenes.



---

## 3DEgo: 3D Editing on the Go!

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-14 | Umar Khalid, Hasan Iqbal, Azib Farooq, Jing Hua, Chen Chen | cs.CV | [PDF](http://arxiv.org/pdf/2407.10102v1){: .btn .btn-green } |

**Abstract**: We introduce 3DEgo to address a novel problem of directly synthesizing
photorealistic 3D scenes from monocular videos guided by textual prompts.
Conventional methods construct a text-conditioned 3D scene through a
three-stage process, involving pose estimation using Structure-from-Motion
(SfM) libraries like COLMAP, initializing the 3D model with unedited images,
and iteratively updating the dataset with edited images to achieve a 3D scene
with text fidelity. Our framework streamlines the conventional multi-stage 3D
editing process into a single-stage workflow by overcoming the reliance on
COLMAP and eliminating the cost of model initialization. We apply a diffusion
model to edit video frames prior to 3D scene creation by incorporating our
designed noise blender module for enhancing multi-view editing consistency, a
step that does not require additional training or fine-tuning of T2I diffusion
models. 3DEgo utilizes 3D Gaussian Splatting to create 3D scenes from the
multi-view consistent edited frames, capitalizing on the inherent temporal
continuity and explicit point cloud data. 3DEgo demonstrates remarkable editing
precision, speed, and adaptability across a variety of video sources, as
validated by extensive evaluations on six datasets, including our own prepared
GS25 dataset. Project Page: https://3dego.github.io/

Comments:
- ECCV 2024 Accepted Paper

---

## RS-NeRF: Neural Radiance Fields from Rolling Shutter Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-14 | Muyao Niu, Tong Chen, Yifan Zhan, Zhuoxiao Li, Xiang Ji, Yinqiang Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2407.10267v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become increasingly popular because of
their impressive ability for novel view synthesis. However, their effectiveness
is hindered by the Rolling Shutter (RS) effects commonly found in most camera
systems. To solve this, we present RS-NeRF, a method designed to synthesize
normal images from novel views using input with RS distortions. This involves a
physical model that replicates the image formation process under RS conditions
and jointly optimizes NeRF parameters and camera extrinsic for each image row.
We further address the inherent shortcomings of the basic RS-NeRF model by
delving into the RS characteristics and developing algorithms to enhance its
functionality. First, we impose a smoothness regularization to better estimate
trajectories and improve the synthesis quality, in line with the camera
movement prior. We also identify and address a fundamental flaw in the vanilla
RS model by introducing a multi-sampling algorithm. This new approach improves
the model's performance by comprehensively exploiting the RGB data across
different rows for each intermediate camera pose. Through rigorous
experimentation, we demonstrate that RS-NeRF surpasses previous methods in both
synthetic and real-world scenarios, proving its ability to correct RS-related
distortions effectively. Codes and data available:
https://github.com/MyNiuuu/RS-NeRF

Comments:
- ECCV 2024 ; Codes and data: https://github.com/MyNiuuu/RS-NeRF

---

## Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-13 | Zhentao Huang, Minglun Gong | cs.CV | [PDF](http://arxiv.org/pdf/2407.09733v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Textured-GS, an innovative method for rendering
Gaussian splatting that incorporates spatially defined color and opacity
variations using Spherical Harmonics (SH). This approach enables each Gaussian
to exhibit a richer representation by accommodating varying colors and
opacities across its surface, significantly enhancing rendering quality
compared to traditional methods. To demonstrate the merits of our approach, we
have adapted the Mini-Splatting architecture to integrate textured Gaussians
without increasing the number of Gaussians. Our experiments across multiple
real-world datasets show that Textured-GS consistently outperforms both the
baseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The
results highlight the potential of Textured-GS to advance Gaussian-based
rendering technologies, promising more efficient and high-quality scene
reconstructions.

Comments:
- 9 pages

---

## Radiance Fields from Photons

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-12 | Sacha Jungerman, Mohit Gupta | cs.CV | [PDF](http://arxiv.org/pdf/2407.09386v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields, or NeRFs, have become the de facto approach for
high-quality view synthesis from a collection of images captured from multiple
viewpoints. However, many issues remain when capturing images in-the-wild under
challenging conditions, such as low light, high dynamic range, or rapid motion
leading to smeared reconstructions with noticeable artifacts. In this work, we
introduce quanta radiance fields, a novel class of neural radiance fields that
are trained at the granularity of individual photons using single-photon
cameras (SPCs). We develop theory and practical computational techniques for
building radiance fields and estimating dense camera poses from unconventional,
stochastic, and high-speed binary frame sequences captured by SPCs. We
demonstrate, both via simulations and a SPC hardware prototype, high-fidelity
reconstructions under high-speed motion, in low light, and for extreme dynamic
range settings.



---

## Physics-Informed Learning of Characteristic Trajectories for Smoke  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-12 | Yiming Wang, Siyu Tang, Mengyu Chu | cs.CV | [PDF](http://arxiv.org/pdf/2407.09679v1){: .btn .btn-green } |

**Abstract**: We delve into the physics-informed neural reconstruction of smoke and
obstacles through sparse-view RGB videos, tackling challenges arising from
limited observation of complex dynamics. Existing physics-informed neural
networks often emphasize short-term physics constraints, leaving the proper
preservation of long-term conservation less explored. We introduce Neural
Characteristic Trajectory Fields, a novel representation utilizing Eulerian
neural fields to implicitly model Lagrangian fluid trajectories. This
topology-free, auto-differentiable representation facilitates efficient flow
map calculations between arbitrary frames as well as efficient velocity
extraction via auto-differentiation. Consequently, it enables end-to-end
supervision covering long-term conservation and short-term physics priors.
Building on the representation, we propose physics-informed trajectory learning
and integration into NeRF-based scene reconstruction. We enable advanced
obstacle handling through self-supervised scene decomposition and seamless
integrated boundary constraints. Our results showcase the ability to overcome
challenges like occlusion uncertainty, density-color ambiguity, and
static-dynamic entanglements. Code and sample tests are at
\url{https://github.com/19reborn/PICT_smoke}.

Comments:
- SIGGRAPH 2024 (conference track), Project Website:
  \url{https://19reborn.github.io/PICT_Smoke.github.io/}

---

## HPC: Hierarchical Progressive Coding Framework for Volumetric Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-12 | Zihan Zheng, Houqiang Zhong, Qiang Hu, Xiaoyun Zhang, Li Song, Ya Zhang, Yanfeng Wang | cs.CV | [PDF](http://arxiv.org/pdf/2407.09026v1){: .btn .btn-green } |

**Abstract**: Volumetric video based on Neural Radiance Field (NeRF) holds vast potential
for various 3D applications, but its substantial data volume poses significant
challenges for compression and transmission. Current NeRF compression lacks the
flexibility to adjust video quality and bitrate within a single model for
various network and device capacities. To address these issues, we propose HPC,
a novel hierarchical progressive volumetric video coding framework achieving
variable bitrate using a single model. Specifically, HPC introduces a
hierarchical representation with a multi-resolution residual radiance field to
reduce temporal redundancy in long-duration sequences while simultaneously
generating various levels of detail. Then, we propose an end-to-end progressive
learning approach with a multi-rate-distortion loss function to jointly
optimize both hierarchical representation and compression. Our HPC trained only
once can realize multiple compression levels, while the current methods need to
train multiple fixed-bitrate models for different rate-distortion (RD)
tradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality
levels with variable bitrate by a single model and exhibits competitive RD
performance, even outperforming fixed-bitrate models across various datasets.

Comments:
- 11 pages, 7 figures

---

## StyleSplat: 3D Object Style Transfer with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-12 | Sahil Jain, Avik Kuthiala, Prabhdeep Singh Sethi, Prakanshul Saxena | cs.CV | [PDF](http://arxiv.org/pdf/2407.09473v1){: .btn .btn-green } |

**Abstract**: Recent advancements in radiance fields have opened new avenues for creating
high-quality 3D assets and scenes. Style transfer can enhance these 3D assets
with diverse artistic styles, transforming creative expression. However,
existing techniques are often slow or unable to localize style transfer to
specific objects. We introduce StyleSplat, a lightweight method for stylizing
3D objects in scenes represented by 3D Gaussians from reference style images.
Our approach first learns a photorealistic representation of the scene using 3D
Gaussian splatting while jointly segmenting individual 3D objects. We then use
a nearest-neighbor feature matching loss to finetune the Gaussians of the
selected objects, aligning their spherical harmonic coefficients with the style
image to ensure consistency and visual appeal. StyleSplat allows for quick,
customizable style transfer and localized stylization of multiple objects
within a scene, each with a different style. We demonstrate its effectiveness
across various 3D scenes and styles, showcasing enhanced control and
customization in 3D creation.

Comments:
- for code and results, see http://bernard0047.github.io/stylesplat

---

## Feasibility of Neural Radiance Fields for Crime Scene Video  Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Shariq Nadeem Malik, Min Hao Chee, Dayan Mario Anthony Perera, Chern Hong Lim | cs.CV | [PDF](http://arxiv.org/pdf/2407.08795v1){: .btn .btn-green } |

**Abstract**: This paper aims to review and determine the feasibility of using variations
of NeRF models in order to reconstruct crime scenes given input videos of the
scene. We focus on three main innovations of NeRF when it comes to
reconstructing crime scenes: Multi-object Synthesis, Deformable Synthesis, and
Lighting. From there, we analyse its innovation progress against the
requirements to be met in order to be able to reconstruct crime scenes with
given videos of such scenes.

Comments:
- 4 pages, 1 table

---

## WildGaussians: 3D Gaussian Splatting in the Wild

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, Torsten Sattler | cs.CV | [PDF](http://arxiv.org/pdf/2407.08447v1){: .btn .btn-green } |

**Abstract**: While the field of 3D scene reconstruction is dominated by NeRFs due to their
photorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,
offering similar quality with real-time rendering speeds. However, both methods
primarily excel with well-controlled 3D scenes, while in-the-wild data -
characterized by occlusions, dynamic objects, and varying illumination -
remains challenging. NeRFs can adapt to such conditions easily through
per-image embedding vectors, but 3DGS struggles due to its explicit
representation and lack of shared parameters. To address this, we introduce
WildGaussians, a novel approach to handle occlusions and appearance changes
with 3DGS. By leveraging robust DINO features and integrating an appearance
modeling module within 3DGS, our method achieves state-of-the-art results. We
demonstrate that WildGaussians matches the real-time rendering speed of 3DGS
while surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all
within a simple architectural framework.

Comments:
- https://wild-gaussians.github.io/

---

## Bayesian uncertainty analysis for underwater 3D reconstruction with  neural radiance fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Haojie Lian, Xinhao Li, Yilin Qu, Jing Du, Zhuxuan Meng, Jie Liu, Leilei Chen | cs.CE | [PDF](http://arxiv.org/pdf/2407.08154v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) are a deep learning technique that can
generate novel views of 3D scenes using sparse 2D images from different viewing
directions and camera poses. As an extension of conventional NeRFs in
underwater environment, where light can get absorbed and scattered by water,
SeaThru-NeRF was proposed to separate the clean appearance and geometric
structure of underwater scene from the effects of the scattering medium. Since
the quality of the appearance and structure of underwater scenes is crucial for
downstream tasks such as underwater infrastructure inspection, the reliability
of the 3D reconstruction model should be considered and evaluated. Nonetheless,
owing to the lack of ability to quantify uncertainty in 3D reconstruction of
underwater scenes under natural ambient illumination, the practical deployment
of NeRFs in unmanned autonomous underwater navigation is limited. To address
this issue, we introduce a spatial perturbation field D_omega based on Bayes'
rays in SeaThru-NeRF and perform Laplace approximation to obtain a Gaussian
distribution N(0,Sigma) of the parameters omega, where the diagonal elements of
Sigma correspond to the uncertainty at each spatial location. We also employ a
simple thresholding method to remove artifacts from the rendered results of
underwater scenes. Numerical experiments are provided to demonstrate the
effectiveness of this approach.



---

## Explicit_NeRF_QA: A Quality Assessment Database for Explicit NeRF Model  Compression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Yuke Xing, Qi Yang, Kaifa Yang, Yilin Xu, Zhu Li | eess.IV | [PDF](http://arxiv.org/pdf/2407.08165v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) have demonstrated significant
advantages in representing and synthesizing 3D scenes. Explicit NeRF models
facilitate the practical NeRF applications with faster rendering speed, and
also attract considerable attention in NeRF compression due to its huge storage
cost. To address the challenge of the NeRF compression study, in this paper, we
construct a new dataset, called Explicit_NeRF_QA. We use 22 3D objects with
diverse geometries, textures, and material complexities to train four typical
explicit NeRF models across five parameter levels. Lossy compression is
introduced during the model generation, pivoting the selection of key
parameters such as hash table size for InstantNGP and voxel grid resolution for
Plenoxels. By rendering NeRF samples to processed video sequences (PVS), a
large scale subjective experiment with lab environment is conducted to collect
subjective scores from 21 viewers. The diversity of content, accuracy of mean
opinion scores (MOS), and characteristics of NeRF distortion are
comprehensively presented, establishing the heterogeneity of the proposed
dataset. The state-of-the-art objective metrics are tested in the new dataset.
Best Person correlation, which is around 0.85, is collected from the
full-reference objective metric. All tested no-reference metrics report very
poor results with 0.4 to 0.6 correlations, demonstrating the need for further
development of more robust no-reference metrics. The dataset, including NeRF
samples, source 3D objects, multiview images for NeRF generation, PVSs, MOS, is
made publicly available at the following location:
https://github.com/LittlericeChloe/Explicit_NeRF_QA.

Comments:
- 5 pages, 4 figures, 2 tables, conference

---

## Survey on Fundamental Deep Learning 3D Reconstruction Techniques

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Yonge Bai, LikHang Wong, TszYin Twan | cs.CV | [PDF](http://arxiv.org/pdf/2407.08137v1){: .btn .btn-green } |

**Abstract**: This survey aims to investigate fundamental deep learning (DL) based 3D
reconstruction techniques that produce photo-realistic 3D models and scenes,
highlighting Neural Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and
3D Gaussian Splatting. We dissect the underlying algorithms, evaluate their
strengths and tradeoffs, and project future research trajectories in this
rapidly evolving field. We provide a comprehensive overview of the fundamental
in DL-driven 3D scene reconstruction, offering insights into their potential
applications and limitations.



---

## MeshAvatar: Learning High-quality Triangular Human Avatars from  Multi-view Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Yushuo Chen, Zerong Zheng, Zhe Li, Chao Xu, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.08414v1){: .btn .btn-green } |

**Abstract**: We present a novel pipeline for learning high-quality triangular human
avatars from multi-view videos. Recent methods for avatar learning are
typically based on neural radiance fields (NeRF), which is not compatible with
traditional graphics pipeline and poses great challenges for operations like
editing or synthesizing under different environments. To overcome these
limitations, our method represents the avatar with an explicit triangular mesh
extracted from an implicit SDF field, complemented by an implicit material
field conditioned on given poses. Leveraging this triangular avatar
representation, we incorporate physics-based rendering to accurately decompose
geometry and texture. To enhance both the geometric and appearance details, we
further employ a 2D UNet as the network backbone and introduce pseudo normal
ground-truth as additional supervision. Experiments show that our method can
learn triangular avatars with high-quality geometry reconstruction and
plausible material decomposition, inherently supporting editing, manipulation
or relighting operations.

Comments:
- Project Page: https://shad0wta9.github.io/meshavatar-page/

---

## Drantal-NeRF: Diffusion-Based Restoration for Anti-aliasing Neural  Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-10 | Ganlin Yang, Kaidong Zhang, Jingjing Fu, Dong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.07461v1){: .btn .btn-green } |

**Abstract**: Aliasing artifacts in renderings produced by Neural Radiance Field (NeRF) is
a long-standing but complex issue in the field of 3D implicit representation,
which arises from a multitude of intricate causes and was mitigated by
designing more advanced but complex scene parameterization methods before. In
this paper, we present a Diffusion-based restoration method for anti-aliasing
Neural Radiance Field (Drantal-NeRF). We consider the anti-aliasing issue from
a low-level restoration perspective by viewing aliasing artifacts as a kind of
degradation model added to clean ground truths. By leveraging the powerful
prior knowledge encapsulated in diffusion model, we could restore the
high-realism anti-aliasing renderings conditioned on aliased low-quality
counterparts. We further employ a feature-wrapping operation to ensure
multi-view restoration consistency and finetune the VAE decoder to better adapt
to the scene-specific data distribution. Our proposed method is easy to
implement and agnostic to various NeRF backbones. We conduct extensive
experiments on challenging large-scale urban scenes as well as unbounded
360-degree scenes and achieve substantial qualitative and quantitative
improvements.



---

## MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-10 | Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras | cs.CV | [PDF](http://arxiv.org/pdf/2407.07284v1){: .btn .btn-green } |

**Abstract**: We introduce MIGS (Multi-Identity Gaussian Splatting), a novel method that
learns a single neural representation for multiple identities, using only
monocular videos. Recent 3D Gaussian Splatting (3DGS) approaches for human
avatars require per-identity optimization. However, learning a multi-identity
representation presents advantages in robustly animating humans under arbitrary
poses. We propose to construct a high-order tensor that combines all the
learnable 3DGS parameters for all the training identities. By assuming a
low-rank structure and factorizing the tensor, we model the complex rigid and
non-rigid deformations of multiple subjects in a unified network, significantly
reducing the total number of parameters. Our proposed approach leverages
information from all the training identities, enabling robust animation under
challenging unseen poses, outperforming existing approaches. We also
demonstrate how it can be extended to learn unseen identities.

Comments:
- Accepted by ECCV 2024. Project page:
  https://aggelinacha.github.io/MIGS/

---

## Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-10 | Qi Song, Ziyuan Luo, Ka Chun Cheung, Simon See, Renjie Wan | cs.CV | [PDF](http://arxiv.org/pdf/2407.07735v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become a key method for 3D scene
representation. With the rising prominence and influence of NeRF, safeguarding
its intellectual property has become increasingly important. In this paper, we
propose \textbf{NeRFProtector}, which adopts a plug-and-play strategy to
protect NeRF's copyright during its creation. NeRFProtector utilizes a
pre-trained watermarking base model, enabling NeRF creators to embed binary
messages directly while creating their NeRF. Our plug-and-play property ensures
NeRF creators can flexibly choose NeRF variants without excessive
modifications. Leveraging our newly designed progressive distillation, we
demonstrate performance on par with several leading-edge neural rendering
methods. Our project is available at:
\url{https://qsong2001.github.io/NeRFProtector}.

Comments:
- Accepted by ECCV2024

---

## Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-09 | Dogyoon Lee, Donghyeong Kim, Jungho Lee, Minhyeok Lee, Seunghoon Lee, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2407.06613v1){: .btn .btn-green } |

**Abstract**: Recent studies construct deblurred neural radiance fields (DeRF) using dozens
of blurry images, which are not practical scenarios if only a limited number of
blurry images are available. This paper focuses on constructing DeRF from
sparse-view for more pragmatic real-world scenarios. As observed in our
experiments, establishing DeRF from sparse views proves to be a more
challenging problem due to the inherent complexity arising from the
simultaneous optimization of blur kernels and NeRF from sparse view.
Sparse-DeRF successfully regularizes the complicated joint optimization,
presenting alleviated overfitting artifacts and enhanced quality on radiance
fields. The regularization consists of three key components: Surface
smoothness, helps the model accurately predict the scene structure utilizing
unseen and additional hidden rays derived from the blur kernel based on
statistical tendencies of real-world; Modulated gradient scaling, helps the
model adjust the amount of the backpropagated gradient according to the
arrangements of scene objects; Perceptual distillation improves the perceptual
quality by overcoming the ill-posed multi-view inconsistency of image
deblurring and distilling the pre-filtered information, compensating for the
lack of clean information in blurry images. We demonstrate the effectiveness of
the Sparse-DeRF with extensive quantitative and qualitative experimental
results by training DeRF from 2-view, 4-view, and 6-view blurry images.

Comments:
- Project page: https://dogyoonlee.github.io/sparsederf/

---

## 3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-09 | Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, Zan Gojcic | cs.GR | [PDF](http://arxiv.org/pdf/2407.07090v2){: .btn .btn-green } |

**Abstract**: Particle-based representations of radiance fields such as 3D Gaussian
Splatting have found great success for reconstructing and re-rendering of
complex scenes. Most existing methods render particles via rasterization,
projecting them to screen space tiles for processing in a sorted order. This
work instead considers ray tracing the particles, building a bounding volume
hierarchy and casting a ray for each pixel using high-performance GPU ray
tracing hardware. To efficiently handle large numbers of semi-transparent
particles, we describe a specialized rendering algorithm which encapsulates
particles with bounding meshes to leverage fast ray-triangle intersections, and
shades batches of intersections in depth-order. The benefits of ray tracing are
well-known in computer graphics: processing incoherent rays for secondary
lighting effects such as shadows and reflections, rendering from
highly-distorted cameras common in robotics, stochastically sampling rays, and
more. With our renderer, this flexibility comes at little cost compared to
rasterization. Experiments demonstrate the speed and accuracy of our approach,
as well as several applications in computer graphics and vision. We further
propose related improvements to the basic Gaussian representation, including a
simple use of generalized kernel functions which significantly reduces particle
hit counts.

Comments:
- Project page: https://gaussiantracer.github.io/

---

## Reference-based Controllable Scene Stylization with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-09 | Yiqun Mei, Jiacong Xu, Vishal M. Patel | cs.CV | [PDF](http://arxiv.org/pdf/2407.07220v1){: .btn .btn-green } |

**Abstract**: Referenced-based scene stylization that edits the appearance based on a
content-aligned reference image is an emerging research area. Starting with a
pretrained neural radiance field (NeRF), existing methods typically learn a
novel appearance that matches the given style. Despite their effectiveness,
they inherently suffer from time-consuming volume rendering, and thus are
impractical for many real-time applications. In this work, we propose ReGS,
which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to
enable real-time stylized view synthesis. Editing the appearance of a
pretrained 3DGS is challenging as it uses discrete Gaussians as 3D
representation, which tightly bind appearance with geometry. Simply optimizing
the appearance as prior methods do is often insufficient for modeling
continuous textures in the given reference image. To address this challenge, we
propose a novel texture-guided control mechanism that adaptively adjusts local
responsible Gaussians to a new geometric arrangement, serving for desired
texture details. The proposed process is guided by texture clues for effective
appearance editing, and regularized by scene depth for preserving original
geometric structure. With these novel designs, we show ReGs can produce
state-of-the-art stylization results that respect the reference texture while
embracing real-time rendering speed for free-view navigation.



---

## Enhancing Neural Radiance Fields with Depth and Normal Completion Priors  from Sparse Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Jiawei Guo, HungChyun Chou, Ning Ding | cs.CV | [PDF](http://arxiv.org/pdf/2407.05666v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) are an advanced technology that creates highly
realistic images by learning about scenes through a neural network model.
However, NeRF often encounters issues when there are not enough images to work
with, leading to problems in accurately rendering views. The main issue is that
NeRF lacks sufficient structural details to guide the rendering process
accurately. To address this, we proposed a Depth and Normal Dense Completion
Priors for NeRF (CP\_NeRF) framework. This framework enhances view rendering by
adding depth and normal dense completion priors to the NeRF optimization
process. Before optimizing NeRF, we obtain sparse depth maps using the
Structure from Motion (SfM) technique used to get camera poses. Based on the
sparse depth maps and a normal estimator, we generate sparse normal maps for
training a normal completion prior with precise standard deviations. During
optimization, we apply depth and normal completion priors to transform sparse
data into dense depth and normal maps with their standard deviations. We use
these dense maps to guide ray sampling, assist distance sampling and construct
a normal loss function for better training accuracy. To improve the rendering
of NeRF's normal outputs, we incorporate an optical centre position embedder
that helps synthesize more accurate normals through volume rendering.
Additionally, we employ a normal patch matching technique to choose accurate
rendered normal maps, ensuring more precise supervision for the model. Our
method is superior to leading techniques in rendering detailed indoor scenes,
even with limited input views.



---

## GeoNLF: Geometry guided Pose-Free Neural LiDAR Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Weiyi Xue, Zehan Zheng, Fan Lu, Haiyun Wei, Guang Chen, Changjun Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2407.05597v1){: .btn .btn-green } |

**Abstract**: Although recent efforts have extended Neural Radiance Fields (NeRF) into
LiDAR point cloud synthesis, the majority of existing works exhibit a strong
dependence on precomputed poses. However, point cloud registration methods
struggle to achieve precise global pose estimation, whereas previous pose-free
NeRFs overlook geometric consistency in global reconstruction. In light of
this, we explore the geometric insights of point clouds, which provide explicit
registration priors for reconstruction. Based on this, we propose Geometry
guided Neural LiDAR Fields(GeoNLF), a hybrid framework performing alternately
global neural reconstruction and pure geometric pose optimization. Furthermore,
NeRFs tend to overfit individual frames and easily get stuck in local minima
under sparse-view inputs. To tackle this issue, we develop a
selective-reweighting strategy and introduce geometric constraints for robust
optimization. Extensive experiments on NuScenes and KITTI-360 datasets
demonstrate the superiority of GeoNLF in both novel view synthesis and
multi-view registration of low-frequency large-scale point clouds.



---

## Dynamic Neural Radiance Field From Defocused Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Xianrui Luo, Huiqiang Sun, Juewen Peng, Zhiguo Cao | cs.CV | [PDF](http://arxiv.org/pdf/2407.05586v1){: .btn .btn-green } |

**Abstract**: Dynamic Neural Radiance Field (NeRF) from monocular videos has recently been
explored for space-time novel view synthesis and achieved excellent results.
However, defocus blur caused by depth variation often occurs in video capture,
compromising the quality of dynamic reconstruction because the lack of sharp
details interferes with modeling temporal consistency between input views. To
tackle this issue, we propose D2RF, the first dynamic NeRF method designed to
restore sharp novel views from defocused monocular videos. We introduce layered
Depth-of-Field (DoF) volume rendering to model the defocus blur and reconstruct
a sharp NeRF supervised by defocused views. The blur model is inspired by the
connection between DoF rendering and volume rendering. The opacity in volume
rendering aligns with the layer visibility in DoF rendering.To execute the
blurring, we modify the layered blur kernel to the ray-based kernel and employ
an optimized sparse kernel to gather the input rays efficiently and render the
optimized rays with our layered DoF volume rendering. We synthesize a dataset
with defocused dynamic scenes for our task, and extensive experiments on our
dataset show that our method outperforms existing approaches in synthesizing
all-in-focus novel views from defocus blur while maintaining spatial-temporal
consistency in the scene.

Comments:
- Accepted by ECCV 2024

---

## PanDORA: Casual HDR Radiance Acquisition for Indoor Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Mohammad Reza Karimi Dastjerdi, Frédéric Fortier-Chouinard, Yannick Hold-Geoffroy, Marc Hébert, Claude Demers, Nima Kalantari, Jean-François Lalonde | cs.CV | [PDF](http://arxiv.org/pdf/2407.06150v1){: .btn .btn-green } |

**Abstract**: Most novel view synthesis methods such as NeRF are unable to capture the true
high dynamic range (HDR) radiance of scenes since they are typically trained on
photos captured with standard low dynamic range (LDR) cameras. While the
traditional exposure bracketing approach which captures several images at
different exposures has recently been adapted to the multi-view case, we find
such methods to fall short of capturing the full dynamic range of indoor
scenes, which includes very bright light sources. In this paper, we present
PanDORA: a PANoramic Dual-Observer Radiance Acquisition system for the casual
capture of indoor scenes in high dynamic range. Our proposed system comprises
two 360{\deg} cameras rigidly attached to a portable tripod. The cameras
simultaneously acquire two 360{\deg} videos: one at a regular exposure and the
other at a very fast exposure, allowing a user to simply wave the apparatus
casually around the scene in a matter of minutes. The resulting images are fed
to a NeRF-based algorithm that reconstructs the scene's full high dynamic
range. Compared to HDR baselines from previous work, our approach reconstructs
the full HDR radiance of indoor scenes without sacrificing the visual quality
while retaining the ease of capture from recent NeRF-like approaches.

Comments:
- 10 pages, 8 figures

---

## RRM: Relightable assets using Radiance guided Material extraction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Diego Gomez, Julien Philip, Adrien Kaiser, Élie Michel | cs.CV | [PDF](http://arxiv.org/pdf/2407.06397v1){: .btn .btn-green } |

**Abstract**: Synthesizing NeRFs under arbitrary lighting has become a seminal problem in
the last few years. Recent efforts tackle the problem via the extraction of
physically-based parameters that can then be rendered under arbitrary lighting,
but they are limited in the range of scenes they can handle, usually
mishandling glossy scenes. We propose RRM, a method that can extract the
materials, geometry, and environment lighting of a scene even in the presence
of highly reflective objects. Our method consists of a physically-aware
radiance field representation that informs physically-based parameters, and an
expressive environment light structure based on a Laplacian Pyramid. We
demonstrate that our contributions outperform the state-of-the-art on parameter
retrieval tasks, leading to high-fidelity relighting and novel view synthesis
on surfacic scenes.

Comments:
- Paper accepted and presented at CGI 2024

---

## PICA: Physics-Integrated Clothed Avatar

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-07 | Bo Peng, Yunfan Tao, Haoyu Zhan, Yudong Guo, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2407.05324v1){: .btn .btn-green } |

**Abstract**: We introduce PICA, a novel representation for high-fidelity animatable
clothed human avatars with physics-accurate dynamics, even for loose clothing.
Previous neural rendering-based representations of animatable clothed humans
typically employ a single model to represent both the clothing and the
underlying body. While efficient, these approaches often fail to accurately
represent complex garment dynamics, leading to incorrect deformations and
noticeable rendering artifacts, especially for sliding or loose garments.
Furthermore, previous works represent garment dynamics as pose-dependent
deformations and facilitate novel pose animations in a data-driven manner. This
often results in outcomes that do not faithfully represent the mechanics of
motion and are prone to generating artifacts in out-of-distribution poses. To
address these issues, we adopt two individual 3D Gaussian Splatting (3DGS)
models with different deformation characteristics, modeling the human body and
clothing separately. This distinction allows for better handling of their
respective motion characteristics. With this representation, we integrate a
graph neural network (GNN)-based clothed body physics simulation module to
ensure an accurate representation of clothing dynamics. Our method, through its
carefully designed features, achieves high-fidelity rendering of clothed human
bodies in complex and novel driving poses, significantly outperforming previous
methods under the same settings.

Comments:
- Project page: https://ustc3dv.github.io/PICA/

---

## GaussReg: Fast 3D Registration with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-07 | Jiahao Chang, Yinglin Xu, Yihao Li, Yuantao Chen, Xiaoguang Han | cs.CV | [PDF](http://arxiv.org/pdf/2407.05254v1){: .btn .btn-green } |

**Abstract**: Point cloud registration is a fundamental problem for large-scale 3D scene
scanning and reconstruction. With the help of deep learning, registration
methods have evolved significantly, reaching a nearly-mature stage. As the
introduction of Neural Radiance Fields (NeRF), it has become the most popular
3D scene representation as its powerful view synthesis capabilities. Regarding
NeRF representation, its registration is also required for large-scale scene
reconstruction. However, this topic extremly lacks exploration. This is due to
the inherent challenge to model the geometric relationship among two scenes
with implicit representations. The existing methods usually convert the
implicit representation to explicit representation for further registration.
Most recently, Gaussian Splatting (GS) is introduced, employing explicit 3D
Gaussian. This method significantly enhances rendering speed while maintaining
high rendering quality. Given two scenes with explicit GS representations, in
this work, we explore the 3D registration task between them. To this end, we
propose GaussReg, a novel coarse-to-fine framework, both fast and accurate. The
coarse stage follows existing point cloud registration methods and estimates a
rough alignment for point clouds from GS. We further newly present an
image-guided fine registration approach, which renders images from GS to
provide more detailed geometric information for precise alignment. To support
comprehensive evaluation, we carefully build a scene-level dataset called
ScanNet-GSReg with 1379 scenes obtained from the ScanNet dataset and collect an
in-the-wild dataset called GSReg. Experimental results demonstrate our method
achieves state-of-the-art performance on multiple datasets. Our GaussReg is 44
times faster than HLoc (SuperPoint as the feature extractor and SuperGlue as
the matcher) with comparable accuracy.

Comments:
- ECCV 2024

---

## SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-06 | Weixing Xie, Junfeng Yao, Xianpeng Cao, Qiqin Lin, Zerui Tang, Xiao Dong, Xiaohu Guo | cs.CV | [PDF](http://arxiv.org/pdf/2407.05023v1){: .btn .btn-green } |

**Abstract**: Dynamic reconstruction of deformable tissues in endoscopic video is a key
technology for robot-assisted surgery. Recent reconstruction methods based on
neural radiance fields (NeRFs) have achieved remarkable results in the
reconstruction of surgical scenes. However, based on implicit representation,
NeRFs struggle to capture the intricate details of objects in the scene and
cannot achieve real-time rendering. In addition, restricted single view
perception and occluded instruments also propose special challenges in surgical
scene reconstruction. To address these issues, we develop SurgicalGaussian, a
deformable 3D Gaussian Splatting method to model dynamic surgical scenes. Our
approach models the spatio-temporal features of soft tissues at each time stamp
via a forward-mapping deformation MLP and regularization to constrain local 3D
Gaussians to comply with consistent movement. With the depth initialization
strategy and tool mask-guided training, our method can remove surgical
instruments and reconstruct high-fidelity surgical scenes. Through experiments
on various surgical videos, our network outperforms existing method on many
aspects, including rendering quality, rendering speed and GPU usage. The
project page can be found at https://surgicalgaussian.github.io.



---

## Gaussian Eigen Models for Human Heads

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-05 | Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2407.04545v1){: .btn .btn-green } |

**Abstract**: We present personalized Gaussian Eigen Models (GEMs) for human heads, a novel
method that compresses dynamic 3D Gaussians into low-dimensional linear spaces.
Our approach is inspired by the seminal work of Blanz and Vetter, where a
mesh-based 3D morphable model (3DMM) is constructed from registered meshes.
Based on dynamic 3D Gaussians, we create a lower-dimensional representation of
primitives that applies to most 3DGS head avatars. Specifically, we propose a
universal method to distill the appearance of a mesh-controlled UNet Gaussian
avatar using an ensemble of linear eigenbasis. We replace heavy CNN-based
architectures with a single linear layer improving speed and enabling a range
of real-time downstream applications. To create a particular facial expression,
one simply needs to perform a dot product between the eigen coefficients and
the distilled basis. This efficient method removes the requirement for an input
mesh during testing, enhancing simplicity and speed in expression generation.
This process is highly efficient and supports real-time rendering on everyday
devices, leveraging the effectiveness of standard Gaussian Splatting. In
addition, we demonstrate how the GEM can be controlled using a ResNet-based
regression architecture. We show and compare self-reenactment and cross-person
reenactment to state-of-the-art 3D avatar methods, demonstrating higher quality
and better control. A real-time demo showcases the applicability of the GEM
representation.

Comments:
- https://zielon.github.io/gem/

---

## GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-05 | Yuxuan Mu, Xinxin Zuo, Chuan Guo, Yilin Wang, Juwei Lu, Xiaofeng Wu, Songcen Xu, Peng Dai, Youliang Yan, Li Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2407.04237v2){: .btn .btn-green } |

**Abstract**: We present GSD, a diffusion model approach based on Gaussian Splatting (GS)
representation for 3D object reconstruction from a single view. Prior works
suffer from inconsistent 3D geometry or mediocre rendering quality due to
improper representations. We take a step towards resolving these shortcomings
by utilizing the recent state-of-the-art 3D explicit representation, Gaussian
Splatting, and an unconditional diffusion model. This model learns to generate
3D objects represented by sets of GS ellipsoids. With these strong generative
3D priors, though learning unconditionally, the diffusion model is ready for
view-guided reconstruction without further model fine-tuning. This is achieved
by propagating fine-grained 2D features through the efficient yet flexible
splatting function and the guided denoising sampling process. In addition, a 2D
diffusion model is further employed to enhance rendering fidelity, and improve
reconstructed GS quality by polishing and re-using the rendered images. The
final reconstructed objects explicitly come with high-quality 3D structure and
texture, and can be efficiently rendered in arbitrary views. Experiments on the
challenging real-world CO3D dataset demonstrate the superiority of our
approach. Project page: $\href{https://yxmu.foo/GSD/}{\text{this https URL}}$

Comments:
- Accepted for ECCV 2024

---

## Segment Any 4D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-05 | Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, Xinggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2407.04504v2){: .btn .btn-green } |

**Abstract**: Modeling, understanding, and reconstructing the real world are crucial in
XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable
success in modeling and understanding 3D scenes. Similarly, various 4D
representations have demonstrated the ability to capture the dynamics of the 4D
world. However, there is a dearth of research focusing on segmentation within
4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D),
one of the first frameworks to segment anything in the 4D digital world based
on 4D Gaussians. In SA4D, an efficient temporal identity feature field is
introduced to handle Gaussian drifting, with the potential to learn precise
identity features from noisy and sparse input. Additionally, a 4D segmentation
refinement process is proposed to remove artifacts. Our SA4D achieves precise,
high-quality segmentation within seconds in 4D Gaussians and shows the ability
to remove, recolor, compose, and render high-quality anything masks. More demos
are available at: https://jsxzs.github.io/sa4d/.

Comments:
- 22 pages

---

## SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-04 | Yijia Guo, Liwen Hu, Lei Ma, Tiejun Huang | cs.CV | [PDF](http://arxiv.org/pdf/2407.03771v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance
in 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.
Fulfilling this requirement can be challenging in real-world scenarios
especially when the camera moves fast, which severely limits the application of
3DGS. To address these challenges, we proposed Spike Gausian Splatting
(SpikeGS), the first framework that integrates the spike streams into 3DGS
pipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With
accumulation rasterization, interval supervision, and a specially designed
pipeline, SpikeGS extracts detailed geometry and texture from high temporal
resolution but texture lacking spike stream, reconstructs 3D scenes captured in
1 second. Extensive experiments on multiple synthetic and real-world datasets
demonstrate the superiority of SpikeGS compared with existing spike-based and
deblur 3D scene reconstruction methods. Codes and data will be released soon.



---

## PFGS: High Fidelity Point Cloud Rendering via Feature Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-04 | Jiaxu Wang, Ziyi Zhang, Junhao He, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2407.03857v1){: .btn .btn-green } |

**Abstract**: Rendering high-fidelity images from sparse point clouds is still challenging.
Existing learning-based approaches suffer from either hole artifacts, missing
details, or expensive computations. In this paper, we propose a novel framework
to render high-quality images from sparse points. This method first attempts to
bridge the 3D Gaussian Splatting and point cloud rendering, which includes
several cascaded modules. We first use a regressor to estimate Gaussian
properties in a point-wise manner, the estimated properties are used to
rasterize neural feature descriptors into 2D planes which are extracted from a
multiscale extractor. The projected feature volume is gradually decoded toward
the final prediction via a multiscale and progressive decoder. The whole
pipeline experiences a two-stage training and is driven by our well-designed
progressive and multiscale reconstruction loss. Experiments on different
benchmarks show the superiority of our method in terms of rendering qualities
and the necessities of our main components.



---

## CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion  Blur Images

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-04 | Junghe Lee, Donghyeong Kim, Dogyoon Lee, Suhwan Cho, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2407.03923v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) have received significant attention due to
their high-quality novel view rendering ability, prompting research to address
various real-world cases. One critical challenge is the camera motion blur
caused by camera movement during exposure time, which prevents accurate 3D
scene reconstruction. In this study, we propose continuous rigid motion-aware
gaussian splatting (CRiM-GS) to reconstruct accurate 3D scene from blurry
images with real-time rendering speed. Considering the actual camera motion
blurring process, which consists of complex motion patterns, we predict the
continuous movement of the camera based on neural ordinary differential
equations (ODEs). Specifically, we leverage rigid body transformations to model
the camera motion with proper regularization, preserving the shape and size of
the object. Furthermore, we introduce a continuous deformable 3D transformation
in the \textit{SE(3)} field to adapt the rigid body transformation to
real-world problems by ensuring a higher degree of freedom. By revisiting
fundamental camera theory and employing advanced neural network training
techniques, we achieve accurate modeling of continuous camera trajectories. We
conduct extensive experiments, demonstrating state-of-the-art performance both
quantitatively and qualitatively on benchmark datasets.

Comments:
- Project Page : https://jho-yonsei.github.io/CRiM-Gaussian/

---

## Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-03 | Jiaxin Guo, Jiangliu Wang, Di Kang, Wenzhen Dong, Wenting Wang, Yun-hui Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02918v1){: .btn .btn-green } |

**Abstract**: Real-time 3D reconstruction of surgical scenes plays a vital role in
computer-assisted surgery, holding a promise to enhance surgeons' visibility.
Recent advancements in 3D Gaussian Splatting (3DGS) have shown great potential
for real-time novel view synthesis of general scenes, which relies on accurate
poses and point clouds generated by Structure-from-Motion (SfM) for
initialization. However, 3DGS with SfM fails to recover accurate camera poses
and geometry in surgical scenes due to the challenges of minimal textures and
photometric inconsistencies. To tackle this problem, in this paper, we propose
the first SfM-free 3DGS-based method for surgical scene reconstruction by
jointly optimizing the camera poses and scene representation. Based on the
video continuity, the key of our method is to exploit the immediate optical
flow priors to guide the projection flow derived from 3D Gaussians. Unlike most
previous methods relying on photometric loss only, we formulate the pose
estimation problem as minimizing the flow loss between the projection flow and
optical flow. A consistency check is further introduced to filter the flow
outliers by detecting the rigid and reliable points that satisfy the epipolar
geometry. During 3D Gaussian optimization, we randomly sample frames to
optimize the scene representations to grow the 3D Gaussian progressively.
Experiments on the SCARED dataset demonstrate our superior performance over
existing methods in novel view synthesis and pose estimation with high
efficiency. Code is available at https://github.com/wrld/Free-SurGS.

Comments:
- Accepted to MICCAI 2024

---

## VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using  Learned Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-03 | Sungwon Hwang, Min-Jung Kim, Taewoong Kang, Jayeon Kang, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2407.02945v2){: .btn .btn-green } |

**Abstract**: Neural rendering-based urban scene reconstruction methods commonly rely on
images collected from driving vehicles with cameras facing and moving forward.
Although these methods can successfully synthesize from views similar to
training camera trajectory, directing the novel view outside the training
camera distribution does not guarantee on-par performance. In this paper, we
tackle the Extrapolated View Synthesis (EVS) problem by evaluating the
reconstructions on views such as looking left, right or downwards with respect
to training camera distributions. To improve rendering quality for EVS, we
initialize our model by constructing dense LiDAR map, and propose to leverage
prior scene knowledge such as surface normal estimator and large-scale
diffusion model. Qualitative and quantitative comparisons demonstrate the
effectiveness of our methods on EVS. To the best of our knowledge, we are the
first to address the EVS problem in urban scene reconstruction. Link to our
project page: https://vegs3d.github.io/.

Comments:
- The first two authors contributed equally. Project Page:
  https://vegs3d.github.io/

---

## AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Mustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02598v2){: .btn .btn-green } |

**Abstract**: Realistic scene reconstruction and view synthesis are essential for advancing
autonomous driving systems by simulating safety-critical scenarios. 3D Gaussian
Splatting excels in real-time rendering and static scene reconstructions but
struggles with modeling driving scenarios due to complex backgrounds, dynamic
objects, and sparse views. We propose AutoSplat, a framework employing Gaussian
splatting to achieve highly realistic reconstructions of autonomous driving
scenes. By imposing geometric constraints on Gaussians representing the road
and sky regions, our method enables multi-view consistent simulation of
challenging scenarios including lane changes. Leveraging 3D templates, we
introduce a reflected Gaussian consistency constraint to supervise both the
visible and unseen side of foreground objects. Moreover, to model the dynamic
appearance of foreground objects, we estimate residual spherical harmonics for
each foreground Gaussian. Extensive experiments on Pandaset and KITTI
demonstrate that AutoSplat outperforms state-of-the-art methods in scene
reconstruction and novel view synthesis across diverse driving scenarios. Visit
our project page at https://autosplat.github.io/.



---

## BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event  Stream

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Wenpu Li, Pian Wan, Peng Wang, Jinghang Li, Yi Zhou, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02174v2){: .btn .btn-green } |

**Abstract**: Neural implicit representation of visual scenes has attracted a lot of
attention in recent research of computer vision and graphics. Most prior
methods focus on how to reconstruct 3D scene representation from a set of
images. In this work, we demonstrate the possibility to recover the neural
radiance fields (NeRF) from a single blurry image and its corresponding event
stream. We model the camera motion with a cubic B-Spline in SE(3) space. Both
the blurry image and the brightness change within a time interval, can then be
synthesized from the 3D scene representation given the 6-DoF poses interpolated
from the cubic B-Spline. Our method can jointly learn both the implicit neural
scene representation and recover the camera motion by minimizing the
differences between the synthesized data and the real measurements without
pre-computed camera poses from COLMAP. We evaluate the proposed method with
both synthetic and real datasets. The experimental results demonstrate that we
are able to render view-consistent latent sharp images from the learned NeRF
and bring a blurry image alive in high quality. Code and data are available at
https://github.com/WU-CVGL/BeNeRF.

Comments:
- Accepted to ECCV 2024

---

## TrAME: Trajectory-Anchored Multi-View Editing for Text-Guided 3D  Gaussian Splatting Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Chaofan Luo, Donglin Di, Yongjia Ma, Zhou Xue, Chen Wei, Xun Yang, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02034v1){: .btn .btn-green } |

**Abstract**: Despite significant strides in the field of 3D scene editing, current methods
encounter substantial challenge, particularly in preserving 3D consistency in
multi-view editing process. To tackle this challenge, we propose a progressive
3D editing strategy that ensures multi-view consistency via a
Trajectory-Anchored Scheme (TAS) with a dual-branch editing mechanism.
Specifically, TAS facilitates a tightly coupled iterative process between 2D
view editing and 3D updating, preventing error accumulation yielded from
text-to-image process. Additionally, we explore the relationship between
optimization-based methods and reconstruction-based methods, offering a unified
perspective for selecting superior design choice, supporting the rationale
behind the designed TAS. We further present a tuning-free View-Consistent
Attention Control (VCAC) module that leverages cross-view semantic and
geometric reference from the source branch to yield aligned views from the
target branch during the editing of 2D views. To validate the effectiveness of
our method, we analyze 2D examples to demonstrate the improved consistency with
the VCAC module. Further extensive quantitative and qualitative results in
text-guided 3D scene editing indicate that our method achieves superior editing
quality compared to state-of-the-art methods. We will make the complete
codebase publicly available following the conclusion of the double-blind review
process.



---

## MomentsNeRF: Leveraging Orthogonal Moments for Few-Shot Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Ahmad AlMughrabi, Ricardo Marques, Petia Radeva | cs.CV | [PDF](http://arxiv.org/pdf/2407.02668v1){: .btn .btn-green } |

**Abstract**: We propose MomentsNeRF, a novel framework for one- and few-shot neural
rendering that predicts a neural representation of a 3D scene using Orthogonal
Moments. Our architecture offers a new transfer learning method to train on
multi-scenes and incorporate a per-scene optimization using one or a few images
at test time. Our approach is the first to successfully harness features
extracted from Gabor and Zernike moments, seamlessly integrating them into the
NeRF architecture. We show that MomentsNeRF performs better in synthesizing
images with complex textures and shapes, achieving a significant noise
reduction, artifact elimination, and completing the missing parts compared to
the recent one- and few-shot neural rendering frameworks. Extensive experiments
on the DTU and Shapenet datasets show that MomentsNeRF improves the
state-of-the-art by {3.39\;dB\;PSNR}, 11.1% SSIM, 17.9% LPIPS, and 8.3% DISTS
metrics. Moreover, it outperforms state-of-the-art performance for both novel
view synthesis and single-image 3D view reconstruction. The source code is
accessible at: https://amughrabi.github.io/momentsnerf/.



---

## Active Human Pose Estimation via an Autonomous UAV Agent

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Jingxi Chen, Botao He, Chahat Deep Singh, Cornelia Fermuller, Yiannis Aloimonos | cs.RO | [PDF](http://arxiv.org/pdf/2407.01811v1){: .btn .btn-green } |

**Abstract**: One of the core activities of an active observer involves moving to secure a
"better" view of the scene, where the definition of "better" is task-dependent.
This paper focuses on the task of human pose estimation from videos capturing a
person's activity. Self-occlusions within the scene can complicate or even
prevent accurate human pose estimation. To address this, relocating the camera
to a new vantage point is necessary to clarify the view, thereby improving 2D
human pose estimation. This paper formalizes the process of achieving an
improved viewpoint. Our proposed solution to this challenge comprises three
main components: a NeRF-based Drone-View Data Generation Framework, an On-Drone
Network for Camera View Error Estimation, and a Combined Planner for devising a
feasible motion plan to reposition the camera based on the predicted errors for
camera views. The Data Generation Framework utilizes NeRF-based methods to
generate a comprehensive dataset of human poses and activities, enhancing the
drone's adaptability in various scenarios. The Camera View Error Estimation
Network is designed to evaluate the current human pose and identify the most
promising next viewing angles for the drone, ensuring a reliable and precise
pose estimation from those angles. Finally, the combined planner incorporates
these angles while considering the drone's physical and environmental
limitations, employing efficient algorithms to navigate safe and effective
flight paths. This system represents a significant advancement in active 2D
human pose estimation for an autonomous UAV agent, offering substantial
potential for applications in aerial cinematography by improving the
performance of autonomous human pose estimation and maintaining the operational
safety and efficiency of UAVs.



---

## DRAGON: Drone and Ground Gaussian Splatting for 3D Building  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Yujin Ham, Mateusz Michalkiewicz, Guha Balakrishnan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01761v1){: .btn .btn-green } |

**Abstract**: 3D building reconstruction from imaging data is an important task for many
applications ranging from urban planning to reconnaissance. Modern Novel View
synthesis (NVS) methods like NeRF and Gaussian Splatting offer powerful
techniques for developing 3D models from natural 2D imagery in an unsupervised
fashion. These algorithms generally require input training views surrounding
the scene of interest, which, in the case of large buildings, is typically not
available across all camera elevations. In particular, the most readily
available camera viewpoints at scale across most buildings are at near-ground
(e.g., with mobile phones) and aerial (drones) elevations. However, due to the
significant difference in viewpoint between drone and ground image sets, camera
registration - a necessary step for NVS algorithms - fails. In this work we
propose a method, DRAGON, that can take drone and ground building imagery as
input and produce a 3D NVS model. The key insight of DRAGON is that
intermediate elevation imagery may be extrapolated by an NVS algorithm itself
in an iterative procedure with perceptual regularization, thereby bridging the
visual feature gap between the two elevations and enabling registration. We
compiled a semi-synthetic dataset of 9 large building scenes using Google Earth
Studio, and quantitatively and qualitatively demonstrate that DRAGON can
generate compelling renderings on this dataset compared to baseline strategies.

Comments:
- 12 pages, 9 figures, accepted to ICCP 2024

---

## Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Zihan Gao, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Yuwei Guo, Shuyuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2407.01220v1){: .btn .btn-green } |

**Abstract**: Understanding 3D scenes is a crucial challenge in computer vision research
with applications spanning multiple domains. Recent advancements in distilling
2D vision-language foundation models into neural fields, like NeRF and 3DGS,
enables open-vocabulary segmentation of 3D scenes from 2D multi-view images
without the need for precise 3D annotations. While effective, however, the
per-pixel distillation of high-dimensional CLIP features introduces ambiguity
and necessitates complex regularization strategies, adding inefficiencies
during training. This paper presents MaskField, which enables fast and
efficient 3D open-vocabulary segmentation with neural fields under weak
supervision. Unlike previous methods, MaskField distills masks rather than
dense high-dimensional CLIP features. MaskFields employ neural fields as binary
mask generators and supervise them with masks generated by SAM and classified
by coarse CLIP features. MaskField overcomes the ambiguous object boundaries by
naturally introducing SAM segmented object shapes without extra regularization
during training. By circumventing the direct handling of high-dimensional CLIP
features during training, MaskField is particularly compatible with explicit
scene representations like 3DGS. Our extensive experiments show that MaskField
not only surpasses prior state-of-the-art methods but also achieves remarkably
fast convergence, outperforming previous methods with just 5 minutes of
training. We hope that MaskField will inspire further exploration into how
neural fields can be trained to comprehend 3D scenes from 2D models.

Comments:
- 16 pages, 7 figures

---

## EndoSparse: Real-Time Sparse View Synthesis of Endoscopic Scenes using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Chenxin Li, Brandon Y. Feng, Yifan Liu, Hengyu Liu, Cheng Wang, Weihao Yu, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01029v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction of biological tissues from a collection of endoscopic
images is a key to unlock various important downstream surgical applications
with 3D capabilities. Existing methods employ various advanced neural rendering
techniques for photorealistic view synthesis, but they often struggle to
recover accurate 3D representations when only sparse observations are
available, which is usually the case in real-world clinical scenarios. To
tackle this {sparsity} challenge, we propose a framework leveraging the prior
knowledge from multiple foundation models during the reconstruction process,
dubbed as \textit{EndoSparse}. Experimental results indicate that our proposed
strategy significantly improves the geometric and appearance quality under
challenging sparse-view conditions, including using only three views. In
rigorous benchmarking experiments against state-of-the-art methods,
\textit{EndoSparse} achieves superior results in terms of accurate geometry,
realistic appearance, and rendering efficiency, confirming the robustness to
sparse-view limitations in endoscopic reconstruction. \textit{EndoSparse}
signifies a steady step towards the practical deployment of neural 3D
reconstruction in real-world clinical scenarios. Project page:
https://endo-sparse.github.io/.

Comments:
- Accpeted by MICCAI2024

---

## GaussianStego: A Generalizable Stenography Pipeline for Generative 3D  Gaussians Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Chenxin Li, Hengyu Liu, Zhiwen Fan, Wuyang Li, Yifan Liu, Panwang Pan, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01301v1){: .btn .btn-green } |

**Abstract**: Recent advancements in large generative models and real-time neural rendering
using point-based techniques pave the way for a future of widespread visual
data distribution through sharing synthesized 3D assets. However, while
standardized methods for embedding proprietary or copyright information, either
overtly or subtly, exist for conventional visual content such as images and
videos, this issue remains unexplored for emerging generative 3D formats like
Gaussian Splatting. We present GaussianStego, a method for embedding
steganographic information in the rendering of generated 3D assets. Our
approach employs an optimization framework that enables the accurate extraction
of hidden information from images rendered using Gaussian assets derived from
large models, while maintaining their original visual quality. We conduct
preliminary evaluations of our method across several potential deployment
scenarios and discuss issues identified through analysis. GaussianStego
represents an initial exploration into the novel challenge of embedding
customizable, imperceptible, and recoverable information within the renders
produced by current 3D generative models, while ensuring minimal impact on the
rendered content's quality.

Comments:
- Project website: https://gaussian-stego.github.io/
