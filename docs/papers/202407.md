---
layout: default
title: July 2024
parent: Papers
nav_order: 202407
---

<!---metadata--->


## WildGaussians: 3D Gaussian Splatting in the Wild

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, Torsten Sattler | cs.CV | [PDF](http://arxiv.org/pdf/2407.08447v1){: .btn .btn-green } |

**Abstract**: While the field of 3D scene reconstruction is dominated by NeRFs due to their
photorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,
offering similar quality with real-time rendering speeds. However, both methods
primarily excel with well-controlled 3D scenes, while in-the-wild data -
characterized by occlusions, dynamic objects, and varying illumination -
remains challenging. NeRFs can adapt to such conditions easily through
per-image embedding vectors, but 3DGS struggles due to its explicit
representation and lack of shared parameters. To address this, we introduce
WildGaussians, a novel approach to handle occlusions and appearance changes
with 3DGS. By leveraging robust DINO features and integrating an appearance
modeling module within 3DGS, our method achieves state-of-the-art results. We
demonstrate that WildGaussians matches the real-time rendering speed of 3DGS
while surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all
within a simple architectural framework.

Comments:
- https://wild-gaussians.github.io/

---

## Survey on Fundamental Deep Learning 3D Reconstruction Techniques

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Yonge Bai, LikHang Wong, TszYin Twan | cs.CV | [PDF](http://arxiv.org/pdf/2407.08137v1){: .btn .btn-green } |

**Abstract**: This survey aims to investigate fundamental deep learning (DL) based 3D
reconstruction techniques that produce photo-realistic 3D models and scenes,
highlighting Neural Radiance Fields (NeRFs), Latent Diffusion Models (LDM), and
3D Gaussian Splatting. We dissect the underlying algorithms, evaluate their
strengths and tradeoffs, and project future research trajectories in this
rapidly evolving field. We provide a comprehensive overview of the fundamental
in DL-driven 3D scene reconstruction, offering insights into their potential
applications and limitations.



---

## MeshAvatar: Learning High-quality Triangular Human Avatars from  Multi-view Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Yushuo Chen, Zerong Zheng, Zhe Li, Chao Xu, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.08414v1){: .btn .btn-green } |

**Abstract**: We present a novel pipeline for learning high-quality triangular human
avatars from multi-view videos. Recent methods for avatar learning are
typically based on neural radiance fields (NeRF), which is not compatible with
traditional graphics pipeline and poses great challenges for operations like
editing or synthesizing under different environments. To overcome these
limitations, our method represents the avatar with an explicit triangular mesh
extracted from an implicit SDF field, complemented by an implicit material
field conditioned on given poses. Leveraging this triangular avatar
representation, we incorporate physics-based rendering to accurately decompose
geometry and texture. To enhance both the geometric and appearance details, we
further employ a 2D UNet as the network backbone and introduce pseudo normal
ground-truth as additional supervision. Experiments show that our method can
learn triangular avatars with high-quality geometry reconstruction and
plausible material decomposition, inherently supporting editing, manipulation
or relighting operations.

Comments:
- Project Page: https://shad0wta9.github.io/meshavatar-page/

---

## Explicit_NeRF_QA: A Quality Assessment Database for Explicit NeRF Model  Compression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Yuke Xing, Qi Yang, Kaifa Yang, Yilin Xu, Zhu Li | eess.IV | [PDF](http://arxiv.org/pdf/2407.08165v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Fields (NeRF) have demonstrated significant
advantages in representing and synthesizing 3D scenes. Explicit NeRF models
facilitate the practical NeRF applications with faster rendering speed, and
also attract considerable attention in NeRF compression due to its huge storage
cost. To address the challenge of the NeRF compression study, in this paper, we
construct a new dataset, called Explicit_NeRF_QA. We use 22 3D objects with
diverse geometries, textures, and material complexities to train four typical
explicit NeRF models across five parameter levels. Lossy compression is
introduced during the model generation, pivoting the selection of key
parameters such as hash table size for InstantNGP and voxel grid resolution for
Plenoxels. By rendering NeRF samples to processed video sequences (PVS), a
large scale subjective experiment with lab environment is conducted to collect
subjective scores from 21 viewers. The diversity of content, accuracy of mean
opinion scores (MOS), and characteristics of NeRF distortion are
comprehensively presented, establishing the heterogeneity of the proposed
dataset. The state-of-the-art objective metrics are tested in the new dataset.
Best Person correlation, which is around 0.85, is collected from the
full-reference objective metric. All tested no-reference metrics report very
poor results with 0.4 to 0.6 correlations, demonstrating the need for further
development of more robust no-reference metrics. The dataset, including NeRF
samples, source 3D objects, multiview images for NeRF generation, PVSs, MOS, is
made publicly available at the following location:
https://github.com/LittlericeChloe/Explicit_NeRF_QA.

Comments:
- 5 pages, 4 figures, 2 tables, conference

---

## Bayesian uncertainty analysis for underwater 3D reconstruction with  neural radiance fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-11 | Haojie Lian, Xinhao Li, Yilin Qu, Jing Du, Zhuxuan Meng, Jie Liu, Leilei Chen | cs.CE | [PDF](http://arxiv.org/pdf/2407.08154v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) are a deep learning technique that can
generate novel views of 3D scenes using sparse 2D images from different viewing
directions and camera poses. As an extension of conventional NeRFs in
underwater environment, where light can get absorbed and scattered by water,
SeaThru-NeRF was proposed to separate the clean appearance and geometric
structure of underwater scene from the effects of the scattering medium. Since
the quality of the appearance and structure of underwater scenes is crucial for
downstream tasks such as underwater infrastructure inspection, the reliability
of the 3D reconstruction model should be considered and evaluated. Nonetheless,
owing to the lack of ability to quantify uncertainty in 3D reconstruction of
underwater scenes under natural ambient illumination, the practical deployment
of NeRFs in unmanned autonomous underwater navigation is limited. To address
this issue, we introduce a spatial perturbation field D_omega based on Bayes'
rays in SeaThru-NeRF and perform Laplace approximation to obtain a Gaussian
distribution N(0,Sigma) of the parameters omega, where the diagonal elements of
Sigma correspond to the uncertainty at each spatial location. We also employ a
simple thresholding method to remove artifacts from the rendered results of
underwater scenes. Numerical experiments are provided to demonstrate the
effectiveness of this approach.



---

## MIGS: Multi-Identity Gaussian Splatting via Tensor Decomposition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-10 | Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras | cs.CV | [PDF](http://arxiv.org/pdf/2407.07284v1){: .btn .btn-green } |

**Abstract**: We introduce MIGS (Multi-Identity Gaussian Splatting), a novel method that
learns a single neural representation for multiple identities, using only
monocular videos. Recent 3D Gaussian Splatting (3DGS) approaches for human
avatars require per-identity optimization. However, learning a multi-identity
representation presents advantages in robustly animating humans under arbitrary
poses. We propose to construct a high-order tensor that combines all the
learnable 3DGS parameters for all the training identities. By assuming a
low-rank structure and factorizing the tensor, we model the complex rigid and
non-rigid deformations of multiple subjects in a unified network, significantly
reducing the total number of parameters. Our proposed approach leverages
information from all the training identities, enabling robust animation under
challenging unseen poses, outperforming existing approaches. We also
demonstrate how it can be extended to learn unseen identities.

Comments:
- Accepted by ECCV 2024. Project page:
  https://aggelinacha.github.io/MIGS/

---

## Drantal-NeRF: Diffusion-Based Restoration for Anti-aliasing Neural  Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-10 | Ganlin Yang, Kaidong Zhang, Jingjing Fu, Dong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.07461v1){: .btn .btn-green } |

**Abstract**: Aliasing artifacts in renderings produced by Neural Radiance Field (NeRF) is
a long-standing but complex issue in the field of 3D implicit representation,
which arises from a multitude of intricate causes and was mitigated by
designing more advanced but complex scene parameterization methods before. In
this paper, we present a Diffusion-based restoration method for anti-aliasing
Neural Radiance Field (Drantal-NeRF). We consider the anti-aliasing issue from
a low-level restoration perspective by viewing aliasing artifacts as a kind of
degradation model added to clean ground truths. By leveraging the powerful
prior knowledge encapsulated in diffusion model, we could restore the
high-realism anti-aliasing renderings conditioned on aliased low-quality
counterparts. We further employ a feature-wrapping operation to ensure
multi-view restoration consistency and finetune the VAE decoder to better adapt
to the scene-specific data distribution. Our proposed method is easy to
implement and agnostic to various NeRF backbones. We conduct extensive
experiments on challenging large-scale urban scenes as well as unbounded
360-degree scenes and achieve substantial qualitative and quantitative
improvements.



---

## Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-10 | Qi Song, Ziyuan Luo, Ka Chun Cheung, Simon See, Renjie Wan | cs.CV | [PDF](http://arxiv.org/pdf/2407.07735v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have become a key method for 3D scene
representation. With the rising prominence and influence of NeRF, safeguarding
its intellectual property has become increasingly important. In this paper, we
propose \textbf{NeRFProtector}, which adopts a plug-and-play strategy to
protect NeRF's copyright during its creation. NeRFProtector utilizes a
pre-trained watermarking base model, enabling NeRF creators to embed binary
messages directly while creating their NeRF. Our plug-and-play property ensures
NeRF creators can flexibly choose NeRF variants without excessive
modifications. Leveraging our newly designed progressive distillation, we
demonstrate performance on par with several leading-edge neural rendering
methods. Our project is available at:
\url{https://qsong2001.github.io/NeRFProtector}.

Comments:
- Accepted by ECCV2024

---

## Reference-based Controllable Scene Stylization with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-09 | Yiqun Mei, Jiacong Xu, Vishal M. Patel | cs.CV | [PDF](http://arxiv.org/pdf/2407.07220v1){: .btn .btn-green } |

**Abstract**: Referenced-based scene stylization that edits the appearance based on a
content-aligned reference image is an emerging research area. Starting with a
pretrained neural radiance field (NeRF), existing methods typically learn a
novel appearance that matches the given style. Despite their effectiveness,
they inherently suffer from time-consuming volume rendering, and thus are
impractical for many real-time applications. In this work, we propose ReGS,
which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to
enable real-time stylized view synthesis. Editing the appearance of a
pretrained 3DGS is challenging as it uses discrete Gaussians as 3D
representation, which tightly bind appearance with geometry. Simply optimizing
the appearance as prior methods do is often insufficient for modeling
continuous textures in the given reference image. To address this challenge, we
propose a novel texture-guided control mechanism that adaptively adjusts local
responsible Gaussians to a new geometric arrangement, serving for desired
texture details. The proposed process is guided by texture clues for effective
appearance editing, and regularized by scene depth for preserving original
geometric structure. With these novel designs, we show ReGs can produce
state-of-the-art stylization results that respect the reference texture while
embracing real-time rendering speed for free-view navigation.



---

## 3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-09 | Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, Zan Gojcic | cs.GR | [PDF](http://arxiv.org/pdf/2407.07090v2){: .btn .btn-green } |

**Abstract**: Particle-based representations of radiance fields such as 3D Gaussian
Splatting have found great success for reconstructing and re-rendering of
complex scenes. Most existing methods render particles via rasterization,
projecting them to screen space tiles for processing in a sorted order. This
work instead considers ray tracing the particles, building a bounding volume
hierarchy and casting a ray for each pixel using high-performance GPU ray
tracing hardware. To efficiently handle large numbers of semi-transparent
particles, we describe a specialized rendering algorithm which encapsulates
particles with bounding meshes to leverage fast ray-triangle intersections, and
shades batches of intersections in depth-order. The benefits of ray tracing are
well-known in computer graphics: processing incoherent rays for secondary
lighting effects such as shadows and reflections, rendering from
highly-distorted cameras common in robotics, stochastically sampling rays, and
more. With our renderer, this flexibility comes at little cost compared to
rasterization. Experiments demonstrate the speed and accuracy of our approach,
as well as several applications in computer graphics and vision. We further
propose related improvements to the basic Gaussian representation, including a
simple use of generalized kernel functions which significantly reduces particle
hit counts.

Comments:
- Project page: https://gaussiantracer.github.io/

---

## Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-09 | Dogyoon Lee, Donghyeong Kim, Jungho Lee, Minhyeok Lee, Seunghoon Lee, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2407.06613v1){: .btn .btn-green } |

**Abstract**: Recent studies construct deblurred neural radiance fields (DeRF) using dozens
of blurry images, which are not practical scenarios if only a limited number of
blurry images are available. This paper focuses on constructing DeRF from
sparse-view for more pragmatic real-world scenarios. As observed in our
experiments, establishing DeRF from sparse views proves to be a more
challenging problem due to the inherent complexity arising from the
simultaneous optimization of blur kernels and NeRF from sparse view.
Sparse-DeRF successfully regularizes the complicated joint optimization,
presenting alleviated overfitting artifacts and enhanced quality on radiance
fields. The regularization consists of three key components: Surface
smoothness, helps the model accurately predict the scene structure utilizing
unseen and additional hidden rays derived from the blur kernel based on
statistical tendencies of real-world; Modulated gradient scaling, helps the
model adjust the amount of the backpropagated gradient according to the
arrangements of scene objects; Perceptual distillation improves the perceptual
quality by overcoming the ill-posed multi-view inconsistency of image
deblurring and distilling the pre-filtered information, compensating for the
lack of clean information in blurry images. We demonstrate the effectiveness of
the Sparse-DeRF with extensive quantitative and qualitative experimental
results by training DeRF from 2-view, 4-view, and 6-view blurry images.

Comments:
- Project page: https://dogyoonlee.github.io/sparsederf/

---

## Enhancing Neural Radiance Fields with Depth and Normal Completion Priors  from Sparse Views

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Jiawei Guo, HungChyun Chou, Ning Ding | cs.CV | [PDF](http://arxiv.org/pdf/2407.05666v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) are an advanced technology that creates highly
realistic images by learning about scenes through a neural network model.
However, NeRF often encounters issues when there are not enough images to work
with, leading to problems in accurately rendering views. The main issue is that
NeRF lacks sufficient structural details to guide the rendering process
accurately. To address this, we proposed a Depth and Normal Dense Completion
Priors for NeRF (CP\_NeRF) framework. This framework enhances view rendering by
adding depth and normal dense completion priors to the NeRF optimization
process. Before optimizing NeRF, we obtain sparse depth maps using the
Structure from Motion (SfM) technique used to get camera poses. Based on the
sparse depth maps and a normal estimator, we generate sparse normal maps for
training a normal completion prior with precise standard deviations. During
optimization, we apply depth and normal completion priors to transform sparse
data into dense depth and normal maps with their standard deviations. We use
these dense maps to guide ray sampling, assist distance sampling and construct
a normal loss function for better training accuracy. To improve the rendering
of NeRF's normal outputs, we incorporate an optical centre position embedder
that helps synthesize more accurate normals through volume rendering.
Additionally, we employ a normal patch matching technique to choose accurate
rendered normal maps, ensuring more precise supervision for the model. Our
method is superior to leading techniques in rendering detailed indoor scenes,
even with limited input views.



---

## Dynamic Neural Radiance Field From Defocused Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Xianrui Luo, Huiqiang Sun, Juewen Peng, Zhiguo Cao | cs.CV | [PDF](http://arxiv.org/pdf/2407.05586v1){: .btn .btn-green } |

**Abstract**: Dynamic Neural Radiance Field (NeRF) from monocular videos has recently been
explored for space-time novel view synthesis and achieved excellent results.
However, defocus blur caused by depth variation often occurs in video capture,
compromising the quality of dynamic reconstruction because the lack of sharp
details interferes with modeling temporal consistency between input views. To
tackle this issue, we propose D2RF, the first dynamic NeRF method designed to
restore sharp novel views from defocused monocular videos. We introduce layered
Depth-of-Field (DoF) volume rendering to model the defocus blur and reconstruct
a sharp NeRF supervised by defocused views. The blur model is inspired by the
connection between DoF rendering and volume rendering. The opacity in volume
rendering aligns with the layer visibility in DoF rendering.To execute the
blurring, we modify the layered blur kernel to the ray-based kernel and employ
an optimized sparse kernel to gather the input rays efficiently and render the
optimized rays with our layered DoF volume rendering. We synthesize a dataset
with defocused dynamic scenes for our task, and extensive experiments on our
dataset show that our method outperforms existing approaches in synthesizing
all-in-focus novel views from defocus blur while maintaining spatial-temporal
consistency in the scene.

Comments:
- Accepted by ECCV 2024

---

## GeoNLF: Geometry guided Pose-Free Neural LiDAR Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Weiyi Xue, Zehan Zheng, Fan Lu, Haiyun Wei, Guang Chen, Changjun Jiang | cs.CV | [PDF](http://arxiv.org/pdf/2407.05597v1){: .btn .btn-green } |

**Abstract**: Although recent efforts have extended Neural Radiance Fields (NeRF) into
LiDAR point cloud synthesis, the majority of existing works exhibit a strong
dependence on precomputed poses. However, point cloud registration methods
struggle to achieve precise global pose estimation, whereas previous pose-free
NeRFs overlook geometric consistency in global reconstruction. In light of
this, we explore the geometric insights of point clouds, which provide explicit
registration priors for reconstruction. Based on this, we propose Geometry
guided Neural LiDAR Fields(GeoNLF), a hybrid framework performing alternately
global neural reconstruction and pure geometric pose optimization. Furthermore,
NeRFs tend to overfit individual frames and easily get stuck in local minima
under sparse-view inputs. To tackle this issue, we develop a
selective-reweighting strategy and introduce geometric constraints for robust
optimization. Extensive experiments on NuScenes and KITTI-360 datasets
demonstrate the superiority of GeoNLF in both novel view synthesis and
multi-view registration of low-frequency large-scale point clouds.



---

## PanDORA: Casual HDR Radiance Acquisition for Indoor Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Mohammad Reza Karimi Dastjerdi, Frédéric Fortier-Chouinard, Yannick Hold-Geoffroy, Marc Hébert, Claude Demers, Nima Kalantari, Jean-François Lalonde | cs.CV | [PDF](http://arxiv.org/pdf/2407.06150v1){: .btn .btn-green } |

**Abstract**: Most novel view synthesis methods such as NeRF are unable to capture the true
high dynamic range (HDR) radiance of scenes since they are typically trained on
photos captured with standard low dynamic range (LDR) cameras. While the
traditional exposure bracketing approach which captures several images at
different exposures has recently been adapted to the multi-view case, we find
such methods to fall short of capturing the full dynamic range of indoor
scenes, which includes very bright light sources. In this paper, we present
PanDORA: a PANoramic Dual-Observer Radiance Acquisition system for the casual
capture of indoor scenes in high dynamic range. Our proposed system comprises
two 360{\deg} cameras rigidly attached to a portable tripod. The cameras
simultaneously acquire two 360{\deg} videos: one at a regular exposure and the
other at a very fast exposure, allowing a user to simply wave the apparatus
casually around the scene in a matter of minutes. The resulting images are fed
to a NeRF-based algorithm that reconstructs the scene's full high dynamic
range. Compared to HDR baselines from previous work, our approach reconstructs
the full HDR radiance of indoor scenes without sacrificing the visual quality
while retaining the ease of capture from recent NeRF-like approaches.

Comments:
- 10 pages, 8 figures

---

## RRM: Relightable assets using Radiance guided Material extraction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-08 | Diego Gomez, Julien Philip, Adrien Kaiser, Élie Michel | cs.CV | [PDF](http://arxiv.org/pdf/2407.06397v1){: .btn .btn-green } |

**Abstract**: Synthesizing NeRFs under arbitrary lighting has become a seminal problem in
the last few years. Recent efforts tackle the problem via the extraction of
physically-based parameters that can then be rendered under arbitrary lighting,
but they are limited in the range of scenes they can handle, usually
mishandling glossy scenes. We propose RRM, a method that can extract the
materials, geometry, and environment lighting of a scene even in the presence
of highly reflective objects. Our method consists of a physically-aware
radiance field representation that informs physically-based parameters, and an
expressive environment light structure based on a Laplacian Pyramid. We
demonstrate that our contributions outperform the state-of-the-art on parameter
retrieval tasks, leading to high-fidelity relighting and novel view synthesis
on surfacic scenes.

Comments:
- Paper accepted and presented at CGI 2024

---

## PICA: Physics-Integrated Clothed Avatar

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-07 | Bo Peng, Yunfan Tao, Haoyu Zhan, Yudong Guo, Juyong Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2407.05324v1){: .btn .btn-green } |

**Abstract**: We introduce PICA, a novel representation for high-fidelity animatable
clothed human avatars with physics-accurate dynamics, even for loose clothing.
Previous neural rendering-based representations of animatable clothed humans
typically employ a single model to represent both the clothing and the
underlying body. While efficient, these approaches often fail to accurately
represent complex garment dynamics, leading to incorrect deformations and
noticeable rendering artifacts, especially for sliding or loose garments.
Furthermore, previous works represent garment dynamics as pose-dependent
deformations and facilitate novel pose animations in a data-driven manner. This
often results in outcomes that do not faithfully represent the mechanics of
motion and are prone to generating artifacts in out-of-distribution poses. To
address these issues, we adopt two individual 3D Gaussian Splatting (3DGS)
models with different deformation characteristics, modeling the human body and
clothing separately. This distinction allows for better handling of their
respective motion characteristics. With this representation, we integrate a
graph neural network (GNN)-based clothed body physics simulation module to
ensure an accurate representation of clothing dynamics. Our method, through its
carefully designed features, achieves high-fidelity rendering of clothed human
bodies in complex and novel driving poses, significantly outperforming previous
methods under the same settings.

Comments:
- Project page: https://ustc3dv.github.io/PICA/

---

## GaussReg: Fast 3D Registration with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-07 | Jiahao Chang, Yinglin Xu, Yihao Li, Yuantao Chen, Xiaoguang Han | cs.CV | [PDF](http://arxiv.org/pdf/2407.05254v1){: .btn .btn-green } |

**Abstract**: Point cloud registration is a fundamental problem for large-scale 3D scene
scanning and reconstruction. With the help of deep learning, registration
methods have evolved significantly, reaching a nearly-mature stage. As the
introduction of Neural Radiance Fields (NeRF), it has become the most popular
3D scene representation as its powerful view synthesis capabilities. Regarding
NeRF representation, its registration is also required for large-scale scene
reconstruction. However, this topic extremly lacks exploration. This is due to
the inherent challenge to model the geometric relationship among two scenes
with implicit representations. The existing methods usually convert the
implicit representation to explicit representation for further registration.
Most recently, Gaussian Splatting (GS) is introduced, employing explicit 3D
Gaussian. This method significantly enhances rendering speed while maintaining
high rendering quality. Given two scenes with explicit GS representations, in
this work, we explore the 3D registration task between them. To this end, we
propose GaussReg, a novel coarse-to-fine framework, both fast and accurate. The
coarse stage follows existing point cloud registration methods and estimates a
rough alignment for point clouds from GS. We further newly present an
image-guided fine registration approach, which renders images from GS to
provide more detailed geometric information for precise alignment. To support
comprehensive evaluation, we carefully build a scene-level dataset called
ScanNet-GSReg with 1379 scenes obtained from the ScanNet dataset and collect an
in-the-wild dataset called GSReg. Experimental results demonstrate our method
achieves state-of-the-art performance on multiple datasets. Our GaussReg is 44
times faster than HLoc (SuperPoint as the feature extractor and SuperGlue as
the matcher) with comparable accuracy.

Comments:
- ECCV 2024

---

## SurgicalGaussian: Deformable 3D Gaussians for High-Fidelity Surgical  Scene Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-06 | Weixing Xie, Junfeng Yao, Xianpeng Cao, Qiqin Lin, Zerui Tang, Xiao Dong, Xiaohu Guo | cs.CV | [PDF](http://arxiv.org/pdf/2407.05023v1){: .btn .btn-green } |

**Abstract**: Dynamic reconstruction of deformable tissues in endoscopic video is a key
technology for robot-assisted surgery. Recent reconstruction methods based on
neural radiance fields (NeRFs) have achieved remarkable results in the
reconstruction of surgical scenes. However, based on implicit representation,
NeRFs struggle to capture the intricate details of objects in the scene and
cannot achieve real-time rendering. In addition, restricted single view
perception and occluded instruments also propose special challenges in surgical
scene reconstruction. To address these issues, we develop SurgicalGaussian, a
deformable 3D Gaussian Splatting method to model dynamic surgical scenes. Our
approach models the spatio-temporal features of soft tissues at each time stamp
via a forward-mapping deformation MLP and regularization to constrain local 3D
Gaussians to comply with consistent movement. With the depth initialization
strategy and tool mask-guided training, our method can remove surgical
instruments and reconstruct high-fidelity surgical scenes. Through experiments
on various surgical videos, our network outperforms existing method on many
aspects, including rendering quality, rendering speed and GPU usage. The
project page can be found at https://surgicalgaussian.github.io.



---

## Gaussian Eigen Models for Human Heads

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-05 | Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies | cs.CV | [PDF](http://arxiv.org/pdf/2407.04545v1){: .btn .btn-green } |

**Abstract**: We present personalized Gaussian Eigen Models (GEMs) for human heads, a novel
method that compresses dynamic 3D Gaussians into low-dimensional linear spaces.
Our approach is inspired by the seminal work of Blanz and Vetter, where a
mesh-based 3D morphable model (3DMM) is constructed from registered meshes.
Based on dynamic 3D Gaussians, we create a lower-dimensional representation of
primitives that applies to most 3DGS head avatars. Specifically, we propose a
universal method to distill the appearance of a mesh-controlled UNet Gaussian
avatar using an ensemble of linear eigenbasis. We replace heavy CNN-based
architectures with a single linear layer improving speed and enabling a range
of real-time downstream applications. To create a particular facial expression,
one simply needs to perform a dot product between the eigen coefficients and
the distilled basis. This efficient method removes the requirement for an input
mesh during testing, enhancing simplicity and speed in expression generation.
This process is highly efficient and supports real-time rendering on everyday
devices, leveraging the effectiveness of standard Gaussian Splatting. In
addition, we demonstrate how the GEM can be controlled using a ResNet-based
regression architecture. We show and compare self-reenactment and cross-person
reenactment to state-of-the-art 3D avatar methods, demonstrating higher quality
and better control. A real-time demo showcases the applicability of the GEM
representation.

Comments:
- https://zielon.github.io/gem/

---

## Segment Any 4D Gaussians

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-05 | Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, Xinggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2407.04504v1){: .btn .btn-green } |

**Abstract**: Modeling, understanding, and reconstructing the real world are crucial in
XR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable
success in modeling and understanding 3D scenes. Similarly, various 4D
representations have demonstrated the ability to capture the dynamics of the 4D
world. However, there is a dearth of research focusing on segmentation within
4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D),
one of the first frameworks to segment anything in the 4D digital world based
on 4D Gaussians. In SA4D, an efficient temporal identity feature field is
introduced to handle Gaussian drifting, with the potential to learn precise
identity features from noisy and sparse input. Additionally, a 4D segmentation
refinement process is proposed to remove artifacts. Our SA4D achieves precise,
high-quality segmentation within seconds in 4D Gaussians and shows the ability
to remove, recolor, compose, and render high-quality anything masks. More demos
are available at: https://jsxzs.github.io/sa4d/.

Comments:
- 22 pages

---

## GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-05 | Yuxuan Mu, Xinxin Zuo, Chuan Guo, Yilin Wang, Juwei Lu, Xiaofeng Wu, Songcen Xu, Peng Dai, Youliang Yan, Li Cheng | cs.CV | [PDF](http://arxiv.org/pdf/2407.04237v2){: .btn .btn-green } |

**Abstract**: We present GSD, a diffusion model approach based on Gaussian Splatting (GS)
representation for 3D object reconstruction from a single view. Prior works
suffer from inconsistent 3D geometry or mediocre rendering quality due to
improper representations. We take a step towards resolving these shortcomings
by utilizing the recent state-of-the-art 3D explicit representation, Gaussian
Splatting, and an unconditional diffusion model. This model learns to generate
3D objects represented by sets of GS ellipsoids. With these strong generative
3D priors, though learning unconditionally, the diffusion model is ready for
view-guided reconstruction without further model fine-tuning. This is achieved
by propagating fine-grained 2D features through the efficient yet flexible
splatting function and the guided denoising sampling process. In addition, a 2D
diffusion model is further employed to enhance rendering fidelity, and improve
reconstructed GS quality by polishing and re-using the rendered images. The
final reconstructed objects explicitly come with high-quality 3D structure and
texture, and can be efficiently rendered in arbitrary views. Experiments on the
challenging real-world CO3D dataset demonstrate the superiority of our
approach. Project page: $\href{https://yxmu.foo/GSD/}{\text{this https URL}}$

Comments:
- Accepted for ECCV 2024

---

## PFGS: High Fidelity Point Cloud Rendering via Feature Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-04 | Jiaxu Wang, Ziyi Zhang, Junhao He, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2407.03857v1){: .btn .btn-green } |

**Abstract**: Rendering high-fidelity images from sparse point clouds is still challenging.
Existing learning-based approaches suffer from either hole artifacts, missing
details, or expensive computations. In this paper, we propose a novel framework
to render high-quality images from sparse points. This method first attempts to
bridge the 3D Gaussian Splatting and point cloud rendering, which includes
several cascaded modules. We first use a regressor to estimate Gaussian
properties in a point-wise manner, the estimated properties are used to
rasterize neural feature descriptors into 2D planes which are extracted from a
multiscale extractor. The projected feature volume is gradually decoded toward
the final prediction via a multiscale and progressive decoder. The whole
pipeline experiences a two-stage training and is driven by our well-designed
progressive and multiscale reconstruction loss. Experiments on different
benchmarks show the superiority of our method in terms of rendering qualities
and the necessities of our main components.



---

## CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion  Blur Images

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-04 | Junghe Lee, Donghyeong Kim, Dogyoon Lee, Suhwan Cho, Sangyoun Lee | cs.CV | [PDF](http://arxiv.org/pdf/2407.03923v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) have received significant attention due to
their high-quality novel view rendering ability, prompting research to address
various real-world cases. One critical challenge is the camera motion blur
caused by camera movement during exposure time, which prevents accurate 3D
scene reconstruction. In this study, we propose continuous rigid motion-aware
gaussian splatting (CRiM-GS) to reconstruct accurate 3D scene from blurry
images with real-time rendering speed. Considering the actual camera motion
blurring process, which consists of complex motion patterns, we predict the
continuous movement of the camera based on neural ordinary differential
equations (ODEs). Specifically, we leverage rigid body transformations to model
the camera motion with proper regularization, preserving the shape and size of
the object. Furthermore, we introduce a continuous deformable 3D transformation
in the \textit{SE(3)} field to adapt the rigid body transformation to
real-world problems by ensuring a higher degree of freedom. By revisiting
fundamental camera theory and employing advanced neural network training
techniques, we achieve accurate modeling of continuous camera trajectories. We
conduct extensive experiments, demonstrating state-of-the-art performance both
quantitatively and qualitatively on benchmark datasets.

Comments:
- Project Page : https://jho-yonsei.github.io/CRiM-Gaussian/

---

## SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-04 | Yijia Guo, Liwen Hu, Lei Ma, Tiejun Huang | cs.CV | [PDF](http://arxiv.org/pdf/2407.03771v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance
in 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.
Fulfilling this requirement can be challenging in real-world scenarios
especially when the camera moves fast, which severely limits the application of
3DGS. To address these challenges, we proposed Spike Gausian Splatting
(SpikeGS), the first framework that integrates the spike streams into 3DGS
pipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With
accumulation rasterization, interval supervision, and a specially designed
pipeline, SpikeGS extracts detailed geometry and texture from high temporal
resolution but texture lacking spike stream, reconstructs 3D scenes captured in
1 second. Extensive experiments on multiple synthetic and real-world datasets
demonstrate the superiority of SpikeGS compared with existing spike-based and
deblur 3D scene reconstruction methods. Codes and data will be released soon.



---

## VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using  Learned Priors

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-03 | Sungwon Hwang, Min-Jung Kim, Taewoong Kang, Jayeon Kang, Jaegul Choo | cs.CV | [PDF](http://arxiv.org/pdf/2407.02945v2){: .btn .btn-green } |

**Abstract**: Neural rendering-based urban scene reconstruction methods commonly rely on
images collected from driving vehicles with cameras facing and moving forward.
Although these methods can successfully synthesize from views similar to
training camera trajectory, directing the novel view outside the training
camera distribution does not guarantee on-par performance. In this paper, we
tackle the Extrapolated View Synthesis (EVS) problem by evaluating the
reconstructions on views such as looking left, right or downwards with respect
to training camera distributions. To improve rendering quality for EVS, we
initialize our model by constructing dense LiDAR map, and propose to leverage
prior scene knowledge such as surface normal estimator and large-scale
diffusion model. Qualitative and quantitative comparisons demonstrate the
effectiveness of our methods on EVS. To the best of our knowledge, we are the
first to address the EVS problem in urban scene reconstruction. Link to our
project page: https://vegs3d.github.io/.

Comments:
- The first two authors contributed equally. Project Page:
  https://vegs3d.github.io/

---

## Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-03 | Jiaxin Guo, Jiangliu Wang, Di Kang, Wenzhen Dong, Wenting Wang, Yun-hui Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02918v1){: .btn .btn-green } |

**Abstract**: Real-time 3D reconstruction of surgical scenes plays a vital role in
computer-assisted surgery, holding a promise to enhance surgeons' visibility.
Recent advancements in 3D Gaussian Splatting (3DGS) have shown great potential
for real-time novel view synthesis of general scenes, which relies on accurate
poses and point clouds generated by Structure-from-Motion (SfM) for
initialization. However, 3DGS with SfM fails to recover accurate camera poses
and geometry in surgical scenes due to the challenges of minimal textures and
photometric inconsistencies. To tackle this problem, in this paper, we propose
the first SfM-free 3DGS-based method for surgical scene reconstruction by
jointly optimizing the camera poses and scene representation. Based on the
video continuity, the key of our method is to exploit the immediate optical
flow priors to guide the projection flow derived from 3D Gaussians. Unlike most
previous methods relying on photometric loss only, we formulate the pose
estimation problem as minimizing the flow loss between the projection flow and
optical flow. A consistency check is further introduced to filter the flow
outliers by detecting the rigid and reliable points that satisfy the epipolar
geometry. During 3D Gaussian optimization, we randomly sample frames to
optimize the scene representations to grow the 3D Gaussian progressively.
Experiments on the SCARED dataset demonstrate our superior performance over
existing methods in novel view synthesis and pose estimation with high
efficiency. Code is available at https://github.com/wrld/Free-SurGS.

Comments:
- Accepted to MICCAI 2024

---

## MomentsNeRF: Leveraging Orthogonal Moments for Few-Shot Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Ahmad AlMughrabi, Ricardo Marques, Petia Radeva | cs.CV | [PDF](http://arxiv.org/pdf/2407.02668v1){: .btn .btn-green } |

**Abstract**: We propose MomentsNeRF, a novel framework for one- and few-shot neural
rendering that predicts a neural representation of a 3D scene using Orthogonal
Moments. Our architecture offers a new transfer learning method to train on
multi-scenes and incorporate a per-scene optimization using one or a few images
at test time. Our approach is the first to successfully harness features
extracted from Gabor and Zernike moments, seamlessly integrating them into the
NeRF architecture. We show that MomentsNeRF performs better in synthesizing
images with complex textures and shapes, achieving a significant noise
reduction, artifact elimination, and completing the missing parts compared to
the recent one- and few-shot neural rendering frameworks. Extensive experiments
on the DTU and Shapenet datasets show that MomentsNeRF improves the
state-of-the-art by {3.39\;dB\;PSNR}, 11.1% SSIM, 17.9% LPIPS, and 8.3% DISTS
metrics. Moreover, it outperforms state-of-the-art performance for both novel
view synthesis and single-image 3D view reconstruction. The source code is
accessible at: https://amughrabi.github.io/momentsnerf/.



---

## AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Mustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02598v2){: .btn .btn-green } |

**Abstract**: Realistic scene reconstruction and view synthesis are essential for advancing
autonomous driving systems by simulating safety-critical scenarios. 3D Gaussian
Splatting excels in real-time rendering and static scene reconstructions but
struggles with modeling driving scenarios due to complex backgrounds, dynamic
objects, and sparse views. We propose AutoSplat, a framework employing Gaussian
splatting to achieve highly realistic reconstructions of autonomous driving
scenes. By imposing geometric constraints on Gaussians representing the road
and sky regions, our method enables multi-view consistent simulation of
challenging scenarios including lane changes. Leveraging 3D templates, we
introduce a reflected Gaussian consistency constraint to supervise both the
visible and unseen side of foreground objects. Moreover, to model the dynamic
appearance of foreground objects, we estimate residual spherical harmonics for
each foreground Gaussian. Extensive experiments on Pandaset and KITTI
demonstrate that AutoSplat outperforms state-of-the-art methods in scene
reconstruction and novel view synthesis across diverse driving scenarios. Visit
our project page at https://autosplat.github.io/.



---

## BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event  Stream

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Wenpu Li, Pian Wan, Peng Wang, Jinghang Li, Yi Zhou, Peidong Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02174v2){: .btn .btn-green } |

**Abstract**: Neural implicit representation of visual scenes has attracted a lot of
attention in recent research of computer vision and graphics. Most prior
methods focus on how to reconstruct 3D scene representation from a set of
images. In this work, we demonstrate the possibility to recover the neural
radiance fields (NeRF) from a single blurry image and its corresponding event
stream. We model the camera motion with a cubic B-Spline in SE(3) space. Both
the blurry image and the brightness change within a time interval, can then be
synthesized from the 3D scene representation given the 6-DoF poses interpolated
from the cubic B-Spline. Our method can jointly learn both the implicit neural
scene representation and recover the camera motion by minimizing the
differences between the synthesized data and the real measurements without
pre-computed camera poses from COLMAP. We evaluate the proposed method with
both synthetic and real datasets. The experimental results demonstrate that we
are able to render view-consistent latent sharp images from the learned NeRF
and bring a blurry image alive in high quality. Code and data are available at
https://github.com/WU-CVGL/BeNeRF.

Comments:
- Accepted to ECCV 2024

---

## TrAME: Trajectory-Anchored Multi-View Editing for Text-Guided 3D  Gaussian Splatting Manipulation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-02 | Chaofan Luo, Donglin Di, Yongjia Ma, Zhou Xue, Chen Wei, Xun Yang, Yebin Liu | cs.CV | [PDF](http://arxiv.org/pdf/2407.02034v1){: .btn .btn-green } |

**Abstract**: Despite significant strides in the field of 3D scene editing, current methods
encounter substantial challenge, particularly in preserving 3D consistency in
multi-view editing process. To tackle this challenge, we propose a progressive
3D editing strategy that ensures multi-view consistency via a
Trajectory-Anchored Scheme (TAS) with a dual-branch editing mechanism.
Specifically, TAS facilitates a tightly coupled iterative process between 2D
view editing and 3D updating, preventing error accumulation yielded from
text-to-image process. Additionally, we explore the relationship between
optimization-based methods and reconstruction-based methods, offering a unified
perspective for selecting superior design choice, supporting the rationale
behind the designed TAS. We further present a tuning-free View-Consistent
Attention Control (VCAC) module that leverages cross-view semantic and
geometric reference from the source branch to yield aligned views from the
target branch during the editing of 2D views. To validate the effectiveness of
our method, we analyze 2D examples to demonstrate the improved consistency with
the VCAC module. Further extensive quantitative and qualitative results in
text-guided 3D scene editing indicate that our method achieves superior editing
quality compared to state-of-the-art methods. We will make the complete
codebase publicly available following the conclusion of the double-blind review
process.



---

## DRAGON: Drone and Ground Gaussian Splatting for 3D Building  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Yujin Ham, Mateusz Michalkiewicz, Guha Balakrishnan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01761v1){: .btn .btn-green } |

**Abstract**: 3D building reconstruction from imaging data is an important task for many
applications ranging from urban planning to reconnaissance. Modern Novel View
synthesis (NVS) methods like NeRF and Gaussian Splatting offer powerful
techniques for developing 3D models from natural 2D imagery in an unsupervised
fashion. These algorithms generally require input training views surrounding
the scene of interest, which, in the case of large buildings, is typically not
available across all camera elevations. In particular, the most readily
available camera viewpoints at scale across most buildings are at near-ground
(e.g., with mobile phones) and aerial (drones) elevations. However, due to the
significant difference in viewpoint between drone and ground image sets, camera
registration - a necessary step for NVS algorithms - fails. In this work we
propose a method, DRAGON, that can take drone and ground building imagery as
input and produce a 3D NVS model. The key insight of DRAGON is that
intermediate elevation imagery may be extrapolated by an NVS algorithm itself
in an iterative procedure with perceptual regularization, thereby bridging the
visual feature gap between the two elevations and enabling registration. We
compiled a semi-synthetic dataset of 9 large building scenes using Google Earth
Studio, and quantitatively and qualitatively demonstrate that DRAGON can
generate compelling renderings on this dataset compared to baseline strategies.

Comments:
- 12 pages, 9 figures, accepted to ICCP 2024

---

## EndoSparse: Real-Time Sparse View Synthesis of Endoscopic Scenes using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Chenxin Li, Brandon Y. Feng, Yifan Liu, Hengyu Liu, Cheng Wang, Weihao Yu, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01029v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction of biological tissues from a collection of endoscopic
images is a key to unlock various important downstream surgical applications
with 3D capabilities. Existing methods employ various advanced neural rendering
techniques for photorealistic view synthesis, but they often struggle to
recover accurate 3D representations when only sparse observations are
available, which is usually the case in real-world clinical scenarios. To
tackle this {sparsity} challenge, we propose a framework leveraging the prior
knowledge from multiple foundation models during the reconstruction process,
dubbed as \textit{EndoSparse}. Experimental results indicate that our proposed
strategy significantly improves the geometric and appearance quality under
challenging sparse-view conditions, including using only three views. In
rigorous benchmarking experiments against state-of-the-art methods,
\textit{EndoSparse} achieves superior results in terms of accurate geometry,
realistic appearance, and rendering efficiency, confirming the robustness to
sparse-view limitations in endoscopic reconstruction. \textit{EndoSparse}
signifies a steady step towards the practical deployment of neural 3D
reconstruction in real-world clinical scenarios. Project page:
https://endo-sparse.github.io/.

Comments:
- Accpeted by MICCAI2024

---

## Active Human Pose Estimation via an Autonomous UAV Agent

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Jingxi Chen, Botao He, Chahat Deep Singh, Cornelia Fermuller, Yiannis Aloimonos | cs.RO | [PDF](http://arxiv.org/pdf/2407.01811v1){: .btn .btn-green } |

**Abstract**: One of the core activities of an active observer involves moving to secure a
"better" view of the scene, where the definition of "better" is task-dependent.
This paper focuses on the task of human pose estimation from videos capturing a
person's activity. Self-occlusions within the scene can complicate or even
prevent accurate human pose estimation. To address this, relocating the camera
to a new vantage point is necessary to clarify the view, thereby improving 2D
human pose estimation. This paper formalizes the process of achieving an
improved viewpoint. Our proposed solution to this challenge comprises three
main components: a NeRF-based Drone-View Data Generation Framework, an On-Drone
Network for Camera View Error Estimation, and a Combined Planner for devising a
feasible motion plan to reposition the camera based on the predicted errors for
camera views. The Data Generation Framework utilizes NeRF-based methods to
generate a comprehensive dataset of human poses and activities, enhancing the
drone's adaptability in various scenarios. The Camera View Error Estimation
Network is designed to evaluate the current human pose and identify the most
promising next viewing angles for the drone, ensuring a reliable and precise
pose estimation from those angles. Finally, the combined planner incorporates
these angles while considering the drone's physical and environmental
limitations, employing efficient algorithms to navigate safe and effective
flight paths. This system represents a significant advancement in active 2D
human pose estimation for an autonomous UAV agent, offering substantial
potential for applications in aerial cinematography by improving the
performance of autonomous human pose estimation and maintaining the operational
safety and efficiency of UAVs.



---

## Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Zihan Gao, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Yuwei Guo, Shuyuan Yang | cs.CV | [PDF](http://arxiv.org/pdf/2407.01220v1){: .btn .btn-green } |

**Abstract**: Understanding 3D scenes is a crucial challenge in computer vision research
with applications spanning multiple domains. Recent advancements in distilling
2D vision-language foundation models into neural fields, like NeRF and 3DGS,
enables open-vocabulary segmentation of 3D scenes from 2D multi-view images
without the need for precise 3D annotations. While effective, however, the
per-pixel distillation of high-dimensional CLIP features introduces ambiguity
and necessitates complex regularization strategies, adding inefficiencies
during training. This paper presents MaskField, which enables fast and
efficient 3D open-vocabulary segmentation with neural fields under weak
supervision. Unlike previous methods, MaskField distills masks rather than
dense high-dimensional CLIP features. MaskFields employ neural fields as binary
mask generators and supervise them with masks generated by SAM and classified
by coarse CLIP features. MaskField overcomes the ambiguous object boundaries by
naturally introducing SAM segmented object shapes without extra regularization
during training. By circumventing the direct handling of high-dimensional CLIP
features during training, MaskField is particularly compatible with explicit
scene representations like 3DGS. Our extensive experiments show that MaskField
not only surpasses prior state-of-the-art methods but also achieves remarkably
fast convergence, outperforming previous methods with just 5 minutes of
training. We hope that MaskField will inspire further exploration into how
neural fields can be trained to comprehend 3D scenes from 2D models.

Comments:
- 16 pages, 7 figures

---

## GaussianStego: A Generalizable Stenography Pipeline for Generative 3D  Gaussians Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-07-01 | Chenxin Li, Hengyu Liu, Zhiwen Fan, Wuyang Li, Yifan Liu, Panwang Pan, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2407.01301v1){: .btn .btn-green } |

**Abstract**: Recent advancements in large generative models and real-time neural rendering
using point-based techniques pave the way for a future of widespread visual
data distribution through sharing synthesized 3D assets. However, while
standardized methods for embedding proprietary or copyright information, either
overtly or subtly, exist for conventional visual content such as images and
videos, this issue remains unexplored for emerging generative 3D formats like
Gaussian Splatting. We present GaussianStego, a method for embedding
steganographic information in the rendering of generated 3D assets. Our
approach employs an optimization framework that enables the accurate extraction
of hidden information from images rendered using Gaussian assets derived from
large models, while maintaining their original visual quality. We conduct
preliminary evaluations of our method across several potential deployment
scenarios and discuss issues identified through analysis. GaussianStego
represents an initial exploration into the novel challenge of embedding
customizable, imperceptible, and recoverable information within the renders
produced by current 3D generative models, while ensuring minimal impact on the
rendered content's quality.

Comments:
- Project website: https://gaussian-stego.github.io/
