---
layout: default
title: June 2024
parent: Papers
nav_order: 202406
---

<!---metadata--->


## Enhancing Temporal Consistency in Video Editing by Reconstructing Videos  with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Inkyu Shin, Qihang Yu, Xiaohui Shen, In So Kweon, Kuk-Jin Yoon, Liang-Chieh Chen | cs.CV | [PDF](http://arxiv.org/pdf/2406.02541v1){: .btn .btn-green } |

**Abstract**: Recent advancements in zero-shot video diffusion models have shown promise
for text-driven video editing, but challenges remain in achieving high temporal
consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting
(3DGS)-based video refiner designed to enhance temporal consistency in
zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian
optimizing process tailored for editing dynamic monocular videos. In the first
stage, Video-3DGS employs an improved version of COLMAP, referred to as
MC-COLMAP, which processes original videos using a Masked and Clipped approach.
For each video clip, MC-COLMAP generates the point clouds for dynamic
foreground objects and complex backgrounds. These point clouds are utilized to
initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent
foreground and background views. Both foreground and background views are then
merged with a 2D learnable parameter map to reconstruct full views. In the
second stage, we leverage the reconstruction ability developed in the first
stage to impose the temporal constraints on the video diffusion model. To
demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive
experiments across two related tasks: Video Reconstruction and Video Editing.
Video-3DGS trained with 3k iterations significantly improves video
reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency
(x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods
on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring
temporal consistency across 58 dynamic monocular videos.



---

## OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary  Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2406.02058v1){: .btn .btn-green } |

**Abstract**: This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting
(3DGS) capable of 3D point-level open vocabulary understanding. Our primary
motivation stems from observing that existing 3DGS-based open vocabulary
methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D
point-level tasks due to weak feature expressiveness and inaccurate 2D-3D
feature associations. To ensure robust feature presentation and 3D point-level
understanding, we first employ SAM masks without cross-frame associations to
train instance features with 3D consistency. These features exhibit both
intra-object consistency and inter-object distinction. Then, we propose a
two-stage codebook to discretize these features from coarse to fine levels. At
the coarse level, we consider the positional information of 3D points to
achieve location-based clustering, which is then refined at the fine level.
Finally, we introduce an instance-level 3D-2D feature association method that
links 3D points to 2D masks, which are further associated with 2D CLIP
features. Extensive experiments, including open vocabulary-based 3D object
selection, 3D point cloud understanding, click-based 3D object selection, and
ablation studies, demonstrate the effectiveness of our proposed method. Project
page: https://3d-aigc.github.io/OpenGaussian

Comments:
- technical report, 15 pages

---

## SatSplatYOLO: 3D Gaussian Splatting-based Virtual Object Detection  Ensembles for Satellite Feature Recognition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Van Minh Nguyen, Emma Sandidge, Trupti Mahendrakar, Ryan T. White | cs.CV | [PDF](http://arxiv.org/pdf/2406.02533v1){: .btn .btn-green } |

**Abstract**: On-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possibly unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. In this article, we present an approach for
mapping geometries and high-confidence detection of components of unknown,
non-cooperative satellites on orbit. We implement accelerated 3D Gaussian
splatting to learn a 3D representation of the satellite, render virtual views
of the target, and ensemble the YOLOv5 object detector over the virtual views,
resulting in reliable, accurate, and precise satellite component detections.
The full pipeline capable of running on-board and stand to enable downstream
machine intelligence tasks necessary for autonomous guidance, navigation, and
control tasks.



---

## FastLGS: Speeding up Language Embedded Gaussians with Feature Grid  Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Yuzhou Ji, He Zhu, Junshu Tang, Wuyi Liu, Zhizhong Zhang, Yuan Xie, Lizhuang Ma, Xin Tan | cs.CV | [PDF](http://arxiv.org/pdf/2406.01916v1){: .btn .btn-green } |

**Abstract**: The semantically interactive radiance field has always been an appealing task
for its potential to facilitate user-friendly and automated real-world 3D scene
understanding applications. However, it is a challenging task to achieve high
quality, efficiency and zero-shot ability at the same time with semantics in
radiance fields. In this work, we present FastLGS, an approach that supports
real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high
resolution. We propose the semantic feature grid to save multi-view CLIP
features which are extracted based on Segment Anything Model (SAM) masks, and
map the grids to low dimensional features for semantic field training through
3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through
feature grids from rendered features for open-vocabulary queries. Comparisons
with other state-of-the-art methods prove that FastLGS can achieve the first
place performance concerning both speed and accuracy, where FastLGS is 98x
faster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that
FastLGS is adaptive and compatible with many downstream tasks, such as 3D
segmentation and 3D object inpainting, which can be easily applied to other 3D
manipulation systems.



---

## DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Zhongpai Gao, Benjamin Planche, Meng Zheng, Xiao Chen, Terrence Chen, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2406.02518v1){: .btn .btn-green } |

**Abstract**: Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images
generated from 3D CT volumes, widely used in preoperative settings but limited
in intraoperative applications due to computational bottlenecks, especially for
accurate but heavy physics-based Monte Carlo methods. While analytical DRR
renderers offer greater efficiency, they overlook anisotropic X-ray image
formation phenomena, such as Compton scattering. We present a novel approach
that marries realistic physics-inspired X-ray simulation with efficient,
differentiable DRR generation using 3D Gaussian splatting (3DGS). Our
direction-disentangled 3DGS (DDGS) method separates the radiosity contribution
into isotropic and direction-dependent components, approximating complex
anisotropic interactions without intricate runtime simulations. Additionally,
we adapt the 3DGS initialization to account for tomography data properties,
enhancing accuracy and efficiency. Our method outperforms state-of-the-art
techniques in image accuracy. Furthermore, our DDGS shows promise for
intraoperative applications and inverse problems such as pose registration,
delivering superior registration accuracy and runtime performance compared to
analytical DRR methods.



---

## Query-based Semantic Gaussian Field for Scene Representation in  Reinforcement Learning

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Jiaxu Wang, Ziyi Zhang, Qiang Zhang, Jia Li, Jingkai Sun, Mingyuan Sun, Junhao He, Renjing Xu | cs.RO | [PDF](http://arxiv.org/pdf/2406.02370v1){: .btn .btn-green } |

**Abstract**: Latent scene representation plays a significant role in training
reinforcement learning (RL) agents. To obtain good latent vectors describing
the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF
pipeline into scene representation learning. However, these NeRF-related
methods struggle to perceive 3D structural information due to the inefficient
dense sampling in volumetric rendering. Moreover, they lack fine-grained
semantic information included in their scene representation vectors because
they evenly consider free and occupied spaces. Both of them can destroy the
performance of downstream RL tasks. To address the above challenges, we propose
a novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to
learn 3D scene representation for the first time. In brief, we present the
Query-based Generalizable 3DGS to bridge the 3DGS technique and scene
representations with more geometrical awareness than those in NeRFs. Moreover,
we present the Hierarchical Semantics Encoding to ground the fine-grained
semantic features to 3D Gaussians and further distilled to the scene
representation vectors. We conduct extensive experiments on two RL platforms
including Maniskill2 and Robomimic across 10 different tasks. The results show
that our method outperforms the other 5 baselines by a large margin. We achieve
the best success rates on 8 tasks and the second-best on the other two tasks.



---

## WE-GS: An In-the-wild Efficient 3D Gaussian Representation for  Unconstrained Photo Collections

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Yuze Wang, Junyi Wang, Yue Qi | cs.CV | [PDF](http://arxiv.org/pdf/2406.02407v1){: .btn .btn-green } |

**Abstract**: Novel View Synthesis (NVS) from unconstrained photo collections is
challenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) has
shown promise for photorealistic and real-time NVS of static scenes. Building
on 3DGS, we propose an efficient point-based differentiable rendering framework
for scene reconstruction from photo collections. Our key innovation is a
residual-based spherical harmonic coefficients transfer module that adapts 3DGS
to varying lighting conditions and photometric post-processing. This
lightweight module can be pre-computed and ensures efficient gradient
propagation from rendered images to 3D Gaussian attributes. Additionally, we
observe that the appearance encoder and the transient mask predictor, the two
most critical parts of NVS from unconstrained photo collections, can be
mutually beneficial. We introduce a plug-and-play lightweight spatial attention
module to simultaneously predict transient occluders and latent appearance
representation for each image. After training and preprocessing, our method
aligns with the standard 3DGS format and rendering pipeline, facilitating
seamlessly integration into various 3DGS applications. Extensive experiments on
diverse datasets show our approach outperforms existing approaches on the
rendering quality of novel view and appearance synthesis with high converge and
rendering speed.

Comments:
- Our project page is available at
  https://yuzewang1998.github.io/we-gs.github.io/

---

## Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Shaojie Ma, Yawei Luo, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2406.01593v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction and simulation, while interrelated, have distinct
objectives: reconstruction demands a flexible 3D representation adaptable to
diverse scenes, whereas simulation requires a structured representation to
model motion principles effectively. This paper introduces the Mesh-adsorbed
Gaussian Splatting (MaGS) method to resolve such a dilemma. MaGS constrains 3D
Gaussians to hover on the mesh surface, creating a mutual-adsorbed
mesh-Gaussian 3D representation that combines the rendering flexibility of 3D
Gaussians with the spatial coherence of meshes. Leveraging this representation,
we introduce a learnable Relative Deformation Field (RDF) to model the relative
displacement between the mesh and 3D Gaussians, extending traditional
mesh-driven deformation paradigms that only rely on ARAP prior, thus capturing
the motion of each 3D Gaussian more precisely. By joint optimizing meshes, 3D
Gaussians, and RDF, MaGS achieves both high rendering accuracy and realistic
deformation. Extensive experiments on the D-NeRF and NeRF-DS datasets
demonstrate that MaGS can generate competitive results in both reconstruction
and simulation.

Comments:
- Project Page: see https://wcwac.github.io/MaGS-page/

---

## Tetrahedron Splatting for 3D Generation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Chun Gu, Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2406.01579v1){: .btn .btn-green } |

**Abstract**: 3D representation is essential to the significant advance of 3D generation
with 2D diffusion priors. As a flexible representation, NeRF has been first
adopted for 3D representation. With density-based volumetric rendering, it
however suffers both intensive computational overhead and inaccurate mesh
extraction. Using a signed distance field and Marching Tetrahedra, DMTet allows
for precise mesh extraction and real-time rendering but is limited in handling
large topological changes in meshes, leading to optimization challenges.
Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and
rendering efficiency while falling short in mesh extraction. In this work, we
introduce a novel 3D representation, Tetrahedron Splatting (TeT-Splatting),
that supports easy convergence during optimization, precise mesh extraction,
and real-time rendering simultaneously. This is achieved by integrating
surface-based volumetric rendering within a structured tetrahedral grid while
preserving the desired ability of precise mesh extraction, and a tile-based
differentiable tetrahedron rasterizer. Furthermore, we incorporate eikonal and
normal consistency regularization terms for the signed distance field to
improve generation quality and stability. Critically, our representation can be
trained without mesh extraction, making the optimization process easier to
converge. Our TeT-Splatting can be readily integrated in existing 3D generation
pipelines, along with polygonal mesh for texture optimization. Extensive
experiments show that our TeT-Splatting strikes a superior tradeoff among
convergence speed, render efficiency, and mesh quality as compared to previous
alternatives under varying 3D generation settings.

Comments:
- Code: https://github.com/fudan-zvg/tet-splatting

---

## DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with  Video Diffusion Priors


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Tianyu Huang, Yihan Zeng, Hui Li, Wangmeng Zuo, Rynson W. H. Lau | cs.CV | [PDF](http://arxiv.org/pdf/2406.01476v1){: .btn .btn-green } |

**Abstract**: Dynamic 3D interaction has witnessed great interest in recent works, while
creating such 4D content remains challenging. One solution is to animate 3D
scenes with physics-based simulation, and the other is to learn the deformation
of static 3D objects with the distillation of video generative models. The
former one requires assigning precise physical properties to the target object,
otherwise the simulated results would become unnatural. The latter tends to
formulate the video with minor motions and discontinuous frames, due to the
absence of physical constraints in deformation learning. We think that video
generative models are trained with real-world captured data, capable of judging
physical phenomenon in simulation environments. To this end, we propose
DreamPhysics in this work, which estimates physical properties of 3D Gaussian
Splatting with video diffusion priors. DreamPhysics supports both image- and
text-conditioned guidance, optimizing physical parameters via score
distillation sampling with frame interpolation and log gradient. Based on a
material point method simulator with proper physical parameters, our method can
generate 4D content with realistic motions. Experimental results demonstrate
that, by distilling the prior knowledge of video diffusion models, inaccurate
physical properties can be gradually refined for high-quality simulation. Codes
are released at: https://github.com/tyhuang0428/DreamPhysics.

Comments:
- Technical report. Codes are released at:
  https://github.com/tyhuang0428/DreamPhysics

---

## RaDe-GS: Rasterizing Depth in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, Ping Tan | cs.GR | [PDF](http://arxiv.org/pdf/2406.01467v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has proven to be highly effective in novel view
synthesis, achieving high-quality and real-time rendering. However, its
potential for reconstructing detailed 3D shapes has not been fully explored.
Existing methods often suffer from limited shape accuracy due to the discrete
and unstructured nature of Gaussian splats, which complicates the shape
extraction. While recent techniques like 2D GS have attempted to improve shape
reconstruction, they often reformulate the Gaussian primitives in ways that
reduce both rendering quality and computational efficiency. To address these
problems, our work introduces a rasterized approach to render the depth maps
and surface normal maps of general 3D Gaussian splats. Our method not only
significantly enhances shape reconstruction accuracy but also maintains the
computational efficiency intrinsic to Gaussian Splatting. Our approach achieves
a Chamfer distance error comparable to NeuraLangelo on the DTU dataset and
similar training and rendering time as traditional Gaussian Splatting on the
Tanks & Temples dataset. Our method is a significant advancement in Gaussian
Splatting and can be directly integrated into existing Gaussian Splatting-based
methods.



---

## Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Fang Li, Hao Zhang, Narendra Ahuja | cs.CV | [PDF](http://arxiv.org/pdf/2406.01042v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has significantly elevated scene reconstruction
efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance
Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS
methods, whether based on GS or NeRF, primarily rely on camera parameters
provided by COLMAP and even utilize sparse point clouds generated by COLMAP for
initialization, which lack accuracy as well are time-consuming. This sometimes
results in poor dynamic scene representation, especially in scenes with large
object movements, or extreme camera conditions e.g. small translations combined
with large rotations. Some studies simultaneously optimize the estimation of
camera parameters and scenes, supervised by additional information like depth,
optical flow, etc. obtained from off-the-shelf models. Using this unverified
information as ground truth can reduce robustness and accuracy, which does
frequently occur for long monocular videos (with e.g. > hundreds of frames). We
propose a novel approach that learns a high-fidelity 4D GS scene representation
with self-calibration of camera parameters. It includes the extraction of 2D
point features that robustly represent 3D structure, and their use for
subsequent joint optimization of camera parameters and 3D structure towards
overall 4D scene optimization. We demonstrate the accuracy and time efficiency
of our method through extensive quantitative and qualitative experimental
results on several standard benchmarks. The results show significant
improvements over state-of-the-art methods for 4D novel view synthesis. The
source code will be released soon at https://github.com/fangli333/SC-4DGS.

Comments:
- GitHub Page: https://github.com/fangli333/SC-4DGS

---

## SuperGaussian: Repurposing Video Models for 3D Super Resolution

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Yuan Shen, Duygu Ceylan, Paul Guerrero, Zexiang Xu, Niloy J. Mitra, Shenlong Wang, Anna Frühstück | cs.CV | [PDF](http://arxiv.org/pdf/2406.00609v2){: .btn .btn-green } |

**Abstract**: We present a simple, modular, and generic method that upsamples coarse 3D
models by adding geometric and appearance details. While generative 3D models
now exist, they do not yet match the quality of their counterparts in image and
video domains. We demonstrate that it is possible to directly repurpose
existing (pretrained) video models for 3D super-resolution and thus sidestep
the problem of the shortage of large repositories of high-quality 3D training
models. We describe how to repurpose video upsampling models, which are not 3D
consistent, and combine them with 3D consolidation to produce 3D-consistent
results. As output, we produce high quality Gaussian Splat models, which are
object centric and effective. Our method is category agnostic and can be easily
incorporated into existing 3D workflows. We evaluate our proposed SuperGaussian
on a variety of 3D inputs, which are diverse both in terms of complexity and
representation (e.g., Gaussian Splats or NeRFs), and demonstrate that our
simple method significantly improves the fidelity of the final 3D models. Check
our project website for details: supergaussian.github.io

Comments:
- Check our project website for details:
  https://supergaussian.github.io

---

## Efficient Neural Light Fields (ENeLF) for Mobile Devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Austin Peng | cs.CV | [PDF](http://arxiv.org/pdf/2406.00598v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) is a challenge in computer vision and graphics,
focusing on generating realistic images of a scene from unobserved camera
poses, given a limited set of authentic input images. Neural radiance fields
(NeRF) achieved impressive results in rendering quality by utilizing volumetric
rendering. However, NeRF and its variants are unsuitable for mobile devices due
to the high computational cost of volumetric rendering. Emerging research in
neural light fields (NeLF) eliminates the need for volumetric rendering by
directly learning a mapping from ray representation to pixel color. NeLF has
demonstrated its capability to achieve results similar to NeRF but requires a
more extensive, computationally intensive network that is not mobile-friendly.
Unlike existing works, this research builds upon the novel network architecture
introduced by MobileR2L and aggressively applies a compression technique
(channel-wise structure pruning) to produce a model that runs efficiently on
mobile devices with lower latency and smaller sizes, with a slight decrease in
performance.



---

## Representing Animatable Avatar via Factorized Neural Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin | cs.CV | [PDF](http://arxiv.org/pdf/2406.00637v1){: .btn .btn-green } |

**Abstract**: For reconstructing high-fidelity human 3D models from monocular videos, it is
crucial to maintain consistent large-scale body shapes along with finely
matched subtle wrinkles. This paper explores the observation that the per-frame
rendering results can be factorized into a pose-independent component and a
corresponding pose-dependent equivalent to facilitate frame consistency. Pose
adaptive textures can be further improved by restricting frequency bands of
these two components. In detail, pose-independent outputs are expected to be
low-frequency, while highfrequency information is linked to pose-dependent
factors. We achieve a coherent preservation of both coarse body contours across
the entire input video and finegrained texture features that are time variant
with a dual-branch network with distinct frequency components. The first branch
takes coordinates in canonical space as input, while the second branch
additionally considers features outputted by the first branch and pose
information of each frame. Our network integrates the information predicted by
both branches and utilizes volume rendering to generate photo-realistic 3D
human images. Through experiments, we demonstrate that our network surpasses
the neural radiance fields (NeRF) based state-of-the-art methods in preserving
high-frequency details and ensuring consistent body contours.



---

## PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Yeonsung Jung, Heecheol Yun, Joonhyung Park, Jin-Hwa Kim, Eunho Yang | cs.CV | [PDF](http://arxiv.org/pdf/2406.00798v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown remarkable performance in learning
3D scenes. However, NeRF exhibits vulnerability when confronted with
distractors in the training images -- unexpected objects are present only
within specific views, such as moving entities like pedestrians or birds.
Excluding distractors during dataset construction is a straightforward
solution, but without prior knowledge of their types and quantities, it becomes
prohibitively expensive. In this paper, we propose PruNeRF, a segment-centric
dataset pruning framework via 3D spatial consistency, that effectively
identifies and prunes the distractors. We first examine existing metrics for
measuring pixel-wise distraction and introduce Influence Functions for more
accurate measurements. Then, we assess 3D spatial consistency using a
depth-based reprojection technique to obtain 3D-aware distraction. Furthermore,
we incorporate segmentation for pixel-to-segment refinement, enabling more
precise identification. Our experiments on benchmark datasets demonstrate that
PruNeRF consistently outperforms state-of-the-art methods in robustness against
distractors.



---

## MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular  Videos

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-01 | Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lv, Peng Wang, Wenping Wang, Junhui Hou | cs.CV | [PDF](http://arxiv.org/pdf/2406.00434v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose MoDGS, a new pipeline to render novel-view images
in dynamic scenes using only casually captured monocular videos. Previous
monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid
movement of input cameras to construct multiview consistency but fail to
reconstruct dynamic scenes on casually captured input videos whose cameras are
static or move slowly. To address this challenging task, MoDGS adopts recent
single-view depth estimation methods to guide the learning of the dynamic
scene. Then, a novel 3D-aware initialization method is proposed to learn a
reasonable deformation field and a new robust depth loss is proposed to guide
the learning of dynamic scene geometry. Comprehensive experiments demonstrate
that MoDGS is able to render high-quality novel view images of dynamic scenes
from just a casually captured monocular video, which outperforms baseline
methods by a significant margin.



---

## Bilateral Guided Radiance Field Processing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-01 | Yuehao Wang, Chaoyi Wang, Bingchen Gong, Tianfan Xue | cs.CV | [PDF](http://arxiv.org/pdf/2406.00448v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) achieves unprecedented performance in
synthesizing novel view synthesis, utilizing multi-view consistency. When
capturing multiple inputs, image signal processing (ISP) in modern cameras will
independently enhance them, including exposure adjustment, color correction,
local tone mapping, etc. While these processings greatly improve image quality,
they often break the multi-view consistency assumption, leading to "floaters"
in the reconstructed radiance fields. To address this concern without
compromising visual aesthetics, we aim to first disentangle the enhancement by
ISP at the NeRF training stage and re-apply user-desired enhancements to the
reconstructed radiance fields at the finishing stage. Furthermore, to make the
re-applied enhancements consistent between novel views, we need to perform
imaging signal processing in 3D space (i.e. "3D ISP"). For this goal, we adopt
the bilateral grid, a locally-affine model, as a generalized representation of
ISP processing. Specifically, we optimize per-view 3D bilateral grids with
radiance fields to approximate the effects of camera pipelines for each input
view. To achieve user-adjustable 3D finishing, we propose to learn a low-rank
4D bilateral grid from a given single view edit, lifting photo enhancements to
the whole 3D scene. We demonstrate our approach can boost the visual quality of
novel view synthesis by effectively removing floaters and performing
enhancements from user retouching. The source code and our data are available
at: https://bilarfpro.github.io.

Comments:
- SIGGRAPH (ACM TOG), 2024. Project page: https://bilarfpro.github.io

---

## Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head  Capture

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-01 | X. Li, Y. Cheng, X. Ren, H. Jia, D. Xu, W. Zhu, Y. Yan | cs.CV | [PDF](http://arxiv.org/pdf/2406.00440v1){: .btn .btn-green } |

**Abstract**: 4D head capture aims to generate dynamic topological meshes and corresponding
texture maps from videos, which is widely utilized in movies and games for its
ability to simulate facial muscle movements and recover dynamic textures in
pore-squeezing. The industry often adopts the method involving multi-view
stereo and non-rigid alignment. However, this approach is prone to errors and
heavily reliant on time-consuming manual processing by artists. To simplify
this process, we propose Topo4D, a novel framework for automatic geometry and
texture generation, which optimizes densely aligned 4D heads and 8K texture
maps directly from calibrated multi-view time-series images. Specifically, we
first represent the time-series faces as a set of dynamic 3D Gaussians with
fixed topology in which the Gaussian centers are bound to the mesh vertices.
Afterward, we perform alternative geometry and texture optimization
frame-by-frame for high-quality geometry and texture learning while maintaining
temporal topology stability. Finally, we can extract dynamic facial meshes in
regular wiring arrangement and high-fidelity textures with pore-level details
from the learned Gaussians. Extensive experiments show that our method achieves
superior results than the current SOTA face reconstruction methods both in the
quality of meshes and textures. Project page:
https://xuanchenli.github.io/Topo4D/.


