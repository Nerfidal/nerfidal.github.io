---
layout: default
title: June 2024
parent: Papers
nav_order: 202406
---

<!---metadata--->


## Rethinking Score Distillation as a Bridge Between Image Distributions

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-13 | David McAllister, Songwei Ge, Jia-Bin Huang, David W. Jacobs, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa | cs.CV | [PDF](http://arxiv.org/pdf/2406.09417v1){: .btn .btn-green } |

**Abstract**: Score distillation sampling (SDS) has proven to be an important tool,
enabling the use of large-scale diffusion priors for tasks operating in
data-poor domains. Unfortunately, SDS has a number of characteristic artifacts
that limit its usefulness in general-purpose applications. In this paper, we
make progress toward understanding the behavior of SDS and its variants by
viewing them as solving an optimal-cost transport path from a source
distribution to a target distribution. Under this new interpretation, these
methods seek to transport corrupted images (source) to the natural image
distribution (target). We argue that current methods' characteristic artifacts
are caused by (1) linear approximation of the optimal path and (2) poor
estimates of the source distribution. We show that calibrating the text
conditioning of the source distribution can produce high-quality generation and
translation results with little extra overhead. Our method can be easily
applied across many domains, matching or beating the performance of specialized
methods. We demonstrate its utility in text-to-2D, text-based NeRF
optimization, translating paintings to real images, optical illusion
generation, and 3D sketch-to-real. We compare our method to existing approaches
for score distillation sampling and show that it can produce high-frequency
details with realistic colors.

Comments:
- Project webpage: https://sds-bridge.github.io/

---

## NeRF Director: Revisiting View Selection in Neural Volume Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-13 | Wenhui Xiao, Rodrigo Santa Cruz, David Ahmedt-Aristizabal, Olivier Salvado, Clinton Fookes, Leo Lebrat | cs.CV | [PDF](http://arxiv.org/pdf/2406.08839v1){: .btn .btn-green } |

**Abstract**: Neural Rendering representations have significantly contributed to the field
of 3D computer vision. Given their potential, considerable efforts have been
invested to improve their performance. Nonetheless, the essential question of
selecting training views is yet to be thoroughly investigated. This key aspect
plays a vital role in achieving high-quality results and aligns with the
well-known tenet of deep learning: "garbage in, garbage out". In this paper, we
first illustrate the importance of view selection by demonstrating how a simple
rotation of the test views within the most pervasive NeRF dataset can lead to
consequential shifts in the performance rankings of state-of-the-art
techniques. To address this challenge, we introduce a unified framework for
view selection methods and devise a thorough benchmark to assess its impact.
Significant improvements can be achieved without leveraging error or
uncertainty estimation but focusing on uniform view coverage of the
reconstructed object, resulting in a training-free approach. Using this
technique, we show that high-quality renderings can be achieved faster by using
fewer views. We conduct extensive experiments on both synthetic datasets and
realistic data to demonstrate the effectiveness of our proposed method compared
with random, conventional error-based, and uncertainty-guided view selection.

Comments:
- CVPR2024

---

## Modeling Ambient Scene Dynamics for Free-view Synthesis


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-13 | Meng-Li Shih, Jia-Bin Huang, Changil Kim, Rajvi Shah, Johannes Kopf, Chen Gao | cs.CV | [PDF](http://arxiv.org/pdf/2406.09395v1){: .btn .btn-green } |

**Abstract**: We introduce a novel method for dynamic free-view synthesis of an ambient
scenes from a monocular capture bringing a immersive quality to the viewing
experience. Our method builds upon the recent advancements in 3D Gaussian
Splatting (3DGS) that can faithfully reconstruct complex static scenes.
Previous attempts to extend 3DGS to represent dynamics have been confined to
bounded scenes or require multi-camera captures, and often fail to generalize
to unseen motions, limiting their practical application. Our approach overcomes
these constraints by leveraging the periodicity of ambient motions to learn the
motion trajectory model, coupled with careful regularization. We also propose
important practical strategies to improve the visual quality of the baseline
3DGS static reconstructions and to improve memory efficiency critical for
GPU-memory intensive learning. We demonstrate high-quality photorealistic novel
view synthesis of several ambient natural scenes with intricate textures and
fine structural elements.



---

## Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for  Compressed Scene Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-13 | Fengyi Zhang, Tianjun Zhang, Lin Zhang, Helen Huang, Yadan Luo | cs.CV | [PDF](http://arxiv.org/pdf/2406.08759v1){: .btn .btn-green } |

**Abstract**: The field of novel-view synthesis has recently witnessed the emergence of 3D
Gaussian Splatting, which represents scenes in a point-based manner and renders
through rasterization. This methodology, in contrast to Radiance Fields that
rely on ray tracing, demonstrates superior rendering quality and speed.
However, the explicit and unstructured nature of 3D Gaussians poses a
significant storage challenge, impeding its broader application. To address
this challenge, we introduce the Gaussian-Forest modeling framework, which
hierarchically represents a scene as a forest of hybrid 3D Gaussians. Each
hybrid Gaussian retains its unique explicit attributes while sharing implicit
ones with its sibling Gaussians, thus optimizing parameterization with
significantly fewer variables. Moreover, adaptive growth and pruning strategies
are designed, ensuring detailed representation in complex regions and a notable
reduction in the number of required Gaussians. Extensive experiments
demonstrate that Gaussian-Forest not only maintains comparable speed and
quality but also achieves a compression rate surpassing 10 times, marking a
significant advancement in efficient scene modeling. Codes are available at
https://github.com/Xian-Bei/GaussianForest.



---

## GGHead: Fast and Generalizable 3D Gaussian Heads


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-13 | Tobias Kirschstein, Simon Giebenhain, Jiapeng Tang, Markos Georgopoulos, Matthias Nießner | cs.CV | [PDF](http://arxiv.org/pdf/2406.09377v1){: .btn .btn-green } |

**Abstract**: Learning 3D head priors from large 2D image collections is an important step
towards high-quality 3D-aware human modeling. A core requirement is an
efficient architecture that scales well to large-scale datasets and large image
resolutions. Unfortunately, existing 3D GANs struggle to scale to generate
samples at high resolutions due to their relatively slow train and render
speeds, and typically have to rely on 2D superresolution networks at the
expense of global 3D consistency. To address these challenges, we propose
Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian
Splatting representation within a 3D GAN framework. To generate a 3D
representation, we employ a powerful 2D CNN generator to predict Gaussian
attributes in the UV space of a template head mesh. This way, GGHead exploits
the regularity of the template's UV layout, substantially facilitating the
challenging task of predicting an unstructured set of 3D Gaussians. We further
improve the geometric fidelity of the generated 3D representations with a novel
total variation loss on rendered UV coordinates. Intuitively, this
regularization encourages that neighboring rendered pixels should stem from
neighboring Gaussians in the template's UV space. Taken together, our pipeline
can efficiently generate 3D heads trained only from single-view 2D image
observations. Our proposed framework matches the quality of existing 3D head
GANs on FFHQ while being both substantially faster and fully 3D consistent. As
a result, we demonstrate real-time generation and rendering of high-quality
3D-consistent heads at $1024^2$ resolution for the first time.

Comments:
- Project Page: https://tobias-kirschstein.github.io/gghead/ ; YouTube
  Video: https://www.youtube.com/watch?v=1iyC74neQXc

---

## AV-GS: Learning Material and Geometry Aware Priors for Novel View  Acoustic Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-13 | Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiankang Deng, Xiatian Zhu | cs.SD | [PDF](http://arxiv.org/pdf/2406.08920v1){: .btn .btn-green } |

**Abstract**: Novel view acoustic synthesis (NVAS) aims to render binaural audio at any
target viewpoint, given a mono audio emitted by a sound source at a 3D scene.
Existing methods have proposed NeRF-based implicit models to exploit visual
cues as a condition for synthesizing binaural audio. However, in addition to
low efficiency originating from heavy NeRF rendering, these methods all have a
limited ability of characterizing the entire scene environment such as room
geometry, material properties, and the spatial relation between the listener
and sound source. To address these issues, we propose a novel Audio-Visual
Gaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware
condition for audio synthesis, we learn an explicit point-based scene
representation with an audio-guidance parameter on locally initialized Gaussian
points, taking into account the space relation from the listener and sound
source. To make the visual scene model audio adaptive, we propose a point
densification and pruning strategy to optimally distribute the Gaussian points,
with the per-point contribution in sound propagation (e.g., more points needed
for texture-less wall surfaces as they affect sound path diversion). Extensive
experiments validate the superiority of our AV-GS over existing alternatives on
the real-world RWAS and simulation-based SoundSpaces datasets.



---

## Preserving Identity with Variational Score for General-purpose 3D  Editing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-13 | Duong H. Le, Tuan Pham, Aniruddha Kembhavi, Stephan Mandt, Wei-Chiu Ma, Jiasen Lu | cs.CV | [PDF](http://arxiv.org/pdf/2406.08953v1){: .btn .btn-green } |

**Abstract**: We present Piva (Preserving Identity with Variational Score Distillation), a
novel optimization-based method for editing images and 3D models based on
diffusion models. Specifically, our approach is inspired by the recently
proposed method for 2D image editing - Delta Denoising Score (DDS). We pinpoint
the limitations in DDS for 2D and 3D editing, which causes detail loss and
over-saturation. To address this, we propose an additional score distillation
term that enforces identity preservation. This results in a more stable editing
process, gradually optimizing NeRF models to match target prompts while
retaining crucial input characteristics. We demonstrate the effectiveness of
our approach in zero-shot image and neural field editing. Our method
successfully alters visual attributes, adds both subtle and substantial
structural elements, translates shapes, and achieves competitive results on
standard 2D and 3D editing benchmarks. Additionally, our method imposes no
constraints like masking or pre-training, making it compatible with a wide
range of pre-trained diffusion models. This allows for versatile editing
without needing neural field-to-mesh conversion, offering a more user-friendly
experience.

Comments:
- 22 pages, 14 figures

---

## Neural NeRF Compression

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-13 | Tuan Pham, Stephan Mandt | cs.CV | [PDF](http://arxiv.org/pdf/2406.08943v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing
detailed 3D scenes through continuous volumetric representations. Recent NeRFs
utilize feature grids to improve rendering quality and speed; however, these
representations introduce significant storage overhead. This paper presents a
novel method for efficiently compressing a grid-based NeRF model, addressing
the storage overhead concern. Our approach is based on the non-linear transform
coding paradigm, employing neural compression for compressing the model's
feature grids. Due to the lack of training data involving many i.i.d scenes, we
design an encoder-free, end-to-end optimized approach for individual scenes,
using lightweight decoders. To leverage the spatial inhomogeneity of the latent
feature grids, we introduce an importance-weighted rate-distortion objective
and a sparse entropy model employing a masking mechanism. Our experimental
results validate that our proposed method surpasses existing works in terms of
grid-based NeRF compression efficacy and reconstruction quality.

Comments:
- Accepted to ICML 2024

---

## ICE-G: Image Conditional Editing of 3D Gaussian Splats

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-12 | Vishnu Jaganathan, Hannah Hanyun Huang, Muhammad Zubair Irshad, Varun Jampani, Amit Raj, Zsolt Kira | cs.CV | [PDF](http://arxiv.org/pdf/2406.08488v1){: .btn .btn-green } |

**Abstract**: Recently many techniques have emerged to create high quality 3D assets and
scenes. When it comes to editing of these objects, however, existing approaches
are either slow, compromise on quality, or do not provide enough customization.
We introduce a novel approach to quickly edit a 3D model from a single
reference view. Our technique first segments the edit image, and then matches
semantically corresponding regions across chosen segmented dataset views using
DINO features. A color or texture change from a particular region of the edit
image can then be applied to other views automatically in a semantically
sensible manner. These edited views act as an updated dataset to further train
and re-style the 3D scene. The end-result is therefore an edited 3D model. Our
framework enables a wide variety of editing tasks such as manual local edits,
correspondence based style transfer from any example image, and a combination
of different styles from multiple example images. We use Gaussian Splats as our
primary 3D representation due to their speed and ease of local editing, but our
technique works for other methods such as NeRFs as well. We show through
multiple examples that our method produces higher quality results while
offering fine-grained control of editing. Project page: ice-gaussian.github.io

Comments:
- Accepted to CVPR AI4CC Workshop 2024. Project page:
  https://ice-gaussian.github.io

---

## Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent  Diffusion Models


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-12 | Yuxuan Xue, Xianghui Xie, Riccardo Marin, Gerard Pons-Moll | cs.CV | [PDF](http://arxiv.org/pdf/2406.08475v1){: .btn .btn-green } |

**Abstract**: Creating realistic avatars from a single RGB image is an attractive yet
challenging problem. Due to its ill-posed nature, recent works leverage
powerful prior from 2D diffusion models pretrained on large datasets. Although
2D diffusion models demonstrate strong generalization capability, they cannot
provide multi-view shape priors with guaranteed 3D consistency. We propose
Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent
Diffusion. Our key insight is that 2D multi-view diffusion and 3D
reconstruction models provide complementary information for each other, and by
coupling them in a tight manner, we can fully leverage the potential of both
models. We introduce a novel image-conditioned generative 3D Gaussian Splats
reconstruction model that leverages the priors from 2D multi-view diffusion
models, and provides an explicit 3D representation, which further guides the 2D
reverse sampling process to have better 3D consistency. Experiments show that
our proposed framework outperforms state-of-the-art methods and enables the
creation of realistic avatars from a single RGB image, achieving high-fidelity
in both geometry and appearance. Extensive ablations also validate the efficacy
of our design, (1) multi-view 2D priors conditioning in generative 3D
reconstruction and (2) consistency refinement of sampling trajectory via the
explicit 3D representation. Our code and models will be released on
https://yuxuan-xue.com/human-3diffusion.

Comments:
- Project Page: https://yuxuan-xue.com/human-3diffusion

---

## From Chaos to Clarity: 3DGS in the Dark

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-12 | Zhihao Li, Yufei Wang, Alex Kot, Bihan Wen | eess.IV | [PDF](http://arxiv.org/pdf/2406.08300v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis from raw images provides superior high dynamic range
(HDR) information compared to reconstructions from low dynamic range RGB
images. However, the inherent noise in unprocessed raw images compromises the
accuracy of 3D scene representation. Our study reveals that 3D Gaussian
Splatting (3DGS) is particularly susceptible to this noise, leading to numerous
elongated Gaussian shapes that overfit the noise, thereby significantly
degrading reconstruction quality and reducing inference speed, especially in
scenarios with limited views. To address these issues, we introduce a novel
self-supervised learning framework designed to reconstruct HDR 3DGS from a
limited number of noisy raw images. This framework enhances 3DGS by integrating
a noise extractor and employing a noise-robust reconstruction loss that
leverages a noise distribution prior. Experimental results show that our method
outperforms LDR/HDR 3DGS and previous state-of-the-art (SOTA) self-supervised
and supervised pre-trained models in both reconstruction quality and inference
speed on the RawNeRF dataset across a broad range of training views. Code can
be found in \url{https://lizhihao6.github.io/Raw3DGS}.



---

## OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with  Fine-Grained Understanding

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-12 | Yinan Deng, Jiahui Wang, Jingyu Zhao, Jianyu Dou, Yi Yang, Yufeng Yue | cs.CV | [PDF](http://arxiv.org/pdf/2406.08009v1){: .btn .btn-green } |

**Abstract**: In recent years, there has been a surge of interest in open-vocabulary 3D
scene reconstruction facilitated by visual language models (VLMs), which
showcase remarkable capabilities in open-set retrieval. However, existing
methods face some limitations: they either focus on learning point-wise
features, resulting in blurry semantic understanding, or solely tackle
object-level reconstruction, thereby overlooking the intricate details of the
object's interior. To address these challenges, we introduce OpenObj, an
innovative approach to build open-vocabulary object-level Neural Radiance
Fields (NeRF) with fine-grained understanding. In essence, OpenObj establishes
a robust framework for efficient and watertight scene modeling and
comprehension at the object-level. Moreover, we incorporate part-level features
into the neural fields, enabling a nuanced representation of object interiors.
This approach captures object-level instances while maintaining a fine-grained
understanding. The results on multiple datasets demonstrate that OpenObj
achieves superior performance in zero-shot semantic segmentation and retrieval
tasks. Additionally, OpenObj supports real-world robotics tasks at multiple
scales, including global movement and local manipulation.

Comments:
- 8 pages, 7figures. Project Url: https://openobj.github.io/

---

## Spatial Annealing Smoothing for Efficient Few-shot Neural Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-12 | Yuru Xiao, Xianming Liu, Deming Zhai, Kui Jiang, Junjun Jiang, Xiangyang Ji | cs.CV | [PDF](http://arxiv.org/pdf/2406.07828v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) with hybrid representations have shown
impressive capabilities in reconstructing scenes for view synthesis, delivering
high efficiency. Nonetheless, their performance significantly drops with sparse
view inputs, due to the issue of overfitting. While various regularization
strategies have been devised to address these challenges, they often depend on
inefficient assumptions or are not compatible with hybrid models. There is a
clear need for a method that maintains efficiency and improves resilience to
sparse views within a hybrid framework. In this paper, we introduce an accurate
and efficient few-shot neural rendering method named Spatial Annealing
smoothing regularized NeRF (SANeRF), which is specifically designed for a
pre-filtering-driven hybrid representation architecture. We implement an
exponential reduction of the sample space size from an initially large value.
This methodology is crucial for stabilizing the early stages of the training
phase and significantly contributes to the enhancement of the subsequent
process of detail refinement. Our extensive experiments reveal that, by adding
merely one line of code, SANeRF delivers superior rendering quality and much
faster reconstruction speed compared to current few-shot NeRF methods. Notably,
SANeRF outperforms FreeNeRF by 0.3 dB in PSNR on the Blender dataset, while
achieving 700x faster reconstruction speed.



---

## Generative Lifting of Multiview to 3D from Unknown Pose: Wrapping NeRF  inside Diffusion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-11 | Xin Yuan, Rana Hanocka, Michael Maire | cs.CV | [PDF](http://arxiv.org/pdf/2406.06972v1){: .btn .btn-green } |

**Abstract**: We cast multiview reconstruction from unknown pose as a generative modeling
problem. From a collection of unannotated 2D images of a scene, our approach
simultaneously learns both a network to predict camera pose from 2D image
input, as well as the parameters of a Neural Radiance Field (NeRF) for the 3D
scene. To drive learning, we wrap both the pose prediction network and NeRF
inside a Denoising Diffusion Probabilistic Model (DDPM) and train the system
via the standard denoising objective. Our framework requires the system
accomplish the task of denoising an input 2D image by predicting its pose and
rendering the NeRF from that pose. Learning to denoise thus forces the system
to concurrently learn the underlying 3D NeRF representation and a mapping from
images to camera extrinsic parameters. To facilitate the latter, we design a
custom network architecture to represent pose as a distribution, granting
implicit capacity for discovering view correspondences when trained end-to-end
for denoising alone. This technique allows our system to successfully build
NeRFs, without pose knowledge, for challenging scenes where competing methods
fail. At the conclusion of training, our learned NeRF can be extracted and used
as a 3D scene model; our full system can be used to sample novel camera poses
and generate novel-view images.



---

## Trim 3D Gaussian Splatting for Accurate Geometry Representation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-11 | Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, Zhaoxiang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2406.07499v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Trim 3D Gaussian Splatting (TrimGS) to
reconstruct accurate 3D geometry from images. Previous arts for geometry
reconstruction from 3D Gaussians mainly focus on exploring strong geometry
regularization. Instead, from a fresh perspective, we propose to obtain
accurate 3D geometry of a scene by Gaussian trimming, which selectively removes
the inaccurate geometry while preserving accurate structures. To achieve this,
we analyze the contributions of individual 3D Gaussians and propose a
contribution-based trimming strategy to remove the redundant or inaccurate
Gaussians. Furthermore, our experimental and theoretical analyses reveal that a
relatively small Gaussian scale is a non-negligible factor in representing and
optimizing the intricate details. Therefore the proposed TrimGS maintains
relatively small Gaussian scales. In addition, TrimGS is also compatible with
the effective geometry regularization strategies in previous arts. When
combined with the original 3DGS and the state-of-the-art 2DGS, TrimGS
consistently yields more accurate geometry and higher perceptual quality. Our
project page is https://trimgs.github.io

Comments:
- Project page: https://trimgs.github.io/

---

## M-LRM: Multi-view Large Reconstruction Model

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-11 | Mengfei Li, Xiaoxiao Long, Yixun Liang, Weiyu Li, Yuan Liu, Peng Li, Xiaowei Chi, Xingqun Qi, Wei Xue, Wenhan Luo, Qifeng Liu, Yike Guo | cs.CV | [PDF](http://arxiv.org/pdf/2406.07648v1){: .btn .btn-green } |

**Abstract**: Despite recent advancements in the Large Reconstruction Model (LRM)
demonstrating impressive results, when extending its input from single image to
multiple images, it exhibits inefficiencies, subpar geometric and texture
quality, as well as slower convergence speed than expected.
  It is attributed to that, LRM formulates 3D reconstruction as a naive
images-to-3D translation problem, ignoring the strong 3D coherence among the
input images. In this paper, we propose a Multi-view Large Reconstruction Model
(M-LRM) designed to efficiently reconstruct high-quality 3D shapes from
multi-views in a 3D-aware manner. Specifically, we introduce a multi-view
consistent cross-attention scheme to enable M-LRM to accurately query
information from the input images. Moreover, we employ the 3D priors of the
input multi-view images to initialize the tri-plane tokens. Compared to LRM,
the proposed M-LRM can produce a tri-plane NeRF with $128 \times 128$
resolution and generate 3D shapes of high fidelity. Experimental studies
demonstrate that our model achieves a significant performance gain and faster
training convergence than LRM. Project page: https://murphylmf.github.io/M-LRM/



---

## Neural Visibility Field for Uncertainty-Driven Active Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-11 | Shangjie Xue, Jesse Dill, Pranay Mathur, Frank Dellaert, Panagiotis Tsiotra, Danfei Xu | cs.CV | [PDF](http://arxiv.org/pdf/2406.06948v1){: .btn .btn-green } |

**Abstract**: This paper presents Neural Visibility Field (NVF), a novel uncertainty
quantification method for Neural Radiance Fields (NeRF) applied to active
mapping. Our key insight is that regions not visible in the training views lead
to inherently unreliable color predictions by NeRF at this region, resulting in
increased uncertainty in the synthesized views. To address this, we propose to
use Bayesian Networks to composite position-based field uncertainty into
ray-based uncertainty in camera observations. Consequently, NVF naturally
assigns higher uncertainty to unobserved regions, aiding robots to select the
most informative next viewpoints. Extensive evaluations show that NVF excels
not only in uncertainty quantification but also in scene reconstruction for
active mapping, outperforming existing methods.

Comments:
- Accepted to CVPR 2024. More details can be found at
  https://sites.google.com/view/nvf-cvpr24/

---

## C3DAG: Controlled 3D Animal Generation using 3D pose guidance

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-11 | Sandeep Mishra, Oindrila Saha, Alan C. Bovik | cs.CV | [PDF](http://arxiv.org/pdf/2406.07742v1){: .btn .btn-green } |

**Abstract**: Recent advancements in text-to-3D generation have demonstrated the ability to
generate high quality 3D assets. However while generating animals these methods
underperform, often portraying inaccurate anatomy and geometry. Towards
ameliorating this defect, we present C3DAG, a novel pose-Controlled text-to-3D
Animal Generation framework which generates a high quality 3D animal consistent
with a given pose. We also introduce an automatic 3D shape creator tool, that
allows dynamic pose generation and modification via a web-based tool, and that
generates a 3D balloon animal using simple geometries. A NeRF is then
initialized using this 3D shape using depth-controlled SDS. In the next stage,
the pre-trained NeRF is fine-tuned using quadruped-pose-controlled SDS. The
pipeline that we have developed not only produces geometrically and
anatomically consistent results, but also renders highly controlled 3D animals,
unlike prior methods which do not allow fine-grained pose control.



---

## Active Scout: Multi-Target Tracking Using Neural Radiance Fields in  Dense Urban Environments

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-11 | Christopher D. Hsu, Pratik Chaudhari | cs.MA | [PDF](http://arxiv.org/pdf/2406.07431v1){: .btn .btn-green } |

**Abstract**: We study pursuit-evasion games in highly occluded urban environments, e.g.
tall buildings in a city, where a scout (quadrotor) tracks multiple dynamic
targets on the ground. We show that we can build a neural radiance field (NeRF)
representation of the city -- online -- using RGB and depth images from
different vantage points. This representation is used to calculate the
information gain to both explore unknown parts of the city and track the
targets -- thereby giving a completely first-principles approach to actively
tracking dynamic targets. We demonstrate, using a custom-built simulator using
Open Street Maps data of Philadelphia and New York City, that we can explore
and locate 20 stationary targets within 300 steps. This is slower than a greedy
baseline which which does not use active perception. But for dynamic targets
that actively hide behind occlusions, we show that our approach maintains, at
worst, a tracking error of 200m; the greedy baseline can have a tracking error
as large as 600m. We observe a number of interesting properties in the scout's
policies, e.g., it switches its attention to track a different target
periodically, as the quality of the NeRF representation improves over time, the
scout also becomes better in terms of target tracking.

Comments:
- 8 pages, 8 figures, 1 table

---

## Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-11 | Chao Wang, Krzysztof Wolski, Bernhard Kerbl, Ana Serrano, Mojtaba Bemana, Hans-Peter Seidel, Karol Myszkowski, Thomas Leimkühler | cs.CV | [PDF](http://arxiv.org/pdf/2406.07329v1){: .btn .btn-green } |

**Abstract**: Radiance field methods represent the state of the art in reconstructing
complex scenes from multi-view photos. However, these reconstructions often
suffer from one or both of the following limitations: First, they typically
represent scenes in low dynamic range (LDR), which restricts their use to
evenly lit environments and hinders immersive viewing experiences. Secondly,
their reliance on a pinhole camera model, assuming all scene elements are in
focus in the input images, presents practical challenges and complicates
refocusing during novel-view synthesis. Addressing these limitations, we
present a lightweight method based on 3D Gaussian Splatting that utilizes
multi-view LDR images of a scene with varying exposure times, apertures, and
focus distances as input to reconstruct a high-dynamic-range (HDR) radiance
field. By incorporating analytical convolutions of Gaussians based on a
thin-lens camera model as well as a tonemapping module, our reconstructions
enable the rendering of HDR content with flexible refocusing capabilities. We
demonstrate that our combined treatment of HDR and depth of field facilitates
real-time cinematic rendering, outperforming the state of the art.



---

## IllumiNeRF: 3D Relighting without Inverse Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-10 | Xiaoming Zhao, Pratul P. Srinivasan, Dor Verbin, Keunhong Park, Ricardo Martin Brualla, Philipp Henzler | cs.CV | [PDF](http://arxiv.org/pdf/2406.06527v1){: .btn .btn-green } |

**Abstract**: Existing methods for relightable view synthesis -- using a set of images of
an object under unknown lighting to recover a 3D representation that can be
rendered from novel viewpoints under a target illumination -- are based on
inverse rendering, and attempt to disentangle the object geometry, materials,
and lighting that explain the input images. Furthermore, this typically
involves optimization through differentiable Monte Carlo rendering, which is
brittle and computationally-expensive. In this work, we propose a simpler
approach: we first relight each input image using an image diffusion model
conditioned on lighting and then reconstruct a Neural Radiance Field (NeRF)
with these relit images, from which we render novel views under the target
lighting. We demonstrate that this strategy is surprisingly competitive and
achieves state-of-the-art results on multiple relighting benchmarks. Please see
our project page at https://illuminerf.github.io/.

Comments:
- Project page: https://illuminerf.github.io/

---

## GaussianCity: Generative Gaussian Splatting for Unbounded 3D City  Generation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-10 | Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2406.06526v1){: .btn .btn-green } |

**Abstract**: 3D city generation with NeRF-based methods shows promising generation results
but is computationally inefficient. Recently 3D Gaussian Splatting (3D-GS) has
emerged as a highly efficient alternative for object-level 3D generation.
However, adapting 3D-GS from finite-scale 3D objects and humans to
infinite-scale 3D cities is non-trivial. Unbounded 3D city generation entails
significant storage overhead (out-of-memory issues), arising from the need to
expand points to billions, often demanding hundreds of Gigabytes of VRAM for a
city scene spanning 10km^2. In this paper, we propose GaussianCity, a
generative Gaussian Splatting framework dedicated to efficiently synthesizing
unbounded 3D cities with a single feed-forward pass. Our key insights are
two-fold: 1) Compact 3D Scene Representation: We introduce BEV-Point as a
highly compact intermediate representation, ensuring that the growth in VRAM
usage for unbounded scenes remains constant, thus enabling unbounded city
generation. 2) Spatial-aware Gaussian Attribute Decoder: We present
spatial-aware BEV-Point decoder to produce 3D Gaussian attributes, which
leverages Point Serializer to integrate the structural and contextual
characteristics of BEV points. Extensive experiments demonstrate that
GaussianCity achieves state-of-the-art results in both drone-view and
street-view 3D city generation. Notably, compared to CityDreamer, GaussianCity
exhibits superior performance with a speedup of 60 times (10.72 FPS v.s. 0.18
FPS).



---

## Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering  for HDR View Synthesis

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-10 | Xin Jin, Pengyi Jiao, Zheng-Peng Duan, Xingchao Yang, Chun-Le Guo, Bo Ren, Chongyi Li | cs.CV | [PDF](http://arxiv.org/pdf/2406.06216v1){: .btn .btn-green } |

**Abstract**: Volumetric rendering based methods, like NeRF, excel in HDR view synthesis
from RAWimages, especially for nighttime scenes. While, they suffer from long
training times and cannot perform real-time rendering due to dense sampling
requirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time
rendering and faster training. However, implementing RAW image-based view
synthesis directly using 3DGS is challenging due to its inherent drawbacks: 1)
in nighttime scenes, extremely low SNR leads to poor structure-from-motion
(SfM) estimation in distant views; 2) the limited representation capacity of
spherical harmonics (SH) function is unsuitable for RAW linear color space; and
3) inaccurate scene structure hampers downstream tasks such as refocusing. To
address these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our
method proposes Cone Scatter Initialization to enrich the estimation of SfM,
and replaces SH with a Color MLP to represent the RAW linear color space.
Additionally, we introduce depth distortion and near-far regularizations to
improve the accuracy of scene structure for downstream tasks. These designs
enable LE3D to perform real-time novel view synthesis, HDR rendering,
refocusing, and tone-mapping changes. Compared to previous volumetric rendering
based methods, LE3D reduces training time to 1% and improves rendering speed by
up to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can
be found in https://github.com/Srameo/LE3D .



---

## MVGamba: Unify 3D Content Generation as State Space Sequence Modeling

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-10 | Xuanyu Yi, Zike Wu, Qiuhong Shen, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Shuicheng Yan, Xinchao Wang, Hanwang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2406.06367v1){: .btn .btn-green } |

**Abstract**: Recent 3D large reconstruction models (LRMs) can generate high-quality 3D
content in sub-seconds by integrating multi-view diffusion models with scalable
multi-view reconstructors. Current works further leverage 3D Gaussian Splatting
as 3D representation for improved visual quality and rendering efficiency.
However, we observe that existing Gaussian reconstruction models often suffer
from multi-view inconsistency and blurred textures. We attribute this to the
compromise of multi-view information propagation in favor of adopting powerful
yet computationally intensive architectures (\eg, Transformers). To address
this issue, we introduce MVGamba, a general and lightweight Gaussian
reconstruction model featuring a multi-view Gaussian reconstructor based on the
RNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal
context containing multi-view information for cross-view self-refinement while
generating a long sequence of Gaussians for fine-detail modeling with linear
complexity. With off-the-shelf multi-view diffusion models integrated, MVGamba
unifies 3D generation tasks from a single image, sparse images, or text
prompts. Extensive experiments demonstrate that MVGamba outperforms
state-of-the-art baselines in all 3D content generation scenarios with
approximately only $0.1\times$ of the model size.



---

## ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields  with Diffusion Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-10 | Meng-Li Shih, Wei-Chiu Ma, Aleksander Holynski, Forrester Cole, Brian L. Curless, Janne Kontkanen | cs.CV | [PDF](http://arxiv.org/pdf/2406.06133v1){: .btn .btn-green } |

**Abstract**: We propose ExtraNeRF, a novel method for extrapolating the range of views
handled by a Neural Radiance Field (NeRF). Our main idea is to leverage NeRFs
to model scene-specific, fine-grained details, while capitalizing on diffusion
models to extrapolate beyond our observed data. A key ingredient is to track
visibility to determine what portions of the scene have not been observed, and
focus on reconstructing those regions consistently with diffusion models. Our
primary contributions include a visibility-aware diffusion-based inpainting
module that is fine-tuned on the input imagery, yielding an initial NeRF with
moderate quality (often blurry) inpainted regions, followed by a second
diffusion model trained on the input imagery to consistently enhance, notably
sharpen, the inpainted imagery from the first pass. We demonstrate high-quality
results, extrapolating beyond a small number of (typically six or fewer) input
views, effectively outpainting the NeRF as well as inpainting newly disoccluded
regions inside the original viewing volume. We compare with related work both
quantitatively and qualitatively and show significant gains over prior art.

Comments:
- 8 pages, 8 figures, CVPR2024

---

## PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity  Surface Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-10 | Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin Liu, Hujun Bao, Guofeng Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2406.06521v1){: .btn .btn-green } |

**Abstract**: Recently, 3D Gaussian Splatting (3DGS) has attracted widespread attention due
to its high-quality rendering, and ultra-fast training and rendering speed.
However, due to the unstructured and irregular nature of Gaussian point clouds,
it is difficult to guarantee geometric reconstruction accuracy and multi-view
consistency simply by relying on image reconstruction loss. Although many
studies on surface reconstruction based on 3DGS have emerged recently, the
quality of their meshes is generally unsatisfactory. To address this problem,
we propose a fast planar-based Gaussian splatting reconstruction representation
(PGSR) to achieve high-fidelity surface reconstruction while ensuring
high-quality rendering. Specifically, we first introduce an unbiased depth
rendering method, which directly renders the distance from the camera origin to
the Gaussian plane and the corresponding normal map based on the Gaussian
distribution of the point cloud, and divides the two to obtain the unbiased
depth. We then introduce single-view geometric, multi-view photometric, and
geometric regularization to preserve global geometric accuracy. We also propose
a camera exposure compensation model to cope with scenes with large
illumination variations. Experiments on indoor and outdoor scenes show that our
method achieves fast training and rendering while maintaining high-fidelity
rendering and geometric reconstruction, outperforming 3DGS-based and NeRF-based
methods.

Comments:
- project page: https://zju3dv.github.io/pgsr/

---

## RefGaussian: Disentangling Reflections from 3D Gaussian Splatting for  Realistic Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-09 | Rui Zhang, Tianyue Luo, Weidong Yang, Ben Fei, Jingyi Xu, Qingyuan Zhou, Keyi Liu, Ying He | cs.CV | [PDF](http://arxiv.org/pdf/2406.05852v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3D-GS) has made a notable advancement in the field of
neural rendering, 3D scene reconstruction, and novel view synthesis.
Nevertheless, 3D-GS encounters the main challenge when it comes to accurately
representing physical reflections, especially in the case of total reflection
and semi-reflection that are commonly found in real-world scenes. This
limitation causes reflections to be mistakenly treated as independent elements
with physical presence, leading to imprecise reconstructions. Herein, to tackle
this challenge, we propose RefGaussian to disentangle reflections from 3D-GS
for realistically modeling reflections. Specifically, we propose to split a
scene into transmitted and reflected components and represent these components
using two Spherical Harmonics (SH). Given that this decomposition is not fully
determined, we employ local regularization techniques to ensure local
smoothness for both the transmitted and reflected components, thereby achieving
more plausible decomposition outcomes than 3D-GS. Experimental results
demonstrate that our approach achieves superior novel view synthesis and
accurate depth estimation outcomes. Furthermore, it enables the utilization of
scene editing applications, ensuring both high-quality results and physical
coherence.



---

## VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-09 | Hanlin Chen, Fangyin Wei, Chen Li, Tianxin Huang, Yunsong Wang, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2406.05774v1){: .btn .btn-green } |

**Abstract**: Although 3D Gaussian Splatting has been widely studied because of its
realistic and efficient novel-view synthesis, it is still challenging to
extract a high-quality surface from the point-based representation. Previous
works improve the surface by incorporating geometric priors from the
off-the-shelf normal estimator. However, there are two main limitations: 1)
Supervising normal rendered from 3D Gaussians updates only the rotation
parameter while neglecting other geometric parameters; 2) The inconsistency of
predicted normal maps across multiple views may lead to severe reconstruction
artifacts. In this paper, we propose a Depth-Normal regularizer that directly
couples normal with other geometric parameters, leading to full updates of the
geometric parameters from normal regularization. We further propose a
confidence term to mitigate inconsistencies of normal predictions across
multiple views. Moreover, we also introduce a densification and splitting
strategy to regularize the size and distribution of 3D Gaussians for more
accurate surface modeling. Compared with Gaussian-based baselines, experiments
show that our approach obtains better reconstruction quality and maintains
competitive appearance quality at faster training speed and 100+ FPS rendering.
Our code will be made open-source upon paper acceptance.



---

## GTR: Improving Large 3D Reconstruction Models through Geometry and  Texture Refinement

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-09 | Peiye Zhuang, Songfang Han, Chaoyang Wang, Aliaksandr Siarohin, Jiaxu Zou, Michael Vasilkovsky, Vladislav Shakhrai, Sergey Korolev, Sergey Tulyakov, Hsin-Ying Lee | cs.CV | [PDF](http://arxiv.org/pdf/2406.05649v1){: .btn .btn-green } |

**Abstract**: We propose a novel approach for 3D mesh reconstruction from multi-view
images. Our method takes inspiration from large reconstruction models like LRM
that use a transformer-based triplane generator and a Neural Radiance Field
(NeRF) model trained on multi-view images. However, in our method, we introduce
several important modifications that allow us to significantly enhance 3D
reconstruction quality. First of all, we examine the original LRM architecture
and find several shortcomings. Subsequently, we introduce respective
modifications to the LRM architecture, which lead to improved multi-view image
representation and more computationally efficient training. Second, in order to
improve geometry reconstruction and enable supervision at full image
resolution, we extract meshes from the NeRF field in a differentiable manner
and fine-tune the NeRF model through mesh rendering. These modifications allow
us to achieve state-of-the-art performance on both 2D and 3D evaluation
metrics, such as a PSNR of 28.67 on Google Scanned Objects (GSO) dataset.
Despite these superior results, our feed-forward model still struggles to
reconstruct complex textures, such as text and portraits on assets. To address
this, we introduce a lightweight per-instance texture refinement procedure.
This procedure fine-tunes the triplane representation and the NeRF color
estimation model on the mesh surface using the input multi-view images in just
4 seconds. This refinement improves the PSNR to 29.79 and achieves faithful
reconstruction of complex textures, such as text. Additionally, our approach
enables various downstream applications, including text- or image-to-3D
generation.

Comments:
- 19 pages, 17 figures. Project page: https://payeah.net/projects/GTR/

---

## Multiplane Prior Guided Few-Shot Aerial Scene Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-07 | Zihan Gao, Licheng Jiao, Lingling Li, Xu Liu, Fang Liu, Puhua Chen, Yuwei Guo | cs.CV | [PDF](http://arxiv.org/pdf/2406.04961v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have been successfully applied in various
aerial scenes, yet they face challenges with sparse views due to limited
supervision. The acquisition of dense aerial views is often prohibitive, as
unmanned aerial vehicles (UAVs) may encounter constraints in perspective range
and energy constraints. In this work, we introduce Multiplane Prior guided NeRF
(MPNeRF), a novel approach tailored for few-shot aerial scene rendering-marking
a pioneering effort in this domain. Our key insight is that the intrinsic
geometric regularities specific to aerial imagery could be leveraged to enhance
NeRF in sparse aerial scenes. By investigating NeRF's and Multiplane Image
(MPI)'s behavior, we propose to guide the training process of NeRF with a
Multiplane Prior. The proposed Multiplane Prior draws upon MPI's benefits and
incorporates advanced image comprehension through a SwinV2 Transformer,
pre-trained via SimMIM. Our extensive experiments demonstrate that MPNeRF
outperforms existing state-of-the-art methods applied in non-aerial contexts,
by tripling the performance in SSIM and LPIPS even with three views available.
We hope our work offers insights into the development of NeRF-based
applications in aerial scenes with limited data.

Comments:
- 17 pages, 8 figures, accepted at CVPR 2024

---

## Multi-style Neural Radiance Field with AdaIN

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-07 | Yu-Wen Pao, An-Jie Li | cs.CV | [PDF](http://arxiv.org/pdf/2406.04960v1){: .btn .btn-green } |

**Abstract**: In this work, we propose a novel pipeline that combines AdaIN and NeRF for
the task of stylized Novel View Synthesis. Compared to previous works, we make
the following contributions: 1) We simplify the pipeline. 2) We extend the
capabilities of model to handle the multi-style task. 3) We modify the model
architecture to perform well on styles with strong brush strokes. 4) We
implement style interpolation on the multi-style model, allowing us to control
the style between any two styles and the style intensity between the stylized
output and the original scene, providing better control over the stylization
strength.



---

## Improving Physics-Augmented Continuum Neural Radiance Field-Based  Geometry-Agnostic System Identification with Lagrangian Particle Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Takuhiro Kaneko | cs.CV | [PDF](http://arxiv.org/pdf/2406.04155v1){: .btn .btn-green } |

**Abstract**: Geometry-agnostic system identification is a technique for identifying the
geometry and physical properties of an object from video sequences without any
geometric assumptions. Recently, physics-augmented continuum neural radiance
fields (PAC-NeRF) has demonstrated promising results for this technique by
utilizing a hybrid Eulerian-Lagrangian representation, in which the geometry is
represented by the Eulerian grid representations of NeRF, the physics is
described by a material point method (MPM), and they are connected via
Lagrangian particles. However, a notable limitation of PAC-NeRF is that its
performance is sensitive to the learning of the geometry from the first frames
owing to its two-step optimization. First, the grid representations are
optimized with the first frames of video sequences, and then the physical
properties are optimized through video sequences utilizing the fixed
first-frame grid representations. This limitation can be critical when learning
of the geometric structure is difficult, for example, in a few-shot (sparse
view) setting. To overcome this limitation, we propose Lagrangian particle
optimization (LPO), in which the positions and features of particles are
optimized through video sequences in Lagrangian space. This method allows for
the optimization of the geometric structure across the entire video sequence
within the physical constraints imposed by the MPM. The experimental results
demonstrate that the LPO is useful for geometric correction and physical
identification in sparse-view settings.

Comments:
- Accepted to CVPR 2024. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/lpo/

---

## Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a  Single Image


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, João F. Henriques, Christian Rupprecht, Andrea Vedaldi | cs.CV | [PDF](http://arxiv.org/pdf/2406.04343v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose Flash3D, a method for scene reconstruction and
novel view synthesis from a single image which is both very generalisable and
efficient. For generalisability, we start from a "foundation" model for
monocular depth estimation and extend it to a full 3D shape and appearance
reconstructor. For efficiency, we base this extension on feed-forward Gaussian
Splatting. Specifically, we predict a first layer of 3D Gaussians at the
predicted depth, and then add additional layers of Gaussians that are offset in
space, allowing the model to complete the reconstruction behind occlusions and
truncations. Flash3D is very efficient, trainable on a single GPU in a day, and
thus accessible to most researchers. It achieves state-of-the-art results when
trained and tested on RealEstate10k. When transferred to unseen datasets like
NYU it outperforms competitors by a large margin. More impressively, when
transferred to KITTI, Flash3D achieves better PSNR than methods trained
specifically on that dataset. In some instances, it even outperforms recent
methods that use multiple views as input. Code, models, demo, and more results
are available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.

Comments:
- Project page: https://www.robots.ox.ac.uk/~vgg/research/flash3d/

---

## Localized Gaussian Point Management

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Haosen Yang, Chenhao Zhang, Wenqing Wang, Marco Volino, Adrian Hilton, Li Zhang, Xiatian Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2406.04251v1){: .btn .btn-green } |

**Abstract**: Point management is a critical component in optimizing 3D Gaussian Splatting
(3DGS) models, as the point initiation (e.g., via structure from motion) is
distributionally inappropriate. Typically, the Adaptive Density Control (ADC)
algorithm is applied, leveraging view-averaged gradient magnitude thresholding
for point densification, opacity thresholding for pruning, and regular
all-points opacity reset. However, we reveal that this strategy is limited in
tackling intricate/special image regions (e.g., transparent) as it is unable to
identify all the 3D zones that require point densification, and lacking an
appropriate mechanism to handle the ill-conditioned points with negative
impacts (occlusion due to false high opacity). To address these limitations, we
propose a Localized Point Management (LPM) strategy, capable of identifying
those error-contributing zones in the highest demand for both point addition
and geometry calibration. Zone identification is achieved by leveraging the
underlying multiview geometry constraints, with the guidance of image rendering
errors. We apply point densification in the identified zone, whilst resetting
the opacity of those points residing in front of these regions so that a new
opportunity is created to correct ill-conditioned points. Serving as a
versatile plugin, LPM can be seamlessly integrated into existing 3D Gaussian
Splatting models. Experimental evaluation across both static 3D and dynamic 4D
scenes validate the efficacy of our LPM strategy in boosting a variety of
existing 3DGS models both quantitatively and qualitatively. Notably, LPM
improves both vanilla 3DGS and SpaceTimeGS to achieve state-of-the-art
rendering quality while retaining real-time speeds, outperforming on
challenging datasets such as Tanks & Temples and the Neural 3D Video Dataset.



---

## Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Diwen Wan, Ruijie Lu, Gang Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2406.03697v1){: .btn .btn-green } |

**Abstract**: Rendering novel view images in dynamic scenes is a crucial yet challenging
task. Current methods mainly utilize NeRF-based methods to represent the static
scene and an additional time-variant MLP to model scene deformations, resulting
in relatively low rendering quality as well as slow inference speed. To tackle
these challenges, we propose a novel framework named Superpoint Gaussian
Splatting (SP-GS). Specifically, our framework first employs explicit 3D
Gaussians to reconstruct the scene and then clusters Gaussians with similar
properties (e.g., rotation, translation, and location) into superpoints.
Empowered by these superpoints, our method manages to extend 3D Gaussian
splatting to dynamic scenes with only a slight increase in computational
expense. Apart from achieving state-of-the-art visual quality and real-time
rendering under high resolutions, the superpoint representation provides a
stronger manipulation capability. Extensive experiments demonstrate the
practicality and effectiveness of our approach on both synthetic and real-world
datasets. Please see our project page at
https://dnvtmf.github.io/SP_GS.github.io.

Comments:
- Accepted by ICML 2024

---

## How Far Can We Compress Instant-NGP-Based NeRF?

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Yihang Chen, Qianyi Wu, Mehrtash Harandi, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2406.04101v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Field (NeRF) has demonstrated remarkable
capabilities in representing 3D scenes. To expedite the rendering process,
learnable explicit representations have been introduced for combination with
implicit NeRF representation, which however results in a large storage space
requirement. In this paper, we introduce the Context-based NeRF Compression
(CNC) framework, which leverages highly efficient context models to provide a
storage-friendly NeRF representation. Specifically, we excavate both level-wise
and dimension-wise context dependencies to enable probability prediction for
information entropy reduction. Additionally, we exploit hash collision and
occupancy grids as strong prior knowledge for better context modeling. To the
best of our knowledge, we are the first to construct and exploit context models
for NeRF compression. We achieve a size reduction of 100$\times$ and 70$\times$
with improved fidelity against the baseline Instant-NGP on Synthesic-NeRF and
Tanks and Temples datasets, respectively. Additionally, we attain 86.7\% and
82.3\% storage size reduction against the SOTA NeRF compression method BiRF.
Our code is available here: https://github.com/YihangChen-ee/CNC.

Comments:
- Project Page: https://yihangchen-ee.github.io/project_cnc/ Code:
  https://github.com/yihangchen-ee/cnc/. We further propose a 3DGS compression
  method HAC, which is based on CNC:
  https://yihangchen-ee.github.io/project_hac/

---

## Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware  Spatio-Temporal Sampling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang, Pedro Miraldo, Suhas Lohit, Moitreya Chatterjee | cs.CV | [PDF](http://arxiv.org/pdf/2406.03723v1){: .btn .btn-green } |

**Abstract**: Extensions of Neural Radiance Fields (NeRFs) to model dynamic scenes have
enabled their near photo-realistic, free-viewpoint rendering. Although these
methods have shown some potential in creating immersive experiences, two
drawbacks limit their ubiquity: (i) a significant reduction in reconstruction
quality when the computing budget is limited, and (ii) a lack of semantic
understanding of the underlying scenes. To address these issues, we introduce
Gear-NeRF, which leverages semantic information from powerful image
segmentation models. Our approach presents a principled way for learning a
spatio-temporal (4D) semantic embedding, based on which we introduce the
concept of gears to allow for stratified modeling of dynamic regions of the
scene based on the extent of their motion. Such differentiation allows us to
adjust the spatio-temporal sampling resolution for each region in proportion to
its motion scale, achieving more photo-realistic dynamic novel view synthesis.
At the same time, almost for free, our approach enables free-viewpoint tracking
of objects of interest - a functionality not yet achieved by existing
NeRF-based methods. Empirical studies validate the effectiveness of our method,
where we achieve state-of-the-art rendering and tracking performance on
multiple challenging datasets.

Comments:
- Paper accepted to IEEE/CVF CVPR 2024 (Spotlight). Work done when XL
  was an intern at MERL. Project Page Link:
  https://merl.com/research/highlights/gear-nerf

---

## A Survey on 3D Human Avatar Modeling -- From Reconstruction to  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Ruihe Wang, Yukang Cao, Kai Han, Kwan-Yee K. Wong | cs.CV | [PDF](http://arxiv.org/pdf/2406.04253v1){: .btn .btn-green } |

**Abstract**: 3D modeling has long been an important area in computer vision and computer
graphics. Recently, thanks to the breakthroughs in neural representations and
generative models, we witnessed a rapid development of 3D modeling. 3D human
modeling, lying at the core of many real-world applications, such as gaming and
animation, has attracted significant attention. Over the past few years, a
large body of work on creating 3D human avatars has been introduced, forming a
new and abundant knowledge base for 3D human modeling. The scale of the
literature makes it difficult for individuals to keep track of all the works.
This survey aims to provide a comprehensive overview of these emerging
techniques for 3D human avatar modeling, from both reconstruction and
generation perspectives. Firstly, we review representative methods for 3D human
reconstruction, including methods based on pixel-aligned implicit function,
neural radiance field, and 3D Gaussian Splatting, etc. We then summarize
representative methods for 3D human generation, especially those using large
language models like CLIP, diffusion models, and various 3D representations,
which demonstrate state-of-the-art performance. Finally, we discuss our
reflection on existing methods and open challenges for 3D human avatar
modeling, shedding light on future research.

Comments:
- 30 pages, 21 figures

---

## Adversarial Generation of Hierarchical Gaussians for 3D Generative Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-05 | Sangeek Hyun, Jae-Pil Heo | cs.CV | [PDF](http://arxiv.org/pdf/2406.02968v1){: .btn .btn-green } |

**Abstract**: Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend
on ray casting-based volume rendering, which incurs demanding rendering costs.
One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS),
providing a much faster rendering speed and explicit 3D representation. In this
paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its
efficient and explicit characteristics. However, in an adversarial framework,
we observe that a na\"ive generator architecture suffers from training
instability and lacks the capability to adjust the scale of Gaussians. This
leads to model divergence and visual artifacts due to the absence of proper
guidance for initialized positions of Gaussians and densification to manage
their scales adaptively. To address these issues, we introduce a generator
architecture with a hierarchical multi-scale Gaussian representation that
effectively regularizes the position and scale of generated Gaussians.
Specifically, we design a hierarchy of Gaussians where finer-level Gaussians
are parameterized by their coarser-level counterparts; the position of
finer-level Gaussians would be located near their coarser-level counterparts,
and the scale would monotonically decrease as the level becomes finer, modeling
both coarse and fine details of the 3D scene. Experimental results demonstrate
that ours achieves a significantly faster rendering speed (x100) compared to
state-of-the-art 3D consistent GANs with comparable 3D generation capability.
Project page: https://hse1032.github.io/gsgan.

Comments:
- Project page: https://hse1032.github.io/gsgan

---

## Event3DGS: Event-Based 3D Gaussian Splatting for High-Speed Robot  Egomotion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-05 | Tianyi Xiong, Jiayi Wu, Botao He, Cornelia Fermuller, Yiannis Aloimonos, Heng Huang, Christopher A. Metzler | cs.CV | [PDF](http://arxiv.org/pdf/2406.02972v2){: .btn .btn-green } |

**Abstract**: By combining differentiable rendering with explicit point-based scene
representations, 3D Gaussian Splatting (3DGS) has demonstrated breakthrough 3D
reconstruction capabilities. However, to date 3DGS has had limited impact on
robotics, where high-speed egomotion is pervasive: Egomotion introduces motion
blur and leads to artifacts in existing frame-based 3DGS reconstruction
methods. To address this challenge, we introduce Event3DGS, an {\em
event-based} 3DGS framework. By exploiting the exceptional temporal resolution
of event cameras, Event3GDS can reconstruct high-fidelity 3D structure and
appearance under high-speed egomotion. Extensive experiments on multiple
synthetic and real-world datasets demonstrate the superiority of Event3DGS
compared with existing event-based dense 3D scene reconstruction frameworks;
Event3DGS substantially improves reconstruction quality (+3dB) while reducing
computational costs by 95\%. Our framework also allows one to incorporate a few
motion-blurred frame-based measurements into the reconstruction process to
further improve appearance fidelity without loss of structural accuracy.



---

## SatSplatYOLO: 3D Gaussian Splatting-based Virtual Object Detection  Ensembles for Satellite Feature Recognition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Van Minh Nguyen, Emma Sandidge, Trupti Mahendrakar, Ryan T. White | cs.CV | [PDF](http://arxiv.org/pdf/2406.02533v1){: .btn .btn-green } |

**Abstract**: On-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possibly unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. In this article, we present an approach for
mapping geometries and high-confidence detection of components of unknown,
non-cooperative satellites on orbit. We implement accelerated 3D Gaussian
splatting to learn a 3D representation of the satellite, render virtual views
of the target, and ensemble the YOLOv5 object detector over the virtual views,
resulting in reliable, accurate, and precise satellite component detections.
The full pipeline capable of running on-board and stand to enable downstream
machine intelligence tasks necessary for autonomous guidance, navigation, and
control tasks.



---

## OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary  Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2406.02058v1){: .btn .btn-green } |

**Abstract**: This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting
(3DGS) capable of 3D point-level open vocabulary understanding. Our primary
motivation stems from observing that existing 3DGS-based open vocabulary
methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D
point-level tasks due to weak feature expressiveness and inaccurate 2D-3D
feature associations. To ensure robust feature presentation and 3D point-level
understanding, we first employ SAM masks without cross-frame associations to
train instance features with 3D consistency. These features exhibit both
intra-object consistency and inter-object distinction. Then, we propose a
two-stage codebook to discretize these features from coarse to fine levels. At
the coarse level, we consider the positional information of 3D points to
achieve location-based clustering, which is then refined at the fine level.
Finally, we introduce an instance-level 3D-2D feature association method that
links 3D points to 2D masks, which are further associated with 2D CLIP
features. Extensive experiments, including open vocabulary-based 3D object
selection, 3D point cloud understanding, click-based 3D object selection, and
ablation studies, demonstrate the effectiveness of our proposed method. Project
page: https://3d-aigc.github.io/OpenGaussian

Comments:
- technical report, 15 pages

---

## Enhancing Temporal Consistency in Video Editing by Reconstructing Videos  with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Inkyu Shin, Qihang Yu, Xiaohui Shen, In So Kweon, Kuk-Jin Yoon, Liang-Chieh Chen | cs.CV | [PDF](http://arxiv.org/pdf/2406.02541v3){: .btn .btn-green } |

**Abstract**: Recent advancements in zero-shot video diffusion models have shown promise
for text-driven video editing, but challenges remain in achieving high temporal
consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting
(3DGS)-based video refiner designed to enhance temporal consistency in
zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian
optimizing process tailored for editing dynamic monocular videos. In the first
stage, Video-3DGS employs an improved version of COLMAP, referred to as
MC-COLMAP, which processes original videos using a Masked and Clipped approach.
For each video clip, MC-COLMAP generates the point clouds for dynamic
foreground objects and complex backgrounds. These point clouds are utilized to
initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent
foreground and background views. Both foreground and background views are then
merged with a 2D learnable parameter map to reconstruct full views. In the
second stage, we leverage the reconstruction ability developed in the first
stage to impose the temporal constraints on the video diffusion model. To
demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive
experiments across two related tasks: Video Reconstruction and Video Editing.
Video-3DGS trained with 3k iterations significantly improves video
reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency
(x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods
on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring
temporal consistency across 58 dynamic monocular videos.

Comments:
- Project page at https://video-3dgs-project.github.io/

---

## 3D-HGS: 3D Half-Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Haolin Li, Jinyang Liu, Mario Sznaier, Octavia Camps | cs.CV | [PDF](http://arxiv.org/pdf/2406.02720v1){: .btn .btn-green } |

**Abstract**: Photo-realistic 3D Reconstruction is a fundamental problem in 3D computer
vision. This domain has seen considerable advancements owing to the advent of
recent neural rendering techniques. These techniques predominantly aim to focus
on learning volumetric representations of 3D scenes and refining these
representations via loss functions derived from rendering. Among these, 3D
Gaussian Splatting (3D-GS) has emerged as a significant method, surpassing
Neural Radiance Fields (NeRFs). 3D-GS uses parameterized 3D Gaussians for
modeling both spatial locations and color information, combined with a
tile-based fast rendering technique. Despite its superior rendering performance
and speed, the use of 3D Gaussian kernels has inherent limitations in
accurately representing discontinuous functions, notably at edges and corners
for shape discontinuities, and across varying textures for color
discontinuities. To address this problem, we propose to employ 3D Half-Gaussian
(3D-HGS) kernels, which can be used as a plug-and-play kernel. Our experiments
demonstrate their capability to improve the performance of current 3D-GS
related methods and achieve state-of-the-art rendering performance on various
datasets without compromising rendering speed.

Comments:
- 9 pages, 6 figures

---

## FastLGS: Speeding up Language Embedded Gaussians with Feature Grid  Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Yuzhou Ji, He Zhu, Junshu Tang, Wuyi Liu, Zhizhong Zhang, Yuan Xie, Lizhuang Ma, Xin Tan | cs.CV | [PDF](http://arxiv.org/pdf/2406.01916v1){: .btn .btn-green } |

**Abstract**: The semantically interactive radiance field has always been an appealing task
for its potential to facilitate user-friendly and automated real-world 3D scene
understanding applications. However, it is a challenging task to achieve high
quality, efficiency and zero-shot ability at the same time with semantics in
radiance fields. In this work, we present FastLGS, an approach that supports
real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high
resolution. We propose the semantic feature grid to save multi-view CLIP
features which are extracted based on Segment Anything Model (SAM) masks, and
map the grids to low dimensional features for semantic field training through
3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through
feature grids from rendered features for open-vocabulary queries. Comparisons
with other state-of-the-art methods prove that FastLGS can achieve the first
place performance concerning both speed and accuracy, where FastLGS is 98x
faster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that
FastLGS is adaptive and compatible with many downstream tasks, such as 3D
segmentation and 3D object inpainting, which can be easily applied to other 3D
manipulation systems.



---

## WE-GS: An In-the-wild Efficient 3D Gaussian Representation for  Unconstrained Photo Collections

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Yuze Wang, Junyi Wang, Yue Qi | cs.CV | [PDF](http://arxiv.org/pdf/2406.02407v1){: .btn .btn-green } |

**Abstract**: Novel View Synthesis (NVS) from unconstrained photo collections is
challenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) has
shown promise for photorealistic and real-time NVS of static scenes. Building
on 3DGS, we propose an efficient point-based differentiable rendering framework
for scene reconstruction from photo collections. Our key innovation is a
residual-based spherical harmonic coefficients transfer module that adapts 3DGS
to varying lighting conditions and photometric post-processing. This
lightweight module can be pre-computed and ensures efficient gradient
propagation from rendered images to 3D Gaussian attributes. Additionally, we
observe that the appearance encoder and the transient mask predictor, the two
most critical parts of NVS from unconstrained photo collections, can be
mutually beneficial. We introduce a plug-and-play lightweight spatial attention
module to simultaneously predict transient occluders and latent appearance
representation for each image. After training and preprocessing, our method
aligns with the standard 3DGS format and rendering pipeline, facilitating
seamlessly integration into various 3DGS applications. Extensive experiments on
diverse datasets show our approach outperforms existing approaches on the
rendering quality of novel view and appearance synthesis with high converge and
rendering speed.

Comments:
- Our project page is available at
  https://yuzewang1998.github.io/we-gs.github.io/

---

## Query-based Semantic Gaussian Field for Scene Representation in  Reinforcement Learning

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Jiaxu Wang, Ziyi Zhang, Qiang Zhang, Jia Li, Jingkai Sun, Mingyuan Sun, Junhao He, Renjing Xu | cs.RO | [PDF](http://arxiv.org/pdf/2406.02370v2){: .btn .btn-green } |

**Abstract**: Latent scene representation plays a significant role in training
reinforcement learning (RL) agents. To obtain good latent vectors describing
the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF
pipeline into scene representation learning. However, these NeRF-related
methods struggle to perceive 3D structural information due to the inefficient
dense sampling in volumetric rendering. Moreover, they lack fine-grained
semantic information included in their scene representation vectors because
they evenly consider free and occupied spaces. Both of them can destroy the
performance of downstream RL tasks. To address the above challenges, we propose
a novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to
learn 3D scene representation for the first time. In brief, we present the
Query-based Generalizable 3DGS to bridge the 3DGS technique and scene
representations with more geometrical awareness than those in NeRFs. Moreover,
we present the Hierarchical Semantics Encoding to ground the fine-grained
semantic features to 3D Gaussians and further distilled to the scene
representation vectors. We conduct extensive experiments on two RL platforms
including Maniskill2 and Robomimic across 10 different tasks. The results show
that our method outperforms the other 5 baselines by a large margin. We achieve
the best success rates on 8 tasks and the second-best on the other two tasks.



---

## DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Zhongpai Gao, Benjamin Planche, Meng Zheng, Xiao Chen, Terrence Chen, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2406.02518v1){: .btn .btn-green } |

**Abstract**: Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images
generated from 3D CT volumes, widely used in preoperative settings but limited
in intraoperative applications due to computational bottlenecks, especially for
accurate but heavy physics-based Monte Carlo methods. While analytical DRR
renderers offer greater efficiency, they overlook anisotropic X-ray image
formation phenomena, such as Compton scattering. We present a novel approach
that marries realistic physics-inspired X-ray simulation with efficient,
differentiable DRR generation using 3D Gaussian splatting (3DGS). Our
direction-disentangled 3DGS (DDGS) method separates the radiosity contribution
into isotropic and direction-dependent components, approximating complex
anisotropic interactions without intricate runtime simulations. Additionally,
we adapt the 3DGS initialization to account for tomography data properties,
enhancing accuracy and efficiency. Our method outperforms state-of-the-art
techniques in image accuracy. Furthermore, our DDGS shows promise for
intraoperative applications and inverse problems such as pose registration,
delivering superior registration accuracy and runtime performance compared to
analytical DRR methods.



---

## Tetrahedron Splatting for 3D Generation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Chun Gu, Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2406.01579v1){: .btn .btn-green } |

**Abstract**: 3D representation is essential to the significant advance of 3D generation
with 2D diffusion priors. As a flexible representation, NeRF has been first
adopted for 3D representation. With density-based volumetric rendering, it
however suffers both intensive computational overhead and inaccurate mesh
extraction. Using a signed distance field and Marching Tetrahedra, DMTet allows
for precise mesh extraction and real-time rendering but is limited in handling
large topological changes in meshes, leading to optimization challenges.
Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and
rendering efficiency while falling short in mesh extraction. In this work, we
introduce a novel 3D representation, Tetrahedron Splatting (TeT-Splatting),
that supports easy convergence during optimization, precise mesh extraction,
and real-time rendering simultaneously. This is achieved by integrating
surface-based volumetric rendering within a structured tetrahedral grid while
preserving the desired ability of precise mesh extraction, and a tile-based
differentiable tetrahedron rasterizer. Furthermore, we incorporate eikonal and
normal consistency regularization terms for the signed distance field to
improve generation quality and stability. Critically, our representation can be
trained without mesh extraction, making the optimization process easier to
converge. Our TeT-Splatting can be readily integrated in existing 3D generation
pipelines, along with polygonal mesh for texture optimization. Extensive
experiments show that our TeT-Splatting strikes a superior tradeoff among
convergence speed, render efficiency, and mesh quality as compared to previous
alternatives under varying 3D generation settings.

Comments:
- Code: https://github.com/fudan-zvg/tet-splatting

---

## Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Shaojie Ma, Yawei Luo, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2406.01593v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction and simulation, while interrelated, have distinct
objectives: reconstruction demands a flexible 3D representation adaptable to
diverse scenes, whereas simulation requires a structured representation to
model motion principles effectively. This paper introduces the Mesh-adsorbed
Gaussian Splatting (MaGS) method to resolve such a dilemma. MaGS constrains 3D
Gaussians to hover on the mesh surface, creating a mutual-adsorbed
mesh-Gaussian 3D representation that combines the rendering flexibility of 3D
Gaussians with the spatial coherence of meshes. Leveraging this representation,
we introduce a learnable Relative Deformation Field (RDF) to model the relative
displacement between the mesh and 3D Gaussians, extending traditional
mesh-driven deformation paradigms that only rely on ARAP prior, thus capturing
the motion of each 3D Gaussian more precisely. By joint optimizing meshes, 3D
Gaussians, and RDF, MaGS achieves both high rendering accuracy and realistic
deformation. Extensive experiments on the D-NeRF and NeRF-DS datasets
demonstrate that MaGS can generate competitive results in both reconstruction
and simulation.

Comments:
- Project Page: see https://wcwac.github.io/MaGS-page/

---

## DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with  Video Diffusion Priors


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Tianyu Huang, Yihan Zeng, Hui Li, Wangmeng Zuo, Rynson W. H. Lau | cs.CV | [PDF](http://arxiv.org/pdf/2406.01476v1){: .btn .btn-green } |

**Abstract**: Dynamic 3D interaction has witnessed great interest in recent works, while
creating such 4D content remains challenging. One solution is to animate 3D
scenes with physics-based simulation, and the other is to learn the deformation
of static 3D objects with the distillation of video generative models. The
former one requires assigning precise physical properties to the target object,
otherwise the simulated results would become unnatural. The latter tends to
formulate the video with minor motions and discontinuous frames, due to the
absence of physical constraints in deformation learning. We think that video
generative models are trained with real-world captured data, capable of judging
physical phenomenon in simulation environments. To this end, we propose
DreamPhysics in this work, which estimates physical properties of 3D Gaussian
Splatting with video diffusion priors. DreamPhysics supports both image- and
text-conditioned guidance, optimizing physical parameters via score
distillation sampling with frame interpolation and log gradient. Based on a
material point method simulator with proper physical parameters, our method can
generate 4D content with realistic motions. Experimental results demonstrate
that, by distilling the prior knowledge of video diffusion models, inaccurate
physical properties can be gradually refined for high-quality simulation. Codes
are released at: https://github.com/tyhuang0428/DreamPhysics.

Comments:
- Technical report. Codes are released at:
  https://github.com/tyhuang0428/DreamPhysics

---

## RaDe-GS: Rasterizing Depth in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, Ping Tan | cs.GR | [PDF](http://arxiv.org/pdf/2406.01467v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has proven to be highly effective in novel view
synthesis, achieving high-quality and real-time rendering. However, its
potential for reconstructing detailed 3D shapes has not been fully explored.
Existing methods often suffer from limited shape accuracy due to the discrete
and unstructured nature of Gaussian splats, which complicates the shape
extraction. While recent techniques like 2D GS have attempted to improve shape
reconstruction, they often reformulate the Gaussian primitives in ways that
reduce both rendering quality and computational efficiency. To address these
problems, our work introduces a rasterized approach to render the depth maps
and surface normal maps of general 3D Gaussian splats. Our method not only
significantly enhances shape reconstruction accuracy but also maintains the
computational efficiency intrinsic to Gaussian Splatting. Our approach achieves
a Chamfer distance error comparable to NeuraLangelo on the DTU dataset and
similar training and rendering time as traditional Gaussian Splatting on the
Tanks & Temples dataset. Our method is a significant advancement in Gaussian
Splatting and can be directly integrated into existing Gaussian Splatting-based
methods.



---

## Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Fang Li, Hao Zhang, Narendra Ahuja | cs.CV | [PDF](http://arxiv.org/pdf/2406.01042v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has significantly elevated scene reconstruction
efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance
Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS
methods, whether based on GS or NeRF, primarily rely on camera parameters
provided by COLMAP and even utilize sparse point clouds generated by COLMAP for
initialization, which lack accuracy as well are time-consuming. This sometimes
results in poor dynamic scene representation, especially in scenes with large
object movements, or extreme camera conditions e.g. small translations combined
with large rotations. Some studies simultaneously optimize the estimation of
camera parameters and scenes, supervised by additional information like depth,
optical flow, etc. obtained from off-the-shelf models. Using this unverified
information as ground truth can reduce robustness and accuracy, which does
frequently occur for long monocular videos (with e.g. > hundreds of frames). We
propose a novel approach that learns a high-fidelity 4D GS scene representation
with self-calibration of camera parameters. It includes the extraction of 2D
point features that robustly represent 3D structure, and their use for
subsequent joint optimization of camera parameters and 3D structure towards
overall 4D scene optimization. We demonstrate the accuracy and time efficiency
of our method through extensive quantitative and qualitative experimental
results on several standard benchmarks. The results show significant
improvements over state-of-the-art methods for 4D novel view synthesis. The
source code will be released soon at https://github.com/fangli333/SC-4DGS.

Comments:
- GitHub Page: https://github.com/fangli333/SC-4DGS

---

## Representing Animatable Avatar via Factorized Neural Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin | cs.CV | [PDF](http://arxiv.org/pdf/2406.00637v1){: .btn .btn-green } |

**Abstract**: For reconstructing high-fidelity human 3D models from monocular videos, it is
crucial to maintain consistent large-scale body shapes along with finely
matched subtle wrinkles. This paper explores the observation that the per-frame
rendering results can be factorized into a pose-independent component and a
corresponding pose-dependent equivalent to facilitate frame consistency. Pose
adaptive textures can be further improved by restricting frequency bands of
these two components. In detail, pose-independent outputs are expected to be
low-frequency, while highfrequency information is linked to pose-dependent
factors. We achieve a coherent preservation of both coarse body contours across
the entire input video and finegrained texture features that are time variant
with a dual-branch network with distinct frequency components. The first branch
takes coordinates in canonical space as input, while the second branch
additionally considers features outputted by the first branch and pose
information of each frame. Our network integrates the information predicted by
both branches and utilizes volume rendering to generate photo-realistic 3D
human images. Through experiments, we demonstrate that our network surpasses
the neural radiance fields (NeRF) based state-of-the-art methods in preserving
high-frequency details and ensuring consistent body contours.



---

## PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Yeonsung Jung, Heecheol Yun, Joonhyung Park, Jin-Hwa Kim, Eunho Yang | cs.CV | [PDF](http://arxiv.org/pdf/2406.00798v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown remarkable performance in learning
3D scenes. However, NeRF exhibits vulnerability when confronted with
distractors in the training images -- unexpected objects are present only
within specific views, such as moving entities like pedestrians or birds.
Excluding distractors during dataset construction is a straightforward
solution, but without prior knowledge of their types and quantities, it becomes
prohibitively expensive. In this paper, we propose PruNeRF, a segment-centric
dataset pruning framework via 3D spatial consistency, that effectively
identifies and prunes the distractors. We first examine existing metrics for
measuring pixel-wise distraction and introduce Influence Functions for more
accurate measurements. Then, we assess 3D spatial consistency using a
depth-based reprojection technique to obtain 3D-aware distraction. Furthermore,
we incorporate segmentation for pixel-to-segment refinement, enabling more
precise identification. Our experiments on benchmark datasets demonstrate that
PruNeRF consistently outperforms state-of-the-art methods in robustness against
distractors.



---

## Efficient Neural Light Fields (ENeLF) for Mobile Devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Austin Peng | cs.CV | [PDF](http://arxiv.org/pdf/2406.00598v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) is a challenge in computer vision and graphics,
focusing on generating realistic images of a scene from unobserved camera
poses, given a limited set of authentic input images. Neural radiance fields
(NeRF) achieved impressive results in rendering quality by utilizing volumetric
rendering. However, NeRF and its variants are unsuitable for mobile devices due
to the high computational cost of volumetric rendering. Emerging research in
neural light fields (NeLF) eliminates the need for volumetric rendering by
directly learning a mapping from ray representation to pixel color. NeLF has
demonstrated its capability to achieve results similar to NeRF but requires a
more extensive, computationally intensive network that is not mobile-friendly.
Unlike existing works, this research builds upon the novel network architecture
introduced by MobileR2L and aggressively applies a compression technique
(channel-wise structure pruning) to produce a model that runs efficiently on
mobile devices with lower latency and smaller sizes, with a slight decrease in
performance.



---

## SuperGaussian: Repurposing Video Models for 3D Super Resolution

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Yuan Shen, Duygu Ceylan, Paul Guerrero, Zexiang Xu, Niloy J. Mitra, Shenlong Wang, Anna Frühstück | cs.CV | [PDF](http://arxiv.org/pdf/2406.00609v2){: .btn .btn-green } |

**Abstract**: We present a simple, modular, and generic method that upsamples coarse 3D
models by adding geometric and appearance details. While generative 3D models
now exist, they do not yet match the quality of their counterparts in image and
video domains. We demonstrate that it is possible to directly repurpose
existing (pretrained) video models for 3D super-resolution and thus sidestep
the problem of the shortage of large repositories of high-quality 3D training
models. We describe how to repurpose video upsampling models, which are not 3D
consistent, and combine them with 3D consolidation to produce 3D-consistent
results. As output, we produce high quality Gaussian Splat models, which are
object centric and effective. Our method is category agnostic and can be easily
incorporated into existing 3D workflows. We evaluate our proposed SuperGaussian
on a variety of 3D inputs, which are diverse both in terms of complexity and
representation (e.g., Gaussian Splats or NeRFs), and demonstrate that our
simple method significantly improves the fidelity of the final 3D models. Check
our project website for details: supergaussian.github.io

Comments:
- Check our project website for details:
  https://supergaussian.github.io

---

## Bilateral Guided Radiance Field Processing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-01 | Yuehao Wang, Chaoyi Wang, Bingchen Gong, Tianfan Xue | cs.CV | [PDF](http://arxiv.org/pdf/2406.00448v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) achieves unprecedented performance in
synthesizing novel view synthesis, utilizing multi-view consistency. When
capturing multiple inputs, image signal processing (ISP) in modern cameras will
independently enhance them, including exposure adjustment, color correction,
local tone mapping, etc. While these processings greatly improve image quality,
they often break the multi-view consistency assumption, leading to "floaters"
in the reconstructed radiance fields. To address this concern without
compromising visual aesthetics, we aim to first disentangle the enhancement by
ISP at the NeRF training stage and re-apply user-desired enhancements to the
reconstructed radiance fields at the finishing stage. Furthermore, to make the
re-applied enhancements consistent between novel views, we need to perform
imaging signal processing in 3D space (i.e. "3D ISP"). For this goal, we adopt
the bilateral grid, a locally-affine model, as a generalized representation of
ISP processing. Specifically, we optimize per-view 3D bilateral grids with
radiance fields to approximate the effects of camera pipelines for each input
view. To achieve user-adjustable 3D finishing, we propose to learn a low-rank
4D bilateral grid from a given single view edit, lifting photo enhancements to
the whole 3D scene. We demonstrate our approach can boost the visual quality of
novel view synthesis by effectively removing floaters and performing
enhancements from user retouching. The source code and our data are available
at: https://bilarfpro.github.io.

Comments:
- SIGGRAPH (ACM TOG), 2024. Project page: https://bilarfpro.github.io

---

## Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head  Capture

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-01 | X. Li, Y. Cheng, X. Ren, H. Jia, D. Xu, W. Zhu, Y. Yan | cs.CV | [PDF](http://arxiv.org/pdf/2406.00440v1){: .btn .btn-green } |

**Abstract**: 4D head capture aims to generate dynamic topological meshes and corresponding
texture maps from videos, which is widely utilized in movies and games for its
ability to simulate facial muscle movements and recover dynamic textures in
pore-squeezing. The industry often adopts the method involving multi-view
stereo and non-rigid alignment. However, this approach is prone to errors and
heavily reliant on time-consuming manual processing by artists. To simplify
this process, we propose Topo4D, a novel framework for automatic geometry and
texture generation, which optimizes densely aligned 4D heads and 8K texture
maps directly from calibrated multi-view time-series images. Specifically, we
first represent the time-series faces as a set of dynamic 3D Gaussians with
fixed topology in which the Gaussian centers are bound to the mesh vertices.
Afterward, we perform alternative geometry and texture optimization
frame-by-frame for high-quality geometry and texture learning while maintaining
temporal topology stability. Finally, we can extract dynamic facial meshes in
regular wiring arrangement and high-fidelity textures with pore-level details
from the learned Gaussians. Extensive experiments show that our method achieves
superior results than the current SOTA face reconstruction methods both in the
quality of meshes and textures. Project page:
https://xuanchenli.github.io/Topo4D/.



---

## MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular  Videos

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-01 | Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lv, Peng Wang, Wenping Wang, Junhui Hou | cs.CV | [PDF](http://arxiv.org/pdf/2406.00434v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose MoDGS, a new pipeline to render novel-view images
in dynamic scenes using only casually captured monocular videos. Previous
monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid
movement of input cameras to construct multiview consistency but fail to
reconstruct dynamic scenes on casually captured input videos whose cameras are
static or move slowly. To address this challenging task, MoDGS adopts recent
single-view depth estimation methods to guide the learning of the dynamic
scene. Then, a novel 3D-aware initialization method is proposed to learn a
reasonable deformation field and a new robust depth loss is proposed to guide
the learning of dynamic scene geometry. Comprehensive experiments demonstrate
that MoDGS is able to render high-quality novel view images of dynamic scenes
from just a casually captured monocular video, which outperforms baseline
methods by a significant margin.


