---
layout: default
title: June 2024
parent: Papers
nav_order: 202406
---

<!---metadata--->


## Multiplane Prior Guided Few-Shot Aerial Scene Rendering

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-07 | Zihan Gao, Licheng Jiao, Lingling Li, Xu Liu, Fang Liu, Puhua Chen, Yuwei Guo | cs.CV | [PDF](http://arxiv.org/pdf/2406.04961v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have been successfully applied in various
aerial scenes, yet they face challenges with sparse views due to limited
supervision. The acquisition of dense aerial views is often prohibitive, as
unmanned aerial vehicles (UAVs) may encounter constraints in perspective range
and energy constraints. In this work, we introduce Multiplane Prior guided NeRF
(MPNeRF), a novel approach tailored for few-shot aerial scene rendering-marking
a pioneering effort in this domain. Our key insight is that the intrinsic
geometric regularities specific to aerial imagery could be leveraged to enhance
NeRF in sparse aerial scenes. By investigating NeRF's and Multiplane Image
(MPI)'s behavior, we propose to guide the training process of NeRF with a
Multiplane Prior. The proposed Multiplane Prior draws upon MPI's benefits and
incorporates advanced image comprehension through a SwinV2 Transformer,
pre-trained via SimMIM. Our extensive experiments demonstrate that MPNeRF
outperforms existing state-of-the-art methods applied in non-aerial contexts,
by tripling the performance in SSIM and LPIPS even with three views available.
We hope our work offers insights into the development of NeRF-based
applications in aerial scenes with limited data.

Comments:
- 17 pages, 8 figures, accepted at CVPR 2024

---

## Multi-style Neural Radiance Field with AdaIN

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-07 | Yu-Wen Pao, An-Jie Li | cs.CV | [PDF](http://arxiv.org/pdf/2406.04960v1){: .btn .btn-green } |

**Abstract**: In this work, we propose a novel pipeline that combines AdaIN and NeRF for
the task of stylized Novel View Synthesis. Compared to previous works, we make
the following contributions: 1) We simplify the pipeline. 2) We extend the
capabilities of model to handle the multi-style task. 3) We modify the model
architecture to perform well on styles with strong brush strokes. 4) We
implement style interpolation on the multi-style model, allowing us to control
the style between any two styles and the style intensity between the stylized
output and the original scene, providing better control over the stylization
strength.



---

## Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a  Single Image


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Jo√£o F. Henriques, Christian Rupprecht, Andrea Vedaldi | cs.CV | [PDF](http://arxiv.org/pdf/2406.04343v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose Flash3D, a method for scene reconstruction and
novel view synthesis from a single image which is both very generalisable and
efficient. For generalisability, we start from a "foundation" model for
monocular depth estimation and extend it to a full 3D shape and appearance
reconstructor. For efficiency, we base this extension on feed-forward Gaussian
Splatting. Specifically, we predict a first layer of 3D Gaussians at the
predicted depth, and then add additional layers of Gaussians that are offset in
space, allowing the model to complete the reconstruction behind occlusions and
truncations. Flash3D is very efficient, trainable on a single GPU in a day, and
thus accessible to most researchers. It achieves state-of-the-art results when
trained and tested on RealEstate10k. When transferred to unseen datasets like
NYU it outperforms competitors by a large margin. More impressively, when
transferred to KITTI, Flash3D achieves better PSNR than methods trained
specifically on that dataset. In some instances, it even outperforms recent
methods that use multiple views as input. Code, models, demo, and more results
are available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.

Comments:
- Project page: https://www.robots.ox.ac.uk/~vgg/research/flash3d/

---

## Localized Gaussian Point Management

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Haosen Yang, Chenhao Zhang, Wenqing Wang, Marco Volino, Adrian Hilton, Li Zhang, Xiatian Zhu | cs.CV | [PDF](http://arxiv.org/pdf/2406.04251v1){: .btn .btn-green } |

**Abstract**: Point management is a critical component in optimizing 3D Gaussian Splatting
(3DGS) models, as the point initiation (e.g., via structure from motion) is
distributionally inappropriate. Typically, the Adaptive Density Control (ADC)
algorithm is applied, leveraging view-averaged gradient magnitude thresholding
for point densification, opacity thresholding for pruning, and regular
all-points opacity reset. However, we reveal that this strategy is limited in
tackling intricate/special image regions (e.g., transparent) as it is unable to
identify all the 3D zones that require point densification, and lacking an
appropriate mechanism to handle the ill-conditioned points with negative
impacts (occlusion due to false high opacity). To address these limitations, we
propose a Localized Point Management (LPM) strategy, capable of identifying
those error-contributing zones in the highest demand for both point addition
and geometry calibration. Zone identification is achieved by leveraging the
underlying multiview geometry constraints, with the guidance of image rendering
errors. We apply point densification in the identified zone, whilst resetting
the opacity of those points residing in front of these regions so that a new
opportunity is created to correct ill-conditioned points. Serving as a
versatile plugin, LPM can be seamlessly integrated into existing 3D Gaussian
Splatting models. Experimental evaluation across both static 3D and dynamic 4D
scenes validate the efficacy of our LPM strategy in boosting a variety of
existing 3DGS models both quantitatively and qualitatively. Notably, LPM
improves both vanilla 3DGS and SpaceTimeGS to achieve state-of-the-art
rendering quality while retaining real-time speeds, outperforming on
challenging datasets such as Tanks & Temples and the Neural 3D Video Dataset.



---

## Improving Physics-Augmented Continuum Neural Radiance Field-Based  Geometry-Agnostic System Identification with Lagrangian Particle Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Takuhiro Kaneko | cs.CV | [PDF](http://arxiv.org/pdf/2406.04155v1){: .btn .btn-green } |

**Abstract**: Geometry-agnostic system identification is a technique for identifying the
geometry and physical properties of an object from video sequences without any
geometric assumptions. Recently, physics-augmented continuum neural radiance
fields (PAC-NeRF) has demonstrated promising results for this technique by
utilizing a hybrid Eulerian-Lagrangian representation, in which the geometry is
represented by the Eulerian grid representations of NeRF, the physics is
described by a material point method (MPM), and they are connected via
Lagrangian particles. However, a notable limitation of PAC-NeRF is that its
performance is sensitive to the learning of the geometry from the first frames
owing to its two-step optimization. First, the grid representations are
optimized with the first frames of video sequences, and then the physical
properties are optimized through video sequences utilizing the fixed
first-frame grid representations. This limitation can be critical when learning
of the geometric structure is difficult, for example, in a few-shot (sparse
view) setting. To overcome this limitation, we propose Lagrangian particle
optimization (LPO), in which the positions and features of particles are
optimized through video sequences in Lagrangian space. This method allows for
the optimization of the geometric structure across the entire video sequence
within the physical constraints imposed by the MPM. The experimental results
demonstrate that the LPO is useful for geometric correction and physical
identification in sparse-view settings.

Comments:
- Accepted to CVPR 2024. Project page:
  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/lpo/

---

## How Far Can We Compress Instant-NGP-Based NeRF?

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Yihang Chen, Qianyi Wu, Mehrtash Harandi, Jianfei Cai | cs.CV | [PDF](http://arxiv.org/pdf/2406.04101v1){: .btn .btn-green } |

**Abstract**: In recent years, Neural Radiance Field (NeRF) has demonstrated remarkable
capabilities in representing 3D scenes. To expedite the rendering process,
learnable explicit representations have been introduced for combination with
implicit NeRF representation, which however results in a large storage space
requirement. In this paper, we introduce the Context-based NeRF Compression
(CNC) framework, which leverages highly efficient context models to provide a
storage-friendly NeRF representation. Specifically, we excavate both level-wise
and dimension-wise context dependencies to enable probability prediction for
information entropy reduction. Additionally, we exploit hash collision and
occupancy grids as strong prior knowledge for better context modeling. To the
best of our knowledge, we are the first to construct and exploit context models
for NeRF compression. We achieve a size reduction of 100$\times$ and 70$\times$
with improved fidelity against the baseline Instant-NGP on Synthesic-NeRF and
Tanks and Temples datasets, respectively. Additionally, we attain 86.7\% and
82.3\% storage size reduction against the SOTA NeRF compression method BiRF.
Our code is available here: https://github.com/YihangChen-ee/CNC.

Comments:
- Project Page: https://yihangchen-ee.github.io/project_cnc/ Code:
  https://github.com/yihangchen-ee/cnc/. We further propose a 3DGS compression
  method HAC, which is based on CNC:
  https://yihangchen-ee.github.io/project_hac/

---

## Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware  Spatio-Temporal Sampling

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang, Pedro Miraldo, Suhas Lohit, Moitreya Chatterjee | cs.CV | [PDF](http://arxiv.org/pdf/2406.03723v1){: .btn .btn-green } |

**Abstract**: Extensions of Neural Radiance Fields (NeRFs) to model dynamic scenes have
enabled their near photo-realistic, free-viewpoint rendering. Although these
methods have shown some potential in creating immersive experiences, two
drawbacks limit their ubiquity: (i) a significant reduction in reconstruction
quality when the computing budget is limited, and (ii) a lack of semantic
understanding of the underlying scenes. To address these issues, we introduce
Gear-NeRF, which leverages semantic information from powerful image
segmentation models. Our approach presents a principled way for learning a
spatio-temporal (4D) semantic embedding, based on which we introduce the
concept of gears to allow for stratified modeling of dynamic regions of the
scene based on the extent of their motion. Such differentiation allows us to
adjust the spatio-temporal sampling resolution for each region in proportion to
its motion scale, achieving more photo-realistic dynamic novel view synthesis.
At the same time, almost for free, our approach enables free-viewpoint tracking
of objects of interest - a functionality not yet achieved by existing
NeRF-based methods. Empirical studies validate the effectiveness of our method,
where we achieve state-of-the-art rendering and tracking performance on
multiple challenging datasets.

Comments:
- Paper accepted to IEEE/CVF CVPR 2024 (Spotlight). Work done when XL
  was an intern at MERL. Project Page Link:
  https://merl.com/research/highlights/gear-nerf

---

## Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Diwen Wan, Ruijie Lu, Gang Zeng | cs.CV | [PDF](http://arxiv.org/pdf/2406.03697v1){: .btn .btn-green } |

**Abstract**: Rendering novel view images in dynamic scenes is a crucial yet challenging
task. Current methods mainly utilize NeRF-based methods to represent the static
scene and an additional time-variant MLP to model scene deformations, resulting
in relatively low rendering quality as well as slow inference speed. To tackle
these challenges, we propose a novel framework named Superpoint Gaussian
Splatting (SP-GS). Specifically, our framework first employs explicit 3D
Gaussians to reconstruct the scene and then clusters Gaussians with similar
properties (e.g., rotation, translation, and location) into superpoints.
Empowered by these superpoints, our method manages to extend 3D Gaussian
splatting to dynamic scenes with only a slight increase in computational
expense. Apart from achieving state-of-the-art visual quality and real-time
rendering under high resolutions, the superpoint representation provides a
stronger manipulation capability. Extensive experiments demonstrate the
practicality and effectiveness of our approach on both synthetic and real-world
datasets. Please see our project page at
https://dnvtmf.github.io/SP_GS.github.io.

Comments:
- Accepted by ICML 2024

---

## A Survey on 3D Human Avatar Modeling -- From Reconstruction to  Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-06 | Ruihe Wang, Yukang Cao, Kai Han, Kwan-Yee K. Wong | cs.CV | [PDF](http://arxiv.org/pdf/2406.04253v1){: .btn .btn-green } |

**Abstract**: 3D modeling has long been an important area in computer vision and computer
graphics. Recently, thanks to the breakthroughs in neural representations and
generative models, we witnessed a rapid development of 3D modeling. 3D human
modeling, lying at the core of many real-world applications, such as gaming and
animation, has attracted significant attention. Over the past few years, a
large body of work on creating 3D human avatars has been introduced, forming a
new and abundant knowledge base for 3D human modeling. The scale of the
literature makes it difficult for individuals to keep track of all the works.
This survey aims to provide a comprehensive overview of these emerging
techniques for 3D human avatar modeling, from both reconstruction and
generation perspectives. Firstly, we review representative methods for 3D human
reconstruction, including methods based on pixel-aligned implicit function,
neural radiance field, and 3D Gaussian Splatting, etc. We then summarize
representative methods for 3D human generation, especially those using large
language models like CLIP, diffusion models, and various 3D representations,
which demonstrate state-of-the-art performance. Finally, we discuss our
reflection on existing methods and open challenges for 3D human avatar
modeling, shedding light on future research.

Comments:
- 30 pages, 21 figures

---

## Adversarial Generation of Hierarchical Gaussians for 3D Generative Model

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-05 | Sangeek Hyun, Jae-Pil Heo | cs.CV | [PDF](http://arxiv.org/pdf/2406.02968v1){: .btn .btn-green } |

**Abstract**: Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend
on ray casting-based volume rendering, which incurs demanding rendering costs.
One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS),
providing a much faster rendering speed and explicit 3D representation. In this
paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its
efficient and explicit characteristics. However, in an adversarial framework,
we observe that a na\"ive generator architecture suffers from training
instability and lacks the capability to adjust the scale of Gaussians. This
leads to model divergence and visual artifacts due to the absence of proper
guidance for initialized positions of Gaussians and densification to manage
their scales adaptively. To address these issues, we introduce a generator
architecture with a hierarchical multi-scale Gaussian representation that
effectively regularizes the position and scale of generated Gaussians.
Specifically, we design a hierarchy of Gaussians where finer-level Gaussians
are parameterized by their coarser-level counterparts; the position of
finer-level Gaussians would be located near their coarser-level counterparts,
and the scale would monotonically decrease as the level becomes finer, modeling
both coarse and fine details of the 3D scene. Experimental results demonstrate
that ours achieves a significantly faster rendering speed (x100) compared to
state-of-the-art 3D consistent GANs with comparable 3D generation capability.
Project page: https://hse1032.github.io/gsgan.

Comments:
- Project page: https://hse1032.github.io/gsgan

---

## Event3DGS: Event-based 3D Gaussian Splatting for Fast Egomotion

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-05 | Tianyi Xiong, Jiayi Wu, Botao He, Cornelia Fermuller, Yiannis Aloimonos, Heng Huang, Christopher A. Metzler | cs.CV | [PDF](http://arxiv.org/pdf/2406.02972v1){: .btn .btn-green } |

**Abstract**: The recent emergence of 3D Gaussian splatting (3DGS) leverages the advantage
of explicit point-based representations, which significantly improves the
rendering speed and quality of novel-view synthesis. However, 3D radiance field
rendering in environments with high-dynamic motion or challenging illumination
condition remains problematic in real-world robotic tasks. The reason is that
fast egomotion is prevalent real-world robotic tasks, which induces motion
blur, leading to inaccuracies and artifacts in the reconstructed structure. To
alleviate this problem, we propose Event3DGS, the first method that learns
Gaussian Splatting solely from raw event streams. By exploiting the high
temporal resolution of event cameras and explicit point-based representation,
Event3DGS can reconstruct high-fidelity 3D structures solely from the event
streams under fast egomotion. Our sparsity-aware sampling and progressive
training approaches allow for better reconstruction quality and consistency. To
further enhance the fidelity of appearance, we explicitly incorporate the
motion blur formation process into a differentiable rasterizer, which is used
with a limited set of blurred RGB images to refine the appearance. Extensive
experiments on multiple datasets validate the superior rendering quality of
Event3DGS compared with existing approaches, with over 95% lower training time
and faster rendering speed in orders of magnitude.



---

## Enhancing Temporal Consistency in Video Editing by Reconstructing Videos  with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Inkyu Shin, Qihang Yu, Xiaohui Shen, In So Kweon, Kuk-Jin Yoon, Liang-Chieh Chen | cs.CV | [PDF](http://arxiv.org/pdf/2406.02541v3){: .btn .btn-green } |

**Abstract**: Recent advancements in zero-shot video diffusion models have shown promise
for text-driven video editing, but challenges remain in achieving high temporal
consistency. To address this, we introduce Video-3DGS, a 3D Gaussian Splatting
(3DGS)-based video refiner designed to enhance temporal consistency in
zero-shot video editors. Our approach utilizes a two-stage 3D Gaussian
optimizing process tailored for editing dynamic monocular videos. In the first
stage, Video-3DGS employs an improved version of COLMAP, referred to as
MC-COLMAP, which processes original videos using a Masked and Clipped approach.
For each video clip, MC-COLMAP generates the point clouds for dynamic
foreground objects and complex backgrounds. These point clouds are utilized to
initialize two sets of 3D Gaussians (Frg-3DGS and Bkg-3DGS) aiming to represent
foreground and background views. Both foreground and background views are then
merged with a 2D learnable parameter map to reconstruct full views. In the
second stage, we leverage the reconstruction ability developed in the first
stage to impose the temporal constraints on the video diffusion model. To
demonstrate the efficacy of Video-3DGS on both stages, we conduct extensive
experiments across two related tasks: Video Reconstruction and Video Editing.
Video-3DGS trained with 3k iterations significantly improves video
reconstruction quality (+3 PSNR, +7 PSNR increase) and training efficiency
(x1.9, x4.5 times faster) over NeRF-based and 3DGS-based state-of-art methods
on DAVIS dataset, respectively. Moreover, it enhances video editing by ensuring
temporal consistency across 58 dynamic monocular videos.

Comments:
- Project page at https://video-3dgs-project.github.io/

---

## 3D-HGS: 3D Half-Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Haolin Li, Jinyang Liu, Mario Sznaier, Octavia Camps | cs.CV | [PDF](http://arxiv.org/pdf/2406.02720v1){: .btn .btn-green } |

**Abstract**: Photo-realistic 3D Reconstruction is a fundamental problem in 3D computer
vision. This domain has seen considerable advancements owing to the advent of
recent neural rendering techniques. These techniques predominantly aim to focus
on learning volumetric representations of 3D scenes and refining these
representations via loss functions derived from rendering. Among these, 3D
Gaussian Splatting (3D-GS) has emerged as a significant method, surpassing
Neural Radiance Fields (NeRFs). 3D-GS uses parameterized 3D Gaussians for
modeling both spatial locations and color information, combined with a
tile-based fast rendering technique. Despite its superior rendering performance
and speed, the use of 3D Gaussian kernels has inherent limitations in
accurately representing discontinuous functions, notably at edges and corners
for shape discontinuities, and across varying textures for color
discontinuities. To address this problem, we propose to employ 3D Half-Gaussian
(3D-HGS) kernels, which can be used as a plug-and-play kernel. Our experiments
demonstrate their capability to improve the performance of current 3D-GS
related methods and achieve state-of-the-art rendering performance on various
datasets without compromising rendering speed.

Comments:
- 9 pages, 6 figures

---

## OpenGaussian: Towards Point-Level 3D Gaussian-based Open Vocabulary  Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Jian Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2406.02058v1){: .btn .btn-green } |

**Abstract**: This paper introduces OpenGaussian, a method based on 3D Gaussian Splatting
(3DGS) capable of 3D point-level open vocabulary understanding. Our primary
motivation stems from observing that existing 3DGS-based open vocabulary
methods mainly focus on 2D pixel-level parsing. These methods struggle with 3D
point-level tasks due to weak feature expressiveness and inaccurate 2D-3D
feature associations. To ensure robust feature presentation and 3D point-level
understanding, we first employ SAM masks without cross-frame associations to
train instance features with 3D consistency. These features exhibit both
intra-object consistency and inter-object distinction. Then, we propose a
two-stage codebook to discretize these features from coarse to fine levels. At
the coarse level, we consider the positional information of 3D points to
achieve location-based clustering, which is then refined at the fine level.
Finally, we introduce an instance-level 3D-2D feature association method that
links 3D points to 2D masks, which are further associated with 2D CLIP
features. Extensive experiments, including open vocabulary-based 3D object
selection, 3D point cloud understanding, click-based 3D object selection, and
ablation studies, demonstrate the effectiveness of our proposed method. Project
page: https://3d-aigc.github.io/OpenGaussian

Comments:
- technical report, 15 pages

---

## SatSplatYOLO: 3D Gaussian Splatting-based Virtual Object Detection  Ensembles for Satellite Feature Recognition

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Van Minh Nguyen, Emma Sandidge, Trupti Mahendrakar, Ryan T. White | cs.CV | [PDF](http://arxiv.org/pdf/2406.02533v1){: .btn .btn-green } |

**Abstract**: On-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possibly unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. In this article, we present an approach for
mapping geometries and high-confidence detection of components of unknown,
non-cooperative satellites on orbit. We implement accelerated 3D Gaussian
splatting to learn a 3D representation of the satellite, render virtual views
of the target, and ensemble the YOLOv5 object detector over the virtual views,
resulting in reliable, accurate, and precise satellite component detections.
The full pipeline capable of running on-board and stand to enable downstream
machine intelligence tasks necessary for autonomous guidance, navigation, and
control tasks.



---

## FastLGS: Speeding up Language Embedded Gaussians with Feature Grid  Mapping

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Yuzhou Ji, He Zhu, Junshu Tang, Wuyi Liu, Zhizhong Zhang, Yuan Xie, Lizhuang Ma, Xin Tan | cs.CV | [PDF](http://arxiv.org/pdf/2406.01916v1){: .btn .btn-green } |

**Abstract**: The semantically interactive radiance field has always been an appealing task
for its potential to facilitate user-friendly and automated real-world 3D scene
understanding applications. However, it is a challenging task to achieve high
quality, efficiency and zero-shot ability at the same time with semantics in
radiance fields. In this work, we present FastLGS, an approach that supports
real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high
resolution. We propose the semantic feature grid to save multi-view CLIP
features which are extracted based on Segment Anything Model (SAM) masks, and
map the grids to low dimensional features for semantic field training through
3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through
feature grids from rendered features for open-vocabulary queries. Comparisons
with other state-of-the-art methods prove that FastLGS can achieve the first
place performance concerning both speed and accuracy, where FastLGS is 98x
faster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that
FastLGS is adaptive and compatible with many downstream tasks, such as 3D
segmentation and 3D object inpainting, which can be easily applied to other 3D
manipulation systems.



---

## DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume  Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Zhongpai Gao, Benjamin Planche, Meng Zheng, Xiao Chen, Terrence Chen, Ziyan Wu | cs.CV | [PDF](http://arxiv.org/pdf/2406.02518v1){: .btn .btn-green } |

**Abstract**: Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images
generated from 3D CT volumes, widely used in preoperative settings but limited
in intraoperative applications due to computational bottlenecks, especially for
accurate but heavy physics-based Monte Carlo methods. While analytical DRR
renderers offer greater efficiency, they overlook anisotropic X-ray image
formation phenomena, such as Compton scattering. We present a novel approach
that marries realistic physics-inspired X-ray simulation with efficient,
differentiable DRR generation using 3D Gaussian splatting (3DGS). Our
direction-disentangled 3DGS (DDGS) method separates the radiosity contribution
into isotropic and direction-dependent components, approximating complex
anisotropic interactions without intricate runtime simulations. Additionally,
we adapt the 3DGS initialization to account for tomography data properties,
enhancing accuracy and efficiency. Our method outperforms state-of-the-art
techniques in image accuracy. Furthermore, our DDGS shows promise for
intraoperative applications and inverse problems such as pose registration,
delivering superior registration accuracy and runtime performance compared to
analytical DRR methods.



---

## Query-based Semantic Gaussian Field for Scene Representation in  Reinforcement Learning

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Jiaxu Wang, Ziyi Zhang, Qiang Zhang, Jia Li, Jingkai Sun, Mingyuan Sun, Junhao He, Renjing Xu | cs.RO | [PDF](http://arxiv.org/pdf/2406.02370v1){: .btn .btn-green } |

**Abstract**: Latent scene representation plays a significant role in training
reinforcement learning (RL) agents. To obtain good latent vectors describing
the scenes, recent works incorporate the 3D-aware latent-conditioned NeRF
pipeline into scene representation learning. However, these NeRF-related
methods struggle to perceive 3D structural information due to the inefficient
dense sampling in volumetric rendering. Moreover, they lack fine-grained
semantic information included in their scene representation vectors because
they evenly consider free and occupied spaces. Both of them can destroy the
performance of downstream RL tasks. To address the above challenges, we propose
a novel framework that adopts the efficient 3D Gaussian Splatting (3DGS) to
learn 3D scene representation for the first time. In brief, we present the
Query-based Generalizable 3DGS to bridge the 3DGS technique and scene
representations with more geometrical awareness than those in NeRFs. Moreover,
we present the Hierarchical Semantics Encoding to ground the fine-grained
semantic features to 3D Gaussians and further distilled to the scene
representation vectors. We conduct extensive experiments on two RL platforms
including Maniskill2 and Robomimic across 10 different tasks. The results show
that our method outperforms the other 5 baselines by a large margin. We achieve
the best success rates on 8 tasks and the second-best on the other two tasks.



---

## WE-GS: An In-the-wild Efficient 3D Gaussian Representation for  Unconstrained Photo Collections

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-04 | Yuze Wang, Junyi Wang, Yue Qi | cs.CV | [PDF](http://arxiv.org/pdf/2406.02407v1){: .btn .btn-green } |

**Abstract**: Novel View Synthesis (NVS) from unconstrained photo collections is
challenging in computer graphics. Recently, 3D Gaussian Splatting (3DGS) has
shown promise for photorealistic and real-time NVS of static scenes. Building
on 3DGS, we propose an efficient point-based differentiable rendering framework
for scene reconstruction from photo collections. Our key innovation is a
residual-based spherical harmonic coefficients transfer module that adapts 3DGS
to varying lighting conditions and photometric post-processing. This
lightweight module can be pre-computed and ensures efficient gradient
propagation from rendered images to 3D Gaussian attributes. Additionally, we
observe that the appearance encoder and the transient mask predictor, the two
most critical parts of NVS from unconstrained photo collections, can be
mutually beneficial. We introduce a plug-and-play lightweight spatial attention
module to simultaneously predict transient occluders and latent appearance
representation for each image. After training and preprocessing, our method
aligns with the standard 3DGS format and rendering pipeline, facilitating
seamlessly integration into various 3DGS applications. Extensive experiments on
diverse datasets show our approach outperforms existing approaches on the
rendering quality of novel view and appearance synthesis with high converge and
rendering speed.

Comments:
- Our project page is available at
  https://yuzewang1998.github.io/we-gs.github.io/

---

## DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with  Video Diffusion Priors


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Tianyu Huang, Yihan Zeng, Hui Li, Wangmeng Zuo, Rynson W. H. Lau | cs.CV | [PDF](http://arxiv.org/pdf/2406.01476v1){: .btn .btn-green } |

**Abstract**: Dynamic 3D interaction has witnessed great interest in recent works, while
creating such 4D content remains challenging. One solution is to animate 3D
scenes with physics-based simulation, and the other is to learn the deformation
of static 3D objects with the distillation of video generative models. The
former one requires assigning precise physical properties to the target object,
otherwise the simulated results would become unnatural. The latter tends to
formulate the video with minor motions and discontinuous frames, due to the
absence of physical constraints in deformation learning. We think that video
generative models are trained with real-world captured data, capable of judging
physical phenomenon in simulation environments. To this end, we propose
DreamPhysics in this work, which estimates physical properties of 3D Gaussian
Splatting with video diffusion priors. DreamPhysics supports both image- and
text-conditioned guidance, optimizing physical parameters via score
distillation sampling with frame interpolation and log gradient. Based on a
material point method simulator with proper physical parameters, our method can
generate 4D content with realistic motions. Experimental results demonstrate
that, by distilling the prior knowledge of video diffusion models, inaccurate
physical properties can be gradually refined for high-quality simulation. Codes
are released at: https://github.com/tyhuang0428/DreamPhysics.

Comments:
- Technical report. Codes are released at:
  https://github.com/tyhuang0428/DreamPhysics

---

## Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Fang Li, Hao Zhang, Narendra Ahuja | cs.CV | [PDF](http://arxiv.org/pdf/2406.01042v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has significantly elevated scene reconstruction
efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance
Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS
methods, whether based on GS or NeRF, primarily rely on camera parameters
provided by COLMAP and even utilize sparse point clouds generated by COLMAP for
initialization, which lack accuracy as well are time-consuming. This sometimes
results in poor dynamic scene representation, especially in scenes with large
object movements, or extreme camera conditions e.g. small translations combined
with large rotations. Some studies simultaneously optimize the estimation of
camera parameters and scenes, supervised by additional information like depth,
optical flow, etc. obtained from off-the-shelf models. Using this unverified
information as ground truth can reduce robustness and accuracy, which does
frequently occur for long monocular videos (with e.g. > hundreds of frames). We
propose a novel approach that learns a high-fidelity 4D GS scene representation
with self-calibration of camera parameters. It includes the extraction of 2D
point features that robustly represent 3D structure, and their use for
subsequent joint optimization of camera parameters and 3D structure towards
overall 4D scene optimization. We demonstrate the accuracy and time efficiency
of our method through extensive quantitative and qualitative experimental
results on several standard benchmarks. The results show significant
improvements over state-of-the-art methods for 4D novel view synthesis. The
source code will be released soon at https://github.com/fangli333/SC-4DGS.

Comments:
- GitHub Page: https://github.com/fangli333/SC-4DGS

---

## Tetrahedron Splatting for 3D Generation

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Chun Gu, Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2406.01579v1){: .btn .btn-green } |

**Abstract**: 3D representation is essential to the significant advance of 3D generation
with 2D diffusion priors. As a flexible representation, NeRF has been first
adopted for 3D representation. With density-based volumetric rendering, it
however suffers both intensive computational overhead and inaccurate mesh
extraction. Using a signed distance field and Marching Tetrahedra, DMTet allows
for precise mesh extraction and real-time rendering but is limited in handling
large topological changes in meshes, leading to optimization challenges.
Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and
rendering efficiency while falling short in mesh extraction. In this work, we
introduce a novel 3D representation, Tetrahedron Splatting (TeT-Splatting),
that supports easy convergence during optimization, precise mesh extraction,
and real-time rendering simultaneously. This is achieved by integrating
surface-based volumetric rendering within a structured tetrahedral grid while
preserving the desired ability of precise mesh extraction, and a tile-based
differentiable tetrahedron rasterizer. Furthermore, we incorporate eikonal and
normal consistency regularization terms for the signed distance field to
improve generation quality and stability. Critically, our representation can be
trained without mesh extraction, making the optimization process easier to
converge. Our TeT-Splatting can be readily integrated in existing 3D generation
pipelines, along with polygonal mesh for texture optimization. Extensive
experiments show that our TeT-Splatting strikes a superior tradeoff among
convergence speed, render efficiency, and mesh quality as compared to previous
alternatives under varying 3D generation settings.

Comments:
- Code: https://github.com/fudan-zvg/tet-splatting

---

## RaDe-GS: Rasterizing Depth in Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, Ping Tan | cs.GR | [PDF](http://arxiv.org/pdf/2406.01467v1){: .btn .btn-green } |

**Abstract**: Gaussian Splatting (GS) has proven to be highly effective in novel view
synthesis, achieving high-quality and real-time rendering. However, its
potential for reconstructing detailed 3D shapes has not been fully explored.
Existing methods often suffer from limited shape accuracy due to the discrete
and unstructured nature of Gaussian splats, which complicates the shape
extraction. While recent techniques like 2D GS have attempted to improve shape
reconstruction, they often reformulate the Gaussian primitives in ways that
reduce both rendering quality and computational efficiency. To address these
problems, our work introduces a rasterized approach to render the depth maps
and surface normal maps of general 3D Gaussian splats. Our method not only
significantly enhances shape reconstruction accuracy but also maintains the
computational efficiency intrinsic to Gaussian Splatting. Our approach achieves
a Chamfer distance error comparable to NeuraLangelo on the DTU dataset and
similar training and rendering time as traditional Gaussian Splatting on the
Tanks & Temples dataset. Our method is a significant advancement in Gaussian
Splatting and can be directly integrated into existing Gaussian Splatting-based
methods.



---

## Reconstructing and Simulating Dynamic 3D Objects with Mesh-adsorbed  Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-03 | Shaojie Ma, Yawei Luo, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2406.01593v1){: .btn .btn-green } |

**Abstract**: 3D reconstruction and simulation, while interrelated, have distinct
objectives: reconstruction demands a flexible 3D representation adaptable to
diverse scenes, whereas simulation requires a structured representation to
model motion principles effectively. This paper introduces the Mesh-adsorbed
Gaussian Splatting (MaGS) method to resolve such a dilemma. MaGS constrains 3D
Gaussians to hover on the mesh surface, creating a mutual-adsorbed
mesh-Gaussian 3D representation that combines the rendering flexibility of 3D
Gaussians with the spatial coherence of meshes. Leveraging this representation,
we introduce a learnable Relative Deformation Field (RDF) to model the relative
displacement between the mesh and 3D Gaussians, extending traditional
mesh-driven deformation paradigms that only rely on ARAP prior, thus capturing
the motion of each 3D Gaussian more precisely. By joint optimizing meshes, 3D
Gaussians, and RDF, MaGS achieves both high rendering accuracy and realistic
deformation. Extensive experiments on the D-NeRF and NeRF-DS datasets
demonstrate that MaGS can generate competitive results in both reconstruction
and simulation.

Comments:
- Project Page: see https://wcwac.github.io/MaGS-page/

---

## SuperGaussian: Repurposing Video Models for 3D Super Resolution

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Yuan Shen, Duygu Ceylan, Paul Guerrero, Zexiang Xu, Niloy J. Mitra, Shenlong Wang, Anna Fr√ºhst√ºck | cs.CV | [PDF](http://arxiv.org/pdf/2406.00609v2){: .btn .btn-green } |

**Abstract**: We present a simple, modular, and generic method that upsamples coarse 3D
models by adding geometric and appearance details. While generative 3D models
now exist, they do not yet match the quality of their counterparts in image and
video domains. We demonstrate that it is possible to directly repurpose
existing (pretrained) video models for 3D super-resolution and thus sidestep
the problem of the shortage of large repositories of high-quality 3D training
models. We describe how to repurpose video upsampling models, which are not 3D
consistent, and combine them with 3D consolidation to produce 3D-consistent
results. As output, we produce high quality Gaussian Splat models, which are
object centric and effective. Our method is category agnostic and can be easily
incorporated into existing 3D workflows. We evaluate our proposed SuperGaussian
on a variety of 3D inputs, which are diverse both in terms of complexity and
representation (e.g., Gaussian Splats or NeRFs), and demonstrate that our
simple method significantly improves the fidelity of the final 3D models. Check
our project website for details: supergaussian.github.io

Comments:
- Check our project website for details:
  https://supergaussian.github.io

---

## Efficient Neural Light Fields (ENeLF) for Mobile Devices

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Austin Peng | cs.CV | [PDF](http://arxiv.org/pdf/2406.00598v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis (NVS) is a challenge in computer vision and graphics,
focusing on generating realistic images of a scene from unobserved camera
poses, given a limited set of authentic input images. Neural radiance fields
(NeRF) achieved impressive results in rendering quality by utilizing volumetric
rendering. However, NeRF and its variants are unsuitable for mobile devices due
to the high computational cost of volumetric rendering. Emerging research in
neural light fields (NeLF) eliminates the need for volumetric rendering by
directly learning a mapping from ray representation to pixel color. NeLF has
demonstrated its capability to achieve results similar to NeRF but requires a
more extensive, computationally intensive network that is not mobile-friendly.
Unlike existing works, this research builds upon the novel network architecture
introduced by MobileR2L and aggressively applies a compression technique
(channel-wise structure pruning) to produce a model that runs efficiently on
mobile devices with lower latency and smaller sizes, with a slight decrease in
performance.



---

## Representing Animatable Avatar via Factorized Neural Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Chunjin Song, Zhijie Wu, Bastian Wandt, Leonid Sigal, Helge Rhodin | cs.CV | [PDF](http://arxiv.org/pdf/2406.00637v1){: .btn .btn-green } |

**Abstract**: For reconstructing high-fidelity human 3D models from monocular videos, it is
crucial to maintain consistent large-scale body shapes along with finely
matched subtle wrinkles. This paper explores the observation that the per-frame
rendering results can be factorized into a pose-independent component and a
corresponding pose-dependent equivalent to facilitate frame consistency. Pose
adaptive textures can be further improved by restricting frequency bands of
these two components. In detail, pose-independent outputs are expected to be
low-frequency, while highfrequency information is linked to pose-dependent
factors. We achieve a coherent preservation of both coarse body contours across
the entire input video and finegrained texture features that are time variant
with a dual-branch network with distinct frequency components. The first branch
takes coordinates in canonical space as input, while the second branch
additionally considers features outputted by the first branch and pose
information of each frame. Our network integrates the information predicted by
both branches and utilizes volume rendering to generate photo-realistic 3D
human images. Through experiments, we demonstrate that our network surpasses
the neural radiance fields (NeRF) based state-of-the-art methods in preserving
high-frequency details and ensuring consistent body contours.



---

## PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-02 | Yeonsung Jung, Heecheol Yun, Joonhyung Park, Jin-Hwa Kim, Eunho Yang | cs.CV | [PDF](http://arxiv.org/pdf/2406.00798v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown remarkable performance in learning
3D scenes. However, NeRF exhibits vulnerability when confronted with
distractors in the training images -- unexpected objects are present only
within specific views, such as moving entities like pedestrians or birds.
Excluding distractors during dataset construction is a straightforward
solution, but without prior knowledge of their types and quantities, it becomes
prohibitively expensive. In this paper, we propose PruNeRF, a segment-centric
dataset pruning framework via 3D spatial consistency, that effectively
identifies and prunes the distractors. We first examine existing metrics for
measuring pixel-wise distraction and introduce Influence Functions for more
accurate measurements. Then, we assess 3D spatial consistency using a
depth-based reprojection technique to obtain 3D-aware distraction. Furthermore,
we incorporate segmentation for pixel-to-segment refinement, enabling more
precise identification. Our experiments on benchmark datasets demonstrate that
PruNeRF consistently outperforms state-of-the-art methods in robustness against
distractors.



---

## MoDGS: Dynamic Gaussian Splatting from Causually-captured Monocular  Videos

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-01 | Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lv, Peng Wang, Wenping Wang, Junhui Hou | cs.CV | [PDF](http://arxiv.org/pdf/2406.00434v1){: .btn .btn-green } |

**Abstract**: In this paper, we propose MoDGS, a new pipeline to render novel-view images
in dynamic scenes using only casually captured monocular videos. Previous
monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid
movement of input cameras to construct multiview consistency but fail to
reconstruct dynamic scenes on casually captured input videos whose cameras are
static or move slowly. To address this challenging task, MoDGS adopts recent
single-view depth estimation methods to guide the learning of the dynamic
scene. Then, a novel 3D-aware initialization method is proposed to learn a
reasonable deformation field and a new robust depth loss is proposed to guide
the learning of dynamic scene geometry. Comprehensive experiments demonstrate
that MoDGS is able to render high-quality novel view images of dynamic scenes
from just a casually captured monocular video, which outperforms baseline
methods by a significant margin.



---

## Bilateral Guided Radiance Field Processing

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-01 | Yuehao Wang, Chaoyi Wang, Bingchen Gong, Tianfan Xue | cs.CV | [PDF](http://arxiv.org/pdf/2406.00448v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) achieves unprecedented performance in
synthesizing novel view synthesis, utilizing multi-view consistency. When
capturing multiple inputs, image signal processing (ISP) in modern cameras will
independently enhance them, including exposure adjustment, color correction,
local tone mapping, etc. While these processings greatly improve image quality,
they often break the multi-view consistency assumption, leading to "floaters"
in the reconstructed radiance fields. To address this concern without
compromising visual aesthetics, we aim to first disentangle the enhancement by
ISP at the NeRF training stage and re-apply user-desired enhancements to the
reconstructed radiance fields at the finishing stage. Furthermore, to make the
re-applied enhancements consistent between novel views, we need to perform
imaging signal processing in 3D space (i.e. "3D ISP"). For this goal, we adopt
the bilateral grid, a locally-affine model, as a generalized representation of
ISP processing. Specifically, we optimize per-view 3D bilateral grids with
radiance fields to approximate the effects of camera pipelines for each input
view. To achieve user-adjustable 3D finishing, we propose to learn a low-rank
4D bilateral grid from a given single view edit, lifting photo enhancements to
the whole 3D scene. We demonstrate our approach can boost the visual quality of
novel view synthesis by effectively removing floaters and performing
enhancements from user retouching. The source code and our data are available
at: https://bilarfpro.github.io.

Comments:
- SIGGRAPH (ACM TOG), 2024. Project page: https://bilarfpro.github.io

---

## Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head  Capture

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-06-01 | X. Li, Y. Cheng, X. Ren, H. Jia, D. Xu, W. Zhu, Y. Yan | cs.CV | [PDF](http://arxiv.org/pdf/2406.00440v1){: .btn .btn-green } |

**Abstract**: 4D head capture aims to generate dynamic topological meshes and corresponding
texture maps from videos, which is widely utilized in movies and games for its
ability to simulate facial muscle movements and recover dynamic textures in
pore-squeezing. The industry often adopts the method involving multi-view
stereo and non-rigid alignment. However, this approach is prone to errors and
heavily reliant on time-consuming manual processing by artists. To simplify
this process, we propose Topo4D, a novel framework for automatic geometry and
texture generation, which optimizes densely aligned 4D heads and 8K texture
maps directly from calibrated multi-view time-series images. Specifically, we
first represent the time-series faces as a set of dynamic 3D Gaussians with
fixed topology in which the Gaussian centers are bound to the mesh vertices.
Afterward, we perform alternative geometry and texture optimization
frame-by-frame for high-quality geometry and texture learning while maintaining
temporal topology stability. Finally, we can extract dynamic facial meshes in
regular wiring arrangement and high-fidelity textures with pore-level details
from the learned Gaussians. Extensive experiments show that our method achieves
superior results than the current SOTA face reconstruction methods both in the
quality of meshes and textures. Project page:
https://xuanchenli.github.io/Topo4D/.


