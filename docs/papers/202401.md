---
layout: default
title: January 2024
parent: Papers
nav_order: 202401
---

<!---metadata--->


## Physical Priors Augmented Event-Based 3D Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-30 | Jiaxu Wang, Junhao He, Ziyi Zhang, Renjing Xu | cs.RO | [PDF](http://arxiv.org/pdf/2401.17121v1){: .btn .btn-green } |

**Abstract**: 3D neural implicit representations play a significant component in many
robotic applications. However, reconstructing neural radiance fields (NeRF)
from realistic event data remains a challenge due to the sparsities and the
lack of information when only event streams are available. In this paper, we
utilize motion, geometry, and density priors behind event data to impose strong
physical constraints to augment NeRF training. The proposed novel pipeline can
directly benefit from those priors to reconstruct 3D scenes without additional
inputs. Moreover, we present a novel density-guided patch-based sampling
strategy for robust and efficient learning, which not only accelerates training
procedures but also conduces to expressions of local geometries. More
importantly, we establish the first large dataset for event-based 3D
reconstruction, which contains 101 objects with various materials and
geometries, along with the groundtruth of images and depth maps for all camera
viewpoints, which significantly facilitates other research in the related
fields. The code and dataset will be publicly available at
https://github.com/Mercerai/PAEv3d.

Comments:
- 6 pages, 6 figures, ICRA 2024

---

## VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System  in Virtual Reality

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-30 | Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, Chenfanfu Jiang | cs.HC | [PDF](http://arxiv.org/pdf/2401.16663v1){: .btn .btn-green } |

**Abstract**: As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain
momentum, there's a growing focus on the development of engagements with 3D
virtual content. Unfortunately, traditional techniques for content creation,
editing, and interaction within these virtual spaces are fraught with
difficulties. They tend to be not only engineering-intensive but also require
extensive expertise, which adds to the frustration and inefficiency in virtual
object manipulation. Our proposed VR-GS system represents a leap forward in
human-centered 3D content interaction, offering a seamless and intuitive user
experience. By developing a physical dynamics-aware interactive Gaussian
Splatting in a Virtual Reality setting, and constructing a highly efficient
two-level embedding strategy alongside deformable body simulations, VR-GS
ensures real-time execution with highly realistic dynamic responses. The
components of our Virtual Reality system are designed for high efficiency and
effectiveness, starting from detailed scene reconstruction and object
segmentation, advancing through multi-view image in-painting, and extending to
interactive physics-based editing. The system also incorporates real-time
deformation embedding and dynamic shadow casting, ensuring a comprehensive and
engaging virtual experience.Our project page is available at:
https://yingjiang96.github.io/VR-GS/.



---

## Endo-4DGS: Distilling Depth Ranking for Endoscopic Monocular Scene  Reconstruction with 4D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-29 | Yiming Huang, Beilei Cui, Long Bai, Ziqi Guo, Mengya Xu, Hongliang Ren | cs.CV | [PDF](http://arxiv.org/pdf/2401.16416v1){: .btn .btn-green } |

**Abstract**: In the realm of robot-assisted minimally invasive surgery, dynamic scene
reconstruction can significantly enhance downstream tasks and improve surgical
outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to
prominence for their exceptional ability to reconstruct scenes. Nonetheless,
these methods are hampered by slow inference, prolonged training, and
substantial computational demands. Additionally, some rely on stereo depth
estimation, which is often infeasible due to the high costs and logistical
challenges associated with stereo cameras. Moreover, the monocular
reconstruction quality for deformable scenes is currently inadequate. To
overcome these obstacles, we present Endo-4DGS, an innovative, real-time
endoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting
(GS) and requires no ground truth depth data. This method extends 3D GS by
incorporating a temporal component and leverages a lightweight MLP to capture
temporal Gaussian deformations. This effectively facilitates the reconstruction
of dynamic surgical scenes with variable conditions. We also integrate
Depth-Anything to generate pseudo-depth maps from monocular views, enhancing
the depth-guided reconstruction process. Our approach has been validated on two
surgical datasets, where it has proven to render in real-time, compute
efficiently, and reconstruct with remarkable accuracy. These results underline
the vast potential of Endo-4DGS to improve surgical assistance.



---

## Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance  Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-29 | Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo, Clinton Fookes, Olivier Salvado | cs.CV | [PDF](http://arxiv.org/pdf/2401.16144v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) have exhibited potential in synthesizing
high-fidelity views of 3D scenes but the standard training paradigm of NeRF
presupposes an equal importance for each image in the training set. This
assumption poses a significant challenge for rendering specific views
presenting intricate geometries, thereby resulting in suboptimal performance.
In this paper, we take a closer look at the implications of the current
training paradigm and redesign this for more superior rendering quality by
NeRFs. Dividing input views into multiple groups based on their visual
similarities and training individual models on each of these groups enables
each model to specialize on specific regions without sacrificing speed or
efficiency. Subsequently, the knowledge of these specialized models is
aggregated into a single entity via a teacher-student distillation paradigm,
enabling spatial efficiency for online render-ing. Empirically, we evaluate our
novel training framework on two publicly available datasets, namely NeRF
synthetic and Tanks&Temples. Our evaluation demonstrates that our DaC training
pipeline enhances the rendering quality of a state-of-the-art baseline model
while exhibiting convergence to a superior minimum.



---

## Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-27 | Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang | cs.GR | [PDF](http://arxiv.org/pdf/2401.15318v1){: .btn .btn-green } |

**Abstract**: We demonstrate the feasibility of integrating physics-based animations of
solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in
virtual scenes reconstructed using 3DGS. Leveraging the coherence of the
Gaussian splatting and position-based dynamics (PBD) in the underlying
representation, we manage rendering, view synthesis, and the dynamics of solids
and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each
Gaussian kernel with an added normal, aligning the kernel's orientation with
the surface normal to refine the PBD simulation. This approach effectively
eliminates spiky noises that arise from rotational deformation in solids. It
also allows us to integrate physically based rendering to augment the dynamic
surface reflections on fluids. Consequently, our framework is capable of
realistically reproducing surface highlights on dynamic fluids and facilitating
interactions between scene objects and fluids from new views. For more
information, please visit our project page at
\url{https://amysteriouscat.github.io/GaussianSplashing/}.



---

## TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And  Image-Prompts

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-26 | Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, Ying Shan | cs.CV | [PDF](http://arxiv.org/pdf/2401.14828v1){: .btn .btn-green } |

**Abstract**: Text-driven 3D scene editing has gained significant attention owing to its
convenience and user-friendliness. However, existing methods still lack
accurate control of the specified appearance and location of the editing result
due to the inherent limitations of the text description. To this end, we
propose a 3D scene editing framework, TIPEditor, that accepts both text and
image prompts and a 3D bounding box to specify the editing region. With the
image prompt, users can conveniently specify the detailed appearance/style of
the target content in complement to the text description, enabling accurate
control of the appearance. Specifically, TIP-Editor employs a stepwise 2D
personalization strategy to better learn the representation of the existing
scene and the reference image, in which a localization loss is proposed to
encourage correct object placement as specified by the bounding box.
Additionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as
the 3D representation to facilitate local editing while keeping the background
unchanged. Extensive experiments have demonstrated that TIP-Editor conducts
accurate editing following the text and image prompts in the specified bounding
box region, consistently outperforming the baselines in editing quality, and
the alignment to the prompts, qualitatively and quantitatively.

Comments:
- Under review

---

## 3D Reconstruction and New View Synthesis of Indoor Environments based on  a Dual Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-26 | Zhenyu Bao, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu, Qing Li, Guoping Qiu | cs.CV | [PDF](http://arxiv.org/pdf/2401.14726v1){: .btn .btn-green } |

**Abstract**: Simultaneously achieving 3D reconstruction and new view synthesis for indoor
environments has widespread applications but is technically very challenging.
State-of-the-art methods based on implicit neural functions can achieve
excellent 3D reconstruction results, but their performances on new view
synthesis can be unsatisfactory. The exciting development of neural radiance
field (NeRF) has revolutionized new view synthesis, however, NeRF-based models
can fail to reconstruct clean geometric surfaces. We have developed a dual
neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry
reconstruction and view rendering. Du-NeRF contains two geometric fields, one
derived from the SDF field to facilitate geometric reconstruction and the other
derived from the density field to boost new view synthesis. One of the
innovative features of Du-NeRF is that it decouples a view-independent
component from the density field and uses it as a label to supervise the
learning process of the SDF field. This reduces shape-radiance ambiguity and
enables geometry and color to benefit from each other during the learning
process. Extensive experiments demonstrate that Du-NeRF can significantly
improve the performance of novel view synthesis and 3D reconstruction for
indoor environments and it is particularly effective in constructing areas
containing fine geometries that do not obey multi-view color consistency.

Comments:
- 20 pages, 8 figures

---

## Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-25 | Minglin Chen, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo | cs.CV | [PDF](http://arxiv.org/pdf/2401.14257v2){: .btn .btn-green } |

**Abstract**: Recently, text-to-3D approaches have achieved high-fidelity 3D content
generation using text description. However, the generated objects are
stochastic and lack fine-grained control. Sketches provide a cheap approach to
introduce such fine-grained control. Nevertheless, it is challenging to achieve
flexible control from these sketches due to their abstraction and ambiguity. In
this paper, we present a multi-view sketch-guided text-to-3D generation
framework (namely, Sketch2NeRF) to add sketch control to 3D generation.
Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable
Diffusion and ControlNet) to supervise the optimization of a 3D scene
represented by a neural radiance field (NeRF). We propose a novel synchronized
generation and reconstruction method to effectively optimize the NeRF. In the
experiments, we collected two kinds of multi-view sketch datasets to evaluate
the proposed method. We demonstrate that our method can synthesize 3D
consistent contents with fine-grained sketch control while being high-fidelity
to text prompts. Extensive results show that our method achieves
state-of-the-art performance in terms of sketch similarity and text alignment.

Comments:
- 11 pages, 9 figures

---

## Learning Robust Generalizable Radiance Field with Visibility and Feature  Augmented Point Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-25 | Jiaxu Wang, Ziyi Zhang, Renjing Xu | cs.CV | [PDF](http://arxiv.org/pdf/2401.14354v1){: .btn .btn-green } |

**Abstract**: This paper introduces a novel paradigm for the generalizable neural radiance
field (NeRF). Previous generic NeRF methods combine multiview stereo techniques
with image-based neural rendering for generalization, yielding impressive
results, while suffering from three issues. First, occlusions often result in
inconsistent feature matching. Then, they deliver distortions and artifacts in
geometric discontinuities and locally sharp shapes due to their individual
process of sampled points and rough feature aggregation. Third, their
image-based representations experience severe degradations when source views
are not near enough to the target view. To address challenges, we propose the
first paradigm that constructs the generalizable neural field based on
point-based rather than image-based rendering, which we call the Generalizable
neural Point Field (GPF). Our approach explicitly models visibilities by
geometric priors and augments them with neural features. We propose a novel
nonuniform log sampling strategy to improve both rendering speed and
reconstruction quality. Moreover, we present a learnable kernel spatially
augmented with features for feature aggregations, mitigating distortions at
places with drastically varying geometries. Besides, our representation can be
easily manipulated. Experiments show that our model can deliver better
geometries, view consistencies, and rendering quality than all counterparts and
benchmarks on three datasets in both generalization and finetuning settings,
preliminarily proving the potential of the new paradigm for generalizable NeRF.

Comments:
- International Conference on Learning Representations 2024

---

## GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D  Reconstruction Dataset Using Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-25 | Butian Xiong, Zhuo Li, Zhen Li | cs.CV | [PDF](http://arxiv.org/pdf/2401.14032v1){: .btn .btn-green } |

**Abstract**: We introduce a novel large-scale scene reconstruction benchmark using the
newly developed 3D representation approach, Gaussian Splatting, on our
expansive U-Scene dataset. U-Scene encompasses over one and a half square
kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground
truth. For data acquisition, we employed the Matrix 300 drone equipped with the
high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This
dataset, offers a unique blend of urban and academic environments for advanced
spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with
Gaussian Splatting includes a detailed analysis across various novel
viewpoints. We also juxtapose these results with those derived from our
accurate point cloud dataset, highlighting significant differences that
underscore the importance of combine multi-modal information

Comments:
- IJCAI2024 submit, 8 pages

---

## EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable  Endoscopic Tissues Reconstruction

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-24 | Yangsen Chen, Hao Wang | cs.CV | [PDF](http://arxiv.org/pdf/2401.13352v1){: .btn .btn-green } |

**Abstract**: The accurate 3D reconstruction of deformable soft body tissues from
endoscopic videos is a pivotal challenge in medical applications such as VR
surgery and medical image analysis. Existing methods often struggle with
accuracy and the ambiguity of hallucinated tissue parts, limiting their
practical utility. In this work, we introduce EndoGaussians, a novel approach
that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This
method marks the first use of Gaussian Splatting in this context, overcoming
the limitations of previous NeRF-based techniques. Our method sets new
state-of-the-art standards, as demonstrated by quantitative assessments on
various endoscope datasets. These advancements make our method a promising tool
for medical professionals, offering more reliable and efficient 3D
reconstructions for practical applications in the medical field.



---

## Methods and strategies for improving the novel view synthesis quality of  neural radiation field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-23 | Shun Fang, Ming Cui, Xing Feng, Yanna Lv | cs.CV | [PDF](http://arxiv.org/pdf/2401.12451v1){: .btn .btn-green } |

**Abstract**: Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a
scene from 2D images and synthesize realistic novel view images. This
technology has received widespread attention from the industry and has good
application prospects. In response to the problem that the rendering quality of
NeRF images needs to be improved, many researchers have proposed various
methods to improve the rendering quality in the past three years. The latest
relevant papers are classified and reviewed, the technical principles behind
quality improvement are analyzed, and the future evolution direction of quality
improvement methods is discussed. This study can help researchers quickly
understand the current state and evolutionary context of technology in this
field, which is helpful in inspiring the development of more efficient
algorithms and promoting the application of NeRF technology in related fields.



---

## Exploration and Improvement of Nerf-based 3D Scene Editing Techniques

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-23 | Shun Fang, Ming Cui, Xing Feng, Yanan Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2401.12456v1){: .btn .btn-green } |

**Abstract**: NeRF's high-quality scene synthesis capability was quickly accepted by
scholars in the years after it was proposed, and significant progress has been
made in 3D scene representation and synthesis. However, the high computational
cost limits intuitive and efficient editing of scenes, making NeRF's
development in the scene editing field facing many challenges. This paper
reviews the preliminary explorations of scholars on NeRF in the scene or object
editing field in recent years, mainly changing the shape and texture of scenes
or objects in new synthesized scenes; through the combination of residual
models such as GaN and Transformer with NeRF, the generalization ability of
NeRF scene editing has been further expanded, including realizing real-time new
perspective editing feedback, multimodal editing of text synthesized 3D scenes,
4D synthesis performance, and in-depth exploration in light and shadow editing,
initially achieving optimization of indirect touch editing and detail
representation in complex scenes. Currently, most NeRF editing methods focus on
the touch points and materials of indirect points, but when dealing with more
complex or larger 3D scenes, it is difficult to balance accuracy, breadth,
efficiency, and quality. Overcoming these challenges may become the direction
of future NeRF 3D scene editing technology.



---

## PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar  Animation with 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-23 | Zhongyuan Zhao, Zhenyu Bao, Qing Li, Guoping Qiu, Kanglin Liu | cs.GR | [PDF](http://arxiv.org/pdf/2401.12900v4){: .btn .btn-green } |

**Abstract**: Despite much progress, achieving real-time high-fidelity head avatar
animation is still difficult and existing methods have to trade-off between
speed and quality. 3DMM based methods often fail to model non-facial structures
such as eyeglasses and hairstyles, while neural implicit models suffer from
deformation inflexibility and rendering inefficiency. Although 3D Gaussian has
been demonstrated to possess promising capability for geometry representation
and radiance field reconstruction, applying 3D Gaussian in head avatar creation
remains a major challenge since it is difficult for 3D Gaussian to model the
head shape variations caused by changing poses and expressions. In this paper,
we introduce PSAvatar, a novel framework for animatable head avatar creation
that utilizes discrete geometric primitive to create a parametric morphable
shape model and employs 3D Gaussian for fine detail representation and high
fidelity rendering. The parametric morphable shape model is a Point-based
Morphable Shape Model (PMSM) which uses points instead of meshes for 3D
representation to achieve enhanced representation flexibility. The PMSM first
converts the FLAME mesh to points by sampling on the surfaces as well as off
the meshes to enable the reconstruction of not only surface-like structures but
also complex geometries such as eyeglasses and hairstyles. By aligning these
points with the head shape in an analysis-by-synthesis manner, the PMSM makes
it possible to utilize 3D Gaussian for fine detail representation and
appearance modeling, thus enabling the creation of high-fidelity avatars. We
show that PSAvatar can reconstruct high-fidelity head avatars of a variety of
subjects and the avatars can be animated in real-time ($\ge$ 25 fps at a
resolution of 512 $\times$ 512 ).

Comments:
- 13 pages, 10 figures

---

## EndoGaussian: Gaussian Splatting for Deformable Surgical Scene  Reconstruction

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-23 | Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2401.12561v1){: .btn .btn-green } |

**Abstract**: Reconstructing deformable tissues from endoscopic stereo videos is essential
in many downstream surgical applications. However, existing methods suffer from
slow inference speed, which greatly limits their practical use. In this paper,
we introduce EndoGaussian, a real-time surgical scene reconstruction framework
that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical
scenes as canonical Gaussians and a time-dependent deformation field, which
predicts Gaussian deformations at novel timestamps. Due to the efficient
Gaussian representation and parallel rendering pipeline, our framework
significantly accelerates the rendering speed compared to previous methods. In
addition, we design the deformation field as the combination of a lightweight
encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian
tracking with a minor rendering burden. Furthermore, we design a holistic
Gaussian initialization method to fully leverage the surface distribution
prior, achieved by searching informative points from across the input image
sequence. Experiments on public endoscope datasets demonstrate that our method
can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain)
while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and
the fastest training speed (within 2 min/scene), showing significant promise
for intraoperative surgery applications. Code is available at:
\url{https://yifliu3.github.io/EndoGaussian/}.



---

## NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for  Talking Face Synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-23 | Chongke Bi, Xiaoxing Liu, Zhilei Liu | cs.CV | [PDF](http://arxiv.org/pdf/2401.12568v1){: .btn .btn-green } |

**Abstract**: Talking face synthesis driven by audio is one of the current research
hotspots in the fields of multidimensional signal processing and multimedia.
Neural Radiance Field (NeRF) has recently been brought to this research field
in order to enhance the realism and 3D effect of the generated faces. However,
most existing NeRF-based methods either burden NeRF with complex learning tasks
while lacking methods for supervised multimodal feature fusion, or cannot
precisely map audio to the facial region related to speech movements. These
reasons ultimately result in existing methods generating inaccurate lip shapes.
This paper moves a portion of NeRF learning tasks ahead and proposes a talking
face synthesis method via NeRF with attention-based disentanglement (NeRF-AD).
In particular, an Attention-based Disentanglement module is introduced to
disentangle the face into Audio-face and Identity-face using speech-related
facial action unit (AU) information. To precisely regulate how audio affects
the talking face, we only fuse the Audio-face with audio feature. In addition,
AU information is also utilized to supervise the fusion of these two
modalities. Extensive qualitative and quantitative experiments demonstrate that
our NeRF-AD outperforms state-of-the-art methods in generating realistic
talking face videos, including image quality and lip synchronization. To view
video results, please refer to https://xiaoxingliu02.github.io/NeRF-AD.

Comments:
- Accepted by ICASSP 2024

---

## Scaling Face Interaction Graph Networks to Real World Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-22 | Tatiana Lopez-Guevara, Yulia Rubanova, William F. Whitney, Tobias Pfaff, Kimberly Stachenfeld, Kelsey R. Allen | cs.LG | [PDF](http://arxiv.org/pdf/2401.11985v1){: .btn .btn-green } |

**Abstract**: Accurately simulating real world object dynamics is essential for various
applications such as robotics, engineering, graphics, and design. To better
capture complex real dynamics such as contact and friction, learned simulators
based on graph networks have recently shown great promise. However, applying
these learned simulators to real scenes comes with two major challenges: first,
scaling learned simulators to handle the complexity of real world scenes which
can involve hundreds of objects each with complicated 3D shapes, and second,
handling inputs from perception rather than 3D state information. Here we
introduce a method which substantially reduces the memory required to run
graph-based learned simulators. Based on this memory-efficient simulation
model, we then present a perceptual interface in the form of editable NeRFs
which can convert real-world scenes into a structured representation that can
be processed by graph network simulator. We show that our method uses
substantially less memory than previous graph-based simulators while retaining
their accuracy, and that the simulators learned in synthetic environments can
be applied to real world scenes captured from multiple camera angles. This
paves the way for expanding the application of learned simulators to settings
where only perceptual information is available at inference time.

Comments:
- 16 pages, 12 figures

---

## HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided  Neural Radiance Fields for Sparse View Inputs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-22 | Zelin Gao, Weichen Dai, Yu Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2401.11711v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have garnered considerable attention as a
paradigm for novel view synthesis by learning scene representations from
discrete observations. Nevertheless, NeRF exhibit pronounced performance
degradation when confronted with sparse view inputs, consequently curtailing
its further applicability. In this work, we introduce Hierarchical Geometric,
Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can
address the aforementioned limitation and enhance consistency of geometry,
semantic content, and appearance across different views. We propose
Hierarchical Geometric Guidance (HGG) to incorporate the attachment of
Structure from Motion (SfM), namely sparse depth prior, into the scene
representations. Different from direct depth supervision, HGG samples volume
points from local-to-global geometric regions, mitigating the misalignment
caused by inherent bias in the depth prior. Furthermore, we draw inspiration
from notable variations in semantic consistency observed across images of
different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn
the coarse-to-fine semantic content, which corresponds to the coarse-to-fine
scene representations. Experimental results demonstrate that HG3-NeRF can
outperform other state-of-the-art methods on different standard benchmarks and
achieve high-fidelity synthesis results for sparse view inputs.

Comments:
- 13 pages, 6 figures

---

## Single-View 3D Human Digitalization with Large Reconstruction Models

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-22 | Zhenzhen Weng, Jingyuan Liu, Hao Tan, Zhan Xu, Yang Zhou, Serena Yeung-Levy, Jimei Yang | cs.CV | [PDF](http://arxiv.org/pdf/2401.12175v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Human-LRM, a single-stage feed-forward Large
Reconstruction Model designed to predict human Neural Radiance Fields (NeRF)
from a single image. Our approach demonstrates remarkable adaptability in
training using extensive datasets containing 3D scans and multi-view capture.
Furthermore, to enhance the model's applicability for in-the-wild scenarios
especially with occlusions, we propose a novel strategy that distills
multi-view reconstruction into single-view via a conditional triplane diffusion
model. This generative extension addresses the inherent variations in human
body shapes when observed from a single view, and makes it possible to
reconstruct the full body human from an occluded image. Through extensive
experiments, we show that Human-LRM surpasses previous methods by a significant
margin on several benchmarks.



---

## Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-21 | Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu | cs.CV | [PDF](http://arxiv.org/pdf/2401.11535v1){: .btn .btn-green } |

**Abstract**: Surgical 3D reconstruction is a critical area of research in robotic surgery,
with recent works adopting variants of dynamic radiance fields to achieve
success in 3D reconstruction of deformable tissues from single-viewpoint
videos. However, these methods often suffer from time-consuming optimization or
inferior quality, limiting their adoption in downstream tasks. Inspired by 3D
Gaussian Splatting, a recent trending 3D representation, we present EndoGS,
applying Gaussian Splatting for deformable endoscopic tissue reconstruction.
Specifically, our approach incorporates deformation fields to handle dynamic
scenes, depth-guided supervision to optimize 3D targets with a single
viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a
result, EndoGS reconstructs and renders high-quality deformable endoscopic
tissues from a single-viewpoint video, estimated depth maps, and labeled tool
masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS
achieves superior rendering quality. Code is available at
https://github.com/HKU-MedAI/EndoGS.

Comments:
- Work in progress. 10 pages, 4 figures

---

## GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-18 | Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen | cs.CV | [PDF](http://arxiv.org/pdf/2401.09720v2){: .btn .btn-green } |

**Abstract**: In this work, we propose a novel clothed human reconstruction method called
GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural
radiance based models, 3D Gaussian Splatting has recently demonstrated great
performance in terms of training time and rendering quality. However, applying
the static 3D Gaussian Splatting model to the dynamic human reconstruction
problem is non-trivial due to complicated non-rigid deformations and rich cloth
details. To address these challenges, our method considers explicit pose-guided
deformation to associate dynamic Gaussians across the canonical space and the
observation space, introducing a physically-based prior with regularized
transformations helps mitigate ambiguity between the two spaces. During the
training process, we further propose a pose refinement strategy to update the
pose regression for compensating the inaccurate initial estimation and a
split-with-scale mechanism to enhance the density of regressed point clouds.
The experiments validate that our method can achieve state-of-the-art
photorealistic novel-view rendering results with high-quality details for
dynamic clothed human bodies, along with explicit geometry reconstruction.



---

## ICON: Incremental CONfidence for Joint Pose and Radiance Field  Optimization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-17 | Weiyao Wang, Pierre Gleize, Hao Tang, Xingyu Chen, Kevin J Liang, Matt Feiszli | cs.CV | [PDF](http://arxiv.org/pdf/2401.08937v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View
Synthesis (NVS) given a set of 2D images. However, NeRF training requires
accurate camera pose for each input view, typically obtained by
Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax
this constraint, but they still often rely on decent initial poses which they
can refine. Here we aim at removing the requirement for pose initialization. We
present Incremental CONfidence (ICON), an optimization procedure for training
NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate
initial guess for poses. Further, ICON introduces ``confidence": an adaptive
measure of model quality used to dynamically reweight gradients. ICON relies on
high-confidence poses to learn NeRF, and high-confidence 3D structure (as
encoded by NeRF) to learn poses. We show that ICON, without prior pose
initialization, achieves superior performance in both CO3D and HO3D versus
methods which use SfM pose.



---

## IPR-NeRF: Ownership Verification meets Neural Radiance Field

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-17 | Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang | cs.CV | [PDF](http://arxiv.org/pdf/2401.09495v4){: .btn .btn-green } |

**Abstract**: Neural Radiance Field (NeRF) models have gained significant attention in the
computer vision community in the recent past with state-of-the-art visual
quality and produced impressive demonstrations. Since then, technopreneurs have
sought to leverage NeRF models into a profitable business. Therefore, NeRF
models make it worth the risk of plagiarizers illegally copying,
re-distributing, or misusing those models. This paper proposes a comprehensive
intellectual property (IP) protection framework for the NeRF model in both
black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a
diffusion-based solution is introduced to embed and extract the watermark via a
two-stage optimization process. In the white-box setting, a designated digital
signature is embedded into the weights of the NeRF model by adopting the sign
loss objective. Our extensive experiments demonstrate that not only does our
approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models,
but it is also robust against both ambiguity and removal attacks compared to
prior arts.

Comments:
- Error on result tabulation of state of the art method which might
  cause misleading to readers

---

## Fast Dynamic 3D Object Generation from a Single-view Video

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-16 | Zijie Pan, Zeyu Yang, Xiatian Zhu, Li Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2401.08742v1){: .btn .btn-green } |

**Abstract**: Generating dynamic three-dimensional (3D) object from a single-view video is
challenging due to the lack of 4D labeled data. Existing methods extend
text-to-3D pipelines by transferring off-the-shelf image generation models such
as score distillation sampling, but they are slow and expensive to scale (e.g.,
150 minutes per object) due to the need for back-propagating the
information-limited supervision signals through a large pretrained model. To
address this limitation, we propose an efficient video-to-4D object generation
framework called Efficient4D. It generates high-quality spacetime-consistent
images under different camera views, and then uses them as labeled data to
directly train a novel 4D Gaussian splatting model with explicit point cloud
geometry, enabling real-time rendering under continuous camera trajectories.
Extensive experiments on synthetic and real videos show that Efficient4D offers
a remarkable 10-fold increase in speed when compared to prior art alternatives
while preserving the same level of innovative view synthesis quality. For
example, Efficient4D takes only 14 minutes to model a dynamic object.

Comments:
- Technical report

---

## ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-16 | Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas | cs.CV | [PDF](http://arxiv.org/pdf/2401.08140v2){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) have gained popularity across various
applications. However, they face challenges in the sparse view setting, lacking
sufficient constraints from volume rendering. Reconstructing and understanding
a 3D scene from sparse and unconstrained cameras is a long-standing problem in
classical computer vision with diverse applications. While recent works have
explored NeRFs in sparse, unconstrained view scenarios, their focus has been
primarily on enhancing reconstruction and novel view synthesis. Our approach
takes a broader perspective by posing the question: "from where has each point
been seen?" -- which gates how well we can understand and reconstruct it. In
other words, we aim to determine the origin or provenance of each 3D point and
its associated information under sparse, unconstrained views. We introduce
ProvNeRF, a model that enriches a traditional NeRF representation by
incorporating per-point provenance, modeling likely source locations for each
point. We achieve this by extending implicit maximum likelihood estimation
(IMLE) for stochastic processes. Notably, our method is compatible with any
pre-trained NeRF model and the associated training camera poses. We demonstrate
that modeling per-point provenance offers several advantages, including
uncertainty estimation, criteria-based view selection, and improved novel view
synthesis, compared to state-of-the-art methods. Please visit our project page
at https://provnerf.github.io



---

## Forging Vision Foundation Models for Autonomous Driving: Challenges,  Methodologies, and Opportunities

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-16 | Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu | cs.CV | [PDF](http://arxiv.org/pdf/2401.08045v1){: .btn .btn-green } |

**Abstract**: The rise of large foundation models, trained on extensive datasets, is
revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4
showcase their adaptability by extracting intricate patterns and performing
effectively across diverse tasks, thereby serving as potent building blocks for
a wide range of AI applications. Autonomous driving, a vibrant front in AI
applications, remains challenged by the lack of dedicated vision foundation
models (VFMs). The scarcity of comprehensive training data, the need for
multi-sensor integration, and the diverse task-specific architectures pose
significant obstacles to the development of VFMs in this field. This paper
delves into the critical challenge of forging VFMs tailored specifically for
autonomous driving, while also outlining future directions. Through a
systematic analysis of over 250 papers, we dissect essential techniques for VFM
development, including data preparation, pre-training strategies, and
downstream task adaptation. Moreover, we explore key advancements such as NeRF,
diffusion models, 3D Gaussian Splatting, and world models, presenting a
comprehensive roadmap for future research. To empower researchers, we have
built and maintained https://github.com/zhanghm1995/Forge_VFM4AD, an
open-access repository constantly updated with the latest advancements in
forging VFMs for autonomous driving.

Comments:
- Github Repo: https://github.com/zhanghm1995/Forge_VFM4AD

---

## 6-DoF Grasp Pose Evaluation and Optimization via Transfer Learning from  NeRFs

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-15 | Gergely Sóti, Xi Huang, Christian Wurll, Björn Hein | cs.RO | [PDF](http://arxiv.org/pdf/2401.07935v1){: .btn .btn-green } |

**Abstract**: We address the problem of robotic grasping of known and unknown objects using
implicit behavior cloning. We train a grasp evaluation model from a small
number of demonstrations that outputs higher values for grasp candidates that
are more likely to succeed in grasping. This evaluation model serves as an
objective function, that we maximize to identify successful grasps. Key to our
approach is the utilization of learned implicit representations of visual and
geometric features derived from a pre-trained NeRF. Though trained exclusively
in a simulated environment with simplified objects and 4-DoF top-down grasps,
our evaluation model and optimization procedure demonstrate generalization to
6-DoF grasps and novel objects both in simulation and in real-world settings,
without the need for additional data. Supplementary material is available at:
https://gergely-soti.github.io/grasp



---

## GO-NeRF: Generating Virtual Objects in Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-11 | Peng Dai, Feitong Tan, Xin Yu, Yinda Zhang, Xiaojuan Qi | cs.CV | [PDF](http://arxiv.org/pdf/2401.05750v1){: .btn .btn-green } |

**Abstract**: Despite advances in 3D generation, the direct creation of 3D objects within
an existing 3D scene represented as NeRF remains underexplored. This process
requires not only high-quality 3D object generation but also seamless
composition of the generated 3D content into the existing NeRF. To this end, we
propose a new method, GO-NeRF, capable of utilizing scene context for
high-quality and harmonious 3D object generation within an existing NeRF. Our
method employs a compositional rendering formulation that allows the generated
3D objects to be seamlessly composited into the scene utilizing learned
3D-aware opacity maps without introducing unintended scene modification.
Moreover, we also develop tailored optimization objectives and training
strategies to enhance the model's ability to exploit scene context and mitigate
artifacts, such as floaters, originating from 3D object generation within a
scene. Extensive experiments on both feed-forward and $360^o$ scenes show the
superior performance of our proposed GO-NeRF in generating objects harmoniously
composited with surrounding scenes and synthesizing high-quality novel view
images. Project page at {\url{https://daipengwa.github.io/GO-NeRF/}.

Comments:
- 12 pages

---

## TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-11 | Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger | cs.CV | [PDF](http://arxiv.org/pdf/2401.06003v1){: .btn .btn-green } |

**Abstract**: Point-based radiance field rendering has demonstrated impressive results for
novel view synthesis, offering a compelling blend of rendering quality and
computational efficiency. However, also latest approaches in this domain are
not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.
2023] struggles when tasked with rendering highly detailed scenes, due to
blurring and cloudy artifacts. On the other hand, ADOP [R\"uckert et al. 2022]
can accommodate crisper images, but the neural reconstruction network decreases
performance, it grapples with temporal instability and it is unable to
effectively address large gaps in the point cloud.
  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that
combines ideas from both Gaussian Splatting and ADOP. The fundamental concept
behind our novel technique involves rasterizing points into a screen-space
image pyramid, with the selection of the pyramid layer determined by the
projected point size. This approach allows rendering arbitrarily large points
using a single trilinear write. A lightweight neural network is then used to
reconstruct a hole-free image including detail beyond splat resolution.
Importantly, our render pipeline is entirely differentiable, allowing for
automatic optimization of both point sizes and positions.
  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art
methods in terms of rendering quality while maintaining a real-time frame rate
of 60 frames per second on readily available hardware. This performance extends
to challenging scenarios, such as scenes featuring intricate geometry,
expansive landscapes, and auto-exposed footage.



---

## TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-11 | Rajaei Khatib, Raja Giryes | cs.CV | [PDF](http://arxiv.org/pdf/2401.06191v1){: .btn .btn-green } |

**Abstract**: In recent years, the neural radiance field (NeRF) model has gained popularity
due to its ability to recover complex 3D scenes. Following its success, many
approaches proposed different NeRF representations in order to further improve
both runtime and performance. One such example is Triplane, in which NeRF is
represented using three 2D feature planes. This enables easily using existing
2D neural networks in this framework, e.g., to generate the three planes.
Despite its advantage, the triplane representation lagged behind in its 3D
recovery quality compared to NeRF solutions. In this work, we propose
TriNeRFLet, a 2D wavelet-based multiscale triplane representation for NeRF,
which closes the 3D recovery performance gap and is competitive with current
state-of-the-art methods. Building upon the triplane framework, we also propose
a novel super-resolution (SR) technique that combines a diffusion model with
TriNeRFLet for improving NeRF resolution.

Comments:
- webpage link: https://rajaeekh.github.io/trinerflet-web

---

## CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with  Dual Feature Fusion

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-11 | Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan | cs.CV | [PDF](http://arxiv.org/pdf/2401.05925v2){: .btn .btn-green } |

**Abstract**: We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a
method for compact 3D-consistent scene segmentation at fast rendering speed
with only RGB images input. Previous NeRF-based segmentation methods have
relied on time-consuming neural scene optimization. While recent 3D Gaussian
Splatting has notably improved speed, existing Gaussian-based segmentation
methods struggle to produce compact masks, especially in zero-shot
segmentation. This issue probably stems from their straightforward assignment
of learnable parameters to each Gaussian, resulting in a lack of robustness
against cross-view inconsistent 2D machine-generated labels. Our method aims to
address this problem by employing Dual Feature Fusion Network as Gaussians'
segmentation field. Specifically, we first optimize 3D Gaussians under RGB
supervision. After Gaussian Locating, DINO features extracted from images are
applied through explicit unprojection, which are further incorporated with
spatial features from the efficient point cloud processing network. Feature
aggregation is utilized to fuse them in a global-to-local strategy for compact
segmentation features. Experimental results show that our model outperforms
baselines on both semantic and panoptic zero-shot segmentation task, meanwhile
consumes less than 10\% inference time compared to NeRF-based methods. Code and
more results will be available at https://David-Dou.github.io/CoSSegGaussians.

Comments:
- Correct writing details

---

## Fast High Dynamic Range Radiance Fields for Dynamic Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-11 | Guanjun Wu, Taoran Yi, Jiemin Fang, Wenyu Liu, Xinggang Wang | cs.CV | [PDF](http://arxiv.org/pdf/2401.06052v1){: .btn .btn-green } |

**Abstract**: Neural Radiances Fields (NeRF) and their extensions have shown great success
in representing 3D scenes and synthesizing novel-view images. However, most
NeRF methods take in low-dynamic-range (LDR) images, which may lose details,
especially with nonuniform illumination. Some previous NeRF methods attempt to
introduce high-dynamic-range (HDR) techniques but mainly target static scenes.
To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF
framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images
captured with various exposures. A learnable exposure mapping function is
constructed to obtain adaptive exposure values for each image. Based on the
monotonically increasing prior, a camera response function is designed for
stable learning. With the proposed model, high-quality novel-view images at any
time point can be rendered with any desired exposure. We further construct a
dataset containing multiple dynamic scenes captured with diverse exposures for
evaluation. All the datasets and code are available at
\url{https://guanjunwu.github.io/HDR-HexPlane/}.

Comments:
- 3DV 2024. Project page: https://guanjunwu.github.io/HDR-HexPlane

---

## Gaussian Shadow Casting for Neural Characters

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-11 | Luis Bolanos, Shih-Yang Su, Helge Rhodin | cs.CV | [PDF](http://arxiv.org/pdf/2401.06116v1){: .btn .btn-green } |

**Abstract**: Neural character models can now reconstruct detailed geometry and texture
from video, but they lack explicit shadows and shading, leading to artifacts
when generating novel views and poses or during relighting. It is particularly
difficult to include shadows as they are a global effect and the required
casting of secondary rays is costly. We propose a new shadow model using a
Gaussian density proxy that replaces sampling with a simple analytic formula.
It supports dynamic motion and is tailored for shadow computation, thereby
avoiding the affine projection approximation and sorting required by the
closely related Gaussian splatting. Combined with a deferred neural rendering
model, our Gaussian shadows enable Lambertian shading and shadow casting with
minimal overhead. We demonstrate improved reconstructions, with better
separation of albedo, shading, and shadows in challenging outdoor scenes with
direct sun light and hard shadows. Our method is able to optimize the light
direction without any input from the user. As a result, novel poses have fewer
shadow artifacts and relighting in novel scenes is more realistic compared to
the state-of-the-art methods, providing new ways to pose neural characters in
novel environments, increasing their applicability.

Comments:
- 14 pages, 13 figures

---

## InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-10 | Mohamad Shahbazi, Liesbeth Claessens, Michael Niemeyer, Edo Collins, Alessio Tonioni, Luc Van Gool, Federico Tombari | cs.CV | [PDF](http://arxiv.org/pdf/2401.05335v1){: .btn .btn-green } |

**Abstract**: We introduce InseRF, a novel method for generative object insertion in the
NeRF reconstructions of 3D scenes. Based on a user-provided textual description
and a 2D bounding box in a reference viewpoint, InseRF generates new objects in
3D scenes. Recently, methods for 3D scene editing have been profoundly
transformed, owing to the use of strong priors of text-to-image diffusion
models in 3D generative modeling. Existing methods are mostly effective in
editing 3D scenes via style and appearance changes or removing existing
objects. Generating new objects, however, remains a challenge for such methods,
which we address in this study. Specifically, we propose grounding the 3D
object insertion to a 2D object insertion in a reference view of the scene. The
2D edit is then lifted to 3D using a single-view object reconstruction method.
The reconstructed object is then inserted into the scene, guided by the priors
of monocular depth estimation methods. We evaluate our method on various 3D
scenes and provide an in-depth analysis of the proposed components. Our
experiments with generative insertion of objects in several 3D scenes indicate
the effectiveness of our method compared to the existing methods. InseRF is
capable of controllable and 3D-consistent object insertion without requiring
explicit 3D information as input. Please visit our project page at
https://mohamad-shahbazi.github.io/inserf.



---

## CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from  Monocular Video

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-10 | Xingyu Miao, Yang Bai, Haoran Duan, Yawen Huang, Fan Wan, Yang Long, Yefeng Zheng | cs.CV | [PDF](http://arxiv.org/pdf/2401.04861v1){: .btn .btn-green } |

**Abstract**: The goal of our work is to generate high-quality novel views from monocular
videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have
shown impressive performance by leveraging time-varying dynamic radiation
fields. However, these methods have limitations when it comes to accurately
modeling the motion of complex objects, which can lead to inaccurate and blurry
renderings of details. To address this limitation, we propose a novel approach
that builds upon a recent generalization NeRF, which aggregates nearby views
onto new viewpoints. However, such methods are typically only effective for
static scenes. To overcome this challenge, we introduce a module that operates
in both the time and frequency domains to aggregate the features of object
motion. This allows us to learn the relationship between frames and generate
higher-quality images. Our experiments demonstrate significant improvements
over state-of-the-art methods on dynamic scene datasets. Specifically, our
approach outperforms existing methods in terms of both the accuracy and visual
quality of the synthesized views.



---

## FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D  Neural Radiance Fields


| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-10 | GeonU Kim, Kim Youwang, Tae-Hyun Oh | cs.CV | [PDF](http://arxiv.org/pdf/2401.05516v1){: .btn .btn-green } |

**Abstract**: We present FPRF, a feed-forward photorealistic style transfer method for
large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with
arbitrary, multiple style reference images without additional optimization
while preserving multi-view appearance consistency. Prior arts required tedious
per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF
efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D
neural radiance field, which inherits AdaIN's feed-forward stylization
machinery, supporting arbitrary style reference images. Furthermore, FPRF
supports multi-reference stylization with the semantic correspondence matching
and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also
preserves multi-view consistency by applying semantic matching and style
transfer processes directly onto queried features in 3D space. In experiments,
we demonstrate that FPRF achieves favorable photorealistic quality 3D scene
stylization for large-scale scenes with diverse reference images. Project page:
https://kim-geonu.github.io/FPRF/

Comments:
- Project page: https://kim-geonu.github.io/FPRF/

---

## Diffusion Priors for Dynamic View Synthesis from Monocular Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-10 | Chaoyang Wang, Peiye Zhuang, Aliaksandr Siarohin, Junli Cao, Guocheng Qian, Hsin-Ying Lee, Sergey Tulyakov | cs.CV | [PDF](http://arxiv.org/pdf/2401.05583v1){: .btn .btn-green } |

**Abstract**: Dynamic novel view synthesis aims to capture the temporal evolution of visual
content within videos. Existing methods struggle to distinguishing between
motion and structure, particularly in scenarios where camera poses are either
unknown or constrained compared to object motion. Furthermore, with information
solely from reference images, it is extremely challenging to hallucinate unseen
regions that are occluded or partially observed in the given videos. To address
these issues, we first finetune a pretrained RGB-D diffusion model on the video
frames using a customization technique. Subsequently, we distill the knowledge
from the finetuned model to a 4D representations encompassing both dynamic and
static Neural Radiance Fields (NeRF) components. The proposed pipeline achieves
geometric consistency while preserving the scene identity. We perform thorough
experiments to evaluate the efficacy of the proposed method qualitatively and
quantitatively. Our results demonstrate the robustness and utility of our
approach in challenging cases, further advancing dynamic novel view synthesis.



---

## NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-08 | Casimir Feldmann, Niall Siegenheim, Nikolas Hars, Lovro Rabuzin, Mert Ertugrul, Luca Wolfart, Marc Pollefeys, Zuria Bauer, Martin R. Oswald | cs.CV | [PDF](http://arxiv.org/pdf/2401.03771v1){: .btn .btn-green } |

**Abstract**: The capabilities of monocular depth estimation (MDE) models are limited by
the availability of sufficient and diverse datasets. In the case of MDE models
for autonomous driving, this issue is exacerbated by the linearity of the
captured data trajectories. We propose a NeRF-based data augmentation pipeline
to introduce synthetic data with more diverse viewing directions into training
datasets and demonstrate the benefits of our approach to model performance and
robustness. Our data augmentation pipeline, which we call "NeRFmentation",
trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on
relevant metrics, and uses them to generate synthetic RGB-D images captured
from new viewing directions. In this work, we apply our technique in
conjunction with three state-of-the-art MDE architectures on the popular
autonomous driving dataset KITTI, augmenting its training set of the Eigen
split. We evaluate the resulting performance gain on the original test set, a
separate popular driving set, and our own synthetic test set.



---

## AGG: Amortized Generative 3D Gaussians for Single Image to 3D

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-08 | Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat | cs.CV | [PDF](http://arxiv.org/pdf/2401.04099v1){: .btn .btn-green } |

**Abstract**: Given the growing need for automatic 3D content creation pipelines, various
3D representations have been studied to generate 3D objects from a single
image. Due to its superior rendering efficiency, 3D Gaussian splatting-based
models have recently excelled in both 3D reconstruction and generation. 3D
Gaussian splatting approaches for image to 3D generation are often
optimization-based, requiring many computationally expensive score-distillation
steps. To overcome these challenges, we introduce an Amortized Generative 3D
Gaussian framework (AGG) that instantly produces 3D Gaussians from a single
image, eliminating the need for per-instance optimization. Utilizing an
intermediate hybrid representation, AGG decomposes the generation of 3D
Gaussian locations and other appearance attributes for joint optimization.
Moreover, we propose a cascaded pipeline that first generates a coarse
representation of the 3D data and later upsamples it with a 3D Gaussian
super-resolution module. Our method is evaluated against existing
optimization-based 3D Gaussian frameworks and sampling-based pipelines
utilizing other 3D representations, where AGG showcases competitive generation
abilities both qualitatively and quantitatively while being several orders of
magnitude faster. Project page: https://ir1d.github.io/AGG/

Comments:
- Project page: https://ir1d.github.io/AGG/

---

## A Survey on 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-08 | Guikun Chen, Wenguan Wang | cs.CV | [PDF](http://arxiv.org/pdf/2401.03890v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian splatting (3D GS) has recently emerged as a transformative
technique in the explicit radiance field and computer graphics landscape. This
innovative approach, characterized by the utilization of millions of 3D
Gaussians, represents a significant departure from the neural radiance field
(NeRF) methodologies, which predominantly use implicit, coordinate-based models
to map spatial coordinates to pixel values. 3D GS, with its explicit scene
representations and differentiable rendering algorithms, not only promises
real-time rendering capabilities but also introduces unprecedented levels of
control and editability. This positions 3D GS as a potential game-changer for
the next generation of 3D reconstruction and representation. In the present
paper, we provide the first systematic overview of the recent developments and
critical contributions in the domain of 3D GS. We begin with a detailed
exploration of the underlying principles and the driving forces behind the
advent of 3D GS, setting the stage for understanding its significance. A focal
point of our discussion is the practical applicability of 3D GS. By
facilitating real-time performance, 3D GS opens up a plethora of applications,
ranging from virtual reality to interactive media and beyond. This is
complemented by a comparative analysis of leading 3D GS models, evaluated
across various benchmark tasks to highlight their performance and practical
utility. The survey concludes by identifying current challenges and suggesting
potential avenues for future research in this domain. Through this survey, we
aim to provide a valuable resource for both newcomers and seasoned researchers,
fostering further exploration and advancement in applicable and explicit
radiance field representation.

Comments:
- Ongoing project

---

## RustNeRF: Robust Neural Radiance Field with Low-Quality Images

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-06 | Mengfei Li, Ming Lu, Xiaofang Li, Shanghang Zhang | cs.CV | [PDF](http://arxiv.org/pdf/2401.03257v1){: .btn .btn-green } |

**Abstract**: Recent work on Neural Radiance Fields (NeRF) exploits multi-view 3D
consistency, achieving impressive results in 3D scene modeling and
high-fidelity novel-view synthesis. However, there are limitations. First,
existing methods assume enough high-quality images are available for training
the NeRF model, ignoring real-world image degradation. Second, previous methods
struggle with ambiguity in the training set due to unmodeled inconsistencies
among different views. In this work, we present RustNeRF for real-world
high-quality NeRF. To improve NeRF's robustness under real-world inputs, we
train a 3D-aware preprocessing network that incorporates real-world degradation
modeling. We propose a novel implicit multi-view guidance to address
information loss during image degradation and restoration. Extensive
experiments demonstrate RustNeRF's advantages over existing approaches under
real-world degradation. The code will be released.



---

## Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity  Monocular Dense Mapping

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-06 | Tongyan Hua, Haotian Bai, Zidong Cao, Ming Liu, Dacheng Tao, Lin Wang | cs.CV | [PDF](http://arxiv.org/pdf/2401.03203v1){: .btn .btn-green } |

**Abstract**: In this paper, we introduce Hi-Map, a novel monocular dense mapping approach
based on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to
achieve efficient and high-fidelity mapping using only posed RGB inputs. Our
method eliminates the need for external depth priors derived from e.g., a depth
estimation model. Our key idea is to represent the scene as a hierarchical
feature grid that encodes the radiance and then factorizes it into feature
planes and vectors. As such, the scene representation becomes simpler and more
generalizable for fast and smooth convergence on new observations. This allows
for efficient computation while alleviating noise patterns by reducing the
complexity of the scene representation. Buttressed by the hierarchical
factorized representation, we leverage the Sign Distance Field (SDF) as a proxy
of rendering for inferring the volume density, demonstrating high mapping
fidelity. Moreover, we introduce a dual-path encoding strategy to strengthen
the photometric cues and further boost the mapping quality, especially for the
distant and textureless regions. Extensive experiments demonstrate our method's
superiority in geometric and textural accuracy over the state-of-the-art
NeRF-based monocular mapping methods.



---

## Progress and Prospects in 3D Generative AI: A Technical Overview  including 3D human

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-05 | Song Bai, Jie Li | cs.AI | [PDF](http://arxiv.org/pdf/2401.02620v1){: .btn .btn-green } |

**Abstract**: While AI-generated text and 2D images continue to expand its territory, 3D
generation has gradually emerged as a trend that cannot be ignored. Since the
year 2023 an abundant amount of research papers has emerged in the domain of 3D
generation. This growth encompasses not just the creation of 3D objects, but
also the rapid development of 3D character and motion generation. Several key
factors contribute to this progress. The enhanced fidelity in stable diffusion,
coupled with control methods that ensure multi-view consistency, and realistic
human models like SMPL-X, contribute synergistically to the production of 3D
models with remarkable consistency and near-realistic appearances. The
advancements in neural network-based 3D storing and rendering models, such as
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have
accelerated the efficiency and realism of neural rendered models. Furthermore,
the multimodality capabilities of large language models have enabled language
inputs to transcend into human motion outputs. This paper aims to provide a
comprehensive overview and summary of the relevant papers published mostly
during the latter half year of 2023. It will begin by discussing the AI
generated object models in 3D, followed by the generated 3D human models, and
finally, the generated 3D human motions, culminating in a conclusive summary
and a vision for the future.



---

## FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face  Video Editing on Dynamic NeRF

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-05 | Hao Zhang, Yu-Wing Tai, Chi-Keung Tang | cs.CV | [PDF](http://arxiv.org/pdf/2401.02616v1){: .btn .btn-green } |

**Abstract**: The success of the GAN-NeRF structure has enabled face editing on NeRF to
maintain 3D view consistency. However, achieving simultaneously multi-view
consistency and temporal coherence while editing video sequences remains a
formidable challenge. This paper proposes a novel face video editing
architecture built upon the dynamic face GAN-NeRF structure, which effectively
utilizes video sequences to restore the latent code and 3D face geometry. By
editing the latent code, multi-view consistent editing on the face can be
ensured, as validated by multiview stereo reconstruction on the resulting
edited images in our dynamic NeRF. As the estimation of face geometries occurs
on a frame-by-frame basis, this may introduce a jittering issue. We propose a
stabilizer that maintains temporal coherence by preserving smooth changes of
face expressions in consecutive frames. Quantitative and qualitative analyses
reveal that our method, as the pioneering 4D face video editor, achieves
state-of-the-art performance in comparison to existing 2D or 3D-based
approaches independently addressing identity and motion. Codes will be
released.

Comments:
- Our code will be available at: https://github.com/ZHANG1023/FED-NeRF

---

## Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-05 | Van Minh Nguyen, Emma Sandidge, Trupti Mahendrakar, Ryan T. White | cs.CV | [PDF](http://arxiv.org/pdf/2401.02588v1){: .btn .btn-green } |

**Abstract**: The accelerating deployment of spacecraft in orbit have generated interest in
on-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possible unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. This requires robust characterization of the
target's geometry. In this article, we present an approach for mapping
geometries of satellites on orbit based on 3D Gaussian Splatting that can run
on computing resources available on current spaceflight hardware. We
demonstrate model training and 3D rendering performance on a
hardware-in-the-loop satellite mock-up under several realistic lighting and
motion conditions. Our model is shown to be capable of training on-board and
rendering higher quality novel views of an unknown satellite nearly 2 orders of
magnitude faster than previous NeRF-based algorithms. Such on-board
capabilities are critical to enable downstream machine intelligence tasks
necessary for autonomous guidance, navigation, and control tasks.

Comments:
- 11 pages, 5 figures

---

## PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for  6DOF Object Pose Dataset Generation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-04 | Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae | cs.CV | [PDF](http://arxiv.org/pdf/2401.02281v1){: .btn .btn-green } |

**Abstract**: We introduce Physically Enhanced Gaussian Splatting Simulation System
(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset
generator based on 3D Gaussian Splatting. Environment and object
representations can be easily obtained using commodity cameras to reconstruct
with Gaussian Splatting. PEGASUS allows the composition of new scenes by
merging the respective underlying Gaussian Splatting point cloud of an
environment with one or multiple objects. Leveraging a physics engine enables
the simulation of natural object placement within a scene through interaction
between meshes extracted for the objects and the environment. Consequently, an
extensive amount of new scenes - static or dynamic - can be created by
combining different environments and objects. By rendering scenes from various
perspectives, diverse data points such as RGB images, depth maps, semantic
masks, and 6DoF object poses can be extracted. Our study demonstrates that
training on data generated by PEGASUS enables pose estimation networks to
successfully transfer from synthetic data to real-world data. Moreover, we
introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This
dataset includes spherical scans that captures images from both object
hemisphere and the Gaussian Splatting reconstruction, making them compatible
with PEGASUS.

Comments:
- Project Page: https://meyerls.github.io/pegasus_web

---

## SIGNeRF: Scene Integrated Generation for Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-03 | Jan-Niklas Dihlmann, Andreas Engelhardt, Hendrik Lensch | cs.CV | [PDF](http://arxiv.org/pdf/2401.01647v1){: .btn .btn-green } |

**Abstract**: Advances in image diffusion models have recently led to notable improvements
in the generation of high-quality images. In combination with Neural Radiance
Fields (NeRFs), they enabled new opportunities in 3D generation. However, most
generative 3D approaches are object-centric and applying them to editing
existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel
approach for fast and controllable NeRF scene editing and scene-integrated
object generation. A new generative update strategy ensures 3D consistency
across the edited images, without requiring iterative optimization. We find
that depth-conditioned diffusion models inherently possess the capability to
generate 3D consistent views by requesting a grid of images instead of single
views. Based on these insights, we introduce a multi-view reference sheet of
modified images. Our method updates an image collection consistently based on
the reference sheet and refines the original NeRF with the newly generated
image set in one go. By exploiting the depth conditioning mechanism of the
image diffusion model, we gain fine control over the spatial location of the
edit and enforce shape guidance by a selected region or an external mesh.

Comments:
- Project Page: https://signerf.jdihlmann.com

---

## FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D  Scene Understanding

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-03 | Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li | cs.CV | [PDF](http://arxiv.org/pdf/2401.01970v1){: .btn .btn-green } |

**Abstract**: Precisely perceiving the geometric and semantic properties of real-world 3D
objects is crucial for the continued evolution of augmented reality and robotic
applications. To this end, we present \algfull{} (\algname{}), which
incorporates vision-language embeddings of foundation models into 3D Gaussian
Splatting (GS). The key contribution of this work is an efficient method to
reconstruct and represent 3D vision-language models. This is achieved by
distilling feature maps generated from image-based foundation models into those
rendered from our 3D model. To ensure high-quality rendering and fast training,
we introduce a novel scene representation by integrating strengths from both GS
and multi-resolution hash encodings (MHE). Our effective training procedure
also introduces a pixel alignment loss that makes the rendered feature distance
of same semantic entities close, following the pixel-level semantic boundaries.
Our results demonstrate remarkable multi-view semantic consistency,
facilitating diverse downstream tasks, beating state-of-the-art methods by
$\mathbf{10.2}$ percent on open-vocabulary language-based object detection,
despite that we are $\mathbf{851\times}$ faster for inference. This research
explores the intersection of vision, language, and 3D scene representation,
paving the way for enhanced scene understanding in uncontrolled real-world
environments. We plan to release the code upon paper acceptance.

Comments:
- 19 pages, Project page coming soon

---

## Street Gaussians for Modeling Dynamic Urban Scenes

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-02 | Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng | cs.CV | [PDF](http://arxiv.org/pdf/2401.01339v1){: .btn .btn-green } |

**Abstract**: This paper aims to tackle the problem of modeling dynamic urban street scenes
from monocular videos. Recent methods extend NeRF by incorporating tracked
vehicle poses to animate vehicles, enabling photo-realistic view synthesis of
dynamic urban street scenes. However, significant limitations are their slow
training and rendering speed, coupled with the critical need for high precision
in tracked vehicle poses. We introduce Street Gaussians, a new explicit scene
representation that tackles all these limitations. Specifically, the dynamic
urban street is represented as a set of point clouds equipped with semantic
logits and 3D Gaussians, each associated with either a foreground vehicle or
the background. To model the dynamics of foreground object vehicles, each
object point cloud is optimized with optimizable tracked poses, along with a
dynamic spherical harmonics model for the dynamic appearance. The explicit
representation allows easy composition of object vehicles and background, which
in turn allows for scene editing operations and rendering at 133 FPS
(1066$\times$1600 resolution) within half an hour of training. The proposed
method is evaluated on multiple challenging benchmarks, including KITTI and
Waymo Open datasets. Experiments show that the proposed method consistently
outperforms state-of-the-art methods across all datasets. Furthermore, the
proposed representation delivers performance on par with that achieved using
precise ground-truth poses, despite relying only on poses from an off-the-shelf
tracker. The code is available at https://zju3dv.github.io/street_gaussians/.

Comments:
- Project page: https://zju3dv.github.io/street_gaussians/

---

## 3D Visibility-aware Generalizable Neural Radiance Fields for Interacting  Hands

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-02 | Xuan Huang, Hanhui Li, Zejun Yang, Zhisheng Wang, Xiaodan Liang | cs.CV | [PDF](http://arxiv.org/pdf/2401.00979v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRFs) are promising 3D representations for scenes,
objects, and humans. However, most existing methods require multi-view inputs
and per-scene training, which limits their real-life applications. Moreover,
current methods focus on single-subject cases, leaving scenes of interacting
hands that involve severe inter-hand occlusions and challenging view variations
remain unsolved. To tackle these issues, this paper proposes a generalizable
visibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically,
given an image of interacting hands as input, our VA-NeRF first obtains a
mesh-based representation of hands and extracts their corresponding geometric
and textural features. Subsequently, a feature fusion module that exploits the
visibility of query points and mesh vertices is introduced to adaptively merge
features of both hands, enabling the recovery of features in unseen areas.
Additionally, our VA-NeRF is optimized together with a novel discriminator
within an adversarial learning paradigm. In contrast to conventional
discriminators that predict a single real/fake label for the synthesized image,
the proposed discriminator generates a pixel-wise visibility map, providing
fine-grained supervision for unseen areas and encouraging the VA-NeRF to
improve the visual quality of synthesized images. Experiments on the
Interhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms
conventional NeRFs significantly. Project Page:
\url{https://github.com/XuanHuang0/VANeRF}.

Comments:
- Accepted by AAAI-24

---

## Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable  Noise

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-02 | Qinglong Huang, Yong Liao, Yanbin Hao, Pengyuan Zhou | cs.CV | [PDF](http://arxiv.org/pdf/2401.01216v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) have been proposed as an innovative 3D
representation method. While attracting lots of attention, NeRF faces critical
issues such as information confidentiality and security. Steganography is a
technique used to embed information in another object as a means of protecting
information security. Currently, there are few related studies on NeRF
steganography, facing challenges in low steganography quality, model weight
damage, and a limited amount of steganographic information. This paper proposes
a novel NeRF steganography method based on trainable noise: Noise-NeRF.
Furthermore, we propose the Adaptive Pixel Selection strategy and Pixel
Perturbation strategy to improve the steganography quality and efficiency. The
extensive experiments on open-source datasets show that Noise-NeRF provides
state-of-the-art performances in both steganography quality and rendering
quality, as well as effectiveness in super-resolution image steganography.



---

## Deblurring 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-01 | Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2401.00834v1){: .btn .btn-green } |

**Abstract**: Recent studies in Radiance Fields have paved the robust way for novel view
synthesis with their photorealistic rendering quality. Nevertheless, they
usually employ neural networks and volumetric rendering, which are costly to
train and impede their broad use in various real-time applications due to the
lengthy rendering time. Lately 3D Gaussians splatting-based approach has been
proposed to model the 3D scene, and it achieves remarkable visual quality while
rendering the images in real-time. However, it suffers from severe degradation
in the rendering quality if the training images are blurry. Blurriness commonly
occurs due to the lens defocusing, object motion, and camera shake, and it
inevitably intervenes in clean image acquisition. Several previous studies have
attempted to render clean and sharp images from blurry input images using
neural fields. The majority of those works, however, are designed only for
volumetric rendering-based neural radiance fields and are not straightforwardly
applicable to rasterization-based 3D Gaussian splatting methods. Thus, we
propose a novel real-time deblurring framework, deblurring 3D Gaussian
Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the
covariance of each 3D Gaussian to model the scene blurriness. While deblurring
3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct
fine and sharp details from blurry images. A variety of experiments have been
conducted on the benchmark, and the results have revealed the effectiveness of
our approach for deblurring. Qualitative results are available at
https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/

Comments:
- 19 pages, 8 figures

---

## Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using  Sharpness Prior

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-01 | Byeonghyeon Lee, Howoong Lee, Usman Ali, Eunbyung Park | cs.CV | [PDF](http://arxiv.org/pdf/2401.00825v1){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) have shown remarkable performance in neural
rendering-based novel view synthesis. However, NeRF suffers from severe visual
quality degradation when the input images have been captured under imperfect
conditions, such as poor illumination, defocus blurring, and lens aberrations.
Especially, defocus blur is quite common in the images when they are normally
captured using cameras. Although few recent studies have proposed to render
sharp images of considerably high-quality, yet they still face many key
challenges. In particular, those methods have employed a Multi-Layer Perceptron
(MLP) based NeRF, which requires tremendous computational time. To overcome
these shortcomings, this paper proposes a novel technique Sharp-NeRF -- a
grid-based NeRF that renders clean and sharp images from the input blurry
images within half an hour of training. To do so, we used several grid-based
kernels to accurately model the sharpness/blurriness of the scene. The
sharpness level of the pixels is computed to learn the spatially varying blur
kernels. We have conducted experiments on the benchmarks consisting of blurry
images and have evaluated full-reference and non-reference metrics. The
qualitative and quantitative results have revealed that our approach renders
the sharp novel views with vivid colors and fine details, and it has
considerably faster training time than the previous works. Our project page is
available at https://benhenryl.github.io/SharpNeRF/

Comments:
- Accepted to WACV 2024

---

## GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for  One-shot Generalizable Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2024-01-01 | Xiao Pan, Zongxin Yang, Shuai Bai, Yi Yang | cs.CV | [PDF](http://arxiv.org/pdf/2401.00616v2){: .btn .btn-green } |

**Abstract**: In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task
which targets synthesizing photo-realistic novel views given only one reference
image per scene. Previous One-shot Generalizable Neural Radiance Fields
(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,
yet suffer the blurry issue due to the encoder-only architecture that highly
relies on the limited reference image. On the other hand, recent
diffusion-based image-to-3d methods show vivid plausible results via distilling
pre-trained 2D diffusion models into a 3D representation, yet require tedious
per-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a
Generative Detail compensation framework via GAN and Diffusion that is both
inference-time finetuning-free and with vivid plausible details. In detail,
following a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a
One-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer
(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model
into the existing OG-NeRF pipeline for primarily relieving the blurry issue
with in-distribution priors captured from the training dataset, achieving a
good balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at
the fine stage, Diff3DE further leverages the pre-trained image diffusion
models to complement rich out-distribution details while maintaining decent 3D
consistency. Extensive experiments on both the synthetic and real-world
datasets show that GD$^2$-NeRF noticeably improves the details while without
per-scene finetuning.

Comments:
- Reading with Macbook Preview is recommended for best quality;
  Submitted to Journal
