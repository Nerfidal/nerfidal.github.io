---
layout: default
title: November 2025
parent: Papers
nav_order: 202511
---

<!---metadata--->


## DentalSplat: Dental Occlusion Novel View Synthesis from Sparse  Intra-Oral Photographs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-05 | Yiyi Miao, Taoyu Wu, Tong Chen, Sihao Li, Ji Jiang, Youpeng Yang, Angelos Stefanidis, Limin Yu, Jionglong Su | cs.CV | [PDF](http://arxiv.org/pdf/2511.03099v1){: .btn .btn-green } |

**Abstract**: In orthodontic treatment, particularly within telemedicine contexts,
observing patients' dental occlusion from multiple viewpoints facilitates
timely clinical decision-making. Recent advances in 3D Gaussian Splatting
(3DGS) have shown strong potential in 3D reconstruction and novel view
synthesis. However, conventional 3DGS pipelines typically rely on densely
captured multi-view inputs and precisely initialized camera poses, limiting
their practicality. Orthodontic cases, in contrast, often comprise only three
sparse images, specifically, the anterior view and bilateral buccal views,
rendering the reconstruction task especially challenging. The extreme sparsity
of input views severely degrades reconstruction quality, while the absence of
camera pose information further complicates the process. To overcome these
limitations, we propose DentalSplat, an effective framework for 3D
reconstruction from sparse orthodontic imagery. Our method leverages a
prior-guided dense stereo reconstruction model to initialize the point cloud,
followed by a scale-adaptive pruning strategy to improve the training
efficiency and reconstruction quality of 3DGS. In scenarios with extremely
sparse viewpoints, we further incorporate optical flow as a geometric
constraint, coupled with gradient regularization, to enhance rendering
fidelity. We validate our approach on a large-scale dataset comprising 950
clinical cases and an additional video-based test set of 195 cases designed to
simulate real-world remote orthodontic imaging conditions. Experimental results
demonstrate that our method effectively handles sparse input scenarios and
achieves superior novel view synthesis quality for dental occlusion
visualization, outperforming state-of-the-art techniques.



---

## Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction  and Phenotyping

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-04 | Jiajia Li, Keyi Zhu, Qianwen Zhang, Dong Chen, Qi Sun, Zhaojian Li | cs.CV | [PDF](http://arxiv.org/pdf/2511.02207v1){: .btn .btn-green } |

**Abstract**: Strawberries are among the most economically significant fruits in the United
States, generating over $2 billion in annual farm-gate sales and accounting for
approximately 13% of the total fruit production value. Plant phenotyping plays
a vital role in selecting superior cultivars by characterizing plant traits
such as morphology, canopy structure, and growth dynamics. However, traditional
plant phenotyping methods are time-consuming, labor-intensive, and often
destructive. Recently, neural rendering techniques, notably Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful
frameworks for high-fidelity 3D reconstruction. By capturing a sequence of
multi-view images or videos around a target plant, these methods enable
non-destructive reconstruction of complex plant architectures. Despite their
promise, most current applications of 3DGS in agricultural domains reconstruct
the entire scene, including background elements, which introduces noise,
increases computational costs, and complicates downstream trait analysis. To
address this limitation, we propose a novel object-centric 3D reconstruction
framework incorporating a preprocessing pipeline that leverages the Segment
Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean
strawberry plant reconstructions. This approach produces more accurate
geometric representations while substantially reducing computational time. With
a background-free reconstruction, our algorithm can automatically estimate
important plant traits, such as plant height and canopy width, using DBSCAN
clustering and Principal Component Analysis (PCA). Experimental results show
that our method outperforms conventional pipelines in both accuracy and
efficiency, offering a scalable and non-destructive solution for strawberry
plant phenotyping.

Comments:
- 11 pages, 4 figures, 3 tables

---

## PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction  & Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-04 | Antonio Oroz, Matthias Nie√üner, Tobias Kirschstein | cs.CV | [PDF](http://arxiv.org/pdf/2511.02777v1){: .btn .btn-green } |

**Abstract**: We present PercHead, a method for single-image 3D head reconstruction and
semantic 3D editing - two tasks that are inherently challenging due to severe
view occlusions, weak perceptual supervision, and the ambiguity of editing in
3D space. We develop a unified base model for reconstructing view-consistent 3D
heads from a single input image. The model employs a dual-branch encoder
followed by a ViT-based decoder that lifts 2D features into 3D space through
iterative cross-attention. Rendering is performed using Gaussian Splatting. At
the heart of our approach is a novel perceptual supervision strategy based on
DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric
and appearance fidelity. Our model achieves state-of-the-art performance in
novel-view synthesis and, furthermore, exhibits exceptional robustness to
extreme viewing angles compared to established baselines. Furthermore, this
base model can be seamlessly extended for semantic 3D editing by swapping the
encoder and finetuning the network. In this variant, we disentangle geometry
and style through two distinct input modalities: a segmentation map to control
geometry and either a text prompt or a reference image to specify appearance.
We highlight the intuitive and powerful 3D editing capabilities of our model
through a lightweight, interactive GUI, where users can effortlessly sculpt
geometry by drawing segmentation maps and stylize appearance via natural
language or image prompts.
  Project Page: https://antoniooroz.github.io/PercHead Video:
https://www.youtube.com/watch?v=4hFybgTk4kE

Comments:
- Project Page: https://antoniooroz.github.io/PercHead/ Video:
  https://www.youtube.com/watch?v=4hFybgTk4kE

---

## LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel  Rasterization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-04 | Jee Won Lee, Jongseong Brad Choi | cs.CV | [PDF](http://arxiv.org/pdf/2511.02510v1){: .btn .btn-green } |

**Abstract**: Sparse-voxel rasterization is a fast, differentiable alternative for
optimization-based scene reconstruction, but it tends to underfit low-frequency
content, depends on brittle pruning heuristics, and can overgrow in ways that
inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that
makes SV rasterization both steadier and lighter. Our loss is made
low-frequency aware via an inverse-Sobel reweighting with a mid-training
gamma-ramp, shifting gradient budget to flat regions only after geometry
stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning
logic on maximum blending weight, stabilized by EMA-hysteresis guards and
refines structure through ray-footprint-based, priority-driven subdivision
under an explicit growth budget. Ablations and full-system results across
Mip-NeRF 360 (6scenes) and Tanks & Temples (3scenes) datasets show mitigation
of errors in low-frequency regions and boundary instability while keeping
PSNR/SSIM, training time, and FPS comparable to a strong SVRaster pipeline.
Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency
detail that prior setups miss, enabling more predictable, memory-efficient
training without sacrificing perceptual quality.



---

## 4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized  Guassian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-01 | Chun-Tin Wu, Jun-Cheng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2511.00560v1){: .btn .btn-green } |

**Abstract**: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

Comments:
- 10 pages, 7 figures
