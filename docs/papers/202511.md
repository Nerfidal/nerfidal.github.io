---
layout: default
title: November 2025
parent: Papers
nav_order: 202511
---

<!---metadata--->


## 4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes  from Monocular Videos

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-07 | Mengqi Guo, Bo Xu, Yanyan Li, Gim Hee Lee | cs.CV | [PDF](http://arxiv.org/pdf/2511.05229v1){: .btn .btn-green } |

**Abstract**: Novel view synthesis from monocular videos of dynamic scenes with unknown
camera poses remains a fundamental challenge in computer vision and graphics.
While recent advances in 3D representations such as Neural Radiance Fields
(NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static
scenes, they struggle with dynamic content and typically rely on pre-computed
camera poses. We present 4D3R, a pose-free dynamic neural rendering framework
that decouples static and dynamic components through a two-stage approach. Our
method first leverages 3D foundational models for initial pose and geometry
estimation, followed by motion-aware refinement. 4D3R introduces two key
technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that
combines transformer-based learned priors with SAM2 for robust dynamic object
segmentation, enabling more accurate camera pose refinement; and (2) an
efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses
control points with a deformation field MLP and linear blend skinning to model
dynamic motion, significantly reducing computational cost while maintaining
high-quality reconstruction. Extensive experiments on real-world dynamic
datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement
over state-of-the-art methods, particularly in challenging scenarios with large
dynamic objects, while reducing computational requirements by 5x compared to
previous dynamic scene representations.

Comments:
- 17 pages, 5 figures

---

## CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-07 | Hexu Zhao, Xiwen Min, Xiaoteng Liu, Moonjun Gong, Yiming Li, Ang Li, Saining Xie, Jinyang Li, Aurojit Panda | cs.CV | [PDF](http://arxiv.org/pdf/2511.04951v1){: .btn .btn-green } |

**Abstract**: 3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis
approach due to its fast rendering time, and high-quality output. However,
scaling 3DGS to large (or intricate) scenes is challenging due to its large
memory requirement, which exceed most GPU's memory capacity. In this paper, we
describe CLM, a system that allows 3DGS to render large scenes using a single
consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU
memory, and loading them into GPU memory only when necessary. To reduce
performance and communication overheads, CLM uses a novel offloading strategy
that exploits observations about 3DGS's memory access pattern for pipelining,
and thus overlap GPU-to-CPU communication, GPU computation and CPU computation.
Furthermore, we also exploit observation about the access pattern to reduce
communication volume. Our evaluation shows that the resulting implementation
can render a large scene that requires 100 million Gaussians on a single
RTX4090 and achieve state-of-the-art reconstruction quality.

Comments:
- Accepted to appear in the 2026 ACM International Conference on
  Architectural Support for Programming Languages and Operating Systems

---

## Channel Knowledge Map Construction: Recent Advances and Open Challenges

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-07 | Zixiang Ren, Juncong Zhou, Jie Xu, Ling Qiu, Yong Zeng, Han Hu, Juyong Zhang, Rui Zhang | eess.SP | [PDF](http://arxiv.org/pdf/2511.04944v1){: .btn .btn-green } |

**Abstract**: Channel knowledge map (CKM) has emerged as a pivotal technology for
environment-aware wireless communications and sensing, which provides a priori
location-specific channel knowledge to facilitate network optimization.
Efficient CKM construction is an important technical problem for its effective
implementation. This article provides a comprehensive overview of recent
advances in CKM construction. First, we examine classical interpolation-based
CKM construction methods, highlighting their limitations in practical
deployments. Next, we explore image processing and generative artificial
intelligence (AI) techniques, which leverage feature extraction to construct
CKMs based on environmental knowledge. Furthermore, we present emerging
wireless radiance field (WRF) frameworks that exploit neural radiance fields or
Gaussian splatting to construct high-fidelity CKMs from sparse measurement
data. Finally, we outline various future research directions in real-time and
cross-domain CKM construction, as well as cost-efficient deployment of CKMs.



---

## Splatography: Sparse multi-view dynamic Gaussian Splatting for  filmmaking challenges

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-07 | Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull | cs.CV | [PDF](http://arxiv.org/pdf/2511.05152v1){: .btn .btn-green } |

**Abstract**: Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D
reconstruction from dense multi-view video (MVV) by learning to deform a
canonical GS representation. However, in filmmaking, tight budgets can result
in sparse camera configurations, which limits state-of-the-art (SotA) methods
when capturing complex dynamic features. To address this issue, we introduce an
approach that splits the canonical Gaussians and deformation field into
foreground and background components using a sparse set of masks for frames at
t=0. Each representation is separately trained on different loss functions
during canonical pre-training. Then, during dynamic training, different
parameters are modeled for each deformation field following common filmmaking
practices. The foreground stage contains diverse dynamic features so changes in
color, position and rotation are learned. While, the background containing
film-crew and equipment, is typically dimmer and less dynamic so only changes
in point position are learned. Experiments on 3-D and 2.5-D entertainment
datasets show that our method produces SotA qualitative and quantitative
results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the
SotA and without the need for dense mask supervision, our method also produces
segmented dynamic reconstructions including transparent and dynamic textures.
Code and video comparisons are available online:
https://interims-git.github.io/



---

## Efficient representation of 3D spatial data for defense-related  applications

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-07 | Benjamin Kahl, Marcus Hebel, Michael Arens | cs.GR | [PDF](http://arxiv.org/pdf/2511.05109v1){: .btn .btn-green } |

**Abstract**: Geospatial sensor data is essential for modern defense and security, offering
indispensable 3D information for situational awareness. This data, gathered
from sources like lidar sensors and optical cameras, allows for the creation of
detailed models of operational environments. In this paper, we provide a
comparative analysis of traditional representation methods, such as point
clouds, voxel grids, and triangle meshes, alongside modern neural and implicit
techniques like Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting
(3DGS). Our evaluation reveals a fundamental trade-off: traditional models
offer robust geometric accuracy ideal for functional tasks like line-of-sight
analysis and physics simulations, while modern methods excel at producing
high-fidelity, photorealistic visuals but often lack geometric reliability.
Based on these findings, we conclude that a hybrid approach is the most
promising path forward. We propose a system architecture that combines a
traditional mesh scaffold for geometric integrity with a neural representation
like 3DGS for visual detail, managed within a hierarchical scene structure to
ensure scalability and performance.



---

## FastGS: Training 3D Gaussian Splatting in 100 Seconds

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-06 | Shiwei Ren, Tianci Wen, Yongchun Fang, Biao Lu | cs.CV | [PDF](http://arxiv.org/pdf/2511.04283v1){: .btn .btn-green } |

**Abstract**: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to
properly regulate the number of Gaussians during training, causing redundant
computational time overhead. In this paper, we propose FastGS, a novel, simple,
and general acceleration framework that fully considers the importance of each
Gaussian based on multi-view consistency, efficiently solving the trade-off
between training time and rendering quality. We innovatively design a
densification and pruning strategy based on multi-view consistency, dispensing
with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &
Temples, and Deep Blending datasets demonstrate that our method significantly
outperforms the state-of-the-art methods in training speed, achieving a
3.32$\times$ training acceleration and comparable rendering quality compared
with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration
compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that
FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration
across various tasks, including dynamic scene reconstruction, surface
reconstruction, sparse-view reconstruction, large-scale reconstruction, and
simultaneous localization and mapping. The project page is available at
https://fastgs.github.io/

Comments:
- Project page: https://fastgs.github.io/

---

## CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian  Splatting Segmentation

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-06 | Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie | cs.CV | [PDF](http://arxiv.org/pdf/2511.03992v1){: .btn .btn-green } |

**Abstract**: Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.



---

## 3D Gaussian Point Encoders

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-06 | Jim James, Ben Wilson, Simon Lucey, James Hays | cs.CV | [PDF](http://arxiv.org/pdf/2511.04797v1){: .btn .btn-green } |

**Abstract**: In this work, we introduce the 3D Gaussian Point Encoder, an explicit
per-point embedding built on mixtures of learned 3D Gaussians. This explicit
geometric representation for 3D recognition tasks is a departure from widely
used implicit representations such as PointNet. However, it is difficult to
learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We
develop optimization techniques based on natural gradients and distillation
from PointNets to find a Gaussian Basis that can reconstruct PointNet
activations. The resulting 3D Gaussian Point Encoders are faster and more
parameter efficient than traditional PointNets. As in the 3D reconstruction
literature where there has been considerable interest in the move from implicit
(e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can
take advantage of computational geometry heuristics to accelerate 3D Gaussian
Point Encoders further. We extend filtering techniques from 3D Gaussian
Splatting to construct encoders that run 2.7 times faster as a comparable
accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore,
we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component
in Mamba3D, running 1.27 times faster and achieving a reduction in memory and
FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight
enough to achieve high framerates on CPU-only devices.

Comments:
- 10 pages, 3 figures, 3 tables

---

## Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation  of Soft-Body Interactions

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-06 | Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, Yunzhu Li | cs.RO | [PDF](http://arxiv.org/pdf/2511.04665v1){: .btn .btn-green } |

**Abstract**: Robotic manipulation policies are advancing rapidly, but their direct
evaluation in the real world remains costly, time-consuming, and difficult to
reproduce, particularly for tasks involving deformable objects. Simulation
provides a scalable and systematic alternative, yet existing simulators often
fail to capture the coupled visual and physical complexity of soft-body
interactions. We present a real-to-sim policy evaluation framework that
constructs soft-body digital twins from real-world videos and renders robots,
objects, and environments with photorealistic fidelity using 3D Gaussian
Splatting. We validate our approach on representative deformable manipulation
tasks, including plush toy packing, rope routing, and T-block pushing,
demonstrating that simulated rollouts correlate strongly with real-world
execution performance and reveal key behavioral patterns of learned policies.
Our results suggest that combining physics-informed reconstruction with
high-quality rendering enables reproducible, scalable, and accurate evaluation
of robotic manipulation policies. Website: https://real2sim-eval.github.io/

Comments:
- Website: https://real2sim-eval.github.io/

---

## DentalSplat: Dental Occlusion Novel View Synthesis from Sparse  Intra-Oral Photographs

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-05 | Yiyi Miao, Taoyu Wu, Tong Chen, Sihao Li, Ji Jiang, Youpeng Yang, Angelos Stefanidis, Limin Yu, Jionglong Su | cs.CV | [PDF](http://arxiv.org/pdf/2511.03099v1){: .btn .btn-green } |

**Abstract**: In orthodontic treatment, particularly within telemedicine contexts,
observing patients' dental occlusion from multiple viewpoints facilitates
timely clinical decision-making. Recent advances in 3D Gaussian Splatting
(3DGS) have shown strong potential in 3D reconstruction and novel view
synthesis. However, conventional 3DGS pipelines typically rely on densely
captured multi-view inputs and precisely initialized camera poses, limiting
their practicality. Orthodontic cases, in contrast, often comprise only three
sparse images, specifically, the anterior view and bilateral buccal views,
rendering the reconstruction task especially challenging. The extreme sparsity
of input views severely degrades reconstruction quality, while the absence of
camera pose information further complicates the process. To overcome these
limitations, we propose DentalSplat, an effective framework for 3D
reconstruction from sparse orthodontic imagery. Our method leverages a
prior-guided dense stereo reconstruction model to initialize the point cloud,
followed by a scale-adaptive pruning strategy to improve the training
efficiency and reconstruction quality of 3DGS. In scenarios with extremely
sparse viewpoints, we further incorporate optical flow as a geometric
constraint, coupled with gradient regularization, to enhance rendering
fidelity. We validate our approach on a large-scale dataset comprising 950
clinical cases and an additional video-based test set of 195 cases designed to
simulate real-world remote orthodontic imaging conditions. Experimental results
demonstrate that our method effectively handles sparse input scenarios and
achieves superior novel view synthesis quality for dental occlusion
visualization, outperforming state-of-the-art techniques.



---

## LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel  Rasterization

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-04 | Jee Won Lee, Jongseong Brad Choi | cs.CV | [PDF](http://arxiv.org/pdf/2511.02510v1){: .btn .btn-green } |

**Abstract**: Sparse-voxel rasterization is a fast, differentiable alternative for
optimization-based scene reconstruction, but it tends to underfit low-frequency
content, depends on brittle pruning heuristics, and can overgrow in ways that
inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that
makes SV rasterization both steadier and lighter. Our loss is made
low-frequency aware via an inverse-Sobel reweighting with a mid-training
gamma-ramp, shifting gradient budget to flat regions only after geometry
stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning
logic on maximum blending weight, stabilized by EMA-hysteresis guards and
refines structure through ray-footprint-based, priority-driven subdivision
under an explicit growth budget. Ablations and full-system results across
Mip-NeRF 360 (6scenes) and Tanks & Temples (3scenes) datasets show mitigation
of errors in low-frequency regions and boundary instability while keeping
PSNR/SSIM, training time, and FPS comparable to a strong SVRaster pipeline.
Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency
detail that prior setups miss, enabling more predictable, memory-efficient
training without sacrificing perceptual quality.



---

## Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction  and Phenotyping

gaussian splatting
{: .label .label-blue }

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-04 | Jiajia Li, Keyi Zhu, Qianwen Zhang, Dong Chen, Qi Sun, Zhaojian Li | cs.CV | [PDF](http://arxiv.org/pdf/2511.02207v1){: .btn .btn-green } |

**Abstract**: Strawberries are among the most economically significant fruits in the United
States, generating over $2 billion in annual farm-gate sales and accounting for
approximately 13% of the total fruit production value. Plant phenotyping plays
a vital role in selecting superior cultivars by characterizing plant traits
such as morphology, canopy structure, and growth dynamics. However, traditional
plant phenotyping methods are time-consuming, labor-intensive, and often
destructive. Recently, neural rendering techniques, notably Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful
frameworks for high-fidelity 3D reconstruction. By capturing a sequence of
multi-view images or videos around a target plant, these methods enable
non-destructive reconstruction of complex plant architectures. Despite their
promise, most current applications of 3DGS in agricultural domains reconstruct
the entire scene, including background elements, which introduces noise,
increases computational costs, and complicates downstream trait analysis. To
address this limitation, we propose a novel object-centric 3D reconstruction
framework incorporating a preprocessing pipeline that leverages the Segment
Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean
strawberry plant reconstructions. This approach produces more accurate
geometric representations while substantially reducing computational time. With
a background-free reconstruction, our algorithm can automatically estimate
important plant traits, such as plant height and canopy width, using DBSCAN
clustering and Principal Component Analysis (PCA). Experimental results show
that our method outperforms conventional pipelines in both accuracy and
efficiency, offering a scalable and non-destructive solution for strawberry
plant phenotyping.

Comments:
- 11 pages, 4 figures, 3 tables

---

## PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction  & Editing

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-04 | Antonio Oroz, Matthias Nie√üner, Tobias Kirschstein | cs.CV | [PDF](http://arxiv.org/pdf/2511.02777v1){: .btn .btn-green } |

**Abstract**: We present PercHead, a method for single-image 3D head reconstruction and
semantic 3D editing - two tasks that are inherently challenging due to severe
view occlusions, weak perceptual supervision, and the ambiguity of editing in
3D space. We develop a unified base model for reconstructing view-consistent 3D
heads from a single input image. The model employs a dual-branch encoder
followed by a ViT-based decoder that lifts 2D features into 3D space through
iterative cross-attention. Rendering is performed using Gaussian Splatting. At
the heart of our approach is a novel perceptual supervision strategy based on
DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric
and appearance fidelity. Our model achieves state-of-the-art performance in
novel-view synthesis and, furthermore, exhibits exceptional robustness to
extreme viewing angles compared to established baselines. Furthermore, this
base model can be seamlessly extended for semantic 3D editing by swapping the
encoder and finetuning the network. In this variant, we disentangle geometry
and style through two distinct input modalities: a segmentation map to control
geometry and either a text prompt or a reference image to specify appearance.
We highlight the intuitive and powerful 3D editing capabilities of our model
through a lightweight, interactive GUI, where users can effortlessly sculpt
geometry by drawing segmentation maps and stylize appearance via natural
language or image prompts.
  Project Page: https://antoniooroz.github.io/PercHead Video:
https://www.youtube.com/watch?v=4hFybgTk4kE

Comments:
- Project Page: https://antoniooroz.github.io/PercHead/ Video:
  https://www.youtube.com/watch?v=4hFybgTk4kE

---

## 4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized  Guassian Splatting

gaussian splatting
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2025-11-01 | Chun-Tin Wu, Jun-Cheng Chen | cs.CV | [PDF](http://arxiv.org/pdf/2511.00560v1){: .btn .btn-green } |

**Abstract**: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

Comments:
- 10 pages, 7 figures
