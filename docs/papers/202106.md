---
layout: default
title: June 2021
parent: Papers
nav_order: 202106
---
<!---metadata--->

## Animatable Neural Radiance Fields from Monocular RGB Videos

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2021-06-25 | Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, Huchuan Lu | cs.CV | [PDF](http://arxiv.org/pdf/2106.13629v2){: .btn .btn-green } |

**Abstract**: We present animatable neural radiance fields (animatable NeRF) for detailed
human avatar creation from monocular videos. Our approach extends neural
radiance fields (NeRF) to the dynamic scenes with human movements via
introducing explicit pose-guided deformation while learning the scene
representation network. In particular, we estimate the human pose for each
frame and learn a constant canonical space for the detailed human template,
which enables natural shape deformation from the observation space to the
canonical space under the explicit control of the pose parameters. To
compensate for inaccurate pose estimation, we introduce the pose refinement
strategy that updates the initial pose during the learning process, which not
only helps to learn more accurate human reconstruction but also accelerates the
convergence. In experiments we show that the proposed approach achieves 1)
implicit human geometry and appearance reconstruction with high-quality
details, 2) photo-realistic rendering of the human from novel views, and 3)
animation of the human with novel poses.

Comments:
- 12 pages, 12 figures

---

## Scene Uncertainty and the Wellington Posterior of Deterministic Image  Classifiers



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2021-06-25 | Stephanie Tsuei, Aditya Golatkar, Stefano Soatto | cs.CV | [PDF](http://arxiv.org/pdf/2106.13870v2){: .btn .btn-green } |

**Abstract**: We propose a method to estimate the uncertainty of the outcome of an image
classifier on a given input datum. Deep neural networks commonly used for image
classification are deterministic maps from an input image to an output class.
As such, their outcome on a given datum involves no uncertainty, so we must
specify what variability we are referring to when defining, measuring and
interpreting uncertainty, and attributing "confidence" to the outcome. To this
end, we introduce the Wellington Posterior, which is the distribution of
outcomes that would have been obtained in response to data that could have been
generated by the same scene that produced the given image. Since there are
infinitely many scenes that could have generated any given image, the
Wellington Posterior involves inductive transfer from scenes other than the one
portrayed. We explore the use of data augmentation, dropout, ensembling,
single-view reconstruction, and model linearization to compute a Wellington
Posterior. Additional methods include the use of conditional generative models
such as generative adversarial networks, neural radiance fields, and
conditional prior networks. We test these methods against the empirical
posterior obtained by performing inference on multiple images of the same
underlying scene. These developments are only a small step towards assessing
the reliability of deep network classifiers in a manner that is compatible with
safety-critical applications and human interpretation.

---

## HyperNeRF: A Higher-Dimensional Representation for Topologically Varying  Neural Radiance Fields

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2021-06-24 | Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz | cs.CV | [PDF](http://arxiv.org/pdf/2106.13228v2){: .btn .btn-green } |

**Abstract**: Neural Radiance Fields (NeRF) are able to reconstruct scenes with
unprecedented fidelity, and various recent works have extended NeRF to handle
dynamic scenes. A common approach to reconstruct such non-rigid scenes is
through the use of a learned deformation field mapping from coordinates in each
input image into a canonical template coordinate space. However, these
deformation-based approaches struggle to model changes in topology, as
topological changes require a discontinuity in the deformation field, but these
deformation fields are necessarily continuous. We address this limitation by
lifting NeRFs into a higher dimensional space, and by representing the 5D
radiance field corresponding to each individual input image as a slice through
this "hyper-space". Our method is inspired by level set methods, which model
the evolution of surfaces as slices through a higher dimensional surface. We
evaluate our method on two tasks: (i) interpolating smoothly between "moments",
i.e., configurations of the scene, seen in the input images while maintaining
visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
that our method, which we dub HyperNeRF, outperforms existing methods on both
tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for
interpolation and 8.6% for novel-view synthesis, as measured by LPIPS.
Additional videos, results, and visualizations are available at
https://hypernerf.github.io.

Comments:
- SIGGRAPH Asia 2021, Project page: https://hypernerf.github.io/

---

## Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single  Panorama

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2021-06-21 | Ching-Yu Hsu, Cheng Sun, Hwann-Tzong Chen | cs.CV | [PDF](http://arxiv.org/pdf/2106.10859v1){: .btn .btn-green } |

**Abstract**: We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first
method to the application of parallax-enabled novel panoramic view synthesis.
Recent works for novel view synthesis focus on perspective images with limited
field-of-view and require sufficient pictures captured in a specific condition.
Conversely, OmniNeRF can generate panorama images for unknown viewpoints given
a single equirectangular image as training data. To this end, we propose to
augment the single RGB-D panorama by projecting back and forth between a 3D
world and different 2D panoramic coordinates at different virtual camera
positions. By doing so, we are able to optimize an Omnidirectional Neural
Radiance Field with visible pixels collecting from omnidirectional viewing
angles at a fixed center for the estimation of new viewing angles from varying
camera positions. As a result, the proposed OmniNeRF achieves convincing
renderings of novel panoramic views that exhibit the parallax effect. We
showcase the effectiveness of each of our proposals on both synthetic and
real-world datasets.

---

## NeuS: Learning Neural Implicit Surfaces by Volume Rendering for  Multi-view Reconstruction

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2021-06-20 | Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang | cs.CV | [PDF](http://arxiv.org/pdf/2106.10689v3){: .btn .btn-green } |

**Abstract**: We present a novel neural surface reconstruction method, called NeuS, for
reconstructing objects and scenes with high fidelity from 2D image inputs.
Existing neural surface reconstruction approaches, such as DVR and IDR, require
foreground mask as supervision, easily get trapped in local minima, and
therefore struggle with the reconstruction of objects with severe
self-occlusion or thin structures. Meanwhile, recent neural methods for novel
view synthesis, such as NeRF and its variants, use volume rendering to produce
a neural scene representation with robustness of optimization, even for highly
complex objects. However, extracting high-quality surfaces from this learned
implicit representation is difficult because there are not sufficient surface
constraints in the representation. In NeuS, we propose to represent a surface
as the zero-level set of a signed distance function (SDF) and develop a new
volume rendering method to train a neural SDF representation. We observe that
the conventional volume rendering method causes inherent geometric errors (i.e.
bias) for surface reconstruction, and therefore propose a new formulation that
is free of bias in the first order of approximation, thus leading to more
accurate surface reconstruction even without the mask supervision. Experiments
on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the
state-of-the-arts in high-quality surface reconstruction, especially for
objects and scenes with complex structures and self-occlusion.

Comments:
- 23 pages

---

## NeRF in detail: Learning to sample for view synthesis

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2021-06-09 | Relja ArandjeloviÄ‡, Andrew Zisserman | cs.CV | [PDF](http://arxiv.org/pdf/2106.05264v1){: .btn .btn-green } |

**Abstract**: Neural radiance fields (NeRF) methods have demonstrated impressive novel view
synthesis performance. The core approach is to render individual rays by
querying a neural network at points sampled along the ray to obtain the density
and colour of the sampled points, and integrating this information using the
rendering equation. Since dense sampling is computationally prohibitive, a
common solution is to perform coarse-to-fine sampling.
  In this work we address a clear limitation of the vanilla coarse-to-fine
approach -- that it is based on a heuristic and not trained end-to-end for the
task at hand. We introduce a differentiable module that learns to propose
samples and their importance for the fine network, and consider and compare
multiple alternatives for its neural architecture. Training the proposal module
from scratch can be unstable due to lack of supervision, so an effective
pre-training strategy is also put forward. The approach, named `NeRF in detail'
(NeRF-ID), achieves superior view synthesis quality over NeRF and the
state-of-the-art on the synthetic Blender benchmark and on par or better
performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the
predicted sample importance, a 25% saving in computation can be achieved
without significantly sacrificing the rendering quality.

---

## Neural Actor: Neural Free-view Synthesis of Human Actors with Pose  Control



| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2021-06-03 | Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt | cs.CV | [PDF](http://arxiv.org/pdf/2106.02019v2){: .btn .btn-green } |

**Abstract**: We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.

---

## NeRFactor: Neural Factorization of Shape and Reflectance Under an  Unknown Illumination

nerf
{: .label .label-blue }

| Published | Authors | Category | |
|:---:|:---:|:---:|:---:|
| 2021-06-03 | Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, Jonathan T. Barron | cs.CV | [PDF](http://arxiv.org/pdf/2106.01970v2){: .btn .btn-green } |

**Abstract**: We address the problem of recovering the shape and spatially-varying
reflectance of an object from multi-view images (and their camera poses) of an
object illuminated by one unknown lighting condition. This enables the
rendering of novel views of the object under arbitrary environment lighting and
editing of the object's material properties. The key to our approach, which we
call Neural Radiance Factorization (NeRFactor), is to distill the volumetric
geometry of a Neural Radiance Field (NeRF) [Mildenhall et al. 2020]
representation of the object into a surface representation and then jointly
refine the geometry while solving for the spatially-varying reflectance and
environment lighting. Specifically, NeRFactor recovers 3D neural fields of
surface normals, light visibility, albedo, and Bidirectional Reflectance
Distribution Functions (BRDFs) without any supervision, using only a
re-rendering loss, simple smoothness priors, and a data-driven BRDF prior
learned from real-world BRDF measurements. By explicitly modeling light
visibility, NeRFactor is able to separate shadows from albedo and synthesize
realistic soft or hard shadows under arbitrary lighting conditions. NeRFactor
is able to recover convincing 3D models for free-viewpoint relighting in this
challenging and underconstrained capture setup for both synthetic and real
scenes. Qualitative and quantitative experiments show that NeRFactor
outperforms classic and deep learning-based state of the art across various
tasks. Our videos, code, and data are available at
people.csail.mit.edu/xiuming/projects/nerfactor/.

Comments:
- Camera-ready version for SIGGRAPH Asia 2021. Project Page:
  https://people.csail.mit.edu/xiuming/projects/nerfactor/